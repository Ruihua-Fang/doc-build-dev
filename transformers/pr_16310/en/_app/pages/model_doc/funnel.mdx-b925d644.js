import{S as w0,i as b0,s as $0,e as r,k as l,w as v,t,M as E0,c as a,d as n,m as d,a as i,x as y,h as o,b as c,F as e,g as h,y as w,q as b,o as $,B as E}from"../../chunks/vendor-6b77c823.js";import{T as ze}from"../../chunks/Tip-39098574.js";import{D as X}from"../../chunks/Docstring-abef54e3.js";import{C as Se}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as qe}from"../../chunks/IconCopyLink-7a11ce68.js";function M0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function z0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function q0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function P0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function C0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function j0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function x0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function L0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function O0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function D0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function A0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function N0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function I0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function S0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function B0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function W0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function Q0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function U0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function R0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function H0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function V0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function Y0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function K0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){u=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var k=i(u);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(p),g=a(p,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,k){h(p,u,k),e(u,M),h(p,m,k),h(p,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(p,K,k),h(p,q,k),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,k),h(p,L,k),e(L,te),h(p,G,k),h(p,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(p){p&&n(u),p&&n(m),p&&n(g),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function G0(W){let u,M,m,g,F;return{c(){u=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var _=i(u);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,u,_),e(u,M),e(u,m),e(m,g),e(u,F)},d(T){T&&n(u)}}}function Z0(W){let u,M,m,g,F,T,_,z,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge,p,k,Z,Te,ye,D,Fe,we,be,x,H,$e,ke,V,Ee,ve,_e,Me,Ya,Iu,Su,Ec,xn,Bu,No,Wu,Qu,Io,Uu,Ru,Mc,Zn,Bt,pl,So,Hu,ul,Vu,zc,Cn,Bo,Yu,jn,Ku,Ka,Gu,Zu,Ga,Xu,Ju,Wo,eh,nh,th,Xn,oh,Za,sh,rh,Xa,ah,ih,qc,Jn,Wt,hl,Qo,lh,fl,dh,Pc,Pe,Uo,ch,ml,ph,uh,Qt,Ja,hh,fh,ei,mh,gh,_h,Ro,Th,ni,Fh,kh,vh,Ln,Ho,yh,gl,wh,bh,Vo,ti,$h,_l,Eh,Mh,oi,zh,Tl,qh,Ph,Ut,Yo,Ch,Ko,jh,Fl,xh,Lh,Oh,yn,Go,Dh,kl,Ah,Nh,Zo,Ih,et,Sh,vl,Bh,Wh,yl,Qh,Uh,Rh,si,Xo,Cc,nt,Rt,wl,Jo,Hh,bl,Vh,jc,Ge,es,Yh,ns,Kh,$l,Gh,Zh,Xh,Ht,ri,Jh,ef,ai,nf,tf,of,ts,sf,ii,rf,af,lf,wn,os,df,El,cf,pf,ss,uf,tt,hf,Ml,ff,mf,zl,gf,_f,xc,ot,Vt,ql,rs,Tf,Pl,Ff,Lc,st,as,kf,is,vf,li,yf,wf,Oc,rt,ls,bf,ds,$f,di,Ef,Mf,Dc,at,Yt,Cl,cs,zf,jl,qf,Ac,We,ps,Pf,xl,Cf,jf,us,xf,hs,Lf,Of,Df,fs,Af,ci,Nf,If,Sf,ms,Bf,gs,Wf,Qf,Uf,Ze,_s,Rf,it,Hf,pi,Vf,Yf,Ll,Kf,Gf,Zf,Kt,Xf,Ol,Jf,em,Ts,Nc,lt,Gt,Dl,Fs,nm,Al,tm,Ic,Qe,ks,om,Nl,sm,rm,vs,am,ys,im,lm,dm,ws,cm,ui,pm,um,hm,bs,fm,$s,mm,gm,_m,Xe,Es,Tm,dt,Fm,hi,km,vm,Il,ym,wm,bm,Zt,$m,Sl,Em,Mm,Ms,Sc,ct,Xt,Bl,zs,zm,Wl,qm,Bc,pt,qs,Pm,Je,Ps,Cm,ut,jm,fi,xm,Lm,Ql,Om,Dm,Am,Jt,Nm,Ul,Im,Sm,Cs,Wc,ht,eo,Rl,js,Bm,Hl,Wm,Qc,Ue,xs,Qm,Ls,Um,Vl,Rm,Hm,Vm,Os,Ym,Ds,Km,Gm,Zm,As,Xm,mi,Jm,eg,ng,Ns,tg,Is,og,sg,rg,en,Ss,ag,ft,ig,gi,lg,dg,Yl,cg,pg,ug,no,hg,Kl,fg,mg,Bs,Uc,mt,to,Gl,Ws,gg,Zl,_g,Rc,Re,Qs,Tg,Xl,Fg,kg,Us,vg,Rs,yg,wg,bg,Hs,$g,_i,Eg,Mg,zg,Vs,qg,Ys,Pg,Cg,jg,Be,Ks,xg,gt,Lg,Ti,Og,Dg,Jl,Ag,Ng,Ig,oo,Sg,ed,Bg,Wg,Gs,Qg,nd,Ug,Rg,Zs,Hc,_t,so,td,Xs,Hg,od,Vg,Vc,He,Js,Yg,sd,Kg,Gg,er,Zg,nr,Xg,Jg,e_,tr,n_,Fi,t_,o_,s_,or,r_,sr,a_,i_,l_,nn,rr,d_,Tt,c_,ki,p_,u_,rd,h_,f_,m_,ro,g_,ad,__,T_,ar,Yc,Ft,ao,id,ir,F_,ld,k_,Kc,Ve,lr,v_,dd,y_,w_,dr,b_,cr,$_,E_,M_,pr,z_,vi,q_,P_,C_,ur,j_,hr,x_,L_,O_,tn,fr,D_,kt,A_,yi,N_,I_,cd,S_,B_,W_,io,Q_,pd,U_,R_,mr,Gc,vt,lo,ud,gr,H_,hd,V_,Zc,Ye,_r,Y_,yt,K_,fd,G_,Z_,md,X_,J_,eT,Tr,nT,Fr,tT,oT,sT,kr,rT,wi,aT,iT,lT,vr,dT,yr,cT,pT,uT,on,wr,hT,wt,fT,bi,mT,gT,gd,_T,TT,FT,co,kT,_d,vT,yT,br,Xc,bt,po,Td,$r,wT,Fd,bT,Jc,je,Er,$T,kd,ET,MT,Mr,zT,zr,qT,PT,CT,qr,jT,$i,xT,LT,OT,Pr,DT,Cr,AT,NT,IT,uo,ST,sn,jr,BT,$t,WT,Ei,QT,UT,vd,RT,HT,VT,ho,YT,yd,KT,GT,xr,ep,Et,fo,wd,Lr,ZT,bd,XT,np,xe,Or,JT,$d,e1,n1,Dr,t1,Ar,o1,s1,r1,Nr,a1,Mi,i1,l1,d1,Ir,c1,Sr,p1,u1,h1,mo,f1,rn,Br,m1,Mt,g1,zi,_1,T1,Ed,F1,k1,v1,go,y1,Md,w1,b1,Wr,tp,zt,_o,zd,Qr,$1,qd,E1,op,Le,Ur,M1,Pd,z1,q1,Rr,P1,Hr,C1,j1,x1,Vr,L1,qi,O1,D1,A1,Yr,N1,Kr,I1,S1,B1,To,W1,an,Gr,Q1,qt,U1,Pi,R1,H1,Cd,V1,Y1,K1,Fo,G1,jd,Z1,X1,Zr,sp,Pt,ko,xd,Xr,J1,Ld,eF,rp,Oe,Jr,nF,ea,tF,Od,oF,sF,rF,na,aF,ta,iF,lF,dF,oa,cF,Ci,pF,uF,hF,sa,fF,ra,mF,gF,_F,vo,TF,ln,aa,FF,Ct,kF,ji,vF,yF,Dd,wF,bF,$F,yo,EF,Ad,MF,zF,ia,ap,jt,wo,Nd,la,qF,Id,PF,ip,De,da,CF,Sd,jF,xF,ca,LF,pa,OF,DF,AF,ua,NF,xi,IF,SF,BF,ha,WF,fa,QF,UF,RF,bo,HF,dn,ma,VF,xt,YF,Li,KF,GF,Bd,ZF,XF,JF,$o,ek,Wd,nk,tk,ga,lp,Lt,Eo,Qd,_a,ok,Ud,sk,dp,Ae,Ta,rk,Rd,ak,ik,Fa,lk,ka,dk,ck,pk,va,uk,Oi,hk,fk,mk,ya,gk,wa,_k,Tk,Fk,Mo,kk,cn,ba,vk,Ot,yk,Di,wk,bk,Hd,$k,Ek,Mk,zo,zk,Vd,qk,Pk,$a,cp,Dt,qo,Yd,Ea,Ck,Kd,jk,pp,Ne,Ma,xk,Gd,Lk,Ok,za,Dk,qa,Ak,Nk,Ik,Pa,Sk,Ai,Bk,Wk,Qk,Ca,Uk,ja,Rk,Hk,Vk,Po,Yk,pn,xa,Kk,At,Gk,Ni,Zk,Xk,Zd,Jk,ev,nv,Co,tv,Xd,ov,sv,La,up,Nt,jo,Jd,Oa,rv,ec,av,hp,Ie,Da,iv,It,lv,nc,dv,cv,tc,pv,uv,hv,Aa,fv,Na,mv,gv,_v,Ia,Tv,Ii,Fv,kv,vv,Sa,yv,Ba,wv,bv,$v,xo,Ev,un,Wa,Mv,St,zv,Si,qv,Pv,oc,Cv,jv,xv,Lo,Lv,sc,Ov,Dv,Qa,fp;return T=new qe({}),ne=new qe({}),So=new qe({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Qo=new qe({}),Uo=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new Se({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new qe({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new Se({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new qe({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L834",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L980",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new qe({}),ps=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L927",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L943",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new ze({props:{$$slots:{default:[M0]},$$scope:{ctx:W}}}),Ts=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Fs=new qe({}),ks=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1004",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1021",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new ze({props:{$$slots:{default:[z0]},$$scope:{ctx:W}}}),Ms=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new qe({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1112"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1121",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new ze({props:{$$slots:{default:[q0]},$$scope:{ctx:W}}}),Cs=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),js=new qe({}),xs=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1195",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ss=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1211",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new ze({props:{$$slots:{default:[P0]},$$scope:{ctx:W}}}),Bs=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ws=new qe({}),Qs=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1275",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ks=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1286",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new ze({props:{$$slots:{default:[C0]},$$scope:{ctx:W}}}),Gs=new Se({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Zs=new Se({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Xs=new qe({}),Js=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1368",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1377",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new ze({props:{$$slots:{default:[j0]},$$scope:{ctx:W}}}),ar=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ir=new qe({}),lr=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1452",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fr=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1464",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new ze({props:{$$slots:{default:[x0]},$$scope:{ctx:W}}}),mr=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),gr=new qe({}),_r=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1526",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),wr=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_funnel.py#L1537",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new ze({props:{$$slots:{default:[L0]},$$scope:{ctx:W}}}),br=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
round(loss.item(), 2)


start_scores = outputs.start_logits
list(start_scores.shape)


end_scores = outputs.end_logits
list(end_scores.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(start_scores.shape)


<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(end_scores.shape)
`}}),$r=new qe({}),Er=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1097",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),uo=new ze({props:{$$slots:{default:[O0]},$$scope:{ctx:W}}}),jr=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1102",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ho=new ze({props:{$$slots:{default:[D0]},$$scope:{ctx:W}}}),xr=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lr=new qe({}),Or=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1145",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[A0]},$$scope:{ctx:W}}}),Br=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1150",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),go=new ze({props:{$$slots:{default:[N0]},$$scope:{ctx:W}}}),Wr=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Qr=new qe({}),Ur=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1196",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),To=new ze({props:{$$slots:{default:[I0]},$$scope:{ctx:W}}}),Gr=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1203",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Fo=new ze({props:{$$slots:{default:[S0]},$$scope:{ctx:W}}}),Zr=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Xr=new qe({}),Jr=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1263",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[B0]},$$scope:{ctx:W}}}),aa=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1277",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[W0]},$$scope:{ctx:W}}}),ia=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),la=new qe({}),da=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1345",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bo=new ze({props:{$$slots:{default:[Q0]},$$scope:{ctx:W}}}),ma=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1353",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$o=new ze({props:{$$slots:{default:[U0]},$$scope:{ctx:W}}}),ga=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),_a=new qe({}),Ta=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1422",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[R0]},$$scope:{ctx:W}}}),ba=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1439",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),zo=new ze({props:{$$slots:{default:[H0]},$$scope:{ctx:W}}}),$a=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ea=new qe({}),Ma=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1539",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[V0]},$$scope:{ctx:W}}}),xa=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1550",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Co=new ze({props:{$$slots:{default:[Y0]},$$scope:{ctx:W}}}),La=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Oa=new qe({}),Da=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1618",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[K0]},$$scope:{ctx:W}}}),Wa=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16310/src/transformers/models/funnel/modeling_tf_funnel.py#L1628",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16310/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16310/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16310/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Lo=new ze({props:{$$slots:{default:[G0]},$$scope:{ctx:W}}}),Qa=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){u=r("meta"),M=l(),m=r("h1"),g=r("a"),F=r("span"),v(T.$$.fragment),_=l(),z=r("span"),ce=t("Funnel Transformer"),K=l(),q=r("h2"),J=r("a"),A=r("span"),v(ne.$$.fragment),pe=l(),N=r("span"),ue=t("Overview"),ie=l(),Y=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),G=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),j=l(),oe=r("p"),Q=t("The abstract from the paper is the following:"),le=l(),se=r("p"),I=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),U=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),S=r("li"),O=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),R=t("FunnelModel"),ge=t(", "),p=r("a"),k=t("FunnelForPreTraining"),Z=t(`,
`),Te=r("a"),ye=t("FunnelForMaskedLM"),D=t(", "),Fe=r("a"),we=t("FunnelForTokenClassification"),be=t(` and
class:`),x=r("em"),H=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),ke=r("a"),V=t("FunnelBaseModel"),Ee=t(", "),ve=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Ya=r("a"),Iu=t("FunnelForMultipleChoice"),Su=t("."),Ec=l(),xn=r("p"),Bu=t("This model was contributed by "),No=r("a"),Wu=t("sgugger"),Qu=t(". The original code can be found "),Io=r("a"),Uu=t("here"),Ru=t("."),Mc=l(),Zn=r("h2"),Bt=r("a"),pl=r("span"),v(So.$$.fragment),Hu=l(),ul=r("span"),Vu=t("FunnelConfig"),zc=l(),Cn=r("div"),v(Bo.$$.fragment),Yu=l(),jn=r("p"),Ku=t("This is the configuration class to store the configuration of a "),Ka=r("a"),Gu=t("FunnelModel"),Zu=t(" or a "),Ga=r("a"),Xu=t("TFBertModel"),Ju=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),eh=t("funnel-transformer/small"),nh=t(" architecture."),th=l(),Xn=r("p"),oh=t("Configuration objects inherit from "),Za=r("a"),sh=t("PretrainedConfig"),rh=t(` and can be used to control the model outputs. Read the
documentation from `),Xa=r("a"),ah=t("PretrainedConfig"),ih=t(" for more information."),qc=l(),Jn=r("h2"),Wt=r("a"),hl=r("span"),v(Qo.$$.fragment),lh=l(),fl=r("span"),dh=t("FunnelTokenizer"),Pc=l(),Pe=r("div"),v(Uo.$$.fragment),ch=l(),ml=r("p"),ph=t("Construct a Funnel Transformer tokenizer."),uh=l(),Qt=r("p"),Ja=r("a"),hh=t("FunnelTokenizer"),fh=t(" is identical to "),ei=r("a"),mh=t("BertTokenizer"),gh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),_h=l(),Ro=r("p"),Th=t("Refer to superclass "),ni=r("a"),Fh=t("BertTokenizer"),kh=t(" for usage examples and documentation concerning parameters."),vh=l(),Ln=r("div"),v(Ho.$$.fragment),yh=l(),gl=r("p"),wh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),bh=l(),Vo=r("ul"),ti=r("li"),$h=t("single sequence: "),_l=r("code"),Eh=t("[CLS] X [SEP]"),Mh=l(),oi=r("li"),zh=t("pair of sequences: "),Tl=r("code"),qh=t("[CLS] A [SEP] B [SEP]"),Ph=l(),Ut=r("div"),v(Yo.$$.fragment),Ch=l(),Ko=r("p"),jh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Fl=r("code"),xh=t("prepare_for_model"),Lh=t(" method."),Oh=l(),yn=r("div"),v(Go.$$.fragment),Dh=l(),kl=r("p"),Ah=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Nh=l(),v(Zo.$$.fragment),Ih=l(),et=r("p"),Sh=t("If "),vl=r("code"),Bh=t("token_ids_1"),Wh=t(" is "),yl=r("code"),Qh=t("None"),Uh=t(", this method only returns the first portion of the mask (0s)."),Rh=l(),si=r("div"),v(Xo.$$.fragment),Cc=l(),nt=r("h2"),Rt=r("a"),wl=r("span"),v(Jo.$$.fragment),Hh=l(),bl=r("span"),Vh=t("FunnelTokenizerFast"),jc=l(),Ge=r("div"),v(es.$$.fragment),Yh=l(),ns=r("p"),Kh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=r("em"),Gh=t("tokenizers"),Zh=t(" library)."),Xh=l(),Ht=r("p"),ri=r("a"),Jh=t("FunnelTokenizerFast"),ef=t(" is identical to "),ai=r("a"),nf=t("BertTokenizerFast"),tf=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),of=l(),ts=r("p"),sf=t("Refer to superclass "),ii=r("a"),rf=t("BertTokenizerFast"),af=t(" for usage examples and documentation concerning parameters."),lf=l(),wn=r("div"),v(os.$$.fragment),df=l(),El=r("p"),cf=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),pf=l(),v(ss.$$.fragment),uf=l(),tt=r("p"),hf=t("If "),Ml=r("code"),ff=t("token_ids_1"),mf=t(" is "),zl=r("code"),gf=t("None"),_f=t(", this method only returns the first portion of the mask (0s)."),xc=l(),ot=r("h2"),Vt=r("a"),ql=r("span"),v(rs.$$.fragment),Tf=l(),Pl=r("span"),Ff=t("Funnel specific outputs"),Lc=l(),st=r("div"),v(as.$$.fragment),kf=l(),is=r("p"),vf=t("Output type of "),li=r("a"),yf=t("FunnelForPreTraining"),wf=t("."),Oc=l(),rt=r("div"),v(ls.$$.fragment),bf=l(),ds=r("p"),$f=t("Output type of "),di=r("a"),Ef=t("FunnelForPreTraining"),Mf=t("."),Dc=l(),at=r("h2"),Yt=r("a"),Cl=r("span"),v(cs.$$.fragment),zf=l(),jl=r("span"),qf=t("FunnelBaseModel"),Ac=l(),We=r("div"),v(ps.$$.fragment),Pf=l(),xl=r("p"),Cf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),jf=l(),us=r("p"),xf=t("The Funnel Transformer model was proposed in "),hs=r("a"),Lf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Of=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Df=l(),fs=r("p"),Af=t("This model inherits from "),ci=r("a"),Nf=t("PreTrainedModel"),If=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sf=l(),ms=r("p"),Bf=t("This model is also a PyTorch "),gs=r("a"),Wf=t("torch.nn.Module"),Qf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Uf=l(),Ze=r("div"),v(_s.$$.fragment),Rf=l(),it=r("p"),Hf=t("The "),pi=r("a"),Vf=t("FunnelBaseModel"),Yf=t(" forward method, overrides the "),Ll=r("code"),Kf=t("__call__"),Gf=t(" special method."),Zf=l(),v(Kt.$$.fragment),Xf=l(),Ol=r("p"),Jf=t("Example:"),em=l(),v(Ts.$$.fragment),Nc=l(),lt=r("h2"),Gt=r("a"),Dl=r("span"),v(Fs.$$.fragment),nm=l(),Al=r("span"),tm=t("FunnelModel"),Ic=l(),Qe=r("div"),v(ks.$$.fragment),om=l(),Nl=r("p"),sm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),rm=l(),vs=r("p"),am=t("The Funnel Transformer model was proposed in "),ys=r("a"),im=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dm=l(),ws=r("p"),cm=t("This model inherits from "),ui=r("a"),pm=t("PreTrainedModel"),um=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hm=l(),bs=r("p"),fm=t("This model is also a PyTorch "),$s=r("a"),mm=t("torch.nn.Module"),gm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_m=l(),Xe=r("div"),v(Es.$$.fragment),Tm=l(),dt=r("p"),Fm=t("The "),hi=r("a"),km=t("FunnelModel"),vm=t(" forward method, overrides the "),Il=r("code"),ym=t("__call__"),wm=t(" special method."),bm=l(),v(Zt.$$.fragment),$m=l(),Sl=r("p"),Em=t("Example:"),Mm=l(),v(Ms.$$.fragment),Sc=l(),ct=r("h2"),Xt=r("a"),Bl=r("span"),v(zs.$$.fragment),zm=l(),Wl=r("span"),qm=t("FunnelModelForPreTraining"),Bc=l(),pt=r("div"),v(qs.$$.fragment),Pm=l(),Je=r("div"),v(Ps.$$.fragment),Cm=l(),ut=r("p"),jm=t("The "),fi=r("a"),xm=t("FunnelForPreTraining"),Lm=t(" forward method, overrides the "),Ql=r("code"),Om=t("__call__"),Dm=t(" special method."),Am=l(),v(Jt.$$.fragment),Nm=l(),Ul=r("p"),Im=t("Examples:"),Sm=l(),v(Cs.$$.fragment),Wc=l(),ht=r("h2"),eo=r("a"),Rl=r("span"),v(js.$$.fragment),Bm=l(),Hl=r("span"),Wm=t("FunnelForMaskedLM"),Qc=l(),Ue=r("div"),v(xs.$$.fragment),Qm=l(),Ls=r("p"),Um=t("Funnel Transformer Model with a "),Vl=r("code"),Rm=t("language modeling"),Hm=t(" head on top."),Vm=l(),Os=r("p"),Ym=t("The Funnel Transformer model was proposed in "),Ds=r("a"),Km=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zm=l(),As=r("p"),Xm=t("This model inherits from "),mi=r("a"),Jm=t("PreTrainedModel"),eg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ng=l(),Ns=r("p"),tg=t("This model is also a PyTorch "),Is=r("a"),og=t("torch.nn.Module"),sg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rg=l(),en=r("div"),v(Ss.$$.fragment),ag=l(),ft=r("p"),ig=t("The "),gi=r("a"),lg=t("FunnelForMaskedLM"),dg=t(" forward method, overrides the "),Yl=r("code"),cg=t("__call__"),pg=t(" special method."),ug=l(),v(no.$$.fragment),hg=l(),Kl=r("p"),fg=t("Example:"),mg=l(),v(Bs.$$.fragment),Uc=l(),mt=r("h2"),to=r("a"),Gl=r("span"),v(Ws.$$.fragment),gg=l(),Zl=r("span"),_g=t("FunnelForSequenceClassification"),Rc=l(),Re=r("div"),v(Qs.$$.fragment),Tg=l(),Xl=r("p"),Fg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),kg=l(),Us=r("p"),vg=t("The Funnel Transformer model was proposed in "),Rs=r("a"),yg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),wg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),bg=l(),Hs=r("p"),$g=t("This model inherits from "),_i=r("a"),Eg=t("PreTrainedModel"),Mg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zg=l(),Vs=r("p"),qg=t("This model is also a PyTorch "),Ys=r("a"),Pg=t("torch.nn.Module"),Cg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jg=l(),Be=r("div"),v(Ks.$$.fragment),xg=l(),gt=r("p"),Lg=t("The "),Ti=r("a"),Og=t("FunnelForSequenceClassification"),Dg=t(" forward method, overrides the "),Jl=r("code"),Ag=t("__call__"),Ng=t(" special method."),Ig=l(),v(oo.$$.fragment),Sg=l(),ed=r("p"),Bg=t("Example of single-label classification:"),Wg=l(),v(Gs.$$.fragment),Qg=l(),nd=r("p"),Ug=t("Example of multi-label classification:"),Rg=l(),v(Zs.$$.fragment),Hc=l(),_t=r("h2"),so=r("a"),td=r("span"),v(Xs.$$.fragment),Hg=l(),od=r("span"),Vg=t("FunnelForMultipleChoice"),Vc=l(),He=r("div"),v(Js.$$.fragment),Yg=l(),sd=r("p"),Kg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Gg=l(),er=r("p"),Zg=t("The Funnel Transformer model was proposed in "),nr=r("a"),Xg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Jg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),e_=l(),tr=r("p"),n_=t("This model inherits from "),Fi=r("a"),t_=t("PreTrainedModel"),o_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s_=l(),or=r("p"),r_=t("This model is also a PyTorch "),sr=r("a"),a_=t("torch.nn.Module"),i_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),l_=l(),nn=r("div"),v(rr.$$.fragment),d_=l(),Tt=r("p"),c_=t("The "),ki=r("a"),p_=t("FunnelForMultipleChoice"),u_=t(" forward method, overrides the "),rd=r("code"),h_=t("__call__"),f_=t(" special method."),m_=l(),v(ro.$$.fragment),g_=l(),ad=r("p"),__=t("Example:"),T_=l(),v(ar.$$.fragment),Yc=l(),Ft=r("h2"),ao=r("a"),id=r("span"),v(ir.$$.fragment),F_=l(),ld=r("span"),k_=t("FunnelForTokenClassification"),Kc=l(),Ve=r("div"),v(lr.$$.fragment),v_=l(),dd=r("p"),y_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),w_=l(),dr=r("p"),b_=t("The Funnel Transformer model was proposed in "),cr=r("a"),$_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),E_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),M_=l(),pr=r("p"),z_=t("This model inherits from "),vi=r("a"),q_=t("PreTrainedModel"),P_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C_=l(),ur=r("p"),j_=t("This model is also a PyTorch "),hr=r("a"),x_=t("torch.nn.Module"),L_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),O_=l(),tn=r("div"),v(fr.$$.fragment),D_=l(),kt=r("p"),A_=t("The "),yi=r("a"),N_=t("FunnelForTokenClassification"),I_=t(" forward method, overrides the "),cd=r("code"),S_=t("__call__"),B_=t(" special method."),W_=l(),v(io.$$.fragment),Q_=l(),pd=r("p"),U_=t("Example:"),R_=l(),v(mr.$$.fragment),Gc=l(),vt=r("h2"),lo=r("a"),ud=r("span"),v(gr.$$.fragment),H_=l(),hd=r("span"),V_=t("FunnelForQuestionAnswering"),Zc=l(),Ye=r("div"),v(_r.$$.fragment),Y_=l(),yt=r("p"),K_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=r("code"),G_=t("span start logits"),Z_=t(" and "),md=r("code"),X_=t("span end logits"),J_=t(")."),eT=l(),Tr=r("p"),nT=t("The Funnel Transformer model was proposed in "),Fr=r("a"),tT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),oT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sT=l(),kr=r("p"),rT=t("This model inherits from "),wi=r("a"),aT=t("PreTrainedModel"),iT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lT=l(),vr=r("p"),dT=t("This model is also a PyTorch "),yr=r("a"),cT=t("torch.nn.Module"),pT=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),uT=l(),on=r("div"),v(wr.$$.fragment),hT=l(),wt=r("p"),fT=t("The "),bi=r("a"),mT=t("FunnelForQuestionAnswering"),gT=t(" forward method, overrides the "),gd=r("code"),_T=t("__call__"),TT=t(" special method."),FT=l(),v(co.$$.fragment),kT=l(),_d=r("p"),vT=t("Example:"),yT=l(),v(br.$$.fragment),Xc=l(),bt=r("h2"),po=r("a"),Td=r("span"),v($r.$$.fragment),wT=l(),Fd=r("span"),bT=t("TFFunnelBaseModel"),Jc=l(),je=r("div"),v(Er.$$.fragment),$T=l(),kd=r("p"),ET=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),MT=l(),Mr=r("p"),zT=t("The Funnel Transformer model was proposed in "),zr=r("a"),qT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),PT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),CT=l(),qr=r("p"),jT=t("This model inherits from "),$i=r("a"),xT=t("TFPreTrainedModel"),LT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),OT=l(),Pr=r("p"),DT=t("This model is also a "),Cr=r("a"),AT=t("tf.keras.Model"),NT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),IT=l(),v(uo.$$.fragment),ST=l(),sn=r("div"),v(jr.$$.fragment),BT=l(),$t=r("p"),WT=t("The "),Ei=r("a"),QT=t("TFFunnelBaseModel"),UT=t(" forward method, overrides the "),vd=r("code"),RT=t("__call__"),HT=t(" special method."),VT=l(),v(ho.$$.fragment),YT=l(),yd=r("p"),KT=t("Example:"),GT=l(),v(xr.$$.fragment),ep=l(),Et=r("h2"),fo=r("a"),wd=r("span"),v(Lr.$$.fragment),ZT=l(),bd=r("span"),XT=t("TFFunnelModel"),np=l(),xe=r("div"),v(Or.$$.fragment),JT=l(),$d=r("p"),e1=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),n1=l(),Dr=r("p"),t1=t("The Funnel Transformer model was proposed in "),Ar=r("a"),o1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),s1=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),r1=l(),Nr=r("p"),a1=t("This model inherits from "),Mi=r("a"),i1=t("TFPreTrainedModel"),l1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),d1=l(),Ir=r("p"),c1=t("This model is also a "),Sr=r("a"),p1=t("tf.keras.Model"),u1=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),h1=l(),v(mo.$$.fragment),f1=l(),rn=r("div"),v(Br.$$.fragment),m1=l(),Mt=r("p"),g1=t("The "),zi=r("a"),_1=t("TFFunnelModel"),T1=t(" forward method, overrides the "),Ed=r("code"),F1=t("__call__"),k1=t(" special method."),v1=l(),v(go.$$.fragment),y1=l(),Md=r("p"),w1=t("Example:"),b1=l(),v(Wr.$$.fragment),tp=l(),zt=r("h2"),_o=r("a"),zd=r("span"),v(Qr.$$.fragment),$1=l(),qd=r("span"),E1=t("TFFunnelModelForPreTraining"),op=l(),Le=r("div"),v(Ur.$$.fragment),M1=l(),Pd=r("p"),z1=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),q1=l(),Rr=r("p"),P1=t("The Funnel Transformer model was proposed in "),Hr=r("a"),C1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),j1=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),x1=l(),Vr=r("p"),L1=t("This model inherits from "),qi=r("a"),O1=t("TFPreTrainedModel"),D1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),A1=l(),Yr=r("p"),N1=t("This model is also a "),Kr=r("a"),I1=t("tf.keras.Model"),S1=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),B1=l(),v(To.$$.fragment),W1=l(),an=r("div"),v(Gr.$$.fragment),Q1=l(),qt=r("p"),U1=t("The "),Pi=r("a"),R1=t("TFFunnelForPreTraining"),H1=t(" forward method, overrides the "),Cd=r("code"),V1=t("__call__"),Y1=t(" special method."),K1=l(),v(Fo.$$.fragment),G1=l(),jd=r("p"),Z1=t("Examples:"),X1=l(),v(Zr.$$.fragment),sp=l(),Pt=r("h2"),ko=r("a"),xd=r("span"),v(Xr.$$.fragment),J1=l(),Ld=r("span"),eF=t("TFFunnelForMaskedLM"),rp=l(),Oe=r("div"),v(Jr.$$.fragment),nF=l(),ea=r("p"),tF=t("Funnel Model with a "),Od=r("code"),oF=t("language modeling"),sF=t(" head on top."),rF=l(),na=r("p"),aF=t("The Funnel Transformer model was proposed in "),ta=r("a"),iF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dF=l(),oa=r("p"),cF=t("This model inherits from "),Ci=r("a"),pF=t("TFPreTrainedModel"),uF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hF=l(),sa=r("p"),fF=t("This model is also a "),ra=r("a"),mF=t("tf.keras.Model"),gF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_F=l(),v(vo.$$.fragment),TF=l(),ln=r("div"),v(aa.$$.fragment),FF=l(),Ct=r("p"),kF=t("The "),ji=r("a"),vF=t("TFFunnelForMaskedLM"),yF=t(" forward method, overrides the "),Dd=r("code"),wF=t("__call__"),bF=t(" special method."),$F=l(),v(yo.$$.fragment),EF=l(),Ad=r("p"),MF=t("Example:"),zF=l(),v(ia.$$.fragment),ap=l(),jt=r("h2"),wo=r("a"),Nd=r("span"),v(la.$$.fragment),qF=l(),Id=r("span"),PF=t("TFFunnelForSequenceClassification"),ip=l(),De=r("div"),v(da.$$.fragment),CF=l(),Sd=r("p"),jF=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),xF=l(),ca=r("p"),LF=t("The Funnel Transformer model was proposed in "),pa=r("a"),OF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),DF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),AF=l(),ua=r("p"),NF=t("This model inherits from "),xi=r("a"),IF=t("TFPreTrainedModel"),SF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),BF=l(),ha=r("p"),WF=t("This model is also a "),fa=r("a"),QF=t("tf.keras.Model"),UF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),RF=l(),v(bo.$$.fragment),HF=l(),dn=r("div"),v(ma.$$.fragment),VF=l(),xt=r("p"),YF=t("The "),Li=r("a"),KF=t("TFFunnelForSequenceClassification"),GF=t(" forward method, overrides the "),Bd=r("code"),ZF=t("__call__"),XF=t(" special method."),JF=l(),v($o.$$.fragment),ek=l(),Wd=r("p"),nk=t("Example:"),tk=l(),v(ga.$$.fragment),lp=l(),Lt=r("h2"),Eo=r("a"),Qd=r("span"),v(_a.$$.fragment),ok=l(),Ud=r("span"),sk=t("TFFunnelForMultipleChoice"),dp=l(),Ae=r("div"),v(Ta.$$.fragment),rk=l(),Rd=r("p"),ak=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ik=l(),Fa=r("p"),lk=t("The Funnel Transformer model was proposed in "),ka=r("a"),dk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ck=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pk=l(),va=r("p"),uk=t("This model inherits from "),Oi=r("a"),hk=t("TFPreTrainedModel"),fk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mk=l(),ya=r("p"),gk=t("This model is also a "),wa=r("a"),_k=t("tf.keras.Model"),Tk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Fk=l(),v(Mo.$$.fragment),kk=l(),cn=r("div"),v(ba.$$.fragment),vk=l(),Ot=r("p"),yk=t("The "),Di=r("a"),wk=t("TFFunnelForMultipleChoice"),bk=t(" forward method, overrides the "),Hd=r("code"),$k=t("__call__"),Ek=t(" special method."),Mk=l(),v(zo.$$.fragment),zk=l(),Vd=r("p"),qk=t("Example:"),Pk=l(),v($a.$$.fragment),cp=l(),Dt=r("h2"),qo=r("a"),Yd=r("span"),v(Ea.$$.fragment),Ck=l(),Kd=r("span"),jk=t("TFFunnelForTokenClassification"),pp=l(),Ne=r("div"),v(Ma.$$.fragment),xk=l(),Gd=r("p"),Lk=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ok=l(),za=r("p"),Dk=t("The Funnel Transformer model was proposed in "),qa=r("a"),Ak=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Nk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ik=l(),Pa=r("p"),Sk=t("This model inherits from "),Ai=r("a"),Bk=t("TFPreTrainedModel"),Wk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qk=l(),Ca=r("p"),Uk=t("This model is also a "),ja=r("a"),Rk=t("tf.keras.Model"),Hk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Vk=l(),v(Po.$$.fragment),Yk=l(),pn=r("div"),v(xa.$$.fragment),Kk=l(),At=r("p"),Gk=t("The "),Ni=r("a"),Zk=t("TFFunnelForTokenClassification"),Xk=t(" forward method, overrides the "),Zd=r("code"),Jk=t("__call__"),ev=t(" special method."),nv=l(),v(Co.$$.fragment),tv=l(),Xd=r("p"),ov=t("Example:"),sv=l(),v(La.$$.fragment),up=l(),Nt=r("h2"),jo=r("a"),Jd=r("span"),v(Oa.$$.fragment),rv=l(),ec=r("span"),av=t("TFFunnelForQuestionAnswering"),hp=l(),Ie=r("div"),v(Da.$$.fragment),iv=l(),It=r("p"),lv=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=r("code"),dv=t("span start logits"),cv=t(" and "),tc=r("code"),pv=t("span end logits"),uv=t(")."),hv=l(),Aa=r("p"),fv=t("The Funnel Transformer model was proposed in "),Na=r("a"),mv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),gv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_v=l(),Ia=r("p"),Tv=t("This model inherits from "),Ii=r("a"),Fv=t("TFPreTrainedModel"),kv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vv=l(),Sa=r("p"),yv=t("This model is also a "),Ba=r("a"),wv=t("tf.keras.Model"),bv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),$v=l(),v(xo.$$.fragment),Ev=l(),un=r("div"),v(Wa.$$.fragment),Mv=l(),St=r("p"),zv=t("The "),Si=r("a"),qv=t("TFFunnelForQuestionAnswering"),Pv=t(" forward method, overrides the "),oc=r("code"),Cv=t("__call__"),jv=t(" special method."),xv=l(),v(Lo.$$.fragment),Lv=l(),sc=r("p"),Ov=t("Example:"),Dv=l(),v(Qa.$$.fragment),this.h()},l(s){const f=E0('[data-svelte="svelte-1phssyn"]',document.head);u=a(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(s),m=a(s,"H1",{class:!0});var Ua=i(m);g=a(Ua,"A",{id:!0,class:!0,href:!0});var rc=i(g);F=a(rc,"SPAN",{});var ac=i(F);y(T.$$.fragment,ac),ac.forEach(n),rc.forEach(n),_=d(Ua),z=a(Ua,"SPAN",{});var ic=i(z);ce=o(ic,"Funnel Transformer"),ic.forEach(n),Ua.forEach(n),K=d(s),q=a(s,"H2",{class:!0});var Ra=i(q);J=a(Ra,"A",{id:!0,class:!0,href:!0});var lc=i(J);A=a(lc,"SPAN",{});var dc=i(A);y(ne.$$.fragment,dc),dc.forEach(n),lc.forEach(n),pe=d(Ra),N=a(Ra,"SPAN",{});var cc=i(N);ue=o(cc,"Overview"),cc.forEach(n),Ra.forEach(n),ie=d(s),Y=a(s,"P",{});var Ha=i(Y);L=o(Ha,"The Funnel Transformer model was proposed in the paper "),te=a(Ha,"A",{href:!0,rel:!0});var pc=i(te);G=o(pc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),pc.forEach(n),P=o(Ha,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Ha.forEach(n),j=d(s),oe=a(s,"P",{});var uc=i(oe);Q=o(uc,"The abstract from the paper is the following:"),uc.forEach(n),le=d(s),se=a(s,"P",{});var hc=i(se);I=a(hc,"EM",{});var fc=i(I);he=o(fc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),fc.forEach(n),hc.forEach(n),de=d(s),C=a(s,"P",{});var mc=i(C);fe=o(mc,"Tips:"),mc.forEach(n),B=d(s),ee=a(s,"UL",{});var Va=i(ee);ae=a(Va,"LI",{});var gc=i(ae);U=o(gc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),gc.forEach(n),me=d(Va),S=a(Va,"LI",{});var Ce=i(S);O=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(Ce,"A",{href:!0});var _c=i(re);R=o(_c,"FunnelModel"),_c.forEach(n),ge=o(Ce,", "),p=a(Ce,"A",{href:!0});var Tc=i(p);k=o(Tc,"FunnelForPreTraining"),Tc.forEach(n),Z=o(Ce,`,
`),Te=a(Ce,"A",{href:!0});var Fc=i(Te);ye=o(Fc,"FunnelForMaskedLM"),Fc.forEach(n),D=o(Ce,", "),Fe=a(Ce,"A",{href:!0});var kc=i(Fe);we=o(kc,"FunnelForTokenClassification"),kc.forEach(n),be=o(Ce,` and
class:`),x=a(Ce,"EM",{});var vc=i(x);H=o(vc,"~transformers.FunnelForQuestionAnswering"),vc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),ke=a(Ce,"A",{href:!0});var yc=i(ke);V=o(yc,"FunnelBaseModel"),yc.forEach(n),Ee=o(Ce,", "),ve=a(Ce,"A",{href:!0});var wc=i(ve);_e=o(wc,"FunnelForSequenceClassification"),wc.forEach(n),Me=o(Ce,` and
`),Ya=a(Ce,"A",{href:!0});var Iv=i(Ya);Iu=o(Iv,"FunnelForMultipleChoice"),Iv.forEach(n),Su=o(Ce,"."),Ce.forEach(n),Va.forEach(n),Ec=d(s),xn=a(s,"P",{});var Bi=i(xn);Bu=o(Bi,"This model was contributed by "),No=a(Bi,"A",{href:!0,rel:!0});var Sv=i(No);Wu=o(Sv,"sgugger"),Sv.forEach(n),Qu=o(Bi,". The original code can be found "),Io=a(Bi,"A",{href:!0,rel:!0});var Bv=i(Io);Uu=o(Bv,"here"),Bv.forEach(n),Ru=o(Bi,"."),Bi.forEach(n),Mc=d(s),Zn=a(s,"H2",{class:!0});var mp=i(Zn);Bt=a(mp,"A",{id:!0,class:!0,href:!0});var Wv=i(Bt);pl=a(Wv,"SPAN",{});var Qv=i(pl);y(So.$$.fragment,Qv),Qv.forEach(n),Wv.forEach(n),Hu=d(mp),ul=a(mp,"SPAN",{});var Uv=i(ul);Vu=o(Uv,"FunnelConfig"),Uv.forEach(n),mp.forEach(n),zc=d(s),Cn=a(s,"DIV",{class:!0});var Wi=i(Cn);y(Bo.$$.fragment,Wi),Yu=d(Wi),jn=a(Wi,"P",{});var Oo=i(jn);Ku=o(Oo,"This is the configuration class to store the configuration of a "),Ka=a(Oo,"A",{href:!0});var Rv=i(Ka);Gu=o(Rv,"FunnelModel"),Rv.forEach(n),Zu=o(Oo," or a "),Ga=a(Oo,"A",{href:!0});var Hv=i(Ga);Xu=o(Hv,"TFBertModel"),Hv.forEach(n),Ju=o(Oo,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Oo,"A",{href:!0,rel:!0});var Vv=i(Wo);eh=o(Vv,"funnel-transformer/small"),Vv.forEach(n),nh=o(Oo," architecture."),Oo.forEach(n),th=d(Wi),Xn=a(Wi,"P",{});var Qi=i(Xn);oh=o(Qi,"Configuration objects inherit from "),Za=a(Qi,"A",{href:!0});var Yv=i(Za);sh=o(Yv,"PretrainedConfig"),Yv.forEach(n),rh=o(Qi,` and can be used to control the model outputs. Read the
documentation from `),Xa=a(Qi,"A",{href:!0});var Kv=i(Xa);ah=o(Kv,"PretrainedConfig"),Kv.forEach(n),ih=o(Qi," for more information."),Qi.forEach(n),Wi.forEach(n),qc=d(s),Jn=a(s,"H2",{class:!0});var gp=i(Jn);Wt=a(gp,"A",{id:!0,class:!0,href:!0});var Gv=i(Wt);hl=a(Gv,"SPAN",{});var Zv=i(hl);y(Qo.$$.fragment,Zv),Zv.forEach(n),Gv.forEach(n),lh=d(gp),fl=a(gp,"SPAN",{});var Xv=i(fl);dh=o(Xv,"FunnelTokenizer"),Xv.forEach(n),gp.forEach(n),Pc=d(s),Pe=a(s,"DIV",{class:!0});var Ke=i(Pe);y(Uo.$$.fragment,Ke),ch=d(Ke),ml=a(Ke,"P",{});var Jv=i(ml);ph=o(Jv,"Construct a Funnel Transformer tokenizer."),Jv.forEach(n),uh=d(Ke),Qt=a(Ke,"P",{});var bc=i(Qt);Ja=a(bc,"A",{href:!0});var ey=i(Ja);hh=o(ey,"FunnelTokenizer"),ey.forEach(n),fh=o(bc," is identical to "),ei=a(bc,"A",{href:!0});var ny=i(ei);mh=o(ny,"BertTokenizer"),ny.forEach(n),gh=o(bc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),bc.forEach(n),_h=d(Ke),Ro=a(Ke,"P",{});var _p=i(Ro);Th=o(_p,"Refer to superclass "),ni=a(_p,"A",{href:!0});var ty=i(ni);Fh=o(ty,"BertTokenizer"),ty.forEach(n),kh=o(_p," for usage examples and documentation concerning parameters."),_p.forEach(n),vh=d(Ke),Ln=a(Ke,"DIV",{class:!0});var Ui=i(Ln);y(Ho.$$.fragment,Ui),yh=d(Ui),gl=a(Ui,"P",{});var oy=i(gl);wh=o(oy,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),oy.forEach(n),bh=d(Ui),Vo=a(Ui,"UL",{});var Tp=i(Vo);ti=a(Tp,"LI",{});var Av=i(ti);$h=o(Av,"single sequence: "),_l=a(Av,"CODE",{});var sy=i(_l);Eh=o(sy,"[CLS] X [SEP]"),sy.forEach(n),Av.forEach(n),Mh=d(Tp),oi=a(Tp,"LI",{});var Nv=i(oi);zh=o(Nv,"pair of sequences: "),Tl=a(Nv,"CODE",{});var ry=i(Tl);qh=o(ry,"[CLS] A [SEP] B [SEP]"),ry.forEach(n),Nv.forEach(n),Tp.forEach(n),Ui.forEach(n),Ph=d(Ke),Ut=a(Ke,"DIV",{class:!0});var Fp=i(Ut);y(Yo.$$.fragment,Fp),Ch=d(Fp),Ko=a(Fp,"P",{});var kp=i(Ko);jh=o(kp,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Fl=a(kp,"CODE",{});var ay=i(Fl);xh=o(ay,"prepare_for_model"),ay.forEach(n),Lh=o(kp," method."),kp.forEach(n),Fp.forEach(n),Oh=d(Ke),yn=a(Ke,"DIV",{class:!0});var Do=i(yn);y(Go.$$.fragment,Do),Dh=d(Do),kl=a(Do,"P",{});var iy=i(kl);Ah=o(iy,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),iy.forEach(n),Nh=d(Do),y(Zo.$$.fragment,Do),Ih=d(Do),et=a(Do,"P",{});var Ri=i(et);Sh=o(Ri,"If "),vl=a(Ri,"CODE",{});var ly=i(vl);Bh=o(ly,"token_ids_1"),ly.forEach(n),Wh=o(Ri," is "),yl=a(Ri,"CODE",{});var dy=i(yl);Qh=o(dy,"None"),dy.forEach(n),Uh=o(Ri,", this method only returns the first portion of the mask (0s)."),Ri.forEach(n),Do.forEach(n),Rh=d(Ke),si=a(Ke,"DIV",{class:!0});var cy=i(si);y(Xo.$$.fragment,cy),cy.forEach(n),Ke.forEach(n),Cc=d(s),nt=a(s,"H2",{class:!0});var vp=i(nt);Rt=a(vp,"A",{id:!0,class:!0,href:!0});var py=i(Rt);wl=a(py,"SPAN",{});var uy=i(wl);y(Jo.$$.fragment,uy),uy.forEach(n),py.forEach(n),Hh=d(vp),bl=a(vp,"SPAN",{});var hy=i(bl);Vh=o(hy,"FunnelTokenizerFast"),hy.forEach(n),vp.forEach(n),jc=d(s),Ge=a(s,"DIV",{class:!0});var On=i(Ge);y(es.$$.fragment,On),Yh=d(On),ns=a(On,"P",{});var yp=i(ns);Kh=o(yp,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=a(yp,"EM",{});var fy=i($l);Gh=o(fy,"tokenizers"),fy.forEach(n),Zh=o(yp," library)."),yp.forEach(n),Xh=d(On),Ht=a(On,"P",{});var $c=i(Ht);ri=a($c,"A",{href:!0});var my=i(ri);Jh=o(my,"FunnelTokenizerFast"),my.forEach(n),ef=o($c," is identical to "),ai=a($c,"A",{href:!0});var gy=i(ai);nf=o(gy,"BertTokenizerFast"),gy.forEach(n),tf=o($c,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$c.forEach(n),of=d(On),ts=a(On,"P",{});var wp=i(ts);sf=o(wp,"Refer to superclass "),ii=a(wp,"A",{href:!0});var _y=i(ii);rf=o(_y,"BertTokenizerFast"),_y.forEach(n),af=o(wp," for usage examples and documentation concerning parameters."),wp.forEach(n),lf=d(On),wn=a(On,"DIV",{class:!0});var Ao=i(wn);y(os.$$.fragment,Ao),df=d(Ao),El=a(Ao,"P",{});var Ty=i(El);cf=o(Ty,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Ty.forEach(n),pf=d(Ao),y(ss.$$.fragment,Ao),uf=d(Ao),tt=a(Ao,"P",{});var Hi=i(tt);hf=o(Hi,"If "),Ml=a(Hi,"CODE",{});var Fy=i(Ml);ff=o(Fy,"token_ids_1"),Fy.forEach(n),mf=o(Hi," is "),zl=a(Hi,"CODE",{});var ky=i(zl);gf=o(ky,"None"),ky.forEach(n),_f=o(Hi,", this method only returns the first portion of the mask (0s)."),Hi.forEach(n),Ao.forEach(n),On.forEach(n),xc=d(s),ot=a(s,"H2",{class:!0});var bp=i(ot);Vt=a(bp,"A",{id:!0,class:!0,href:!0});var vy=i(Vt);ql=a(vy,"SPAN",{});var yy=i(ql);y(rs.$$.fragment,yy),yy.forEach(n),vy.forEach(n),Tf=d(bp),Pl=a(bp,"SPAN",{});var wy=i(Pl);Ff=o(wy,"Funnel specific outputs"),wy.forEach(n),bp.forEach(n),Lc=d(s),st=a(s,"DIV",{class:!0});var $p=i(st);y(as.$$.fragment,$p),kf=d($p),is=a($p,"P",{});var Ep=i(is);vf=o(Ep,"Output type of "),li=a(Ep,"A",{href:!0});var by=i(li);yf=o(by,"FunnelForPreTraining"),by.forEach(n),wf=o(Ep,"."),Ep.forEach(n),$p.forEach(n),Oc=d(s),rt=a(s,"DIV",{class:!0});var Mp=i(rt);y(ls.$$.fragment,Mp),bf=d(Mp),ds=a(Mp,"P",{});var zp=i(ds);$f=o(zp,"Output type of "),di=a(zp,"A",{href:!0});var $y=i(di);Ef=o($y,"FunnelForPreTraining"),$y.forEach(n),Mf=o(zp,"."),zp.forEach(n),Mp.forEach(n),Dc=d(s),at=a(s,"H2",{class:!0});var qp=i(at);Yt=a(qp,"A",{id:!0,class:!0,href:!0});var Ey=i(Yt);Cl=a(Ey,"SPAN",{});var My=i(Cl);y(cs.$$.fragment,My),My.forEach(n),Ey.forEach(n),zf=d(qp),jl=a(qp,"SPAN",{});var zy=i(jl);qf=o(zy,"FunnelBaseModel"),zy.forEach(n),qp.forEach(n),Ac=d(s),We=a(s,"DIV",{class:!0});var bn=i(We);y(ps.$$.fragment,bn),Pf=d(bn),xl=a(bn,"P",{});var qy=i(xl);Cf=o(qy,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qy.forEach(n),jf=d(bn),us=a(bn,"P",{});var Pp=i(us);xf=o(Pp,"The Funnel Transformer model was proposed in "),hs=a(Pp,"A",{href:!0,rel:!0});var Py=i(hs);Lf=o(Py,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Py.forEach(n),Of=o(Pp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pp.forEach(n),Df=d(bn),fs=a(bn,"P",{});var Cp=i(fs);Af=o(Cp,"This model inherits from "),ci=a(Cp,"A",{href:!0});var Cy=i(ci);Nf=o(Cy,"PreTrainedModel"),Cy.forEach(n),If=o(Cp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cp.forEach(n),Sf=d(bn),ms=a(bn,"P",{});var jp=i(ms);Bf=o(jp,"This model is also a PyTorch "),gs=a(jp,"A",{href:!0,rel:!0});var jy=i(gs);Wf=o(jy,"torch.nn.Module"),jy.forEach(n),Qf=o(jp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jp.forEach(n),Uf=d(bn),Ze=a(bn,"DIV",{class:!0});var Dn=i(Ze);y(_s.$$.fragment,Dn),Rf=d(Dn),it=a(Dn,"P",{});var Vi=i(it);Hf=o(Vi,"The "),pi=a(Vi,"A",{href:!0});var xy=i(pi);Vf=o(xy,"FunnelBaseModel"),xy.forEach(n),Yf=o(Vi," forward method, overrides the "),Ll=a(Vi,"CODE",{});var Ly=i(Ll);Kf=o(Ly,"__call__"),Ly.forEach(n),Gf=o(Vi," special method."),Vi.forEach(n),Zf=d(Dn),y(Kt.$$.fragment,Dn),Xf=d(Dn),Ol=a(Dn,"P",{});var Oy=i(Ol);Jf=o(Oy,"Example:"),Oy.forEach(n),em=d(Dn),y(Ts.$$.fragment,Dn),Dn.forEach(n),bn.forEach(n),Nc=d(s),lt=a(s,"H2",{class:!0});var xp=i(lt);Gt=a(xp,"A",{id:!0,class:!0,href:!0});var Dy=i(Gt);Dl=a(Dy,"SPAN",{});var Ay=i(Dl);y(Fs.$$.fragment,Ay),Ay.forEach(n),Dy.forEach(n),nm=d(xp),Al=a(xp,"SPAN",{});var Ny=i(Al);tm=o(Ny,"FunnelModel"),Ny.forEach(n),xp.forEach(n),Ic=d(s),Qe=a(s,"DIV",{class:!0});var $n=i(Qe);y(ks.$$.fragment,$n),om=d($n),Nl=a($n,"P",{});var Iy=i(Nl);sm=o(Iy,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Iy.forEach(n),rm=d($n),vs=a($n,"P",{});var Lp=i(vs);am=o(Lp,"The Funnel Transformer model was proposed in "),ys=a(Lp,"A",{href:!0,rel:!0});var Sy=i(ys);im=o(Sy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sy.forEach(n),lm=o(Lp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Lp.forEach(n),dm=d($n),ws=a($n,"P",{});var Op=i(ws);cm=o(Op,"This model inherits from "),ui=a(Op,"A",{href:!0});var By=i(ui);pm=o(By,"PreTrainedModel"),By.forEach(n),um=o(Op,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Op.forEach(n),hm=d($n),bs=a($n,"P",{});var Dp=i(bs);fm=o(Dp,"This model is also a PyTorch "),$s=a(Dp,"A",{href:!0,rel:!0});var Wy=i($s);mm=o(Wy,"torch.nn.Module"),Wy.forEach(n),gm=o(Dp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Dp.forEach(n),_m=d($n),Xe=a($n,"DIV",{class:!0});var An=i(Xe);y(Es.$$.fragment,An),Tm=d(An),dt=a(An,"P",{});var Yi=i(dt);Fm=o(Yi,"The "),hi=a(Yi,"A",{href:!0});var Qy=i(hi);km=o(Qy,"FunnelModel"),Qy.forEach(n),vm=o(Yi," forward method, overrides the "),Il=a(Yi,"CODE",{});var Uy=i(Il);ym=o(Uy,"__call__"),Uy.forEach(n),wm=o(Yi," special method."),Yi.forEach(n),bm=d(An),y(Zt.$$.fragment,An),$m=d(An),Sl=a(An,"P",{});var Ry=i(Sl);Em=o(Ry,"Example:"),Ry.forEach(n),Mm=d(An),y(Ms.$$.fragment,An),An.forEach(n),$n.forEach(n),Sc=d(s),ct=a(s,"H2",{class:!0});var Ap=i(ct);Xt=a(Ap,"A",{id:!0,class:!0,href:!0});var Hy=i(Xt);Bl=a(Hy,"SPAN",{});var Vy=i(Bl);y(zs.$$.fragment,Vy),Vy.forEach(n),Hy.forEach(n),zm=d(Ap),Wl=a(Ap,"SPAN",{});var Yy=i(Wl);qm=o(Yy,"FunnelModelForPreTraining"),Yy.forEach(n),Ap.forEach(n),Bc=d(s),pt=a(s,"DIV",{class:!0});var Np=i(pt);y(qs.$$.fragment,Np),Pm=d(Np),Je=a(Np,"DIV",{class:!0});var Nn=i(Je);y(Ps.$$.fragment,Nn),Cm=d(Nn),ut=a(Nn,"P",{});var Ki=i(ut);jm=o(Ki,"The "),fi=a(Ki,"A",{href:!0});var Ky=i(fi);xm=o(Ky,"FunnelForPreTraining"),Ky.forEach(n),Lm=o(Ki," forward method, overrides the "),Ql=a(Ki,"CODE",{});var Gy=i(Ql);Om=o(Gy,"__call__"),Gy.forEach(n),Dm=o(Ki," special method."),Ki.forEach(n),Am=d(Nn),y(Jt.$$.fragment,Nn),Nm=d(Nn),Ul=a(Nn,"P",{});var Zy=i(Ul);Im=o(Zy,"Examples:"),Zy.forEach(n),Sm=d(Nn),y(Cs.$$.fragment,Nn),Nn.forEach(n),Np.forEach(n),Wc=d(s),ht=a(s,"H2",{class:!0});var Ip=i(ht);eo=a(Ip,"A",{id:!0,class:!0,href:!0});var Xy=i(eo);Rl=a(Xy,"SPAN",{});var Jy=i(Rl);y(js.$$.fragment,Jy),Jy.forEach(n),Xy.forEach(n),Bm=d(Ip),Hl=a(Ip,"SPAN",{});var ew=i(Hl);Wm=o(ew,"FunnelForMaskedLM"),ew.forEach(n),Ip.forEach(n),Qc=d(s),Ue=a(s,"DIV",{class:!0});var En=i(Ue);y(xs.$$.fragment,En),Qm=d(En),Ls=a(En,"P",{});var Sp=i(Ls);Um=o(Sp,"Funnel Transformer Model with a "),Vl=a(Sp,"CODE",{});var nw=i(Vl);Rm=o(nw,"language modeling"),nw.forEach(n),Hm=o(Sp," head on top."),Sp.forEach(n),Vm=d(En),Os=a(En,"P",{});var Bp=i(Os);Ym=o(Bp,"The Funnel Transformer model was proposed in "),Ds=a(Bp,"A",{href:!0,rel:!0});var tw=i(Ds);Km=o(tw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),tw.forEach(n),Gm=o(Bp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bp.forEach(n),Zm=d(En),As=a(En,"P",{});var Wp=i(As);Xm=o(Wp,"This model inherits from "),mi=a(Wp,"A",{href:!0});var ow=i(mi);Jm=o(ow,"PreTrainedModel"),ow.forEach(n),eg=o(Wp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wp.forEach(n),ng=d(En),Ns=a(En,"P",{});var Qp=i(Ns);tg=o(Qp,"This model is also a PyTorch "),Is=a(Qp,"A",{href:!0,rel:!0});var sw=i(Is);og=o(sw,"torch.nn.Module"),sw.forEach(n),sg=o(Qp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qp.forEach(n),rg=d(En),en=a(En,"DIV",{class:!0});var In=i(en);y(Ss.$$.fragment,In),ag=d(In),ft=a(In,"P",{});var Gi=i(ft);ig=o(Gi,"The "),gi=a(Gi,"A",{href:!0});var rw=i(gi);lg=o(rw,"FunnelForMaskedLM"),rw.forEach(n),dg=o(Gi," forward method, overrides the "),Yl=a(Gi,"CODE",{});var aw=i(Yl);cg=o(aw,"__call__"),aw.forEach(n),pg=o(Gi," special method."),Gi.forEach(n),ug=d(In),y(no.$$.fragment,In),hg=d(In),Kl=a(In,"P",{});var iw=i(Kl);fg=o(iw,"Example:"),iw.forEach(n),mg=d(In),y(Bs.$$.fragment,In),In.forEach(n),En.forEach(n),Uc=d(s),mt=a(s,"H2",{class:!0});var Up=i(mt);to=a(Up,"A",{id:!0,class:!0,href:!0});var lw=i(to);Gl=a(lw,"SPAN",{});var dw=i(Gl);y(Ws.$$.fragment,dw),dw.forEach(n),lw.forEach(n),gg=d(Up),Zl=a(Up,"SPAN",{});var cw=i(Zl);_g=o(cw,"FunnelForSequenceClassification"),cw.forEach(n),Up.forEach(n),Rc=d(s),Re=a(s,"DIV",{class:!0});var Mn=i(Re);y(Qs.$$.fragment,Mn),Tg=d(Mn),Xl=a(Mn,"P",{});var pw=i(Xl);Fg=o(pw,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),pw.forEach(n),kg=d(Mn),Us=a(Mn,"P",{});var Rp=i(Us);vg=o(Rp,"The Funnel Transformer model was proposed in "),Rs=a(Rp,"A",{href:!0,rel:!0});var uw=i(Rs);yg=o(uw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),uw.forEach(n),wg=o(Rp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Rp.forEach(n),bg=d(Mn),Hs=a(Mn,"P",{});var Hp=i(Hs);$g=o(Hp,"This model inherits from "),_i=a(Hp,"A",{href:!0});var hw=i(_i);Eg=o(hw,"PreTrainedModel"),hw.forEach(n),Mg=o(Hp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hp.forEach(n),zg=d(Mn),Vs=a(Mn,"P",{});var Vp=i(Vs);qg=o(Vp,"This model is also a PyTorch "),Ys=a(Vp,"A",{href:!0,rel:!0});var fw=i(Ys);Pg=o(fw,"torch.nn.Module"),fw.forEach(n),Cg=o(Vp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vp.forEach(n),jg=d(Mn),Be=a(Mn,"DIV",{class:!0});var hn=i(Be);y(Ks.$$.fragment,hn),xg=d(hn),gt=a(hn,"P",{});var Zi=i(gt);Lg=o(Zi,"The "),Ti=a(Zi,"A",{href:!0});var mw=i(Ti);Og=o(mw,"FunnelForSequenceClassification"),mw.forEach(n),Dg=o(Zi," forward method, overrides the "),Jl=a(Zi,"CODE",{});var gw=i(Jl);Ag=o(gw,"__call__"),gw.forEach(n),Ng=o(Zi," special method."),Zi.forEach(n),Ig=d(hn),y(oo.$$.fragment,hn),Sg=d(hn),ed=a(hn,"P",{});var _w=i(ed);Bg=o(_w,"Example of single-label classification:"),_w.forEach(n),Wg=d(hn),y(Gs.$$.fragment,hn),Qg=d(hn),nd=a(hn,"P",{});var Tw=i(nd);Ug=o(Tw,"Example of multi-label classification:"),Tw.forEach(n),Rg=d(hn),y(Zs.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Hc=d(s),_t=a(s,"H2",{class:!0});var Yp=i(_t);so=a(Yp,"A",{id:!0,class:!0,href:!0});var Fw=i(so);td=a(Fw,"SPAN",{});var kw=i(td);y(Xs.$$.fragment,kw),kw.forEach(n),Fw.forEach(n),Hg=d(Yp),od=a(Yp,"SPAN",{});var vw=i(od);Vg=o(vw,"FunnelForMultipleChoice"),vw.forEach(n),Yp.forEach(n),Vc=d(s),He=a(s,"DIV",{class:!0});var zn=i(He);y(Js.$$.fragment,zn),Yg=d(zn),sd=a(zn,"P",{});var yw=i(sd);Kg=o(yw,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),yw.forEach(n),Gg=d(zn),er=a(zn,"P",{});var Kp=i(er);Zg=o(Kp,"The Funnel Transformer model was proposed in "),nr=a(Kp,"A",{href:!0,rel:!0});var ww=i(nr);Xg=o(ww,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ww.forEach(n),Jg=o(Kp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Kp.forEach(n),e_=d(zn),tr=a(zn,"P",{});var Gp=i(tr);n_=o(Gp,"This model inherits from "),Fi=a(Gp,"A",{href:!0});var bw=i(Fi);t_=o(bw,"PreTrainedModel"),bw.forEach(n),o_=o(Gp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Gp.forEach(n),s_=d(zn),or=a(zn,"P",{});var Zp=i(or);r_=o(Zp,"This model is also a PyTorch "),sr=a(Zp,"A",{href:!0,rel:!0});var $w=i(sr);a_=o($w,"torch.nn.Module"),$w.forEach(n),i_=o(Zp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zp.forEach(n),l_=d(zn),nn=a(zn,"DIV",{class:!0});var Sn=i(nn);y(rr.$$.fragment,Sn),d_=d(Sn),Tt=a(Sn,"P",{});var Xi=i(Tt);c_=o(Xi,"The "),ki=a(Xi,"A",{href:!0});var Ew=i(ki);p_=o(Ew,"FunnelForMultipleChoice"),Ew.forEach(n),u_=o(Xi," forward method, overrides the "),rd=a(Xi,"CODE",{});var Mw=i(rd);h_=o(Mw,"__call__"),Mw.forEach(n),f_=o(Xi," special method."),Xi.forEach(n),m_=d(Sn),y(ro.$$.fragment,Sn),g_=d(Sn),ad=a(Sn,"P",{});var zw=i(ad);__=o(zw,"Example:"),zw.forEach(n),T_=d(Sn),y(ar.$$.fragment,Sn),Sn.forEach(n),zn.forEach(n),Yc=d(s),Ft=a(s,"H2",{class:!0});var Xp=i(Ft);ao=a(Xp,"A",{id:!0,class:!0,href:!0});var qw=i(ao);id=a(qw,"SPAN",{});var Pw=i(id);y(ir.$$.fragment,Pw),Pw.forEach(n),qw.forEach(n),F_=d(Xp),ld=a(Xp,"SPAN",{});var Cw=i(ld);k_=o(Cw,"FunnelForTokenClassification"),Cw.forEach(n),Xp.forEach(n),Kc=d(s),Ve=a(s,"DIV",{class:!0});var qn=i(Ve);y(lr.$$.fragment,qn),v_=d(qn),dd=a(qn,"P",{});var jw=i(dd);y_=o(jw,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),jw.forEach(n),w_=d(qn),dr=a(qn,"P",{});var Jp=i(dr);b_=o(Jp,"The Funnel Transformer model was proposed in "),cr=a(Jp,"A",{href:!0,rel:!0});var xw=i(cr);$_=o(xw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xw.forEach(n),E_=o(Jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Jp.forEach(n),M_=d(qn),pr=a(qn,"P",{});var eu=i(pr);z_=o(eu,"This model inherits from "),vi=a(eu,"A",{href:!0});var Lw=i(vi);q_=o(Lw,"PreTrainedModel"),Lw.forEach(n),P_=o(eu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eu.forEach(n),C_=d(qn),ur=a(qn,"P",{});var nu=i(ur);j_=o(nu,"This model is also a PyTorch "),hr=a(nu,"A",{href:!0,rel:!0});var Ow=i(hr);x_=o(Ow,"torch.nn.Module"),Ow.forEach(n),L_=o(nu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),nu.forEach(n),O_=d(qn),tn=a(qn,"DIV",{class:!0});var Bn=i(tn);y(fr.$$.fragment,Bn),D_=d(Bn),kt=a(Bn,"P",{});var Ji=i(kt);A_=o(Ji,"The "),yi=a(Ji,"A",{href:!0});var Dw=i(yi);N_=o(Dw,"FunnelForTokenClassification"),Dw.forEach(n),I_=o(Ji," forward method, overrides the "),cd=a(Ji,"CODE",{});var Aw=i(cd);S_=o(Aw,"__call__"),Aw.forEach(n),B_=o(Ji," special method."),Ji.forEach(n),W_=d(Bn),y(io.$$.fragment,Bn),Q_=d(Bn),pd=a(Bn,"P",{});var Nw=i(pd);U_=o(Nw,"Example:"),Nw.forEach(n),R_=d(Bn),y(mr.$$.fragment,Bn),Bn.forEach(n),qn.forEach(n),Gc=d(s),vt=a(s,"H2",{class:!0});var tu=i(vt);lo=a(tu,"A",{id:!0,class:!0,href:!0});var Iw=i(lo);ud=a(Iw,"SPAN",{});var Sw=i(ud);y(gr.$$.fragment,Sw),Sw.forEach(n),Iw.forEach(n),H_=d(tu),hd=a(tu,"SPAN",{});var Bw=i(hd);V_=o(Bw,"FunnelForQuestionAnswering"),Bw.forEach(n),tu.forEach(n),Zc=d(s),Ye=a(s,"DIV",{class:!0});var Pn=i(Ye);y(_r.$$.fragment,Pn),Y_=d(Pn),yt=a(Pn,"P",{});var el=i(yt);K_=o(el,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=a(el,"CODE",{});var Ww=i(fd);G_=o(Ww,"span start logits"),Ww.forEach(n),Z_=o(el," and "),md=a(el,"CODE",{});var Qw=i(md);X_=o(Qw,"span end logits"),Qw.forEach(n),J_=o(el,")."),el.forEach(n),eT=d(Pn),Tr=a(Pn,"P",{});var ou=i(Tr);nT=o(ou,"The Funnel Transformer model was proposed in "),Fr=a(ou,"A",{href:!0,rel:!0});var Uw=i(Fr);tT=o(Uw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Uw.forEach(n),oT=o(ou," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ou.forEach(n),sT=d(Pn),kr=a(Pn,"P",{});var su=i(kr);rT=o(su,"This model inherits from "),wi=a(su,"A",{href:!0});var Rw=i(wi);aT=o(Rw,"PreTrainedModel"),Rw.forEach(n),iT=o(su,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),su.forEach(n),lT=d(Pn),vr=a(Pn,"P",{});var ru=i(vr);dT=o(ru,"This model is also a PyTorch "),yr=a(ru,"A",{href:!0,rel:!0});var Hw=i(yr);cT=o(Hw,"torch.nn.Module"),Hw.forEach(n),pT=o(ru,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ru.forEach(n),uT=d(Pn),on=a(Pn,"DIV",{class:!0});var Wn=i(on);y(wr.$$.fragment,Wn),hT=d(Wn),wt=a(Wn,"P",{});var nl=i(wt);fT=o(nl,"The "),bi=a(nl,"A",{href:!0});var Vw=i(bi);mT=o(Vw,"FunnelForQuestionAnswering"),Vw.forEach(n),gT=o(nl," forward method, overrides the "),gd=a(nl,"CODE",{});var Yw=i(gd);_T=o(Yw,"__call__"),Yw.forEach(n),TT=o(nl," special method."),nl.forEach(n),FT=d(Wn),y(co.$$.fragment,Wn),kT=d(Wn),_d=a(Wn,"P",{});var Kw=i(_d);vT=o(Kw,"Example:"),Kw.forEach(n),yT=d(Wn),y(br.$$.fragment,Wn),Wn.forEach(n),Pn.forEach(n),Xc=d(s),bt=a(s,"H2",{class:!0});var au=i(bt);po=a(au,"A",{id:!0,class:!0,href:!0});var Gw=i(po);Td=a(Gw,"SPAN",{});var Zw=i(Td);y($r.$$.fragment,Zw),Zw.forEach(n),Gw.forEach(n),wT=d(au),Fd=a(au,"SPAN",{});var Xw=i(Fd);bT=o(Xw,"TFFunnelBaseModel"),Xw.forEach(n),au.forEach(n),Jc=d(s),je=a(s,"DIV",{class:!0});var fn=i(je);y(Er.$$.fragment,fn),$T=d(fn),kd=a(fn,"P",{});var Jw=i(kd);ET=o(Jw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Jw.forEach(n),MT=d(fn),Mr=a(fn,"P",{});var iu=i(Mr);zT=o(iu,"The Funnel Transformer model was proposed in "),zr=a(iu,"A",{href:!0,rel:!0});var eb=i(zr);qT=o(eb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),eb.forEach(n),PT=o(iu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),iu.forEach(n),CT=d(fn),qr=a(fn,"P",{});var lu=i(qr);jT=o(lu,"This model inherits from "),$i=a(lu,"A",{href:!0});var nb=i($i);xT=o(nb,"TFPreTrainedModel"),nb.forEach(n),LT=o(lu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lu.forEach(n),OT=d(fn),Pr=a(fn,"P",{});var du=i(Pr);DT=o(du,"This model is also a "),Cr=a(du,"A",{href:!0,rel:!0});var tb=i(Cr);AT=o(tb,"tf.keras.Model"),tb.forEach(n),NT=o(du,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),du.forEach(n),IT=d(fn),y(uo.$$.fragment,fn),ST=d(fn),sn=a(fn,"DIV",{class:!0});var Qn=i(sn);y(jr.$$.fragment,Qn),BT=d(Qn),$t=a(Qn,"P",{});var tl=i($t);WT=o(tl,"The "),Ei=a(tl,"A",{href:!0});var ob=i(Ei);QT=o(ob,"TFFunnelBaseModel"),ob.forEach(n),UT=o(tl," forward method, overrides the "),vd=a(tl,"CODE",{});var sb=i(vd);RT=o(sb,"__call__"),sb.forEach(n),HT=o(tl," special method."),tl.forEach(n),VT=d(Qn),y(ho.$$.fragment,Qn),YT=d(Qn),yd=a(Qn,"P",{});var rb=i(yd);KT=o(rb,"Example:"),rb.forEach(n),GT=d(Qn),y(xr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),ep=d(s),Et=a(s,"H2",{class:!0});var cu=i(Et);fo=a(cu,"A",{id:!0,class:!0,href:!0});var ab=i(fo);wd=a(ab,"SPAN",{});var ib=i(wd);y(Lr.$$.fragment,ib),ib.forEach(n),ab.forEach(n),ZT=d(cu),bd=a(cu,"SPAN",{});var lb=i(bd);XT=o(lb,"TFFunnelModel"),lb.forEach(n),cu.forEach(n),np=d(s),xe=a(s,"DIV",{class:!0});var mn=i(xe);y(Or.$$.fragment,mn),JT=d(mn),$d=a(mn,"P",{});var db=i($d);e1=o(db,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),db.forEach(n),n1=d(mn),Dr=a(mn,"P",{});var pu=i(Dr);t1=o(pu,"The Funnel Transformer model was proposed in "),Ar=a(pu,"A",{href:!0,rel:!0});var cb=i(Ar);o1=o(cb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),cb.forEach(n),s1=o(pu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pu.forEach(n),r1=d(mn),Nr=a(mn,"P",{});var uu=i(Nr);a1=o(uu,"This model inherits from "),Mi=a(uu,"A",{href:!0});var pb=i(Mi);i1=o(pb,"TFPreTrainedModel"),pb.forEach(n),l1=o(uu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),uu.forEach(n),d1=d(mn),Ir=a(mn,"P",{});var hu=i(Ir);c1=o(hu,"This model is also a "),Sr=a(hu,"A",{href:!0,rel:!0});var ub=i(Sr);p1=o(ub,"tf.keras.Model"),ub.forEach(n),u1=o(hu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hu.forEach(n),h1=d(mn),y(mo.$$.fragment,mn),f1=d(mn),rn=a(mn,"DIV",{class:!0});var Un=i(rn);y(Br.$$.fragment,Un),m1=d(Un),Mt=a(Un,"P",{});var ol=i(Mt);g1=o(ol,"The "),zi=a(ol,"A",{href:!0});var hb=i(zi);_1=o(hb,"TFFunnelModel"),hb.forEach(n),T1=o(ol," forward method, overrides the "),Ed=a(ol,"CODE",{});var fb=i(Ed);F1=o(fb,"__call__"),fb.forEach(n),k1=o(ol," special method."),ol.forEach(n),v1=d(Un),y(go.$$.fragment,Un),y1=d(Un),Md=a(Un,"P",{});var mb=i(Md);w1=o(mb,"Example:"),mb.forEach(n),b1=d(Un),y(Wr.$$.fragment,Un),Un.forEach(n),mn.forEach(n),tp=d(s),zt=a(s,"H2",{class:!0});var fu=i(zt);_o=a(fu,"A",{id:!0,class:!0,href:!0});var gb=i(_o);zd=a(gb,"SPAN",{});var _b=i(zd);y(Qr.$$.fragment,_b),_b.forEach(n),gb.forEach(n),$1=d(fu),qd=a(fu,"SPAN",{});var Tb=i(qd);E1=o(Tb,"TFFunnelModelForPreTraining"),Tb.forEach(n),fu.forEach(n),op=d(s),Le=a(s,"DIV",{class:!0});var gn=i(Le);y(Ur.$$.fragment,gn),M1=d(gn),Pd=a(gn,"P",{});var Fb=i(Pd);z1=o(Fb,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),Fb.forEach(n),q1=d(gn),Rr=a(gn,"P",{});var mu=i(Rr);P1=o(mu,"The Funnel Transformer model was proposed in "),Hr=a(mu,"A",{href:!0,rel:!0});var kb=i(Hr);C1=o(kb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),kb.forEach(n),j1=o(mu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mu.forEach(n),x1=d(gn),Vr=a(gn,"P",{});var gu=i(Vr);L1=o(gu,"This model inherits from "),qi=a(gu,"A",{href:!0});var vb=i(qi);O1=o(vb,"TFPreTrainedModel"),vb.forEach(n),D1=o(gu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gu.forEach(n),A1=d(gn),Yr=a(gn,"P",{});var _u=i(Yr);N1=o(_u,"This model is also a "),Kr=a(_u,"A",{href:!0,rel:!0});var yb=i(Kr);I1=o(yb,"tf.keras.Model"),yb.forEach(n),S1=o(_u,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_u.forEach(n),B1=d(gn),y(To.$$.fragment,gn),W1=d(gn),an=a(gn,"DIV",{class:!0});var Rn=i(an);y(Gr.$$.fragment,Rn),Q1=d(Rn),qt=a(Rn,"P",{});var sl=i(qt);U1=o(sl,"The "),Pi=a(sl,"A",{href:!0});var wb=i(Pi);R1=o(wb,"TFFunnelForPreTraining"),wb.forEach(n),H1=o(sl," forward method, overrides the "),Cd=a(sl,"CODE",{});var bb=i(Cd);V1=o(bb,"__call__"),bb.forEach(n),Y1=o(sl," special method."),sl.forEach(n),K1=d(Rn),y(Fo.$$.fragment,Rn),G1=d(Rn),jd=a(Rn,"P",{});var $b=i(jd);Z1=o($b,"Examples:"),$b.forEach(n),X1=d(Rn),y(Zr.$$.fragment,Rn),Rn.forEach(n),gn.forEach(n),sp=d(s),Pt=a(s,"H2",{class:!0});var Tu=i(Pt);ko=a(Tu,"A",{id:!0,class:!0,href:!0});var Eb=i(ko);xd=a(Eb,"SPAN",{});var Mb=i(xd);y(Xr.$$.fragment,Mb),Mb.forEach(n),Eb.forEach(n),J1=d(Tu),Ld=a(Tu,"SPAN",{});var zb=i(Ld);eF=o(zb,"TFFunnelForMaskedLM"),zb.forEach(n),Tu.forEach(n),rp=d(s),Oe=a(s,"DIV",{class:!0});var _n=i(Oe);y(Jr.$$.fragment,_n),nF=d(_n),ea=a(_n,"P",{});var Fu=i(ea);tF=o(Fu,"Funnel Model with a "),Od=a(Fu,"CODE",{});var qb=i(Od);oF=o(qb,"language modeling"),qb.forEach(n),sF=o(Fu," head on top."),Fu.forEach(n),rF=d(_n),na=a(_n,"P",{});var ku=i(na);aF=o(ku,"The Funnel Transformer model was proposed in "),ta=a(ku,"A",{href:!0,rel:!0});var Pb=i(ta);iF=o(Pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Pb.forEach(n),lF=o(ku," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ku.forEach(n),dF=d(_n),oa=a(_n,"P",{});var vu=i(oa);cF=o(vu,"This model inherits from "),Ci=a(vu,"A",{href:!0});var Cb=i(Ci);pF=o(Cb,"TFPreTrainedModel"),Cb.forEach(n),uF=o(vu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vu.forEach(n),hF=d(_n),sa=a(_n,"P",{});var yu=i(sa);fF=o(yu,"This model is also a "),ra=a(yu,"A",{href:!0,rel:!0});var jb=i(ra);mF=o(jb,"tf.keras.Model"),jb.forEach(n),gF=o(yu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),yu.forEach(n),_F=d(_n),y(vo.$$.fragment,_n),TF=d(_n),ln=a(_n,"DIV",{class:!0});var Hn=i(ln);y(aa.$$.fragment,Hn),FF=d(Hn),Ct=a(Hn,"P",{});var rl=i(Ct);kF=o(rl,"The "),ji=a(rl,"A",{href:!0});var xb=i(ji);vF=o(xb,"TFFunnelForMaskedLM"),xb.forEach(n),yF=o(rl," forward method, overrides the "),Dd=a(rl,"CODE",{});var Lb=i(Dd);wF=o(Lb,"__call__"),Lb.forEach(n),bF=o(rl," special method."),rl.forEach(n),$F=d(Hn),y(yo.$$.fragment,Hn),EF=d(Hn),Ad=a(Hn,"P",{});var Ob=i(Ad);MF=o(Ob,"Example:"),Ob.forEach(n),zF=d(Hn),y(ia.$$.fragment,Hn),Hn.forEach(n),_n.forEach(n),ap=d(s),jt=a(s,"H2",{class:!0});var wu=i(jt);wo=a(wu,"A",{id:!0,class:!0,href:!0});var Db=i(wo);Nd=a(Db,"SPAN",{});var Ab=i(Nd);y(la.$$.fragment,Ab),Ab.forEach(n),Db.forEach(n),qF=d(wu),Id=a(wu,"SPAN",{});var Nb=i(Id);PF=o(Nb,"TFFunnelForSequenceClassification"),Nb.forEach(n),wu.forEach(n),ip=d(s),De=a(s,"DIV",{class:!0});var Tn=i(De);y(da.$$.fragment,Tn),CF=d(Tn),Sd=a(Tn,"P",{});var Ib=i(Sd);jF=o(Ib,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Ib.forEach(n),xF=d(Tn),ca=a(Tn,"P",{});var bu=i(ca);LF=o(bu,"The Funnel Transformer model was proposed in "),pa=a(bu,"A",{href:!0,rel:!0});var Sb=i(pa);OF=o(Sb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sb.forEach(n),DF=o(bu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),bu.forEach(n),AF=d(Tn),ua=a(Tn,"P",{});var $u=i(ua);NF=o($u,"This model inherits from "),xi=a($u,"A",{href:!0});var Bb=i(xi);IF=o(Bb,"TFPreTrainedModel"),Bb.forEach(n),SF=o($u,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$u.forEach(n),BF=d(Tn),ha=a(Tn,"P",{});var Eu=i(ha);WF=o(Eu,"This model is also a "),fa=a(Eu,"A",{href:!0,rel:!0});var Wb=i(fa);QF=o(Wb,"tf.keras.Model"),Wb.forEach(n),UF=o(Eu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Eu.forEach(n),RF=d(Tn),y(bo.$$.fragment,Tn),HF=d(Tn),dn=a(Tn,"DIV",{class:!0});var Vn=i(dn);y(ma.$$.fragment,Vn),VF=d(Vn),xt=a(Vn,"P",{});var al=i(xt);YF=o(al,"The "),Li=a(al,"A",{href:!0});var Qb=i(Li);KF=o(Qb,"TFFunnelForSequenceClassification"),Qb.forEach(n),GF=o(al," forward method, overrides the "),Bd=a(al,"CODE",{});var Ub=i(Bd);ZF=o(Ub,"__call__"),Ub.forEach(n),XF=o(al," special method."),al.forEach(n),JF=d(Vn),y($o.$$.fragment,Vn),ek=d(Vn),Wd=a(Vn,"P",{});var Rb=i(Wd);nk=o(Rb,"Example:"),Rb.forEach(n),tk=d(Vn),y(ga.$$.fragment,Vn),Vn.forEach(n),Tn.forEach(n),lp=d(s),Lt=a(s,"H2",{class:!0});var Mu=i(Lt);Eo=a(Mu,"A",{id:!0,class:!0,href:!0});var Hb=i(Eo);Qd=a(Hb,"SPAN",{});var Vb=i(Qd);y(_a.$$.fragment,Vb),Vb.forEach(n),Hb.forEach(n),ok=d(Mu),Ud=a(Mu,"SPAN",{});var Yb=i(Ud);sk=o(Yb,"TFFunnelForMultipleChoice"),Yb.forEach(n),Mu.forEach(n),dp=d(s),Ae=a(s,"DIV",{class:!0});var Fn=i(Ae);y(Ta.$$.fragment,Fn),rk=d(Fn),Rd=a(Fn,"P",{});var Kb=i(Rd);ak=o(Kb,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Kb.forEach(n),ik=d(Fn),Fa=a(Fn,"P",{});var zu=i(Fa);lk=o(zu,"The Funnel Transformer model was proposed in "),ka=a(zu,"A",{href:!0,rel:!0});var Gb=i(ka);dk=o(Gb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gb.forEach(n),ck=o(zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zu.forEach(n),pk=d(Fn),va=a(Fn,"P",{});var qu=i(va);uk=o(qu,"This model inherits from "),Oi=a(qu,"A",{href:!0});var Zb=i(Oi);hk=o(Zb,"TFPreTrainedModel"),Zb.forEach(n),fk=o(qu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qu.forEach(n),mk=d(Fn),ya=a(Fn,"P",{});var Pu=i(ya);gk=o(Pu,"This model is also a "),wa=a(Pu,"A",{href:!0,rel:!0});var Xb=i(wa);_k=o(Xb,"tf.keras.Model"),Xb.forEach(n),Tk=o(Pu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pu.forEach(n),Fk=d(Fn),y(Mo.$$.fragment,Fn),kk=d(Fn),cn=a(Fn,"DIV",{class:!0});var Yn=i(cn);y(ba.$$.fragment,Yn),vk=d(Yn),Ot=a(Yn,"P",{});var il=i(Ot);yk=o(il,"The "),Di=a(il,"A",{href:!0});var Jb=i(Di);wk=o(Jb,"TFFunnelForMultipleChoice"),Jb.forEach(n),bk=o(il," forward method, overrides the "),Hd=a(il,"CODE",{});var e0=i(Hd);$k=o(e0,"__call__"),e0.forEach(n),Ek=o(il," special method."),il.forEach(n),Mk=d(Yn),y(zo.$$.fragment,Yn),zk=d(Yn),Vd=a(Yn,"P",{});var n0=i(Vd);qk=o(n0,"Example:"),n0.forEach(n),Pk=d(Yn),y($a.$$.fragment,Yn),Yn.forEach(n),Fn.forEach(n),cp=d(s),Dt=a(s,"H2",{class:!0});var Cu=i(Dt);qo=a(Cu,"A",{id:!0,class:!0,href:!0});var t0=i(qo);Yd=a(t0,"SPAN",{});var o0=i(Yd);y(Ea.$$.fragment,o0),o0.forEach(n),t0.forEach(n),Ck=d(Cu),Kd=a(Cu,"SPAN",{});var s0=i(Kd);jk=o(s0,"TFFunnelForTokenClassification"),s0.forEach(n),Cu.forEach(n),pp=d(s),Ne=a(s,"DIV",{class:!0});var kn=i(Ne);y(Ma.$$.fragment,kn),xk=d(kn),Gd=a(kn,"P",{});var r0=i(Gd);Lk=o(r0,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),r0.forEach(n),Ok=d(kn),za=a(kn,"P",{});var ju=i(za);Dk=o(ju,"The Funnel Transformer model was proposed in "),qa=a(ju,"A",{href:!0,rel:!0});var a0=i(qa);Ak=o(a0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),a0.forEach(n),Nk=o(ju," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ju.forEach(n),Ik=d(kn),Pa=a(kn,"P",{});var xu=i(Pa);Sk=o(xu,"This model inherits from "),Ai=a(xu,"A",{href:!0});var i0=i(Ai);Bk=o(i0,"TFPreTrainedModel"),i0.forEach(n),Wk=o(xu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xu.forEach(n),Qk=d(kn),Ca=a(kn,"P",{});var Lu=i(Ca);Uk=o(Lu,"This model is also a "),ja=a(Lu,"A",{href:!0,rel:!0});var l0=i(ja);Rk=o(l0,"tf.keras.Model"),l0.forEach(n),Hk=o(Lu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lu.forEach(n),Vk=d(kn),y(Po.$$.fragment,kn),Yk=d(kn),pn=a(kn,"DIV",{class:!0});var Kn=i(pn);y(xa.$$.fragment,Kn),Kk=d(Kn),At=a(Kn,"P",{});var ll=i(At);Gk=o(ll,"The "),Ni=a(ll,"A",{href:!0});var d0=i(Ni);Zk=o(d0,"TFFunnelForTokenClassification"),d0.forEach(n),Xk=o(ll," forward method, overrides the "),Zd=a(ll,"CODE",{});var c0=i(Zd);Jk=o(c0,"__call__"),c0.forEach(n),ev=o(ll," special method."),ll.forEach(n),nv=d(Kn),y(Co.$$.fragment,Kn),tv=d(Kn),Xd=a(Kn,"P",{});var p0=i(Xd);ov=o(p0,"Example:"),p0.forEach(n),sv=d(Kn),y(La.$$.fragment,Kn),Kn.forEach(n),kn.forEach(n),up=d(s),Nt=a(s,"H2",{class:!0});var Ou=i(Nt);jo=a(Ou,"A",{id:!0,class:!0,href:!0});var u0=i(jo);Jd=a(u0,"SPAN",{});var h0=i(Jd);y(Oa.$$.fragment,h0),h0.forEach(n),u0.forEach(n),rv=d(Ou),ec=a(Ou,"SPAN",{});var f0=i(ec);av=o(f0,"TFFunnelForQuestionAnswering"),f0.forEach(n),Ou.forEach(n),hp=d(s),Ie=a(s,"DIV",{class:!0});var vn=i(Ie);y(Da.$$.fragment,vn),iv=d(vn),It=a(vn,"P",{});var dl=i(It);lv=o(dl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=a(dl,"CODE",{});var m0=i(nc);dv=o(m0,"span start logits"),m0.forEach(n),cv=o(dl," and "),tc=a(dl,"CODE",{});var g0=i(tc);pv=o(g0,"span end logits"),g0.forEach(n),uv=o(dl,")."),dl.forEach(n),hv=d(vn),Aa=a(vn,"P",{});var Du=i(Aa);fv=o(Du,"The Funnel Transformer model was proposed in "),Na=a(Du,"A",{href:!0,rel:!0});var _0=i(Na);mv=o(_0,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),_0.forEach(n),gv=o(Du," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Du.forEach(n),_v=d(vn),Ia=a(vn,"P",{});var Au=i(Ia);Tv=o(Au,"This model inherits from "),Ii=a(Au,"A",{href:!0});var T0=i(Ii);Fv=o(T0,"TFPreTrainedModel"),T0.forEach(n),kv=o(Au,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Au.forEach(n),vv=d(vn),Sa=a(vn,"P",{});var Nu=i(Sa);yv=o(Nu,"This model is also a "),Ba=a(Nu,"A",{href:!0,rel:!0});var F0=i(Ba);wv=o(F0,"tf.keras.Model"),F0.forEach(n),bv=o(Nu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Nu.forEach(n),$v=d(vn),y(xo.$$.fragment,vn),Ev=d(vn),un=a(vn,"DIV",{class:!0});var Gn=i(un);y(Wa.$$.fragment,Gn),Mv=d(Gn),St=a(Gn,"P",{});var cl=i(St);zv=o(cl,"The "),Si=a(cl,"A",{href:!0});var k0=i(Si);qv=o(k0,"TFFunnelForQuestionAnswering"),k0.forEach(n),Pv=o(cl," forward method, overrides the "),oc=a(cl,"CODE",{});var v0=i(oc);Cv=o(v0,"__call__"),v0.forEach(n),jv=o(cl," special method."),cl.forEach(n),xv=d(Gn),y(Lo.$$.fragment,Gn),Lv=d(Gn),sc=a(Gn,"P",{});var y0=i(sc);Ov=o(y0,"Example:"),y0.forEach(n),Dv=d(Gn),y(Qa.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(X0)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelModel"),c(p,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Fe,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ke,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ve,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Ya,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(No,"href","https://huggingface.co/sgugger"),c(No,"rel","nofollow"),c(Io,"href","https://github.com/laiguokun/Funnel-Transformer"),c(Io,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Zn,"class","relative group"),c(Ka,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelModel"),c(Ga,"href","/docs/transformers/pr_16310/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(Za,"href","/docs/transformers/pr_16310/en/main_classes/configuration#transformers.PretrainedConfig"),c(Xa,"href","/docs/transformers/pr_16310/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ja,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizer"),c(ei,"href","/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer"),c(ni,"href","/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Ut,"class","docstring"),c(yn,"class","docstring"),c(si,"class","docstring"),c(Pe,"class","docstring"),c(Rt,"id","transformers.FunnelTokenizerFast"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ri,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ai,"href","/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizerFast"),c(ii,"href","/docs/transformers/pr_16310/en/model_doc/bert#transformers.BertTokenizerFast"),c(wn,"class","docstring"),c(Ge,"class","docstring"),c(Vt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(li,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(di,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(ci,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(pi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ze,"class","docstring"),c(We,"class","docstring"),c(Gt,"id","transformers.FunnelModel"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ys,"href","https://arxiv.org/abs/2006.03236"),c(ys,"rel","nofollow"),c(ui,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(hi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(fi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(pt,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(mi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(gi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Ue,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Rs,"href","https://arxiv.org/abs/2006.03236"),c(Rs,"rel","nofollow"),c(_i,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c(Ys,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ys,"rel","nofollow"),c(Ti,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(Re,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(nr,"href","https://arxiv.org/abs/2006.03236"),c(nr,"rel","nofollow"),c(Fi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c(sr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(sr,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(He,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(Ft,"class","relative group"),c(cr,"href","https://arxiv.org/abs/2006.03236"),c(cr,"rel","nofollow"),c(vi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c(hr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(hr,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ve,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(vt,"class","relative group"),c(Fr,"href","https://arxiv.org/abs/2006.03236"),c(Fr,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.PreTrainedModel"),c(yr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(yr,"rel","nofollow"),c(bi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ye,"class","docstring"),c(po,"id","transformers.TFFunnelBaseModel"),c(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(po,"href","#transformers.TFFunnelBaseModel"),c(bt,"class","relative group"),c(zr,"href","https://arxiv.org/abs/2006.03236"),c(zr,"rel","nofollow"),c($i,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(je,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Ar,"href","https://arxiv.org/abs/2006.03236"),c(Ar,"rel","nofollow"),c(Mi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(Sr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Sr,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(xe,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Hr,"href","https://arxiv.org/abs/2006.03236"),c(Hr,"rel","nofollow"),c(qi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(Kr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Kr,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(ko,"id","transformers.TFFunnelForMaskedLM"),c(ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ko,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ta,"href","https://arxiv.org/abs/2006.03236"),c(ta,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(ra,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ra,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Oe,"class","docstring"),c(wo,"id","transformers.TFFunnelForSequenceClassification"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFFunnelForSequenceClassification"),c(jt,"class","relative group"),c(pa,"href","https://arxiv.org/abs/2006.03236"),c(pa,"rel","nofollow"),c(xi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(fa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(fa,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c(ka,"href","https://arxiv.org/abs/2006.03236"),c(ka,"rel","nofollow"),c(Oi,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(wa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(wa,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ae,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(qa,"href","https://arxiv.org/abs/2006.03236"),c(qa,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(ja,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ja,"rel","nofollow"),c(Ni,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(pn,"class","docstring"),c(Ne,"class","docstring"),c(jo,"id","transformers.TFFunnelForQuestionAnswering"),c(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jo,"href","#transformers.TFFunnelForQuestionAnswering"),c(Nt,"class","relative group"),c(Na,"href","https://arxiv.org/abs/2006.03236"),c(Na,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_16310/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ba,"rel","nofollow"),c(Si,"href","/docs/transformers/pr_16310/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(un,"class","docstring"),c(Ie,"class","docstring")},m(s,f){e(document.head,u),h(s,M,f),h(s,m,f),e(m,g),e(g,F),w(T,F,null),e(m,_),e(m,z),e(z,ce),h(s,K,f),h(s,q,f),e(q,J),e(J,A),w(ne,A,null),e(q,pe),e(q,N),e(N,ue),h(s,ie,f),h(s,Y,f),e(Y,L),e(Y,te),e(te,G),e(Y,P),h(s,j,f),h(s,oe,f),e(oe,Q),h(s,le,f),h(s,se,f),e(se,I),e(I,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,U),e(ee,me),e(ee,S),e(S,O),e(S,re),e(re,R),e(S,ge),e(S,p),e(p,k),e(S,Z),e(S,Te),e(Te,ye),e(S,D),e(S,Fe),e(Fe,we),e(S,be),e(S,x),e(x,H),e(S,$e),e(S,ke),e(ke,V),e(S,Ee),e(S,ve),e(ve,_e),e(S,Me),e(S,Ya),e(Ya,Iu),e(S,Su),h(s,Ec,f),h(s,xn,f),e(xn,Bu),e(xn,No),e(No,Wu),e(xn,Qu),e(xn,Io),e(Io,Uu),e(xn,Ru),h(s,Mc,f),h(s,Zn,f),e(Zn,Bt),e(Bt,pl),w(So,pl,null),e(Zn,Hu),e(Zn,ul),e(ul,Vu),h(s,zc,f),h(s,Cn,f),w(Bo,Cn,null),e(Cn,Yu),e(Cn,jn),e(jn,Ku),e(jn,Ka),e(Ka,Gu),e(jn,Zu),e(jn,Ga),e(Ga,Xu),e(jn,Ju),e(jn,Wo),e(Wo,eh),e(jn,nh),e(Cn,th),e(Cn,Xn),e(Xn,oh),e(Xn,Za),e(Za,sh),e(Xn,rh),e(Xn,Xa),e(Xa,ah),e(Xn,ih),h(s,qc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,hl),w(Qo,hl,null),e(Jn,lh),e(Jn,fl),e(fl,dh),h(s,Pc,f),h(s,Pe,f),w(Uo,Pe,null),e(Pe,ch),e(Pe,ml),e(ml,ph),e(Pe,uh),e(Pe,Qt),e(Qt,Ja),e(Ja,hh),e(Qt,fh),e(Qt,ei),e(ei,mh),e(Qt,gh),e(Pe,_h),e(Pe,Ro),e(Ro,Th),e(Ro,ni),e(ni,Fh),e(Ro,kh),e(Pe,vh),e(Pe,Ln),w(Ho,Ln,null),e(Ln,yh),e(Ln,gl),e(gl,wh),e(Ln,bh),e(Ln,Vo),e(Vo,ti),e(ti,$h),e(ti,_l),e(_l,Eh),e(Vo,Mh),e(Vo,oi),e(oi,zh),e(oi,Tl),e(Tl,qh),e(Pe,Ph),e(Pe,Ut),w(Yo,Ut,null),e(Ut,Ch),e(Ut,Ko),e(Ko,jh),e(Ko,Fl),e(Fl,xh),e(Ko,Lh),e(Pe,Oh),e(Pe,yn),w(Go,yn,null),e(yn,Dh),e(yn,kl),e(kl,Ah),e(yn,Nh),w(Zo,yn,null),e(yn,Ih),e(yn,et),e(et,Sh),e(et,vl),e(vl,Bh),e(et,Wh),e(et,yl),e(yl,Qh),e(et,Uh),e(Pe,Rh),e(Pe,si),w(Xo,si,null),h(s,Cc,f),h(s,nt,f),e(nt,Rt),e(Rt,wl),w(Jo,wl,null),e(nt,Hh),e(nt,bl),e(bl,Vh),h(s,jc,f),h(s,Ge,f),w(es,Ge,null),e(Ge,Yh),e(Ge,ns),e(ns,Kh),e(ns,$l),e($l,Gh),e(ns,Zh),e(Ge,Xh),e(Ge,Ht),e(Ht,ri),e(ri,Jh),e(Ht,ef),e(Ht,ai),e(ai,nf),e(Ht,tf),e(Ge,of),e(Ge,ts),e(ts,sf),e(ts,ii),e(ii,rf),e(ts,af),e(Ge,lf),e(Ge,wn),w(os,wn,null),e(wn,df),e(wn,El),e(El,cf),e(wn,pf),w(ss,wn,null),e(wn,uf),e(wn,tt),e(tt,hf),e(tt,Ml),e(Ml,ff),e(tt,mf),e(tt,zl),e(zl,gf),e(tt,_f),h(s,xc,f),h(s,ot,f),e(ot,Vt),e(Vt,ql),w(rs,ql,null),e(ot,Tf),e(ot,Pl),e(Pl,Ff),h(s,Lc,f),h(s,st,f),w(as,st,null),e(st,kf),e(st,is),e(is,vf),e(is,li),e(li,yf),e(is,wf),h(s,Oc,f),h(s,rt,f),w(ls,rt,null),e(rt,bf),e(rt,ds),e(ds,$f),e(ds,di),e(di,Ef),e(ds,Mf),h(s,Dc,f),h(s,at,f),e(at,Yt),e(Yt,Cl),w(cs,Cl,null),e(at,zf),e(at,jl),e(jl,qf),h(s,Ac,f),h(s,We,f),w(ps,We,null),e(We,Pf),e(We,xl),e(xl,Cf),e(We,jf),e(We,us),e(us,xf),e(us,hs),e(hs,Lf),e(us,Of),e(We,Df),e(We,fs),e(fs,Af),e(fs,ci),e(ci,Nf),e(fs,If),e(We,Sf),e(We,ms),e(ms,Bf),e(ms,gs),e(gs,Wf),e(ms,Qf),e(We,Uf),e(We,Ze),w(_s,Ze,null),e(Ze,Rf),e(Ze,it),e(it,Hf),e(it,pi),e(pi,Vf),e(it,Yf),e(it,Ll),e(Ll,Kf),e(it,Gf),e(Ze,Zf),w(Kt,Ze,null),e(Ze,Xf),e(Ze,Ol),e(Ol,Jf),e(Ze,em),w(Ts,Ze,null),h(s,Nc,f),h(s,lt,f),e(lt,Gt),e(Gt,Dl),w(Fs,Dl,null),e(lt,nm),e(lt,Al),e(Al,tm),h(s,Ic,f),h(s,Qe,f),w(ks,Qe,null),e(Qe,om),e(Qe,Nl),e(Nl,sm),e(Qe,rm),e(Qe,vs),e(vs,am),e(vs,ys),e(ys,im),e(vs,lm),e(Qe,dm),e(Qe,ws),e(ws,cm),e(ws,ui),e(ui,pm),e(ws,um),e(Qe,hm),e(Qe,bs),e(bs,fm),e(bs,$s),e($s,mm),e(bs,gm),e(Qe,_m),e(Qe,Xe),w(Es,Xe,null),e(Xe,Tm),e(Xe,dt),e(dt,Fm),e(dt,hi),e(hi,km),e(dt,vm),e(dt,Il),e(Il,ym),e(dt,wm),e(Xe,bm),w(Zt,Xe,null),e(Xe,$m),e(Xe,Sl),e(Sl,Em),e(Xe,Mm),w(Ms,Xe,null),h(s,Sc,f),h(s,ct,f),e(ct,Xt),e(Xt,Bl),w(zs,Bl,null),e(ct,zm),e(ct,Wl),e(Wl,qm),h(s,Bc,f),h(s,pt,f),w(qs,pt,null),e(pt,Pm),e(pt,Je),w(Ps,Je,null),e(Je,Cm),e(Je,ut),e(ut,jm),e(ut,fi),e(fi,xm),e(ut,Lm),e(ut,Ql),e(Ql,Om),e(ut,Dm),e(Je,Am),w(Jt,Je,null),e(Je,Nm),e(Je,Ul),e(Ul,Im),e(Je,Sm),w(Cs,Je,null),h(s,Wc,f),h(s,ht,f),e(ht,eo),e(eo,Rl),w(js,Rl,null),e(ht,Bm),e(ht,Hl),e(Hl,Wm),h(s,Qc,f),h(s,Ue,f),w(xs,Ue,null),e(Ue,Qm),e(Ue,Ls),e(Ls,Um),e(Ls,Vl),e(Vl,Rm),e(Ls,Hm),e(Ue,Vm),e(Ue,Os),e(Os,Ym),e(Os,Ds),e(Ds,Km),e(Os,Gm),e(Ue,Zm),e(Ue,As),e(As,Xm),e(As,mi),e(mi,Jm),e(As,eg),e(Ue,ng),e(Ue,Ns),e(Ns,tg),e(Ns,Is),e(Is,og),e(Ns,sg),e(Ue,rg),e(Ue,en),w(Ss,en,null),e(en,ag),e(en,ft),e(ft,ig),e(ft,gi),e(gi,lg),e(ft,dg),e(ft,Yl),e(Yl,cg),e(ft,pg),e(en,ug),w(no,en,null),e(en,hg),e(en,Kl),e(Kl,fg),e(en,mg),w(Bs,en,null),h(s,Uc,f),h(s,mt,f),e(mt,to),e(to,Gl),w(Ws,Gl,null),e(mt,gg),e(mt,Zl),e(Zl,_g),h(s,Rc,f),h(s,Re,f),w(Qs,Re,null),e(Re,Tg),e(Re,Xl),e(Xl,Fg),e(Re,kg),e(Re,Us),e(Us,vg),e(Us,Rs),e(Rs,yg),e(Us,wg),e(Re,bg),e(Re,Hs),e(Hs,$g),e(Hs,_i),e(_i,Eg),e(Hs,Mg),e(Re,zg),e(Re,Vs),e(Vs,qg),e(Vs,Ys),e(Ys,Pg),e(Vs,Cg),e(Re,jg),e(Re,Be),w(Ks,Be,null),e(Be,xg),e(Be,gt),e(gt,Lg),e(gt,Ti),e(Ti,Og),e(gt,Dg),e(gt,Jl),e(Jl,Ag),e(gt,Ng),e(Be,Ig),w(oo,Be,null),e(Be,Sg),e(Be,ed),e(ed,Bg),e(Be,Wg),w(Gs,Be,null),e(Be,Qg),e(Be,nd),e(nd,Ug),e(Be,Rg),w(Zs,Be,null),h(s,Hc,f),h(s,_t,f),e(_t,so),e(so,td),w(Xs,td,null),e(_t,Hg),e(_t,od),e(od,Vg),h(s,Vc,f),h(s,He,f),w(Js,He,null),e(He,Yg),e(He,sd),e(sd,Kg),e(He,Gg),e(He,er),e(er,Zg),e(er,nr),e(nr,Xg),e(er,Jg),e(He,e_),e(He,tr),e(tr,n_),e(tr,Fi),e(Fi,t_),e(tr,o_),e(He,s_),e(He,or),e(or,r_),e(or,sr),e(sr,a_),e(or,i_),e(He,l_),e(He,nn),w(rr,nn,null),e(nn,d_),e(nn,Tt),e(Tt,c_),e(Tt,ki),e(ki,p_),e(Tt,u_),e(Tt,rd),e(rd,h_),e(Tt,f_),e(nn,m_),w(ro,nn,null),e(nn,g_),e(nn,ad),e(ad,__),e(nn,T_),w(ar,nn,null),h(s,Yc,f),h(s,Ft,f),e(Ft,ao),e(ao,id),w(ir,id,null),e(Ft,F_),e(Ft,ld),e(ld,k_),h(s,Kc,f),h(s,Ve,f),w(lr,Ve,null),e(Ve,v_),e(Ve,dd),e(dd,y_),e(Ve,w_),e(Ve,dr),e(dr,b_),e(dr,cr),e(cr,$_),e(dr,E_),e(Ve,M_),e(Ve,pr),e(pr,z_),e(pr,vi),e(vi,q_),e(pr,P_),e(Ve,C_),e(Ve,ur),e(ur,j_),e(ur,hr),e(hr,x_),e(ur,L_),e(Ve,O_),e(Ve,tn),w(fr,tn,null),e(tn,D_),e(tn,kt),e(kt,A_),e(kt,yi),e(yi,N_),e(kt,I_),e(kt,cd),e(cd,S_),e(kt,B_),e(tn,W_),w(io,tn,null),e(tn,Q_),e(tn,pd),e(pd,U_),e(tn,R_),w(mr,tn,null),h(s,Gc,f),h(s,vt,f),e(vt,lo),e(lo,ud),w(gr,ud,null),e(vt,H_),e(vt,hd),e(hd,V_),h(s,Zc,f),h(s,Ye,f),w(_r,Ye,null),e(Ye,Y_),e(Ye,yt),e(yt,K_),e(yt,fd),e(fd,G_),e(yt,Z_),e(yt,md),e(md,X_),e(yt,J_),e(Ye,eT),e(Ye,Tr),e(Tr,nT),e(Tr,Fr),e(Fr,tT),e(Tr,oT),e(Ye,sT),e(Ye,kr),e(kr,rT),e(kr,wi),e(wi,aT),e(kr,iT),e(Ye,lT),e(Ye,vr),e(vr,dT),e(vr,yr),e(yr,cT),e(vr,pT),e(Ye,uT),e(Ye,on),w(wr,on,null),e(on,hT),e(on,wt),e(wt,fT),e(wt,bi),e(bi,mT),e(wt,gT),e(wt,gd),e(gd,_T),e(wt,TT),e(on,FT),w(co,on,null),e(on,kT),e(on,_d),e(_d,vT),e(on,yT),w(br,on,null),h(s,Xc,f),h(s,bt,f),e(bt,po),e(po,Td),w($r,Td,null),e(bt,wT),e(bt,Fd),e(Fd,bT),h(s,Jc,f),h(s,je,f),w(Er,je,null),e(je,$T),e(je,kd),e(kd,ET),e(je,MT),e(je,Mr),e(Mr,zT),e(Mr,zr),e(zr,qT),e(Mr,PT),e(je,CT),e(je,qr),e(qr,jT),e(qr,$i),e($i,xT),e(qr,LT),e(je,OT),e(je,Pr),e(Pr,DT),e(Pr,Cr),e(Cr,AT),e(Pr,NT),e(je,IT),w(uo,je,null),e(je,ST),e(je,sn),w(jr,sn,null),e(sn,BT),e(sn,$t),e($t,WT),e($t,Ei),e(Ei,QT),e($t,UT),e($t,vd),e(vd,RT),e($t,HT),e(sn,VT),w(ho,sn,null),e(sn,YT),e(sn,yd),e(yd,KT),e(sn,GT),w(xr,sn,null),h(s,ep,f),h(s,Et,f),e(Et,fo),e(fo,wd),w(Lr,wd,null),e(Et,ZT),e(Et,bd),e(bd,XT),h(s,np,f),h(s,xe,f),w(Or,xe,null),e(xe,JT),e(xe,$d),e($d,e1),e(xe,n1),e(xe,Dr),e(Dr,t1),e(Dr,Ar),e(Ar,o1),e(Dr,s1),e(xe,r1),e(xe,Nr),e(Nr,a1),e(Nr,Mi),e(Mi,i1),e(Nr,l1),e(xe,d1),e(xe,Ir),e(Ir,c1),e(Ir,Sr),e(Sr,p1),e(Ir,u1),e(xe,h1),w(mo,xe,null),e(xe,f1),e(xe,rn),w(Br,rn,null),e(rn,m1),e(rn,Mt),e(Mt,g1),e(Mt,zi),e(zi,_1),e(Mt,T1),e(Mt,Ed),e(Ed,F1),e(Mt,k1),e(rn,v1),w(go,rn,null),e(rn,y1),e(rn,Md),e(Md,w1),e(rn,b1),w(Wr,rn,null),h(s,tp,f),h(s,zt,f),e(zt,_o),e(_o,zd),w(Qr,zd,null),e(zt,$1),e(zt,qd),e(qd,E1),h(s,op,f),h(s,Le,f),w(Ur,Le,null),e(Le,M1),e(Le,Pd),e(Pd,z1),e(Le,q1),e(Le,Rr),e(Rr,P1),e(Rr,Hr),e(Hr,C1),e(Rr,j1),e(Le,x1),e(Le,Vr),e(Vr,L1),e(Vr,qi),e(qi,O1),e(Vr,D1),e(Le,A1),e(Le,Yr),e(Yr,N1),e(Yr,Kr),e(Kr,I1),e(Yr,S1),e(Le,B1),w(To,Le,null),e(Le,W1),e(Le,an),w(Gr,an,null),e(an,Q1),e(an,qt),e(qt,U1),e(qt,Pi),e(Pi,R1),e(qt,H1),e(qt,Cd),e(Cd,V1),e(qt,Y1),e(an,K1),w(Fo,an,null),e(an,G1),e(an,jd),e(jd,Z1),e(an,X1),w(Zr,an,null),h(s,sp,f),h(s,Pt,f),e(Pt,ko),e(ko,xd),w(Xr,xd,null),e(Pt,J1),e(Pt,Ld),e(Ld,eF),h(s,rp,f),h(s,Oe,f),w(Jr,Oe,null),e(Oe,nF),e(Oe,ea),e(ea,tF),e(ea,Od),e(Od,oF),e(ea,sF),e(Oe,rF),e(Oe,na),e(na,aF),e(na,ta),e(ta,iF),e(na,lF),e(Oe,dF),e(Oe,oa),e(oa,cF),e(oa,Ci),e(Ci,pF),e(oa,uF),e(Oe,hF),e(Oe,sa),e(sa,fF),e(sa,ra),e(ra,mF),e(sa,gF),e(Oe,_F),w(vo,Oe,null),e(Oe,TF),e(Oe,ln),w(aa,ln,null),e(ln,FF),e(ln,Ct),e(Ct,kF),e(Ct,ji),e(ji,vF),e(Ct,yF),e(Ct,Dd),e(Dd,wF),e(Ct,bF),e(ln,$F),w(yo,ln,null),e(ln,EF),e(ln,Ad),e(Ad,MF),e(ln,zF),w(ia,ln,null),h(s,ap,f),h(s,jt,f),e(jt,wo),e(wo,Nd),w(la,Nd,null),e(jt,qF),e(jt,Id),e(Id,PF),h(s,ip,f),h(s,De,f),w(da,De,null),e(De,CF),e(De,Sd),e(Sd,jF),e(De,xF),e(De,ca),e(ca,LF),e(ca,pa),e(pa,OF),e(ca,DF),e(De,AF),e(De,ua),e(ua,NF),e(ua,xi),e(xi,IF),e(ua,SF),e(De,BF),e(De,ha),e(ha,WF),e(ha,fa),e(fa,QF),e(ha,UF),e(De,RF),w(bo,De,null),e(De,HF),e(De,dn),w(ma,dn,null),e(dn,VF),e(dn,xt),e(xt,YF),e(xt,Li),e(Li,KF),e(xt,GF),e(xt,Bd),e(Bd,ZF),e(xt,XF),e(dn,JF),w($o,dn,null),e(dn,ek),e(dn,Wd),e(Wd,nk),e(dn,tk),w(ga,dn,null),h(s,lp,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Qd),w(_a,Qd,null),e(Lt,ok),e(Lt,Ud),e(Ud,sk),h(s,dp,f),h(s,Ae,f),w(Ta,Ae,null),e(Ae,rk),e(Ae,Rd),e(Rd,ak),e(Ae,ik),e(Ae,Fa),e(Fa,lk),e(Fa,ka),e(ka,dk),e(Fa,ck),e(Ae,pk),e(Ae,va),e(va,uk),e(va,Oi),e(Oi,hk),e(va,fk),e(Ae,mk),e(Ae,ya),e(ya,gk),e(ya,wa),e(wa,_k),e(ya,Tk),e(Ae,Fk),w(Mo,Ae,null),e(Ae,kk),e(Ae,cn),w(ba,cn,null),e(cn,vk),e(cn,Ot),e(Ot,yk),e(Ot,Di),e(Di,wk),e(Ot,bk),e(Ot,Hd),e(Hd,$k),e(Ot,Ek),e(cn,Mk),w(zo,cn,null),e(cn,zk),e(cn,Vd),e(Vd,qk),e(cn,Pk),w($a,cn,null),h(s,cp,f),h(s,Dt,f),e(Dt,qo),e(qo,Yd),w(Ea,Yd,null),e(Dt,Ck),e(Dt,Kd),e(Kd,jk),h(s,pp,f),h(s,Ne,f),w(Ma,Ne,null),e(Ne,xk),e(Ne,Gd),e(Gd,Lk),e(Ne,Ok),e(Ne,za),e(za,Dk),e(za,qa),e(qa,Ak),e(za,Nk),e(Ne,Ik),e(Ne,Pa),e(Pa,Sk),e(Pa,Ai),e(Ai,Bk),e(Pa,Wk),e(Ne,Qk),e(Ne,Ca),e(Ca,Uk),e(Ca,ja),e(ja,Rk),e(Ca,Hk),e(Ne,Vk),w(Po,Ne,null),e(Ne,Yk),e(Ne,pn),w(xa,pn,null),e(pn,Kk),e(pn,At),e(At,Gk),e(At,Ni),e(Ni,Zk),e(At,Xk),e(At,Zd),e(Zd,Jk),e(At,ev),e(pn,nv),w(Co,pn,null),e(pn,tv),e(pn,Xd),e(Xd,ov),e(pn,sv),w(La,pn,null),h(s,up,f),h(s,Nt,f),e(Nt,jo),e(jo,Jd),w(Oa,Jd,null),e(Nt,rv),e(Nt,ec),e(ec,av),h(s,hp,f),h(s,Ie,f),w(Da,Ie,null),e(Ie,iv),e(Ie,It),e(It,lv),e(It,nc),e(nc,dv),e(It,cv),e(It,tc),e(tc,pv),e(It,uv),e(Ie,hv),e(Ie,Aa),e(Aa,fv),e(Aa,Na),e(Na,mv),e(Aa,gv),e(Ie,_v),e(Ie,Ia),e(Ia,Tv),e(Ia,Ii),e(Ii,Fv),e(Ia,kv),e(Ie,vv),e(Ie,Sa),e(Sa,yv),e(Sa,Ba),e(Ba,wv),e(Sa,bv),e(Ie,$v),w(xo,Ie,null),e(Ie,Ev),e(Ie,un),w(Wa,un,null),e(un,Mv),e(un,St),e(St,zv),e(St,Si),e(Si,qv),e(St,Pv),e(St,oc),e(oc,Cv),e(St,jv),e(un,xv),w(Lo,un,null),e(un,Lv),e(un,sc),e(sc,Ov),e(un,Dv),w(Qa,un,null),fp=!0},p(s,[f]){const Ua={};f&2&&(Ua.$$scope={dirty:f,ctx:s}),Kt.$set(Ua);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:s}),Zt.$set(rc);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:s}),Jt.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:s}),no.$set(ic);const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:s}),oo.$set(Ra);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:s}),ro.$set(lc);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:s}),io.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),co.$set(cc);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:s}),uo.$set(Ha);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),ho.$set(pc);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),mo.$set(uc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),go.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),To.$set(fc);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),Fo.$set(mc);const Va={};f&2&&(Va.$$scope={dirty:f,ctx:s}),vo.$set(Va);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),yo.$set(gc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:s}),bo.$set(Ce);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),$o.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),Mo.$set(Tc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),zo.$set(Fc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),Po.$set(kc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),Co.$set(vc);const yc={};f&2&&(yc.$$scope={dirty:f,ctx:s}),xo.$set(yc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),Lo.$set(wc)},i(s){fp||(b(T.$$.fragment,s),b(ne.$$.fragment,s),b(So.$$.fragment,s),b(Bo.$$.fragment,s),b(Qo.$$.fragment,s),b(Uo.$$.fragment,s),b(Ho.$$.fragment,s),b(Yo.$$.fragment,s),b(Go.$$.fragment,s),b(Zo.$$.fragment,s),b(Xo.$$.fragment,s),b(Jo.$$.fragment,s),b(es.$$.fragment,s),b(os.$$.fragment,s),b(ss.$$.fragment,s),b(rs.$$.fragment,s),b(as.$$.fragment,s),b(ls.$$.fragment,s),b(cs.$$.fragment,s),b(ps.$$.fragment,s),b(_s.$$.fragment,s),b(Kt.$$.fragment,s),b(Ts.$$.fragment,s),b(Fs.$$.fragment,s),b(ks.$$.fragment,s),b(Es.$$.fragment,s),b(Zt.$$.fragment,s),b(Ms.$$.fragment,s),b(zs.$$.fragment,s),b(qs.$$.fragment,s),b(Ps.$$.fragment,s),b(Jt.$$.fragment,s),b(Cs.$$.fragment,s),b(js.$$.fragment,s),b(xs.$$.fragment,s),b(Ss.$$.fragment,s),b(no.$$.fragment,s),b(Bs.$$.fragment,s),b(Ws.$$.fragment,s),b(Qs.$$.fragment,s),b(Ks.$$.fragment,s),b(oo.$$.fragment,s),b(Gs.$$.fragment,s),b(Zs.$$.fragment,s),b(Xs.$$.fragment,s),b(Js.$$.fragment,s),b(rr.$$.fragment,s),b(ro.$$.fragment,s),b(ar.$$.fragment,s),b(ir.$$.fragment,s),b(lr.$$.fragment,s),b(fr.$$.fragment,s),b(io.$$.fragment,s),b(mr.$$.fragment,s),b(gr.$$.fragment,s),b(_r.$$.fragment,s),b(wr.$$.fragment,s),b(co.$$.fragment,s),b(br.$$.fragment,s),b($r.$$.fragment,s),b(Er.$$.fragment,s),b(uo.$$.fragment,s),b(jr.$$.fragment,s),b(ho.$$.fragment,s),b(xr.$$.fragment,s),b(Lr.$$.fragment,s),b(Or.$$.fragment,s),b(mo.$$.fragment,s),b(Br.$$.fragment,s),b(go.$$.fragment,s),b(Wr.$$.fragment,s),b(Qr.$$.fragment,s),b(Ur.$$.fragment,s),b(To.$$.fragment,s),b(Gr.$$.fragment,s),b(Fo.$$.fragment,s),b(Zr.$$.fragment,s),b(Xr.$$.fragment,s),b(Jr.$$.fragment,s),b(vo.$$.fragment,s),b(aa.$$.fragment,s),b(yo.$$.fragment,s),b(ia.$$.fragment,s),b(la.$$.fragment,s),b(da.$$.fragment,s),b(bo.$$.fragment,s),b(ma.$$.fragment,s),b($o.$$.fragment,s),b(ga.$$.fragment,s),b(_a.$$.fragment,s),b(Ta.$$.fragment,s),b(Mo.$$.fragment,s),b(ba.$$.fragment,s),b(zo.$$.fragment,s),b($a.$$.fragment,s),b(Ea.$$.fragment,s),b(Ma.$$.fragment,s),b(Po.$$.fragment,s),b(xa.$$.fragment,s),b(Co.$$.fragment,s),b(La.$$.fragment,s),b(Oa.$$.fragment,s),b(Da.$$.fragment,s),b(xo.$$.fragment,s),b(Wa.$$.fragment,s),b(Lo.$$.fragment,s),b(Qa.$$.fragment,s),fp=!0)},o(s){$(T.$$.fragment,s),$(ne.$$.fragment,s),$(So.$$.fragment,s),$(Bo.$$.fragment,s),$(Qo.$$.fragment,s),$(Uo.$$.fragment,s),$(Ho.$$.fragment,s),$(Yo.$$.fragment,s),$(Go.$$.fragment,s),$(Zo.$$.fragment,s),$(Xo.$$.fragment,s),$(Jo.$$.fragment,s),$(es.$$.fragment,s),$(os.$$.fragment,s),$(ss.$$.fragment,s),$(rs.$$.fragment,s),$(as.$$.fragment,s),$(ls.$$.fragment,s),$(cs.$$.fragment,s),$(ps.$$.fragment,s),$(_s.$$.fragment,s),$(Kt.$$.fragment,s),$(Ts.$$.fragment,s),$(Fs.$$.fragment,s),$(ks.$$.fragment,s),$(Es.$$.fragment,s),$(Zt.$$.fragment,s),$(Ms.$$.fragment,s),$(zs.$$.fragment,s),$(qs.$$.fragment,s),$(Ps.$$.fragment,s),$(Jt.$$.fragment,s),$(Cs.$$.fragment,s),$(js.$$.fragment,s),$(xs.$$.fragment,s),$(Ss.$$.fragment,s),$(no.$$.fragment,s),$(Bs.$$.fragment,s),$(Ws.$$.fragment,s),$(Qs.$$.fragment,s),$(Ks.$$.fragment,s),$(oo.$$.fragment,s),$(Gs.$$.fragment,s),$(Zs.$$.fragment,s),$(Xs.$$.fragment,s),$(Js.$$.fragment,s),$(rr.$$.fragment,s),$(ro.$$.fragment,s),$(ar.$$.fragment,s),$(ir.$$.fragment,s),$(lr.$$.fragment,s),$(fr.$$.fragment,s),$(io.$$.fragment,s),$(mr.$$.fragment,s),$(gr.$$.fragment,s),$(_r.$$.fragment,s),$(wr.$$.fragment,s),$(co.$$.fragment,s),$(br.$$.fragment,s),$($r.$$.fragment,s),$(Er.$$.fragment,s),$(uo.$$.fragment,s),$(jr.$$.fragment,s),$(ho.$$.fragment,s),$(xr.$$.fragment,s),$(Lr.$$.fragment,s),$(Or.$$.fragment,s),$(mo.$$.fragment,s),$(Br.$$.fragment,s),$(go.$$.fragment,s),$(Wr.$$.fragment,s),$(Qr.$$.fragment,s),$(Ur.$$.fragment,s),$(To.$$.fragment,s),$(Gr.$$.fragment,s),$(Fo.$$.fragment,s),$(Zr.$$.fragment,s),$(Xr.$$.fragment,s),$(Jr.$$.fragment,s),$(vo.$$.fragment,s),$(aa.$$.fragment,s),$(yo.$$.fragment,s),$(ia.$$.fragment,s),$(la.$$.fragment,s),$(da.$$.fragment,s),$(bo.$$.fragment,s),$(ma.$$.fragment,s),$($o.$$.fragment,s),$(ga.$$.fragment,s),$(_a.$$.fragment,s),$(Ta.$$.fragment,s),$(Mo.$$.fragment,s),$(ba.$$.fragment,s),$(zo.$$.fragment,s),$($a.$$.fragment,s),$(Ea.$$.fragment,s),$(Ma.$$.fragment,s),$(Po.$$.fragment,s),$(xa.$$.fragment,s),$(Co.$$.fragment,s),$(La.$$.fragment,s),$(Oa.$$.fragment,s),$(Da.$$.fragment,s),$(xo.$$.fragment,s),$(Wa.$$.fragment,s),$(Lo.$$.fragment,s),$(Qa.$$.fragment,s),fp=!1},d(s){n(u),s&&n(M),s&&n(m),E(T),s&&n(K),s&&n(q),E(ne),s&&n(ie),s&&n(Y),s&&n(j),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Ec),s&&n(xn),s&&n(Mc),s&&n(Zn),E(So),s&&n(zc),s&&n(Cn),E(Bo),s&&n(qc),s&&n(Jn),E(Qo),s&&n(Pc),s&&n(Pe),E(Uo),E(Ho),E(Yo),E(Go),E(Zo),E(Xo),s&&n(Cc),s&&n(nt),E(Jo),s&&n(jc),s&&n(Ge),E(es),E(os),E(ss),s&&n(xc),s&&n(ot),E(rs),s&&n(Lc),s&&n(st),E(as),s&&n(Oc),s&&n(rt),E(ls),s&&n(Dc),s&&n(at),E(cs),s&&n(Ac),s&&n(We),E(ps),E(_s),E(Kt),E(Ts),s&&n(Nc),s&&n(lt),E(Fs),s&&n(Ic),s&&n(Qe),E(ks),E(Es),E(Zt),E(Ms),s&&n(Sc),s&&n(ct),E(zs),s&&n(Bc),s&&n(pt),E(qs),E(Ps),E(Jt),E(Cs),s&&n(Wc),s&&n(ht),E(js),s&&n(Qc),s&&n(Ue),E(xs),E(Ss),E(no),E(Bs),s&&n(Uc),s&&n(mt),E(Ws),s&&n(Rc),s&&n(Re),E(Qs),E(Ks),E(oo),E(Gs),E(Zs),s&&n(Hc),s&&n(_t),E(Xs),s&&n(Vc),s&&n(He),E(Js),E(rr),E(ro),E(ar),s&&n(Yc),s&&n(Ft),E(ir),s&&n(Kc),s&&n(Ve),E(lr),E(fr),E(io),E(mr),s&&n(Gc),s&&n(vt),E(gr),s&&n(Zc),s&&n(Ye),E(_r),E(wr),E(co),E(br),s&&n(Xc),s&&n(bt),E($r),s&&n(Jc),s&&n(je),E(Er),E(uo),E(jr),E(ho),E(xr),s&&n(ep),s&&n(Et),E(Lr),s&&n(np),s&&n(xe),E(Or),E(mo),E(Br),E(go),E(Wr),s&&n(tp),s&&n(zt),E(Qr),s&&n(op),s&&n(Le),E(Ur),E(To),E(Gr),E(Fo),E(Zr),s&&n(sp),s&&n(Pt),E(Xr),s&&n(rp),s&&n(Oe),E(Jr),E(vo),E(aa),E(yo),E(ia),s&&n(ap),s&&n(jt),E(la),s&&n(ip),s&&n(De),E(da),E(bo),E(ma),E($o),E(ga),s&&n(lp),s&&n(Lt),E(_a),s&&n(dp),s&&n(Ae),E(Ta),E(Mo),E(ba),E(zo),E($a),s&&n(cp),s&&n(Dt),E(Ea),s&&n(pp),s&&n(Ne),E(Ma),E(Po),E(xa),E(Co),E(La),s&&n(up),s&&n(Nt),E(Oa),s&&n(hp),s&&n(Ie),E(Da),E(xo),E(Wa),E(Lo),E(Qa)}}}const X0={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function J0(W,u,M){let{fw:m}=u;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class r$ extends w0{constructor(u){super();b0(this,u,J0,Z0,$0,{fw:0})}}export{r$ as default,X0 as metadata};
