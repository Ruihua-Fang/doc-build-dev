---
local: efficient-training-on-a-single-gpu
sections:
- local: less-memory
  sections:
  - local: fp16-bf16
    title: fp16 / bf16
  - local: gradient-accumulation
    title: Gradient Accumulation
  - local: gradient-checkpointing
    title: Gradient Checkpointing
  - local: optimizer
    title: Optimizer
  - local: deepspeed-zero
    title: Deepspeed ZeRO
  title: Less Memory
- local: faster-speed
  sections:
  - local: gradient-accumulation
    title: Gradient Accumulation
  - local: batch-sizes
    title: Batch sizes
  title: Faster Speed
- local: scalability-strategy
  title: Scalability Strategy
title: Efficient Training on a Single GPU
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import ColabDropdown from "./ColabDropdown.svelte";
import IconCopyLink from "./IconCopyLink.svelte";
export let fw: "pt" | "tf"
</script>
<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
-->

<h1 id="efficient-training-on-a-single-gpu">Efficient Training on a Single GPU</h1>



<h2 id="less-memory">Less Memory</h2>


<h3 id="fp16-bf16">fp16 / bf16</h3>

Enabling mixed precision will make your training both faster and use less memory. For full details see ...

bf16 can only be used with Ampere based NVIDIA gpus (or newer). bf16 makes the training more stable than fp16 since it has almost the same numerical range as fp32.

To activate these in HF Trainer-based examples simply add `--fp16` or `--bf16` to the command line arguments

To activate these in your custom HF Trainer-based program, pass one of:

```
TrainingArguments(fp16=True)
TrainingArguments(bf16=True)
```

To activate these in `accelerate` use: ... (XXX?)

to use these in your own custom training loop, you need to enable `torch.cuda.amp` for more details see ...


<h3 id="gradient-accumulation">Gradient Accumulation</h3>


<h3 id="gradient-checkpointing">Gradient Checkpointing</h3>


<h3 id="optimizer">Optimizer</h3>


<h3 id="deepspeed-zero">Deepspeed ZeRO</h3>




<h2 id="faster-speed">Faster Speed</h2>

<h3 id="gradient-accumulation">Gradient Accumulation</h3>

<h3 id="batch-sizes">Batch sizes</h3>




<h2 id="scalability-strategy">Scalability Strategy</h2>

* Model fits onto a single GPU:

    1. Normal use

* Model doesn't fit onto a single GPU:

    1. ZeRO + Offload CPU and optionally NVMe
    2. as above plus Memory Centric Tiling (see below for details) if the largest layer can't fit into a single GPU

* Largest Layer not fitting into a single GPU:

1. ZeRO - Enable [Memory Centric Tiling](https://deepspeed.readthedocs.io/en/latest/zero3.html#memory-centric-tiling) (MCT). It allows you to run arbitrarily large layers by automatically splitting them and executing them sequentially. MCT reduces the number of parameters that are live on a GPU, but it does not affect the activation memory. As this need is very rare as of this writing a manual override of `torch.nn.Linear` needs to be done by the user.
