import{S as Um,i as Vm,s as Km,e as s,k as l,w as m,t,M as Zm,c as a,d as o,m as d,a as r,x as g,h as n,b as c,F as e,g as w,y as _,q as u,o as h,B as f}from"../../chunks/vendor-4833417e.js";import{T as Rm}from"../../chunks/Tip-fffd6df1.js";import{D as be}from"../../chunks/Docstring-4f315ed9.js";import{C as ye}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as Cs}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function Xm(it){let p,P,j,T,I,E,xe,W,G,B,v;return{c(){p=s("p"),P=t("Apart from "),j=s("code"),T=t("inputs"),I=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=s("code"),xe=t("config.json"),W=t(`) which in turn defaults to the
`),G=s("a"),B=t("PretrainedConfig"),v=t(" of the model."),this.h()},l($){p=a($,"P",{});var b=r(p);P=n(b,"Apart from "),j=a(b,"CODE",{});var Me=r(j);T=n(Me,"inputs"),Me.forEach(o),I=n(b,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=a(b,"CODE",{});var Le=r(E);xe=n(Le,"config.json"),Le.forEach(o),W=n(b,`) which in turn defaults to the
`),G=a(b,"A",{href:!0});var H=r(G);B=n(H,"PretrainedConfig"),H.forEach(o),v=n(b," of the model."),b.forEach(o),this.h()},h(){c(G,"href","/docs/transformers/pr_15988/en/main_classes/configuration#transformers.PretrainedConfig")},m($,b){w($,p,b),e(p,P),e(p,j),e(j,T),e(p,I),e(p,E),e(E,xe),e(p,W),e(p,G),e(G,B),e(p,v)},d($){$&&o(p)}}}function Jm(it){let p,P,j,T,I,E,xe,W,G,B,v;return{c(){p=s("p"),P=t("Apart from "),j=s("code"),T=t("inputs"),I=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=s("code"),xe=t("config.json"),W=t(`) which in turn defaults to the
`),G=s("a"),B=t("PretrainedConfig"),v=t(" of the model."),this.h()},l($){p=a($,"P",{});var b=r(p);P=n(b,"Apart from "),j=a(b,"CODE",{});var Me=r(j);T=n(Me,"inputs"),Me.forEach(o),I=n(b,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=a(b,"CODE",{});var Le=r(E);xe=n(Le,"config.json"),Le.forEach(o),W=n(b,`) which in turn defaults to the
`),G=a(b,"A",{href:!0});var H=r(G);B=n(H,"PretrainedConfig"),H.forEach(o),v=n(b," of the model."),b.forEach(o),this.h()},h(){c(G,"href","/docs/transformers/pr_15988/en/main_classes/configuration#transformers.PretrainedConfig")},m($,b){w($,p,b),e(p,P),e(p,j),e(j,T),e(p,I),e(p,E),e(E,xe),e(p,W),e(p,G),e(G,B),e(p,v)},d($){$&&o(p)}}}function Qm(it){let p,P,j,T,I,E,xe,W,G,B,v,$,b,Me,Le,H,Is,Ws,en,Bs,Hs,tn,Rs,Us,nn,Vs,Ks,on,Zs,Xs,_s,S,Js,On,Qs,Ys,qn,ea,ta,sn,na,oa,an,sa,aa,rn,ra,ia,us,we,Ze,Gn,lt,la,Sn,da,hs,k,dt,ca,ct,pa,ln,ma,ga,_a,pt,ua,dn,ha,fa,ba,F,R,$n,xa,ka,cn,va,ya,Fn,ja,Ma,An,La,wa,Ta,U,zn,Ea,Oa,pn,qa,Ga,Pn,Sa,$a,Dn,Fa,Aa,za,V,Nn,Pa,Da,mn,Na,Ca,Cn,Ia,Wa,In,Ba,Ha,Ra,K,Wn,Ua,Va,gn,Ka,Za,Bn,Xa,Ja,Hn,Qa,Ya,er,Z,Rn,tr,nr,_n,or,sr,Un,ar,rr,Vn,ir,lr,dr,X,Kn,cr,pr,un,mr,gr,Zn,_r,ur,Xn,hr,fr,br,x,mt,xr,Jn,kr,vr,A,J,Qn,yr,jr,hn,Mr,Lr,Yn,wr,Tr,eo,Er,Or,qr,Q,to,Gr,Sr,fn,$r,Fr,no,Ar,zr,oo,Pr,Dr,Nr,Y,so,Cr,Ir,bn,Wr,Br,ao,Hr,Rr,ro,Ur,Vr,Kr,ee,io,Zr,Xr,xn,Jr,Qr,lo,Yr,ei,co,ti,ni,oi,te,po,si,ai,kn,ri,ii,mo,li,di,go,ci,pi,mi,ne,_o,gi,_i,vn,ui,hi,uo,fi,bi,ho,xi,ki,vi,Xe,yi,gt,ji,_t,Mi,Li,wi,fo,Ti,Ei,bo,Oi,qi,ut,Gi,xo,Si,$i,ht,Fi,ko,Ai,zi,ft,Pi,oe,bt,Di,xt,Ni,vo,Ci,Ii,Wi,yo,Bi,Hi,kt,Ri,se,vt,Ui,yt,Vi,jo,Ki,Zi,Xi,Mo,Ji,Qi,jt,Yi,ae,Mt,el,Lt,tl,Lo,nl,ol,sl,wo,al,rl,wt,il,re,Tt,ll,Et,dl,To,cl,pl,ml,Eo,gl,_l,Ot,ul,ie,qt,hl,Gt,fl,Oo,bl,xl,kl,qo,vl,yl,St,jl,le,$t,Ml,Ft,Ll,Go,wl,Tl,El,So,Ol,ql,At,fs,Te,Je,$o,zt,Gl,Fo,Sl,bs,ke,Pt,$l,Dt,Fl,yn,Al,zl,Pl,O,Nt,Dl,Ao,Nl,Cl,Ct,Il,It,Wl,Bl,Hl,ve,Rl,zo,Ul,Vl,Po,Kl,Zl,jn,Xl,Jl,Ql,Wt,Yl,Bt,ed,td,nd,Do,od,sd,Ht,xs,Ee,Qe,No,Rt,ad,Co,rd,ks,z,Ut,id,Vt,ld,Mn,dd,cd,pd,Kt,md,Ln,gd,_d,ud,Oe,de,Io,hd,fd,Wo,bd,xd,Bo,kd,vd,Ho,yd,jd,Md,ce,Ro,Ld,wd,Uo,Td,Ed,Vo,Od,qd,Ko,Gd,Sd,$d,pe,Zo,Fd,Ad,Xo,zd,Pd,Jo,Dd,Nd,Qo,Cd,Id,Wd,q,Zt,Bd,Yo,Hd,Rd,qe,me,es,Ud,Vd,ts,Kd,Zd,ns,Xd,Jd,os,Qd,Yd,ec,ge,ss,tc,nc,as,oc,sc,rs,ac,rc,is,ic,lc,dc,_e,ls,cc,pc,ds,mc,gc,cs,_c,uc,ps,hc,fc,bc,Ye,xc,Xt,kc,Jt,vc,yc,jc,ms,Mc,Lc,Qt,vs;return E=new Cs({}),lt=new Cs({}),dt=new be({props:{name:"class transformers.generation_utils.GenerationMixin",anchor:"transformers.generation_utils.GenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L378"}}),mt=new be({props:{name:"generate",anchor:"transformers.generation_utils.GenerationMixin.generate",parameters:[{name:"inputs",val:": typing.Optional[torch.Tensor] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"typical_p",val:": typing.Optional[float] = None"},{name:"repetition_penalty",val:": typing.Optional[float] = None"},{name:"bad_words_ids",val:": typing.Optional[typing.Iterable[int]] = None"},{name:"force_words_ids",val:": typing.Union[typing.Iterable[int], typing.Iterable[typing.Iterable[int]], NoneType] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"encoder_no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"num_return_sequences",val:": typing.Optional[int] = None"},{name:"max_time",val:": typing.Optional[float] = None"},{name:"max_new_tokens",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"num_beam_groups",val:": typing.Optional[int] = None"},{name:"diversity_penalty",val:": typing.Optional[float] = None"},{name:"prefix_allowed_tokens_fn",val:": typing.Union[typing.Callable[[int, torch.Tensor], typing.List[int]], NoneType] = None"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = []"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = []"},{name:"constraints",val:": typing.Optional[typing.List[transformers.generation_beam_constraints.Constraint]] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"remove_invalid_values",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L820",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.generate.inputs",description:`<strong>inputs</strong> (<code>torch.Tensor</code> of varying shape depending on the modality, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"inputs"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to <code>model.config.max_length</code>) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_new_tokens",description:`<strong>max_new_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
The maximum numbers of tokens to generate, ignore the current number of tokens. Use either
<code>max_new_tokens</code> or <code>max_length</code> but not both, they serve the same purpose.`,name:"max_new_tokens"},{anchor:"transformers.generation_utils.GenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_utils.GenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_utils.GenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_utils.GenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the
model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer
sequences.`,name:"length_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.encoder_no_repeat_ngram_size",description:`<strong>encoder_no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size that occur in the <code>encoder_input_ids</code> cannot occur in the
<code>decoder_input_ids</code>.`,name:"encoder_no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bad_words_ids(List[List[int]],",description:`<strong>bad_words_ids(<code>List[List[int]]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the token ids of the words that
should not appear in the generated text, use <code>tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids</code>.`,name:"bad_words_ids(List[List[int]],"},{anchor:"transformers.generation_utils.GenerationMixin.generate.force_words_ids(List[List[int]]",description:`<strong>force_words_ids(<code>List[List[int]]</code></strong> or <code>List[List[List[int]]]</code>, <em>optional</em>) &#x2014;
List of token ids that must be generated. If given a <code>List[List[int]]</code>, this is treated as a simple
list of words that must be included, the opposite to <code>bad_words_ids</code>. If given <code>List[List[List[int]]]</code>,
this triggers a <a href="https://github.com/huggingface/transformers/issues/14081" rel="nofollow">disjunctive constraint</a>,
where one can allow different forms of each word.`,name:"force_words_ids(List[List[int]]"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_time(float,",description:`<strong>max_time(<code>float</code>,</strong> <em>optional</em>, defaults to None) &#x2014;
The maximum amount of time you allow the computation to run for in seconds. generation will still
finish the current pass after allocated time has been passed.`,name:"max_time(float,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape
as <code>input_ids</code> that masks the pad token. <a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_utils.GenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beam_groups",description:`<strong>num_beam_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of groups to divide <code>num_beams</code> into in order to ensure diversity among different groups of
beams. <a href="https://arxiv.org/pdf/1610.02424.pdf" rel="nofollow">this paper</a> for more details.`,name:"num_beam_groups"},{anchor:"transformers.generation_utils.GenerationMixin.generate.diversity_penalty",description:`<strong>diversity_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
This value is subtracted from a beam&#x2019;s score if it generates a token same as any beam from other group
at a particular time. Note that <code>diversity_penalty</code> is only effective if <code>group beam search</code> is
enabled.
prefix_allowed_tokens_fn &#x2014; (<code>Callable[[int, torch.Tensor], List[int]]</code>, <em>optional</em>):
If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904" rel="nofollow">Autoregressive Entity
Retrieval</a>.`,name:"diversity_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
Custom logits processors that complement the default logits processors built from arguments and a
model&#x2019;s config. If a logit processor is passed that is already created with the arguments or a model&#x2019;s
config an error is thrown. This feature is intended for advanced users.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.generate.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
Custom stopping criteria that complement the default stopping criteria built from arguments and a
model&#x2019;s config. If a stopping criteria is passed that is already created with the arguments or a
model&#x2019;s config an error is thrown. This feature is intended for advanced users.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.generate.constraints",description:`<strong>constraints</strong> (<code>List[Constraint]</code>, <em>optional</em>) &#x2014;
Custom constraints that can be added to the generation to ensure that the output will contain the use
of certain tokens as defined by <code>Constraint</code> objects, in the most sensible way possible.`,name:"constraints"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.`,name:"forced_eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.remove_invalid_values",description:`<strong>remove_invalid_values</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to remove possible <em>nan</em> and <em>inf</em> outputs of the model to prevent the generation method to
crash. Note that using <code>remove_invalid_values</code> can slow down generation.`,name:"remove_invalid_values"},{anchor:"transformers.generation_utils.GenerationMixin.generate.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max<em>length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*.`,name:"synced_gpus"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> (if
<code>return_dict_in_generate=True</code> or when <code>config.return_dict_in_generate=True</code>) or a <code>torch.FloatTensor</code>.</p>
<p>If the model is <em>not</em> an encoder-decoder model (<code>model.config.is_encoder_decoder=False</code>), the possible
<a
  href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a></li>
</ul>
<p>If the model is an encoder-decoder model (<code>model.config.is_encoder_decoder=True</code>), the possible
<a
  href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a></li>
</ul>
`}}),Xe=new Rm({props:{warning:"&lcub;true}",$$slots:{default:[Xm]},$$scope:{ctx:it}}}),ut=new ye({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# generate up to 30 tokens
outputs = model.generate(input_ids, do_sample=False, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n&#x27;</span>]`}}),ht=new ye({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# sample up to 30 tokens
torch.manual_seed(0)
outputs = model.generate(input_ids, do_sample=True, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get rid of discrimination,&quot; said Rep. Mark Pocan (D-Wis.).\\n\\n&quot;Just look at the&#x27;</span>]`}}),ft=new ye({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-de")

sentence = "Paris is one of the densest populated areas in Europe."
input_ids = tokenizer(sentence, return_tensors="pt").input_ids

outputs = model.generate(input_ids)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sentence = <span class="hljs-string">&quot;Paris is one of the densest populated areas in Europe.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Paris ist eines der dichtesten besiedelten Gebiete Europas.&#x27;</span>]`}}),bt=new be({props:{name:"greedy_search",anchor:"transformers.generation_utils.GenerationMixin.greedy_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L1467",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific keyword arguments will be forwarded to the <code>forward</code> function of the model.
If model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>
A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),kt=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    StoppingCriteriaList,
    MaxLengthCriteria,
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "It might be possible to"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),
    ]
)
stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

outputs = model.greedy_search(
    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;It might be possible to&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">10</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.greedy_search(
<span class="hljs-meta">... </span>    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&quot;It might be possible to get a better understanding of the nature of the problem, but it&#x27;s not&quot;</span>]`}}),vt=new be({props:{name:"sample",anchor:"transformers.generation_utils.GenerationMixin.sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L1699",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> or
A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),jt=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    StoppingCriteriaList,
    MaxLengthCriteria,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
    ]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

torch.manual_seed(0)
outputs = model.sample(
    input_ids,
    logits_processor=logits_processor,
    logits_warper=logits_warper,
    stopping_criteria=stopping_criteria,
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;Today is a beautiful day, and&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">15</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.sample(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    logits_processor=logits_processor,
<span class="hljs-meta">... </span>    logits_warper=logits_warper,
<span class="hljs-meta">... </span>    stopping_criteria=stopping_criteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the&#x27;</span>]`}}),Mt=new be({props:{name:"beam_search",anchor:"transformers.generation_utils.GenerationMixin.beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L1955",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),wt=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Tt=new be({props:{name:"beam_sample",anchor:"transformers.generation_utils.GenerationMixin.beam_sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L2267",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> or
A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),Ot=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

outputs = model.beam_sample(
    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id)]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_sample(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),qt=new be({props:{name:"group_beam_search",anchor:"transformers.generation_utils.GenerationMixin.group_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L2589",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
<p>model_kwargs &#x2014;
Additional model specific kwargs that will be forwarded to the <code>forward</code> function of the model. If
model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if
<code>model.config.is_encoder_decoder=False</code> and <code>return_dict_in_generate=True</code> or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if <code>model.config.is_encoder_decoder=True</code>.</p>
`}}),St=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    HammingDiversityLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
    num_beam_groups=3,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.group_beam_search(
    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    HammingDiversityLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run diverse beam search using 6 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>    num_beam_groups=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        HammingDiversityLogitsProcessor(<span class="hljs-number">5.5</span>, num_beams=<span class="hljs-number">6</span>, num_beam_groups=<span class="hljs-number">3</span>),
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.group_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),$t=new be({props:{name:"constrained_beam_search",anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"constrained_beam_scorer",val:": ConstrainedBeamSearchScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_utils.py#L2954",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.constrained_beam_scorer",description:`<strong>constrained_beam_scorer</strong> (<code>ConstrainedBeamSearchScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation, while satisfying a list of positive constraints. For more information, the
documentation of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer">ConstrainedBeamSearchScorer</a> should be read.`,name:"constrained_beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_15988/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),At=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    ConstrainedBeamSearchScorer,
    PhrasalConstraint,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

constraint_str = "sind"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.constrained_beam_search(
    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    ConstrainedBeamSearchScorer,
<span class="hljs-meta">... </span>    PhrasalConstraint,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_str = <span class="hljs-string">&quot;sind&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_token_ids = tokenizer.encode(constraint_str)[:-<span class="hljs-number">1</span>]  <span class="hljs-comment"># slice to remove eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = ConstrainedBeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>, num_beams=num_beams, device=model.device, constraints=constraints
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.constrained_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt sind Sie?&#x27;</span>]`}}),zt=new Cs({}),Pt=new be({props:{name:"class transformers.generation_tf_utils.TFGenerationMixin",anchor:"transformers.generation_tf_utils.TFGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_tf_utils.py#L342"}}),Nt=new be({props:{name:"generate",anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate",parameters:[{name:"input_ids",val:" = None"},{name:"max_length",val:" = None"},{name:"min_length",val:" = None"},{name:"do_sample",val:" = None"},{name:"early_stopping",val:" = None"},{name:"num_beams",val:" = None"},{name:"temperature",val:" = None"},{name:"top_k",val:" = None"},{name:"top_p",val:" = None"},{name:"repetition_penalty",val:" = None"},{name:"bad_words_ids",val:" = None"},{name:"bos_token_id",val:" = None"},{name:"pad_token_id",val:" = None"},{name:"eos_token_id",val:" = None"},{name:"length_penalty",val:" = None"},{name:"no_repeat_ngram_size",val:" = None"},{name:"num_return_sequences",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_start_token_id",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_scores",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict_in_generate",val:" = None"},{name:"forced_bos_token_id",val:" = None"},{name:"forced_eos_token_id",val:" = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_tf_utils.py#L362",parametersDescription:[{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.input_ids",description:"<strong>input_ids</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, `(batch_size, sequence_length, &#x2014;",name:"input_ids"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.feature_dim)`",description:`<strong>feature_dim)\`</strong> or <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"feature_dim)`"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.`,name:"length_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bad_words_ids(List[int],",description:`<strong>bad_words_ids(<code>List[int]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code>tokenizer.encode(bad_word, add_prefix_space=True)</code>.`,name:"bad_words_ids(List[int],"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>tf.Tensor</code> of <code>dtype=tf.int32</code> and shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code>input_ids</code> that masks the pad token.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.
model_specific_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model.`,name:"forced_eos_token_id"}]}}),Ht=new ye({props:{code:`tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
outputs = model.generate(max_length=40)  # do greedy decoding
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("openai-gpt")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "openai-gpt"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5
)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True
)  # generate 3 candidates using sampling
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("ctrl")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "ctrl"
)  # Download model and configuration from huggingface.co and cache.
input_context = "Legal My neighbor is"  # "Legal" is one of the control codes for ctrl
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2
)  # generate sequences
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "gpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "My cute dog"
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ["idiot", "stupid", "shut up"]
]
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids
)  # generate sequences without allowing bad_words to be generated`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
outputs = model.generate(max_length=<span class="hljs-number">40</span>)  <span class="hljs-comment"># do greedy decoding</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-gpt&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;openai-gpt&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>, temperature=<span class="hljs-number">1.5</span>
)  <span class="hljs-comment"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#x27;The dog&#x27;</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">40</span>, temperature=<span class="hljs-number">0.7</span>, num_return_sequences=<span class="hljs-number">3</span>, do_sample=<span class="hljs-literal">True</span>
)  <span class="hljs-comment"># generate 3 candidates using sampling</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ctrl&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;ctrl&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;Legal My neighbor is&quot;</span>  <span class="hljs-comment"># &quot;Legal&quot; is one of the control codes for ctrl</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">0.7</span>, repetition_penalty=<span class="hljs-number">1.2</span>
)  <span class="hljs-comment"># generate sequences</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;My cute dog&quot;</span>
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=<span class="hljs-literal">True</span>) <span class="hljs-keyword">for</span> bad_word <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idiot&quot;</span>, <span class="hljs-string">&quot;stupid&quot;</span>, <span class="hljs-string">&quot;shut up&quot;</span>]
]
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>, bad_words_ids=bad_words_ids
)  <span class="hljs-comment"># generate sequences without allowing bad_words to be generated</span>`}}),Rt=new Cs({}),Ut=new be({props:{name:"class transformers.generation_flax_utils.FlaxGenerationMixin",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_flax_utils.py#L119"}}),Zt=new be({props:{name:"generate",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate",parameters:[{name:"input_ids",val:": ndarray"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"prng_key",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"trace",val:": bool = True"},{name:"params",val:": typing.Union[typing.Dict[str, jax._src.numpy.lax_numpy.ndarray], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15988/src/transformers/generation_flax_utils.py#L163",parametersDescription:[{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.trace",description:`<strong>trace</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trace generation. Setting <code>trace=False</code> should only be used for debugging and will lead to a
considerably slower runtime.`,name:"trace"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.params",description:`<strong>params</strong> (<code>Dict[str, jnp.ndarray]</code>, <em>optional</em>) &#x2014;
Optionally the model parameters can be passed. Can be useful for parallelized generation.
model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*. Also accepts <code>encoder_outputs</code> to skip encoder part.`,name:"params"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_15988/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a>.</p>
`}}),Ye=new Rm({props:{warning:"&lcub;true}",$$slots:{default:[Jm]},$$scope:{ctx:it}}}),Qt=new ye({props:{code:`from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = FlaxAutoModelForCausalLM.from_pretrained("distilgpt2")
input_context = "The dog"
# encode input context
input_ids = tokenizer(input_context, return_tensors="np").input_ids
# generate candidates using sampling
outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_context = <span class="hljs-string">&quot;The dog&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># encode input context</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_context, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate candidates using sampling</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids=input_ids, max_length=<span class="hljs-number">20</span>, top_k=<span class="hljs-number">30</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),{c(){p=s("meta"),P=l(),j=s("h1"),T=s("a"),I=s("span"),m(E.$$.fragment),xe=l(),W=s("span"),G=t("Generation"),B=l(),v=s("p"),$=t("The methods for auto-regressive text generation, namely "),b=s("a"),Me=t("generate()"),Le=t(" (for the PyTorch models), "),H=s("a"),Is=t("generate()"),Ws=t(" (for the TensorFlow models) and "),en=s("a"),Bs=t("generate()"),Hs=t(" (for the Flax/JAX models), are implemented in "),tn=s("a"),Rs=t("GenerationMixin"),Us=t(", "),nn=s("a"),Vs=t("TFGenerationMixin"),Ks=t(" and "),on=s("a"),Zs=t("FlaxGenerationMixin"),Xs=t(" respectively."),_s=l(),S=s("p"),Js=t("The "),On=s("code"),Qs=t("GenerationMixin"),Ys=t(" classes are inherited by the corresponding base model classes, "),qn=s("em"),ea=t("e.g."),ta=l(),sn=s("a"),na=t("PreTrainedModel"),oa=t(", "),an=s("a"),sa=t("TFPreTrainedModel"),aa=t(", and "),rn=s("a"),ra=t("FlaxPreTrainedModel"),ia=t(` respectively, therefore exposing all
methods for auto-regressive text generation to every model class.`),us=l(),we=s("h2"),Ze=s("a"),Gn=s("span"),m(lt.$$.fragment),la=l(),Sn=s("span"),da=t("GenerationMixn"),hs=l(),k=s("div"),m(dt.$$.fragment),ca=l(),ct=s("p"),pa=t("A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=s("a"),ma=t("PreTrainedModel"),ga=t("."),_a=l(),pt=s("p"),ua=t("The class exposes "),dn=s("a"),ha=t("generate()"),fa=t(", which can be used for:"),ba=l(),F=s("ul"),R=s("li"),$n=s("em"),xa=t("greedy decoding"),ka=t(" by calling "),cn=s("a"),va=t("greedy_search()"),ya=t(" if "),Fn=s("code"),ja=t("num_beams=1"),Ma=t(` and
`),An=s("code"),La=t("do_sample=False"),wa=t("."),Ta=l(),U=s("li"),zn=s("em"),Ea=t("multinomial sampling"),Oa=t(" by calling "),pn=s("a"),qa=t("sample()"),Ga=t(" if "),Pn=s("code"),Sa=t("num_beams=1"),$a=t(` and
`),Dn=s("code"),Fa=t("do_sample=True"),Aa=t("."),za=l(),V=s("li"),Nn=s("em"),Pa=t("beam-search decoding"),Da=t(" by calling "),mn=s("a"),Na=t("beam_search()"),Ca=t(" if "),Cn=s("code"),Ia=t("num_beams>1"),Wa=t(` and
`),In=s("code"),Ba=t("do_sample=False"),Ha=t("."),Ra=l(),K=s("li"),Wn=s("em"),Ua=t("beam-search multinomial sampling"),Va=t(" by calling "),gn=s("a"),Ka=t("beam_sample()"),Za=t(` if
`),Bn=s("code"),Xa=t("num_beams>1"),Ja=t(" and "),Hn=s("code"),Qa=t("do_sample=True"),Ya=t("."),er=l(),Z=s("li"),Rn=s("em"),tr=t("diverse beam-search decoding"),nr=t(" by calling "),_n=s("a"),or=t("group_beam_search()"),sr=t(`, if
`),Un=s("code"),ar=t("num_beams>1"),rr=t(" and "),Vn=s("code"),ir=t("num_beam_groups>1"),lr=t("."),dr=l(),X=s("li"),Kn=s("em"),cr=t("constrained beam-search decoding"),pr=t(" by calling "),un=s("a"),mr=t("constrained_beam_search()"),gr=t(`,
if `),Zn=s("code"),_r=t("constraints!=None"),ur=t(" or "),Xn=s("code"),hr=t("force_words_ids!=None"),fr=t("."),br=l(),x=s("div"),m(mt.$$.fragment),xr=l(),Jn=s("p"),kr=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),vr=l(),A=s("ul"),J=s("li"),Qn=s("em"),yr=t("greedy decoding"),jr=t(" by calling "),hn=s("a"),Mr=t("greedy_search()"),Lr=t(" if "),Yn=s("code"),wr=t("num_beams=1"),Tr=t(` and
`),eo=s("code"),Er=t("do_sample=False"),Or=t("."),qr=l(),Q=s("li"),to=s("em"),Gr=t("multinomial sampling"),Sr=t(" by calling "),fn=s("a"),$r=t("sample()"),Fr=t(" if "),no=s("code"),Ar=t("num_beams=1"),zr=t(` and
`),oo=s("code"),Pr=t("do_sample=True"),Dr=t("."),Nr=l(),Y=s("li"),so=s("em"),Cr=t("beam-search decoding"),Ir=t(" by calling "),bn=s("a"),Wr=t("beam_search()"),Br=t(" if "),ao=s("code"),Hr=t("num_beams>1"),Rr=t(` and
`),ro=s("code"),Ur=t("do_sample=False"),Vr=t("."),Kr=l(),ee=s("li"),io=s("em"),Zr=t("beam-search multinomial sampling"),Xr=t(" by calling "),xn=s("a"),Jr=t("beam_sample()"),Qr=t(` if
`),lo=s("code"),Yr=t("num_beams>1"),ei=t(" and "),co=s("code"),ti=t("do_sample=True"),ni=t("."),oi=l(),te=s("li"),po=s("em"),si=t("diverse beam-search decoding"),ai=t(" by calling "),kn=s("a"),ri=t("group_beam_search()"),ii=t(`, if
`),mo=s("code"),li=t("num_beams>1"),di=t(" and "),go=s("code"),ci=t("num_beam_groups>1"),pi=t("."),mi=l(),ne=s("li"),_o=s("em"),gi=t("constrained beam-search decoding"),_i=t(` by calling
`),vn=s("a"),ui=t("constrained_beam_search()"),hi=t(", if "),uo=s("code"),fi=t("constraints!=None"),bi=t(` or
`),ho=s("code"),xi=t("force_words_ids!=None"),ki=t("."),vi=l(),m(Xe.$$.fragment),yi=l(),gt=s("p"),ji=t("Most of these parameters are explained in more detail in "),_t=s("a"),Mi=t(`this blog
post`),Li=t("."),wi=l(),fo=s("p"),Ti=t("Examples:"),Ei=l(),bo=s("p"),Oi=t("Greedy Decoding:"),qi=l(),m(ut.$$.fragment),Gi=l(),xo=s("p"),Si=t("Multinomial Sampling:"),$i=l(),m(ht.$$.fragment),Fi=l(),ko=s("p"),Ai=t("Beam-search decoding:"),zi=l(),m(ft.$$.fragment),Pi=l(),oe=s("div"),m(bt.$$.fragment),Di=l(),xt=s("p"),Ni=t("Generates sequences of token ids for models with a language modeling head using "),vo=s("strong"),Ci=t("greedy decoding"),Ii=t(` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Wi=l(),yo=s("p"),Bi=t("Examples:"),Hi=l(),m(kt.$$.fragment),Ri=l(),se=s("div"),m(vt.$$.fragment),Ui=l(),yt=s("p"),Vi=t("Generates sequences of token ids for models with a language modeling head using "),jo=s("strong"),Ki=t("multinomial sampling"),Zi=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Xi=l(),Mo=s("p"),Ji=t("Examples:"),Qi=l(),m(jt.$$.fragment),Yi=l(),ae=s("div"),m(Mt.$$.fragment),el=l(),Lt=s("p"),tl=t("Generates sequences of token ids for models with a language modeling head using "),Lo=s("strong"),nl=t("beam search decoding"),ol=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),sl=l(),wo=s("p"),al=t("Examples:"),rl=l(),m(wt.$$.fragment),il=l(),re=s("div"),m(Tt.$$.fragment),ll=l(),Et=s("p"),dl=t("Generates sequences of token ids for models with a language modeling head using "),To=s("strong"),cl=t(`beam search multinomial
sampling`),pl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),ml=l(),Eo=s("p"),gl=t("Examples:"),_l=l(),m(Ot.$$.fragment),ul=l(),ie=s("div"),m(qt.$$.fragment),hl=l(),Gt=s("p"),fl=t("Generates sequences of token ids for models with a language modeling head using "),Oo=s("strong"),bl=t(`diverse beam search
decoding`),xl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),kl=l(),qo=s("p"),vl=t("Examples:"),yl=l(),m(St.$$.fragment),jl=l(),le=s("div"),m($t.$$.fragment),Ml=l(),Ft=s("p"),Ll=t("Generates sequences of token ids for models with a language modeling head using "),Go=s("strong"),wl=t(`constrained beam search
decoding`),Tl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),El=l(),So=s("p"),Ol=t("Examples:"),ql=l(),m(At.$$.fragment),fs=l(),Te=s("h2"),Je=s("a"),$o=s("span"),m(zt.$$.fragment),Gl=l(),Fo=s("span"),Sl=t("TFGenerationMixn"),bs=l(),ke=s("div"),m(Pt.$$.fragment),$l=l(),Dt=s("p"),Fl=t("A class containing all of the functions supporting generation, to be used as a mixin in "),yn=s("a"),Al=t("TFPreTrainedModel"),zl=t("."),Pl=l(),O=s("div"),m(Nt.$$.fragment),Dl=l(),Ao=s("p"),Nl=t(`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),Cl=l(),Ct=s("p"),Il=t("Adapted in part from "),It=s("a"),Wl=t(`Facebook\u2019s XLM beam search
code`),Bl=t("."),Hl=l(),ve=s("p"),Rl=t("Apart from "),zo=s("code"),Ul=t("input_ids"),Vl=t(" and "),Po=s("code"),Kl=t("attention_mask"),Zl=t(`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=s("a"),Xl=t("PretrainedConfig"),Jl=t(` of the model. The default values indicated are the default
values of those config.`),Ql=l(),Wt=s("p"),Yl=t("Most of these parameters are explained in more detail in "),Bt=s("a"),ed=t(`this blog
post`),td=t("."),nd=l(),Do=s("p"),od=t("Examples:"),sd=l(),m(Ht.$$.fragment),xs=l(),Ee=s("h2"),Qe=s("a"),No=s("span"),m(Rt.$$.fragment),ad=l(),Co=s("span"),rd=t("FlaxGenerationMixn"),ks=l(),z=s("div"),m(Ut.$$.fragment),id=l(),Vt=s("p"),ld=t(`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Mn=s("a"),dd=t("FlaxPreTrainedModel"),cd=t("."),pd=l(),Kt=s("p"),md=t("The class exposes "),Ln=s("a"),gd=t("generate()"),_d=t(", which can be used for:"),ud=l(),Oe=s("ul"),de=s("li"),Io=s("em"),hd=t("greedy decoding"),fd=t(" by calling "),Wo=s("code"),bd=t("_greedy_search()"),xd=t(`if
`),Bo=s("code"),kd=t("num_beams=1"),vd=t(" and "),Ho=s("code"),yd=t("do_sample=False"),jd=t("."),Md=l(),ce=s("li"),Ro=s("em"),Ld=t("multinomial sampling"),wd=t(" by calling "),Uo=s("code"),Td=t("_sample()"),Ed=t("if "),Vo=s("code"),Od=t("num_beams=1"),qd=t(`
and `),Ko=s("code"),Gd=t("do_sample=True"),Sd=t("."),$d=l(),pe=s("li"),Zo=s("em"),Fd=t("beam-search decoding"),Ad=t(" by calling "),Xo=s("code"),zd=t("_beam_search"),Pd=t(" if "),Jo=s("code"),Dd=t("num_beams>1"),Nd=t(`
and `),Qo=s("code"),Cd=t("do_sample=False"),Id=t("."),Wd=l(),q=s("div"),m(Zt.$$.fragment),Bd=l(),Yo=s("p"),Hd=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Rd=l(),qe=s("ul"),me=s("li"),es=s("em"),Ud=t("greedy decoding"),Vd=t(" by calling "),ts=s("code"),Kd=t("_greedy_search()"),Zd=t(`if
`),ns=s("code"),Xd=t("num_beams=1"),Jd=t(" and "),os=s("code"),Qd=t("do_sample=False"),Yd=t("."),ec=l(),ge=s("li"),ss=s("em"),tc=t("multinomial sampling"),nc=t(" by calling "),as=s("code"),oc=t("_sample()"),sc=t("if "),rs=s("code"),ac=t("num_beams=1"),rc=t(`
and `),is=s("code"),ic=t("do_sample=True"),lc=t("."),dc=l(),_e=s("li"),ls=s("em"),cc=t("beam-search decoding"),pc=t(" by calling "),ds=s("code"),mc=t("_beam_search"),gc=t(" if "),cs=s("code"),_c=t("num_beams>1"),uc=t(`
and `),ps=s("code"),hc=t("do_sample=False"),fc=t("."),bc=l(),m(Ye.$$.fragment),xc=l(),Xt=s("p"),kc=t("Most of these parameters are explained in more detail in "),Jt=s("a"),vc=t(`this blog
post`),yc=t("."),jc=l(),ms=s("p"),Mc=t("Examples:"),Lc=l(),m(Qt.$$.fragment),this.h()},l(i){const y=Zm('[data-svelte="svelte-1phssyn"]',document.head);p=a(y,"META",{name:!0,content:!0}),y.forEach(o),P=d(i),j=a(i,"H1",{class:!0});var Yt=r(j);T=a(Yt,"A",{id:!0,class:!0,href:!0});var gs=r(T);I=a(gs,"SPAN",{});var wc=r(I);g(E.$$.fragment,wc),wc.forEach(o),gs.forEach(o),xe=d(Yt),W=a(Yt,"SPAN",{});var Tc=r(W);G=n(Tc,"Generation"),Tc.forEach(o),Yt.forEach(o),B=d(i),v=a(i,"P",{});var D=r(v);$=n(D,"The methods for auto-regressive text generation, namely "),b=a(D,"A",{href:!0});var Ec=r(b);Me=n(Ec,"generate()"),Ec.forEach(o),Le=n(D," (for the PyTorch models), "),H=a(D,"A",{href:!0});var Oc=r(H);Is=n(Oc,"generate()"),Oc.forEach(o),Ws=n(D," (for the TensorFlow models) and "),en=a(D,"A",{href:!0});var qc=r(en);Bs=n(qc,"generate()"),qc.forEach(o),Hs=n(D," (for the Flax/JAX models), are implemented in "),tn=a(D,"A",{href:!0});var Gc=r(tn);Rs=n(Gc,"GenerationMixin"),Gc.forEach(o),Us=n(D,", "),nn=a(D,"A",{href:!0});var Sc=r(nn);Vs=n(Sc,"TFGenerationMixin"),Sc.forEach(o),Ks=n(D," and "),on=a(D,"A",{href:!0});var $c=r(on);Zs=n($c,"FlaxGenerationMixin"),$c.forEach(o),Xs=n(D," respectively."),D.forEach(o),_s=d(i),S=a(i,"P",{});var ue=r(S);Js=n(ue,"The "),On=a(ue,"CODE",{});var Fc=r(On);Qs=n(Fc,"GenerationMixin"),Fc.forEach(o),Ys=n(ue," classes are inherited by the corresponding base model classes, "),qn=a(ue,"EM",{});var Ac=r(qn);ea=n(Ac,"e.g."),Ac.forEach(o),ta=d(ue),sn=a(ue,"A",{href:!0});var zc=r(sn);na=n(zc,"PreTrainedModel"),zc.forEach(o),oa=n(ue,", "),an=a(ue,"A",{href:!0});var Pc=r(an);sa=n(Pc,"TFPreTrainedModel"),Pc.forEach(o),aa=n(ue,", and "),rn=a(ue,"A",{href:!0});var Dc=r(rn);ra=n(Dc,"FlaxPreTrainedModel"),Dc.forEach(o),ia=n(ue,` respectively, therefore exposing all
methods for auto-regressive text generation to every model class.`),ue.forEach(o),us=d(i),we=a(i,"H2",{class:!0});var ys=r(we);Ze=a(ys,"A",{id:!0,class:!0,href:!0});var Nc=r(Ze);Gn=a(Nc,"SPAN",{});var Cc=r(Gn);g(lt.$$.fragment,Cc),Cc.forEach(o),Nc.forEach(o),la=d(ys),Sn=a(ys,"SPAN",{});var Ic=r(Sn);da=n(Ic,"GenerationMixn"),Ic.forEach(o),ys.forEach(o),hs=d(i),k=a(i,"DIV",{class:!0});var L=r(k);g(dt.$$.fragment,L),ca=d(L),ct=a(L,"P",{});var js=r(ct);pa=n(js,"A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=a(js,"A",{href:!0});var Wc=r(ln);ma=n(Wc,"PreTrainedModel"),Wc.forEach(o),ga=n(js,"."),js.forEach(o),_a=d(L),pt=a(L,"P",{});var Ms=r(pt);ua=n(Ms,"The class exposes "),dn=a(Ms,"A",{href:!0});var Bc=r(dn);ha=n(Bc,"generate()"),Bc.forEach(o),fa=n(Ms,", which can be used for:"),Ms.forEach(o),ba=d(L),F=a(L,"UL",{});var he=r(F);R=a(he,"LI",{});var Ge=r(R);$n=a(Ge,"EM",{});var Hc=r($n);xa=n(Hc,"greedy decoding"),Hc.forEach(o),ka=n(Ge," by calling "),cn=a(Ge,"A",{href:!0});var Rc=r(cn);va=n(Rc,"greedy_search()"),Rc.forEach(o),ya=n(Ge," if "),Fn=a(Ge,"CODE",{});var Uc=r(Fn);ja=n(Uc,"num_beams=1"),Uc.forEach(o),Ma=n(Ge,` and
`),An=a(Ge,"CODE",{});var Vc=r(An);La=n(Vc,"do_sample=False"),Vc.forEach(o),wa=n(Ge,"."),Ge.forEach(o),Ta=d(he),U=a(he,"LI",{});var Se=r(U);zn=a(Se,"EM",{});var Kc=r(zn);Ea=n(Kc,"multinomial sampling"),Kc.forEach(o),Oa=n(Se," by calling "),pn=a(Se,"A",{href:!0});var Zc=r(pn);qa=n(Zc,"sample()"),Zc.forEach(o),Ga=n(Se," if "),Pn=a(Se,"CODE",{});var Xc=r(Pn);Sa=n(Xc,"num_beams=1"),Xc.forEach(o),$a=n(Se,` and
`),Dn=a(Se,"CODE",{});var Jc=r(Dn);Fa=n(Jc,"do_sample=True"),Jc.forEach(o),Aa=n(Se,"."),Se.forEach(o),za=d(he),V=a(he,"LI",{});var $e=r(V);Nn=a($e,"EM",{});var Qc=r(Nn);Pa=n(Qc,"beam-search decoding"),Qc.forEach(o),Da=n($e," by calling "),mn=a($e,"A",{href:!0});var Yc=r(mn);Na=n(Yc,"beam_search()"),Yc.forEach(o),Ca=n($e," if "),Cn=a($e,"CODE",{});var ep=r(Cn);Ia=n(ep,"num_beams>1"),ep.forEach(o),Wa=n($e,` and
`),In=a($e,"CODE",{});var tp=r(In);Ba=n(tp,"do_sample=False"),tp.forEach(o),Ha=n($e,"."),$e.forEach(o),Ra=d(he),K=a(he,"LI",{});var Fe=r(K);Wn=a(Fe,"EM",{});var np=r(Wn);Ua=n(np,"beam-search multinomial sampling"),np.forEach(o),Va=n(Fe," by calling "),gn=a(Fe,"A",{href:!0});var op=r(gn);Ka=n(op,"beam_sample()"),op.forEach(o),Za=n(Fe,` if
`),Bn=a(Fe,"CODE",{});var sp=r(Bn);Xa=n(sp,"num_beams>1"),sp.forEach(o),Ja=n(Fe," and "),Hn=a(Fe,"CODE",{});var ap=r(Hn);Qa=n(ap,"do_sample=True"),ap.forEach(o),Ya=n(Fe,"."),Fe.forEach(o),er=d(he),Z=a(he,"LI",{});var Ae=r(Z);Rn=a(Ae,"EM",{});var rp=r(Rn);tr=n(rp,"diverse beam-search decoding"),rp.forEach(o),nr=n(Ae," by calling "),_n=a(Ae,"A",{href:!0});var ip=r(_n);or=n(ip,"group_beam_search()"),ip.forEach(o),sr=n(Ae,`, if
`),Un=a(Ae,"CODE",{});var lp=r(Un);ar=n(lp,"num_beams>1"),lp.forEach(o),rr=n(Ae," and "),Vn=a(Ae,"CODE",{});var dp=r(Vn);ir=n(dp,"num_beam_groups>1"),dp.forEach(o),lr=n(Ae,"."),Ae.forEach(o),dr=d(he),X=a(he,"LI",{});var ze=r(X);Kn=a(ze,"EM",{});var cp=r(Kn);cr=n(cp,"constrained beam-search decoding"),cp.forEach(o),pr=n(ze," by calling "),un=a(ze,"A",{href:!0});var pp=r(un);mr=n(pp,"constrained_beam_search()"),pp.forEach(o),gr=n(ze,`,
if `),Zn=a(ze,"CODE",{});var mp=r(Zn);_r=n(mp,"constraints!=None"),mp.forEach(o),ur=n(ze," or "),Xn=a(ze,"CODE",{});var gp=r(Xn);hr=n(gp,"force_words_ids!=None"),gp.forEach(o),fr=n(ze,"."),ze.forEach(o),he.forEach(o),br=d(L),x=a(L,"DIV",{class:!0});var M=r(x);g(mt.$$.fragment,M),xr=d(M),Jn=a(M,"P",{});var _p=r(Jn);kr=n(_p,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),_p.forEach(o),vr=d(M),A=a(M,"UL",{});var fe=r(A);J=a(fe,"LI",{});var Pe=r(J);Qn=a(Pe,"EM",{});var up=r(Qn);yr=n(up,"greedy decoding"),up.forEach(o),jr=n(Pe," by calling "),hn=a(Pe,"A",{href:!0});var hp=r(hn);Mr=n(hp,"greedy_search()"),hp.forEach(o),Lr=n(Pe," if "),Yn=a(Pe,"CODE",{});var fp=r(Yn);wr=n(fp,"num_beams=1"),fp.forEach(o),Tr=n(Pe,` and
`),eo=a(Pe,"CODE",{});var bp=r(eo);Er=n(bp,"do_sample=False"),bp.forEach(o),Or=n(Pe,"."),Pe.forEach(o),qr=d(fe),Q=a(fe,"LI",{});var De=r(Q);to=a(De,"EM",{});var xp=r(to);Gr=n(xp,"multinomial sampling"),xp.forEach(o),Sr=n(De," by calling "),fn=a(De,"A",{href:!0});var kp=r(fn);$r=n(kp,"sample()"),kp.forEach(o),Fr=n(De," if "),no=a(De,"CODE",{});var vp=r(no);Ar=n(vp,"num_beams=1"),vp.forEach(o),zr=n(De,` and
`),oo=a(De,"CODE",{});var yp=r(oo);Pr=n(yp,"do_sample=True"),yp.forEach(o),Dr=n(De,"."),De.forEach(o),Nr=d(fe),Y=a(fe,"LI",{});var Ne=r(Y);so=a(Ne,"EM",{});var jp=r(so);Cr=n(jp,"beam-search decoding"),jp.forEach(o),Ir=n(Ne," by calling "),bn=a(Ne,"A",{href:!0});var Mp=r(bn);Wr=n(Mp,"beam_search()"),Mp.forEach(o),Br=n(Ne," if "),ao=a(Ne,"CODE",{});var Lp=r(ao);Hr=n(Lp,"num_beams>1"),Lp.forEach(o),Rr=n(Ne,` and
`),ro=a(Ne,"CODE",{});var wp=r(ro);Ur=n(wp,"do_sample=False"),wp.forEach(o),Vr=n(Ne,"."),Ne.forEach(o),Kr=d(fe),ee=a(fe,"LI",{});var Ce=r(ee);io=a(Ce,"EM",{});var Tp=r(io);Zr=n(Tp,"beam-search multinomial sampling"),Tp.forEach(o),Xr=n(Ce," by calling "),xn=a(Ce,"A",{href:!0});var Ep=r(xn);Jr=n(Ep,"beam_sample()"),Ep.forEach(o),Qr=n(Ce,` if
`),lo=a(Ce,"CODE",{});var Op=r(lo);Yr=n(Op,"num_beams>1"),Op.forEach(o),ei=n(Ce," and "),co=a(Ce,"CODE",{});var qp=r(co);ti=n(qp,"do_sample=True"),qp.forEach(o),ni=n(Ce,"."),Ce.forEach(o),oi=d(fe),te=a(fe,"LI",{});var Ie=r(te);po=a(Ie,"EM",{});var Gp=r(po);si=n(Gp,"diverse beam-search decoding"),Gp.forEach(o),ai=n(Ie," by calling "),kn=a(Ie,"A",{href:!0});var Sp=r(kn);ri=n(Sp,"group_beam_search()"),Sp.forEach(o),ii=n(Ie,`, if
`),mo=a(Ie,"CODE",{});var $p=r(mo);li=n($p,"num_beams>1"),$p.forEach(o),di=n(Ie," and "),go=a(Ie,"CODE",{});var Fp=r(go);ci=n(Fp,"num_beam_groups>1"),Fp.forEach(o),pi=n(Ie,"."),Ie.forEach(o),mi=d(fe),ne=a(fe,"LI",{});var We=r(ne);_o=a(We,"EM",{});var Ap=r(_o);gi=n(Ap,"constrained beam-search decoding"),Ap.forEach(o),_i=n(We,` by calling
`),vn=a(We,"A",{href:!0});var zp=r(vn);ui=n(zp,"constrained_beam_search()"),zp.forEach(o),hi=n(We,", if "),uo=a(We,"CODE",{});var Pp=r(uo);fi=n(Pp,"constraints!=None"),Pp.forEach(o),bi=n(We,` or
`),ho=a(We,"CODE",{});var Dp=r(ho);xi=n(Dp,"force_words_ids!=None"),Dp.forEach(o),ki=n(We,"."),We.forEach(o),fe.forEach(o),vi=d(M),g(Xe.$$.fragment,M),yi=d(M),gt=a(M,"P",{});var Ls=r(gt);ji=n(Ls,"Most of these parameters are explained in more detail in "),_t=a(Ls,"A",{href:!0,rel:!0});var Np=r(_t);Mi=n(Np,`this blog
post`),Np.forEach(o),Li=n(Ls,"."),Ls.forEach(o),wi=d(M),fo=a(M,"P",{});var Cp=r(fo);Ti=n(Cp,"Examples:"),Cp.forEach(o),Ei=d(M),bo=a(M,"P",{});var Ip=r(bo);Oi=n(Ip,"Greedy Decoding:"),Ip.forEach(o),qi=d(M),g(ut.$$.fragment,M),Gi=d(M),xo=a(M,"P",{});var Wp=r(xo);Si=n(Wp,"Multinomial Sampling:"),Wp.forEach(o),$i=d(M),g(ht.$$.fragment,M),Fi=d(M),ko=a(M,"P",{});var Bp=r(ko);Ai=n(Bp,"Beam-search decoding:"),Bp.forEach(o),zi=d(M),g(ft.$$.fragment,M),M.forEach(o),Pi=d(L),oe=a(L,"DIV",{class:!0});var et=r(oe);g(bt.$$.fragment,et),Di=d(et),xt=a(et,"P",{});var ws=r(xt);Ni=n(ws,"Generates sequences of token ids for models with a language modeling head using "),vo=a(ws,"STRONG",{});var Hp=r(vo);Ci=n(Hp,"greedy decoding"),Hp.forEach(o),Ii=n(ws,` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),ws.forEach(o),Wi=d(et),yo=a(et,"P",{});var Rp=r(yo);Bi=n(Rp,"Examples:"),Rp.forEach(o),Hi=d(et),g(kt.$$.fragment,et),et.forEach(o),Ri=d(L),se=a(L,"DIV",{class:!0});var tt=r(se);g(vt.$$.fragment,tt),Ui=d(tt),yt=a(tt,"P",{});var Ts=r(yt);Vi=n(Ts,"Generates sequences of token ids for models with a language modeling head using "),jo=a(Ts,"STRONG",{});var Up=r(jo);Ki=n(Up,"multinomial sampling"),Up.forEach(o),Zi=n(Ts,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Ts.forEach(o),Xi=d(tt),Mo=a(tt,"P",{});var Vp=r(Mo);Ji=n(Vp,"Examples:"),Vp.forEach(o),Qi=d(tt),g(jt.$$.fragment,tt),tt.forEach(o),Yi=d(L),ae=a(L,"DIV",{class:!0});var nt=r(ae);g(Mt.$$.fragment,nt),el=d(nt),Lt=a(nt,"P",{});var Es=r(Lt);tl=n(Es,"Generates sequences of token ids for models with a language modeling head using "),Lo=a(Es,"STRONG",{});var Kp=r(Lo);nl=n(Kp,"beam search decoding"),Kp.forEach(o),ol=n(Es,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Es.forEach(o),sl=d(nt),wo=a(nt,"P",{});var Zp=r(wo);al=n(Zp,"Examples:"),Zp.forEach(o),rl=d(nt),g(wt.$$.fragment,nt),nt.forEach(o),il=d(L),re=a(L,"DIV",{class:!0});var ot=r(re);g(Tt.$$.fragment,ot),ll=d(ot),Et=a(ot,"P",{});var Os=r(Et);dl=n(Os,"Generates sequences of token ids for models with a language modeling head using "),To=a(Os,"STRONG",{});var Xp=r(To);cl=n(Xp,`beam search multinomial
sampling`),Xp.forEach(o),pl=n(Os," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Os.forEach(o),ml=d(ot),Eo=a(ot,"P",{});var Jp=r(Eo);gl=n(Jp,"Examples:"),Jp.forEach(o),_l=d(ot),g(Ot.$$.fragment,ot),ot.forEach(o),ul=d(L),ie=a(L,"DIV",{class:!0});var st=r(ie);g(qt.$$.fragment,st),hl=d(st),Gt=a(st,"P",{});var qs=r(Gt);fl=n(qs,"Generates sequences of token ids for models with a language modeling head using "),Oo=a(qs,"STRONG",{});var Qp=r(Oo);bl=n(Qp,`diverse beam search
decoding`),Qp.forEach(o),xl=n(qs," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),qs.forEach(o),kl=d(st),qo=a(st,"P",{});var Yp=r(qo);vl=n(Yp,"Examples:"),Yp.forEach(o),yl=d(st),g(St.$$.fragment,st),st.forEach(o),jl=d(L),le=a(L,"DIV",{class:!0});var at=r(le);g($t.$$.fragment,at),Ml=d(at),Ft=a(at,"P",{});var Gs=r(Ft);Ll=n(Gs,"Generates sequences of token ids for models with a language modeling head using "),Go=a(Gs,"STRONG",{});var em=r(Go);wl=n(em,`constrained beam search
decoding`),em.forEach(o),Tl=n(Gs," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Gs.forEach(o),El=d(at),So=a(at,"P",{});var tm=r(So);Ol=n(tm,"Examples:"),tm.forEach(o),ql=d(at),g(At.$$.fragment,at),at.forEach(o),L.forEach(o),fs=d(i),Te=a(i,"H2",{class:!0});var Ss=r(Te);Je=a(Ss,"A",{id:!0,class:!0,href:!0});var nm=r(Je);$o=a(nm,"SPAN",{});var om=r($o);g(zt.$$.fragment,om),om.forEach(o),nm.forEach(o),Gl=d(Ss),Fo=a(Ss,"SPAN",{});var sm=r(Fo);Sl=n(sm,"TFGenerationMixn"),sm.forEach(o),Ss.forEach(o),bs=d(i),ke=a(i,"DIV",{class:!0});var wn=r(ke);g(Pt.$$.fragment,wn),$l=d(wn),Dt=a(wn,"P",{});var $s=r(Dt);Fl=n($s,"A class containing all of the functions supporting generation, to be used as a mixin in "),yn=a($s,"A",{href:!0});var am=r(yn);Al=n(am,"TFPreTrainedModel"),am.forEach(o),zl=n($s,"."),$s.forEach(o),Pl=d(wn),O=a(wn,"DIV",{class:!0});var N=r(O);g(Nt.$$.fragment,N),Dl=d(N),Ao=a(N,"P",{});var rm=r(Ao);Nl=n(rm,`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),rm.forEach(o),Cl=d(N),Ct=a(N,"P",{});var Fs=r(Ct);Il=n(Fs,"Adapted in part from "),It=a(Fs,"A",{href:!0,rel:!0});var im=r(It);Wl=n(im,`Facebook\u2019s XLM beam search
code`),im.forEach(o),Bl=n(Fs,"."),Fs.forEach(o),Hl=d(N),ve=a(N,"P",{});var rt=r(ve);Rl=n(rt,"Apart from "),zo=a(rt,"CODE",{});var lm=r(zo);Ul=n(lm,"input_ids"),lm.forEach(o),Vl=n(rt," and "),Po=a(rt,"CODE",{});var dm=r(Po);Kl=n(dm,"attention_mask"),dm.forEach(o),Zl=n(rt,`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=a(rt,"A",{href:!0});var cm=r(jn);Xl=n(cm,"PretrainedConfig"),cm.forEach(o),Jl=n(rt,` of the model. The default values indicated are the default
values of those config.`),rt.forEach(o),Ql=d(N),Wt=a(N,"P",{});var As=r(Wt);Yl=n(As,"Most of these parameters are explained in more detail in "),Bt=a(As,"A",{href:!0,rel:!0});var pm=r(Bt);ed=n(pm,`this blog
post`),pm.forEach(o),td=n(As,"."),As.forEach(o),nd=d(N),Do=a(N,"P",{});var mm=r(Do);od=n(mm,"Examples:"),mm.forEach(o),sd=d(N),g(Ht.$$.fragment,N),N.forEach(o),wn.forEach(o),xs=d(i),Ee=a(i,"H2",{class:!0});var zs=r(Ee);Qe=a(zs,"A",{id:!0,class:!0,href:!0});var gm=r(Qe);No=a(gm,"SPAN",{});var _m=r(No);g(Rt.$$.fragment,_m),_m.forEach(o),gm.forEach(o),ad=d(zs),Co=a(zs,"SPAN",{});var um=r(Co);rd=n(um,"FlaxGenerationMixn"),um.forEach(o),zs.forEach(o),ks=d(i),z=a(i,"DIV",{class:!0});var je=r(z);g(Ut.$$.fragment,je),id=d(je),Vt=a(je,"P",{});var Ps=r(Vt);ld=n(Ps,`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Mn=a(Ps,"A",{href:!0});var hm=r(Mn);dd=n(hm,"FlaxPreTrainedModel"),hm.forEach(o),cd=n(Ps,"."),Ps.forEach(o),pd=d(je),Kt=a(je,"P",{});var Ds=r(Kt);md=n(Ds,"The class exposes "),Ln=a(Ds,"A",{href:!0});var fm=r(Ln);gd=n(fm,"generate()"),fm.forEach(o),_d=n(Ds,", which can be used for:"),Ds.forEach(o),ud=d(je),Oe=a(je,"UL",{});var Tn=r(Oe);de=a(Tn,"LI",{});var Be=r(de);Io=a(Be,"EM",{});var bm=r(Io);hd=n(bm,"greedy decoding"),bm.forEach(o),fd=n(Be," by calling "),Wo=a(Be,"CODE",{});var xm=r(Wo);bd=n(xm,"_greedy_search()"),xm.forEach(o),xd=n(Be,`if
`),Bo=a(Be,"CODE",{});var km=r(Bo);kd=n(km,"num_beams=1"),km.forEach(o),vd=n(Be," and "),Ho=a(Be,"CODE",{});var vm=r(Ho);yd=n(vm,"do_sample=False"),vm.forEach(o),jd=n(Be,"."),Be.forEach(o),Md=d(Tn),ce=a(Tn,"LI",{});var He=r(ce);Ro=a(He,"EM",{});var ym=r(Ro);Ld=n(ym,"multinomial sampling"),ym.forEach(o),wd=n(He," by calling "),Uo=a(He,"CODE",{});var jm=r(Uo);Td=n(jm,"_sample()"),jm.forEach(o),Ed=n(He,"if "),Vo=a(He,"CODE",{});var Mm=r(Vo);Od=n(Mm,"num_beams=1"),Mm.forEach(o),qd=n(He,`
and `),Ko=a(He,"CODE",{});var Lm=r(Ko);Gd=n(Lm,"do_sample=True"),Lm.forEach(o),Sd=n(He,"."),He.forEach(o),$d=d(Tn),pe=a(Tn,"LI",{});var Re=r(pe);Zo=a(Re,"EM",{});var wm=r(Zo);Fd=n(wm,"beam-search decoding"),wm.forEach(o),Ad=n(Re," by calling "),Xo=a(Re,"CODE",{});var Tm=r(Xo);zd=n(Tm,"_beam_search"),Tm.forEach(o),Pd=n(Re," if "),Jo=a(Re,"CODE",{});var Em=r(Jo);Dd=n(Em,"num_beams>1"),Em.forEach(o),Nd=n(Re,`
and `),Qo=a(Re,"CODE",{});var Om=r(Qo);Cd=n(Om,"do_sample=False"),Om.forEach(o),Id=n(Re,"."),Re.forEach(o),Tn.forEach(o),Wd=d(je),q=a(je,"DIV",{class:!0});var C=r(q);g(Zt.$$.fragment,C),Bd=d(C),Yo=a(C,"P",{});var qm=r(Yo);Hd=n(qm,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),qm.forEach(o),Rd=d(C),qe=a(C,"UL",{});var En=r(qe);me=a(En,"LI",{});var Ue=r(me);es=a(Ue,"EM",{});var Gm=r(es);Ud=n(Gm,"greedy decoding"),Gm.forEach(o),Vd=n(Ue," by calling "),ts=a(Ue,"CODE",{});var Sm=r(ts);Kd=n(Sm,"_greedy_search()"),Sm.forEach(o),Zd=n(Ue,`if
`),ns=a(Ue,"CODE",{});var $m=r(ns);Xd=n($m,"num_beams=1"),$m.forEach(o),Jd=n(Ue," and "),os=a(Ue,"CODE",{});var Fm=r(os);Qd=n(Fm,"do_sample=False"),Fm.forEach(o),Yd=n(Ue,"."),Ue.forEach(o),ec=d(En),ge=a(En,"LI",{});var Ve=r(ge);ss=a(Ve,"EM",{});var Am=r(ss);tc=n(Am,"multinomial sampling"),Am.forEach(o),nc=n(Ve," by calling "),as=a(Ve,"CODE",{});var zm=r(as);oc=n(zm,"_sample()"),zm.forEach(o),sc=n(Ve,"if "),rs=a(Ve,"CODE",{});var Pm=r(rs);ac=n(Pm,"num_beams=1"),Pm.forEach(o),rc=n(Ve,`
and `),is=a(Ve,"CODE",{});var Dm=r(is);ic=n(Dm,"do_sample=True"),Dm.forEach(o),lc=n(Ve,"."),Ve.forEach(o),dc=d(En),_e=a(En,"LI",{});var Ke=r(_e);ls=a(Ke,"EM",{});var Nm=r(ls);cc=n(Nm,"beam-search decoding"),Nm.forEach(o),pc=n(Ke," by calling "),ds=a(Ke,"CODE",{});var Cm=r(ds);mc=n(Cm,"_beam_search"),Cm.forEach(o),gc=n(Ke," if "),cs=a(Ke,"CODE",{});var Im=r(cs);_c=n(Im,"num_beams>1"),Im.forEach(o),uc=n(Ke,`
and `),ps=a(Ke,"CODE",{});var Wm=r(ps);hc=n(Wm,"do_sample=False"),Wm.forEach(o),fc=n(Ke,"."),Ke.forEach(o),En.forEach(o),bc=d(C),g(Ye.$$.fragment,C),xc=d(C),Xt=a(C,"P",{});var Ns=r(Xt);kc=n(Ns,"Most of these parameters are explained in more detail in "),Jt=a(Ns,"A",{href:!0,rel:!0});var Bm=r(Jt);vc=n(Bm,`this blog
post`),Bm.forEach(o),yc=n(Ns,"."),Ns.forEach(o),jc=d(C),ms=a(C,"P",{});var Hm=r(ms);Mc=n(Hm,"Examples:"),Hm.forEach(o),Lc=d(C),g(Qt.$$.fragment,C),C.forEach(o),je.forEach(o),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Ym)),c(T,"id","generation"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#generation"),c(j,"class","relative group"),c(b,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(H,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin.generate"),c(en,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(tn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin"),c(nn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin"),c(on,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin"),c(sn,"href","/docs/transformers/pr_15988/en/main_classes/model#transformers.PreTrainedModel"),c(an,"href","/docs/transformers/pr_15988/en/main_classes/model#transformers.TFPreTrainedModel"),c(rn,"href","/docs/transformers/pr_15988/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ze,"id","transformers.generation_utils.GenerationMixin"),c(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ze,"href","#transformers.generation_utils.GenerationMixin"),c(we,"class","relative group"),c(ln,"href","/docs/transformers/pr_15988/en/main_classes/model#transformers.PreTrainedModel"),c(dn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(cn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(pn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(mn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(gn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(_n,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(un,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(hn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(fn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(bn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(xn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(kn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(vn,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(_t,"href","https://huggingface.co/blog/how-to-generate"),c(_t,"rel","nofollow"),c(x,"class","docstring"),c(oe,"class","docstring"),c(se,"class","docstring"),c(ae,"class","docstring"),c(re,"class","docstring"),c(ie,"class","docstring"),c(le,"class","docstring"),c(k,"class","docstring"),c(Je,"id","transformers.generation_tf_utils.TFGenerationMixin"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#transformers.generation_tf_utils.TFGenerationMixin"),c(Te,"class","relative group"),c(yn,"href","/docs/transformers/pr_15988/en/main_classes/model#transformers.TFPreTrainedModel"),c(It,"href","https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529"),c(It,"rel","nofollow"),c(jn,"href","/docs/transformers/pr_15988/en/main_classes/configuration#transformers.PretrainedConfig"),c(Bt,"href","https://huggingface.co/blog/how-to-generate"),c(Bt,"rel","nofollow"),c(O,"class","docstring"),c(ke,"class","docstring"),c(Qe,"id","transformers.generation_flax_utils.FlaxGenerationMixin"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Ee,"class","relative group"),c(Mn,"href","/docs/transformers/pr_15988/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ln,"href","/docs/transformers/pr_15988/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(Jt,"href","https://huggingface.co/blog/how-to-generate"),c(Jt,"rel","nofollow"),c(q,"class","docstring"),c(z,"class","docstring")},m(i,y){e(document.head,p),w(i,P,y),w(i,j,y),e(j,T),e(T,I),_(E,I,null),e(j,xe),e(j,W),e(W,G),w(i,B,y),w(i,v,y),e(v,$),e(v,b),e(b,Me),e(v,Le),e(v,H),e(H,Is),e(v,Ws),e(v,en),e(en,Bs),e(v,Hs),e(v,tn),e(tn,Rs),e(v,Us),e(v,nn),e(nn,Vs),e(v,Ks),e(v,on),e(on,Zs),e(v,Xs),w(i,_s,y),w(i,S,y),e(S,Js),e(S,On),e(On,Qs),e(S,Ys),e(S,qn),e(qn,ea),e(S,ta),e(S,sn),e(sn,na),e(S,oa),e(S,an),e(an,sa),e(S,aa),e(S,rn),e(rn,ra),e(S,ia),w(i,us,y),w(i,we,y),e(we,Ze),e(Ze,Gn),_(lt,Gn,null),e(we,la),e(we,Sn),e(Sn,da),w(i,hs,y),w(i,k,y),_(dt,k,null),e(k,ca),e(k,ct),e(ct,pa),e(ct,ln),e(ln,ma),e(ct,ga),e(k,_a),e(k,pt),e(pt,ua),e(pt,dn),e(dn,ha),e(pt,fa),e(k,ba),e(k,F),e(F,R),e(R,$n),e($n,xa),e(R,ka),e(R,cn),e(cn,va),e(R,ya),e(R,Fn),e(Fn,ja),e(R,Ma),e(R,An),e(An,La),e(R,wa),e(F,Ta),e(F,U),e(U,zn),e(zn,Ea),e(U,Oa),e(U,pn),e(pn,qa),e(U,Ga),e(U,Pn),e(Pn,Sa),e(U,$a),e(U,Dn),e(Dn,Fa),e(U,Aa),e(F,za),e(F,V),e(V,Nn),e(Nn,Pa),e(V,Da),e(V,mn),e(mn,Na),e(V,Ca),e(V,Cn),e(Cn,Ia),e(V,Wa),e(V,In),e(In,Ba),e(V,Ha),e(F,Ra),e(F,K),e(K,Wn),e(Wn,Ua),e(K,Va),e(K,gn),e(gn,Ka),e(K,Za),e(K,Bn),e(Bn,Xa),e(K,Ja),e(K,Hn),e(Hn,Qa),e(K,Ya),e(F,er),e(F,Z),e(Z,Rn),e(Rn,tr),e(Z,nr),e(Z,_n),e(_n,or),e(Z,sr),e(Z,Un),e(Un,ar),e(Z,rr),e(Z,Vn),e(Vn,ir),e(Z,lr),e(F,dr),e(F,X),e(X,Kn),e(Kn,cr),e(X,pr),e(X,un),e(un,mr),e(X,gr),e(X,Zn),e(Zn,_r),e(X,ur),e(X,Xn),e(Xn,hr),e(X,fr),e(k,br),e(k,x),_(mt,x,null),e(x,xr),e(x,Jn),e(Jn,kr),e(x,vr),e(x,A),e(A,J),e(J,Qn),e(Qn,yr),e(J,jr),e(J,hn),e(hn,Mr),e(J,Lr),e(J,Yn),e(Yn,wr),e(J,Tr),e(J,eo),e(eo,Er),e(J,Or),e(A,qr),e(A,Q),e(Q,to),e(to,Gr),e(Q,Sr),e(Q,fn),e(fn,$r),e(Q,Fr),e(Q,no),e(no,Ar),e(Q,zr),e(Q,oo),e(oo,Pr),e(Q,Dr),e(A,Nr),e(A,Y),e(Y,so),e(so,Cr),e(Y,Ir),e(Y,bn),e(bn,Wr),e(Y,Br),e(Y,ao),e(ao,Hr),e(Y,Rr),e(Y,ro),e(ro,Ur),e(Y,Vr),e(A,Kr),e(A,ee),e(ee,io),e(io,Zr),e(ee,Xr),e(ee,xn),e(xn,Jr),e(ee,Qr),e(ee,lo),e(lo,Yr),e(ee,ei),e(ee,co),e(co,ti),e(ee,ni),e(A,oi),e(A,te),e(te,po),e(po,si),e(te,ai),e(te,kn),e(kn,ri),e(te,ii),e(te,mo),e(mo,li),e(te,di),e(te,go),e(go,ci),e(te,pi),e(A,mi),e(A,ne),e(ne,_o),e(_o,gi),e(ne,_i),e(ne,vn),e(vn,ui),e(ne,hi),e(ne,uo),e(uo,fi),e(ne,bi),e(ne,ho),e(ho,xi),e(ne,ki),e(x,vi),_(Xe,x,null),e(x,yi),e(x,gt),e(gt,ji),e(gt,_t),e(_t,Mi),e(gt,Li),e(x,wi),e(x,fo),e(fo,Ti),e(x,Ei),e(x,bo),e(bo,Oi),e(x,qi),_(ut,x,null),e(x,Gi),e(x,xo),e(xo,Si),e(x,$i),_(ht,x,null),e(x,Fi),e(x,ko),e(ko,Ai),e(x,zi),_(ft,x,null),e(k,Pi),e(k,oe),_(bt,oe,null),e(oe,Di),e(oe,xt),e(xt,Ni),e(xt,vo),e(vo,Ci),e(xt,Ii),e(oe,Wi),e(oe,yo),e(yo,Bi),e(oe,Hi),_(kt,oe,null),e(k,Ri),e(k,se),_(vt,se,null),e(se,Ui),e(se,yt),e(yt,Vi),e(yt,jo),e(jo,Ki),e(yt,Zi),e(se,Xi),e(se,Mo),e(Mo,Ji),e(se,Qi),_(jt,se,null),e(k,Yi),e(k,ae),_(Mt,ae,null),e(ae,el),e(ae,Lt),e(Lt,tl),e(Lt,Lo),e(Lo,nl),e(Lt,ol),e(ae,sl),e(ae,wo),e(wo,al),e(ae,rl),_(wt,ae,null),e(k,il),e(k,re),_(Tt,re,null),e(re,ll),e(re,Et),e(Et,dl),e(Et,To),e(To,cl),e(Et,pl),e(re,ml),e(re,Eo),e(Eo,gl),e(re,_l),_(Ot,re,null),e(k,ul),e(k,ie),_(qt,ie,null),e(ie,hl),e(ie,Gt),e(Gt,fl),e(Gt,Oo),e(Oo,bl),e(Gt,xl),e(ie,kl),e(ie,qo),e(qo,vl),e(ie,yl),_(St,ie,null),e(k,jl),e(k,le),_($t,le,null),e(le,Ml),e(le,Ft),e(Ft,Ll),e(Ft,Go),e(Go,wl),e(Ft,Tl),e(le,El),e(le,So),e(So,Ol),e(le,ql),_(At,le,null),w(i,fs,y),w(i,Te,y),e(Te,Je),e(Je,$o),_(zt,$o,null),e(Te,Gl),e(Te,Fo),e(Fo,Sl),w(i,bs,y),w(i,ke,y),_(Pt,ke,null),e(ke,$l),e(ke,Dt),e(Dt,Fl),e(Dt,yn),e(yn,Al),e(Dt,zl),e(ke,Pl),e(ke,O),_(Nt,O,null),e(O,Dl),e(O,Ao),e(Ao,Nl),e(O,Cl),e(O,Ct),e(Ct,Il),e(Ct,It),e(It,Wl),e(Ct,Bl),e(O,Hl),e(O,ve),e(ve,Rl),e(ve,zo),e(zo,Ul),e(ve,Vl),e(ve,Po),e(Po,Kl),e(ve,Zl),e(ve,jn),e(jn,Xl),e(ve,Jl),e(O,Ql),e(O,Wt),e(Wt,Yl),e(Wt,Bt),e(Bt,ed),e(Wt,td),e(O,nd),e(O,Do),e(Do,od),e(O,sd),_(Ht,O,null),w(i,xs,y),w(i,Ee,y),e(Ee,Qe),e(Qe,No),_(Rt,No,null),e(Ee,ad),e(Ee,Co),e(Co,rd),w(i,ks,y),w(i,z,y),_(Ut,z,null),e(z,id),e(z,Vt),e(Vt,ld),e(Vt,Mn),e(Mn,dd),e(Vt,cd),e(z,pd),e(z,Kt),e(Kt,md),e(Kt,Ln),e(Ln,gd),e(Kt,_d),e(z,ud),e(z,Oe),e(Oe,de),e(de,Io),e(Io,hd),e(de,fd),e(de,Wo),e(Wo,bd),e(de,xd),e(de,Bo),e(Bo,kd),e(de,vd),e(de,Ho),e(Ho,yd),e(de,jd),e(Oe,Md),e(Oe,ce),e(ce,Ro),e(Ro,Ld),e(ce,wd),e(ce,Uo),e(Uo,Td),e(ce,Ed),e(ce,Vo),e(Vo,Od),e(ce,qd),e(ce,Ko),e(Ko,Gd),e(ce,Sd),e(Oe,$d),e(Oe,pe),e(pe,Zo),e(Zo,Fd),e(pe,Ad),e(pe,Xo),e(Xo,zd),e(pe,Pd),e(pe,Jo),e(Jo,Dd),e(pe,Nd),e(pe,Qo),e(Qo,Cd),e(pe,Id),e(z,Wd),e(z,q),_(Zt,q,null),e(q,Bd),e(q,Yo),e(Yo,Hd),e(q,Rd),e(q,qe),e(qe,me),e(me,es),e(es,Ud),e(me,Vd),e(me,ts),e(ts,Kd),e(me,Zd),e(me,ns),e(ns,Xd),e(me,Jd),e(me,os),e(os,Qd),e(me,Yd),e(qe,ec),e(qe,ge),e(ge,ss),e(ss,tc),e(ge,nc),e(ge,as),e(as,oc),e(ge,sc),e(ge,rs),e(rs,ac),e(ge,rc),e(ge,is),e(is,ic),e(ge,lc),e(qe,dc),e(qe,_e),e(_e,ls),e(ls,cc),e(_e,pc),e(_e,ds),e(ds,mc),e(_e,gc),e(_e,cs),e(cs,_c),e(_e,uc),e(_e,ps),e(ps,hc),e(_e,fc),e(q,bc),_(Ye,q,null),e(q,xc),e(q,Xt),e(Xt,kc),e(Xt,Jt),e(Jt,vc),e(Xt,yc),e(q,jc),e(q,ms),e(ms,Mc),e(q,Lc),_(Qt,q,null),vs=!0},p(i,[y]){const Yt={};y&2&&(Yt.$$scope={dirty:y,ctx:i}),Xe.$set(Yt);const gs={};y&2&&(gs.$$scope={dirty:y,ctx:i}),Ye.$set(gs)},i(i){vs||(u(E.$$.fragment,i),u(lt.$$.fragment,i),u(dt.$$.fragment,i),u(mt.$$.fragment,i),u(Xe.$$.fragment,i),u(ut.$$.fragment,i),u(ht.$$.fragment,i),u(ft.$$.fragment,i),u(bt.$$.fragment,i),u(kt.$$.fragment,i),u(vt.$$.fragment,i),u(jt.$$.fragment,i),u(Mt.$$.fragment,i),u(wt.$$.fragment,i),u(Tt.$$.fragment,i),u(Ot.$$.fragment,i),u(qt.$$.fragment,i),u(St.$$.fragment,i),u($t.$$.fragment,i),u(At.$$.fragment,i),u(zt.$$.fragment,i),u(Pt.$$.fragment,i),u(Nt.$$.fragment,i),u(Ht.$$.fragment,i),u(Rt.$$.fragment,i),u(Ut.$$.fragment,i),u(Zt.$$.fragment,i),u(Ye.$$.fragment,i),u(Qt.$$.fragment,i),vs=!0)},o(i){h(E.$$.fragment,i),h(lt.$$.fragment,i),h(dt.$$.fragment,i),h(mt.$$.fragment,i),h(Xe.$$.fragment,i),h(ut.$$.fragment,i),h(ht.$$.fragment,i),h(ft.$$.fragment,i),h(bt.$$.fragment,i),h(kt.$$.fragment,i),h(vt.$$.fragment,i),h(jt.$$.fragment,i),h(Mt.$$.fragment,i),h(wt.$$.fragment,i),h(Tt.$$.fragment,i),h(Ot.$$.fragment,i),h(qt.$$.fragment,i),h(St.$$.fragment,i),h($t.$$.fragment,i),h(At.$$.fragment,i),h(zt.$$.fragment,i),h(Pt.$$.fragment,i),h(Nt.$$.fragment,i),h(Ht.$$.fragment,i),h(Rt.$$.fragment,i),h(Ut.$$.fragment,i),h(Zt.$$.fragment,i),h(Ye.$$.fragment,i),h(Qt.$$.fragment,i),vs=!1},d(i){o(p),i&&o(P),i&&o(j),f(E),i&&o(B),i&&o(v),i&&o(_s),i&&o(S),i&&o(us),i&&o(we),f(lt),i&&o(hs),i&&o(k),f(dt),f(mt),f(Xe),f(ut),f(ht),f(ft),f(bt),f(kt),f(vt),f(jt),f(Mt),f(wt),f(Tt),f(Ot),f(qt),f(St),f($t),f(At),i&&o(fs),i&&o(Te),f(zt),i&&o(bs),i&&o(ke),f(Pt),f(Nt),f(Ht),i&&o(xs),i&&o(Ee),f(Rt),i&&o(ks),i&&o(z),f(Ut),f(Zt),f(Ye),f(Qt)}}}const Ym={local:"generation",sections:[{local:"transformers.generation_utils.GenerationMixin",title:"GenerationMixn"},{local:"transformers.generation_tf_utils.TFGenerationMixin",title:"TFGenerationMixn"},{local:"transformers.generation_flax_utils.FlaxGenerationMixin",title:"FlaxGenerationMixn"}],title:"Generation"};function eg(it,p,P){let{fw:j}=p;return it.$$set=T=>{"fw"in T&&P(0,j=T.fw)},[j]}class ig extends Um{constructor(p){super();Vm(this,p,eg,Qm,Km,{fw:0})}}export{ig as default,Ym as metadata};
