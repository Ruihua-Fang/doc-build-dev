import{S as gc,i as uc,s as _c,e as n,k as d,w as u,t as s,M as Ac,c as a,d as o,m as c,a as r,x as _,h as i,b as l,F as e,g as h,y as A,q as v,o as b,B as F,v as vc}from"../../chunks/vendor-6b77c823.js";import{T as mo}from"../../chunks/Tip-39098574.js";import{D as T}from"../../chunks/Docstring-90e3aa51.js";import{C as Ze}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as D}from"../../chunks/IconCopyLink-7a11ce68.js";function bc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function Fc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function Lc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function wc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function kc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function Vc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function yc(x){let f,V,g,w,k;return{c(){f=n("p"),V=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){f=a(L,"P",{});var p=r(f);V=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(p,"CODE",{});var y=r(g);w=i(y,"Module"),y.forEach(o),k=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(o)},m(L,p){h(L,f,p),e(f,V),e(f,g),e(g,w),e(f,k)},d(L){L&&o(f)}}}function Tc(x){let f,V,g,w,k,L,p,y,Qa,Zn,ne,Me,rn,et,Xa,sn,Ya,ea,xe,Za,tt,er,tr,ta,fo,or,oa,ho,nr,na,po,ar,aa,Ce,rr,ot,sr,ir,ra,ae,ze,ln,nt,lr,dn,dr,sa,$,at,cr,Ee,go,mr,fr,uo,hr,pr,gr,re,ur,_o,_r,Ar,Ao,vr,br,Fr,cn,Lr,wr,rt,kr,Pe,st,Vr,it,yr,vo,Tr,$r,ia,se,Ie,mn,lt,Mr,fn,xr,la,C,dt,Cr,ie,zr,bo,Er,Pr,ct,Ir,jr,qr,le,Or,Fo,Nr,Dr,Lo,Wr,Sr,Br,hn,Rr,Hr,mt,da,de,je,pn,ft,Ur,gn,Gr,ca,z,ht,Kr,ce,Jr,wo,Qr,Xr,pt,Yr,Zr,es,me,ts,ko,os,ns,Vo,as,rs,ss,un,is,ls,gt,ma,fe,qe,_n,ut,ds,An,cs,fa,E,_t,ms,he,fs,yo,hs,ps,At,gs,us,_s,pe,As,To,vs,bs,$o,Fs,Ls,ws,vn,ks,Vs,vt,ha,ge,Oe,bn,bt,ys,Fn,Ts,pa,Ft,Lt,ga,ue,Ne,Ln,wt,$s,wn,Ms,ua,N,kt,xs,kn,Cs,zs,Vt,Es,yt,Ps,Is,js,U,Tt,qs,_e,Os,Mo,Ns,Ds,Vn,Ws,Ss,Bs,De,_a,Ae,We,yn,$t,Rs,Tn,Hs,Aa,P,Mt,Us,xt,Gs,Ct,Ks,Js,Qs,I,zt,Xs,ve,Ys,xo,Zs,ei,$n,ti,oi,ni,Se,ai,Mn,ri,si,Et,ii,G,Pt,li,be,di,Co,ci,mi,xn,fi,hi,pi,Be,gi,K,It,ui,Fe,_i,zo,Ai,vi,Cn,bi,Fi,Li,Re,va,Le,He,zn,jt,wi,En,ki,ba,M,qt,Vi,Ot,yi,Pn,Ti,$i,Mi,Nt,xi,Dt,Ci,zi,Ei,Eo,Wt,Pi,Po,St,Ii,Io,Bt,Fa,we,Ue,In,Rt,ji,jn,qi,La,B,Ht,Oi,Ut,Ni,Gt,Di,Wi,Si,j,Kt,Bi,ke,Ri,jo,Hi,Ui,qn,Gi,Ki,Ji,Ge,Qi,On,Xi,Yi,Jt,wa,Ve,Ke,Nn,Qt,Zi,Dn,el,ka,R,Xt,tl,Yt,ol,Zt,nl,al,rl,q,eo,sl,ye,il,qo,ll,dl,Wn,cl,ml,fl,Je,hl,Sn,pl,gl,to,Va,Te,Qe,Bn,oo,ul,Rn,_l,ya,H,no,Al,ao,vl,ro,bl,Fl,Ll,O,so,wl,$e,kl,Oo,Vl,yl,Hn,Tl,$l,Ml,Xe,xl,Un,Cl,zl,io,Ta;return L=new D({}),et=new D({}),nt=new D({}),at=new T({props:{name:"class transformers.FLAVAConfig",anchor:"transformers.FLAVAConfig",parameters:[{name:"text_config_dict",val:" = None"},{name:"image_config_dict",val:" = None"},{name:"multimodal_config_dict",val:" = None"},{name:"hidden_size",val:" = 768"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"projection_dim",val:" = 768"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"initializer_range",val:" = 0.02"},{name:"ce_ignore_index",val:" = -100"},{name:"mim_weight",val:" = 1.0"},{name:"mlm_weight",val:" = 1.0"},{name:"global_contrastive_weight",val:" = 1.0"},{name:"itm_weight",val:" = 1.0"},{name:"mmm_image_weight",val:" = 1.0"},{name:"mmm_text_weight",val:" = 1.0"},{name:"global_backprop_contrastive",val:" = True"},{name:"skip_unmasked_multimodal_encoder",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.FLAVAConfig.image_config_dict",description:`<strong>image_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>.`,name:"image_config_dict"},{anchor:"transformers.FLAVAConfig.multimodal_config_dict",description:`<strong>multimodal_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>.`,name:"multimodal_config_dict"},{anchor:"transformers.FLAVAConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FLAVAConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FLAVAConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FLAVAConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM unimodal loss`,name:"mim_weight"},{anchor:"transformers.FLAVAConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FLAVAConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FLAVAConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FLAVAConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FLAVAConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FLAVAConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FLAVAConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FLAVAConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L434"}}),rt=new Ze({props:{code:`from transformers import FLAVAModel, FLAVAForPretraining, FLAVAConfig

# Initializing a FLAVAConfig with style configuration
configuration = FLAVAConfig()

# Initializing a FLAVAModel and FLAVAForPretraining model from the style configuration
model = FLAVAModel(configuration)
model_pre = FLAVAForPretraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAModel, FLAVAForPretraining, FLAVAConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAModel and FLAVAForPretraining model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FLAVAForPretraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),st=new T({props:{name:"from_configs",anchor:"transformers.FLAVAConfig.from_configs",parameters:[{name:"text_config",val:": FLAVATextConfig"},{name:"image_config",val:": FLAVAImageConfig"},{name:"multimodal_config",val:": FLAVAMultimodalConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L564",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"
>FLAVAConfig</a></p>
`}}),lt=new D({}),dt=new T({props:{name:"class transformers.FLAVATextConfig",anchor:"transformers.FLAVATextConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"type_vocab_size",val:" = 2"},{name:"max_position_embeddings",val:" = 512"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVATextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FLAVATextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FLAVATextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FLAVATextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FLAVATextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVATextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVATextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVATextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVATextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVATextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVATextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVATextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVATextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVATextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVATextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVATextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVATextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L148"}}),mt=new Ze({props:{code:`from transformers import FLAVATextModel, FLAVATextConfig

# Initializing a FLAVATextModel with  style configuration
configuration = FLAVATextConfig()

# Initializing a FLAVATextConfig from the  style configuration
model = FLAVATextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVATextModel, FLAVATextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVATextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextConfig from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),ft=new D({}),ht=new T({props:{name:"class transformers.FLAVAImageConfig",anchor:"transformers.FLAVAImageConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"mask_token",val:" = True"},{name:"vocab_size",val:" = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVAImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVAImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVAImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use a mask token or not. Used in MIM loss.`,name:"mask_token"},{anchor:"transformers.FLAVAImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebook">FLAVACodebook</a> used in conjunction with <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel">FLAVAImageModel</a> for MIM.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L32"}}),gt=new Ze({props:{code:`from transformers import FLAVAImageModel, FLAVAImageConfig

# Initializing a FLAVAImageModel with  style configuration
configuration = FLAVAImageConfig()

# Initializing a FLAVAImageModel model from the  style configuration
model = FLAVAImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAImageModel, FLAVAImageConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),ut=new D({}),_t=new T({props:{name:"class transformers.FLAVAMultimodalConfig",anchor:"transformers.FLAVAMultimodalConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 6"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"qkv_bias",val:" = True"},{name:"use_cls_token",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L275"}}),vt=new Ze({props:{code:`from transformers import FLAVAMultimodalModel, FLAVAMultimodalConfig

# Initializing a FLAVAMultimodalModel with  style configuration
configuration = FLAVAMultimodalConfig()

# Initializing a FLAVAMultimodalModel model from the  style configuration
model = FLAVAMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAMultimodalModel, FLAVAMultimodalConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),bt=new D({}),Lt=new T({props:{name:"class transformers.FLAVACodebookConfig",anchor:"transformers.FLAVACodebookConfig",parameters:[{name:"num_groups",val:" = 4"},{name:"input_channels",val:" = 3"},{name:"num_blocks_per_group",val:" = 2"},{name:"hidden_size",val:" = 256"},{name:"vocab_size",val:" = 8192"},{name:"freeze",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L375"}}),wt=new D({}),kt=new T({props:{name:"class transformers.FLAVAForPretraining",anchor:"transformers.FLAVAForPretraining",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAForPretraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1728"}}),Tt=new T({props:{name:"forward",anchor:"transformers.FLAVAForPretraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAForPretraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1758",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAForPretrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>losses</strong> (<code>FLAVALosses</code>) \u2014 Losses for FLAVA Pretraining. Check <code>FLAVALosses</code> class description for the information on the keys.</p>
</li>
<li>
<p><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
</ul>
<ul>
<li><strong>mim_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches, image_vocab_size)</code>, <em>optional</em>, returned
when <code>pixel_values</code> are present and <code>input_ids_masked</code> are not): The logits for MIM unimodal loss. Uses
<code>book_masked_pos</code> to get masked images.</li>
<li><strong>mlm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when
<code>input_ids_masked</code> are present and <code>pixel_values</code> are not): The logits for MLM unimodal loss.</li>
<li><strong>itm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and
<code>pixel_values</code> are present): The logits for ITM loss. Note that ITM loss is calculated on masked pairs in
FLAVA.</li>
<li><strong>mmm_image_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches, image_vocab_size)</code>, <em>optional</em>, returned
when <code>pixel_values</code> and <code>input_ids_masked</code> are present): The logits for MMM image multimodal loss. Uses
<code>book_masked_pos</code> to get masked images.</li>
<li><strong>mmm_text_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when
<code>pixel_values</code> and <code>input_ids_masked</code> are present): The logits for MMM text multimodal loss. Uses
<code>book_masked_pos</code> to get masked images.</li>
<li><strong>contrastive_logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</li>
<li><strong>contrastive_logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAForPretrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new mo({props:{$$slots:{default:[bc]},$$scope:{ctx:x}}}),$t=new D({}),Mt=new T({props:{name:"class transformers.FLAVAModel",anchor:"transformers.FLAVAModel",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1197"}}),zt=new T({props:{name:"forward",anchor:"transformers.FLAVAModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <code>FLAVAFeatureExtractor</code>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FLAVAModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1339",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new mo({props:{$$slots:{default:[Fc]},$$scope:{ctx:x}}}),Et=new Ze({props:{code:`from PIL import Image
import requests
from transformers import FLAVAProcessor, FLAVAModel

model = FLAVAModel.from_pretrained("aps/flava-full")
processor = FLAVAProcessor.from_pretrained("aps/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAProcessor, FLAVAModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FLAVAProcessor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),Pt=new T({props:{name:"get_text_features",anchor:"transformers.FLAVAModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1241"}}),Be=new mo({props:{$$slots:{default:[Lc]},$$scope:{ctx:x}}}),It=new T({props:{name:"get_image_features",anchor:"transformers.FLAVAModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <code>FLAVAFeatureExtractor</code>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1287"}}),Re=new mo({props:{$$slots:{default:[wc]},$$scope:{ctx:x}}}),jt=new D({}),qt=new T({props:{name:"class transformers.FLAVACodebook",anchor:"transformers.FLAVACodebook",parameters:[{name:"config",val:": FLAVACodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FLAVACodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebookConfig">FLAVACodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1476"}}),Wt=new T({props:{name:"forward",anchor:"transformers.FLAVACodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1599"}}),St=new T({props:{name:"get_codebook_indices",anchor:"transformers.FLAVACodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1566"}}),Bt=new T({props:{name:"get_codebook_probs",anchor:"transformers.FLAVACodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1595"}}),Rt=new D({}),Ht=new T({props:{name:"class transformers.FLAVATextModel",anchor:"transformers.FLAVATextModel",parameters:[{name:"config",val:": FLAVATextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L994"}}),Kt=new T({props:{name:"forward",anchor:"transformers.FLAVATextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVATextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVATextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVATextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVATextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVATextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVATextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1024",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig"
>FLAVATextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new mo({props:{$$slots:{default:[kc]},$$scope:{ctx:x}}}),Jt=new Ze({props:{code:`from transformers import BertTokenizer, FLAVATextModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVATextModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVATextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Qt=new D({}),Xt=new T({props:{name:"class transformers.FLAVAImageModel",anchor:"transformers.FLAVAImageModel",parameters:[{name:"config",val:": FLAVAImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L895"}}),eo=new T({props:{name:"forward",anchor:"transformers.FLAVAImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <code>FLAVAFeatureExtractor</code>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L927",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig"
>FLAVAImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new mo({props:{$$slots:{default:[Vc]},$$scope:{ctx:x}}}),to=new Ze({props:{code:`from transformers import FLAVAFeatureExtractor, FLAVAImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FLAVAFeatureExtractor.from_pretrained("aps/flava-full")
model = FLAVAImageModel.from_pretrained("aps/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAFeatureExtractor, FLAVAImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FLAVAFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),oo=new D({}),no=new T({props:{name:"class transformers.FLAVAMultimodalModel",anchor:"transformers.FLAVAMultimodalModel",parameters:[{name:"config",val:": FLAVAMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1099"}}),so=new T({props:{name:"forward",anchor:"transformers.FLAVAMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1126",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig"
>FLAVAMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xe=new mo({props:{$$slots:{default:[yc]},$$scope:{ctx:x}}}),io=new Ze({props:{code:`from transformers import BertTokenizer, FLAVAMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVAMultimodalModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVAMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=n("meta"),V=d(),g=n("h1"),w=n("a"),k=n("span"),u(L.$$.fragment),p=d(),y=n("span"),Qa=s("FLAVA"),Zn=d(),ne=n("h2"),Me=n("a"),rn=n("span"),u(et.$$.fragment),Xa=d(),sn=n("span"),Ya=s("Overview"),ea=d(),xe=n("p"),Za=s("The FLAVA model was proposed in "),tt=n("a"),er=s("FLAVA: A Foundational Language And Vision Alignment Model"),tr=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),ta=d(),fo=n("p"),or=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),oa=d(),ho=n("p"),nr=s("The abstract from the paper is the following:"),na=d(),po=n("p"),ar=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),aa=d(),Ce=n("p"),rr=s("This model was contributed by "),ot=n("a"),sr=s("aps"),ir=s("."),ra=d(),ae=n("h2"),ze=n("a"),ln=n("span"),u(nt.$$.fragment),lr=d(),dn=n("span"),dr=s("FLAVAConfig"),sa=d(),$=n("div"),u(at.$$.fragment),cr=d(),Ee=n("p"),go=n("a"),mr=s("FLAVAConfig"),fr=s(" is the configuration class to store the configuration of a "),uo=n("a"),hr=s("FLAVAModel"),pr=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),gr=d(),re=n("p"),ur=s("Configuration objects inherit from "),_o=n("a"),_r=s("PretrainedConfig"),Ar=s(` and can be used to control the model outputs. Read the
documentation from `),Ao=n("a"),vr=s("PretrainedConfig"),br=s(" for more information."),Fr=d(),cn=n("p"),Lr=s("Example:"),wr=d(),u(rt.$$.fragment),kr=d(),Pe=n("div"),u(st.$$.fragment),Vr=d(),it=n("p"),yr=s("Instantiate a "),vo=n("a"),Tr=s("FLAVAConfig"),$r=s(` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),ia=d(),se=n("h2"),Ie=n("a"),mn=n("span"),u(lt.$$.fragment),Mr=d(),fn=n("span"),xr=s("FLAVATextConfig"),la=d(),C=n("div"),u(dt.$$.fragment),Cr=d(),ie=n("p"),zr=s("This is the configuration class to store the configuration of a "),bo=n("a"),Er=s("FLAVATextModel"),Pr=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),ct=n("a"),Ir=s("full"),jr=s(" architecture."),qr=d(),le=n("p"),Or=s("Configuration objects inherit from "),Fo=n("a"),Nr=s("PretrainedConfig"),Dr=s(` and can be used to control the model outputs. Read the
documentation from `),Lo=n("a"),Wr=s("PretrainedConfig"),Sr=s(" for more information."),Br=d(),hn=n("p"),Rr=s("Example:"),Hr=d(),u(mt.$$.fragment),da=d(),de=n("h2"),je=n("a"),pn=n("span"),u(ft.$$.fragment),Ur=d(),gn=n("span"),Gr=s("FLAVAImageConfig"),ca=d(),z=n("div"),u(ht.$$.fragment),Kr=d(),ce=n("p"),Jr=s("This is the configuration class to store the configuration of a "),wo=n("a"),Qr=s("FLAVAImageModel"),Xr=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),pt=n("a"),Yr=s("full"),Zr=s(" architecture."),es=d(),me=n("p"),ts=s("Configuration objects inherit from "),ko=n("a"),os=s("PretrainedConfig"),ns=s(` and can be used to control the model outputs. Read the
documentation from `),Vo=n("a"),as=s("PretrainedConfig"),rs=s(" for more information."),ss=d(),un=n("p"),is=s("Example:"),ls=d(),u(gt.$$.fragment),ma=d(),fe=n("h2"),qe=n("a"),_n=n("span"),u(ut.$$.fragment),ds=d(),An=n("span"),cs=s("FLAVAMultimodalConfig"),fa=d(),E=n("div"),u(_t.$$.fragment),ms=d(),he=n("p"),fs=s("This is the configuration class to store the configuration of a "),yo=n("a"),hs=s("FLAVAMultimodalModel"),ps=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),At=n("a"),gs=s("full"),us=s(" architecture."),_s=d(),pe=n("p"),As=s("Configuration objects inherit from "),To=n("a"),vs=s("PretrainedConfig"),bs=s(` and can be used to control the model outputs. Read the
documentation from `),$o=n("a"),Fs=s("PretrainedConfig"),Ls=s(" for more information."),ws=d(),vn=n("p"),ks=s("Example:"),Vs=d(),u(vt.$$.fragment),ha=d(),ge=n("h2"),Oe=n("a"),bn=n("span"),u(bt.$$.fragment),ys=d(),Fn=n("span"),Ts=s("FLAVACodebookConfig"),pa=d(),Ft=n("div"),u(Lt.$$.fragment),ga=d(),ue=n("h2"),Ne=n("a"),Ln=n("span"),u(wt.$$.fragment),$s=d(),wn=n("span"),Ms=s("FLAVAForPretraining"),ua=d(),N=n("div"),u(kt.$$.fragment),xs=d(),kn=n("p"),Cs=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),zs=d(),Vt=n("p"),Es=s("This model is a PyTorch "),yt=n("a"),Ps=s("torch.nn.Module"),Is=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),js=d(),U=n("div"),u(Tt.$$.fragment),qs=d(),_e=n("p"),Os=s("The "),Mo=n("a"),Ns=s("FLAVAForPretraining"),Ds=s(" forward method, overrides the "),Vn=n("code"),Ws=s("__call__"),Ss=s(" special method."),Bs=d(),u(De.$$.fragment),_a=d(),Ae=n("h2"),We=n("a"),yn=n("span"),u($t.$$.fragment),Rs=d(),Tn=n("span"),Hs=s("FLAVAModel"),Aa=d(),P=n("div"),u(Mt.$$.fragment),Us=d(),xt=n("p"),Gs=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ct=n("a"),Ks=s("torch.nn.Module"),Js=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Qs=d(),I=n("div"),u(zt.$$.fragment),Xs=d(),ve=n("p"),Ys=s("The "),xo=n("a"),Zs=s("FLAVAModel"),ei=s(" forward method, overrides the "),$n=n("code"),ti=s("__call__"),oi=s(" special method."),ni=d(),u(Se.$$.fragment),ai=d(),Mn=n("p"),ri=s("Examples:"),si=d(),u(Et.$$.fragment),ii=d(),G=n("div"),u(Pt.$$.fragment),li=d(),be=n("p"),di=s("The "),Co=n("a"),ci=s("FLAVAModel"),mi=s(" forward method, overrides the "),xn=n("code"),fi=s("__call__"),hi=s(" special method."),pi=d(),u(Be.$$.fragment),gi=d(),K=n("div"),u(It.$$.fragment),ui=d(),Fe=n("p"),_i=s("The "),zo=n("a"),Ai=s("FLAVAModel"),vi=s(" forward method, overrides the "),Cn=n("code"),bi=s("__call__"),Fi=s(" special method."),Li=d(),u(Re.$$.fragment),va=d(),Le=n("h2"),He=n("a"),zn=n("span"),u(jt.$$.fragment),wi=d(),En=n("span"),ki=s("FLAVACodebook"),ba=d(),M=n("div"),u(qt.$$.fragment),Vi=d(),Ot=n("p"),yi=s(`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),Pn=n("code"),Ti=s("get_codebook_indices"),$i=s(" to get image tokens for an image."),Mi=d(),Nt=n("p"),xi=s("This model is a PyTorch "),Dt=n("a"),Ci=s("torch.nn.Module"),zi=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ei=d(),Eo=n("div"),u(Wt.$$.fragment),Pi=d(),Po=n("div"),u(St.$$.fragment),Ii=d(),Io=n("div"),u(Bt.$$.fragment),Fa=d(),we=n("h2"),Ue=n("a"),In=n("span"),u(Rt.$$.fragment),ji=d(),jn=n("span"),qi=s("FLAVATextModel"),La=d(),B=n("div"),u(Ht.$$.fragment),Oi=d(),Ut=n("p"),Ni=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Gt=n("a"),Di=s("torch.nn.Module"),Wi=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Si=d(),j=n("div"),u(Kt.$$.fragment),Bi=d(),ke=n("p"),Ri=s("The "),jo=n("a"),Hi=s("FLAVATextModel"),Ui=s(" forward method, overrides the "),qn=n("code"),Gi=s("__call__"),Ki=s(" special method."),Ji=d(),u(Ge.$$.fragment),Qi=d(),On=n("p"),Xi=s("Example:"),Yi=d(),u(Jt.$$.fragment),wa=d(),Ve=n("h2"),Ke=n("a"),Nn=n("span"),u(Qt.$$.fragment),Zi=d(),Dn=n("span"),el=s("FLAVAImageModel"),ka=d(),R=n("div"),u(Xt.$$.fragment),tl=d(),Yt=n("p"),ol=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Zt=n("a"),nl=s("torch.nn.Module"),al=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),rl=d(),q=n("div"),u(eo.$$.fragment),sl=d(),ye=n("p"),il=s("The "),qo=n("a"),ll=s("FLAVAImageModel"),dl=s(" forward method, overrides the "),Wn=n("code"),cl=s("__call__"),ml=s(" special method."),fl=d(),u(Je.$$.fragment),hl=d(),Sn=n("p"),pl=s("Example:"),gl=d(),u(to.$$.fragment),Va=d(),Te=n("h2"),Qe=n("a"),Bn=n("span"),u(oo.$$.fragment),ul=d(),Rn=n("span"),_l=s("FLAVAMultimodalModel"),ya=d(),H=n("div"),u(no.$$.fragment),Al=d(),ao=n("p"),vl=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),ro=n("a"),bl=s("torch.nn.Module"),Fl=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ll=d(),O=n("div"),u(so.$$.fragment),wl=d(),$e=n("p"),kl=s("The "),Oo=n("a"),Vl=s("FLAVAMultimodalModel"),yl=s(" forward method, overrides the "),Hn=n("code"),Tl=s("__call__"),$l=s(" special method."),Ml=d(),u(Xe.$$.fragment),xl=d(),Un=n("p"),Cl=s("Example:"),zl=d(),u(io.$$.fragment),this.h()},l(t){const m=Ac('[data-svelte="svelte-1phssyn"]',document.head);f=a(m,"META",{name:!0,content:!0}),m.forEach(o),V=c(t),g=a(t,"H1",{class:!0});var lo=r(g);w=a(lo,"A",{id:!0,class:!0,href:!0});var Gn=r(w);k=a(Gn,"SPAN",{});var Kn=r(k);_(L.$$.fragment,Kn),Kn.forEach(o),Gn.forEach(o),p=c(lo),y=a(lo,"SPAN",{});var Jn=r(y);Qa=i(Jn,"FLAVA"),Jn.forEach(o),lo.forEach(o),Zn=c(t),ne=a(t,"H2",{class:!0});var co=r(ne);Me=a(co,"A",{id:!0,class:!0,href:!0});var Qn=r(Me);rn=a(Qn,"SPAN",{});var Xn=r(rn);_(et.$$.fragment,Xn),Xn.forEach(o),Qn.forEach(o),Xa=c(co),sn=a(co,"SPAN",{});var El=r(sn);Ya=i(El,"Overview"),El.forEach(o),co.forEach(o),ea=c(t),xe=a(t,"P",{});var $a=r(xe);Za=i($a,"The FLAVA model was proposed in "),tt=a($a,"A",{href:!0,rel:!0});var Pl=r(tt);er=i(Pl,"FLAVA: A Foundational Language And Vision Alignment Model"),Pl.forEach(o),tr=i($a," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),$a.forEach(o),ta=c(t),fo=a(t,"P",{});var Il=r(fo);or=i(Il,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Il.forEach(o),oa=c(t),ho=a(t,"P",{});var jl=r(ho);nr=i(jl,"The abstract from the paper is the following:"),jl.forEach(o),na=c(t),po=a(t,"P",{});var ql=r(po);ar=i(ql,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),ql.forEach(o),aa=c(t),Ce=a(t,"P",{});var Ma=r(Ce);rr=i(Ma,"This model was contributed by "),ot=a(Ma,"A",{href:!0,rel:!0});var Ol=r(ot);sr=i(Ol,"aps"),Ol.forEach(o),ir=i(Ma,"."),Ma.forEach(o),ra=c(t),ae=a(t,"H2",{class:!0});var xa=r(ae);ze=a(xa,"A",{id:!0,class:!0,href:!0});var Nl=r(ze);ln=a(Nl,"SPAN",{});var Dl=r(ln);_(nt.$$.fragment,Dl),Dl.forEach(o),Nl.forEach(o),lr=c(xa),dn=a(xa,"SPAN",{});var Wl=r(dn);dr=i(Wl,"FLAVAConfig"),Wl.forEach(o),xa.forEach(o),sa=c(t),$=a(t,"DIV",{class:!0});var W=r($);_(at.$$.fragment,W),cr=c(W),Ee=a(W,"P",{});var Yn=r(Ee);go=a(Yn,"A",{href:!0});var Sl=r(go);mr=i(Sl,"FLAVAConfig"),Sl.forEach(o),fr=i(Yn," is the configuration class to store the configuration of a "),uo=a(Yn,"A",{href:!0});var Bl=r(uo);hr=i(Bl,"FLAVAModel"),Bl.forEach(o),pr=i(Yn,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),Yn.forEach(o),gr=c(W),re=a(W,"P",{});var No=r(re);ur=i(No,"Configuration objects inherit from "),_o=a(No,"A",{href:!0});var Rl=r(_o);_r=i(Rl,"PretrainedConfig"),Rl.forEach(o),Ar=i(No,` and can be used to control the model outputs. Read the
documentation from `),Ao=a(No,"A",{href:!0});var Hl=r(Ao);vr=i(Hl,"PretrainedConfig"),Hl.forEach(o),br=i(No," for more information."),No.forEach(o),Fr=c(W),cn=a(W,"P",{});var Ul=r(cn);Lr=i(Ul,"Example:"),Ul.forEach(o),wr=c(W),_(rt.$$.fragment,W),kr=c(W),Pe=a(W,"DIV",{class:!0});var Ca=r(Pe);_(st.$$.fragment,Ca),Vr=c(Ca),it=a(Ca,"P",{});var za=r(it);yr=i(za,"Instantiate a "),vo=a(za,"A",{href:!0});var Gl=r(vo);Tr=i(Gl,"FLAVAConfig"),Gl.forEach(o),$r=i(za,` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),za.forEach(o),Ca.forEach(o),W.forEach(o),ia=c(t),se=a(t,"H2",{class:!0});var Ea=r(se);Ie=a(Ea,"A",{id:!0,class:!0,href:!0});var Kl=r(Ie);mn=a(Kl,"SPAN",{});var Jl=r(mn);_(lt.$$.fragment,Jl),Jl.forEach(o),Kl.forEach(o),Mr=c(Ea),fn=a(Ea,"SPAN",{});var Ql=r(fn);xr=i(Ql,"FLAVATextConfig"),Ql.forEach(o),Ea.forEach(o),la=c(t),C=a(t,"DIV",{class:!0});var J=r(C);_(dt.$$.fragment,J),Cr=c(J),ie=a(J,"P",{});var Do=r(ie);zr=i(Do,"This is the configuration class to store the configuration of a "),bo=a(Do,"A",{href:!0});var Xl=r(bo);Er=i(Xl,"FLAVATextModel"),Xl.forEach(o),Pr=i(Do,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),ct=a(Do,"A",{href:!0,rel:!0});var Yl=r(ct);Ir=i(Yl,"full"),Yl.forEach(o),jr=i(Do," architecture."),Do.forEach(o),qr=c(J),le=a(J,"P",{});var Wo=r(le);Or=i(Wo,"Configuration objects inherit from "),Fo=a(Wo,"A",{href:!0});var Zl=r(Fo);Nr=i(Zl,"PretrainedConfig"),Zl.forEach(o),Dr=i(Wo,` and can be used to control the model outputs. Read the
documentation from `),Lo=a(Wo,"A",{href:!0});var ed=r(Lo);Wr=i(ed,"PretrainedConfig"),ed.forEach(o),Sr=i(Wo," for more information."),Wo.forEach(o),Br=c(J),hn=a(J,"P",{});var td=r(hn);Rr=i(td,"Example:"),td.forEach(o),Hr=c(J),_(mt.$$.fragment,J),J.forEach(o),da=c(t),de=a(t,"H2",{class:!0});var Pa=r(de);je=a(Pa,"A",{id:!0,class:!0,href:!0});var od=r(je);pn=a(od,"SPAN",{});var nd=r(pn);_(ft.$$.fragment,nd),nd.forEach(o),od.forEach(o),Ur=c(Pa),gn=a(Pa,"SPAN",{});var ad=r(gn);Gr=i(ad,"FLAVAImageConfig"),ad.forEach(o),Pa.forEach(o),ca=c(t),z=a(t,"DIV",{class:!0});var Q=r(z);_(ht.$$.fragment,Q),Kr=c(Q),ce=a(Q,"P",{});var So=r(ce);Jr=i(So,"This is the configuration class to store the configuration of a "),wo=a(So,"A",{href:!0});var rd=r(wo);Qr=i(rd,"FLAVAImageModel"),rd.forEach(o),Xr=i(So,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),pt=a(So,"A",{href:!0,rel:!0});var sd=r(pt);Yr=i(sd,"full"),sd.forEach(o),Zr=i(So," architecture."),So.forEach(o),es=c(Q),me=a(Q,"P",{});var Bo=r(me);ts=i(Bo,"Configuration objects inherit from "),ko=a(Bo,"A",{href:!0});var id=r(ko);os=i(id,"PretrainedConfig"),id.forEach(o),ns=i(Bo,` and can be used to control the model outputs. Read the
documentation from `),Vo=a(Bo,"A",{href:!0});var ld=r(Vo);as=i(ld,"PretrainedConfig"),ld.forEach(o),rs=i(Bo," for more information."),Bo.forEach(o),ss=c(Q),un=a(Q,"P",{});var dd=r(un);is=i(dd,"Example:"),dd.forEach(o),ls=c(Q),_(gt.$$.fragment,Q),Q.forEach(o),ma=c(t),fe=a(t,"H2",{class:!0});var Ia=r(fe);qe=a(Ia,"A",{id:!0,class:!0,href:!0});var cd=r(qe);_n=a(cd,"SPAN",{});var md=r(_n);_(ut.$$.fragment,md),md.forEach(o),cd.forEach(o),ds=c(Ia),An=a(Ia,"SPAN",{});var fd=r(An);cs=i(fd,"FLAVAMultimodalConfig"),fd.forEach(o),Ia.forEach(o),fa=c(t),E=a(t,"DIV",{class:!0});var X=r(E);_(_t.$$.fragment,X),ms=c(X),he=a(X,"P",{});var Ro=r(he);fs=i(Ro,"This is the configuration class to store the configuration of a "),yo=a(Ro,"A",{href:!0});var hd=r(yo);hs=i(hd,"FLAVAMultimodalModel"),hd.forEach(o),ps=i(Ro,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),At=a(Ro,"A",{href:!0,rel:!0});var pd=r(At);gs=i(pd,"full"),pd.forEach(o),us=i(Ro," architecture."),Ro.forEach(o),_s=c(X),pe=a(X,"P",{});var Ho=r(pe);As=i(Ho,"Configuration objects inherit from "),To=a(Ho,"A",{href:!0});var gd=r(To);vs=i(gd,"PretrainedConfig"),gd.forEach(o),bs=i(Ho,` and can be used to control the model outputs. Read the
documentation from `),$o=a(Ho,"A",{href:!0});var ud=r($o);Fs=i(ud,"PretrainedConfig"),ud.forEach(o),Ls=i(Ho," for more information."),Ho.forEach(o),ws=c(X),vn=a(X,"P",{});var _d=r(vn);ks=i(_d,"Example:"),_d.forEach(o),Vs=c(X),_(vt.$$.fragment,X),X.forEach(o),ha=c(t),ge=a(t,"H2",{class:!0});var ja=r(ge);Oe=a(ja,"A",{id:!0,class:!0,href:!0});var Ad=r(Oe);bn=a(Ad,"SPAN",{});var vd=r(bn);_(bt.$$.fragment,vd),vd.forEach(o),Ad.forEach(o),ys=c(ja),Fn=a(ja,"SPAN",{});var bd=r(Fn);Ts=i(bd,"FLAVACodebookConfig"),bd.forEach(o),ja.forEach(o),pa=c(t),Ft=a(t,"DIV",{class:!0});var Fd=r(Ft);_(Lt.$$.fragment,Fd),Fd.forEach(o),ga=c(t),ue=a(t,"H2",{class:!0});var qa=r(ue);Ne=a(qa,"A",{id:!0,class:!0,href:!0});var Ld=r(Ne);Ln=a(Ld,"SPAN",{});var wd=r(Ln);_(wt.$$.fragment,wd),wd.forEach(o),Ld.forEach(o),$s=c(qa),wn=a(qa,"SPAN",{});var kd=r(wn);Ms=i(kd,"FLAVAForPretraining"),kd.forEach(o),qa.forEach(o),ua=c(t),N=a(t,"DIV",{class:!0});var Ye=r(N);_(kt.$$.fragment,Ye),xs=c(Ye),kn=a(Ye,"P",{});var Vd=r(kn);Cs=i(Vd,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),Vd.forEach(o),zs=c(Ye),Vt=a(Ye,"P",{});var Oa=r(Vt);Es=i(Oa,"This model is a PyTorch "),yt=a(Oa,"A",{href:!0,rel:!0});var yd=r(yt);Ps=i(yd,"torch.nn.Module"),yd.forEach(o),Is=i(Oa,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Oa.forEach(o),js=c(Ye),U=a(Ye,"DIV",{class:!0});var Uo=r(U);_(Tt.$$.fragment,Uo),qs=c(Uo),_e=a(Uo,"P",{});var Go=r(_e);Os=i(Go,"The "),Mo=a(Go,"A",{href:!0});var Td=r(Mo);Ns=i(Td,"FLAVAForPretraining"),Td.forEach(o),Ds=i(Go," forward method, overrides the "),Vn=a(Go,"CODE",{});var $d=r(Vn);Ws=i($d,"__call__"),$d.forEach(o),Ss=i(Go," special method."),Go.forEach(o),Bs=c(Uo),_(De.$$.fragment,Uo),Uo.forEach(o),Ye.forEach(o),_a=c(t),Ae=a(t,"H2",{class:!0});var Na=r(Ae);We=a(Na,"A",{id:!0,class:!0,href:!0});var Md=r(We);yn=a(Md,"SPAN",{});var xd=r(yn);_($t.$$.fragment,xd),xd.forEach(o),Md.forEach(o),Rs=c(Na),Tn=a(Na,"SPAN",{});var Cd=r(Tn);Hs=i(Cd,"FLAVAModel"),Cd.forEach(o),Na.forEach(o),Aa=c(t),P=a(t,"DIV",{class:!0});var Y=r(P);_(Mt.$$.fragment,Y),Us=c(Y),xt=a(Y,"P",{});var Da=r(xt);Gs=i(Da,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ct=a(Da,"A",{href:!0,rel:!0});var zd=r(Ct);Ks=i(zd,"torch.nn.Module"),zd.forEach(o),Js=i(Da,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Da.forEach(o),Qs=c(Y),I=a(Y,"DIV",{class:!0});var Z=r(I);_(zt.$$.fragment,Z),Xs=c(Z),ve=a(Z,"P",{});var Ko=r(ve);Ys=i(Ko,"The "),xo=a(Ko,"A",{href:!0});var Ed=r(xo);Zs=i(Ed,"FLAVAModel"),Ed.forEach(o),ei=i(Ko," forward method, overrides the "),$n=a(Ko,"CODE",{});var Pd=r($n);ti=i(Pd,"__call__"),Pd.forEach(o),oi=i(Ko," special method."),Ko.forEach(o),ni=c(Z),_(Se.$$.fragment,Z),ai=c(Z),Mn=a(Z,"P",{});var Id=r(Mn);ri=i(Id,"Examples:"),Id.forEach(o),si=c(Z),_(Et.$$.fragment,Z),Z.forEach(o),ii=c(Y),G=a(Y,"DIV",{class:!0});var Jo=r(G);_(Pt.$$.fragment,Jo),li=c(Jo),be=a(Jo,"P",{});var Qo=r(be);di=i(Qo,"The "),Co=a(Qo,"A",{href:!0});var jd=r(Co);ci=i(jd,"FLAVAModel"),jd.forEach(o),mi=i(Qo," forward method, overrides the "),xn=a(Qo,"CODE",{});var qd=r(xn);fi=i(qd,"__call__"),qd.forEach(o),hi=i(Qo," special method."),Qo.forEach(o),pi=c(Jo),_(Be.$$.fragment,Jo),Jo.forEach(o),gi=c(Y),K=a(Y,"DIV",{class:!0});var Xo=r(K);_(It.$$.fragment,Xo),ui=c(Xo),Fe=a(Xo,"P",{});var Yo=r(Fe);_i=i(Yo,"The "),zo=a(Yo,"A",{href:!0});var Od=r(zo);Ai=i(Od,"FLAVAModel"),Od.forEach(o),vi=i(Yo," forward method, overrides the "),Cn=a(Yo,"CODE",{});var Nd=r(Cn);bi=i(Nd,"__call__"),Nd.forEach(o),Fi=i(Yo," special method."),Yo.forEach(o),Li=c(Xo),_(Re.$$.fragment,Xo),Xo.forEach(o),Y.forEach(o),va=c(t),Le=a(t,"H2",{class:!0});var Wa=r(Le);He=a(Wa,"A",{id:!0,class:!0,href:!0});var Dd=r(He);zn=a(Dd,"SPAN",{});var Wd=r(zn);_(jt.$$.fragment,Wd),Wd.forEach(o),Dd.forEach(o),wi=c(Wa),En=a(Wa,"SPAN",{});var Sd=r(En);ki=i(Sd,"FLAVACodebook"),Sd.forEach(o),Wa.forEach(o),ba=c(t),M=a(t,"DIV",{class:!0});var S=r(M);_(qt.$$.fragment,S),Vi=c(S),Ot=a(S,"P",{});var Sa=r(Ot);yi=i(Sa,`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),Pn=a(Sa,"CODE",{});var Bd=r(Pn);Ti=i(Bd,"get_codebook_indices"),Bd.forEach(o),$i=i(Sa," to get image tokens for an image."),Sa.forEach(o),Mi=c(S),Nt=a(S,"P",{});var Ba=r(Nt);xi=i(Ba,"This model is a PyTorch "),Dt=a(Ba,"A",{href:!0,rel:!0});var Rd=r(Dt);Ci=i(Rd,"torch.nn.Module"),Rd.forEach(o),zi=i(Ba,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ba.forEach(o),Ei=c(S),Eo=a(S,"DIV",{class:!0});var Hd=r(Eo);_(Wt.$$.fragment,Hd),Hd.forEach(o),Pi=c(S),Po=a(S,"DIV",{class:!0});var Ud=r(Po);_(St.$$.fragment,Ud),Ud.forEach(o),Ii=c(S),Io=a(S,"DIV",{class:!0});var Gd=r(Io);_(Bt.$$.fragment,Gd),Gd.forEach(o),S.forEach(o),Fa=c(t),we=a(t,"H2",{class:!0});var Ra=r(we);Ue=a(Ra,"A",{id:!0,class:!0,href:!0});var Kd=r(Ue);In=a(Kd,"SPAN",{});var Jd=r(In);_(Rt.$$.fragment,Jd),Jd.forEach(o),Kd.forEach(o),ji=c(Ra),jn=a(Ra,"SPAN",{});var Qd=r(jn);qi=i(Qd,"FLAVATextModel"),Qd.forEach(o),Ra.forEach(o),La=c(t),B=a(t,"DIV",{class:!0});var Zo=r(B);_(Ht.$$.fragment,Zo),Oi=c(Zo),Ut=a(Zo,"P",{});var Ha=r(Ut);Ni=i(Ha,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Gt=a(Ha,"A",{href:!0,rel:!0});var Xd=r(Gt);Di=i(Xd,"torch.nn.Module"),Xd.forEach(o),Wi=i(Ha,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ha.forEach(o),Si=c(Zo),j=a(Zo,"DIV",{class:!0});var ee=r(j);_(Kt.$$.fragment,ee),Bi=c(ee),ke=a(ee,"P",{});var en=r(ke);Ri=i(en,"The "),jo=a(en,"A",{href:!0});var Yd=r(jo);Hi=i(Yd,"FLAVATextModel"),Yd.forEach(o),Ui=i(en," forward method, overrides the "),qn=a(en,"CODE",{});var Zd=r(qn);Gi=i(Zd,"__call__"),Zd.forEach(o),Ki=i(en," special method."),en.forEach(o),Ji=c(ee),_(Ge.$$.fragment,ee),Qi=c(ee),On=a(ee,"P",{});var ec=r(On);Xi=i(ec,"Example:"),ec.forEach(o),Yi=c(ee),_(Jt.$$.fragment,ee),ee.forEach(o),Zo.forEach(o),wa=c(t),Ve=a(t,"H2",{class:!0});var Ua=r(Ve);Ke=a(Ua,"A",{id:!0,class:!0,href:!0});var tc=r(Ke);Nn=a(tc,"SPAN",{});var oc=r(Nn);_(Qt.$$.fragment,oc),oc.forEach(o),tc.forEach(o),Zi=c(Ua),Dn=a(Ua,"SPAN",{});var nc=r(Dn);el=i(nc,"FLAVAImageModel"),nc.forEach(o),Ua.forEach(o),ka=c(t),R=a(t,"DIV",{class:!0});var tn=r(R);_(Xt.$$.fragment,tn),tl=c(tn),Yt=a(tn,"P",{});var Ga=r(Yt);ol=i(Ga,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Zt=a(Ga,"A",{href:!0,rel:!0});var ac=r(Zt);nl=i(ac,"torch.nn.Module"),ac.forEach(o),al=i(Ga,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ga.forEach(o),rl=c(tn),q=a(tn,"DIV",{class:!0});var te=r(q);_(eo.$$.fragment,te),sl=c(te),ye=a(te,"P",{});var on=r(ye);il=i(on,"The "),qo=a(on,"A",{href:!0});var rc=r(qo);ll=i(rc,"FLAVAImageModel"),rc.forEach(o),dl=i(on," forward method, overrides the "),Wn=a(on,"CODE",{});var sc=r(Wn);cl=i(sc,"__call__"),sc.forEach(o),ml=i(on," special method."),on.forEach(o),fl=c(te),_(Je.$$.fragment,te),hl=c(te),Sn=a(te,"P",{});var ic=r(Sn);pl=i(ic,"Example:"),ic.forEach(o),gl=c(te),_(to.$$.fragment,te),te.forEach(o),tn.forEach(o),Va=c(t),Te=a(t,"H2",{class:!0});var Ka=r(Te);Qe=a(Ka,"A",{id:!0,class:!0,href:!0});var lc=r(Qe);Bn=a(lc,"SPAN",{});var dc=r(Bn);_(oo.$$.fragment,dc),dc.forEach(o),lc.forEach(o),ul=c(Ka),Rn=a(Ka,"SPAN",{});var cc=r(Rn);_l=i(cc,"FLAVAMultimodalModel"),cc.forEach(o),Ka.forEach(o),ya=c(t),H=a(t,"DIV",{class:!0});var nn=r(H);_(no.$$.fragment,nn),Al=c(nn),ao=a(nn,"P",{});var Ja=r(ao);vl=i(Ja,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),ro=a(Ja,"A",{href:!0,rel:!0});var mc=r(ro);bl=i(mc,"torch.nn.Module"),mc.forEach(o),Fl=i(Ja,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ja.forEach(o),Ll=c(nn),O=a(nn,"DIV",{class:!0});var oe=r(O);_(so.$$.fragment,oe),wl=c(oe),$e=a(oe,"P",{});var an=r($e);kl=i(an,"The "),Oo=a(an,"A",{href:!0});var fc=r(Oo);Vl=i(fc,"FLAVAMultimodalModel"),fc.forEach(o),yl=i(an," forward method, overrides the "),Hn=a(an,"CODE",{});var hc=r(Hn);Tl=i(hc,"__call__"),hc.forEach(o),$l=i(an," special method."),an.forEach(o),Ml=c(oe),_(Xe.$$.fragment,oe),xl=c(oe),Un=a(oe,"P",{});var pc=r(Un);Cl=i(pc,"Example:"),pc.forEach(o),zl=c(oe),_(io.$$.fragment,oe),oe.forEach(o),nn.forEach(o),this.h()},h(){l(f,"name","hf:doc:metadata"),l(f,"content",JSON.stringify($c)),l(w,"id","flava"),l(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(w,"href","#flava"),l(g,"class","relative group"),l(Me,"id","overview"),l(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Me,"href","#overview"),l(ne,"class","relative group"),l(tt,"href","https://arxiv.org/abs/2112.04482"),l(tt,"rel","nofollow"),l(ot,"href","https://huggingface.co/aps"),l(ot,"rel","nofollow"),l(ze,"id","transformers.FLAVAConfig"),l(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ze,"href","#transformers.FLAVAConfig"),l(ae,"class","relative group"),l(go,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(uo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(_o,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Ao,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(vo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(Pe,"class","docstring"),l($,"class","docstring"),l(Ie,"id","transformers.FLAVATextConfig"),l(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ie,"href","#transformers.FLAVATextConfig"),l(se,"class","relative group"),l(bo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(ct,"href","https://huggingface.co/aps/flava-full"),l(ct,"rel","nofollow"),l(Fo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Lo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(C,"class","docstring"),l(je,"id","transformers.FLAVAImageConfig"),l(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(je,"href","#transformers.FLAVAImageConfig"),l(de,"class","relative group"),l(wo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(pt,"href","https://huggingface.co/aps/flava-full"),l(pt,"rel","nofollow"),l(ko,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Vo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(z,"class","docstring"),l(qe,"id","transformers.FLAVAMultimodalConfig"),l(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(qe,"href","#transformers.FLAVAMultimodalConfig"),l(fe,"class","relative group"),l(yo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(At,"href","https://huggingface.co/aps/flava-full"),l(At,"rel","nofollow"),l(To,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l($o,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(E,"class","docstring"),l(Oe,"id","transformers.FLAVACodebookConfig"),l(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Oe,"href","#transformers.FLAVACodebookConfig"),l(ge,"class","relative group"),l(Ft,"class","docstring"),l(Ne,"id","transformers.FLAVAForPretraining"),l(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ne,"href","#transformers.FLAVAForPretraining"),l(ue,"class","relative group"),l(yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(yt,"rel","nofollow"),l(Mo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAForPretraining"),l(U,"class","docstring"),l(N,"class","docstring"),l(We,"id","transformers.FLAVAModel"),l(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(We,"href","#transformers.FLAVAModel"),l(Ae,"class","relative group"),l(Ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ct,"rel","nofollow"),l(xo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(I,"class","docstring"),l(Co,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(G,"class","docstring"),l(zo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(K,"class","docstring"),l(P,"class","docstring"),l(He,"id","transformers.FLAVACodebook"),l(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(He,"href","#transformers.FLAVACodebook"),l(Le,"class","relative group"),l(Dt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Dt,"rel","nofollow"),l(Eo,"class","docstring"),l(Po,"class","docstring"),l(Io,"class","docstring"),l(M,"class","docstring"),l(Ue,"id","transformers.FLAVATextModel"),l(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ue,"href","#transformers.FLAVATextModel"),l(we,"class","relative group"),l(Gt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Gt,"rel","nofollow"),l(jo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(j,"class","docstring"),l(B,"class","docstring"),l(Ke,"id","transformers.FLAVAImageModel"),l(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ke,"href","#transformers.FLAVAImageModel"),l(Ve,"class","relative group"),l(Zt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Zt,"rel","nofollow"),l(qo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(q,"class","docstring"),l(R,"class","docstring"),l(Qe,"id","transformers.FLAVAMultimodalModel"),l(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Qe,"href","#transformers.FLAVAMultimodalModel"),l(Te,"class","relative group"),l(ro,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(ro,"rel","nofollow"),l(Oo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(O,"class","docstring"),l(H,"class","docstring")},m(t,m){e(document.head,f),h(t,V,m),h(t,g,m),e(g,w),e(w,k),A(L,k,null),e(g,p),e(g,y),e(y,Qa),h(t,Zn,m),h(t,ne,m),e(ne,Me),e(Me,rn),A(et,rn,null),e(ne,Xa),e(ne,sn),e(sn,Ya),h(t,ea,m),h(t,xe,m),e(xe,Za),e(xe,tt),e(tt,er),e(xe,tr),h(t,ta,m),h(t,fo,m),e(fo,or),h(t,oa,m),h(t,ho,m),e(ho,nr),h(t,na,m),h(t,po,m),e(po,ar),h(t,aa,m),h(t,Ce,m),e(Ce,rr),e(Ce,ot),e(ot,sr),e(Ce,ir),h(t,ra,m),h(t,ae,m),e(ae,ze),e(ze,ln),A(nt,ln,null),e(ae,lr),e(ae,dn),e(dn,dr),h(t,sa,m),h(t,$,m),A(at,$,null),e($,cr),e($,Ee),e(Ee,go),e(go,mr),e(Ee,fr),e(Ee,uo),e(uo,hr),e(Ee,pr),e($,gr),e($,re),e(re,ur),e(re,_o),e(_o,_r),e(re,Ar),e(re,Ao),e(Ao,vr),e(re,br),e($,Fr),e($,cn),e(cn,Lr),e($,wr),A(rt,$,null),e($,kr),e($,Pe),A(st,Pe,null),e(Pe,Vr),e(Pe,it),e(it,yr),e(it,vo),e(vo,Tr),e(it,$r),h(t,ia,m),h(t,se,m),e(se,Ie),e(Ie,mn),A(lt,mn,null),e(se,Mr),e(se,fn),e(fn,xr),h(t,la,m),h(t,C,m),A(dt,C,null),e(C,Cr),e(C,ie),e(ie,zr),e(ie,bo),e(bo,Er),e(ie,Pr),e(ie,ct),e(ct,Ir),e(ie,jr),e(C,qr),e(C,le),e(le,Or),e(le,Fo),e(Fo,Nr),e(le,Dr),e(le,Lo),e(Lo,Wr),e(le,Sr),e(C,Br),e(C,hn),e(hn,Rr),e(C,Hr),A(mt,C,null),h(t,da,m),h(t,de,m),e(de,je),e(je,pn),A(ft,pn,null),e(de,Ur),e(de,gn),e(gn,Gr),h(t,ca,m),h(t,z,m),A(ht,z,null),e(z,Kr),e(z,ce),e(ce,Jr),e(ce,wo),e(wo,Qr),e(ce,Xr),e(ce,pt),e(pt,Yr),e(ce,Zr),e(z,es),e(z,me),e(me,ts),e(me,ko),e(ko,os),e(me,ns),e(me,Vo),e(Vo,as),e(me,rs),e(z,ss),e(z,un),e(un,is),e(z,ls),A(gt,z,null),h(t,ma,m),h(t,fe,m),e(fe,qe),e(qe,_n),A(ut,_n,null),e(fe,ds),e(fe,An),e(An,cs),h(t,fa,m),h(t,E,m),A(_t,E,null),e(E,ms),e(E,he),e(he,fs),e(he,yo),e(yo,hs),e(he,ps),e(he,At),e(At,gs),e(he,us),e(E,_s),e(E,pe),e(pe,As),e(pe,To),e(To,vs),e(pe,bs),e(pe,$o),e($o,Fs),e(pe,Ls),e(E,ws),e(E,vn),e(vn,ks),e(E,Vs),A(vt,E,null),h(t,ha,m),h(t,ge,m),e(ge,Oe),e(Oe,bn),A(bt,bn,null),e(ge,ys),e(ge,Fn),e(Fn,Ts),h(t,pa,m),h(t,Ft,m),A(Lt,Ft,null),h(t,ga,m),h(t,ue,m),e(ue,Ne),e(Ne,Ln),A(wt,Ln,null),e(ue,$s),e(ue,wn),e(wn,Ms),h(t,ua,m),h(t,N,m),A(kt,N,null),e(N,xs),e(N,kn),e(kn,Cs),e(N,zs),e(N,Vt),e(Vt,Es),e(Vt,yt),e(yt,Ps),e(Vt,Is),e(N,js),e(N,U),A(Tt,U,null),e(U,qs),e(U,_e),e(_e,Os),e(_e,Mo),e(Mo,Ns),e(_e,Ds),e(_e,Vn),e(Vn,Ws),e(_e,Ss),e(U,Bs),A(De,U,null),h(t,_a,m),h(t,Ae,m),e(Ae,We),e(We,yn),A($t,yn,null),e(Ae,Rs),e(Ae,Tn),e(Tn,Hs),h(t,Aa,m),h(t,P,m),A(Mt,P,null),e(P,Us),e(P,xt),e(xt,Gs),e(xt,Ct),e(Ct,Ks),e(xt,Js),e(P,Qs),e(P,I),A(zt,I,null),e(I,Xs),e(I,ve),e(ve,Ys),e(ve,xo),e(xo,Zs),e(ve,ei),e(ve,$n),e($n,ti),e(ve,oi),e(I,ni),A(Se,I,null),e(I,ai),e(I,Mn),e(Mn,ri),e(I,si),A(Et,I,null),e(P,ii),e(P,G),A(Pt,G,null),e(G,li),e(G,be),e(be,di),e(be,Co),e(Co,ci),e(be,mi),e(be,xn),e(xn,fi),e(be,hi),e(G,pi),A(Be,G,null),e(P,gi),e(P,K),A(It,K,null),e(K,ui),e(K,Fe),e(Fe,_i),e(Fe,zo),e(zo,Ai),e(Fe,vi),e(Fe,Cn),e(Cn,bi),e(Fe,Fi),e(K,Li),A(Re,K,null),h(t,va,m),h(t,Le,m),e(Le,He),e(He,zn),A(jt,zn,null),e(Le,wi),e(Le,En),e(En,ki),h(t,ba,m),h(t,M,m),A(qt,M,null),e(M,Vi),e(M,Ot),e(Ot,yi),e(Ot,Pn),e(Pn,Ti),e(Ot,$i),e(M,Mi),e(M,Nt),e(Nt,xi),e(Nt,Dt),e(Dt,Ci),e(Nt,zi),e(M,Ei),e(M,Eo),A(Wt,Eo,null),e(M,Pi),e(M,Po),A(St,Po,null),e(M,Ii),e(M,Io),A(Bt,Io,null),h(t,Fa,m),h(t,we,m),e(we,Ue),e(Ue,In),A(Rt,In,null),e(we,ji),e(we,jn),e(jn,qi),h(t,La,m),h(t,B,m),A(Ht,B,null),e(B,Oi),e(B,Ut),e(Ut,Ni),e(Ut,Gt),e(Gt,Di),e(Ut,Wi),e(B,Si),e(B,j),A(Kt,j,null),e(j,Bi),e(j,ke),e(ke,Ri),e(ke,jo),e(jo,Hi),e(ke,Ui),e(ke,qn),e(qn,Gi),e(ke,Ki),e(j,Ji),A(Ge,j,null),e(j,Qi),e(j,On),e(On,Xi),e(j,Yi),A(Jt,j,null),h(t,wa,m),h(t,Ve,m),e(Ve,Ke),e(Ke,Nn),A(Qt,Nn,null),e(Ve,Zi),e(Ve,Dn),e(Dn,el),h(t,ka,m),h(t,R,m),A(Xt,R,null),e(R,tl),e(R,Yt),e(Yt,ol),e(Yt,Zt),e(Zt,nl),e(Yt,al),e(R,rl),e(R,q),A(eo,q,null),e(q,sl),e(q,ye),e(ye,il),e(ye,qo),e(qo,ll),e(ye,dl),e(ye,Wn),e(Wn,cl),e(ye,ml),e(q,fl),A(Je,q,null),e(q,hl),e(q,Sn),e(Sn,pl),e(q,gl),A(to,q,null),h(t,Va,m),h(t,Te,m),e(Te,Qe),e(Qe,Bn),A(oo,Bn,null),e(Te,ul),e(Te,Rn),e(Rn,_l),h(t,ya,m),h(t,H,m),A(no,H,null),e(H,Al),e(H,ao),e(ao,vl),e(ao,ro),e(ro,bl),e(ao,Fl),e(H,Ll),e(H,O),A(so,O,null),e(O,wl),e(O,$e),e($e,kl),e($e,Oo),e(Oo,Vl),e($e,yl),e($e,Hn),e(Hn,Tl),e($e,$l),e(O,Ml),A(Xe,O,null),e(O,xl),e(O,Un),e(Un,Cl),e(O,zl),A(io,O,null),Ta=!0},p(t,[m]){const lo={};m&2&&(lo.$$scope={dirty:m,ctx:t}),De.$set(lo);const Gn={};m&2&&(Gn.$$scope={dirty:m,ctx:t}),Se.$set(Gn);const Kn={};m&2&&(Kn.$$scope={dirty:m,ctx:t}),Be.$set(Kn);const Jn={};m&2&&(Jn.$$scope={dirty:m,ctx:t}),Re.$set(Jn);const co={};m&2&&(co.$$scope={dirty:m,ctx:t}),Ge.$set(co);const Qn={};m&2&&(Qn.$$scope={dirty:m,ctx:t}),Je.$set(Qn);const Xn={};m&2&&(Xn.$$scope={dirty:m,ctx:t}),Xe.$set(Xn)},i(t){Ta||(v(L.$$.fragment,t),v(et.$$.fragment,t),v(nt.$$.fragment,t),v(at.$$.fragment,t),v(rt.$$.fragment,t),v(st.$$.fragment,t),v(lt.$$.fragment,t),v(dt.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(ht.$$.fragment,t),v(gt.$$.fragment,t),v(ut.$$.fragment,t),v(_t.$$.fragment,t),v(vt.$$.fragment,t),v(bt.$$.fragment,t),v(Lt.$$.fragment,t),v(wt.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(De.$$.fragment,t),v($t.$$.fragment,t),v(Mt.$$.fragment,t),v(zt.$$.fragment,t),v(Se.$$.fragment,t),v(Et.$$.fragment,t),v(Pt.$$.fragment,t),v(Be.$$.fragment,t),v(It.$$.fragment,t),v(Re.$$.fragment,t),v(jt.$$.fragment,t),v(qt.$$.fragment,t),v(Wt.$$.fragment,t),v(St.$$.fragment,t),v(Bt.$$.fragment,t),v(Rt.$$.fragment,t),v(Ht.$$.fragment,t),v(Kt.$$.fragment,t),v(Ge.$$.fragment,t),v(Jt.$$.fragment,t),v(Qt.$$.fragment,t),v(Xt.$$.fragment,t),v(eo.$$.fragment,t),v(Je.$$.fragment,t),v(to.$$.fragment,t),v(oo.$$.fragment,t),v(no.$$.fragment,t),v(so.$$.fragment,t),v(Xe.$$.fragment,t),v(io.$$.fragment,t),Ta=!0)},o(t){b(L.$$.fragment,t),b(et.$$.fragment,t),b(nt.$$.fragment,t),b(at.$$.fragment,t),b(rt.$$.fragment,t),b(st.$$.fragment,t),b(lt.$$.fragment,t),b(dt.$$.fragment,t),b(mt.$$.fragment,t),b(ft.$$.fragment,t),b(ht.$$.fragment,t),b(gt.$$.fragment,t),b(ut.$$.fragment,t),b(_t.$$.fragment,t),b(vt.$$.fragment,t),b(bt.$$.fragment,t),b(Lt.$$.fragment,t),b(wt.$$.fragment,t),b(kt.$$.fragment,t),b(Tt.$$.fragment,t),b(De.$$.fragment,t),b($t.$$.fragment,t),b(Mt.$$.fragment,t),b(zt.$$.fragment,t),b(Se.$$.fragment,t),b(Et.$$.fragment,t),b(Pt.$$.fragment,t),b(Be.$$.fragment,t),b(It.$$.fragment,t),b(Re.$$.fragment,t),b(jt.$$.fragment,t),b(qt.$$.fragment,t),b(Wt.$$.fragment,t),b(St.$$.fragment,t),b(Bt.$$.fragment,t),b(Rt.$$.fragment,t),b(Ht.$$.fragment,t),b(Kt.$$.fragment,t),b(Ge.$$.fragment,t),b(Jt.$$.fragment,t),b(Qt.$$.fragment,t),b(Xt.$$.fragment,t),b(eo.$$.fragment,t),b(Je.$$.fragment,t),b(to.$$.fragment,t),b(oo.$$.fragment,t),b(no.$$.fragment,t),b(so.$$.fragment,t),b(Xe.$$.fragment,t),b(io.$$.fragment,t),Ta=!1},d(t){o(f),t&&o(V),t&&o(g),F(L),t&&o(Zn),t&&o(ne),F(et),t&&o(ea),t&&o(xe),t&&o(ta),t&&o(fo),t&&o(oa),t&&o(ho),t&&o(na),t&&o(po),t&&o(aa),t&&o(Ce),t&&o(ra),t&&o(ae),F(nt),t&&o(sa),t&&o($),F(at),F(rt),F(st),t&&o(ia),t&&o(se),F(lt),t&&o(la),t&&o(C),F(dt),F(mt),t&&o(da),t&&o(de),F(ft),t&&o(ca),t&&o(z),F(ht),F(gt),t&&o(ma),t&&o(fe),F(ut),t&&o(fa),t&&o(E),F(_t),F(vt),t&&o(ha),t&&o(ge),F(bt),t&&o(pa),t&&o(Ft),F(Lt),t&&o(ga),t&&o(ue),F(wt),t&&o(ua),t&&o(N),F(kt),F(Tt),F(De),t&&o(_a),t&&o(Ae),F($t),t&&o(Aa),t&&o(P),F(Mt),F(zt),F(Se),F(Et),F(Pt),F(Be),F(It),F(Re),t&&o(va),t&&o(Le),F(jt),t&&o(ba),t&&o(M),F(qt),F(Wt),F(St),F(Bt),t&&o(Fa),t&&o(we),F(Rt),t&&o(La),t&&o(B),F(Ht),F(Kt),F(Ge),F(Jt),t&&o(wa),t&&o(Ve),F(Qt),t&&o(ka),t&&o(R),F(Xt),F(eo),F(Je),F(to),t&&o(Va),t&&o(Te),F(oo),t&&o(ya),t&&o(H),F(no),F(so),F(Xe),F(io)}}}const $c={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FLAVAConfig",title:"FLAVAConfig"},{local:"transformers.FLAVATextConfig",title:"FLAVATextConfig"},{local:"transformers.FLAVAImageConfig",title:"FLAVAImageConfig"},{local:"transformers.FLAVAMultimodalConfig",title:"FLAVAMultimodalConfig"},{local:"transformers.FLAVACodebookConfig",title:"FLAVACodebookConfig"},{local:"transformers.FLAVAForPretraining",title:"FLAVAForPretraining"},{local:"transformers.FLAVAModel",title:"FLAVAModel"},{local:"transformers.FLAVACodebook",title:"FLAVACodebook"},{local:"transformers.FLAVATextModel",title:"FLAVATextModel"},{local:"transformers.FLAVAImageModel",title:"FLAVAImageModel"},{local:"transformers.FLAVAMultimodalModel",title:"FLAVAMultimodalModel"}],title:"FLAVA"};function Mc(x){return vc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ic extends gc{constructor(f){super();uc(this,f,Mc,Tc,_c,{})}}export{Ic as default,$c as metadata};
