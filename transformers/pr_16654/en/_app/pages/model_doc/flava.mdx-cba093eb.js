import{S as gc,i as uc,s as _c,e as n,k as d,w as u,t as s,M as Ac,c as a,d as t,m as c,a as r,x as _,h as i,b as l,F as e,g as p,y as A,q as v,o as b,B as F,v as vc}from"../../chunks/vendor-6b77c823.js";import{T as mt}from"../../chunks/Tip-39098574.js";import{D as T}from"../../chunks/Docstring-17b815d9.js";import{C as Ze}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as D}from"../../chunks/IconCopyLink-7a11ce68.js";function bc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function Fc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function Lc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function wc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function kc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function yc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function Vc(x){let h,y,g,w,k;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),w=s("Module"),k=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(f,"CODE",{});var V=r(g);w=i(V,"Module"),V.forEach(t),k=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(L,f){p(L,h,f),e(h,y),e(h,g),e(g,w),e(h,k)},d(L){L&&t(h)}}}function Tc(x){let h,y,g,w,k,L,f,V,Qa,Zn,ne,Me,rn,eo,Xa,sn,Ya,ea,xe,Za,oo,er,or,oa,ht,tr,ta,pt,nr,na,ft,ar,aa,Ce,rr,to,sr,ir,ra,ae,ze,ln,no,lr,dn,dr,sa,$,ao,cr,Ee,gt,mr,hr,ut,pr,fr,gr,re,ur,_t,_r,Ar,At,vr,br,Fr,cn,Lr,wr,ro,kr,Pe,so,yr,io,Vr,vt,Tr,$r,ia,se,Ie,mn,lo,Mr,hn,xr,la,C,co,Cr,ie,zr,bt,Er,Pr,mo,Ir,jr,qr,le,Or,Ft,Nr,Dr,Lt,Wr,Sr,Br,pn,Rr,Hr,ho,da,de,je,fn,po,Ur,gn,Gr,ca,z,fo,Kr,ce,Jr,wt,Qr,Xr,go,Yr,Zr,es,me,os,kt,ts,ns,yt,as,rs,ss,un,is,ls,uo,ma,he,qe,_n,_o,ds,An,cs,ha,E,Ao,ms,pe,hs,Vt,ps,fs,vo,gs,us,_s,fe,As,Tt,vs,bs,$t,Fs,Ls,ws,vn,ks,ys,bo,pa,ge,Oe,bn,Fo,Vs,Fn,Ts,fa,Lo,wo,ga,ue,Ne,Ln,ko,$s,wn,Ms,ua,N,yo,xs,kn,Cs,zs,Vo,Es,To,Ps,Is,js,U,$o,qs,_e,Os,Mt,Ns,Ds,yn,Ws,Ss,Bs,De,_a,Ae,We,Vn,Mo,Rs,Tn,Hs,Aa,P,xo,Us,Co,Gs,zo,Ks,Js,Qs,I,Eo,Xs,ve,Ys,xt,Zs,ei,$n,oi,ti,ni,Se,ai,Mn,ri,si,Po,ii,G,Io,li,be,di,Ct,ci,mi,xn,hi,pi,fi,Be,gi,K,jo,ui,Fe,_i,zt,Ai,vi,Cn,bi,Fi,Li,Re,va,Le,He,zn,qo,wi,En,ki,ba,M,Oo,yi,No,Vi,Pn,Ti,$i,Mi,Do,xi,Wo,Ci,zi,Ei,Et,So,Pi,Pt,Bo,Ii,It,Ro,Fa,we,Ue,In,Ho,ji,jn,qi,La,B,Uo,Oi,Go,Ni,Ko,Di,Wi,Si,j,Jo,Bi,ke,Ri,jt,Hi,Ui,qn,Gi,Ki,Ji,Ge,Qi,On,Xi,Yi,Qo,wa,ye,Ke,Nn,Xo,Zi,Dn,el,ka,R,Yo,ol,Zo,tl,et,nl,al,rl,q,ot,sl,Ve,il,qt,ll,dl,Wn,cl,ml,hl,Je,pl,Sn,fl,gl,tt,ya,Te,Qe,Bn,nt,ul,Rn,_l,Va,H,at,Al,rt,vl,st,bl,Fl,Ll,O,it,wl,$e,kl,Ot,yl,Vl,Hn,Tl,$l,Ml,Xe,xl,Un,Cl,zl,lt,Ta;return L=new D({}),eo=new D({}),no=new D({}),ao=new T({props:{name:"class transformers.FLAVAConfig",anchor:"transformers.FLAVAConfig",parameters:[{name:"image_config_dict",val:" = None"},{name:"text_config_dict",val:" = None"},{name:"multimodal_config_dict",val:" = None"},{name:"hidden_size",val:" = 768"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"projection_dim",val:" = 768"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"initializer_range",val:" = 0.02"},{name:"ce_ignore_index",val:" = -100"},{name:"mim_weight",val:" = 1.0"},{name:"mlm_weight",val:" = 1.0"},{name:"global_contrastive_weight",val:" = 1.0"},{name:"itm_weight",val:" = 1.0"},{name:"mmm_image_weight",val:" = 1.0"},{name:"mmm_text_weight",val:" = 1.0"},{name:"global_backprop_contrastive",val:" = True"},{name:"skip_unmasked_multimodal_encoder",val:" = True"},{name:"return_loss",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.FLAVAConfig.image_config_dict",description:`<strong>image_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>.`,name:"image_config_dict"},{anchor:"transformers.FLAVAConfig.multimodal_config_dict",description:`<strong>multimodal_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>.`,name:"multimodal_config_dict"},{anchor:"transformers.FLAVAConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FLAVAConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FLAVAConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FLAVAConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM unimodal loss`,name:"mim_weight"},{anchor:"transformers.FLAVAConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FLAVAConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FLAVAConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FLAVAConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FLAVAConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FLAVAConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FLAVAConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FLAVAConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FLAVAConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L442"}}),ro=new Ze({props:{code:`from transformers import FLAVAModel, FLAVAForPretraining, FLAVAConfig

# Initializing a FLAVAConfig with style configuration
configuration = FLAVAConfig()

# Initializing a FLAVAModel and FLAVAForPretraining model from the style configuration
model = FLAVAModel(configuration)
model_pre = FLAVAForPretraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAModel, FLAVAForPretraining, FLAVAConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAModel and FLAVAForPretraining model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FLAVAForPretraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),so=new T({props:{name:"from_configs",anchor:"transformers.FLAVAConfig.from_configs",parameters:[{name:"image_config",val:": FLAVAImageConfig"},{name:"text_config",val:": FLAVATextConfig"},{name:"multimodal_config",val:": FLAVAMultimodalConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L576",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"
>FLAVAConfig</a></p>
`}}),lo=new D({}),co=new T({props:{name:"class transformers.FLAVATextConfig",anchor:"transformers.FLAVATextConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"type_vocab_size",val:" = 2"},{name:"max_position_embeddings",val:" = 512"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVATextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FLAVATextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FLAVATextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FLAVATextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FLAVATextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVATextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVATextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVATextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVATextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVATextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVATextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVATextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVATextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVATextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVATextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVATextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVATextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L148"}}),ho=new Ze({props:{code:`from transformers import FLAVATextModel, FLAVATextConfig

# Initializing a FLAVATextModel with  style configuration
configuration = FLAVATextConfig()

# Initializing a FLAVATextConfig from the  style configuration
model = FLAVATextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVATextModel, FLAVATextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVATextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextConfig from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),po=new D({}),fo=new T({props:{name:"class transformers.FLAVAImageConfig",anchor:"transformers.FLAVAImageConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"mask_token",val:" = True"},{name:"vocab_size",val:" = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVAImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVAImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVAImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use a mask token or not. Used in MIM loss.`,name:"mask_token"},{anchor:"transformers.FLAVAImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebook">FLAVACodebook</a> used in conjunction with <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel">FLAVAImageModel</a> for MIM.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L32"}}),uo=new Ze({props:{code:`from transformers import FLAVAImageModel, FLAVAImageConfig

# Initializing a FLAVAImageModel with  style configuration
configuration = FLAVAImageConfig()

# Initializing a FLAVAImageModel model from the  style configuration
model = FLAVAImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAImageModel, FLAVAImageConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),_o=new D({}),Ao=new T({props:{name:"class transformers.FLAVAMultimodalConfig",anchor:"transformers.FLAVAMultimodalConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 6"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"qkv_bias",val:" = True"},{name:"use_cls_token",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L275"}}),bo=new Ze({props:{code:`from transformers import FLAVAMultimodalModel, FLAVAMultimodalConfig

# Initializing a FLAVAMultimodalModel with  style configuration
configuration = FLAVAMultimodalConfig()

# Initializing a FLAVAMultimodalModel model from the  style configuration
model = FLAVAMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAMultimodalModel, FLAVAMultimodalConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Fo=new D({}),wo=new T({props:{name:"class transformers.FLAVACodebookConfig",anchor:"transformers.FLAVACodebookConfig",parameters:[{name:"num_groups",val:" = 4"},{name:"input_channels",val:" = 3"},{name:"num_blocks_per_group",val:" = 2"},{name:"hidden_size",val:" = 256"},{name:"vocab_size",val:" = 8192"},{name:"freeze",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L375"}}),ko=new D({}),yo=new T({props:{name:"class transformers.FLAVAForPretraining",anchor:"transformers.FLAVAForPretraining",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAForPretraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1765"}}),$o=new T({props:{name:"forward",anchor:"transformers.FLAVAForPretraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAForPretraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1797",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAForPretrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) \u2014 Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FLAVALosses</code>) \u2014 Detailed info for FLAVA Pretraining losses. Check <code>FLAVALosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
</ul>
<ul>
<li><strong>mim_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape
<code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and
<code>input_ids_masked</code> are not): The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked
patches. The flattened output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mlm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape
<code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and
<code>pixel_values</code> are not): The logits for MLM unimodal loss. The flattened output is returned when
<code>input_ids_masked</code> has some of the tokens masked.</li>
<li><strong>itm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and
<code>pixel_values</code> are present): The logits for ITM loss. Note that ITM loss is calculated on masked pairs in
FLAVA.</li>
<li><strong>mmm_image_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape
<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code>
are present): The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The
flattened output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mmm_text_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape
<code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present): The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has some of the tokens masked.</li>
<li><strong>contrastive_logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</li>
<li><strong>contrastive_logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAForPretrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new mt({props:{$$slots:{default:[bc]},$$scope:{ctx:x}}}),Mo=new D({}),xo=new T({props:{name:"class transformers.FLAVAModel",anchor:"transformers.FLAVAModel",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1223"}}),Eo=new T({props:{name:"forward",anchor:"transformers.FLAVAModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <code>FLAVAFeatureExtractor</code>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FLAVAModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1365",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new mt({props:{$$slots:{default:[Fc]},$$scope:{ctx:x}}}),Po=new Ze({props:{code:`from PIL import Image
import requests
from transformers import FLAVAProcessor, FLAVAModel

model = FLAVAModel.from_pretrained("aps/flava-full")
processor = FLAVAProcessor.from_pretrained("aps/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAProcessor, FLAVAModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FLAVAProcessor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),Io=new T({props:{name:"get_text_features",anchor:"transformers.FLAVAModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1267"}}),Be=new mt({props:{$$slots:{default:[Lc]},$$scope:{ctx:x}}}),jo=new T({props:{name:"get_image_features",anchor:"transformers.FLAVAModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <code>FLAVAFeatureExtractor</code>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1313"}}),Re=new mt({props:{$$slots:{default:[wc]},$$scope:{ctx:x}}}),qo=new D({}),Oo=new T({props:{name:"class transformers.FLAVACodebook",anchor:"transformers.FLAVACodebook",parameters:[{name:"config",val:": FLAVACodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FLAVACodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebookConfig">FLAVACodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1512"}}),So=new T({props:{name:"forward",anchor:"transformers.FLAVACodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1636"}}),Bo=new T({props:{name:"get_codebook_indices",anchor:"transformers.FLAVACodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1603"}}),Ro=new T({props:{name:"get_codebook_probs",anchor:"transformers.FLAVACodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1632"}}),Ho=new D({}),Uo=new T({props:{name:"class transformers.FLAVATextModel",anchor:"transformers.FLAVATextModel",parameters:[{name:"config",val:": FLAVATextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1020"}}),Jo=new T({props:{name:"forward",anchor:"transformers.FLAVATextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVATextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVATextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVATextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVATextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVATextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVATextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1050",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig"
>FLAVATextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new mt({props:{$$slots:{default:[kc]},$$scope:{ctx:x}}}),Qo=new Ze({props:{code:`from transformers import BertTokenizer, FLAVATextModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVATextModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVATextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Xo=new D({}),Yo=new T({props:{name:"class transformers.FLAVAImageModel",anchor:"transformers.FLAVAImageModel",parameters:[{name:"config",val:": FLAVAImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L921"}}),ot=new T({props:{name:"forward",anchor:"transformers.FLAVAImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <code>FLAVAFeatureExtractor</code>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L953",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig"
>FLAVAImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new mt({props:{$$slots:{default:[yc]},$$scope:{ctx:x}}}),tt=new Ze({props:{code:`from transformers import FLAVAFeatureExtractor, FLAVAImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FLAVAFeatureExtractor.from_pretrained("aps/flava-full")
model = FLAVAImageModel.from_pretrained("aps/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAFeatureExtractor, FLAVAImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FLAVAFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),nt=new D({}),at=new T({props:{name:"class transformers.FLAVAMultimodalModel",anchor:"transformers.FLAVAMultimodalModel",parameters:[{name:"config",val:": FLAVAMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1125"}}),it=new T({props:{name:"forward",anchor:"transformers.FLAVAMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1152",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig"
>FLAVAMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xe=new mt({props:{$$slots:{default:[Vc]},$$scope:{ctx:x}}}),lt=new Ze({props:{code:`from transformers import BertTokenizer, FLAVAMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVAMultimodalModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVAMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){h=n("meta"),y=d(),g=n("h1"),w=n("a"),k=n("span"),u(L.$$.fragment),f=d(),V=n("span"),Qa=s("FLAVA"),Zn=d(),ne=n("h2"),Me=n("a"),rn=n("span"),u(eo.$$.fragment),Xa=d(),sn=n("span"),Ya=s("Overview"),ea=d(),xe=n("p"),Za=s("The FLAVA model was proposed in "),oo=n("a"),er=s("FLAVA: A Foundational Language And Vision Alignment Model"),or=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),oa=d(),ht=n("p"),tr=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),ta=d(),pt=n("p"),nr=s("The abstract from the paper is the following:"),na=d(),ft=n("p"),ar=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),aa=d(),Ce=n("p"),rr=s("This model was contributed by "),to=n("a"),sr=s("aps"),ir=s("."),ra=d(),ae=n("h2"),ze=n("a"),ln=n("span"),u(no.$$.fragment),lr=d(),dn=n("span"),dr=s("FLAVAConfig"),sa=d(),$=n("div"),u(ao.$$.fragment),cr=d(),Ee=n("p"),gt=n("a"),mr=s("FLAVAConfig"),hr=s(" is the configuration class to store the configuration of a "),ut=n("a"),pr=s("FLAVAModel"),fr=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),gr=d(),re=n("p"),ur=s("Configuration objects inherit from "),_t=n("a"),_r=s("PretrainedConfig"),Ar=s(` and can be used to control the model outputs. Read the
documentation from `),At=n("a"),vr=s("PretrainedConfig"),br=s(" for more information."),Fr=d(),cn=n("p"),Lr=s("Example:"),wr=d(),u(ro.$$.fragment),kr=d(),Pe=n("div"),u(so.$$.fragment),yr=d(),io=n("p"),Vr=s("Instantiate a "),vt=n("a"),Tr=s("FLAVAConfig"),$r=s(` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),ia=d(),se=n("h2"),Ie=n("a"),mn=n("span"),u(lo.$$.fragment),Mr=d(),hn=n("span"),xr=s("FLAVATextConfig"),la=d(),C=n("div"),u(co.$$.fragment),Cr=d(),ie=n("p"),zr=s("This is the configuration class to store the configuration of a "),bt=n("a"),Er=s("FLAVATextModel"),Pr=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),mo=n("a"),Ir=s("full"),jr=s(" architecture."),qr=d(),le=n("p"),Or=s("Configuration objects inherit from "),Ft=n("a"),Nr=s("PretrainedConfig"),Dr=s(` and can be used to control the model outputs. Read the
documentation from `),Lt=n("a"),Wr=s("PretrainedConfig"),Sr=s(" for more information."),Br=d(),pn=n("p"),Rr=s("Example:"),Hr=d(),u(ho.$$.fragment),da=d(),de=n("h2"),je=n("a"),fn=n("span"),u(po.$$.fragment),Ur=d(),gn=n("span"),Gr=s("FLAVAImageConfig"),ca=d(),z=n("div"),u(fo.$$.fragment),Kr=d(),ce=n("p"),Jr=s("This is the configuration class to store the configuration of a "),wt=n("a"),Qr=s("FLAVAImageModel"),Xr=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),go=n("a"),Yr=s("full"),Zr=s(" architecture."),es=d(),me=n("p"),os=s("Configuration objects inherit from "),kt=n("a"),ts=s("PretrainedConfig"),ns=s(` and can be used to control the model outputs. Read the
documentation from `),yt=n("a"),as=s("PretrainedConfig"),rs=s(" for more information."),ss=d(),un=n("p"),is=s("Example:"),ls=d(),u(uo.$$.fragment),ma=d(),he=n("h2"),qe=n("a"),_n=n("span"),u(_o.$$.fragment),ds=d(),An=n("span"),cs=s("FLAVAMultimodalConfig"),ha=d(),E=n("div"),u(Ao.$$.fragment),ms=d(),pe=n("p"),hs=s("This is the configuration class to store the configuration of a "),Vt=n("a"),ps=s("FLAVAMultimodalModel"),fs=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),vo=n("a"),gs=s("full"),us=s(" architecture."),_s=d(),fe=n("p"),As=s("Configuration objects inherit from "),Tt=n("a"),vs=s("PretrainedConfig"),bs=s(` and can be used to control the model outputs. Read the
documentation from `),$t=n("a"),Fs=s("PretrainedConfig"),Ls=s(" for more information."),ws=d(),vn=n("p"),ks=s("Example:"),ys=d(),u(bo.$$.fragment),pa=d(),ge=n("h2"),Oe=n("a"),bn=n("span"),u(Fo.$$.fragment),Vs=d(),Fn=n("span"),Ts=s("FLAVACodebookConfig"),fa=d(),Lo=n("div"),u(wo.$$.fragment),ga=d(),ue=n("h2"),Ne=n("a"),Ln=n("span"),u(ko.$$.fragment),$s=d(),wn=n("span"),Ms=s("FLAVAForPretraining"),ua=d(),N=n("div"),u(yo.$$.fragment),xs=d(),kn=n("p"),Cs=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),zs=d(),Vo=n("p"),Es=s("This model is a PyTorch "),To=n("a"),Ps=s("torch.nn.Module"),Is=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),js=d(),U=n("div"),u($o.$$.fragment),qs=d(),_e=n("p"),Os=s("The "),Mt=n("a"),Ns=s("FLAVAForPretraining"),Ds=s(" forward method, overrides the "),yn=n("code"),Ws=s("__call__"),Ss=s(" special method."),Bs=d(),u(De.$$.fragment),_a=d(),Ae=n("h2"),We=n("a"),Vn=n("span"),u(Mo.$$.fragment),Rs=d(),Tn=n("span"),Hs=s("FLAVAModel"),Aa=d(),P=n("div"),u(xo.$$.fragment),Us=d(),Co=n("p"),Gs=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),zo=n("a"),Ks=s("torch.nn.Module"),Js=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Qs=d(),I=n("div"),u(Eo.$$.fragment),Xs=d(),ve=n("p"),Ys=s("The "),xt=n("a"),Zs=s("FLAVAModel"),ei=s(" forward method, overrides the "),$n=n("code"),oi=s("__call__"),ti=s(" special method."),ni=d(),u(Se.$$.fragment),ai=d(),Mn=n("p"),ri=s("Examples:"),si=d(),u(Po.$$.fragment),ii=d(),G=n("div"),u(Io.$$.fragment),li=d(),be=n("p"),di=s("The "),Ct=n("a"),ci=s("FLAVAModel"),mi=s(" forward method, overrides the "),xn=n("code"),hi=s("__call__"),pi=s(" special method."),fi=d(),u(Be.$$.fragment),gi=d(),K=n("div"),u(jo.$$.fragment),ui=d(),Fe=n("p"),_i=s("The "),zt=n("a"),Ai=s("FLAVAModel"),vi=s(" forward method, overrides the "),Cn=n("code"),bi=s("__call__"),Fi=s(" special method."),Li=d(),u(Re.$$.fragment),va=d(),Le=n("h2"),He=n("a"),zn=n("span"),u(qo.$$.fragment),wi=d(),En=n("span"),ki=s("FLAVACodebook"),ba=d(),M=n("div"),u(Oo.$$.fragment),yi=d(),No=n("p"),Vi=s(`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),Pn=n("code"),Ti=s("get_codebook_indices"),$i=s(" to get image tokens for an image."),Mi=d(),Do=n("p"),xi=s("This model is a PyTorch "),Wo=n("a"),Ci=s("torch.nn.Module"),zi=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ei=d(),Et=n("div"),u(So.$$.fragment),Pi=d(),Pt=n("div"),u(Bo.$$.fragment),Ii=d(),It=n("div"),u(Ro.$$.fragment),Fa=d(),we=n("h2"),Ue=n("a"),In=n("span"),u(Ho.$$.fragment),ji=d(),jn=n("span"),qi=s("FLAVATextModel"),La=d(),B=n("div"),u(Uo.$$.fragment),Oi=d(),Go=n("p"),Ni=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ko=n("a"),Di=s("torch.nn.Module"),Wi=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Si=d(),j=n("div"),u(Jo.$$.fragment),Bi=d(),ke=n("p"),Ri=s("The "),jt=n("a"),Hi=s("FLAVATextModel"),Ui=s(" forward method, overrides the "),qn=n("code"),Gi=s("__call__"),Ki=s(" special method."),Ji=d(),u(Ge.$$.fragment),Qi=d(),On=n("p"),Xi=s("Example:"),Yi=d(),u(Qo.$$.fragment),wa=d(),ye=n("h2"),Ke=n("a"),Nn=n("span"),u(Xo.$$.fragment),Zi=d(),Dn=n("span"),el=s("FLAVAImageModel"),ka=d(),R=n("div"),u(Yo.$$.fragment),ol=d(),Zo=n("p"),tl=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),et=n("a"),nl=s("torch.nn.Module"),al=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),rl=d(),q=n("div"),u(ot.$$.fragment),sl=d(),Ve=n("p"),il=s("The "),qt=n("a"),ll=s("FLAVAImageModel"),dl=s(" forward method, overrides the "),Wn=n("code"),cl=s("__call__"),ml=s(" special method."),hl=d(),u(Je.$$.fragment),pl=d(),Sn=n("p"),fl=s("Example:"),gl=d(),u(tt.$$.fragment),ya=d(),Te=n("h2"),Qe=n("a"),Bn=n("span"),u(nt.$$.fragment),ul=d(),Rn=n("span"),_l=s("FLAVAMultimodalModel"),Va=d(),H=n("div"),u(at.$$.fragment),Al=d(),rt=n("p"),vl=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),st=n("a"),bl=s("torch.nn.Module"),Fl=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ll=d(),O=n("div"),u(it.$$.fragment),wl=d(),$e=n("p"),kl=s("The "),Ot=n("a"),yl=s("FLAVAMultimodalModel"),Vl=s(" forward method, overrides the "),Hn=n("code"),Tl=s("__call__"),$l=s(" special method."),Ml=d(),u(Xe.$$.fragment),xl=d(),Un=n("p"),Cl=s("Example:"),zl=d(),u(lt.$$.fragment),this.h()},l(o){const m=Ac('[data-svelte="svelte-1phssyn"]',document.head);h=a(m,"META",{name:!0,content:!0}),m.forEach(t),y=c(o),g=a(o,"H1",{class:!0});var dt=r(g);w=a(dt,"A",{id:!0,class:!0,href:!0});var Gn=r(w);k=a(Gn,"SPAN",{});var Kn=r(k);_(L.$$.fragment,Kn),Kn.forEach(t),Gn.forEach(t),f=c(dt),V=a(dt,"SPAN",{});var Jn=r(V);Qa=i(Jn,"FLAVA"),Jn.forEach(t),dt.forEach(t),Zn=c(o),ne=a(o,"H2",{class:!0});var ct=r(ne);Me=a(ct,"A",{id:!0,class:!0,href:!0});var Qn=r(Me);rn=a(Qn,"SPAN",{});var Xn=r(rn);_(eo.$$.fragment,Xn),Xn.forEach(t),Qn.forEach(t),Xa=c(ct),sn=a(ct,"SPAN",{});var El=r(sn);Ya=i(El,"Overview"),El.forEach(t),ct.forEach(t),ea=c(o),xe=a(o,"P",{});var $a=r(xe);Za=i($a,"The FLAVA model was proposed in "),oo=a($a,"A",{href:!0,rel:!0});var Pl=r(oo);er=i(Pl,"FLAVA: A Foundational Language And Vision Alignment Model"),Pl.forEach(t),or=i($a," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),$a.forEach(t),oa=c(o),ht=a(o,"P",{});var Il=r(ht);tr=i(Il,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Il.forEach(t),ta=c(o),pt=a(o,"P",{});var jl=r(pt);nr=i(jl,"The abstract from the paper is the following:"),jl.forEach(t),na=c(o),ft=a(o,"P",{});var ql=r(ft);ar=i(ql,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),ql.forEach(t),aa=c(o),Ce=a(o,"P",{});var Ma=r(Ce);rr=i(Ma,"This model was contributed by "),to=a(Ma,"A",{href:!0,rel:!0});var Ol=r(to);sr=i(Ol,"aps"),Ol.forEach(t),ir=i(Ma,"."),Ma.forEach(t),ra=c(o),ae=a(o,"H2",{class:!0});var xa=r(ae);ze=a(xa,"A",{id:!0,class:!0,href:!0});var Nl=r(ze);ln=a(Nl,"SPAN",{});var Dl=r(ln);_(no.$$.fragment,Dl),Dl.forEach(t),Nl.forEach(t),lr=c(xa),dn=a(xa,"SPAN",{});var Wl=r(dn);dr=i(Wl,"FLAVAConfig"),Wl.forEach(t),xa.forEach(t),sa=c(o),$=a(o,"DIV",{class:!0});var W=r($);_(ao.$$.fragment,W),cr=c(W),Ee=a(W,"P",{});var Yn=r(Ee);gt=a(Yn,"A",{href:!0});var Sl=r(gt);mr=i(Sl,"FLAVAConfig"),Sl.forEach(t),hr=i(Yn," is the configuration class to store the configuration of a "),ut=a(Yn,"A",{href:!0});var Bl=r(ut);pr=i(Bl,"FLAVAModel"),Bl.forEach(t),fr=i(Yn,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),Yn.forEach(t),gr=c(W),re=a(W,"P",{});var Nt=r(re);ur=i(Nt,"Configuration objects inherit from "),_t=a(Nt,"A",{href:!0});var Rl=r(_t);_r=i(Rl,"PretrainedConfig"),Rl.forEach(t),Ar=i(Nt,` and can be used to control the model outputs. Read the
documentation from `),At=a(Nt,"A",{href:!0});var Hl=r(At);vr=i(Hl,"PretrainedConfig"),Hl.forEach(t),br=i(Nt," for more information."),Nt.forEach(t),Fr=c(W),cn=a(W,"P",{});var Ul=r(cn);Lr=i(Ul,"Example:"),Ul.forEach(t),wr=c(W),_(ro.$$.fragment,W),kr=c(W),Pe=a(W,"DIV",{class:!0});var Ca=r(Pe);_(so.$$.fragment,Ca),yr=c(Ca),io=a(Ca,"P",{});var za=r(io);Vr=i(za,"Instantiate a "),vt=a(za,"A",{href:!0});var Gl=r(vt);Tr=i(Gl,"FLAVAConfig"),Gl.forEach(t),$r=i(za,` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),za.forEach(t),Ca.forEach(t),W.forEach(t),ia=c(o),se=a(o,"H2",{class:!0});var Ea=r(se);Ie=a(Ea,"A",{id:!0,class:!0,href:!0});var Kl=r(Ie);mn=a(Kl,"SPAN",{});var Jl=r(mn);_(lo.$$.fragment,Jl),Jl.forEach(t),Kl.forEach(t),Mr=c(Ea),hn=a(Ea,"SPAN",{});var Ql=r(hn);xr=i(Ql,"FLAVATextConfig"),Ql.forEach(t),Ea.forEach(t),la=c(o),C=a(o,"DIV",{class:!0});var J=r(C);_(co.$$.fragment,J),Cr=c(J),ie=a(J,"P",{});var Dt=r(ie);zr=i(Dt,"This is the configuration class to store the configuration of a "),bt=a(Dt,"A",{href:!0});var Xl=r(bt);Er=i(Xl,"FLAVATextModel"),Xl.forEach(t),Pr=i(Dt,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),mo=a(Dt,"A",{href:!0,rel:!0});var Yl=r(mo);Ir=i(Yl,"full"),Yl.forEach(t),jr=i(Dt," architecture."),Dt.forEach(t),qr=c(J),le=a(J,"P",{});var Wt=r(le);Or=i(Wt,"Configuration objects inherit from "),Ft=a(Wt,"A",{href:!0});var Zl=r(Ft);Nr=i(Zl,"PretrainedConfig"),Zl.forEach(t),Dr=i(Wt,` and can be used to control the model outputs. Read the
documentation from `),Lt=a(Wt,"A",{href:!0});var ed=r(Lt);Wr=i(ed,"PretrainedConfig"),ed.forEach(t),Sr=i(Wt," for more information."),Wt.forEach(t),Br=c(J),pn=a(J,"P",{});var od=r(pn);Rr=i(od,"Example:"),od.forEach(t),Hr=c(J),_(ho.$$.fragment,J),J.forEach(t),da=c(o),de=a(o,"H2",{class:!0});var Pa=r(de);je=a(Pa,"A",{id:!0,class:!0,href:!0});var td=r(je);fn=a(td,"SPAN",{});var nd=r(fn);_(po.$$.fragment,nd),nd.forEach(t),td.forEach(t),Ur=c(Pa),gn=a(Pa,"SPAN",{});var ad=r(gn);Gr=i(ad,"FLAVAImageConfig"),ad.forEach(t),Pa.forEach(t),ca=c(o),z=a(o,"DIV",{class:!0});var Q=r(z);_(fo.$$.fragment,Q),Kr=c(Q),ce=a(Q,"P",{});var St=r(ce);Jr=i(St,"This is the configuration class to store the configuration of a "),wt=a(St,"A",{href:!0});var rd=r(wt);Qr=i(rd,"FLAVAImageModel"),rd.forEach(t),Xr=i(St,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),go=a(St,"A",{href:!0,rel:!0});var sd=r(go);Yr=i(sd,"full"),sd.forEach(t),Zr=i(St," architecture."),St.forEach(t),es=c(Q),me=a(Q,"P",{});var Bt=r(me);os=i(Bt,"Configuration objects inherit from "),kt=a(Bt,"A",{href:!0});var id=r(kt);ts=i(id,"PretrainedConfig"),id.forEach(t),ns=i(Bt,` and can be used to control the model outputs. Read the
documentation from `),yt=a(Bt,"A",{href:!0});var ld=r(yt);as=i(ld,"PretrainedConfig"),ld.forEach(t),rs=i(Bt," for more information."),Bt.forEach(t),ss=c(Q),un=a(Q,"P",{});var dd=r(un);is=i(dd,"Example:"),dd.forEach(t),ls=c(Q),_(uo.$$.fragment,Q),Q.forEach(t),ma=c(o),he=a(o,"H2",{class:!0});var Ia=r(he);qe=a(Ia,"A",{id:!0,class:!0,href:!0});var cd=r(qe);_n=a(cd,"SPAN",{});var md=r(_n);_(_o.$$.fragment,md),md.forEach(t),cd.forEach(t),ds=c(Ia),An=a(Ia,"SPAN",{});var hd=r(An);cs=i(hd,"FLAVAMultimodalConfig"),hd.forEach(t),Ia.forEach(t),ha=c(o),E=a(o,"DIV",{class:!0});var X=r(E);_(Ao.$$.fragment,X),ms=c(X),pe=a(X,"P",{});var Rt=r(pe);hs=i(Rt,"This is the configuration class to store the configuration of a "),Vt=a(Rt,"A",{href:!0});var pd=r(Vt);ps=i(pd,"FLAVAMultimodalModel"),pd.forEach(t),fs=i(Rt,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),vo=a(Rt,"A",{href:!0,rel:!0});var fd=r(vo);gs=i(fd,"full"),fd.forEach(t),us=i(Rt," architecture."),Rt.forEach(t),_s=c(X),fe=a(X,"P",{});var Ht=r(fe);As=i(Ht,"Configuration objects inherit from "),Tt=a(Ht,"A",{href:!0});var gd=r(Tt);vs=i(gd,"PretrainedConfig"),gd.forEach(t),bs=i(Ht,` and can be used to control the model outputs. Read the
documentation from `),$t=a(Ht,"A",{href:!0});var ud=r($t);Fs=i(ud,"PretrainedConfig"),ud.forEach(t),Ls=i(Ht," for more information."),Ht.forEach(t),ws=c(X),vn=a(X,"P",{});var _d=r(vn);ks=i(_d,"Example:"),_d.forEach(t),ys=c(X),_(bo.$$.fragment,X),X.forEach(t),pa=c(o),ge=a(o,"H2",{class:!0});var ja=r(ge);Oe=a(ja,"A",{id:!0,class:!0,href:!0});var Ad=r(Oe);bn=a(Ad,"SPAN",{});var vd=r(bn);_(Fo.$$.fragment,vd),vd.forEach(t),Ad.forEach(t),Vs=c(ja),Fn=a(ja,"SPAN",{});var bd=r(Fn);Ts=i(bd,"FLAVACodebookConfig"),bd.forEach(t),ja.forEach(t),fa=c(o),Lo=a(o,"DIV",{class:!0});var Fd=r(Lo);_(wo.$$.fragment,Fd),Fd.forEach(t),ga=c(o),ue=a(o,"H2",{class:!0});var qa=r(ue);Ne=a(qa,"A",{id:!0,class:!0,href:!0});var Ld=r(Ne);Ln=a(Ld,"SPAN",{});var wd=r(Ln);_(ko.$$.fragment,wd),wd.forEach(t),Ld.forEach(t),$s=c(qa),wn=a(qa,"SPAN",{});var kd=r(wn);Ms=i(kd,"FLAVAForPretraining"),kd.forEach(t),qa.forEach(t),ua=c(o),N=a(o,"DIV",{class:!0});var Ye=r(N);_(yo.$$.fragment,Ye),xs=c(Ye),kn=a(Ye,"P",{});var yd=r(kn);Cs=i(yd,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),yd.forEach(t),zs=c(Ye),Vo=a(Ye,"P",{});var Oa=r(Vo);Es=i(Oa,"This model is a PyTorch "),To=a(Oa,"A",{href:!0,rel:!0});var Vd=r(To);Ps=i(Vd,"torch.nn.Module"),Vd.forEach(t),Is=i(Oa,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Oa.forEach(t),js=c(Ye),U=a(Ye,"DIV",{class:!0});var Ut=r(U);_($o.$$.fragment,Ut),qs=c(Ut),_e=a(Ut,"P",{});var Gt=r(_e);Os=i(Gt,"The "),Mt=a(Gt,"A",{href:!0});var Td=r(Mt);Ns=i(Td,"FLAVAForPretraining"),Td.forEach(t),Ds=i(Gt," forward method, overrides the "),yn=a(Gt,"CODE",{});var $d=r(yn);Ws=i($d,"__call__"),$d.forEach(t),Ss=i(Gt," special method."),Gt.forEach(t),Bs=c(Ut),_(De.$$.fragment,Ut),Ut.forEach(t),Ye.forEach(t),_a=c(o),Ae=a(o,"H2",{class:!0});var Na=r(Ae);We=a(Na,"A",{id:!0,class:!0,href:!0});var Md=r(We);Vn=a(Md,"SPAN",{});var xd=r(Vn);_(Mo.$$.fragment,xd),xd.forEach(t),Md.forEach(t),Rs=c(Na),Tn=a(Na,"SPAN",{});var Cd=r(Tn);Hs=i(Cd,"FLAVAModel"),Cd.forEach(t),Na.forEach(t),Aa=c(o),P=a(o,"DIV",{class:!0});var Y=r(P);_(xo.$$.fragment,Y),Us=c(Y),Co=a(Y,"P",{});var Da=r(Co);Gs=i(Da,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),zo=a(Da,"A",{href:!0,rel:!0});var zd=r(zo);Ks=i(zd,"torch.nn.Module"),zd.forEach(t),Js=i(Da,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Da.forEach(t),Qs=c(Y),I=a(Y,"DIV",{class:!0});var Z=r(I);_(Eo.$$.fragment,Z),Xs=c(Z),ve=a(Z,"P",{});var Kt=r(ve);Ys=i(Kt,"The "),xt=a(Kt,"A",{href:!0});var Ed=r(xt);Zs=i(Ed,"FLAVAModel"),Ed.forEach(t),ei=i(Kt," forward method, overrides the "),$n=a(Kt,"CODE",{});var Pd=r($n);oi=i(Pd,"__call__"),Pd.forEach(t),ti=i(Kt," special method."),Kt.forEach(t),ni=c(Z),_(Se.$$.fragment,Z),ai=c(Z),Mn=a(Z,"P",{});var Id=r(Mn);ri=i(Id,"Examples:"),Id.forEach(t),si=c(Z),_(Po.$$.fragment,Z),Z.forEach(t),ii=c(Y),G=a(Y,"DIV",{class:!0});var Jt=r(G);_(Io.$$.fragment,Jt),li=c(Jt),be=a(Jt,"P",{});var Qt=r(be);di=i(Qt,"The "),Ct=a(Qt,"A",{href:!0});var jd=r(Ct);ci=i(jd,"FLAVAModel"),jd.forEach(t),mi=i(Qt," forward method, overrides the "),xn=a(Qt,"CODE",{});var qd=r(xn);hi=i(qd,"__call__"),qd.forEach(t),pi=i(Qt," special method."),Qt.forEach(t),fi=c(Jt),_(Be.$$.fragment,Jt),Jt.forEach(t),gi=c(Y),K=a(Y,"DIV",{class:!0});var Xt=r(K);_(jo.$$.fragment,Xt),ui=c(Xt),Fe=a(Xt,"P",{});var Yt=r(Fe);_i=i(Yt,"The "),zt=a(Yt,"A",{href:!0});var Od=r(zt);Ai=i(Od,"FLAVAModel"),Od.forEach(t),vi=i(Yt," forward method, overrides the "),Cn=a(Yt,"CODE",{});var Nd=r(Cn);bi=i(Nd,"__call__"),Nd.forEach(t),Fi=i(Yt," special method."),Yt.forEach(t),Li=c(Xt),_(Re.$$.fragment,Xt),Xt.forEach(t),Y.forEach(t),va=c(o),Le=a(o,"H2",{class:!0});var Wa=r(Le);He=a(Wa,"A",{id:!0,class:!0,href:!0});var Dd=r(He);zn=a(Dd,"SPAN",{});var Wd=r(zn);_(qo.$$.fragment,Wd),Wd.forEach(t),Dd.forEach(t),wi=c(Wa),En=a(Wa,"SPAN",{});var Sd=r(En);ki=i(Sd,"FLAVACodebook"),Sd.forEach(t),Wa.forEach(t),ba=c(o),M=a(o,"DIV",{class:!0});var S=r(M);_(Oo.$$.fragment,S),yi=c(S),No=a(S,"P",{});var Sa=r(No);Vi=i(Sa,`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),Pn=a(Sa,"CODE",{});var Bd=r(Pn);Ti=i(Bd,"get_codebook_indices"),Bd.forEach(t),$i=i(Sa," to get image tokens for an image."),Sa.forEach(t),Mi=c(S),Do=a(S,"P",{});var Ba=r(Do);xi=i(Ba,"This model is a PyTorch "),Wo=a(Ba,"A",{href:!0,rel:!0});var Rd=r(Wo);Ci=i(Rd,"torch.nn.Module"),Rd.forEach(t),zi=i(Ba,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ba.forEach(t),Ei=c(S),Et=a(S,"DIV",{class:!0});var Hd=r(Et);_(So.$$.fragment,Hd),Hd.forEach(t),Pi=c(S),Pt=a(S,"DIV",{class:!0});var Ud=r(Pt);_(Bo.$$.fragment,Ud),Ud.forEach(t),Ii=c(S),It=a(S,"DIV",{class:!0});var Gd=r(It);_(Ro.$$.fragment,Gd),Gd.forEach(t),S.forEach(t),Fa=c(o),we=a(o,"H2",{class:!0});var Ra=r(we);Ue=a(Ra,"A",{id:!0,class:!0,href:!0});var Kd=r(Ue);In=a(Kd,"SPAN",{});var Jd=r(In);_(Ho.$$.fragment,Jd),Jd.forEach(t),Kd.forEach(t),ji=c(Ra),jn=a(Ra,"SPAN",{});var Qd=r(jn);qi=i(Qd,"FLAVATextModel"),Qd.forEach(t),Ra.forEach(t),La=c(o),B=a(o,"DIV",{class:!0});var Zt=r(B);_(Uo.$$.fragment,Zt),Oi=c(Zt),Go=a(Zt,"P",{});var Ha=r(Go);Ni=i(Ha,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ko=a(Ha,"A",{href:!0,rel:!0});var Xd=r(Ko);Di=i(Xd,"torch.nn.Module"),Xd.forEach(t),Wi=i(Ha,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ha.forEach(t),Si=c(Zt),j=a(Zt,"DIV",{class:!0});var ee=r(j);_(Jo.$$.fragment,ee),Bi=c(ee),ke=a(ee,"P",{});var en=r(ke);Ri=i(en,"The "),jt=a(en,"A",{href:!0});var Yd=r(jt);Hi=i(Yd,"FLAVATextModel"),Yd.forEach(t),Ui=i(en," forward method, overrides the "),qn=a(en,"CODE",{});var Zd=r(qn);Gi=i(Zd,"__call__"),Zd.forEach(t),Ki=i(en," special method."),en.forEach(t),Ji=c(ee),_(Ge.$$.fragment,ee),Qi=c(ee),On=a(ee,"P",{});var ec=r(On);Xi=i(ec,"Example:"),ec.forEach(t),Yi=c(ee),_(Qo.$$.fragment,ee),ee.forEach(t),Zt.forEach(t),wa=c(o),ye=a(o,"H2",{class:!0});var Ua=r(ye);Ke=a(Ua,"A",{id:!0,class:!0,href:!0});var oc=r(Ke);Nn=a(oc,"SPAN",{});var tc=r(Nn);_(Xo.$$.fragment,tc),tc.forEach(t),oc.forEach(t),Zi=c(Ua),Dn=a(Ua,"SPAN",{});var nc=r(Dn);el=i(nc,"FLAVAImageModel"),nc.forEach(t),Ua.forEach(t),ka=c(o),R=a(o,"DIV",{class:!0});var on=r(R);_(Yo.$$.fragment,on),ol=c(on),Zo=a(on,"P",{});var Ga=r(Zo);tl=i(Ga,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),et=a(Ga,"A",{href:!0,rel:!0});var ac=r(et);nl=i(ac,"torch.nn.Module"),ac.forEach(t),al=i(Ga,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ga.forEach(t),rl=c(on),q=a(on,"DIV",{class:!0});var oe=r(q);_(ot.$$.fragment,oe),sl=c(oe),Ve=a(oe,"P",{});var tn=r(Ve);il=i(tn,"The "),qt=a(tn,"A",{href:!0});var rc=r(qt);ll=i(rc,"FLAVAImageModel"),rc.forEach(t),dl=i(tn," forward method, overrides the "),Wn=a(tn,"CODE",{});var sc=r(Wn);cl=i(sc,"__call__"),sc.forEach(t),ml=i(tn," special method."),tn.forEach(t),hl=c(oe),_(Je.$$.fragment,oe),pl=c(oe),Sn=a(oe,"P",{});var ic=r(Sn);fl=i(ic,"Example:"),ic.forEach(t),gl=c(oe),_(tt.$$.fragment,oe),oe.forEach(t),on.forEach(t),ya=c(o),Te=a(o,"H2",{class:!0});var Ka=r(Te);Qe=a(Ka,"A",{id:!0,class:!0,href:!0});var lc=r(Qe);Bn=a(lc,"SPAN",{});var dc=r(Bn);_(nt.$$.fragment,dc),dc.forEach(t),lc.forEach(t),ul=c(Ka),Rn=a(Ka,"SPAN",{});var cc=r(Rn);_l=i(cc,"FLAVAMultimodalModel"),cc.forEach(t),Ka.forEach(t),Va=c(o),H=a(o,"DIV",{class:!0});var nn=r(H);_(at.$$.fragment,nn),Al=c(nn),rt=a(nn,"P",{});var Ja=r(rt);vl=i(Ja,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),st=a(Ja,"A",{href:!0,rel:!0});var mc=r(st);bl=i(mc,"torch.nn.Module"),mc.forEach(t),Fl=i(Ja,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ja.forEach(t),Ll=c(nn),O=a(nn,"DIV",{class:!0});var te=r(O);_(it.$$.fragment,te),wl=c(te),$e=a(te,"P",{});var an=r($e);kl=i(an,"The "),Ot=a(an,"A",{href:!0});var hc=r(Ot);yl=i(hc,"FLAVAMultimodalModel"),hc.forEach(t),Vl=i(an," forward method, overrides the "),Hn=a(an,"CODE",{});var pc=r(Hn);Tl=i(pc,"__call__"),pc.forEach(t),$l=i(an," special method."),an.forEach(t),Ml=c(te),_(Xe.$$.fragment,te),xl=c(te),Un=a(te,"P",{});var fc=r(Un);Cl=i(fc,"Example:"),fc.forEach(t),zl=c(te),_(lt.$$.fragment,te),te.forEach(t),nn.forEach(t),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify($c)),l(w,"id","flava"),l(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(w,"href","#flava"),l(g,"class","relative group"),l(Me,"id","overview"),l(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Me,"href","#overview"),l(ne,"class","relative group"),l(oo,"href","https://arxiv.org/abs/2112.04482"),l(oo,"rel","nofollow"),l(to,"href","https://huggingface.co/aps"),l(to,"rel","nofollow"),l(ze,"id","transformers.FLAVAConfig"),l(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ze,"href","#transformers.FLAVAConfig"),l(ae,"class","relative group"),l(gt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(ut,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(_t,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(At,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(vt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ie,"id","transformers.FLAVATextConfig"),l(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ie,"href","#transformers.FLAVATextConfig"),l(se,"class","relative group"),l(bt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(mo,"href","https://huggingface.co/aps/flava-full"),l(mo,"rel","nofollow"),l(Ft,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Lt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(je,"id","transformers.FLAVAImageConfig"),l(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(je,"href","#transformers.FLAVAImageConfig"),l(de,"class","relative group"),l(wt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(go,"href","https://huggingface.co/aps/flava-full"),l(go,"rel","nofollow"),l(kt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(yt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(qe,"id","transformers.FLAVAMultimodalConfig"),l(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(qe,"href","#transformers.FLAVAMultimodalConfig"),l(he,"class","relative group"),l(Vt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(vo,"href","https://huggingface.co/aps/flava-full"),l(vo,"rel","nofollow"),l(Tt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l($t,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Oe,"id","transformers.FLAVACodebookConfig"),l(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Oe,"href","#transformers.FLAVACodebookConfig"),l(ge,"class","relative group"),l(Lo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ne,"id","transformers.FLAVAForPretraining"),l(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ne,"href","#transformers.FLAVAForPretraining"),l(ue,"class","relative group"),l(To,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(To,"rel","nofollow"),l(Mt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAForPretraining"),l(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(We,"id","transformers.FLAVAModel"),l(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(We,"href","#transformers.FLAVAModel"),l(Ae,"class","relative group"),l(zo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(zo,"rel","nofollow"),l(xt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ct,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(zt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(He,"id","transformers.FLAVACodebook"),l(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(He,"href","#transformers.FLAVACodebook"),l(Le,"class","relative group"),l(Wo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Wo,"rel","nofollow"),l(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(It,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ue,"id","transformers.FLAVATextModel"),l(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ue,"href","#transformers.FLAVATextModel"),l(we,"class","relative group"),l(Ko,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ko,"rel","nofollow"),l(jt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ke,"id","transformers.FLAVAImageModel"),l(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ke,"href","#transformers.FLAVAImageModel"),l(ye,"class","relative group"),l(et,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(et,"rel","nofollow"),l(qt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Qe,"id","transformers.FLAVAMultimodalModel"),l(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Qe,"href","#transformers.FLAVAMultimodalModel"),l(Te,"class","relative group"),l(st,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(st,"rel","nofollow"),l(Ot,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),p(o,y,m),p(o,g,m),e(g,w),e(w,k),A(L,k,null),e(g,f),e(g,V),e(V,Qa),p(o,Zn,m),p(o,ne,m),e(ne,Me),e(Me,rn),A(eo,rn,null),e(ne,Xa),e(ne,sn),e(sn,Ya),p(o,ea,m),p(o,xe,m),e(xe,Za),e(xe,oo),e(oo,er),e(xe,or),p(o,oa,m),p(o,ht,m),e(ht,tr),p(o,ta,m),p(o,pt,m),e(pt,nr),p(o,na,m),p(o,ft,m),e(ft,ar),p(o,aa,m),p(o,Ce,m),e(Ce,rr),e(Ce,to),e(to,sr),e(Ce,ir),p(o,ra,m),p(o,ae,m),e(ae,ze),e(ze,ln),A(no,ln,null),e(ae,lr),e(ae,dn),e(dn,dr),p(o,sa,m),p(o,$,m),A(ao,$,null),e($,cr),e($,Ee),e(Ee,gt),e(gt,mr),e(Ee,hr),e(Ee,ut),e(ut,pr),e(Ee,fr),e($,gr),e($,re),e(re,ur),e(re,_t),e(_t,_r),e(re,Ar),e(re,At),e(At,vr),e(re,br),e($,Fr),e($,cn),e(cn,Lr),e($,wr),A(ro,$,null),e($,kr),e($,Pe),A(so,Pe,null),e(Pe,yr),e(Pe,io),e(io,Vr),e(io,vt),e(vt,Tr),e(io,$r),p(o,ia,m),p(o,se,m),e(se,Ie),e(Ie,mn),A(lo,mn,null),e(se,Mr),e(se,hn),e(hn,xr),p(o,la,m),p(o,C,m),A(co,C,null),e(C,Cr),e(C,ie),e(ie,zr),e(ie,bt),e(bt,Er),e(ie,Pr),e(ie,mo),e(mo,Ir),e(ie,jr),e(C,qr),e(C,le),e(le,Or),e(le,Ft),e(Ft,Nr),e(le,Dr),e(le,Lt),e(Lt,Wr),e(le,Sr),e(C,Br),e(C,pn),e(pn,Rr),e(C,Hr),A(ho,C,null),p(o,da,m),p(o,de,m),e(de,je),e(je,fn),A(po,fn,null),e(de,Ur),e(de,gn),e(gn,Gr),p(o,ca,m),p(o,z,m),A(fo,z,null),e(z,Kr),e(z,ce),e(ce,Jr),e(ce,wt),e(wt,Qr),e(ce,Xr),e(ce,go),e(go,Yr),e(ce,Zr),e(z,es),e(z,me),e(me,os),e(me,kt),e(kt,ts),e(me,ns),e(me,yt),e(yt,as),e(me,rs),e(z,ss),e(z,un),e(un,is),e(z,ls),A(uo,z,null),p(o,ma,m),p(o,he,m),e(he,qe),e(qe,_n),A(_o,_n,null),e(he,ds),e(he,An),e(An,cs),p(o,ha,m),p(o,E,m),A(Ao,E,null),e(E,ms),e(E,pe),e(pe,hs),e(pe,Vt),e(Vt,ps),e(pe,fs),e(pe,vo),e(vo,gs),e(pe,us),e(E,_s),e(E,fe),e(fe,As),e(fe,Tt),e(Tt,vs),e(fe,bs),e(fe,$t),e($t,Fs),e(fe,Ls),e(E,ws),e(E,vn),e(vn,ks),e(E,ys),A(bo,E,null),p(o,pa,m),p(o,ge,m),e(ge,Oe),e(Oe,bn),A(Fo,bn,null),e(ge,Vs),e(ge,Fn),e(Fn,Ts),p(o,fa,m),p(o,Lo,m),A(wo,Lo,null),p(o,ga,m),p(o,ue,m),e(ue,Ne),e(Ne,Ln),A(ko,Ln,null),e(ue,$s),e(ue,wn),e(wn,Ms),p(o,ua,m),p(o,N,m),A(yo,N,null),e(N,xs),e(N,kn),e(kn,Cs),e(N,zs),e(N,Vo),e(Vo,Es),e(Vo,To),e(To,Ps),e(Vo,Is),e(N,js),e(N,U),A($o,U,null),e(U,qs),e(U,_e),e(_e,Os),e(_e,Mt),e(Mt,Ns),e(_e,Ds),e(_e,yn),e(yn,Ws),e(_e,Ss),e(U,Bs),A(De,U,null),p(o,_a,m),p(o,Ae,m),e(Ae,We),e(We,Vn),A(Mo,Vn,null),e(Ae,Rs),e(Ae,Tn),e(Tn,Hs),p(o,Aa,m),p(o,P,m),A(xo,P,null),e(P,Us),e(P,Co),e(Co,Gs),e(Co,zo),e(zo,Ks),e(Co,Js),e(P,Qs),e(P,I),A(Eo,I,null),e(I,Xs),e(I,ve),e(ve,Ys),e(ve,xt),e(xt,Zs),e(ve,ei),e(ve,$n),e($n,oi),e(ve,ti),e(I,ni),A(Se,I,null),e(I,ai),e(I,Mn),e(Mn,ri),e(I,si),A(Po,I,null),e(P,ii),e(P,G),A(Io,G,null),e(G,li),e(G,be),e(be,di),e(be,Ct),e(Ct,ci),e(be,mi),e(be,xn),e(xn,hi),e(be,pi),e(G,fi),A(Be,G,null),e(P,gi),e(P,K),A(jo,K,null),e(K,ui),e(K,Fe),e(Fe,_i),e(Fe,zt),e(zt,Ai),e(Fe,vi),e(Fe,Cn),e(Cn,bi),e(Fe,Fi),e(K,Li),A(Re,K,null),p(o,va,m),p(o,Le,m),e(Le,He),e(He,zn),A(qo,zn,null),e(Le,wi),e(Le,En),e(En,ki),p(o,ba,m),p(o,M,m),A(Oo,M,null),e(M,yi),e(M,No),e(No,Vi),e(No,Pn),e(Pn,Ti),e(No,$i),e(M,Mi),e(M,Do),e(Do,xi),e(Do,Wo),e(Wo,Ci),e(Do,zi),e(M,Ei),e(M,Et),A(So,Et,null),e(M,Pi),e(M,Pt),A(Bo,Pt,null),e(M,Ii),e(M,It),A(Ro,It,null),p(o,Fa,m),p(o,we,m),e(we,Ue),e(Ue,In),A(Ho,In,null),e(we,ji),e(we,jn),e(jn,qi),p(o,La,m),p(o,B,m),A(Uo,B,null),e(B,Oi),e(B,Go),e(Go,Ni),e(Go,Ko),e(Ko,Di),e(Go,Wi),e(B,Si),e(B,j),A(Jo,j,null),e(j,Bi),e(j,ke),e(ke,Ri),e(ke,jt),e(jt,Hi),e(ke,Ui),e(ke,qn),e(qn,Gi),e(ke,Ki),e(j,Ji),A(Ge,j,null),e(j,Qi),e(j,On),e(On,Xi),e(j,Yi),A(Qo,j,null),p(o,wa,m),p(o,ye,m),e(ye,Ke),e(Ke,Nn),A(Xo,Nn,null),e(ye,Zi),e(ye,Dn),e(Dn,el),p(o,ka,m),p(o,R,m),A(Yo,R,null),e(R,ol),e(R,Zo),e(Zo,tl),e(Zo,et),e(et,nl),e(Zo,al),e(R,rl),e(R,q),A(ot,q,null),e(q,sl),e(q,Ve),e(Ve,il),e(Ve,qt),e(qt,ll),e(Ve,dl),e(Ve,Wn),e(Wn,cl),e(Ve,ml),e(q,hl),A(Je,q,null),e(q,pl),e(q,Sn),e(Sn,fl),e(q,gl),A(tt,q,null),p(o,ya,m),p(o,Te,m),e(Te,Qe),e(Qe,Bn),A(nt,Bn,null),e(Te,ul),e(Te,Rn),e(Rn,_l),p(o,Va,m),p(o,H,m),A(at,H,null),e(H,Al),e(H,rt),e(rt,vl),e(rt,st),e(st,bl),e(rt,Fl),e(H,Ll),e(H,O),A(it,O,null),e(O,wl),e(O,$e),e($e,kl),e($e,Ot),e(Ot,yl),e($e,Vl),e($e,Hn),e(Hn,Tl),e($e,$l),e(O,Ml),A(Xe,O,null),e(O,xl),e(O,Un),e(Un,Cl),e(O,zl),A(lt,O,null),Ta=!0},p(o,[m]){const dt={};m&2&&(dt.$$scope={dirty:m,ctx:o}),De.$set(dt);const Gn={};m&2&&(Gn.$$scope={dirty:m,ctx:o}),Se.$set(Gn);const Kn={};m&2&&(Kn.$$scope={dirty:m,ctx:o}),Be.$set(Kn);const Jn={};m&2&&(Jn.$$scope={dirty:m,ctx:o}),Re.$set(Jn);const ct={};m&2&&(ct.$$scope={dirty:m,ctx:o}),Ge.$set(ct);const Qn={};m&2&&(Qn.$$scope={dirty:m,ctx:o}),Je.$set(Qn);const Xn={};m&2&&(Xn.$$scope={dirty:m,ctx:o}),Xe.$set(Xn)},i(o){Ta||(v(L.$$.fragment,o),v(eo.$$.fragment,o),v(no.$$.fragment,o),v(ao.$$.fragment,o),v(ro.$$.fragment,o),v(so.$$.fragment,o),v(lo.$$.fragment,o),v(co.$$.fragment,o),v(ho.$$.fragment,o),v(po.$$.fragment,o),v(fo.$$.fragment,o),v(uo.$$.fragment,o),v(_o.$$.fragment,o),v(Ao.$$.fragment,o),v(bo.$$.fragment,o),v(Fo.$$.fragment,o),v(wo.$$.fragment,o),v(ko.$$.fragment,o),v(yo.$$.fragment,o),v($o.$$.fragment,o),v(De.$$.fragment,o),v(Mo.$$.fragment,o),v(xo.$$.fragment,o),v(Eo.$$.fragment,o),v(Se.$$.fragment,o),v(Po.$$.fragment,o),v(Io.$$.fragment,o),v(Be.$$.fragment,o),v(jo.$$.fragment,o),v(Re.$$.fragment,o),v(qo.$$.fragment,o),v(Oo.$$.fragment,o),v(So.$$.fragment,o),v(Bo.$$.fragment,o),v(Ro.$$.fragment,o),v(Ho.$$.fragment,o),v(Uo.$$.fragment,o),v(Jo.$$.fragment,o),v(Ge.$$.fragment,o),v(Qo.$$.fragment,o),v(Xo.$$.fragment,o),v(Yo.$$.fragment,o),v(ot.$$.fragment,o),v(Je.$$.fragment,o),v(tt.$$.fragment,o),v(nt.$$.fragment,o),v(at.$$.fragment,o),v(it.$$.fragment,o),v(Xe.$$.fragment,o),v(lt.$$.fragment,o),Ta=!0)},o(o){b(L.$$.fragment,o),b(eo.$$.fragment,o),b(no.$$.fragment,o),b(ao.$$.fragment,o),b(ro.$$.fragment,o),b(so.$$.fragment,o),b(lo.$$.fragment,o),b(co.$$.fragment,o),b(ho.$$.fragment,o),b(po.$$.fragment,o),b(fo.$$.fragment,o),b(uo.$$.fragment,o),b(_o.$$.fragment,o),b(Ao.$$.fragment,o),b(bo.$$.fragment,o),b(Fo.$$.fragment,o),b(wo.$$.fragment,o),b(ko.$$.fragment,o),b(yo.$$.fragment,o),b($o.$$.fragment,o),b(De.$$.fragment,o),b(Mo.$$.fragment,o),b(xo.$$.fragment,o),b(Eo.$$.fragment,o),b(Se.$$.fragment,o),b(Po.$$.fragment,o),b(Io.$$.fragment,o),b(Be.$$.fragment,o),b(jo.$$.fragment,o),b(Re.$$.fragment,o),b(qo.$$.fragment,o),b(Oo.$$.fragment,o),b(So.$$.fragment,o),b(Bo.$$.fragment,o),b(Ro.$$.fragment,o),b(Ho.$$.fragment,o),b(Uo.$$.fragment,o),b(Jo.$$.fragment,o),b(Ge.$$.fragment,o),b(Qo.$$.fragment,o),b(Xo.$$.fragment,o),b(Yo.$$.fragment,o),b(ot.$$.fragment,o),b(Je.$$.fragment,o),b(tt.$$.fragment,o),b(nt.$$.fragment,o),b(at.$$.fragment,o),b(it.$$.fragment,o),b(Xe.$$.fragment,o),b(lt.$$.fragment,o),Ta=!1},d(o){t(h),o&&t(y),o&&t(g),F(L),o&&t(Zn),o&&t(ne),F(eo),o&&t(ea),o&&t(xe),o&&t(oa),o&&t(ht),o&&t(ta),o&&t(pt),o&&t(na),o&&t(ft),o&&t(aa),o&&t(Ce),o&&t(ra),o&&t(ae),F(no),o&&t(sa),o&&t($),F(ao),F(ro),F(so),o&&t(ia),o&&t(se),F(lo),o&&t(la),o&&t(C),F(co),F(ho),o&&t(da),o&&t(de),F(po),o&&t(ca),o&&t(z),F(fo),F(uo),o&&t(ma),o&&t(he),F(_o),o&&t(ha),o&&t(E),F(Ao),F(bo),o&&t(pa),o&&t(ge),F(Fo),o&&t(fa),o&&t(Lo),F(wo),o&&t(ga),o&&t(ue),F(ko),o&&t(ua),o&&t(N),F(yo),F($o),F(De),o&&t(_a),o&&t(Ae),F(Mo),o&&t(Aa),o&&t(P),F(xo),F(Eo),F(Se),F(Po),F(Io),F(Be),F(jo),F(Re),o&&t(va),o&&t(Le),F(qo),o&&t(ba),o&&t(M),F(Oo),F(So),F(Bo),F(Ro),o&&t(Fa),o&&t(we),F(Ho),o&&t(La),o&&t(B),F(Uo),F(Jo),F(Ge),F(Qo),o&&t(wa),o&&t(ye),F(Xo),o&&t(ka),o&&t(R),F(Yo),F(ot),F(Je),F(tt),o&&t(ya),o&&t(Te),F(nt),o&&t(Va),o&&t(H),F(at),F(it),F(Xe),F(lt)}}}const $c={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FLAVAConfig",title:"FLAVAConfig"},{local:"transformers.FLAVATextConfig",title:"FLAVATextConfig"},{local:"transformers.FLAVAImageConfig",title:"FLAVAImageConfig"},{local:"transformers.FLAVAMultimodalConfig",title:"FLAVAMultimodalConfig"},{local:"transformers.FLAVACodebookConfig",title:"FLAVACodebookConfig"},{local:"transformers.FLAVAForPretraining",title:"FLAVAForPretraining"},{local:"transformers.FLAVAModel",title:"FLAVAModel"},{local:"transformers.FLAVACodebook",title:"FLAVACodebook"},{local:"transformers.FLAVATextModel",title:"FLAVATextModel"},{local:"transformers.FLAVAImageModel",title:"FLAVAImageModel"},{local:"transformers.FLAVAMultimodalModel",title:"FLAVAMultimodalModel"}],title:"FLAVA"};function Mc(x){return vc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ic extends gc{constructor(h){super();uc(this,h,Mc,Tc,_c,{})}}export{Ic as default,$c as metadata};
