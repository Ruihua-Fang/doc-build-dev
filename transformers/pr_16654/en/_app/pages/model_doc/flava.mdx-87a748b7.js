import{S as vf,i as bf,s as Ff,e as n,k as d,w as p,t as s,M as Lf,c as a,d as t,m as c,a as r,x as g,h as i,b as l,F as e,g as f,y as u,q as _,o as A,B as v,v as Vf}from"../../chunks/vendor-6b77c823.js";import{T as jt}from"../../chunks/Tip-39098574.js";import{D as T}from"../../chunks/Docstring-17b815d9.js";import{C as ho}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as E}from"../../chunks/IconCopyLink-7a11ce68.js";function wf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function kf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function Tf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function yf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function $f(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function xf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function Mf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var y=r(F);V=i(y,"Module"),y.forEach(t),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&t(h)}}}function Ef(M){let h,k,F,V,w,L,b,y,Qr,Wa,le,qe,On,po,Yr,Nn,es,Sa,Oe,os,go,ts,ns,Ba,qt,as,Ha,Ot,rs,Ra,Nt,ss,Ua,Ne,is,uo,ls,ds,Ga,de,De,Dn,_o,cs,Wn,ms,Ka,$,Ao,fs,We,Dt,hs,ps,Wt,gs,us,_s,ce,As,St,vs,bs,Bt,Fs,Ls,Vs,Sn,ws,ks,vo,Ts,Se,bo,ys,Fo,$s,Ht,xs,Ms,Ja,me,Be,Bn,Lo,Es,Hn,Cs,Xa,C,Vo,zs,fe,Ps,Rt,Is,js,wo,qs,Os,Ns,he,Ds,Ut,Ws,Ss,Gt,Bs,Hs,Rs,Rn,Us,Gs,ko,Za,pe,He,Un,To,Ks,Gn,Js,Qa,z,yo,Xs,ge,Zs,Kt,Qs,Ys,$o,ei,oi,ti,ue,ni,Jt,ai,ri,Xt,si,ii,li,Kn,di,ci,xo,Ya,_e,Re,Jn,Mo,mi,Xn,fi,er,P,Eo,hi,Ae,pi,Zt,gi,ui,Co,_i,Ai,vi,ve,bi,Qt,Fi,Li,Yt,Vi,wi,ki,Zn,Ti,yi,zo,or,be,Ue,Qn,Po,$i,Yn,xi,tr,Io,jo,nr,Fe,Ge,ea,qo,Mi,oa,Ei,ar,I,Oo,Ci,ta,zi,Pi,q,en,Ii,ji,on,qi,Oi,na,Ni,Di,aa,Wi,Si,tn,Bi,Hi,Ri,Ke,No,Ui,Do,Gi,nn,Ki,Ji,Xi,Je,Wo,Zi,So,Qi,an,Yi,el,rr,Le,Xe,ra,Bo,ol,sa,tl,sr,R,Ho,nl,ia,al,rl,Ro,sl,rn,il,ll,ir,Ve,Ze,la,Uo,dl,da,cl,lr,Go,Ko,dr,we,Qe,ca,Jo,ml,ma,fl,cr,S,Xo,hl,fa,pl,gl,Zo,ul,Qo,_l,Al,vl,X,Yo,bl,ke,Fl,sn,Ll,Vl,ha,wl,kl,Tl,Ye,mr,Te,eo,pa,et,yl,ga,$l,fr,j,ot,xl,tt,Ml,nt,El,Cl,zl,O,at,Pl,ye,Il,ln,jl,ql,ua,Ol,Nl,Dl,oo,Wl,_a,Sl,Bl,rt,Hl,Z,st,Rl,$e,Ul,dn,Gl,Kl,Aa,Jl,Xl,Zl,to,Ql,Q,it,Yl,xe,ed,cn,od,td,va,nd,ad,rd,no,hr,Me,ao,ba,lt,sd,Fa,id,pr,x,dt,ld,ct,dd,La,cd,md,fd,mt,hd,ft,pd,gd,ud,mn,ht,_d,fn,pt,Ad,hn,gt,gr,Ee,ro,Va,ut,vd,wa,bd,ur,U,_t,Fd,At,Ld,vt,Vd,wd,kd,N,bt,Td,Ce,yd,pn,$d,xd,ka,Md,Ed,Cd,so,zd,Ta,Pd,Id,Ft,_r,ze,io,ya,Lt,jd,$a,qd,Ar,G,Vt,Od,wt,Nd,kt,Dd,Wd,Sd,D,Tt,Bd,Pe,Hd,gn,Rd,Ud,xa,Gd,Kd,Jd,lo,Xd,Ma,Zd,Qd,yt,vr,Ie,co,Ea,$t,Yd,Ca,ec,br,K,xt,oc,Mt,tc,Et,nc,ac,rc,W,Ct,sc,je,ic,un,lc,dc,za,cc,mc,fc,mo,hc,Pa,pc,gc,zt,Fr;return L=new E({}),po=new E({}),_o=new E({}),Ao=new T({props:{name:"class transformers.FLAVAConfig",anchor:"transformers.FLAVAConfig",parameters:[{name:"image_config_dict",val:" = None"},{name:"text_config_dict",val:" = None"},{name:"multimodal_config_dict",val:" = None"},{name:"hidden_size",val:" = 768"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"projection_dim",val:" = 768"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"initializer_range",val:" = 0.02"},{name:"ce_ignore_index",val:" = -100"},{name:"mim_weight",val:" = 1.0"},{name:"mlm_weight",val:" = 1.0"},{name:"global_contrastive_weight",val:" = 1.0"},{name:"itm_weight",val:" = 1.0"},{name:"mmm_image_weight",val:" = 1.0"},{name:"mmm_text_weight",val:" = 1.0"},{name:"global_backprop_contrastive",val:" = True"},{name:"skip_unmasked_multimodal_encoder",val:" = True"},{name:"return_loss",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.FLAVAConfig.image_config_dict",description:`<strong>image_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>.`,name:"image_config_dict"},{anchor:"transformers.FLAVAConfig.multimodal_config_dict",description:`<strong>multimodal_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>.`,name:"multimodal_config_dict"},{anchor:"transformers.FLAVAConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FLAVAConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FLAVAConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FLAVAConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM unimodal loss`,name:"mim_weight"},{anchor:"transformers.FLAVAConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FLAVAConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FLAVAConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FLAVAConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FLAVAConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FLAVAConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FLAVAConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FLAVAConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FLAVAConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L442"}}),vo=new ho({props:{code:`from transformers import FLAVAModel, FLAVAForPreTraining, FLAVAConfig

# Initializing a FLAVAConfig with style configuration
configuration = FLAVAConfig()

# Initializing a FLAVAModel and FLAVAForPreTraining model from the style configuration
model = FLAVAModel(configuration)
model_pre = FLAVAForPreTraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAModel, FLAVAForPreTraining, FLAVAConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAModel and FLAVAForPreTraining model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FLAVAForPreTraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),bo=new T({props:{name:"from_configs",anchor:"transformers.FLAVAConfig.from_configs",parameters:[{name:"image_config",val:": FLAVAImageConfig"},{name:"text_config",val:": FLAVATextConfig"},{name:"multimodal_config",val:": FLAVAMultimodalConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L576",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"
>FLAVAConfig</a></p>
`}}),Lo=new E({}),Vo=new T({props:{name:"class transformers.FLAVATextConfig",anchor:"transformers.FLAVATextConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"type_vocab_size",val:" = 2"},{name:"max_position_embeddings",val:" = 512"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVATextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FLAVATextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FLAVATextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FLAVATextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FLAVATextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVATextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVATextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVATextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVATextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVATextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVATextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVATextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVATextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVATextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVATextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVATextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVATextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L148"}}),ko=new ho({props:{code:`from transformers import FLAVATextModel, FLAVATextConfig

# Initializing a FLAVATextModel with  style configuration
configuration = FLAVATextConfig()

# Initializing a FLAVATextConfig from the  style configuration
model = FLAVATextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVATextModel, FLAVATextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVATextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextConfig from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),To=new E({}),yo=new T({props:{name:"class transformers.FLAVAImageConfig",anchor:"transformers.FLAVAImageConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"mask_token",val:" = True"},{name:"vocab_size",val:" = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVAImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVAImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVAImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use a mask token or not. Used in MIM loss.`,name:"mask_token"},{anchor:"transformers.FLAVAImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebook">FLAVACodebook</a> used in conjunction with <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel">FLAVAImageModel</a> for MIM.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L32"}}),xo=new ho({props:{code:`from transformers import FLAVAImageModel, FLAVAImageConfig

# Initializing a FLAVAImageModel with  style configuration
configuration = FLAVAImageConfig()

# Initializing a FLAVAImageModel model from the  style configuration
model = FLAVAImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAImageModel, FLAVAImageConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Mo=new E({}),Eo=new T({props:{name:"class transformers.FLAVAMultimodalConfig",anchor:"transformers.FLAVAMultimodalConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 6"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"qkv_bias",val:" = True"},{name:"use_cls_token",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L275"}}),zo=new ho({props:{code:`from transformers import FLAVAMultimodalModel, FLAVAMultimodalConfig

# Initializing a FLAVAMultimodalModel with  style configuration
configuration = FLAVAMultimodalConfig()

# Initializing a FLAVAMultimodalModel model from the  style configuration
model = FLAVAMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAMultimodalModel, FLAVAMultimodalConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Po=new E({}),jo=new T({props:{name:"class transformers.FLAVACodebookConfig",anchor:"transformers.FLAVACodebookConfig",parameters:[{name:"num_groups",val:" = 4"},{name:"input_channels",val:" = 3"},{name:"num_blocks_per_group",val:" = 2"},{name:"hidden_size",val:" = 256"},{name:"vocab_size",val:" = 8192"},{name:"freeze",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L375"}}),qo=new E({}),Oo=new T({props:{name:"class transformers.FLAVAProcessor",anchor:"transformers.FLAVAProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""},{name:"mlm_probability",val:" = 0.15"}],parametersDescription:[{anchor:"transformers.FLAVAProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.FLAVAProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>FLAVATokenizerFast</code>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/processing_flava.py#L29"}}),No=new T({props:{name:"batch_decode",anchor:"transformers.FLAVAProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/processing_flava.py#L134"}}),Wo=new T({props:{name:"decode",anchor:"transformers.FLAVAProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/processing_flava.py#L141"}}),Bo=new E({}),Ho=new T({props:{name:"class transformers.FLAVAFeatureExtractor",anchor:"transformers.FLAVAFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 224"},{name:"resample",val:" = None"},{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 224"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": typing.Optional[int] = 16"},{name:"mask_group_max_patches",val:": int = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.FLAVAFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.FLAVAFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect`,name:"resample"},{anchor:"transformers.FLAVAFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.FLAVAFeatureExtractor.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FLAVAFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>.`,name:"do_normalize"},{anchor:"transformers.FLAVAFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.FLAVAFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/feature_extraction_flava.py#L120"}}),Uo=new E({}),Ko=new T({props:{name:"class transformers.FLAVACodebookFeatureExtractor",anchor:"transformers.FLAVACodebookFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 112"},{name:"resample",val:" = None"},{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 112"},{name:"do_map_pixels",val:" = True"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/feature_extraction_flava.py#L271"}}),Jo=new E({}),Xo=new T({props:{name:"class transformers.FLAVAForPreTraining",anchor:"transformers.FLAVAForPreTraining",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1765"}}),Yo=new T({props:{name:"forward",anchor:"transformers.FLAVAForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAForPreTraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1797",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) \u2014 Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FLAVALosses</code>) \u2014 Detailed info for FLAVA Pretraining losses. Check <code>FLAVALosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
</ul>
<ul>
<li><strong>mim_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape
<code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and
<code>input_ids_masked</code> are not): The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked
patches. The flattened output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mlm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape
<code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and
<code>pixel_values</code> are not): The logits for MLM unimodal loss. The flattened output is returned when
<code>input_ids_masked</code> has some of the tokens masked.</li>
<li><strong>itm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and
<code>pixel_values</code> are present): The logits for ITM loss. Note that ITM loss is calculated on masked pairs in
FLAVA.</li>
<li><strong>mmm_image_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape
<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code>
are present): The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The
flattened output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mmm_text_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape
<code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present): The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has some of the tokens masked.</li>
<li><strong>contrastive_logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</li>
<li><strong>contrastive_logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new jt({props:{$$slots:{default:[wf]},$$scope:{ctx:M}}}),et=new E({}),ot=new T({props:{name:"class transformers.FLAVAModel",anchor:"transformers.FLAVAModel",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1223"}}),at=new T({props:{name:"forward",anchor:"transformers.FLAVAModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FLAVAModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1365",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new jt({props:{$$slots:{default:[kf]},$$scope:{ctx:M}}}),rt=new ho({props:{code:`from PIL import Image
import requests
from transformers import FLAVAProcessor, FLAVAModel

model = FLAVAModel.from_pretrained("aps/flava-full")
processor = FLAVAProcessor.from_pretrained("aps/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAProcessor, FLAVAModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FLAVAProcessor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),st=new T({props:{name:"get_text_features",anchor:"transformers.FLAVAModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1267"}}),to=new jt({props:{$$slots:{default:[Tf]},$$scope:{ctx:M}}}),it=new T({props:{name:"get_image_features",anchor:"transformers.FLAVAModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1313"}}),no=new jt({props:{$$slots:{default:[yf]},$$scope:{ctx:M}}}),lt=new E({}),dt=new T({props:{name:"class transformers.FLAVACodebook",anchor:"transformers.FLAVACodebook",parameters:[{name:"config",val:": FLAVACodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FLAVACodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebookConfig">FLAVACodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1512"}}),ht=new T({props:{name:"forward",anchor:"transformers.FLAVACodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1636"}}),pt=new T({props:{name:"get_codebook_indices",anchor:"transformers.FLAVACodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1603"}}),gt=new T({props:{name:"get_codebook_probs",anchor:"transformers.FLAVACodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1632"}}),ut=new E({}),_t=new T({props:{name:"class transformers.FLAVATextModel",anchor:"transformers.FLAVATextModel",parameters:[{name:"config",val:": FLAVATextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1020"}}),bt=new T({props:{name:"forward",anchor:"transformers.FLAVATextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVATextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVATextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVATextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVATextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVATextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVATextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1050",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig"
>FLAVATextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),so=new jt({props:{$$slots:{default:[$f]},$$scope:{ctx:M}}}),Ft=new ho({props:{code:`from transformers import BertTokenizer, FLAVATextModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVATextModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVATextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lt=new E({}),Vt=new T({props:{name:"class transformers.FLAVAImageModel",anchor:"transformers.FLAVAImageModel",parameters:[{name:"config",val:": FLAVAImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L921"}}),Tt=new T({props:{name:"forward",anchor:"transformers.FLAVAImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L953",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig"
>FLAVAImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),lo=new jt({props:{$$slots:{default:[xf]},$$scope:{ctx:M}}}),yt=new ho({props:{code:`from transformers import FLAVAFeatureExtractor, FLAVAImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FLAVAFeatureExtractor.from_pretrained("aps/flava-full")
model = FLAVAImageModel.from_pretrained("aps/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAFeatureExtractor, FLAVAImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FLAVAFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),$t=new E({}),xt=new T({props:{name:"class transformers.FLAVAMultimodalModel",anchor:"transformers.FLAVAMultimodalModel",parameters:[{name:"config",val:": FLAVAMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1125"}}),Ct=new T({props:{name:"forward",anchor:"transformers.FLAVAMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1152",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig"
>FLAVAMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),mo=new jt({props:{$$slots:{default:[Mf]},$$scope:{ctx:M}}}),zt=new ho({props:{code:`from transformers import BertTokenizer, FLAVAMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVAMultimodalModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVAMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){h=n("meta"),k=d(),F=n("h1"),V=n("a"),w=n("span"),p(L.$$.fragment),b=d(),y=n("span"),Qr=s("FLAVA"),Wa=d(),le=n("h2"),qe=n("a"),On=n("span"),p(po.$$.fragment),Yr=d(),Nn=n("span"),es=s("Overview"),Sa=d(),Oe=n("p"),os=s("The FLAVA model was proposed in "),go=n("a"),ts=s("FLAVA: A Foundational Language And Vision Alignment Model"),ns=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),Ba=d(),qt=n("p"),as=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Ha=d(),Ot=n("p"),rs=s("The abstract from the paper is the following:"),Ra=d(),Nt=n("p"),ss=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),Ua=d(),Ne=n("p"),is=s("This model was contributed by "),uo=n("a"),ls=s("aps"),ds=s("."),Ga=d(),de=n("h2"),De=n("a"),Dn=n("span"),p(_o.$$.fragment),cs=d(),Wn=n("span"),ms=s("FLAVAConfig"),Ka=d(),$=n("div"),p(Ao.$$.fragment),fs=d(),We=n("p"),Dt=n("a"),hs=s("FLAVAConfig"),ps=s(" is the configuration class to store the configuration of a "),Wt=n("a"),gs=s("FLAVAModel"),us=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),_s=d(),ce=n("p"),As=s("Configuration objects inherit from "),St=n("a"),vs=s("PretrainedConfig"),bs=s(` and can be used to control the model outputs. Read the
documentation from `),Bt=n("a"),Fs=s("PretrainedConfig"),Ls=s(" for more information."),Vs=d(),Sn=n("p"),ws=s("Example:"),ks=d(),p(vo.$$.fragment),Ts=d(),Se=n("div"),p(bo.$$.fragment),ys=d(),Fo=n("p"),$s=s("Instantiate a "),Ht=n("a"),xs=s("FLAVAConfig"),Ms=s(` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),Ja=d(),me=n("h2"),Be=n("a"),Bn=n("span"),p(Lo.$$.fragment),Es=d(),Hn=n("span"),Cs=s("FLAVATextConfig"),Xa=d(),C=n("div"),p(Vo.$$.fragment),zs=d(),fe=n("p"),Ps=s("This is the configuration class to store the configuration of a "),Rt=n("a"),Is=s("FLAVATextModel"),js=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),wo=n("a"),qs=s("full"),Os=s(" architecture."),Ns=d(),he=n("p"),Ds=s("Configuration objects inherit from "),Ut=n("a"),Ws=s("PretrainedConfig"),Ss=s(` and can be used to control the model outputs. Read the
documentation from `),Gt=n("a"),Bs=s("PretrainedConfig"),Hs=s(" for more information."),Rs=d(),Rn=n("p"),Us=s("Example:"),Gs=d(),p(ko.$$.fragment),Za=d(),pe=n("h2"),He=n("a"),Un=n("span"),p(To.$$.fragment),Ks=d(),Gn=n("span"),Js=s("FLAVAImageConfig"),Qa=d(),z=n("div"),p(yo.$$.fragment),Xs=d(),ge=n("p"),Zs=s("This is the configuration class to store the configuration of a "),Kt=n("a"),Qs=s("FLAVAImageModel"),Ys=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),$o=n("a"),ei=s("full"),oi=s(" architecture."),ti=d(),ue=n("p"),ni=s("Configuration objects inherit from "),Jt=n("a"),ai=s("PretrainedConfig"),ri=s(` and can be used to control the model outputs. Read the
documentation from `),Xt=n("a"),si=s("PretrainedConfig"),ii=s(" for more information."),li=d(),Kn=n("p"),di=s("Example:"),ci=d(),p(xo.$$.fragment),Ya=d(),_e=n("h2"),Re=n("a"),Jn=n("span"),p(Mo.$$.fragment),mi=d(),Xn=n("span"),fi=s("FLAVAMultimodalConfig"),er=d(),P=n("div"),p(Eo.$$.fragment),hi=d(),Ae=n("p"),pi=s("This is the configuration class to store the configuration of a "),Zt=n("a"),gi=s("FLAVAMultimodalModel"),ui=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Co=n("a"),_i=s("full"),Ai=s(" architecture."),vi=d(),ve=n("p"),bi=s("Configuration objects inherit from "),Qt=n("a"),Fi=s("PretrainedConfig"),Li=s(` and can be used to control the model outputs. Read the
documentation from `),Yt=n("a"),Vi=s("PretrainedConfig"),wi=s(" for more information."),ki=d(),Zn=n("p"),Ti=s("Example:"),yi=d(),p(zo.$$.fragment),or=d(),be=n("h2"),Ue=n("a"),Qn=n("span"),p(Po.$$.fragment),$i=d(),Yn=n("span"),xi=s("FLAVACodebookConfig"),tr=d(),Io=n("div"),p(jo.$$.fragment),nr=d(),Fe=n("h2"),Ge=n("a"),ea=n("span"),p(qo.$$.fragment),Mi=d(),oa=n("span"),Ei=s("FLAVAProcessor"),ar=d(),I=n("div"),p(Oo.$$.fragment),Ci=d(),ta=n("p"),zi=s("Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),Pi=d(),q=n("p"),en=n("a"),Ii=s("FLAVAProcessor"),ji=s(" offers all the functionalities of "),on=n("a"),qi=s("FLAVAFeatureExtractor"),Oi=s(" and "),na=n("code"),Ni=s("FLAVATokenizerFast"),Di=s(`. See the
`),aa=n("code"),Wi=s("__call__()"),Si=s(" and "),tn=n("a"),Bi=s("decode()"),Hi=s(" for more information."),Ri=d(),Ke=n("div"),p(No.$$.fragment),Ui=d(),Do=n("p"),Gi=s("This method forwards all its arguments to FLAVATokenizerFast\u2019s "),nn=n("a"),Ki=s("batch_decode()"),Ji=s(`. Please
refer to the docstring of this method for more information.`),Xi=d(),Je=n("div"),p(Wo.$$.fragment),Zi=d(),So=n("p"),Qi=s("This method forwards all its arguments to FLAVATokenizerFast\u2019s "),an=n("a"),Yi=s("decode()"),el=s(`. Please refer to
the docstring of this method for more information.`),rr=d(),Le=n("h2"),Xe=n("a"),ra=n("span"),p(Bo.$$.fragment),ol=d(),sa=n("span"),tl=s("FLAVAFeatureExtractor"),sr=d(),R=n("div"),p(Ho.$$.fragment),nl=d(),ia=n("p"),al=s("Constructs a FLAVA feature extractor."),rl=d(),Ro=n("p"),sl=s("This feature extractor inherits from "),rn=n("a"),il=s("FeatureExtractionMixin"),ll=s(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),ir=d(),Ve=n("h2"),Ze=n("a"),la=n("span"),p(Uo.$$.fragment),dl=d(),da=n("span"),cl=s("FLAVACodebookFeatureExtractor"),lr=d(),Go=n("div"),p(Ko.$$.fragment),dr=d(),we=n("h2"),Qe=n("a"),ca=n("span"),p(Jo.$$.fragment),ml=d(),ma=n("span"),fl=s("FLAVAForPreTraining"),cr=d(),S=n("div"),p(Xo.$$.fragment),hl=d(),fa=n("p"),pl=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),gl=d(),Zo=n("p"),ul=s("This model is a PyTorch "),Qo=n("a"),_l=s("torch.nn.Module"),Al=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vl=d(),X=n("div"),p(Yo.$$.fragment),bl=d(),ke=n("p"),Fl=s("The "),sn=n("a"),Ll=s("FLAVAForPreTraining"),Vl=s(" forward method, overrides the "),ha=n("code"),wl=s("__call__"),kl=s(" special method."),Tl=d(),p(Ye.$$.fragment),mr=d(),Te=n("h2"),eo=n("a"),pa=n("span"),p(et.$$.fragment),yl=d(),ga=n("span"),$l=s("FLAVAModel"),fr=d(),j=n("div"),p(ot.$$.fragment),xl=d(),tt=n("p"),Ml=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),nt=n("a"),El=s("torch.nn.Module"),Cl=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),zl=d(),O=n("div"),p(at.$$.fragment),Pl=d(),ye=n("p"),Il=s("The "),ln=n("a"),jl=s("FLAVAModel"),ql=s(" forward method, overrides the "),ua=n("code"),Ol=s("__call__"),Nl=s(" special method."),Dl=d(),p(oo.$$.fragment),Wl=d(),_a=n("p"),Sl=s("Examples:"),Bl=d(),p(rt.$$.fragment),Hl=d(),Z=n("div"),p(st.$$.fragment),Rl=d(),$e=n("p"),Ul=s("The "),dn=n("a"),Gl=s("FLAVAModel"),Kl=s(" forward method, overrides the "),Aa=n("code"),Jl=s("__call__"),Xl=s(" special method."),Zl=d(),p(to.$$.fragment),Ql=d(),Q=n("div"),p(it.$$.fragment),Yl=d(),xe=n("p"),ed=s("The "),cn=n("a"),od=s("FLAVAModel"),td=s(" forward method, overrides the "),va=n("code"),nd=s("__call__"),ad=s(" special method."),rd=d(),p(no.$$.fragment),hr=d(),Me=n("h2"),ao=n("a"),ba=n("span"),p(lt.$$.fragment),sd=d(),Fa=n("span"),id=s("FLAVACodebook"),pr=d(),x=n("div"),p(dt.$$.fragment),ld=d(),ct=n("p"),dd=s(`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),La=n("code"),cd=s("get_codebook_indices"),md=s(" to get image tokens for an image."),fd=d(),mt=n("p"),hd=s("This model is a PyTorch "),ft=n("a"),pd=s("torch.nn.Module"),gd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ud=d(),mn=n("div"),p(ht.$$.fragment),_d=d(),fn=n("div"),p(pt.$$.fragment),Ad=d(),hn=n("div"),p(gt.$$.fragment),gr=d(),Ee=n("h2"),ro=n("a"),Va=n("span"),p(ut.$$.fragment),vd=d(),wa=n("span"),bd=s("FLAVATextModel"),ur=d(),U=n("div"),p(_t.$$.fragment),Fd=d(),At=n("p"),Ld=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),vt=n("a"),Vd=s("torch.nn.Module"),wd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),kd=d(),N=n("div"),p(bt.$$.fragment),Td=d(),Ce=n("p"),yd=s("The "),pn=n("a"),$d=s("FLAVATextModel"),xd=s(" forward method, overrides the "),ka=n("code"),Md=s("__call__"),Ed=s(" special method."),Cd=d(),p(so.$$.fragment),zd=d(),Ta=n("p"),Pd=s("Example:"),Id=d(),p(Ft.$$.fragment),_r=d(),ze=n("h2"),io=n("a"),ya=n("span"),p(Lt.$$.fragment),jd=d(),$a=n("span"),qd=s("FLAVAImageModel"),Ar=d(),G=n("div"),p(Vt.$$.fragment),Od=d(),wt=n("p"),Nd=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),kt=n("a"),Dd=s("torch.nn.Module"),Wd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sd=d(),D=n("div"),p(Tt.$$.fragment),Bd=d(),Pe=n("p"),Hd=s("The "),gn=n("a"),Rd=s("FLAVAImageModel"),Ud=s(" forward method, overrides the "),xa=n("code"),Gd=s("__call__"),Kd=s(" special method."),Jd=d(),p(lo.$$.fragment),Xd=d(),Ma=n("p"),Zd=s("Example:"),Qd=d(),p(yt.$$.fragment),vr=d(),Ie=n("h2"),co=n("a"),Ea=n("span"),p($t.$$.fragment),Yd=d(),Ca=n("span"),ec=s("FLAVAMultimodalModel"),br=d(),K=n("div"),p(xt.$$.fragment),oc=d(),Mt=n("p"),tc=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Et=n("a"),nc=s("torch.nn.Module"),ac=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),rc=d(),W=n("div"),p(Ct.$$.fragment),sc=d(),je=n("p"),ic=s("The "),un=n("a"),lc=s("FLAVAMultimodalModel"),dc=s(" forward method, overrides the "),za=n("code"),cc=s("__call__"),mc=s(" special method."),fc=d(),p(mo.$$.fragment),hc=d(),Pa=n("p"),pc=s("Example:"),gc=d(),p(zt.$$.fragment),this.h()},l(o){const m=Lf('[data-svelte="svelte-1phssyn"]',document.head);h=a(m,"META",{name:!0,content:!0}),m.forEach(t),k=c(o),F=a(o,"H1",{class:!0});var Pt=r(F);V=a(Pt,"A",{id:!0,class:!0,href:!0});var Ia=r(V);w=a(Ia,"SPAN",{});var ja=r(w);g(L.$$.fragment,ja),ja.forEach(t),Ia.forEach(t),b=c(Pt),y=a(Pt,"SPAN",{});var qa=r(y);Qr=i(qa,"FLAVA"),qa.forEach(t),Pt.forEach(t),Wa=c(o),le=a(o,"H2",{class:!0});var It=r(le);qe=a(It,"A",{id:!0,class:!0,href:!0});var Oa=r(qe);On=a(Oa,"SPAN",{});var Na=r(On);g(po.$$.fragment,Na),Na.forEach(t),Oa.forEach(t),Yr=c(It),Nn=a(It,"SPAN",{});var uc=r(Nn);es=i(uc,"Overview"),uc.forEach(t),It.forEach(t),Sa=c(o),Oe=a(o,"P",{});var Lr=r(Oe);os=i(Lr,"The FLAVA model was proposed in "),go=a(Lr,"A",{href:!0,rel:!0});var _c=r(go);ts=i(_c,"FLAVA: A Foundational Language And Vision Alignment Model"),_c.forEach(t),ns=i(Lr," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),Lr.forEach(t),Ba=c(o),qt=a(o,"P",{});var Ac=r(qt);as=i(Ac,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Ac.forEach(t),Ha=c(o),Ot=a(o,"P",{});var vc=r(Ot);rs=i(vc,"The abstract from the paper is the following:"),vc.forEach(t),Ra=c(o),Nt=a(o,"P",{});var bc=r(Nt);ss=i(bc,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),bc.forEach(t),Ua=c(o),Ne=a(o,"P",{});var Vr=r(Ne);is=i(Vr,"This model was contributed by "),uo=a(Vr,"A",{href:!0,rel:!0});var Fc=r(uo);ls=i(Fc,"aps"),Fc.forEach(t),ds=i(Vr,"."),Vr.forEach(t),Ga=c(o),de=a(o,"H2",{class:!0});var wr=r(de);De=a(wr,"A",{id:!0,class:!0,href:!0});var Lc=r(De);Dn=a(Lc,"SPAN",{});var Vc=r(Dn);g(_o.$$.fragment,Vc),Vc.forEach(t),Lc.forEach(t),cs=c(wr),Wn=a(wr,"SPAN",{});var wc=r(Wn);ms=i(wc,"FLAVAConfig"),wc.forEach(t),wr.forEach(t),Ka=c(o),$=a(o,"DIV",{class:!0});var B=r($);g(Ao.$$.fragment,B),fs=c(B),We=a(B,"P",{});var Da=r(We);Dt=a(Da,"A",{href:!0});var kc=r(Dt);hs=i(kc,"FLAVAConfig"),kc.forEach(t),ps=i(Da," is the configuration class to store the configuration of a "),Wt=a(Da,"A",{href:!0});var Tc=r(Wt);gs=i(Tc,"FLAVAModel"),Tc.forEach(t),us=i(Da,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),Da.forEach(t),_s=c(B),ce=a(B,"P",{});var _n=r(ce);As=i(_n,"Configuration objects inherit from "),St=a(_n,"A",{href:!0});var yc=r(St);vs=i(yc,"PretrainedConfig"),yc.forEach(t),bs=i(_n,` and can be used to control the model outputs. Read the
documentation from `),Bt=a(_n,"A",{href:!0});var $c=r(Bt);Fs=i($c,"PretrainedConfig"),$c.forEach(t),Ls=i(_n," for more information."),_n.forEach(t),Vs=c(B),Sn=a(B,"P",{});var xc=r(Sn);ws=i(xc,"Example:"),xc.forEach(t),ks=c(B),g(vo.$$.fragment,B),Ts=c(B),Se=a(B,"DIV",{class:!0});var kr=r(Se);g(bo.$$.fragment,kr),ys=c(kr),Fo=a(kr,"P",{});var Tr=r(Fo);$s=i(Tr,"Instantiate a "),Ht=a(Tr,"A",{href:!0});var Mc=r(Ht);xs=i(Mc,"FLAVAConfig"),Mc.forEach(t),Ms=i(Tr,` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),Tr.forEach(t),kr.forEach(t),B.forEach(t),Ja=c(o),me=a(o,"H2",{class:!0});var yr=r(me);Be=a(yr,"A",{id:!0,class:!0,href:!0});var Ec=r(Be);Bn=a(Ec,"SPAN",{});var Cc=r(Bn);g(Lo.$$.fragment,Cc),Cc.forEach(t),Ec.forEach(t),Es=c(yr),Hn=a(yr,"SPAN",{});var zc=r(Hn);Cs=i(zc,"FLAVATextConfig"),zc.forEach(t),yr.forEach(t),Xa=c(o),C=a(o,"DIV",{class:!0});var Y=r(C);g(Vo.$$.fragment,Y),zs=c(Y),fe=a(Y,"P",{});var An=r(fe);Ps=i(An,"This is the configuration class to store the configuration of a "),Rt=a(An,"A",{href:!0});var Pc=r(Rt);Is=i(Pc,"FLAVATextModel"),Pc.forEach(t),js=i(An,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),wo=a(An,"A",{href:!0,rel:!0});var Ic=r(wo);qs=i(Ic,"full"),Ic.forEach(t),Os=i(An," architecture."),An.forEach(t),Ns=c(Y),he=a(Y,"P",{});var vn=r(he);Ds=i(vn,"Configuration objects inherit from "),Ut=a(vn,"A",{href:!0});var jc=r(Ut);Ws=i(jc,"PretrainedConfig"),jc.forEach(t),Ss=i(vn,` and can be used to control the model outputs. Read the
documentation from `),Gt=a(vn,"A",{href:!0});var qc=r(Gt);Bs=i(qc,"PretrainedConfig"),qc.forEach(t),Hs=i(vn," for more information."),vn.forEach(t),Rs=c(Y),Rn=a(Y,"P",{});var Oc=r(Rn);Us=i(Oc,"Example:"),Oc.forEach(t),Gs=c(Y),g(ko.$$.fragment,Y),Y.forEach(t),Za=c(o),pe=a(o,"H2",{class:!0});var $r=r(pe);He=a($r,"A",{id:!0,class:!0,href:!0});var Nc=r(He);Un=a(Nc,"SPAN",{});var Dc=r(Un);g(To.$$.fragment,Dc),Dc.forEach(t),Nc.forEach(t),Ks=c($r),Gn=a($r,"SPAN",{});var Wc=r(Gn);Js=i(Wc,"FLAVAImageConfig"),Wc.forEach(t),$r.forEach(t),Qa=c(o),z=a(o,"DIV",{class:!0});var ee=r(z);g(yo.$$.fragment,ee),Xs=c(ee),ge=a(ee,"P",{});var bn=r(ge);Zs=i(bn,"This is the configuration class to store the configuration of a "),Kt=a(bn,"A",{href:!0});var Sc=r(Kt);Qs=i(Sc,"FLAVAImageModel"),Sc.forEach(t),Ys=i(bn,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),$o=a(bn,"A",{href:!0,rel:!0});var Bc=r($o);ei=i(Bc,"full"),Bc.forEach(t),oi=i(bn," architecture."),bn.forEach(t),ti=c(ee),ue=a(ee,"P",{});var Fn=r(ue);ni=i(Fn,"Configuration objects inherit from "),Jt=a(Fn,"A",{href:!0});var Hc=r(Jt);ai=i(Hc,"PretrainedConfig"),Hc.forEach(t),ri=i(Fn,` and can be used to control the model outputs. Read the
documentation from `),Xt=a(Fn,"A",{href:!0});var Rc=r(Xt);si=i(Rc,"PretrainedConfig"),Rc.forEach(t),ii=i(Fn," for more information."),Fn.forEach(t),li=c(ee),Kn=a(ee,"P",{});var Uc=r(Kn);di=i(Uc,"Example:"),Uc.forEach(t),ci=c(ee),g(xo.$$.fragment,ee),ee.forEach(t),Ya=c(o),_e=a(o,"H2",{class:!0});var xr=r(_e);Re=a(xr,"A",{id:!0,class:!0,href:!0});var Gc=r(Re);Jn=a(Gc,"SPAN",{});var Kc=r(Jn);g(Mo.$$.fragment,Kc),Kc.forEach(t),Gc.forEach(t),mi=c(xr),Xn=a(xr,"SPAN",{});var Jc=r(Xn);fi=i(Jc,"FLAVAMultimodalConfig"),Jc.forEach(t),xr.forEach(t),er=c(o),P=a(o,"DIV",{class:!0});var oe=r(P);g(Eo.$$.fragment,oe),hi=c(oe),Ae=a(oe,"P",{});var Ln=r(Ae);pi=i(Ln,"This is the configuration class to store the configuration of a "),Zt=a(Ln,"A",{href:!0});var Xc=r(Zt);gi=i(Xc,"FLAVAMultimodalModel"),Xc.forEach(t),ui=i(Ln,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Co=a(Ln,"A",{href:!0,rel:!0});var Zc=r(Co);_i=i(Zc,"full"),Zc.forEach(t),Ai=i(Ln," architecture."),Ln.forEach(t),vi=c(oe),ve=a(oe,"P",{});var Vn=r(ve);bi=i(Vn,"Configuration objects inherit from "),Qt=a(Vn,"A",{href:!0});var Qc=r(Qt);Fi=i(Qc,"PretrainedConfig"),Qc.forEach(t),Li=i(Vn,` and can be used to control the model outputs. Read the
documentation from `),Yt=a(Vn,"A",{href:!0});var Yc=r(Yt);Vi=i(Yc,"PretrainedConfig"),Yc.forEach(t),wi=i(Vn," for more information."),Vn.forEach(t),ki=c(oe),Zn=a(oe,"P",{});var em=r(Zn);Ti=i(em,"Example:"),em.forEach(t),yi=c(oe),g(zo.$$.fragment,oe),oe.forEach(t),or=c(o),be=a(o,"H2",{class:!0});var Mr=r(be);Ue=a(Mr,"A",{id:!0,class:!0,href:!0});var om=r(Ue);Qn=a(om,"SPAN",{});var tm=r(Qn);g(Po.$$.fragment,tm),tm.forEach(t),om.forEach(t),$i=c(Mr),Yn=a(Mr,"SPAN",{});var nm=r(Yn);xi=i(nm,"FLAVACodebookConfig"),nm.forEach(t),Mr.forEach(t),tr=c(o),Io=a(o,"DIV",{class:!0});var am=r(Io);g(jo.$$.fragment,am),am.forEach(t),nr=c(o),Fe=a(o,"H2",{class:!0});var Er=r(Fe);Ge=a(Er,"A",{id:!0,class:!0,href:!0});var rm=r(Ge);ea=a(rm,"SPAN",{});var sm=r(ea);g(qo.$$.fragment,sm),sm.forEach(t),rm.forEach(t),Mi=c(Er),oa=a(Er,"SPAN",{});var im=r(oa);Ei=i(im,"FLAVAProcessor"),im.forEach(t),Er.forEach(t),ar=c(o),I=a(o,"DIV",{class:!0});var te=r(I);g(Oo.$$.fragment,te),Ci=c(te),ta=a(te,"P",{});var lm=r(ta);zi=i(lm,"Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),lm.forEach(t),Pi=c(te),q=a(te,"P",{});var J=r(q);en=a(J,"A",{href:!0});var dm=r(en);Ii=i(dm,"FLAVAProcessor"),dm.forEach(t),ji=i(J," offers all the functionalities of "),on=a(J,"A",{href:!0});var cm=r(on);qi=i(cm,"FLAVAFeatureExtractor"),cm.forEach(t),Oi=i(J," and "),na=a(J,"CODE",{});var mm=r(na);Ni=i(mm,"FLAVATokenizerFast"),mm.forEach(t),Di=i(J,`. See the
`),aa=a(J,"CODE",{});var fm=r(aa);Wi=i(fm,"__call__()"),fm.forEach(t),Si=i(J," and "),tn=a(J,"A",{href:!0});var hm=r(tn);Bi=i(hm,"decode()"),hm.forEach(t),Hi=i(J," for more information."),J.forEach(t),Ri=c(te),Ke=a(te,"DIV",{class:!0});var Cr=r(Ke);g(No.$$.fragment,Cr),Ui=c(Cr),Do=a(Cr,"P",{});var zr=r(Do);Gi=i(zr,"This method forwards all its arguments to FLAVATokenizerFast\u2019s "),nn=a(zr,"A",{href:!0});var pm=r(nn);Ki=i(pm,"batch_decode()"),pm.forEach(t),Ji=i(zr,`. Please
refer to the docstring of this method for more information.`),zr.forEach(t),Cr.forEach(t),Xi=c(te),Je=a(te,"DIV",{class:!0});var Pr=r(Je);g(Wo.$$.fragment,Pr),Zi=c(Pr),So=a(Pr,"P",{});var Ir=r(So);Qi=i(Ir,"This method forwards all its arguments to FLAVATokenizerFast\u2019s "),an=a(Ir,"A",{href:!0});var gm=r(an);Yi=i(gm,"decode()"),gm.forEach(t),el=i(Ir,`. Please refer to
the docstring of this method for more information.`),Ir.forEach(t),Pr.forEach(t),te.forEach(t),rr=c(o),Le=a(o,"H2",{class:!0});var jr=r(Le);Xe=a(jr,"A",{id:!0,class:!0,href:!0});var um=r(Xe);ra=a(um,"SPAN",{});var _m=r(ra);g(Bo.$$.fragment,_m),_m.forEach(t),um.forEach(t),ol=c(jr),sa=a(jr,"SPAN",{});var Am=r(sa);tl=i(Am,"FLAVAFeatureExtractor"),Am.forEach(t),jr.forEach(t),sr=c(o),R=a(o,"DIV",{class:!0});var wn=r(R);g(Ho.$$.fragment,wn),nl=c(wn),ia=a(wn,"P",{});var vm=r(ia);al=i(vm,"Constructs a FLAVA feature extractor."),vm.forEach(t),rl=c(wn),Ro=a(wn,"P",{});var qr=r(Ro);sl=i(qr,"This feature extractor inherits from "),rn=a(qr,"A",{href:!0});var bm=r(rn);il=i(bm,"FeatureExtractionMixin"),bm.forEach(t),ll=i(qr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),qr.forEach(t),wn.forEach(t),ir=c(o),Ve=a(o,"H2",{class:!0});var Or=r(Ve);Ze=a(Or,"A",{id:!0,class:!0,href:!0});var Fm=r(Ze);la=a(Fm,"SPAN",{});var Lm=r(la);g(Uo.$$.fragment,Lm),Lm.forEach(t),Fm.forEach(t),dl=c(Or),da=a(Or,"SPAN",{});var Vm=r(da);cl=i(Vm,"FLAVACodebookFeatureExtractor"),Vm.forEach(t),Or.forEach(t),lr=c(o),Go=a(o,"DIV",{class:!0});var wm=r(Go);g(Ko.$$.fragment,wm),wm.forEach(t),dr=c(o),we=a(o,"H2",{class:!0});var Nr=r(we);Qe=a(Nr,"A",{id:!0,class:!0,href:!0});var km=r(Qe);ca=a(km,"SPAN",{});var Tm=r(ca);g(Jo.$$.fragment,Tm),Tm.forEach(t),km.forEach(t),ml=c(Nr),ma=a(Nr,"SPAN",{});var ym=r(ma);fl=i(ym,"FLAVAForPreTraining"),ym.forEach(t),Nr.forEach(t),cr=c(o),S=a(o,"DIV",{class:!0});var fo=r(S);g(Xo.$$.fragment,fo),hl=c(fo),fa=a(fo,"P",{});var $m=r(fa);pl=i($m,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),$m.forEach(t),gl=c(fo),Zo=a(fo,"P",{});var Dr=r(Zo);ul=i(Dr,"This model is a PyTorch "),Qo=a(Dr,"A",{href:!0,rel:!0});var xm=r(Qo);_l=i(xm,"torch.nn.Module"),xm.forEach(t),Al=i(Dr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Dr.forEach(t),vl=c(fo),X=a(fo,"DIV",{class:!0});var kn=r(X);g(Yo.$$.fragment,kn),bl=c(kn),ke=a(kn,"P",{});var Tn=r(ke);Fl=i(Tn,"The "),sn=a(Tn,"A",{href:!0});var Mm=r(sn);Ll=i(Mm,"FLAVAForPreTraining"),Mm.forEach(t),Vl=i(Tn," forward method, overrides the "),ha=a(Tn,"CODE",{});var Em=r(ha);wl=i(Em,"__call__"),Em.forEach(t),kl=i(Tn," special method."),Tn.forEach(t),Tl=c(kn),g(Ye.$$.fragment,kn),kn.forEach(t),fo.forEach(t),mr=c(o),Te=a(o,"H2",{class:!0});var Wr=r(Te);eo=a(Wr,"A",{id:!0,class:!0,href:!0});var Cm=r(eo);pa=a(Cm,"SPAN",{});var zm=r(pa);g(et.$$.fragment,zm),zm.forEach(t),Cm.forEach(t),yl=c(Wr),ga=a(Wr,"SPAN",{});var Pm=r(ga);$l=i(Pm,"FLAVAModel"),Pm.forEach(t),Wr.forEach(t),fr=c(o),j=a(o,"DIV",{class:!0});var ne=r(j);g(ot.$$.fragment,ne),xl=c(ne),tt=a(ne,"P",{});var Sr=r(tt);Ml=i(Sr,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),nt=a(Sr,"A",{href:!0,rel:!0});var Im=r(nt);El=i(Im,"torch.nn.Module"),Im.forEach(t),Cl=i(Sr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sr.forEach(t),zl=c(ne),O=a(ne,"DIV",{class:!0});var ae=r(O);g(at.$$.fragment,ae),Pl=c(ae),ye=a(ae,"P",{});var yn=r(ye);Il=i(yn,"The "),ln=a(yn,"A",{href:!0});var jm=r(ln);jl=i(jm,"FLAVAModel"),jm.forEach(t),ql=i(yn," forward method, overrides the "),ua=a(yn,"CODE",{});var qm=r(ua);Ol=i(qm,"__call__"),qm.forEach(t),Nl=i(yn," special method."),yn.forEach(t),Dl=c(ae),g(oo.$$.fragment,ae),Wl=c(ae),_a=a(ae,"P",{});var Om=r(_a);Sl=i(Om,"Examples:"),Om.forEach(t),Bl=c(ae),g(rt.$$.fragment,ae),ae.forEach(t),Hl=c(ne),Z=a(ne,"DIV",{class:!0});var $n=r(Z);g(st.$$.fragment,$n),Rl=c($n),$e=a($n,"P",{});var xn=r($e);Ul=i(xn,"The "),dn=a(xn,"A",{href:!0});var Nm=r(dn);Gl=i(Nm,"FLAVAModel"),Nm.forEach(t),Kl=i(xn," forward method, overrides the "),Aa=a(xn,"CODE",{});var Dm=r(Aa);Jl=i(Dm,"__call__"),Dm.forEach(t),Xl=i(xn," special method."),xn.forEach(t),Zl=c($n),g(to.$$.fragment,$n),$n.forEach(t),Ql=c(ne),Q=a(ne,"DIV",{class:!0});var Mn=r(Q);g(it.$$.fragment,Mn),Yl=c(Mn),xe=a(Mn,"P",{});var En=r(xe);ed=i(En,"The "),cn=a(En,"A",{href:!0});var Wm=r(cn);od=i(Wm,"FLAVAModel"),Wm.forEach(t),td=i(En," forward method, overrides the "),va=a(En,"CODE",{});var Sm=r(va);nd=i(Sm,"__call__"),Sm.forEach(t),ad=i(En," special method."),En.forEach(t),rd=c(Mn),g(no.$$.fragment,Mn),Mn.forEach(t),ne.forEach(t),hr=c(o),Me=a(o,"H2",{class:!0});var Br=r(Me);ao=a(Br,"A",{id:!0,class:!0,href:!0});var Bm=r(ao);ba=a(Bm,"SPAN",{});var Hm=r(ba);g(lt.$$.fragment,Hm),Hm.forEach(t),Bm.forEach(t),sd=c(Br),Fa=a(Br,"SPAN",{});var Rm=r(Fa);id=i(Rm,"FLAVACodebook"),Rm.forEach(t),Br.forEach(t),pr=c(o),x=a(o,"DIV",{class:!0});var H=r(x);g(dt.$$.fragment,H),ld=c(H),ct=a(H,"P",{});var Hr=r(ct);dd=i(Hr,`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),La=a(Hr,"CODE",{});var Um=r(La);cd=i(Um,"get_codebook_indices"),Um.forEach(t),md=i(Hr," to get image tokens for an image."),Hr.forEach(t),fd=c(H),mt=a(H,"P",{});var Rr=r(mt);hd=i(Rr,"This model is a PyTorch "),ft=a(Rr,"A",{href:!0,rel:!0});var Gm=r(ft);pd=i(Gm,"torch.nn.Module"),Gm.forEach(t),gd=i(Rr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Rr.forEach(t),ud=c(H),mn=a(H,"DIV",{class:!0});var Km=r(mn);g(ht.$$.fragment,Km),Km.forEach(t),_d=c(H),fn=a(H,"DIV",{class:!0});var Jm=r(fn);g(pt.$$.fragment,Jm),Jm.forEach(t),Ad=c(H),hn=a(H,"DIV",{class:!0});var Xm=r(hn);g(gt.$$.fragment,Xm),Xm.forEach(t),H.forEach(t),gr=c(o),Ee=a(o,"H2",{class:!0});var Ur=r(Ee);ro=a(Ur,"A",{id:!0,class:!0,href:!0});var Zm=r(ro);Va=a(Zm,"SPAN",{});var Qm=r(Va);g(ut.$$.fragment,Qm),Qm.forEach(t),Zm.forEach(t),vd=c(Ur),wa=a(Ur,"SPAN",{});var Ym=r(wa);bd=i(Ym,"FLAVATextModel"),Ym.forEach(t),Ur.forEach(t),ur=c(o),U=a(o,"DIV",{class:!0});var Cn=r(U);g(_t.$$.fragment,Cn),Fd=c(Cn),At=a(Cn,"P",{});var Gr=r(At);Ld=i(Gr,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),vt=a(Gr,"A",{href:!0,rel:!0});var ef=r(vt);Vd=i(ef,"torch.nn.Module"),ef.forEach(t),wd=i(Gr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Gr.forEach(t),kd=c(Cn),N=a(Cn,"DIV",{class:!0});var re=r(N);g(bt.$$.fragment,re),Td=c(re),Ce=a(re,"P",{});var zn=r(Ce);yd=i(zn,"The "),pn=a(zn,"A",{href:!0});var of=r(pn);$d=i(of,"FLAVATextModel"),of.forEach(t),xd=i(zn," forward method, overrides the "),ka=a(zn,"CODE",{});var tf=r(ka);Md=i(tf,"__call__"),tf.forEach(t),Ed=i(zn," special method."),zn.forEach(t),Cd=c(re),g(so.$$.fragment,re),zd=c(re),Ta=a(re,"P",{});var nf=r(Ta);Pd=i(nf,"Example:"),nf.forEach(t),Id=c(re),g(Ft.$$.fragment,re),re.forEach(t),Cn.forEach(t),_r=c(o),ze=a(o,"H2",{class:!0});var Kr=r(ze);io=a(Kr,"A",{id:!0,class:!0,href:!0});var af=r(io);ya=a(af,"SPAN",{});var rf=r(ya);g(Lt.$$.fragment,rf),rf.forEach(t),af.forEach(t),jd=c(Kr),$a=a(Kr,"SPAN",{});var sf=r($a);qd=i(sf,"FLAVAImageModel"),sf.forEach(t),Kr.forEach(t),Ar=c(o),G=a(o,"DIV",{class:!0});var Pn=r(G);g(Vt.$$.fragment,Pn),Od=c(Pn),wt=a(Pn,"P",{});var Jr=r(wt);Nd=i(Jr,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),kt=a(Jr,"A",{href:!0,rel:!0});var lf=r(kt);Dd=i(lf,"torch.nn.Module"),lf.forEach(t),Wd=i(Jr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Jr.forEach(t),Sd=c(Pn),D=a(Pn,"DIV",{class:!0});var se=r(D);g(Tt.$$.fragment,se),Bd=c(se),Pe=a(se,"P",{});var In=r(Pe);Hd=i(In,"The "),gn=a(In,"A",{href:!0});var df=r(gn);Rd=i(df,"FLAVAImageModel"),df.forEach(t),Ud=i(In," forward method, overrides the "),xa=a(In,"CODE",{});var cf=r(xa);Gd=i(cf,"__call__"),cf.forEach(t),Kd=i(In," special method."),In.forEach(t),Jd=c(se),g(lo.$$.fragment,se),Xd=c(se),Ma=a(se,"P",{});var mf=r(Ma);Zd=i(mf,"Example:"),mf.forEach(t),Qd=c(se),g(yt.$$.fragment,se),se.forEach(t),Pn.forEach(t),vr=c(o),Ie=a(o,"H2",{class:!0});var Xr=r(Ie);co=a(Xr,"A",{id:!0,class:!0,href:!0});var ff=r(co);Ea=a(ff,"SPAN",{});var hf=r(Ea);g($t.$$.fragment,hf),hf.forEach(t),ff.forEach(t),Yd=c(Xr),Ca=a(Xr,"SPAN",{});var pf=r(Ca);ec=i(pf,"FLAVAMultimodalModel"),pf.forEach(t),Xr.forEach(t),br=c(o),K=a(o,"DIV",{class:!0});var jn=r(K);g(xt.$$.fragment,jn),oc=c(jn),Mt=a(jn,"P",{});var Zr=r(Mt);tc=i(Zr,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Et=a(Zr,"A",{href:!0,rel:!0});var gf=r(Et);nc=i(gf,"torch.nn.Module"),gf.forEach(t),ac=i(Zr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Zr.forEach(t),rc=c(jn),W=a(jn,"DIV",{class:!0});var ie=r(W);g(Ct.$$.fragment,ie),sc=c(ie),je=a(ie,"P",{});var qn=r(je);ic=i(qn,"The "),un=a(qn,"A",{href:!0});var uf=r(un);lc=i(uf,"FLAVAMultimodalModel"),uf.forEach(t),dc=i(qn," forward method, overrides the "),za=a(qn,"CODE",{});var _f=r(za);cc=i(_f,"__call__"),_f.forEach(t),mc=i(qn," special method."),qn.forEach(t),fc=c(ie),g(mo.$$.fragment,ie),hc=c(ie),Pa=a(ie,"P",{});var Af=r(Pa);pc=i(Af,"Example:"),Af.forEach(t),gc=c(ie),g(zt.$$.fragment,ie),ie.forEach(t),jn.forEach(t),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify(Cf)),l(V,"id","flava"),l(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(V,"href","#flava"),l(F,"class","relative group"),l(qe,"id","overview"),l(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(qe,"href","#overview"),l(le,"class","relative group"),l(go,"href","https://arxiv.org/abs/2112.04482"),l(go,"rel","nofollow"),l(uo,"href","https://huggingface.co/aps"),l(uo,"rel","nofollow"),l(De,"id","transformers.FLAVAConfig"),l(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(De,"href","#transformers.FLAVAConfig"),l(de,"class","relative group"),l(Dt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(Wt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(St,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Bt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Ht,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Be,"id","transformers.FLAVATextConfig"),l(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Be,"href","#transformers.FLAVATextConfig"),l(me,"class","relative group"),l(Rt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(wo,"href","https://huggingface.co/aps/flava-full"),l(wo,"rel","nofollow"),l(Ut,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Gt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(He,"id","transformers.FLAVAImageConfig"),l(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(He,"href","#transformers.FLAVAImageConfig"),l(pe,"class","relative group"),l(Kt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l($o,"href","https://huggingface.co/aps/flava-full"),l($o,"rel","nofollow"),l(Jt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Xt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Re,"id","transformers.FLAVAMultimodalConfig"),l(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Re,"href","#transformers.FLAVAMultimodalConfig"),l(_e,"class","relative group"),l(Zt,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(Co,"href","https://huggingface.co/aps/flava-full"),l(Co,"rel","nofollow"),l(Qt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Yt,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ue,"id","transformers.FLAVACodebookConfig"),l(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ue,"href","#transformers.FLAVACodebookConfig"),l(be,"class","relative group"),l(Io,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ge,"id","transformers.FLAVAProcessor"),l(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ge,"href","#transformers.FLAVAProcessor"),l(Fe,"class","relative group"),l(en,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAProcessor"),l(on,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor"),l(tn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAProcessor.decode"),l(nn,"href","/docs/transformers/pr_16654/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.batch_decode"),l(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(an,"href","/docs/transformers/pr_16654/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.decode"),l(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Xe,"id","transformers.FLAVAFeatureExtractor"),l(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Xe,"href","#transformers.FLAVAFeatureExtractor"),l(Le,"class","relative group"),l(rn,"href","/docs/transformers/pr_16654/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),l(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ze,"id","transformers.FLAVACodebookFeatureExtractor"),l(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ze,"href","#transformers.FLAVACodebookFeatureExtractor"),l(Ve,"class","relative group"),l(Go,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Qe,"id","transformers.FLAVAForPreTraining"),l(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Qe,"href","#transformers.FLAVAForPreTraining"),l(we,"class","relative group"),l(Qo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Qo,"rel","nofollow"),l(sn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAForPreTraining"),l(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(eo,"id","transformers.FLAVAModel"),l(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(eo,"href","#transformers.FLAVAModel"),l(Te,"class","relative group"),l(nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(nt,"rel","nofollow"),l(ln,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(dn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(cn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ao,"id","transformers.FLAVACodebook"),l(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ao,"href","#transformers.FLAVACodebook"),l(Me,"class","relative group"),l(ft,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(ft,"rel","nofollow"),l(mn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(hn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ro,"id","transformers.FLAVATextModel"),l(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ro,"href","#transformers.FLAVATextModel"),l(Ee,"class","relative group"),l(vt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(vt,"rel","nofollow"),l(pn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(io,"id","transformers.FLAVAImageModel"),l(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(io,"href","#transformers.FLAVAImageModel"),l(ze,"class","relative group"),l(kt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(kt,"rel","nofollow"),l(gn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(co,"id","transformers.FLAVAMultimodalModel"),l(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(co,"href","#transformers.FLAVAMultimodalModel"),l(Ie,"class","relative group"),l(Et,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Et,"rel","nofollow"),l(un,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),f(o,k,m),f(o,F,m),e(F,V),e(V,w),u(L,w,null),e(F,b),e(F,y),e(y,Qr),f(o,Wa,m),f(o,le,m),e(le,qe),e(qe,On),u(po,On,null),e(le,Yr),e(le,Nn),e(Nn,es),f(o,Sa,m),f(o,Oe,m),e(Oe,os),e(Oe,go),e(go,ts),e(Oe,ns),f(o,Ba,m),f(o,qt,m),e(qt,as),f(o,Ha,m),f(o,Ot,m),e(Ot,rs),f(o,Ra,m),f(o,Nt,m),e(Nt,ss),f(o,Ua,m),f(o,Ne,m),e(Ne,is),e(Ne,uo),e(uo,ls),e(Ne,ds),f(o,Ga,m),f(o,de,m),e(de,De),e(De,Dn),u(_o,Dn,null),e(de,cs),e(de,Wn),e(Wn,ms),f(o,Ka,m),f(o,$,m),u(Ao,$,null),e($,fs),e($,We),e(We,Dt),e(Dt,hs),e(We,ps),e(We,Wt),e(Wt,gs),e(We,us),e($,_s),e($,ce),e(ce,As),e(ce,St),e(St,vs),e(ce,bs),e(ce,Bt),e(Bt,Fs),e(ce,Ls),e($,Vs),e($,Sn),e(Sn,ws),e($,ks),u(vo,$,null),e($,Ts),e($,Se),u(bo,Se,null),e(Se,ys),e(Se,Fo),e(Fo,$s),e(Fo,Ht),e(Ht,xs),e(Fo,Ms),f(o,Ja,m),f(o,me,m),e(me,Be),e(Be,Bn),u(Lo,Bn,null),e(me,Es),e(me,Hn),e(Hn,Cs),f(o,Xa,m),f(o,C,m),u(Vo,C,null),e(C,zs),e(C,fe),e(fe,Ps),e(fe,Rt),e(Rt,Is),e(fe,js),e(fe,wo),e(wo,qs),e(fe,Os),e(C,Ns),e(C,he),e(he,Ds),e(he,Ut),e(Ut,Ws),e(he,Ss),e(he,Gt),e(Gt,Bs),e(he,Hs),e(C,Rs),e(C,Rn),e(Rn,Us),e(C,Gs),u(ko,C,null),f(o,Za,m),f(o,pe,m),e(pe,He),e(He,Un),u(To,Un,null),e(pe,Ks),e(pe,Gn),e(Gn,Js),f(o,Qa,m),f(o,z,m),u(yo,z,null),e(z,Xs),e(z,ge),e(ge,Zs),e(ge,Kt),e(Kt,Qs),e(ge,Ys),e(ge,$o),e($o,ei),e(ge,oi),e(z,ti),e(z,ue),e(ue,ni),e(ue,Jt),e(Jt,ai),e(ue,ri),e(ue,Xt),e(Xt,si),e(ue,ii),e(z,li),e(z,Kn),e(Kn,di),e(z,ci),u(xo,z,null),f(o,Ya,m),f(o,_e,m),e(_e,Re),e(Re,Jn),u(Mo,Jn,null),e(_e,mi),e(_e,Xn),e(Xn,fi),f(o,er,m),f(o,P,m),u(Eo,P,null),e(P,hi),e(P,Ae),e(Ae,pi),e(Ae,Zt),e(Zt,gi),e(Ae,ui),e(Ae,Co),e(Co,_i),e(Ae,Ai),e(P,vi),e(P,ve),e(ve,bi),e(ve,Qt),e(Qt,Fi),e(ve,Li),e(ve,Yt),e(Yt,Vi),e(ve,wi),e(P,ki),e(P,Zn),e(Zn,Ti),e(P,yi),u(zo,P,null),f(o,or,m),f(o,be,m),e(be,Ue),e(Ue,Qn),u(Po,Qn,null),e(be,$i),e(be,Yn),e(Yn,xi),f(o,tr,m),f(o,Io,m),u(jo,Io,null),f(o,nr,m),f(o,Fe,m),e(Fe,Ge),e(Ge,ea),u(qo,ea,null),e(Fe,Mi),e(Fe,oa),e(oa,Ei),f(o,ar,m),f(o,I,m),u(Oo,I,null),e(I,Ci),e(I,ta),e(ta,zi),e(I,Pi),e(I,q),e(q,en),e(en,Ii),e(q,ji),e(q,on),e(on,qi),e(q,Oi),e(q,na),e(na,Ni),e(q,Di),e(q,aa),e(aa,Wi),e(q,Si),e(q,tn),e(tn,Bi),e(q,Hi),e(I,Ri),e(I,Ke),u(No,Ke,null),e(Ke,Ui),e(Ke,Do),e(Do,Gi),e(Do,nn),e(nn,Ki),e(Do,Ji),e(I,Xi),e(I,Je),u(Wo,Je,null),e(Je,Zi),e(Je,So),e(So,Qi),e(So,an),e(an,Yi),e(So,el),f(o,rr,m),f(o,Le,m),e(Le,Xe),e(Xe,ra),u(Bo,ra,null),e(Le,ol),e(Le,sa),e(sa,tl),f(o,sr,m),f(o,R,m),u(Ho,R,null),e(R,nl),e(R,ia),e(ia,al),e(R,rl),e(R,Ro),e(Ro,sl),e(Ro,rn),e(rn,il),e(Ro,ll),f(o,ir,m),f(o,Ve,m),e(Ve,Ze),e(Ze,la),u(Uo,la,null),e(Ve,dl),e(Ve,da),e(da,cl),f(o,lr,m),f(o,Go,m),u(Ko,Go,null),f(o,dr,m),f(o,we,m),e(we,Qe),e(Qe,ca),u(Jo,ca,null),e(we,ml),e(we,ma),e(ma,fl),f(o,cr,m),f(o,S,m),u(Xo,S,null),e(S,hl),e(S,fa),e(fa,pl),e(S,gl),e(S,Zo),e(Zo,ul),e(Zo,Qo),e(Qo,_l),e(Zo,Al),e(S,vl),e(S,X),u(Yo,X,null),e(X,bl),e(X,ke),e(ke,Fl),e(ke,sn),e(sn,Ll),e(ke,Vl),e(ke,ha),e(ha,wl),e(ke,kl),e(X,Tl),u(Ye,X,null),f(o,mr,m),f(o,Te,m),e(Te,eo),e(eo,pa),u(et,pa,null),e(Te,yl),e(Te,ga),e(ga,$l),f(o,fr,m),f(o,j,m),u(ot,j,null),e(j,xl),e(j,tt),e(tt,Ml),e(tt,nt),e(nt,El),e(tt,Cl),e(j,zl),e(j,O),u(at,O,null),e(O,Pl),e(O,ye),e(ye,Il),e(ye,ln),e(ln,jl),e(ye,ql),e(ye,ua),e(ua,Ol),e(ye,Nl),e(O,Dl),u(oo,O,null),e(O,Wl),e(O,_a),e(_a,Sl),e(O,Bl),u(rt,O,null),e(j,Hl),e(j,Z),u(st,Z,null),e(Z,Rl),e(Z,$e),e($e,Ul),e($e,dn),e(dn,Gl),e($e,Kl),e($e,Aa),e(Aa,Jl),e($e,Xl),e(Z,Zl),u(to,Z,null),e(j,Ql),e(j,Q),u(it,Q,null),e(Q,Yl),e(Q,xe),e(xe,ed),e(xe,cn),e(cn,od),e(xe,td),e(xe,va),e(va,nd),e(xe,ad),e(Q,rd),u(no,Q,null),f(o,hr,m),f(o,Me,m),e(Me,ao),e(ao,ba),u(lt,ba,null),e(Me,sd),e(Me,Fa),e(Fa,id),f(o,pr,m),f(o,x,m),u(dt,x,null),e(x,ld),e(x,ct),e(ct,dd),e(ct,La),e(La,cd),e(ct,md),e(x,fd),e(x,mt),e(mt,hd),e(mt,ft),e(ft,pd),e(mt,gd),e(x,ud),e(x,mn),u(ht,mn,null),e(x,_d),e(x,fn),u(pt,fn,null),e(x,Ad),e(x,hn),u(gt,hn,null),f(o,gr,m),f(o,Ee,m),e(Ee,ro),e(ro,Va),u(ut,Va,null),e(Ee,vd),e(Ee,wa),e(wa,bd),f(o,ur,m),f(o,U,m),u(_t,U,null),e(U,Fd),e(U,At),e(At,Ld),e(At,vt),e(vt,Vd),e(At,wd),e(U,kd),e(U,N),u(bt,N,null),e(N,Td),e(N,Ce),e(Ce,yd),e(Ce,pn),e(pn,$d),e(Ce,xd),e(Ce,ka),e(ka,Md),e(Ce,Ed),e(N,Cd),u(so,N,null),e(N,zd),e(N,Ta),e(Ta,Pd),e(N,Id),u(Ft,N,null),f(o,_r,m),f(o,ze,m),e(ze,io),e(io,ya),u(Lt,ya,null),e(ze,jd),e(ze,$a),e($a,qd),f(o,Ar,m),f(o,G,m),u(Vt,G,null),e(G,Od),e(G,wt),e(wt,Nd),e(wt,kt),e(kt,Dd),e(wt,Wd),e(G,Sd),e(G,D),u(Tt,D,null),e(D,Bd),e(D,Pe),e(Pe,Hd),e(Pe,gn),e(gn,Rd),e(Pe,Ud),e(Pe,xa),e(xa,Gd),e(Pe,Kd),e(D,Jd),u(lo,D,null),e(D,Xd),e(D,Ma),e(Ma,Zd),e(D,Qd),u(yt,D,null),f(o,vr,m),f(o,Ie,m),e(Ie,co),e(co,Ea),u($t,Ea,null),e(Ie,Yd),e(Ie,Ca),e(Ca,ec),f(o,br,m),f(o,K,m),u(xt,K,null),e(K,oc),e(K,Mt),e(Mt,tc),e(Mt,Et),e(Et,nc),e(Mt,ac),e(K,rc),e(K,W),u(Ct,W,null),e(W,sc),e(W,je),e(je,ic),e(je,un),e(un,lc),e(je,dc),e(je,za),e(za,cc),e(je,mc),e(W,fc),u(mo,W,null),e(W,hc),e(W,Pa),e(Pa,pc),e(W,gc),u(zt,W,null),Fr=!0},p(o,[m]){const Pt={};m&2&&(Pt.$$scope={dirty:m,ctx:o}),Ye.$set(Pt);const Ia={};m&2&&(Ia.$$scope={dirty:m,ctx:o}),oo.$set(Ia);const ja={};m&2&&(ja.$$scope={dirty:m,ctx:o}),to.$set(ja);const qa={};m&2&&(qa.$$scope={dirty:m,ctx:o}),no.$set(qa);const It={};m&2&&(It.$$scope={dirty:m,ctx:o}),so.$set(It);const Oa={};m&2&&(Oa.$$scope={dirty:m,ctx:o}),lo.$set(Oa);const Na={};m&2&&(Na.$$scope={dirty:m,ctx:o}),mo.$set(Na)},i(o){Fr||(_(L.$$.fragment,o),_(po.$$.fragment,o),_(_o.$$.fragment,o),_(Ao.$$.fragment,o),_(vo.$$.fragment,o),_(bo.$$.fragment,o),_(Lo.$$.fragment,o),_(Vo.$$.fragment,o),_(ko.$$.fragment,o),_(To.$$.fragment,o),_(yo.$$.fragment,o),_(xo.$$.fragment,o),_(Mo.$$.fragment,o),_(Eo.$$.fragment,o),_(zo.$$.fragment,o),_(Po.$$.fragment,o),_(jo.$$.fragment,o),_(qo.$$.fragment,o),_(Oo.$$.fragment,o),_(No.$$.fragment,o),_(Wo.$$.fragment,o),_(Bo.$$.fragment,o),_(Ho.$$.fragment,o),_(Uo.$$.fragment,o),_(Ko.$$.fragment,o),_(Jo.$$.fragment,o),_(Xo.$$.fragment,o),_(Yo.$$.fragment,o),_(Ye.$$.fragment,o),_(et.$$.fragment,o),_(ot.$$.fragment,o),_(at.$$.fragment,o),_(oo.$$.fragment,o),_(rt.$$.fragment,o),_(st.$$.fragment,o),_(to.$$.fragment,o),_(it.$$.fragment,o),_(no.$$.fragment,o),_(lt.$$.fragment,o),_(dt.$$.fragment,o),_(ht.$$.fragment,o),_(pt.$$.fragment,o),_(gt.$$.fragment,o),_(ut.$$.fragment,o),_(_t.$$.fragment,o),_(bt.$$.fragment,o),_(so.$$.fragment,o),_(Ft.$$.fragment,o),_(Lt.$$.fragment,o),_(Vt.$$.fragment,o),_(Tt.$$.fragment,o),_(lo.$$.fragment,o),_(yt.$$.fragment,o),_($t.$$.fragment,o),_(xt.$$.fragment,o),_(Ct.$$.fragment,o),_(mo.$$.fragment,o),_(zt.$$.fragment,o),Fr=!0)},o(o){A(L.$$.fragment,o),A(po.$$.fragment,o),A(_o.$$.fragment,o),A(Ao.$$.fragment,o),A(vo.$$.fragment,o),A(bo.$$.fragment,o),A(Lo.$$.fragment,o),A(Vo.$$.fragment,o),A(ko.$$.fragment,o),A(To.$$.fragment,o),A(yo.$$.fragment,o),A(xo.$$.fragment,o),A(Mo.$$.fragment,o),A(Eo.$$.fragment,o),A(zo.$$.fragment,o),A(Po.$$.fragment,o),A(jo.$$.fragment,o),A(qo.$$.fragment,o),A(Oo.$$.fragment,o),A(No.$$.fragment,o),A(Wo.$$.fragment,o),A(Bo.$$.fragment,o),A(Ho.$$.fragment,o),A(Uo.$$.fragment,o),A(Ko.$$.fragment,o),A(Jo.$$.fragment,o),A(Xo.$$.fragment,o),A(Yo.$$.fragment,o),A(Ye.$$.fragment,o),A(et.$$.fragment,o),A(ot.$$.fragment,o),A(at.$$.fragment,o),A(oo.$$.fragment,o),A(rt.$$.fragment,o),A(st.$$.fragment,o),A(to.$$.fragment,o),A(it.$$.fragment,o),A(no.$$.fragment,o),A(lt.$$.fragment,o),A(dt.$$.fragment,o),A(ht.$$.fragment,o),A(pt.$$.fragment,o),A(gt.$$.fragment,o),A(ut.$$.fragment,o),A(_t.$$.fragment,o),A(bt.$$.fragment,o),A(so.$$.fragment,o),A(Ft.$$.fragment,o),A(Lt.$$.fragment,o),A(Vt.$$.fragment,o),A(Tt.$$.fragment,o),A(lo.$$.fragment,o),A(yt.$$.fragment,o),A($t.$$.fragment,o),A(xt.$$.fragment,o),A(Ct.$$.fragment,o),A(mo.$$.fragment,o),A(zt.$$.fragment,o),Fr=!1},d(o){t(h),o&&t(k),o&&t(F),v(L),o&&t(Wa),o&&t(le),v(po),o&&t(Sa),o&&t(Oe),o&&t(Ba),o&&t(qt),o&&t(Ha),o&&t(Ot),o&&t(Ra),o&&t(Nt),o&&t(Ua),o&&t(Ne),o&&t(Ga),o&&t(de),v(_o),o&&t(Ka),o&&t($),v(Ao),v(vo),v(bo),o&&t(Ja),o&&t(me),v(Lo),o&&t(Xa),o&&t(C),v(Vo),v(ko),o&&t(Za),o&&t(pe),v(To),o&&t(Qa),o&&t(z),v(yo),v(xo),o&&t(Ya),o&&t(_e),v(Mo),o&&t(er),o&&t(P),v(Eo),v(zo),o&&t(or),o&&t(be),v(Po),o&&t(tr),o&&t(Io),v(jo),o&&t(nr),o&&t(Fe),v(qo),o&&t(ar),o&&t(I),v(Oo),v(No),v(Wo),o&&t(rr),o&&t(Le),v(Bo),o&&t(sr),o&&t(R),v(Ho),o&&t(ir),o&&t(Ve),v(Uo),o&&t(lr),o&&t(Go),v(Ko),o&&t(dr),o&&t(we),v(Jo),o&&t(cr),o&&t(S),v(Xo),v(Yo),v(Ye),o&&t(mr),o&&t(Te),v(et),o&&t(fr),o&&t(j),v(ot),v(at),v(oo),v(rt),v(st),v(to),v(it),v(no),o&&t(hr),o&&t(Me),v(lt),o&&t(pr),o&&t(x),v(dt),v(ht),v(pt),v(gt),o&&t(gr),o&&t(Ee),v(ut),o&&t(ur),o&&t(U),v(_t),v(bt),v(so),v(Ft),o&&t(_r),o&&t(ze),v(Lt),o&&t(Ar),o&&t(G),v(Vt),v(Tt),v(lo),v(yt),o&&t(vr),o&&t(Ie),v($t),o&&t(br),o&&t(K),v(xt),v(Ct),v(mo),v(zt)}}}const Cf={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FLAVAConfig",title:"FLAVAConfig"},{local:"transformers.FLAVATextConfig",title:"FLAVATextConfig"},{local:"transformers.FLAVAImageConfig",title:"FLAVAImageConfig"},{local:"transformers.FLAVAMultimodalConfig",title:"FLAVAMultimodalConfig"},{local:"transformers.FLAVACodebookConfig",title:"FLAVACodebookConfig"},{local:"transformers.FLAVAProcessor",title:"FLAVAProcessor"},{local:"transformers.FLAVAFeatureExtractor",title:"FLAVAFeatureExtractor"},{local:"transformers.FLAVACodebookFeatureExtractor",title:"FLAVACodebookFeatureExtractor"},{local:"transformers.FLAVAForPreTraining",title:"FLAVAForPreTraining"},{local:"transformers.FLAVAModel",title:"FLAVAModel"},{local:"transformers.FLAVACodebook",title:"FLAVACodebook"},{local:"transformers.FLAVATextModel",title:"FLAVATextModel"},{local:"transformers.FLAVAImageModel",title:"FLAVAImageModel"},{local:"transformers.FLAVAMultimodalModel",title:"FLAVAMultimodalModel"}],title:"FLAVA"};function zf(M){return Vf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nf extends vf{constructor(h){super();bf(this,h,zf,Ef,Ff,{})}}export{Nf as default,Cf as metadata};
