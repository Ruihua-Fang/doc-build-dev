import{S as vf,i as bf,s as Ff,e as n,k as d,w as p,t as s,M as Lf,c as a,d as o,m as c,a as r,x as g,h as i,b as l,F as e,g as f,y as u,q as _,o as A,B as v,v as Vf}from"../../chunks/vendor-6b77c823.js";import{T as jo}from"../../chunks/Tip-39098574.js";import{D as y}from"../../chunks/Docstring-17b815d9.js";import{C as ft}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as E}from"../../chunks/IconCopyLink-7a11ce68.js";function wf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function kf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function yf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function Tf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function $f(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function xf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function Mf(M){let h,k,F,V,w;return{c(){h=n("p"),k=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n("code"),V=s("Module"),w=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(L){h=a(L,"P",{});var b=r(h);k=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a(b,"CODE",{});var T=r(F);V=i(T,"Module"),T.forEach(o),w=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(L,b){f(L,h,b),e(h,k),e(h,F),e(F,V),e(h,w)},d(L){L&&o(h)}}}function Ef(M){let h,k,F,V,w,L,b,T,Qr,Wa,le,qe,On,ht,Yr,Nn,es,Sa,Oe,ts,pt,os,ns,Ba,qo,as,Ra,Oo,rs,Ha,No,ss,Ua,Ne,is,gt,ls,ds,Ga,de,De,Dn,ut,cs,Wn,ms,Ka,$,_t,fs,We,Do,hs,ps,Wo,gs,us,_s,ce,As,So,vs,bs,Bo,Fs,Ls,Vs,Sn,ws,ks,At,ys,Se,vt,Ts,bt,$s,Ro,xs,Ms,Za,me,Be,Bn,Ft,Es,Rn,Cs,Ja,C,Lt,zs,fe,Ps,Ho,Is,js,Vt,qs,Os,Ns,he,Ds,Uo,Ws,Ss,Go,Bs,Rs,Hs,Hn,Us,Gs,wt,Xa,pe,Re,Un,kt,Ks,Gn,Zs,Qa,z,yt,Js,ge,Xs,Ko,Qs,Ys,Tt,ei,ti,oi,ue,ni,Zo,ai,ri,Jo,si,ii,li,Kn,di,ci,$t,Ya,_e,He,Zn,xt,mi,Jn,fi,er,P,Mt,hi,Ae,pi,Xo,gi,ui,Et,_i,Ai,vi,ve,bi,Qo,Fi,Li,Yo,Vi,wi,ki,Xn,yi,Ti,Ct,tr,be,Ue,Qn,zt,$i,Yn,xi,or,Pt,It,nr,Fe,Ge,ea,jt,Mi,ta,Ei,ar,I,qt,Ci,oa,zi,Pi,q,en,Ii,ji,tn,qi,Oi,na,Ni,Di,aa,Wi,Si,on,Bi,Ri,Hi,Ke,Ot,Ui,Nt,Gi,nn,Ki,Zi,Ji,Ze,Dt,Xi,Wt,Qi,an,Yi,el,rr,Le,Je,ra,St,tl,sa,ol,sr,H,Bt,nl,ia,al,rl,Rt,sl,rn,il,ll,ir,Ve,Xe,la,Ht,dl,da,cl,lr,Ut,Gt,dr,we,Qe,ca,Kt,ml,ma,fl,cr,S,Zt,hl,fa,pl,gl,Jt,ul,Xt,_l,Al,vl,J,Qt,bl,ke,Fl,sn,Ll,Vl,ha,wl,kl,yl,Ye,mr,ye,et,pa,Yt,Tl,ga,$l,fr,j,eo,xl,to,Ml,oo,El,Cl,zl,O,no,Pl,Te,Il,ln,jl,ql,ua,Ol,Nl,Dl,tt,Wl,_a,Sl,Bl,ao,Rl,X,ro,Hl,$e,Ul,dn,Gl,Kl,Aa,Zl,Jl,Xl,ot,Ql,Q,so,Yl,xe,ed,cn,td,od,va,nd,ad,rd,nt,hr,Me,at,ba,io,sd,Fa,id,pr,x,lo,ld,co,dd,La,cd,md,fd,mo,hd,fo,pd,gd,ud,mn,ho,_d,fn,po,Ad,hn,go,gr,Ee,rt,Va,uo,vd,wa,bd,ur,U,_o,Fd,Ao,Ld,vo,Vd,wd,kd,N,bo,yd,Ce,Td,pn,$d,xd,ka,Md,Ed,Cd,st,zd,ya,Pd,Id,Fo,_r,ze,it,Ta,Lo,jd,$a,qd,Ar,G,Vo,Od,wo,Nd,ko,Dd,Wd,Sd,D,yo,Bd,Pe,Rd,gn,Hd,Ud,xa,Gd,Kd,Zd,lt,Jd,Ma,Xd,Qd,To,vr,Ie,dt,Ea,$o,Yd,Ca,ec,br,K,xo,tc,Mo,oc,Eo,nc,ac,rc,W,Co,sc,je,ic,un,lc,dc,za,cc,mc,fc,ct,hc,Pa,pc,gc,zo,Fr;return L=new E({}),ht=new E({}),ut=new E({}),_t=new y({props:{name:"class transformers.FLAVAConfig",anchor:"transformers.FLAVAConfig",parameters:[{name:"image_config_dict",val:" = None"},{name:"text_config_dict",val:" = None"},{name:"multimodal_config_dict",val:" = None"},{name:"hidden_size",val:" = 768"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"projection_dim",val:" = 768"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"initializer_range",val:" = 0.02"},{name:"ce_ignore_index",val:" = -100"},{name:"mim_weight",val:" = 1.0"},{name:"mlm_weight",val:" = 1.0"},{name:"global_contrastive_weight",val:" = 1.0"},{name:"itm_weight",val:" = 1.0"},{name:"mmm_image_weight",val:" = 1.0"},{name:"mmm_text_weight",val:" = 1.0"},{name:"global_backprop_contrastive",val:" = True"},{name:"skip_unmasked_multimodal_encoder",val:" = True"},{name:"return_loss",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.FLAVAConfig.image_config_dict",description:`<strong>image_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>.`,name:"image_config_dict"},{anchor:"transformers.FLAVAConfig.multimodal_config_dict",description:`<strong>multimodal_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>.`,name:"multimodal_config_dict"},{anchor:"transformers.FLAVAConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FLAVAConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FLAVAConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FLAVAConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM unimodal loss`,name:"mim_weight"},{anchor:"transformers.FLAVAConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FLAVAConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FLAVAConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FLAVAConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FLAVAConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FLAVAConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FLAVAConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FLAVAConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FLAVAConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L442"}}),At=new ft({props:{code:`from transformers import FLAVAModel, FLAVAForPretraining, FLAVAConfig

# Initializing a FLAVAConfig with style configuration
configuration = FLAVAConfig()

# Initializing a FLAVAModel and FLAVAForPretraining model from the style configuration
model = FLAVAModel(configuration)
model_pre = FLAVAForPretraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAModel, FLAVAForPretraining, FLAVAConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAModel and FLAVAForPretraining model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FLAVAForPretraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),vt=new y({props:{name:"from_configs",anchor:"transformers.FLAVAConfig.from_configs",parameters:[{name:"image_config",val:": FLAVAImageConfig"},{name:"text_config",val:": FLAVATextConfig"},{name:"multimodal_config",val:": FLAVAMultimodalConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L576",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"
>FLAVAConfig</a></p>
`}}),Ft=new E({}),Lt=new y({props:{name:"class transformers.FLAVATextConfig",anchor:"transformers.FLAVATextConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"type_vocab_size",val:" = 2"},{name:"max_position_embeddings",val:" = 512"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVATextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FLAVATextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel">FLAVATextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FLAVATextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FLAVATextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FLAVATextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVATextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVATextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVATextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVATextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVATextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVATextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVATextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVATextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVATextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVATextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVATextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVATextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L148"}}),wt=new ft({props:{code:`from transformers import FLAVATextModel, FLAVATextConfig

# Initializing a FLAVATextModel with  style configuration
configuration = FLAVATextConfig()

# Initializing a FLAVATextConfig from the  style configuration
model = FLAVATextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVATextModel, FLAVATextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVATextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVATextConfig from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),kt=new E({}),yt=new y({props:{name:"class transformers.FLAVAImageConfig",anchor:"transformers.FLAVAImageConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"mask_token",val:" = True"},{name:"vocab_size",val:" = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>224</code>) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FLAVAImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>16</code>) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FLAVAImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FLAVAImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to True) &#x2014;
Whether to use a mask token or not. Used in MIM loss.`,name:"mask_token"},{anchor:"transformers.FLAVAImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebook">FLAVACodebook</a> used in conjunction with <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel">FLAVAImageModel</a> for MIM.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L32"}}),$t=new ft({props:{code:`from transformers import FLAVAImageModel, FLAVAImageConfig

# Initializing a FLAVAImageModel with  style configuration
configuration = FLAVAImageConfig()

# Initializing a FLAVAImageModel model from the  style configuration
model = FLAVAImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAImageModel, FLAVAImageConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAImageModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),xt=new E({}),Mt=new y({props:{name:"class transformers.FLAVAMultimodalConfig",anchor:"transformers.FLAVAMultimodalConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 6"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"qkv_bias",val:" = True"},{name:"use_cls_token",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FLAVAMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FLAVAMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FLAVAMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FLAVAMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FLAVAMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FLAVAMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FLAVAMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FLAVAMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L275"}}),Ct=new ft({props:{code:`from transformers import FLAVAMultimodalModel, FLAVAMultimodalConfig

# Initializing a FLAVAMultimodalModel with  style configuration
configuration = FLAVAMultimodalConfig()

# Initializing a FLAVAMultimodalModel model from the  style configuration
model = FLAVAMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAMultimodalModel, FLAVAMultimodalConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FLAVAMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FLAVAMultimodalModel model from the  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),zt=new E({}),It=new y({props:{name:"class transformers.FLAVACodebookConfig",anchor:"transformers.FLAVACodebookConfig",parameters:[{name:"num_groups",val:" = 4"},{name:"input_channels",val:" = 3"},{name:"num_blocks_per_group",val:" = 2"},{name:"hidden_size",val:" = 256"},{name:"vocab_size",val:" = 8192"},{name:"freeze",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/configuration_flava.py#L375"}}),jt=new E({}),qt=new y({props:{name:"class transformers.FLAVAProcessor",anchor:"transformers.FLAVAProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""},{name:"mlm_probability",val:" = 0.15"}],parametersDescription:[{anchor:"transformers.FLAVAProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.FLAVAProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>FLAVATokenizerFast</code>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/processing_flava.py#L29"}}),Ot=new y({props:{name:"batch_decode",anchor:"transformers.FLAVAProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/processing_flava.py#L134"}}),Dt=new y({props:{name:"decode",anchor:"transformers.FLAVAProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/processing_flava.py#L141"}}),St=new E({}),Bt=new y({props:{name:"class transformers.FLAVAFeatureExtractor",anchor:"transformers.FLAVAFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 224"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 224"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": typing.Optional[int] = 16"},{name:"mask_group_max_patches",val:": int = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FLAVAFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.FLAVAFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.FLAVAFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.FLAVAFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.FLAVAFeatureExtractor.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FLAVAFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>.`,name:"do_normalize"},{anchor:"transformers.FLAVAFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.FLAVAFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/feature_extraction_flava.py#L120"}}),Ht=new E({}),Gt=new y({props:{name:"class transformers.FLAVACodebookFeatureExtractor",anchor:"transformers.FLAVACodebookFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = 112"},{name:"resample",val:" = <Resampling.LANCZOS: 1>"},{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 112"},{name:"do_map_pixels",val:" = True"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/feature_extraction_flava.py#L272"}}),Kt=new E({}),Zt=new y({props:{name:"class transformers.FLAVAForPretraining",anchor:"transformers.FLAVAForPretraining",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAForPretraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1765"}}),Qt=new y({props:{name:"forward",anchor:"transformers.FLAVAForPretraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAForPretraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1797",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAForPretrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) \u2014 Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FLAVALosses</code>) \u2014 Detailed info for FLAVA Pretraining losses. Check <code>FLAVALosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</p>
</li>
</ul>
<ul>
<li><strong>mim_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape
<code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and
<code>input_ids_masked</code> are not): The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked
patches. The flattened output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mlm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape
<code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and
<code>pixel_values</code> are not): The logits for MLM unimodal loss. The flattened output is returned when
<code>input_ids_masked</code> has some of the tokens masked.</li>
<li><strong>itm_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and
<code>pixel_values</code> are present): The logits for ITM loss. Note that ITM loss is calculated on masked pairs in
FLAVA.</li>
<li><strong>mmm_image_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape
<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code>
are present): The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The
flattened output is returned when <code>bool_masked_pos</code> has some of the patches masked.</li>
<li><strong>mmm_text_logits:</strong>
(<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape
<code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present): The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has some of the tokens masked.</li>
<li><strong>contrastive_logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</li>
<li><strong>contrastive_logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAForPretrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new jo({props:{$$slots:{default:[wf]},$$scope:{ctx:M}}}),Yt=new E({}),eo=new y({props:{name:"class transformers.FLAVAModel",anchor:"transformers.FLAVAModel",parameters:[{name:"config",val:": FLAVAConfig"}],parametersDescription:[{anchor:"transformers.FLAVAModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig">FLAVAConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1223"}}),no=new y({props:{name:"forward",anchor:"transformers.FLAVAModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FLAVAModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1365",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FLAVAConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>image_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"
>FLAVAImageModel</a>.</li>
<li><strong>text_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>text_output(<code>BaseModelOutputWithPooling</code>,</strong> <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_embeddings(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>), <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"
>FLAVATextModel</a>.</li>
<li><strong>multimodal_output(<code>BaseModelOutputWithPooling</code>,</strong> returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"
>FLAVAMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FLAVAModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),tt=new jo({props:{$$slots:{default:[kf]},$$scope:{ctx:M}}}),ao=new ft({props:{code:`from PIL import Image
import requests
from transformers import FLAVAProcessor, FLAVAModel

model = FLAVAModel.from_pretrained("aps/flava-full")
processor = FLAVAProcessor.from_pretrained("aps/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAProcessor, FLAVAModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FLAVAProcessor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),ro=new y({props:{name:"get_text_features",anchor:"transformers.FLAVAModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVAModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVAModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1267"}}),ot=new jo({props:{$$slots:{default:[yf]},$$scope:{ctx:M}}}),so=new y({props:{name:"get_image_features",anchor:"transformers.FLAVAModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1313"}}),nt=new jo({props:{$$slots:{default:[Tf]},$$scope:{ctx:M}}}),io=new E({}),lo=new y({props:{name:"class transformers.FLAVACodebook",anchor:"transformers.FLAVACodebook",parameters:[{name:"config",val:": FLAVACodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FLAVACodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVACodebookConfig">FLAVACodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1512"}}),ho=new y({props:{name:"forward",anchor:"transformers.FLAVACodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1636"}}),po=new y({props:{name:"get_codebook_indices",anchor:"transformers.FLAVACodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1603"}}),go=new y({props:{name:"get_codebook_probs",anchor:"transformers.FLAVACodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1632"}}),uo=new E({}),_o=new y({props:{name:"class transformers.FLAVATextModel",anchor:"transformers.FLAVATextModel",parameters:[{name:"config",val:": FLAVATextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig">FLAVATextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1020"}}),bo=new y({props:{name:"forward",anchor:"transformers.FLAVATextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVATextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/pr_16654/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_16654/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FLAVATextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FLAVATextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVATextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVATextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVATextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVATextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1050",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextConfig"
>FLAVATextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),st=new jo({props:{$$slots:{default:[$f]},$$scope:{ctx:M}}}),Fo=new ft({props:{code:`from transformers import BertTokenizer, FLAVATextModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVATextModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVATextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVATextModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lo=new E({}),Vo=new y({props:{name:"class transformers.FLAVAImageModel",anchor:"transformers.FLAVAImageModel",parameters:[{name:"config",val:": FLAVAImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig">FLAVAImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L921"}}),yo=new y({props:{name:"forward",anchor:"transformers.FLAVAImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor">FLAVAFeatureExtractor</a>. See
<code>FLAVAFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FLAVAImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FLAVAImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FLAVAImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L953",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageConfig"
>FLAVAImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),lt=new jo({props:{$$slots:{default:[xf]},$$scope:{ctx:M}}}),To=new ft({props:{code:`from transformers import FLAVAFeatureExtractor, FLAVAImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FLAVAFeatureExtractor.from_pretrained("aps/flava-full")
model = FLAVAImageModel.from_pretrained("aps/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FLAVAFeatureExtractor, FLAVAImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FLAVAFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAImageModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),$o=new E({}),xo=new y({props:{name:"class transformers.FLAVAMultimodalModel",anchor:"transformers.FLAVAMultimodalModel",parameters:[{name:"config",val:": FLAVAMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig">FLAVAMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16654/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1125"}}),Co=new y({props:{name:"forward",anchor:"transformers.FLAVAMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FLAVAMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FLAVAMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FLAVAMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16654/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16654/src/transformers/models/flava/modeling_flava.py#L1152",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalConfig"
>FLAVAMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16654/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ct=new jo({props:{$$slots:{default:[Mf]},$$scope:{ctx:M}}}),zo=new ft({props:{code:`from transformers import BertTokenizer, FLAVAMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("aps/flava-full")
model = FLAVAMultimodalModel.from_pretrained("aps/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FLAVAMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FLAVAMultimodalModel.from_pretrained(<span class="hljs-string">&quot;aps/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){h=n("meta"),k=d(),F=n("h1"),V=n("a"),w=n("span"),p(L.$$.fragment),b=d(),T=n("span"),Qr=s("FLAVA"),Wa=d(),le=n("h2"),qe=n("a"),On=n("span"),p(ht.$$.fragment),Yr=d(),Nn=n("span"),es=s("Overview"),Sa=d(),Oe=n("p"),ts=s("The FLAVA model was proposed in "),pt=n("a"),os=s("FLAVA: A Foundational Language And Vision Alignment Model"),ns=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),Ba=d(),qo=n("p"),as=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Ra=d(),Oo=n("p"),rs=s("The abstract from the paper is the following:"),Ha=d(),No=n("p"),ss=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),Ua=d(),Ne=n("p"),is=s("This model was contributed by "),gt=n("a"),ls=s("aps"),ds=s("."),Ga=d(),de=n("h2"),De=n("a"),Dn=n("span"),p(ut.$$.fragment),cs=d(),Wn=n("span"),ms=s("FLAVAConfig"),Ka=d(),$=n("div"),p(_t.$$.fragment),fs=d(),We=n("p"),Do=n("a"),hs=s("FLAVAConfig"),ps=s(" is the configuration class to store the configuration of a "),Wo=n("a"),gs=s("FLAVAModel"),us=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),_s=d(),ce=n("p"),As=s("Configuration objects inherit from "),So=n("a"),vs=s("PretrainedConfig"),bs=s(` and can be used to control the model outputs. Read the
documentation from `),Bo=n("a"),Fs=s("PretrainedConfig"),Ls=s(" for more information."),Vs=d(),Sn=n("p"),ws=s("Example:"),ks=d(),p(At.$$.fragment),ys=d(),Se=n("div"),p(vt.$$.fragment),Ts=d(),bt=n("p"),$s=s("Instantiate a "),Ro=n("a"),xs=s("FLAVAConfig"),Ms=s(` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),Za=d(),me=n("h2"),Be=n("a"),Bn=n("span"),p(Ft.$$.fragment),Es=d(),Rn=n("span"),Cs=s("FLAVATextConfig"),Ja=d(),C=n("div"),p(Lt.$$.fragment),zs=d(),fe=n("p"),Ps=s("This is the configuration class to store the configuration of a "),Ho=n("a"),Is=s("FLAVATextModel"),js=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Vt=n("a"),qs=s("full"),Os=s(" architecture."),Ns=d(),he=n("p"),Ds=s("Configuration objects inherit from "),Uo=n("a"),Ws=s("PretrainedConfig"),Ss=s(` and can be used to control the model outputs. Read the
documentation from `),Go=n("a"),Bs=s("PretrainedConfig"),Rs=s(" for more information."),Hs=d(),Hn=n("p"),Us=s("Example:"),Gs=d(),p(wt.$$.fragment),Xa=d(),pe=n("h2"),Re=n("a"),Un=n("span"),p(kt.$$.fragment),Ks=d(),Gn=n("span"),Zs=s("FLAVAImageConfig"),Qa=d(),z=n("div"),p(yt.$$.fragment),Js=d(),ge=n("p"),Xs=s("This is the configuration class to store the configuration of a "),Ko=n("a"),Qs=s("FLAVAImageModel"),Ys=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Tt=n("a"),ei=s("full"),ti=s(" architecture."),oi=d(),ue=n("p"),ni=s("Configuration objects inherit from "),Zo=n("a"),ai=s("PretrainedConfig"),ri=s(` and can be used to control the model outputs. Read the
documentation from `),Jo=n("a"),si=s("PretrainedConfig"),ii=s(" for more information."),li=d(),Kn=n("p"),di=s("Example:"),ci=d(),p($t.$$.fragment),Ya=d(),_e=n("h2"),He=n("a"),Zn=n("span"),p(xt.$$.fragment),mi=d(),Jn=n("span"),fi=s("FLAVAMultimodalConfig"),er=d(),P=n("div"),p(Mt.$$.fragment),hi=d(),Ae=n("p"),pi=s("This is the configuration class to store the configuration of a "),Xo=n("a"),gi=s("FLAVAMultimodalModel"),ui=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Et=n("a"),_i=s("full"),Ai=s(" architecture."),vi=d(),ve=n("p"),bi=s("Configuration objects inherit from "),Qo=n("a"),Fi=s("PretrainedConfig"),Li=s(` and can be used to control the model outputs. Read the
documentation from `),Yo=n("a"),Vi=s("PretrainedConfig"),wi=s(" for more information."),ki=d(),Xn=n("p"),yi=s("Example:"),Ti=d(),p(Ct.$$.fragment),tr=d(),be=n("h2"),Ue=n("a"),Qn=n("span"),p(zt.$$.fragment),$i=d(),Yn=n("span"),xi=s("FLAVACodebookConfig"),or=d(),Pt=n("div"),p(It.$$.fragment),nr=d(),Fe=n("h2"),Ge=n("a"),ea=n("span"),p(jt.$$.fragment),Mi=d(),ta=n("span"),Ei=s("FLAVAProcessor"),ar=d(),I=n("div"),p(qt.$$.fragment),Ci=d(),oa=n("p"),zi=s("Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),Pi=d(),q=n("p"),en=n("a"),Ii=s("FLAVAProcessor"),ji=s(" offers all the functionalities of "),tn=n("a"),qi=s("FLAVAFeatureExtractor"),Oi=s(" and "),na=n("code"),Ni=s("FLAVATokenizerFast"),Di=s(`. See the
`),aa=n("code"),Wi=s("__call__()"),Si=s(" and "),on=n("a"),Bi=s("decode()"),Ri=s(" for more information."),Hi=d(),Ke=n("div"),p(Ot.$$.fragment),Ui=d(),Nt=n("p"),Gi=s("This method forwards all its arguments to FLAVATokenizerFast\u2019s "),nn=n("a"),Ki=s("batch_decode()"),Zi=s(`. Please
refer to the docstring of this method for more information.`),Ji=d(),Ze=n("div"),p(Dt.$$.fragment),Xi=d(),Wt=n("p"),Qi=s("This method forwards all its arguments to FLAVATokenizerFast\u2019s "),an=n("a"),Yi=s("decode()"),el=s(`. Please refer to
the docstring of this method for more information.`),rr=d(),Le=n("h2"),Je=n("a"),ra=n("span"),p(St.$$.fragment),tl=d(),sa=n("span"),ol=s("FLAVAFeatureExtractor"),sr=d(),H=n("div"),p(Bt.$$.fragment),nl=d(),ia=n("p"),al=s("Constructs a FLAVA feature extractor."),rl=d(),Rt=n("p"),sl=s("This feature extractor inherits from "),rn=n("a"),il=s("FeatureExtractionMixin"),ll=s(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),ir=d(),Ve=n("h2"),Xe=n("a"),la=n("span"),p(Ht.$$.fragment),dl=d(),da=n("span"),cl=s("FLAVACodebookFeatureExtractor"),lr=d(),Ut=n("div"),p(Gt.$$.fragment),dr=d(),we=n("h2"),Qe=n("a"),ca=n("span"),p(Kt.$$.fragment),ml=d(),ma=n("span"),fl=s("FLAVAForPretraining"),cr=d(),S=n("div"),p(Zt.$$.fragment),hl=d(),fa=n("p"),pl=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),gl=d(),Jt=n("p"),ul=s("This model is a PyTorch "),Xt=n("a"),_l=s("torch.nn.Module"),Al=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vl=d(),J=n("div"),p(Qt.$$.fragment),bl=d(),ke=n("p"),Fl=s("The "),sn=n("a"),Ll=s("FLAVAForPretraining"),Vl=s(" forward method, overrides the "),ha=n("code"),wl=s("__call__"),kl=s(" special method."),yl=d(),p(Ye.$$.fragment),mr=d(),ye=n("h2"),et=n("a"),pa=n("span"),p(Yt.$$.fragment),Tl=d(),ga=n("span"),$l=s("FLAVAModel"),fr=d(),j=n("div"),p(eo.$$.fragment),xl=d(),to=n("p"),Ml=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),oo=n("a"),El=s("torch.nn.Module"),Cl=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),zl=d(),O=n("div"),p(no.$$.fragment),Pl=d(),Te=n("p"),Il=s("The "),ln=n("a"),jl=s("FLAVAModel"),ql=s(" forward method, overrides the "),ua=n("code"),Ol=s("__call__"),Nl=s(" special method."),Dl=d(),p(tt.$$.fragment),Wl=d(),_a=n("p"),Sl=s("Examples:"),Bl=d(),p(ao.$$.fragment),Rl=d(),X=n("div"),p(ro.$$.fragment),Hl=d(),$e=n("p"),Ul=s("The "),dn=n("a"),Gl=s("FLAVAModel"),Kl=s(" forward method, overrides the "),Aa=n("code"),Zl=s("__call__"),Jl=s(" special method."),Xl=d(),p(ot.$$.fragment),Ql=d(),Q=n("div"),p(so.$$.fragment),Yl=d(),xe=n("p"),ed=s("The "),cn=n("a"),td=s("FLAVAModel"),od=s(" forward method, overrides the "),va=n("code"),nd=s("__call__"),ad=s(" special method."),rd=d(),p(nt.$$.fragment),hr=d(),Me=n("h2"),at=n("a"),ba=n("span"),p(io.$$.fragment),sd=d(),Fa=n("span"),id=s("FLAVACodebook"),pr=d(),x=n("div"),p(lo.$$.fragment),ld=d(),co=n("p"),dd=s(`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),La=n("code"),cd=s("get_codebook_indices"),md=s(" to get image tokens for an image."),fd=d(),mo=n("p"),hd=s("This model is a PyTorch "),fo=n("a"),pd=s("torch.nn.Module"),gd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ud=d(),mn=n("div"),p(ho.$$.fragment),_d=d(),fn=n("div"),p(po.$$.fragment),Ad=d(),hn=n("div"),p(go.$$.fragment),gr=d(),Ee=n("h2"),rt=n("a"),Va=n("span"),p(uo.$$.fragment),vd=d(),wa=n("span"),bd=s("FLAVATextModel"),ur=d(),U=n("div"),p(_o.$$.fragment),Fd=d(),Ao=n("p"),Ld=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),vo=n("a"),Vd=s("torch.nn.Module"),wd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),kd=d(),N=n("div"),p(bo.$$.fragment),yd=d(),Ce=n("p"),Td=s("The "),pn=n("a"),$d=s("FLAVATextModel"),xd=s(" forward method, overrides the "),ka=n("code"),Md=s("__call__"),Ed=s(" special method."),Cd=d(),p(st.$$.fragment),zd=d(),ya=n("p"),Pd=s("Example:"),Id=d(),p(Fo.$$.fragment),_r=d(),ze=n("h2"),it=n("a"),Ta=n("span"),p(Lo.$$.fragment),jd=d(),$a=n("span"),qd=s("FLAVAImageModel"),Ar=d(),G=n("div"),p(Vo.$$.fragment),Od=d(),wo=n("p"),Nd=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),ko=n("a"),Dd=s("torch.nn.Module"),Wd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sd=d(),D=n("div"),p(yo.$$.fragment),Bd=d(),Pe=n("p"),Rd=s("The "),gn=n("a"),Hd=s("FLAVAImageModel"),Ud=s(" forward method, overrides the "),xa=n("code"),Gd=s("__call__"),Kd=s(" special method."),Zd=d(),p(lt.$$.fragment),Jd=d(),Ma=n("p"),Xd=s("Example:"),Qd=d(),p(To.$$.fragment),vr=d(),Ie=n("h2"),dt=n("a"),Ea=n("span"),p($o.$$.fragment),Yd=d(),Ca=n("span"),ec=s("FLAVAMultimodalModel"),br=d(),K=n("div"),p(xo.$$.fragment),tc=d(),Mo=n("p"),oc=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Eo=n("a"),nc=s("torch.nn.Module"),ac=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),rc=d(),W=n("div"),p(Co.$$.fragment),sc=d(),je=n("p"),ic=s("The "),un=n("a"),lc=s("FLAVAMultimodalModel"),dc=s(" forward method, overrides the "),za=n("code"),cc=s("__call__"),mc=s(" special method."),fc=d(),p(ct.$$.fragment),hc=d(),Pa=n("p"),pc=s("Example:"),gc=d(),p(zo.$$.fragment),this.h()},l(t){const m=Lf('[data-svelte="svelte-1phssyn"]',document.head);h=a(m,"META",{name:!0,content:!0}),m.forEach(o),k=c(t),F=a(t,"H1",{class:!0});var Po=r(F);V=a(Po,"A",{id:!0,class:!0,href:!0});var Ia=r(V);w=a(Ia,"SPAN",{});var ja=r(w);g(L.$$.fragment,ja),ja.forEach(o),Ia.forEach(o),b=c(Po),T=a(Po,"SPAN",{});var qa=r(T);Qr=i(qa,"FLAVA"),qa.forEach(o),Po.forEach(o),Wa=c(t),le=a(t,"H2",{class:!0});var Io=r(le);qe=a(Io,"A",{id:!0,class:!0,href:!0});var Oa=r(qe);On=a(Oa,"SPAN",{});var Na=r(On);g(ht.$$.fragment,Na),Na.forEach(o),Oa.forEach(o),Yr=c(Io),Nn=a(Io,"SPAN",{});var uc=r(Nn);es=i(uc,"Overview"),uc.forEach(o),Io.forEach(o),Sa=c(t),Oe=a(t,"P",{});var Lr=r(Oe);ts=i(Lr,"The FLAVA model was proposed in "),pt=a(Lr,"A",{href:!0,rel:!0});var _c=r(pt);os=i(_c,"FLAVA: A Foundational Language And Vision Alignment Model"),_c.forEach(o),ns=i(Lr," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),Lr.forEach(o),Ba=c(t),qo=a(t,"P",{});var Ac=r(qo);as=i(Ac,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Ac.forEach(o),Ra=c(t),Oo=a(t,"P",{});var vc=r(Oo);rs=i(vc,"The abstract from the paper is the following:"),vc.forEach(o),Ha=c(t),No=a(t,"P",{});var bc=r(No);ss=i(bc,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),bc.forEach(o),Ua=c(t),Ne=a(t,"P",{});var Vr=r(Ne);is=i(Vr,"This model was contributed by "),gt=a(Vr,"A",{href:!0,rel:!0});var Fc=r(gt);ls=i(Fc,"aps"),Fc.forEach(o),ds=i(Vr,"."),Vr.forEach(o),Ga=c(t),de=a(t,"H2",{class:!0});var wr=r(de);De=a(wr,"A",{id:!0,class:!0,href:!0});var Lc=r(De);Dn=a(Lc,"SPAN",{});var Vc=r(Dn);g(ut.$$.fragment,Vc),Vc.forEach(o),Lc.forEach(o),cs=c(wr),Wn=a(wr,"SPAN",{});var wc=r(Wn);ms=i(wc,"FLAVAConfig"),wc.forEach(o),wr.forEach(o),Ka=c(t),$=a(t,"DIV",{class:!0});var B=r($);g(_t.$$.fragment,B),fs=c(B),We=a(B,"P",{});var Da=r(We);Do=a(Da,"A",{href:!0});var kc=r(Do);hs=i(kc,"FLAVAConfig"),kc.forEach(o),ps=i(Da," is the configuration class to store the configuration of a "),Wo=a(Da,"A",{href:!0});var yc=r(Wo);gs=i(yc,"FLAVAModel"),yc.forEach(o),us=i(Da,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model and multimodal
model configs.`),Da.forEach(o),_s=c(B),ce=a(B,"P",{});var _n=r(ce);As=i(_n,"Configuration objects inherit from "),So=a(_n,"A",{href:!0});var Tc=r(So);vs=i(Tc,"PretrainedConfig"),Tc.forEach(o),bs=i(_n,` and can be used to control the model outputs. Read the
documentation from `),Bo=a(_n,"A",{href:!0});var $c=r(Bo);Fs=i($c,"PretrainedConfig"),$c.forEach(o),Ls=i(_n," for more information."),_n.forEach(o),Vs=c(B),Sn=a(B,"P",{});var xc=r(Sn);ws=i(xc,"Example:"),xc.forEach(o),ks=c(B),g(At.$$.fragment,B),ys=c(B),Se=a(B,"DIV",{class:!0});var kr=r(Se);g(vt.$$.fragment,kr),Ts=c(kr),bt=a(kr,"P",{});var yr=r(bt);$s=i(yr,"Instantiate a "),Ro=a(yr,"A",{href:!0});var Mc=r(Ro);xs=i(Mc,"FLAVAConfig"),Mc.forEach(o),Ms=i(yr,` (or a derived class) from flava text model configuration, flava image model
configuration and flava multimodal model configuration.`),yr.forEach(o),kr.forEach(o),B.forEach(o),Za=c(t),me=a(t,"H2",{class:!0});var Tr=r(me);Be=a(Tr,"A",{id:!0,class:!0,href:!0});var Ec=r(Be);Bn=a(Ec,"SPAN",{});var Cc=r(Bn);g(Ft.$$.fragment,Cc),Cc.forEach(o),Ec.forEach(o),Es=c(Tr),Rn=a(Tr,"SPAN",{});var zc=r(Rn);Cs=i(zc,"FLAVATextConfig"),zc.forEach(o),Tr.forEach(o),Ja=c(t),C=a(t,"DIV",{class:!0});var Y=r(C);g(Lt.$$.fragment,Y),zs=c(Y),fe=a(Y,"P",{});var An=r(fe);Ps=i(An,"This is the configuration class to store the configuration of a "),Ho=a(An,"A",{href:!0});var Pc=r(Ho);Is=i(Pc,"FLAVATextModel"),Pc.forEach(o),js=i(An,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Vt=a(An,"A",{href:!0,rel:!0});var Ic=r(Vt);qs=i(Ic,"full"),Ic.forEach(o),Os=i(An," architecture."),An.forEach(o),Ns=c(Y),he=a(Y,"P",{});var vn=r(he);Ds=i(vn,"Configuration objects inherit from "),Uo=a(vn,"A",{href:!0});var jc=r(Uo);Ws=i(jc,"PretrainedConfig"),jc.forEach(o),Ss=i(vn,` and can be used to control the model outputs. Read the
documentation from `),Go=a(vn,"A",{href:!0});var qc=r(Go);Bs=i(qc,"PretrainedConfig"),qc.forEach(o),Rs=i(vn," for more information."),vn.forEach(o),Hs=c(Y),Hn=a(Y,"P",{});var Oc=r(Hn);Us=i(Oc,"Example:"),Oc.forEach(o),Gs=c(Y),g(wt.$$.fragment,Y),Y.forEach(o),Xa=c(t),pe=a(t,"H2",{class:!0});var $r=r(pe);Re=a($r,"A",{id:!0,class:!0,href:!0});var Nc=r(Re);Un=a(Nc,"SPAN",{});var Dc=r(Un);g(kt.$$.fragment,Dc),Dc.forEach(o),Nc.forEach(o),Ks=c($r),Gn=a($r,"SPAN",{});var Wc=r(Gn);Zs=i(Wc,"FLAVAImageConfig"),Wc.forEach(o),$r.forEach(o),Qa=c(t),z=a(t,"DIV",{class:!0});var ee=r(z);g(yt.$$.fragment,ee),Js=c(ee),ge=a(ee,"P",{});var bn=r(ge);Xs=i(bn,"This is the configuration class to store the configuration of a "),Ko=a(bn,"A",{href:!0});var Sc=r(Ko);Qs=i(Sc,"FLAVAImageModel"),Sc.forEach(o),Ys=i(bn,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Tt=a(bn,"A",{href:!0,rel:!0});var Bc=r(Tt);ei=i(Bc,"full"),Bc.forEach(o),ti=i(bn," architecture."),bn.forEach(o),oi=c(ee),ue=a(ee,"P",{});var Fn=r(ue);ni=i(Fn,"Configuration objects inherit from "),Zo=a(Fn,"A",{href:!0});var Rc=r(Zo);ai=i(Rc,"PretrainedConfig"),Rc.forEach(o),ri=i(Fn,` and can be used to control the model outputs. Read the
documentation from `),Jo=a(Fn,"A",{href:!0});var Hc=r(Jo);si=i(Hc,"PretrainedConfig"),Hc.forEach(o),ii=i(Fn," for more information."),Fn.forEach(o),li=c(ee),Kn=a(ee,"P",{});var Uc=r(Kn);di=i(Uc,"Example:"),Uc.forEach(o),ci=c(ee),g($t.$$.fragment,ee),ee.forEach(o),Ya=c(t),_e=a(t,"H2",{class:!0});var xr=r(_e);He=a(xr,"A",{id:!0,class:!0,href:!0});var Gc=r(He);Zn=a(Gc,"SPAN",{});var Kc=r(Zn);g(xt.$$.fragment,Kc),Kc.forEach(o),Gc.forEach(o),mi=c(xr),Jn=a(xr,"SPAN",{});var Zc=r(Jn);fi=i(Zc,"FLAVAMultimodalConfig"),Zc.forEach(o),xr.forEach(o),er=c(t),P=a(t,"DIV",{class:!0});var te=r(P);g(Mt.$$.fragment,te),hi=c(te),Ae=a(te,"P",{});var Ln=r(Ae);pi=i(Ln,"This is the configuration class to store the configuration of a "),Xo=a(Ln,"A",{href:!0});var Jc=r(Xo);gi=i(Jc,"FLAVAMultimodalModel"),Jc.forEach(o),ui=i(Ln,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FLAVA
`),Et=a(Ln,"A",{href:!0,rel:!0});var Xc=r(Et);_i=i(Xc,"full"),Xc.forEach(o),Ai=i(Ln," architecture."),Ln.forEach(o),vi=c(te),ve=a(te,"P",{});var Vn=r(ve);bi=i(Vn,"Configuration objects inherit from "),Qo=a(Vn,"A",{href:!0});var Qc=r(Qo);Fi=i(Qc,"PretrainedConfig"),Qc.forEach(o),Li=i(Vn,` and can be used to control the model outputs. Read the
documentation from `),Yo=a(Vn,"A",{href:!0});var Yc=r(Yo);Vi=i(Yc,"PretrainedConfig"),Yc.forEach(o),wi=i(Vn," for more information."),Vn.forEach(o),ki=c(te),Xn=a(te,"P",{});var em=r(Xn);yi=i(em,"Example:"),em.forEach(o),Ti=c(te),g(Ct.$$.fragment,te),te.forEach(o),tr=c(t),be=a(t,"H2",{class:!0});var Mr=r(be);Ue=a(Mr,"A",{id:!0,class:!0,href:!0});var tm=r(Ue);Qn=a(tm,"SPAN",{});var om=r(Qn);g(zt.$$.fragment,om),om.forEach(o),tm.forEach(o),$i=c(Mr),Yn=a(Mr,"SPAN",{});var nm=r(Yn);xi=i(nm,"FLAVACodebookConfig"),nm.forEach(o),Mr.forEach(o),or=c(t),Pt=a(t,"DIV",{class:!0});var am=r(Pt);g(It.$$.fragment,am),am.forEach(o),nr=c(t),Fe=a(t,"H2",{class:!0});var Er=r(Fe);Ge=a(Er,"A",{id:!0,class:!0,href:!0});var rm=r(Ge);ea=a(rm,"SPAN",{});var sm=r(ea);g(jt.$$.fragment,sm),sm.forEach(o),rm.forEach(o),Mi=c(Er),ta=a(Er,"SPAN",{});var im=r(ta);Ei=i(im,"FLAVAProcessor"),im.forEach(o),Er.forEach(o),ar=c(t),I=a(t,"DIV",{class:!0});var oe=r(I);g(qt.$$.fragment,oe),Ci=c(oe),oa=a(oe,"P",{});var lm=r(oa);zi=i(lm,"Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),lm.forEach(o),Pi=c(oe),q=a(oe,"P",{});var Z=r(q);en=a(Z,"A",{href:!0});var dm=r(en);Ii=i(dm,"FLAVAProcessor"),dm.forEach(o),ji=i(Z," offers all the functionalities of "),tn=a(Z,"A",{href:!0});var cm=r(tn);qi=i(cm,"FLAVAFeatureExtractor"),cm.forEach(o),Oi=i(Z," and "),na=a(Z,"CODE",{});var mm=r(na);Ni=i(mm,"FLAVATokenizerFast"),mm.forEach(o),Di=i(Z,`. See the
`),aa=a(Z,"CODE",{});var fm=r(aa);Wi=i(fm,"__call__()"),fm.forEach(o),Si=i(Z," and "),on=a(Z,"A",{href:!0});var hm=r(on);Bi=i(hm,"decode()"),hm.forEach(o),Ri=i(Z," for more information."),Z.forEach(o),Hi=c(oe),Ke=a(oe,"DIV",{class:!0});var Cr=r(Ke);g(Ot.$$.fragment,Cr),Ui=c(Cr),Nt=a(Cr,"P",{});var zr=r(Nt);Gi=i(zr,"This method forwards all its arguments to FLAVATokenizerFast\u2019s "),nn=a(zr,"A",{href:!0});var pm=r(nn);Ki=i(pm,"batch_decode()"),pm.forEach(o),Zi=i(zr,`. Please
refer to the docstring of this method for more information.`),zr.forEach(o),Cr.forEach(o),Ji=c(oe),Ze=a(oe,"DIV",{class:!0});var Pr=r(Ze);g(Dt.$$.fragment,Pr),Xi=c(Pr),Wt=a(Pr,"P",{});var Ir=r(Wt);Qi=i(Ir,"This method forwards all its arguments to FLAVATokenizerFast\u2019s "),an=a(Ir,"A",{href:!0});var gm=r(an);Yi=i(gm,"decode()"),gm.forEach(o),el=i(Ir,`. Please refer to
the docstring of this method for more information.`),Ir.forEach(o),Pr.forEach(o),oe.forEach(o),rr=c(t),Le=a(t,"H2",{class:!0});var jr=r(Le);Je=a(jr,"A",{id:!0,class:!0,href:!0});var um=r(Je);ra=a(um,"SPAN",{});var _m=r(ra);g(St.$$.fragment,_m),_m.forEach(o),um.forEach(o),tl=c(jr),sa=a(jr,"SPAN",{});var Am=r(sa);ol=i(Am,"FLAVAFeatureExtractor"),Am.forEach(o),jr.forEach(o),sr=c(t),H=a(t,"DIV",{class:!0});var wn=r(H);g(Bt.$$.fragment,wn),nl=c(wn),ia=a(wn,"P",{});var vm=r(ia);al=i(vm,"Constructs a FLAVA feature extractor."),vm.forEach(o),rl=c(wn),Rt=a(wn,"P",{});var qr=r(Rt);sl=i(qr,"This feature extractor inherits from "),rn=a(qr,"A",{href:!0});var bm=r(rn);il=i(bm,"FeatureExtractionMixin"),bm.forEach(o),ll=i(qr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),qr.forEach(o),wn.forEach(o),ir=c(t),Ve=a(t,"H2",{class:!0});var Or=r(Ve);Xe=a(Or,"A",{id:!0,class:!0,href:!0});var Fm=r(Xe);la=a(Fm,"SPAN",{});var Lm=r(la);g(Ht.$$.fragment,Lm),Lm.forEach(o),Fm.forEach(o),dl=c(Or),da=a(Or,"SPAN",{});var Vm=r(da);cl=i(Vm,"FLAVACodebookFeatureExtractor"),Vm.forEach(o),Or.forEach(o),lr=c(t),Ut=a(t,"DIV",{class:!0});var wm=r(Ut);g(Gt.$$.fragment,wm),wm.forEach(o),dr=c(t),we=a(t,"H2",{class:!0});var Nr=r(we);Qe=a(Nr,"A",{id:!0,class:!0,href:!0});var km=r(Qe);ca=a(km,"SPAN",{});var ym=r(ca);g(Kt.$$.fragment,ym),ym.forEach(o),km.forEach(o),ml=c(Nr),ma=a(Nr,"SPAN",{});var Tm=r(ma);fl=i(Tm,"FLAVAForPretraining"),Tm.forEach(o),Nr.forEach(o),cr=c(t),S=a(t,"DIV",{class:!0});var mt=r(S);g(Zt.$$.fragment,mt),hl=c(mt),fa=a(mt,"P",{});var $m=r(fa);pl=i($m,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),$m.forEach(o),gl=c(mt),Jt=a(mt,"P",{});var Dr=r(Jt);ul=i(Dr,"This model is a PyTorch "),Xt=a(Dr,"A",{href:!0,rel:!0});var xm=r(Xt);_l=i(xm,"torch.nn.Module"),xm.forEach(o),Al=i(Dr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Dr.forEach(o),vl=c(mt),J=a(mt,"DIV",{class:!0});var kn=r(J);g(Qt.$$.fragment,kn),bl=c(kn),ke=a(kn,"P",{});var yn=r(ke);Fl=i(yn,"The "),sn=a(yn,"A",{href:!0});var Mm=r(sn);Ll=i(Mm,"FLAVAForPretraining"),Mm.forEach(o),Vl=i(yn," forward method, overrides the "),ha=a(yn,"CODE",{});var Em=r(ha);wl=i(Em,"__call__"),Em.forEach(o),kl=i(yn," special method."),yn.forEach(o),yl=c(kn),g(Ye.$$.fragment,kn),kn.forEach(o),mt.forEach(o),mr=c(t),ye=a(t,"H2",{class:!0});var Wr=r(ye);et=a(Wr,"A",{id:!0,class:!0,href:!0});var Cm=r(et);pa=a(Cm,"SPAN",{});var zm=r(pa);g(Yt.$$.fragment,zm),zm.forEach(o),Cm.forEach(o),Tl=c(Wr),ga=a(Wr,"SPAN",{});var Pm=r(ga);$l=i(Pm,"FLAVAModel"),Pm.forEach(o),Wr.forEach(o),fr=c(t),j=a(t,"DIV",{class:!0});var ne=r(j);g(eo.$$.fragment,ne),xl=c(ne),to=a(ne,"P",{});var Sr=r(to);Ml=i(Sr,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),oo=a(Sr,"A",{href:!0,rel:!0});var Im=r(oo);El=i(Im,"torch.nn.Module"),Im.forEach(o),Cl=i(Sr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sr.forEach(o),zl=c(ne),O=a(ne,"DIV",{class:!0});var ae=r(O);g(no.$$.fragment,ae),Pl=c(ae),Te=a(ae,"P",{});var Tn=r(Te);Il=i(Tn,"The "),ln=a(Tn,"A",{href:!0});var jm=r(ln);jl=i(jm,"FLAVAModel"),jm.forEach(o),ql=i(Tn," forward method, overrides the "),ua=a(Tn,"CODE",{});var qm=r(ua);Ol=i(qm,"__call__"),qm.forEach(o),Nl=i(Tn," special method."),Tn.forEach(o),Dl=c(ae),g(tt.$$.fragment,ae),Wl=c(ae),_a=a(ae,"P",{});var Om=r(_a);Sl=i(Om,"Examples:"),Om.forEach(o),Bl=c(ae),g(ao.$$.fragment,ae),ae.forEach(o),Rl=c(ne),X=a(ne,"DIV",{class:!0});var $n=r(X);g(ro.$$.fragment,$n),Hl=c($n),$e=a($n,"P",{});var xn=r($e);Ul=i(xn,"The "),dn=a(xn,"A",{href:!0});var Nm=r(dn);Gl=i(Nm,"FLAVAModel"),Nm.forEach(o),Kl=i(xn," forward method, overrides the "),Aa=a(xn,"CODE",{});var Dm=r(Aa);Zl=i(Dm,"__call__"),Dm.forEach(o),Jl=i(xn," special method."),xn.forEach(o),Xl=c($n),g(ot.$$.fragment,$n),$n.forEach(o),Ql=c(ne),Q=a(ne,"DIV",{class:!0});var Mn=r(Q);g(so.$$.fragment,Mn),Yl=c(Mn),xe=a(Mn,"P",{});var En=r(xe);ed=i(En,"The "),cn=a(En,"A",{href:!0});var Wm=r(cn);td=i(Wm,"FLAVAModel"),Wm.forEach(o),od=i(En," forward method, overrides the "),va=a(En,"CODE",{});var Sm=r(va);nd=i(Sm,"__call__"),Sm.forEach(o),ad=i(En," special method."),En.forEach(o),rd=c(Mn),g(nt.$$.fragment,Mn),Mn.forEach(o),ne.forEach(o),hr=c(t),Me=a(t,"H2",{class:!0});var Br=r(Me);at=a(Br,"A",{id:!0,class:!0,href:!0});var Bm=r(at);ba=a(Bm,"SPAN",{});var Rm=r(ba);g(io.$$.fragment,Rm),Rm.forEach(o),Bm.forEach(o),sd=c(Br),Fa=a(Br,"SPAN",{});var Hm=r(Fa);id=i(Hm,"FLAVACodebook"),Hm.forEach(o),Br.forEach(o),pr=c(t),x=a(t,"DIV",{class:!0});var R=r(x);g(lo.$$.fragment,R),ld=c(R),co=a(R,"P",{});var Rr=r(co);dd=i(Rr,`The FLAVA\u2019s codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used to
generate image tokens for an image based on DALL-E\u2019s vocab. To be used to generate labels for MIM. Use
`),La=a(Rr,"CODE",{});var Um=r(La);cd=i(Um,"get_codebook_indices"),Um.forEach(o),md=i(Rr," to get image tokens for an image."),Rr.forEach(o),fd=c(R),mo=a(R,"P",{});var Hr=r(mo);hd=i(Hr,"This model is a PyTorch "),fo=a(Hr,"A",{href:!0,rel:!0});var Gm=r(fo);pd=i(Gm,"torch.nn.Module"),Gm.forEach(o),gd=i(Hr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Hr.forEach(o),ud=c(R),mn=a(R,"DIV",{class:!0});var Km=r(mn);g(ho.$$.fragment,Km),Km.forEach(o),_d=c(R),fn=a(R,"DIV",{class:!0});var Zm=r(fn);g(po.$$.fragment,Zm),Zm.forEach(o),Ad=c(R),hn=a(R,"DIV",{class:!0});var Jm=r(hn);g(go.$$.fragment,Jm),Jm.forEach(o),R.forEach(o),gr=c(t),Ee=a(t,"H2",{class:!0});var Ur=r(Ee);rt=a(Ur,"A",{id:!0,class:!0,href:!0});var Xm=r(rt);Va=a(Xm,"SPAN",{});var Qm=r(Va);g(uo.$$.fragment,Qm),Qm.forEach(o),Xm.forEach(o),vd=c(Ur),wa=a(Ur,"SPAN",{});var Ym=r(wa);bd=i(Ym,"FLAVATextModel"),Ym.forEach(o),Ur.forEach(o),ur=c(t),U=a(t,"DIV",{class:!0});var Cn=r(U);g(_o.$$.fragment,Cn),Fd=c(Cn),Ao=a(Cn,"P",{});var Gr=r(Ao);Ld=i(Gr,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),vo=a(Gr,"A",{href:!0,rel:!0});var ef=r(vo);Vd=i(ef,"torch.nn.Module"),ef.forEach(o),wd=i(Gr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Gr.forEach(o),kd=c(Cn),N=a(Cn,"DIV",{class:!0});var re=r(N);g(bo.$$.fragment,re),yd=c(re),Ce=a(re,"P",{});var zn=r(Ce);Td=i(zn,"The "),pn=a(zn,"A",{href:!0});var tf=r(pn);$d=i(tf,"FLAVATextModel"),tf.forEach(o),xd=i(zn," forward method, overrides the "),ka=a(zn,"CODE",{});var of=r(ka);Md=i(of,"__call__"),of.forEach(o),Ed=i(zn," special method."),zn.forEach(o),Cd=c(re),g(st.$$.fragment,re),zd=c(re),ya=a(re,"P",{});var nf=r(ya);Pd=i(nf,"Example:"),nf.forEach(o),Id=c(re),g(Fo.$$.fragment,re),re.forEach(o),Cn.forEach(o),_r=c(t),ze=a(t,"H2",{class:!0});var Kr=r(ze);it=a(Kr,"A",{id:!0,class:!0,href:!0});var af=r(it);Ta=a(af,"SPAN",{});var rf=r(Ta);g(Lo.$$.fragment,rf),rf.forEach(o),af.forEach(o),jd=c(Kr),$a=a(Kr,"SPAN",{});var sf=r($a);qd=i(sf,"FLAVAImageModel"),sf.forEach(o),Kr.forEach(o),Ar=c(t),G=a(t,"DIV",{class:!0});var Pn=r(G);g(Vo.$$.fragment,Pn),Od=c(Pn),wo=a(Pn,"P",{});var Zr=r(wo);Nd=i(Zr,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),ko=a(Zr,"A",{href:!0,rel:!0});var lf=r(ko);Dd=i(lf,"torch.nn.Module"),lf.forEach(o),Wd=i(Zr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Zr.forEach(o),Sd=c(Pn),D=a(Pn,"DIV",{class:!0});var se=r(D);g(yo.$$.fragment,se),Bd=c(se),Pe=a(se,"P",{});var In=r(Pe);Rd=i(In,"The "),gn=a(In,"A",{href:!0});var df=r(gn);Hd=i(df,"FLAVAImageModel"),df.forEach(o),Ud=i(In," forward method, overrides the "),xa=a(In,"CODE",{});var cf=r(xa);Gd=i(cf,"__call__"),cf.forEach(o),Kd=i(In," special method."),In.forEach(o),Zd=c(se),g(lt.$$.fragment,se),Jd=c(se),Ma=a(se,"P",{});var mf=r(Ma);Xd=i(mf,"Example:"),mf.forEach(o),Qd=c(se),g(To.$$.fragment,se),se.forEach(o),Pn.forEach(o),vr=c(t),Ie=a(t,"H2",{class:!0});var Jr=r(Ie);dt=a(Jr,"A",{id:!0,class:!0,href:!0});var ff=r(dt);Ea=a(ff,"SPAN",{});var hf=r(Ea);g($o.$$.fragment,hf),hf.forEach(o),ff.forEach(o),Yd=c(Jr),Ca=a(Jr,"SPAN",{});var pf=r(Ca);ec=i(pf,"FLAVAMultimodalModel"),pf.forEach(o),Jr.forEach(o),br=c(t),K=a(t,"DIV",{class:!0});var jn=r(K);g(xo.$$.fragment,jn),tc=c(jn),Mo=a(jn,"P",{});var Xr=r(Mo);oc=i(Xr,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Eo=a(Xr,"A",{href:!0,rel:!0});var gf=r(Eo);nc=i(gf,"torch.nn.Module"),gf.forEach(o),ac=i(Xr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Xr.forEach(o),rc=c(jn),W=a(jn,"DIV",{class:!0});var ie=r(W);g(Co.$$.fragment,ie),sc=c(ie),je=a(ie,"P",{});var qn=r(je);ic=i(qn,"The "),un=a(qn,"A",{href:!0});var uf=r(un);lc=i(uf,"FLAVAMultimodalModel"),uf.forEach(o),dc=i(qn," forward method, overrides the "),za=a(qn,"CODE",{});var _f=r(za);cc=i(_f,"__call__"),_f.forEach(o),mc=i(qn," special method."),qn.forEach(o),fc=c(ie),g(ct.$$.fragment,ie),hc=c(ie),Pa=a(ie,"P",{});var Af=r(Pa);pc=i(Af,"Example:"),Af.forEach(o),gc=c(ie),g(zo.$$.fragment,ie),ie.forEach(o),jn.forEach(o),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify(Cf)),l(V,"id","flava"),l(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(V,"href","#flava"),l(F,"class","relative group"),l(qe,"id","overview"),l(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(qe,"href","#overview"),l(le,"class","relative group"),l(pt,"href","https://arxiv.org/abs/2112.04482"),l(pt,"rel","nofollow"),l(gt,"href","https://huggingface.co/aps"),l(gt,"rel","nofollow"),l(De,"id","transformers.FLAVAConfig"),l(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(De,"href","#transformers.FLAVAConfig"),l(de,"class","relative group"),l(Do,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(Wo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(So,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Bo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Ro,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAConfig"),l(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Be,"id","transformers.FLAVATextConfig"),l(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Be,"href","#transformers.FLAVATextConfig"),l(me,"class","relative group"),l(Ho,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(Vt,"href","https://huggingface.co/aps/flava-full"),l(Vt,"rel","nofollow"),l(Uo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Go,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Re,"id","transformers.FLAVAImageConfig"),l(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Re,"href","#transformers.FLAVAImageConfig"),l(pe,"class","relative group"),l(Ko,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(Tt,"href","https://huggingface.co/aps/flava-full"),l(Tt,"rel","nofollow"),l(Zo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Jo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(He,"id","transformers.FLAVAMultimodalConfig"),l(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(He,"href","#transformers.FLAVAMultimodalConfig"),l(_e,"class","relative group"),l(Xo,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(Et,"href","https://huggingface.co/aps/flava-full"),l(Et,"rel","nofollow"),l(Qo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(Yo,"href","/docs/transformers/pr_16654/en/main_classes/configuration#transformers.PretrainedConfig"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ue,"id","transformers.FLAVACodebookConfig"),l(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ue,"href","#transformers.FLAVACodebookConfig"),l(be,"class","relative group"),l(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ge,"id","transformers.FLAVAProcessor"),l(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ge,"href","#transformers.FLAVAProcessor"),l(Fe,"class","relative group"),l(en,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAProcessor"),l(tn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAFeatureExtractor"),l(on,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAProcessor.decode"),l(nn,"href","/docs/transformers/pr_16654/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.batch_decode"),l(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(an,"href","/docs/transformers/pr_16654/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.decode"),l(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Je,"id","transformers.FLAVAFeatureExtractor"),l(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Je,"href","#transformers.FLAVAFeatureExtractor"),l(Le,"class","relative group"),l(rn,"href","/docs/transformers/pr_16654/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),l(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Xe,"id","transformers.FLAVACodebookFeatureExtractor"),l(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Xe,"href","#transformers.FLAVACodebookFeatureExtractor"),l(Ve,"class","relative group"),l(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Qe,"id","transformers.FLAVAForPretraining"),l(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Qe,"href","#transformers.FLAVAForPretraining"),l(we,"class","relative group"),l(Xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Xt,"rel","nofollow"),l(sn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAForPretraining"),l(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(et,"id","transformers.FLAVAModel"),l(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(et,"href","#transformers.FLAVAModel"),l(ye,"class","relative group"),l(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(oo,"rel","nofollow"),l(ln,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(dn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(cn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAModel"),l(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(at,"id","transformers.FLAVACodebook"),l(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(at,"href","#transformers.FLAVACodebook"),l(Me,"class","relative group"),l(fo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(fo,"rel","nofollow"),l(mn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(hn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(rt,"id","transformers.FLAVATextModel"),l(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(rt,"href","#transformers.FLAVATextModel"),l(Ee,"class","relative group"),l(vo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(vo,"rel","nofollow"),l(pn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVATextModel"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(it,"id","transformers.FLAVAImageModel"),l(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(it,"href","#transformers.FLAVAImageModel"),l(ze,"class","relative group"),l(ko,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(ko,"rel","nofollow"),l(gn,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAImageModel"),l(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(dt,"id","transformers.FLAVAMultimodalModel"),l(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(dt,"href","#transformers.FLAVAMultimodalModel"),l(Ie,"class","relative group"),l(Eo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Eo,"rel","nofollow"),l(un,"href","/docs/transformers/pr_16654/en/model_doc/flava#transformers.FLAVAMultimodalModel"),l(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,h),f(t,k,m),f(t,F,m),e(F,V),e(V,w),u(L,w,null),e(F,b),e(F,T),e(T,Qr),f(t,Wa,m),f(t,le,m),e(le,qe),e(qe,On),u(ht,On,null),e(le,Yr),e(le,Nn),e(Nn,es),f(t,Sa,m),f(t,Oe,m),e(Oe,ts),e(Oe,pt),e(pt,os),e(Oe,ns),f(t,Ba,m),f(t,qo,m),e(qo,as),f(t,Ra,m),f(t,Oo,m),e(Oo,rs),f(t,Ha,m),f(t,No,m),e(No,ss),f(t,Ua,m),f(t,Ne,m),e(Ne,is),e(Ne,gt),e(gt,ls),e(Ne,ds),f(t,Ga,m),f(t,de,m),e(de,De),e(De,Dn),u(ut,Dn,null),e(de,cs),e(de,Wn),e(Wn,ms),f(t,Ka,m),f(t,$,m),u(_t,$,null),e($,fs),e($,We),e(We,Do),e(Do,hs),e(We,ps),e(We,Wo),e(Wo,gs),e(We,us),e($,_s),e($,ce),e(ce,As),e(ce,So),e(So,vs),e(ce,bs),e(ce,Bo),e(Bo,Fs),e(ce,Ls),e($,Vs),e($,Sn),e(Sn,ws),e($,ks),u(At,$,null),e($,ys),e($,Se),u(vt,Se,null),e(Se,Ts),e(Se,bt),e(bt,$s),e(bt,Ro),e(Ro,xs),e(bt,Ms),f(t,Za,m),f(t,me,m),e(me,Be),e(Be,Bn),u(Ft,Bn,null),e(me,Es),e(me,Rn),e(Rn,Cs),f(t,Ja,m),f(t,C,m),u(Lt,C,null),e(C,zs),e(C,fe),e(fe,Ps),e(fe,Ho),e(Ho,Is),e(fe,js),e(fe,Vt),e(Vt,qs),e(fe,Os),e(C,Ns),e(C,he),e(he,Ds),e(he,Uo),e(Uo,Ws),e(he,Ss),e(he,Go),e(Go,Bs),e(he,Rs),e(C,Hs),e(C,Hn),e(Hn,Us),e(C,Gs),u(wt,C,null),f(t,Xa,m),f(t,pe,m),e(pe,Re),e(Re,Un),u(kt,Un,null),e(pe,Ks),e(pe,Gn),e(Gn,Zs),f(t,Qa,m),f(t,z,m),u(yt,z,null),e(z,Js),e(z,ge),e(ge,Xs),e(ge,Ko),e(Ko,Qs),e(ge,Ys),e(ge,Tt),e(Tt,ei),e(ge,ti),e(z,oi),e(z,ue),e(ue,ni),e(ue,Zo),e(Zo,ai),e(ue,ri),e(ue,Jo),e(Jo,si),e(ue,ii),e(z,li),e(z,Kn),e(Kn,di),e(z,ci),u($t,z,null),f(t,Ya,m),f(t,_e,m),e(_e,He),e(He,Zn),u(xt,Zn,null),e(_e,mi),e(_e,Jn),e(Jn,fi),f(t,er,m),f(t,P,m),u(Mt,P,null),e(P,hi),e(P,Ae),e(Ae,pi),e(Ae,Xo),e(Xo,gi),e(Ae,ui),e(Ae,Et),e(Et,_i),e(Ae,Ai),e(P,vi),e(P,ve),e(ve,bi),e(ve,Qo),e(Qo,Fi),e(ve,Li),e(ve,Yo),e(Yo,Vi),e(ve,wi),e(P,ki),e(P,Xn),e(Xn,yi),e(P,Ti),u(Ct,P,null),f(t,tr,m),f(t,be,m),e(be,Ue),e(Ue,Qn),u(zt,Qn,null),e(be,$i),e(be,Yn),e(Yn,xi),f(t,or,m),f(t,Pt,m),u(It,Pt,null),f(t,nr,m),f(t,Fe,m),e(Fe,Ge),e(Ge,ea),u(jt,ea,null),e(Fe,Mi),e(Fe,ta),e(ta,Ei),f(t,ar,m),f(t,I,m),u(qt,I,null),e(I,Ci),e(I,oa),e(oa,zi),e(I,Pi),e(I,q),e(q,en),e(en,Ii),e(q,ji),e(q,tn),e(tn,qi),e(q,Oi),e(q,na),e(na,Ni),e(q,Di),e(q,aa),e(aa,Wi),e(q,Si),e(q,on),e(on,Bi),e(q,Ri),e(I,Hi),e(I,Ke),u(Ot,Ke,null),e(Ke,Ui),e(Ke,Nt),e(Nt,Gi),e(Nt,nn),e(nn,Ki),e(Nt,Zi),e(I,Ji),e(I,Ze),u(Dt,Ze,null),e(Ze,Xi),e(Ze,Wt),e(Wt,Qi),e(Wt,an),e(an,Yi),e(Wt,el),f(t,rr,m),f(t,Le,m),e(Le,Je),e(Je,ra),u(St,ra,null),e(Le,tl),e(Le,sa),e(sa,ol),f(t,sr,m),f(t,H,m),u(Bt,H,null),e(H,nl),e(H,ia),e(ia,al),e(H,rl),e(H,Rt),e(Rt,sl),e(Rt,rn),e(rn,il),e(Rt,ll),f(t,ir,m),f(t,Ve,m),e(Ve,Xe),e(Xe,la),u(Ht,la,null),e(Ve,dl),e(Ve,da),e(da,cl),f(t,lr,m),f(t,Ut,m),u(Gt,Ut,null),f(t,dr,m),f(t,we,m),e(we,Qe),e(Qe,ca),u(Kt,ca,null),e(we,ml),e(we,ma),e(ma,fl),f(t,cr,m),f(t,S,m),u(Zt,S,null),e(S,hl),e(S,fa),e(fa,pl),e(S,gl),e(S,Jt),e(Jt,ul),e(Jt,Xt),e(Xt,_l),e(Jt,Al),e(S,vl),e(S,J),u(Qt,J,null),e(J,bl),e(J,ke),e(ke,Fl),e(ke,sn),e(sn,Ll),e(ke,Vl),e(ke,ha),e(ha,wl),e(ke,kl),e(J,yl),u(Ye,J,null),f(t,mr,m),f(t,ye,m),e(ye,et),e(et,pa),u(Yt,pa,null),e(ye,Tl),e(ye,ga),e(ga,$l),f(t,fr,m),f(t,j,m),u(eo,j,null),e(j,xl),e(j,to),e(to,Ml),e(to,oo),e(oo,El),e(to,Cl),e(j,zl),e(j,O),u(no,O,null),e(O,Pl),e(O,Te),e(Te,Il),e(Te,ln),e(ln,jl),e(Te,ql),e(Te,ua),e(ua,Ol),e(Te,Nl),e(O,Dl),u(tt,O,null),e(O,Wl),e(O,_a),e(_a,Sl),e(O,Bl),u(ao,O,null),e(j,Rl),e(j,X),u(ro,X,null),e(X,Hl),e(X,$e),e($e,Ul),e($e,dn),e(dn,Gl),e($e,Kl),e($e,Aa),e(Aa,Zl),e($e,Jl),e(X,Xl),u(ot,X,null),e(j,Ql),e(j,Q),u(so,Q,null),e(Q,Yl),e(Q,xe),e(xe,ed),e(xe,cn),e(cn,td),e(xe,od),e(xe,va),e(va,nd),e(xe,ad),e(Q,rd),u(nt,Q,null),f(t,hr,m),f(t,Me,m),e(Me,at),e(at,ba),u(io,ba,null),e(Me,sd),e(Me,Fa),e(Fa,id),f(t,pr,m),f(t,x,m),u(lo,x,null),e(x,ld),e(x,co),e(co,dd),e(co,La),e(La,cd),e(co,md),e(x,fd),e(x,mo),e(mo,hd),e(mo,fo),e(fo,pd),e(mo,gd),e(x,ud),e(x,mn),u(ho,mn,null),e(x,_d),e(x,fn),u(po,fn,null),e(x,Ad),e(x,hn),u(go,hn,null),f(t,gr,m),f(t,Ee,m),e(Ee,rt),e(rt,Va),u(uo,Va,null),e(Ee,vd),e(Ee,wa),e(wa,bd),f(t,ur,m),f(t,U,m),u(_o,U,null),e(U,Fd),e(U,Ao),e(Ao,Ld),e(Ao,vo),e(vo,Vd),e(Ao,wd),e(U,kd),e(U,N),u(bo,N,null),e(N,yd),e(N,Ce),e(Ce,Td),e(Ce,pn),e(pn,$d),e(Ce,xd),e(Ce,ka),e(ka,Md),e(Ce,Ed),e(N,Cd),u(st,N,null),e(N,zd),e(N,ya),e(ya,Pd),e(N,Id),u(Fo,N,null),f(t,_r,m),f(t,ze,m),e(ze,it),e(it,Ta),u(Lo,Ta,null),e(ze,jd),e(ze,$a),e($a,qd),f(t,Ar,m),f(t,G,m),u(Vo,G,null),e(G,Od),e(G,wo),e(wo,Nd),e(wo,ko),e(ko,Dd),e(wo,Wd),e(G,Sd),e(G,D),u(yo,D,null),e(D,Bd),e(D,Pe),e(Pe,Rd),e(Pe,gn),e(gn,Hd),e(Pe,Ud),e(Pe,xa),e(xa,Gd),e(Pe,Kd),e(D,Zd),u(lt,D,null),e(D,Jd),e(D,Ma),e(Ma,Xd),e(D,Qd),u(To,D,null),f(t,vr,m),f(t,Ie,m),e(Ie,dt),e(dt,Ea),u($o,Ea,null),e(Ie,Yd),e(Ie,Ca),e(Ca,ec),f(t,br,m),f(t,K,m),u(xo,K,null),e(K,tc),e(K,Mo),e(Mo,oc),e(Mo,Eo),e(Eo,nc),e(Mo,ac),e(K,rc),e(K,W),u(Co,W,null),e(W,sc),e(W,je),e(je,ic),e(je,un),e(un,lc),e(je,dc),e(je,za),e(za,cc),e(je,mc),e(W,fc),u(ct,W,null),e(W,hc),e(W,Pa),e(Pa,pc),e(W,gc),u(zo,W,null),Fr=!0},p(t,[m]){const Po={};m&2&&(Po.$$scope={dirty:m,ctx:t}),Ye.$set(Po);const Ia={};m&2&&(Ia.$$scope={dirty:m,ctx:t}),tt.$set(Ia);const ja={};m&2&&(ja.$$scope={dirty:m,ctx:t}),ot.$set(ja);const qa={};m&2&&(qa.$$scope={dirty:m,ctx:t}),nt.$set(qa);const Io={};m&2&&(Io.$$scope={dirty:m,ctx:t}),st.$set(Io);const Oa={};m&2&&(Oa.$$scope={dirty:m,ctx:t}),lt.$set(Oa);const Na={};m&2&&(Na.$$scope={dirty:m,ctx:t}),ct.$set(Na)},i(t){Fr||(_(L.$$.fragment,t),_(ht.$$.fragment,t),_(ut.$$.fragment,t),_(_t.$$.fragment,t),_(At.$$.fragment,t),_(vt.$$.fragment,t),_(Ft.$$.fragment,t),_(Lt.$$.fragment,t),_(wt.$$.fragment,t),_(kt.$$.fragment,t),_(yt.$$.fragment,t),_($t.$$.fragment,t),_(xt.$$.fragment,t),_(Mt.$$.fragment,t),_(Ct.$$.fragment,t),_(zt.$$.fragment,t),_(It.$$.fragment,t),_(jt.$$.fragment,t),_(qt.$$.fragment,t),_(Ot.$$.fragment,t),_(Dt.$$.fragment,t),_(St.$$.fragment,t),_(Bt.$$.fragment,t),_(Ht.$$.fragment,t),_(Gt.$$.fragment,t),_(Kt.$$.fragment,t),_(Zt.$$.fragment,t),_(Qt.$$.fragment,t),_(Ye.$$.fragment,t),_(Yt.$$.fragment,t),_(eo.$$.fragment,t),_(no.$$.fragment,t),_(tt.$$.fragment,t),_(ao.$$.fragment,t),_(ro.$$.fragment,t),_(ot.$$.fragment,t),_(so.$$.fragment,t),_(nt.$$.fragment,t),_(io.$$.fragment,t),_(lo.$$.fragment,t),_(ho.$$.fragment,t),_(po.$$.fragment,t),_(go.$$.fragment,t),_(uo.$$.fragment,t),_(_o.$$.fragment,t),_(bo.$$.fragment,t),_(st.$$.fragment,t),_(Fo.$$.fragment,t),_(Lo.$$.fragment,t),_(Vo.$$.fragment,t),_(yo.$$.fragment,t),_(lt.$$.fragment,t),_(To.$$.fragment,t),_($o.$$.fragment,t),_(xo.$$.fragment,t),_(Co.$$.fragment,t),_(ct.$$.fragment,t),_(zo.$$.fragment,t),Fr=!0)},o(t){A(L.$$.fragment,t),A(ht.$$.fragment,t),A(ut.$$.fragment,t),A(_t.$$.fragment,t),A(At.$$.fragment,t),A(vt.$$.fragment,t),A(Ft.$$.fragment,t),A(Lt.$$.fragment,t),A(wt.$$.fragment,t),A(kt.$$.fragment,t),A(yt.$$.fragment,t),A($t.$$.fragment,t),A(xt.$$.fragment,t),A(Mt.$$.fragment,t),A(Ct.$$.fragment,t),A(zt.$$.fragment,t),A(It.$$.fragment,t),A(jt.$$.fragment,t),A(qt.$$.fragment,t),A(Ot.$$.fragment,t),A(Dt.$$.fragment,t),A(St.$$.fragment,t),A(Bt.$$.fragment,t),A(Ht.$$.fragment,t),A(Gt.$$.fragment,t),A(Kt.$$.fragment,t),A(Zt.$$.fragment,t),A(Qt.$$.fragment,t),A(Ye.$$.fragment,t),A(Yt.$$.fragment,t),A(eo.$$.fragment,t),A(no.$$.fragment,t),A(tt.$$.fragment,t),A(ao.$$.fragment,t),A(ro.$$.fragment,t),A(ot.$$.fragment,t),A(so.$$.fragment,t),A(nt.$$.fragment,t),A(io.$$.fragment,t),A(lo.$$.fragment,t),A(ho.$$.fragment,t),A(po.$$.fragment,t),A(go.$$.fragment,t),A(uo.$$.fragment,t),A(_o.$$.fragment,t),A(bo.$$.fragment,t),A(st.$$.fragment,t),A(Fo.$$.fragment,t),A(Lo.$$.fragment,t),A(Vo.$$.fragment,t),A(yo.$$.fragment,t),A(lt.$$.fragment,t),A(To.$$.fragment,t),A($o.$$.fragment,t),A(xo.$$.fragment,t),A(Co.$$.fragment,t),A(ct.$$.fragment,t),A(zo.$$.fragment,t),Fr=!1},d(t){o(h),t&&o(k),t&&o(F),v(L),t&&o(Wa),t&&o(le),v(ht),t&&o(Sa),t&&o(Oe),t&&o(Ba),t&&o(qo),t&&o(Ra),t&&o(Oo),t&&o(Ha),t&&o(No),t&&o(Ua),t&&o(Ne),t&&o(Ga),t&&o(de),v(ut),t&&o(Ka),t&&o($),v(_t),v(At),v(vt),t&&o(Za),t&&o(me),v(Ft),t&&o(Ja),t&&o(C),v(Lt),v(wt),t&&o(Xa),t&&o(pe),v(kt),t&&o(Qa),t&&o(z),v(yt),v($t),t&&o(Ya),t&&o(_e),v(xt),t&&o(er),t&&o(P),v(Mt),v(Ct),t&&o(tr),t&&o(be),v(zt),t&&o(or),t&&o(Pt),v(It),t&&o(nr),t&&o(Fe),v(jt),t&&o(ar),t&&o(I),v(qt),v(Ot),v(Dt),t&&o(rr),t&&o(Le),v(St),t&&o(sr),t&&o(H),v(Bt),t&&o(ir),t&&o(Ve),v(Ht),t&&o(lr),t&&o(Ut),v(Gt),t&&o(dr),t&&o(we),v(Kt),t&&o(cr),t&&o(S),v(Zt),v(Qt),v(Ye),t&&o(mr),t&&o(ye),v(Yt),t&&o(fr),t&&o(j),v(eo),v(no),v(tt),v(ao),v(ro),v(ot),v(so),v(nt),t&&o(hr),t&&o(Me),v(io),t&&o(pr),t&&o(x),v(lo),v(ho),v(po),v(go),t&&o(gr),t&&o(Ee),v(uo),t&&o(ur),t&&o(U),v(_o),v(bo),v(st),v(Fo),t&&o(_r),t&&o(ze),v(Lo),t&&o(Ar),t&&o(G),v(Vo),v(yo),v(lt),v(To),t&&o(vr),t&&o(Ie),v($o),t&&o(br),t&&o(K),v(xo),v(Co),v(ct),v(zo)}}}const Cf={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FLAVAConfig",title:"FLAVAConfig"},{local:"transformers.FLAVATextConfig",title:"FLAVATextConfig"},{local:"transformers.FLAVAImageConfig",title:"FLAVAImageConfig"},{local:"transformers.FLAVAMultimodalConfig",title:"FLAVAMultimodalConfig"},{local:"transformers.FLAVACodebookConfig",title:"FLAVACodebookConfig"},{local:"transformers.FLAVAProcessor",title:"FLAVAProcessor"},{local:"transformers.FLAVAFeatureExtractor",title:"FLAVAFeatureExtractor"},{local:"transformers.FLAVACodebookFeatureExtractor",title:"FLAVACodebookFeatureExtractor"},{local:"transformers.FLAVAForPretraining",title:"FLAVAForPretraining"},{local:"transformers.FLAVAModel",title:"FLAVAModel"},{local:"transformers.FLAVACodebook",title:"FLAVACodebook"},{local:"transformers.FLAVATextModel",title:"FLAVATextModel"},{local:"transformers.FLAVAImageModel",title:"FLAVAImageModel"},{local:"transformers.FLAVAMultimodalModel",title:"FLAVAMultimodalModel"}],title:"FLAVA"};function zf(M){return Vf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nf extends vf{constructor(h){super();bf(this,h,zf,Ef,Ff,{})}}export{Nf as default,Cf as metadata};
