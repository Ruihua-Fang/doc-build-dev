import{S as hw,i as fw,s as pw,e as l,k as c,w as p,t as r,L as mw,c as s,d as o,m as h,a as i,x as m,h as a,b as f,J as e,g as d,y as u,K as uw,q as _,o as v,B as E}from"../chunks/vendor-9e2b328e.js";import{I as $}from"../chunks/IconCopyLink-fd0e58fd.js";import{C as x}from"../chunks/CodeBlock-88e23343.js";import"../chunks/CopyButton-4ae140ab.js";function _w(Ec){let Z,Zo,A,F,Lr,no,bc,Ir,wc,Bi,ve,Ge,Sr,co,yc,Ue,kc,Mr,Tc,gc,jr,$c,qi,er,Cc,Ni,Ee,Xe,Fr,ho,Oc,Br,Dc,Hi,tr,xc,Ri,or,Ac,Wi,Ke,qr,zc,Pc,Nr,Lc,Gi,be,Je,Hr,fo,Ic,Rr,Sc,Ui,Qe,po,Mc,Ve,jc,Wr,Fc,Bc,qc,Nc,we,Hc,Gr,Rc,Wc,Ur,Gc,Uc,Xi,Ye,Xc,Xr,Kc,Jc,Ki,mo,Ji,Ze,Qc,Kr,Vc,Yc,Qi,uo,Vi,ye,et,Jr,_o,Zc,Qr,eh,Yi,z,th,Vr,oh,rh,Yr,ah,lh,Zr,sh,ih,Zi,tt,nh,ea,dh,ch,en,T,ta,oa,hh,fh,ra,aa,ph,mh,la,sa,uh,_h,ia,na,vh,Eh,da,ca,bh,wh,ha,fa,yh,kh,pa,ma,Th,gh,ua,_a,$h,tn,ke,ot,va,vo,Ch,Ea,Oh,on,ee,Dh,ba,xh,Ah,wa,zh,Ph,rn,rt,Lh,ya,Ih,Sh,an,Eo,ln,at,Mh,ka,jh,Fh,sn,bo,nn,rr,Bh,dn,wo,cn,Te,lt,Ta,yo,qh,ga,Nh,hn,st,Hh,$a,Rh,Wh,fn,ar,Gh,pn,ge,it,Ca,ko,Uh,Oa,Xh,mn,nt,Kh,Da,Jh,Qh,un,dt,Vh,xa,Yh,Zh,_n,To,vn,ct,ef,Aa,tf,of,En,go,bn,$e,ht,za,$o,rf,Ce,af,Pa,lf,sf,La,nf,df,wn,ft,cf,Co,Ia,hf,ff,pf,yn,pt,mf,Sa,uf,_f,kn,Oe,mt,Ma,Oo,vf,ja,Ef,Tn,P,bf,Fa,wf,yf,Ba,kf,Tf,qa,gf,$f,gn,ut,Cf,Na,Of,Df,$n,Do,Cn,_t,xf,Ha,Af,zf,On,xo,Dn,lr,Pf,xn,Ao,An,De,vt,Ra,zo,Lf,Wa,If,zn,Et,Sf,Po,Mf,jf,Pn,sr,Ff,Ln,ir,Bf,In,b,L,Ga,qf,Nf,Ua,Hf,Rf,Xa,Wf,Gf,Ka,Uf,Xf,Kf,I,Ja,Jf,Qf,Qa,Vf,Yf,Va,Zf,ep,Ya,tp,op,rp,te,Za,ap,lp,el,sp,ip,tl,np,dp,cp,oe,ol,hp,fp,rl,pp,mp,al,up,_p,vp,re,ll,Ep,bp,sl,wp,yp,il,kp,Tp,gp,ae,nl,$p,Cp,dl,Op,Dp,cl,xp,Ap,zp,le,hl,Pp,Lp,fl,Ip,Sp,pl,Mp,jp,Fp,se,ml,Bp,qp,ul,Np,Hp,_l,Rp,Wp,Gp,ie,vl,Up,Xp,El,Kp,Jp,bl,Qp,Vp,Yp,ne,wl,Zp,em,yl,tm,om,kl,rm,am,lm,de,Tl,sm,im,gl,nm,dm,$l,cm,hm,Sn,nr,fm,Mn,S,bt,Cl,pm,mm,Ol,um,_m,vm,wt,Dl,Em,bm,xl,wm,ym,km,yt,Al,Tm,gm,zl,$m,Cm,Om,kt,Pl,Dm,xm,Ll,Am,zm,jn,dr,Pm,Fn,ce,xe,Lm,Il,Im,Sm,Sl,Mm,jm,Fm,Ae,Bm,Ml,qm,Nm,jl,Hm,Rm,Wm,ze,Gm,Fl,Um,Xm,Bl,Km,Jm,Bn,Tt,Qm,ql,Vm,Ym,qn,g,B,Zm,Nl,eu,tu,Hl,ou,ru,Rl,au,lu,su,q,iu,Wl,nu,du,Gl,cu,hu,Ul,fu,pu,mu,Pe,uu,Xl,_u,vu,Kl,Eu,bu,wu,N,yu,Jl,ku,Tu,Ql,gu,$u,Vl,Cu,Ou,Du,H,xu,Yl,Au,zu,Zl,Pu,Lu,es,Iu,Su,Mu,R,ju,ts,Fu,Bu,os,qu,Nu,rs,Hu,Ru,Wu,W,Gu,as,Uu,Xu,ls,Ku,Ju,ss,Qu,Vu,Yu,G,Zu,is,e_,t_,ns,o_,r_,ds,a_,l_,Nn,gt,s_,cs,i_,n_,Hn,D,U,d_,hs,c_,h_,fs,f_,p_,ps,m_,u_,__,X,v_,ms,E_,b_,us,w_,y_,_s,k_,T_,g_,K,$_,vs,C_,O_,Es,D_,x_,bs,A_,z_,P_,J,L_,ws,I_,S_,ys,M_,j_,ks,F_,B_,q_,Q,N_,Ts,H_,R_,gs,W_,G_,$s,U_,X_,Rn,$t,K_,Cs,J_,Q_,Wn,cr,V,V_,Os,Y_,Z_,Ds,ev,tv,xs,ov,rv,Gn,hr,av,Un,Ct,Le,lv,As,sv,iv,zs,nv,dv,cv,Ie,hv,Ps,fv,pv,Ls,mv,uv,Xn,fr,_v,Kn,pr,Y,vv,Is,Ev,bv,Ss,wv,yv,Ms,kv,Tv,Jn,Se,Ot,js,Lo,gv,Fs,$v,Qn,Dt,Cv,Bs,Ov,Dv,Vn,Me,xt,qs,Io,xv,je,Av,Ns,zv,Pv,Hs,Lv,Iv,Yn,M,Sv,Rs,Mv,jv,Ws,Fv,Bv,Gs,qv,Nv,Zn,At,Hv,Us,Rv,Wv,ed,zt,Gv,Xs,Uv,Xv,td,Fe,Pt,Ks,So,Kv,Js,Jv,od,Lt,Qv,Qs,Vv,Yv,rd,Be,It,Vs,Mo,Zv,mr,e1,Ys,t1,ad,he,o1,Zs,r1,a1,ei,l1,s1,ld,St,i1,jo,n1,d1,sd,Mt,c1,ti,h1,f1,id,fe,p1,oi,m1,u1,ri,_1,v1,nd,Fo,dd,qe,jt,ai,Bo,E1,li,b1,cd,Ft,w1,si,y1,k1,hd,Bt,ii,Ne,T1,ni,g1,$1,di,C1,O1,D1,ci,w,x1,hi,A1,z1,fi,P1,L1,pi,I1,S1,mi,M1,j1,ui,F1,B1,_i,q1,N1,vi,H1,R1,Ei,W1,G1,bi,U1,X1,wi,K1,J1,fd,qt,Q1,yi,V1,Y1,pd,ur,Z1,md,qo,ud,He,Nt,ki,No,eE,Ti,tE,_d,j,oE,gi,rE,aE,$i,lE,sE,Ci,iE,nE,vd,pe,Oi,dE,cE,Di,hE,fE,xi,pE,Ed,me,mE,Ai,uE,_E,zi,vE,EE,bd,Ht,bE,Ho,wE,yE,wd,ue,kE,Pi,TE,gE,Li,$E,CE,yd,Ro,kd;return no=new $({}),co=new $({}),ho=new $({}),fo=new $({}),mo=new x({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),uo=new x({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, use_fast=<span class="hljs-literal">False</span>)`}}),_o=new $({}),vo=new $({}),Eo=new x({props:{code:"pip install transformers,",highlighted:"pip install transformers"}}),bo=new x({props:{code:"pip install transformers[sentencepiece],",highlighted:"pip install transformers[sentencepiece]"}}),wo=new x({props:{code:"pip install transformers sentencepiece,",highlighted:"pip install transformers sentencepiece"}}),yo=new $({}),ko=new $({}),To=new x({props:{code:"from transformers.modeling_bert import BertLayer,",highlighted:"from transformers.modeling_bert import BertLayer"}}),go=new x({props:{code:"from transformers.models.bert.modeling_bert import BertLayer,",highlighted:"from transformers.models.bert.modeling_bert import BertLayer"}}),$o=new $({}),Oo=new $({}),Do=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased")
outputs = model(**inputs),`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
outputs = model(**inputs)`}}),xo=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased")
outputs = model(**inputs, return_dict=False),`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
outputs = model(**inputs, return_dict=False)`}}),Ao=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased", return_dict=False)
outputs = model(**inputs),`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, return_dict=False)
outputs = model(**inputs)`}}),zo=new $({}),Lo=new $({}),Io=new $({}),So=new $({}),Mo=new $({}),Fo=new x({props:{code:`# Let's load our model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# If you used to have this line in pytorch-pretrained-bert:
loss = model(input_ids, labels=labels)

# Now just use this line in \u{1F917} Transformers to extract the loss from the output tuple:
outputs = model(input_ids, labels=labels)
loss = outputs[0]

# In \u{1F917} Transformers you can also have access to the logits:
loss, logits = outputs[:2]

# And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", output_attentions=True)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs,`,highlighted:`<span class="hljs-comment"># Let&#x27;s load our model</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment"># If you used to have this line in pytorch-pretrained-bert:</span>
loss = model(input_ids, labels=labels)

<span class="hljs-comment"># Now just use this line in \u{1F917} Transformers to extract the loss from the output tuple:</span>
outputs = model(input_ids, labels=labels)
loss = outputs[<span class="hljs-number">0</span>]

<span class="hljs-comment"># In \u{1F917} Transformers you can also have access to the logits:</span>
loss, logits = outputs[:<span class="hljs-number">2</span>]

<span class="hljs-comment"># And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs`}}),Bo=new $({}),qo=new x({props:{code:`### Let's load a model and tokenizer
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

### Do some stuff to our model and tokenizer
# Ex: add new tokens to the vocabulary and embeddings of our model
tokenizer.add_tokens(["[SPECIAL_TOKEN_1]", "[SPECIAL_TOKEN_2]"])
model.resize_token_embeddings(len(tokenizer))
# Train our model
train(model)

### Now let's save our model and tokenizer to a directory
model.save_pretrained("./my_saved_model_directory/")
tokenizer.save_pretrained("./my_saved_model_directory/")

### Reload the model and the tokenizer
model = BertForSequenceClassification.from_pretrained("./my_saved_model_directory/")
tokenizer = BertTokenizer.from_pretrained("./my_saved_model_directory/"),`,highlighted:`<span class="hljs-comment">### Let&#x27;s load a model and tokenizer</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment">### Do some stuff to our model and tokenizer</span>
<span class="hljs-comment"># Ex: add new tokens to the vocabulary and embeddings of our model</span>
tokenizer.add_tokens([<span class="hljs-string">&quot;[SPECIAL_TOKEN_1]&quot;</span>, <span class="hljs-string">&quot;[SPECIAL_TOKEN_2]&quot;</span>])
model.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))
<span class="hljs-comment"># Train our model</span>
train(model)

<span class="hljs-comment">### Now let&#x27;s save our model and tokenizer to a directory</span>
model.save_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)

<span class="hljs-comment">### Reload the model and the tokenizer</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)`}}),No=new $({}),Ro=new x({props:{code:`# Parameters:
lr = 1e-3
max_grad_norm = 1.0
num_training_steps = 1000
num_warmup_steps = 100
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

### Previously BertAdam optimizer was instantiated like this:
optimizer = BertAdam(
    model.parameters(),
    lr=lr,
    schedule="warmup_linear",
    warmup=warmup_proportion,
    num_training_steps=num_training_steps,
)
### and used like this:
for batch in train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

### In \u{1F917} Transformers, optimizer and schedules are split and instantiated like this:
optimizer = AdamW(
    model.parameters(), lr=lr, correct_bias=False
)  # To reproduce BertAdam specific behavior set correct_bias=False
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
)  # PyTorch scheduler
### and used like this:
for batch in train_data:
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_grad_norm
    )  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)
    optimizer.step()
    scheduler.step(),`,highlighted:`<span class="hljs-comment"># Parameters:</span>
lr = <span class="hljs-number">1e-3</span>
max_grad_norm = <span class="hljs-number">1.0</span>
num_training_steps = <span class="hljs-number">1000</span>
num_warmup_steps = <span class="hljs-number">100</span>
warmup_proportion = <span class="hljs-built_in">float</span>(num_warmup_steps) / <span class="hljs-built_in">float</span>(num_training_steps)  <span class="hljs-comment"># 0.1</span>

<span class="hljs-comment">### Previously BertAdam optimizer was instantiated like this:</span>
optimizer = BertAdam(
    model.parameters(),
    lr=lr,
    schedule=<span class="hljs-string">&quot;warmup_linear&quot;</span>,
    warmup=warmup_proportion,
    num_training_steps=num_training_steps,
)
<span class="hljs-comment">### and used like this:</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

<span class="hljs-comment">### In \u{1F917} Transformers, optimizer and schedules are split and instantiated like this:</span>
optimizer = AdamW(
    model.parameters(), lr=lr, correct_bias=<span class="hljs-literal">False</span>
)  <span class="hljs-comment"># To reproduce BertAdam specific behavior set correct_bias=False</span>
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
)  <span class="hljs-comment"># PyTorch scheduler</span>
<span class="hljs-comment">### and used like this:</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_data:
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_grad_norm
    )  <span class="hljs-comment"># Gradient clipping is not in AdamW anymore (so you can use amp without issue)</span>
    optimizer.step()
    scheduler.step()`}}),{c(){Z=l("meta"),Zo=c(),A=l("h1"),F=l("a"),Lr=l("span"),p(no.$$.fragment),bc=c(),Ir=l("span"),wc=r("Migrating from previous packages"),Bi=c(),ve=l("h2"),Ge=l("a"),Sr=l("span"),p(co.$$.fragment),yc=c(),Ue=l("span"),kc=r("Migrating from transformers "),Mr=l("code"),Tc=r("v3.x"),gc=r(" to "),jr=l("code"),$c=r("v4.x"),qi=c(),er=l("p"),Cc=r(`A couple of changes were introduced when the switch from version 3 to version 4 was done. Below is a summary of the
expected changes:`),Ni=c(),Ee=l("h4"),Xe=l("a"),Fr=l("span"),p(ho.$$.fragment),Oc=c(),Br=l("span"),Dc=r("1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."),Hi=c(),tr=l("p"),xc=r("The python and rust tokenizers have roughly the same API, but the rust tokenizers have a more complete feature set."),Ri=c(),or=l("p"),Ac=r("This introduces two breaking changes:"),Wi=c(),Ke=l("ul"),qr=l("li"),zc=r("The handling of overflowing tokens between the python and rust tokenizers is different."),Pc=c(),Nr=l("li"),Lc=r("The rust tokenizers do not accept integers in the encoding methods."),Gi=c(),be=l("h5"),Je=l("a"),Hr=l("span"),p(fo.$$.fragment),Ic=c(),Rr=l("span"),Sc=r("How to obtain the same behavior as v3.x in v4.x"),Ui=c(),Qe=l("ul"),po=l("li"),Mc=r("The pipelines now contain additional features out of the box. See the "),Ve=l("a"),jc=r("token-classification pipeline with the "),Wr=l("code"),Fc=r("grouped_entities"),Bc=r(" flag"),qc=r("."),Nc=c(),we=l("li"),Hc=r("The auto-tokenizers now return rust tokenizers. In order to obtain the python tokenizers instead, the user may use the "),Gr=l("code"),Rc=r("use_fast"),Wc=r(" flag by setting it to "),Ur=l("code"),Gc=r("False"),Uc=r(":"),Xi=c(),Ye=l("p"),Xc=r("In version "),Xr=l("code"),Kc=r("v3.x"),Jc=r(":"),Ki=c(),p(mo.$$.fragment),Ji=c(),Ze=l("p"),Qc=r("to obtain the same in version "),Kr=l("code"),Vc=r("v4.x"),Yc=r(":"),Qi=c(),p(uo.$$.fragment),Vi=c(),ye=l("h4"),et=l("a"),Jr=l("span"),p(_o.$$.fragment),Zc=c(),Qr=l("span"),eh=r("2. SentencePiece is removed from the required dependencies"),Yi=c(),z=l("p"),th=r("The requirement on the SentencePiece dependency has been lifted from the "),Vr=l("code"),oh=r("setup.py"),rh=r(". This is done so that we may have a channel on anaconda cloud without relying on "),Yr=l("code"),ah=r("conda-forge"),lh=r(". This means that the tokenizers that depend on the SentencePiece library will not be available with a standard "),Zr=l("code"),sh=r("transformers"),ih=r(" installation."),Zi=c(),tt=l("p"),nh=r("This includes the "),ea=l("strong"),dh=r("slow"),ch=r(" versions of:"),en=c(),T=l("ul"),ta=l("li"),oa=l("code"),hh=r("XLNetTokenizer"),fh=c(),ra=l("li"),aa=l("code"),ph=r("AlbertTokenizer"),mh=c(),la=l("li"),sa=l("code"),uh=r("CamembertTokenizer"),_h=c(),ia=l("li"),na=l("code"),vh=r("MBartTokenizer"),Eh=c(),da=l("li"),ca=l("code"),bh=r("PegasusTokenizer"),wh=c(),ha=l("li"),fa=l("code"),yh=r("T5Tokenizer"),kh=c(),pa=l("li"),ma=l("code"),Th=r("ReformerTokenizer"),gh=c(),ua=l("li"),_a=l("code"),$h=r("XLMRobertaTokenizer"),tn=c(),ke=l("h5"),ot=l("a"),va=l("span"),p(vo.$$.fragment),Ch=c(),Ea=l("span"),Oh=r("How to obtain the same behavior as v3.x in v4.x"),on=c(),ee=l("p"),Dh=r("In order to obtain the same behavior as version "),ba=l("code"),xh=r("v3.x"),Ah=r(", you should install "),wa=l("code"),zh=r("sentencepiece"),Ph=r(" additionally:"),rn=c(),rt=l("p"),Lh=r("In version "),ya=l("code"),Ih=r("v3.x"),Sh=r(":"),an=c(),p(Eo.$$.fragment),ln=c(),at=l("p"),Mh=r("to obtain the same in version "),ka=l("code"),jh=r("v4.x"),Fh=r(":"),sn=c(),p(bo.$$.fragment),nn=c(),rr=l("p"),Bh=r("or"),dn=c(),p(wo.$$.fragment),cn=c(),Te=l("h4"),lt=l("a"),Ta=l("span"),p(yo.$$.fragment),qh=c(),ga=l("span"),Nh=r("3. The architecture of the repo has been updated so that each model resides in its folder"),hn=c(),st=l("p"),Hh=r("The past and foreseeable addition of new models means that the number of files in the directory "),$a=l("code"),Rh=r("src/transformers"),Wh=r(" keeps growing and becomes harder to navigate and understand. We made the choice to put each model and the files accompanying it in their own sub-directories."),fn=c(),ar=l("p"),Gh=r("This is a breaking change as importing intermediary layers using a model\u2019s module directly needs to be done via a different path."),pn=c(),ge=l("h5"),it=l("a"),Ca=l("span"),p(ko.$$.fragment),Uh=c(),Oa=l("span"),Xh=r("How to obtain the same behavior as v3.x in v4.x"),mn=c(),nt=l("p"),Kh=r("In order to obtain the same behavior as version "),Da=l("code"),Jh=r("v3.x"),Qh=r(", you should update the path used to access the layers."),un=c(),dt=l("p"),Vh=r("In version "),xa=l("code"),Yh=r("v3.x"),Zh=r(":"),_n=c(),p(To.$$.fragment),vn=c(),ct=l("p"),ef=r("to obtain the same in version "),Aa=l("code"),tf=r("v4.x"),of=r(":"),En=c(),p(go.$$.fragment),bn=c(),$e=l("h4"),ht=l("a"),za=l("span"),p($o.$$.fragment),rf=c(),Ce=l("span"),af=r("4. Switching the "),Pa=l("code"),lf=r("return_dict"),sf=r(" argument to "),La=l("code"),nf=r("True"),df=r(" by default"),wn=c(),ft=l("p"),cf=r("The "),Co=l("a"),Ia=l("code"),hf=r("return_dict"),ff=r(" argument"),pf=r(" enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. This object is self-documented as keys can be used to retrieve values, while also behaving as a tuple as users may retrieve objects by index or by slice."),yn=c(),pt=l("p"),mf=r("This is a breaking change as the limitation of that tuple is that it cannot be unpacked: "),Sa=l("code"),uf=r("value0, value1 = outputs"),_f=r(" will not work."),kn=c(),Oe=l("h5"),mt=l("a"),Ma=l("span"),p(Oo.$$.fragment),vf=c(),ja=l("span"),Ef=r("How to obtain the same behavior as v3.x in v4.x"),Tn=c(),P=l("p"),bf=r("In order to obtain the same behavior as version "),Fa=l("code"),wf=r("v3.x"),yf=r(", you should specify the "),Ba=l("code"),kf=r("return_dict"),Tf=r(" argument to "),qa=l("code"),gf=r("False"),$f=r(", either in the model configuration or during the forward pass."),gn=c(),ut=l("p"),Cf=r("In version "),Na=l("code"),Of=r("v3.x"),Df=r(":"),$n=c(),p(Do.$$.fragment),Cn=c(),_t=l("p"),xf=r("to obtain the same in version "),Ha=l("code"),Af=r("v4.x"),zf=r(":"),On=c(),p(xo.$$.fragment),Dn=c(),lr=l("p"),Pf=r("or"),xn=c(),p(Ao.$$.fragment),An=c(),De=l("h4"),vt=l("a"),Ra=l("span"),p(zo.$$.fragment),Lf=c(),Wa=l("span"),If=r("5. Removed some deprecated attributes"),zn=c(),Et=l("p"),Sf=r("Attributes that were deprecated have been removed if they had been deprecated for at least a month. The full list of deprecated attributes can be found in "),Po=l("a"),Mf=r("#8604"),jf=r("."),Pn=c(),sr=l("p"),Ff=r("Here is a list of these attributes/methods/arguments and what their replacements should be:"),Ln=c(),ir=l("p"),Bf=r("In several models, the labels become consistent with the other models:"),In=c(),b=l("ul"),L=l("li"),Ga=l("code"),qf=r("masked_lm_labels"),Nf=r(" becomes "),Ua=l("code"),Hf=r("labels"),Rf=r(" in "),Xa=l("code"),Wf=r("AlbertForMaskedLM"),Gf=r(" and "),Ka=l("code"),Uf=r("AlbertForPreTraining"),Xf=r("."),Kf=c(),I=l("li"),Ja=l("code"),Jf=r("masked_lm_labels"),Qf=r(" becomes "),Qa=l("code"),Vf=r("labels"),Yf=r(" in "),Va=l("code"),Zf=r("BertForMaskedLM"),ep=r(" and "),Ya=l("code"),tp=r("BertForPreTraining"),op=r("."),rp=c(),te=l("li"),Za=l("code"),ap=r("masked_lm_labels"),lp=r(" becomes "),el=l("code"),sp=r("labels"),ip=r(" in "),tl=l("code"),np=r("DistilBertForMaskedLM"),dp=r("."),cp=c(),oe=l("li"),ol=l("code"),hp=r("masked_lm_labels"),fp=r(" becomes "),rl=l("code"),pp=r("labels"),mp=r(" in "),al=l("code"),up=r("ElectraForMaskedLM"),_p=r("."),vp=c(),re=l("li"),ll=l("code"),Ep=r("masked_lm_labels"),bp=r(" becomes "),sl=l("code"),wp=r("labels"),yp=r(" in "),il=l("code"),kp=r("LongformerForMaskedLM"),Tp=r("."),gp=c(),ae=l("li"),nl=l("code"),$p=r("masked_lm_labels"),Cp=r(" becomes "),dl=l("code"),Op=r("labels"),Dp=r(" in "),cl=l("code"),xp=r("MobileBertForMaskedLM"),Ap=r("."),zp=c(),le=l("li"),hl=l("code"),Pp=r("masked_lm_labels"),Lp=r(" becomes "),fl=l("code"),Ip=r("labels"),Sp=r(" in "),pl=l("code"),Mp=r("RobertaForMaskedLM"),jp=r("."),Fp=c(),se=l("li"),ml=l("code"),Bp=r("lm_labels"),qp=r(" becomes "),ul=l("code"),Np=r("labels"),Hp=r(" in "),_l=l("code"),Rp=r("BartForConditionalGeneration"),Wp=r("."),Gp=c(),ie=l("li"),vl=l("code"),Up=r("lm_labels"),Xp=r(" becomes "),El=l("code"),Kp=r("labels"),Jp=r(" in "),bl=l("code"),Qp=r("GPT2DoubleHeadsModel"),Vp=r("."),Yp=c(),ne=l("li"),wl=l("code"),Zp=r("lm_labels"),em=r(" becomes "),yl=l("code"),tm=r("labels"),om=r(" in "),kl=l("code"),rm=r("OpenAIGPTDoubleHeadsModel"),am=r("."),lm=c(),de=l("li"),Tl=l("code"),sm=r("lm_labels"),im=r(" becomes "),gl=l("code"),nm=r("labels"),dm=r(" in "),$l=l("code"),cm=r("T5ForConditionalGeneration"),hm=r("."),Sn=c(),nr=l("p"),fm=r("In several models, the caching mechanism becomes consistent with the other models:"),Mn=c(),S=l("ul"),bt=l("li"),Cl=l("code"),pm=r("decoder_cached_states"),mm=r(" becomes "),Ol=l("code"),um=r("past_key_values"),_m=r(" in all BART-like, FSMT and T5 models."),vm=c(),wt=l("li"),Dl=l("code"),Em=r("decoder_past_key_values"),bm=r(" becomes "),xl=l("code"),wm=r("past_key_values"),ym=r(" in all BART-like, FSMT and T5 models."),km=c(),yt=l("li"),Al=l("code"),Tm=r("past"),gm=r(" becomes "),zl=l("code"),$m=r("past_key_values"),Cm=r(" in all CTRL models."),Om=c(),kt=l("li"),Pl=l("code"),Dm=r("past"),xm=r(" becomes "),Ll=l("code"),Am=r("past_key_values"),zm=r(" in all GPT-2 models."),jn=c(),dr=l("p"),Pm=r("Regarding the tokenizer classes:"),Fn=c(),ce=l("ul"),xe=l("li"),Lm=r("The tokenizer attribute "),Il=l("code"),Im=r("max_len"),Sm=r(" becomes "),Sl=l("code"),Mm=r("model_max_length"),jm=r("."),Fm=c(),Ae=l("li"),Bm=r("The tokenizer attribute "),Ml=l("code"),qm=r("return_lengths"),Nm=r(" becomes "),jl=l("code"),Hm=r("return_length"),Rm=r("."),Wm=c(),ze=l("li"),Gm=r("The tokenizer encoding argument "),Fl=l("code"),Um=r("is_pretokenized"),Xm=r(" becomes "),Bl=l("code"),Km=r("is_split_into_words"),Jm=r("."),Bn=c(),Tt=l("p"),Qm=r("Regarding the "),ql=l("code"),Vm=r("Trainer"),Ym=r(" class:"),qn=c(),g=l("ul"),B=l("li"),Zm=r("The "),Nl=l("code"),eu=r("Trainer"),tu=r(" argument "),Hl=l("code"),ou=r("tb_writer"),ru=r(" is removed in favor of the callback "),Rl=l("code"),au=r("TensorBoardCallback(tb_writer=...)"),lu=r("."),su=c(),q=l("li"),iu=r("The "),Wl=l("code"),nu=r("Trainer"),du=r(" argument "),Gl=l("code"),cu=r("prediction_loss_only"),hu=r(" is removed in favor of the class argument "),Ul=l("code"),fu=r("args.prediction_loss_only"),pu=r("."),mu=c(),Pe=l("li"),uu=r("The "),Xl=l("code"),_u=r("Trainer"),vu=r(" attribute "),Kl=l("code"),Eu=r("data_collator"),bu=r(" should be a callable."),wu=c(),N=l("li"),yu=r("The "),Jl=l("code"),ku=r("Trainer"),Tu=r(" method "),Ql=l("code"),gu=r("_log"),$u=r(" is deprecated in favor of "),Vl=l("code"),Cu=r("log"),Ou=r("."),Du=c(),H=l("li"),xu=r("The "),Yl=l("code"),Au=r("Trainer"),zu=r(" method "),Zl=l("code"),Pu=r("_training_step"),Lu=r(" is deprecated in favor of "),es=l("code"),Iu=r("training_step"),Su=r("."),Mu=c(),R=l("li"),ju=r("The "),ts=l("code"),Fu=r("Trainer"),Bu=r(" method "),os=l("code"),qu=r("_prediction_loop"),Nu=r(" is deprecated in favor of "),rs=l("code"),Hu=r("prediction_loop"),Ru=r("."),Wu=c(),W=l("li"),Gu=r("The "),as=l("code"),Uu=r("Trainer"),Xu=r(" method "),ls=l("code"),Ku=r("is_local_master"),Ju=r(" is deprecated in favor of "),ss=l("code"),Qu=r("is_local_process_zero"),Vu=r("."),Yu=c(),G=l("li"),Zu=r("The "),is=l("code"),e_=r("Trainer"),t_=r(" method "),ns=l("code"),o_=r("is_world_master"),r_=r(" is deprecated in favor of "),ds=l("code"),a_=r("is_world_process_zero"),l_=r("."),Nn=c(),gt=l("p"),s_=r("Regarding the "),cs=l("code"),i_=r("TFTrainer"),n_=r(" class:"),Hn=c(),D=l("ul"),U=l("li"),d_=r("The "),hs=l("code"),c_=r("TFTrainer"),h_=r(" argument "),fs=l("code"),f_=r("prediction_loss_only"),p_=r(" is removed in favor of the class argument "),ps=l("code"),m_=r("args.prediction_loss_only"),u_=r("."),__=c(),X=l("li"),v_=r("The "),ms=l("code"),E_=r("Trainer"),b_=r(" method "),us=l("code"),w_=r("_log"),y_=r(" is deprecated in favor of "),_s=l("code"),k_=r("log"),T_=r("."),g_=c(),K=l("li"),$_=r("The "),vs=l("code"),C_=r("TFTrainer"),O_=r(" method "),Es=l("code"),D_=r("_prediction_loop"),x_=r(" is deprecated in favor of "),bs=l("code"),A_=r("prediction_loop"),z_=r("."),P_=c(),J=l("li"),L_=r("The "),ws=l("code"),I_=r("TFTrainer"),S_=r(" method "),ys=l("code"),M_=r("_setup_wandb"),j_=r(" is deprecated in favor of "),ks=l("code"),F_=r("setup_wandb"),B_=r("."),q_=c(),Q=l("li"),N_=r("The "),Ts=l("code"),H_=r("TFTrainer"),R_=r(" method "),gs=l("code"),W_=r("_run_model"),G_=r(" is deprecated in favor of "),$s=l("code"),U_=r("run_model"),X_=r("."),Rn=c(),$t=l("p"),K_=r("Regarding the "),Cs=l("code"),J_=r("TrainingArguments"),Q_=r(" class:"),Wn=c(),cr=l("ul"),V=l("li"),V_=r("The "),Os=l("code"),Y_=r("TrainingArguments"),Z_=r(" argument "),Ds=l("code"),ev=r("evaluate_during_training"),tv=r(" is deprecated in favor of "),xs=l("code"),ov=r("evaluation_strategy"),rv=r("."),Gn=c(),hr=l("p"),av=r("Regarding the Transfo-XL model:"),Un=c(),Ct=l("ul"),Le=l("li"),lv=r("The Transfo-XL configuration attribute "),As=l("code"),sv=r("tie_weight"),iv=r(" becomes "),zs=l("code"),nv=r("tie_words_embeddings"),dv=r("."),cv=c(),Ie=l("li"),hv=r("The Transfo-XL modeling method "),Ps=l("code"),fv=r("reset_length"),pv=r(" becomes "),Ls=l("code"),mv=r("reset_memory_length"),uv=r("."),Xn=c(),fr=l("p"),_v=r("Regarding pipelines:"),Kn=c(),pr=l("ul"),Y=l("li"),vv=r("The "),Is=l("code"),Ev=r("FillMaskPipeline"),bv=r(" argument "),Ss=l("code"),wv=r("topk"),yv=r(" becomes "),Ms=l("code"),kv=r("top_k"),Tv=r("."),Jn=c(),Se=l("h2"),Ot=l("a"),js=l("span"),p(Lo.$$.fragment),gv=c(),Fs=l("span"),$v=r("Migrating from pytorch-transformers to \u{1F917} Transformers"),Qn=c(),Dt=l("p"),Cv=r("Here is a quick summary of what you should take care of when migrating from "),Bs=l("code"),Ov=r("pytorch-transformers"),Dv=r(" to \u{1F917} Transformers."),Vn=c(),Me=l("h3"),xt=l("a"),qs=l("span"),p(Io.$$.fragment),xv=c(),je=l("span"),Av=r("Positional order of some models' keywords inputs ("),Ns=l("code"),zv=r("attention_mask"),Pv=r(", "),Hs=l("code"),Lv=r("token_type_ids"),Iv=r("...) changed"),Yn=c(),M=l("p"),Sv=r("To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models "),Rs=l("strong"),Mv=r("keywords inputs"),jv=r(" ("),Ws=l("code"),Fv=r("attention_mask"),Bv=r(", "),Gs=l("code"),qv=r("token_type_ids"),Nv=r("\u2026) has been changed."),Zn=c(),At=l("p"),Hv=r("If you used to call the models with keyword names for keyword arguments, e.g. "),Us=l("code"),Rv=r("model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"),Wv=r(", this should not cause any change."),ed=c(),zt=l("p"),Gv=r("If you used to call the models with positional inputs for keyword arguments, e.g. "),Xs=l("code"),Uv=r("model(inputs_ids, attention_mask, token_type_ids)"),Xv=r(", you may have to double check the exact order of input arguments."),td=c(),Fe=l("h2"),Pt=l("a"),Ks=l("span"),p(So.$$.fragment),Kv=c(),Js=l("span"),Jv=r("Migrating from pytorch-pretrained-bert"),od=c(),Lt=l("p"),Qv=r("Here is a quick summary of what you should take care of when migrating from "),Qs=l("code"),Vv=r("pytorch-pretrained-bert"),Yv=r(" to \u{1F917} Transformers"),rd=c(),Be=l("h3"),It=l("a"),Vs=l("span"),p(Mo.$$.fragment),Zv=c(),mr=l("span"),e1=r("Models always output "),Ys=l("code"),t1=r("tuples"),ad=c(),he=l("p"),o1=r("The main breaking change when migrating from "),Zs=l("code"),r1=r("pytorch-pretrained-bert"),a1=r(" to \u{1F917} Transformers is that the models forward method always outputs a "),ei=l("code"),l1=r("tuple"),s1=r(" with various elements depending on the model and the configuration parameters."),ld=c(),St=l("p"),i1=r("The exact content of the tuples for each model are detailed in the models\u2019 docstrings and the "),jo=l("a"),n1=r("documentation"),d1=r("."),sd=c(),Mt=l("p"),c1=r("In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in "),ti=l("code"),h1=r("pytorch-pretrained-bert"),f1=r("."),id=c(),fe=l("p"),p1=r("Here is a "),oi=l("code"),m1=r("pytorch-pretrained-bert"),u1=r(" to \u{1F917} Transformers conversion example for a "),ri=l("code"),_1=r("BertForSequenceClassification"),v1=r(" classification model:"),nd=c(),p(Fo.$$.fragment),dd=c(),qe=l("h3"),jt=l("a"),ai=l("span"),p(Bo.$$.fragment),E1=c(),li=l("span"),b1=r("Serialization"),cd=c(),Ft=l("p"),w1=r("Breaking change in the "),si=l("code"),y1=r("from_pretrained()"),k1=r("method:"),hd=c(),Bt=l("ol"),ii=l("li"),Ne=l("p"),T1=r("Models are now set in evaluation mode by default when instantiated with the "),ni=l("code"),g1=r("from_pretrained()"),$1=r(" method. To train them don\u2019t forget to set them back in training mode ("),di=l("code"),C1=r("model.train()"),O1=r(") to activate the dropout modules."),D1=c(),ci=l("li"),w=l("p"),x1=r("The additional "),hi=l("code"),A1=r("*inputs"),z1=r(" and "),fi=l("code"),P1=r("**kwargs"),L1=r(" arguments supplied to the "),pi=l("code"),I1=r("from_pretrained()"),S1=r(" method used to be directly passed to the underlying model\u2019s class "),mi=l("code"),M1=r("__init__()"),j1=r(" method. They are now used to update the model configuration attribute first which can break derived model classes build based on the previous "),ui=l("code"),F1=r("BertForSequenceClassification"),B1=r(" examples. More precisely, the positional arguments "),_i=l("code"),q1=r("*inputs"),N1=r(" provided to "),vi=l("code"),H1=r("from_pretrained()"),R1=r(" are directly forwarded the model "),Ei=l("code"),W1=r("__init__()"),G1=r(" method while the keyword arguments "),bi=l("code"),U1=r("**kwargs"),X1=r(" (i) which match configuration class attributes are used to update said attributes (ii) which don\u2019t match any configuration class attributes are forwarded to the model "),wi=l("code"),K1=r("__init__()"),J1=r(" method."),fd=c(),qt=l("p"),Q1=r("Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method "),yi=l("code"),V1=r("save_pretrained(save_directory)"),Y1=r(" if you were using any other serialization method before."),pd=c(),ur=l("p"),Z1=r("Here is an example:"),md=c(),p(qo.$$.fragment),ud=c(),He=l("h3"),Nt=l("a"),ki=l("span"),p(No.$$.fragment),eE=c(),Ti=l("span"),tE=r("Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"),_d=c(),j=l("p"),oE=r("The two optimizers previously included, "),gi=l("code"),rE=r("BertAdam"),aE=r(" and "),$i=l("code"),lE=r("OpenAIAdam"),sE=r(", have been replaced by a single "),Ci=l("code"),iE=r("AdamW"),nE=r(" optimizer which has a few differences:"),vd=c(),pe=l("ul"),Oi=l("li"),dE=r("it only implements weights decay correction,"),cE=c(),Di=l("li"),hE=r("schedules are now externals (see below),"),fE=c(),xi=l("li"),pE=r("gradient clipping is now also external (see below)."),Ed=c(),me=l("p"),mE=r("The new optimizer "),Ai=l("code"),uE=r("AdamW"),_E=r(" matches PyTorch "),zi=l("code"),vE=r("Adam"),EE=r(" optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping."),bd=c(),Ht=l("p"),bE=r("The schedules are now standard "),Ho=l("a"),wE=r("PyTorch learning rate schedulers"),yE=r(" and not part of the optimizer anymore."),wd=c(),ue=l("p"),kE=r("Here is a conversion examples from "),Pi=l("code"),TE=r("BertAdam"),gE=r(" with a linear warmup and decay schedule to "),Li=l("code"),$E=r("AdamW"),CE=r(" and the same schedule:"),yd=c(),p(Ro.$$.fragment),this.h()},l(t){const n=mw('[data-svelte="svelte-1phssyn"]',document.head);Z=s(n,"META",{name:!0,content:!0}),n.forEach(o),Zo=h(t),A=s(t,"H1",{class:!0});var Td=i(A);F=s(Td,"A",{id:!0,class:!0,href:!0});var xE=i(F);Lr=s(xE,"SPAN",{});var AE=i(Lr);m(no.$$.fragment,AE),AE.forEach(o),xE.forEach(o),bc=h(Td),Ir=s(Td,"SPAN",{});var zE=i(Ir);wc=a(zE,"Migrating from previous packages"),zE.forEach(o),Td.forEach(o),Bi=h(t),ve=s(t,"H2",{class:!0});var gd=i(ve);Ge=s(gd,"A",{id:!0,class:!0,href:!0});var PE=i(Ge);Sr=s(PE,"SPAN",{});var LE=i(Sr);m(co.$$.fragment,LE),LE.forEach(o),PE.forEach(o),yc=h(gd),Ue=s(gd,"SPAN",{});var Ii=i(Ue);kc=a(Ii,"Migrating from transformers "),Mr=s(Ii,"CODE",{});var IE=i(Mr);Tc=a(IE,"v3.x"),IE.forEach(o),gc=a(Ii," to "),jr=s(Ii,"CODE",{});var SE=i(jr);$c=a(SE,"v4.x"),SE.forEach(o),Ii.forEach(o),gd.forEach(o),qi=h(t),er=s(t,"P",{});var ME=i(er);Cc=a(ME,`A couple of changes were introduced when the switch from version 3 to version 4 was done. Below is a summary of the
expected changes:`),ME.forEach(o),Ni=h(t),Ee=s(t,"H4",{class:!0});var $d=i(Ee);Xe=s($d,"A",{id:!0,class:!0,href:!0});var jE=i(Xe);Fr=s(jE,"SPAN",{});var FE=i(Fr);m(ho.$$.fragment,FE),FE.forEach(o),jE.forEach(o),Oc=h($d),Br=s($d,"SPAN",{});var BE=i(Br);Dc=a(BE,"1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."),BE.forEach(o),$d.forEach(o),Hi=h(t),tr=s(t,"P",{});var qE=i(tr);xc=a(qE,"The python and rust tokenizers have roughly the same API, but the rust tokenizers have a more complete feature set."),qE.forEach(o),Ri=h(t),or=s(t,"P",{});var NE=i(or);Ac=a(NE,"This introduces two breaking changes:"),NE.forEach(o),Wi=h(t),Ke=s(t,"UL",{});var Cd=i(Ke);qr=s(Cd,"LI",{});var HE=i(qr);zc=a(HE,"The handling of overflowing tokens between the python and rust tokenizers is different."),HE.forEach(o),Pc=h(Cd),Nr=s(Cd,"LI",{});var RE=i(Nr);Lc=a(RE,"The rust tokenizers do not accept integers in the encoding methods."),RE.forEach(o),Cd.forEach(o),Gi=h(t),be=s(t,"H5",{class:!0});var Od=i(be);Je=s(Od,"A",{id:!0,class:!0,href:!0});var WE=i(Je);Hr=s(WE,"SPAN",{});var GE=i(Hr);m(fo.$$.fragment,GE),GE.forEach(o),WE.forEach(o),Ic=h(Od),Rr=s(Od,"SPAN",{});var UE=i(Rr);Sc=a(UE,"How to obtain the same behavior as v3.x in v4.x"),UE.forEach(o),Od.forEach(o),Ui=h(t),Qe=s(t,"UL",{});var Dd=i(Qe);po=s(Dd,"LI",{});var xd=i(po);Mc=a(xd,"The pipelines now contain additional features out of the box. See the "),Ve=s(xd,"A",{href:!0});var Ad=i(Ve);jc=a(Ad,"token-classification pipeline with the "),Wr=s(Ad,"CODE",{});var XE=i(Wr);Fc=a(XE,"grouped_entities"),XE.forEach(o),Bc=a(Ad," flag"),Ad.forEach(o),qc=a(xd,"."),xd.forEach(o),Nc=h(Dd),we=s(Dd,"LI",{});var _r=i(we);Hc=a(_r,"The auto-tokenizers now return rust tokenizers. In order to obtain the python tokenizers instead, the user may use the "),Gr=s(_r,"CODE",{});var KE=i(Gr);Rc=a(KE,"use_fast"),KE.forEach(o),Wc=a(_r," flag by setting it to "),Ur=s(_r,"CODE",{});var JE=i(Ur);Gc=a(JE,"False"),JE.forEach(o),Uc=a(_r,":"),_r.forEach(o),Dd.forEach(o),Xi=h(t),Ye=s(t,"P",{});var zd=i(Ye);Xc=a(zd,"In version "),Xr=s(zd,"CODE",{});var QE=i(Xr);Kc=a(QE,"v3.x"),QE.forEach(o),Jc=a(zd,":"),zd.forEach(o),Ki=h(t),m(mo.$$.fragment,t),Ji=h(t),Ze=s(t,"P",{});var Pd=i(Ze);Qc=a(Pd,"to obtain the same in version "),Kr=s(Pd,"CODE",{});var VE=i(Kr);Vc=a(VE,"v4.x"),VE.forEach(o),Yc=a(Pd,":"),Pd.forEach(o),Qi=h(t),m(uo.$$.fragment,t),Vi=h(t),ye=s(t,"H4",{class:!0});var Ld=i(ye);et=s(Ld,"A",{id:!0,class:!0,href:!0});var YE=i(et);Jr=s(YE,"SPAN",{});var ZE=i(Jr);m(_o.$$.fragment,ZE),ZE.forEach(o),YE.forEach(o),Zc=h(Ld),Qr=s(Ld,"SPAN",{});var eb=i(Qr);eh=a(eb,"2. SentencePiece is removed from the required dependencies"),eb.forEach(o),Ld.forEach(o),Yi=h(t),z=s(t,"P",{});var Rt=i(z);th=a(Rt,"The requirement on the SentencePiece dependency has been lifted from the "),Vr=s(Rt,"CODE",{});var tb=i(Vr);oh=a(tb,"setup.py"),tb.forEach(o),rh=a(Rt,". This is done so that we may have a channel on anaconda cloud without relying on "),Yr=s(Rt,"CODE",{});var ob=i(Yr);ah=a(ob,"conda-forge"),ob.forEach(o),lh=a(Rt,". This means that the tokenizers that depend on the SentencePiece library will not be available with a standard "),Zr=s(Rt,"CODE",{});var rb=i(Zr);sh=a(rb,"transformers"),rb.forEach(o),ih=a(Rt," installation."),Rt.forEach(o),Zi=h(t),tt=s(t,"P",{});var Id=i(tt);nh=a(Id,"This includes the "),ea=s(Id,"STRONG",{});var ab=i(ea);dh=a(ab,"slow"),ab.forEach(o),ch=a(Id," versions of:"),Id.forEach(o),en=h(t),T=s(t,"UL",{});var C=i(T);ta=s(C,"LI",{});var lb=i(ta);oa=s(lb,"CODE",{});var sb=i(oa);hh=a(sb,"XLNetTokenizer"),sb.forEach(o),lb.forEach(o),fh=h(C),ra=s(C,"LI",{});var ib=i(ra);aa=s(ib,"CODE",{});var nb=i(aa);ph=a(nb,"AlbertTokenizer"),nb.forEach(o),ib.forEach(o),mh=h(C),la=s(C,"LI",{});var db=i(la);sa=s(db,"CODE",{});var cb=i(sa);uh=a(cb,"CamembertTokenizer"),cb.forEach(o),db.forEach(o),_h=h(C),ia=s(C,"LI",{});var hb=i(ia);na=s(hb,"CODE",{});var fb=i(na);vh=a(fb,"MBartTokenizer"),fb.forEach(o),hb.forEach(o),Eh=h(C),da=s(C,"LI",{});var pb=i(da);ca=s(pb,"CODE",{});var mb=i(ca);bh=a(mb,"PegasusTokenizer"),mb.forEach(o),pb.forEach(o),wh=h(C),ha=s(C,"LI",{});var ub=i(ha);fa=s(ub,"CODE",{});var _b=i(fa);yh=a(_b,"T5Tokenizer"),_b.forEach(o),ub.forEach(o),kh=h(C),pa=s(C,"LI",{});var vb=i(pa);ma=s(vb,"CODE",{});var Eb=i(ma);Th=a(Eb,"ReformerTokenizer"),Eb.forEach(o),vb.forEach(o),gh=h(C),ua=s(C,"LI",{});var bb=i(ua);_a=s(bb,"CODE",{});var wb=i(_a);$h=a(wb,"XLMRobertaTokenizer"),wb.forEach(o),bb.forEach(o),C.forEach(o),tn=h(t),ke=s(t,"H5",{class:!0});var Sd=i(ke);ot=s(Sd,"A",{id:!0,class:!0,href:!0});var yb=i(ot);va=s(yb,"SPAN",{});var kb=i(va);m(vo.$$.fragment,kb),kb.forEach(o),yb.forEach(o),Ch=h(Sd),Ea=s(Sd,"SPAN",{});var Tb=i(Ea);Oh=a(Tb,"How to obtain the same behavior as v3.x in v4.x"),Tb.forEach(o),Sd.forEach(o),on=h(t),ee=s(t,"P",{});var vr=i(ee);Dh=a(vr,"In order to obtain the same behavior as version "),ba=s(vr,"CODE",{});var gb=i(ba);xh=a(gb,"v3.x"),gb.forEach(o),Ah=a(vr,", you should install "),wa=s(vr,"CODE",{});var $b=i(wa);zh=a($b,"sentencepiece"),$b.forEach(o),Ph=a(vr," additionally:"),vr.forEach(o),rn=h(t),rt=s(t,"P",{});var Md=i(rt);Lh=a(Md,"In version "),ya=s(Md,"CODE",{});var Cb=i(ya);Ih=a(Cb,"v3.x"),Cb.forEach(o),Sh=a(Md,":"),Md.forEach(o),an=h(t),m(Eo.$$.fragment,t),ln=h(t),at=s(t,"P",{});var jd=i(at);Mh=a(jd,"to obtain the same in version "),ka=s(jd,"CODE",{});var Ob=i(ka);jh=a(Ob,"v4.x"),Ob.forEach(o),Fh=a(jd,":"),jd.forEach(o),sn=h(t),m(bo.$$.fragment,t),nn=h(t),rr=s(t,"P",{});var Db=i(rr);Bh=a(Db,"or"),Db.forEach(o),dn=h(t),m(wo.$$.fragment,t),cn=h(t),Te=s(t,"H4",{class:!0});var Fd=i(Te);lt=s(Fd,"A",{id:!0,class:!0,href:!0});var xb=i(lt);Ta=s(xb,"SPAN",{});var Ab=i(Ta);m(yo.$$.fragment,Ab),Ab.forEach(o),xb.forEach(o),qh=h(Fd),ga=s(Fd,"SPAN",{});var zb=i(ga);Nh=a(zb,"3. The architecture of the repo has been updated so that each model resides in its folder"),zb.forEach(o),Fd.forEach(o),hn=h(t),st=s(t,"P",{});var Bd=i(st);Hh=a(Bd,"The past and foreseeable addition of new models means that the number of files in the directory "),$a=s(Bd,"CODE",{});var Pb=i($a);Rh=a(Pb,"src/transformers"),Pb.forEach(o),Wh=a(Bd," keeps growing and becomes harder to navigate and understand. We made the choice to put each model and the files accompanying it in their own sub-directories."),Bd.forEach(o),fn=h(t),ar=s(t,"P",{});var Lb=i(ar);Gh=a(Lb,"This is a breaking change as importing intermediary layers using a model\u2019s module directly needs to be done via a different path."),Lb.forEach(o),pn=h(t),ge=s(t,"H5",{class:!0});var qd=i(ge);it=s(qd,"A",{id:!0,class:!0,href:!0});var Ib=i(it);Ca=s(Ib,"SPAN",{});var Sb=i(Ca);m(ko.$$.fragment,Sb),Sb.forEach(o),Ib.forEach(o),Uh=h(qd),Oa=s(qd,"SPAN",{});var Mb=i(Oa);Xh=a(Mb,"How to obtain the same behavior as v3.x in v4.x"),Mb.forEach(o),qd.forEach(o),mn=h(t),nt=s(t,"P",{});var Nd=i(nt);Kh=a(Nd,"In order to obtain the same behavior as version "),Da=s(Nd,"CODE",{});var jb=i(Da);Jh=a(jb,"v3.x"),jb.forEach(o),Qh=a(Nd,", you should update the path used to access the layers."),Nd.forEach(o),un=h(t),dt=s(t,"P",{});var Hd=i(dt);Vh=a(Hd,"In version "),xa=s(Hd,"CODE",{});var Fb=i(xa);Yh=a(Fb,"v3.x"),Fb.forEach(o),Zh=a(Hd,":"),Hd.forEach(o),_n=h(t),m(To.$$.fragment,t),vn=h(t),ct=s(t,"P",{});var Rd=i(ct);ef=a(Rd,"to obtain the same in version "),Aa=s(Rd,"CODE",{});var Bb=i(Aa);tf=a(Bb,"v4.x"),Bb.forEach(o),of=a(Rd,":"),Rd.forEach(o),En=h(t),m(go.$$.fragment,t),bn=h(t),$e=s(t,"H4",{class:!0});var Wd=i($e);ht=s(Wd,"A",{id:!0,class:!0,href:!0});var qb=i(ht);za=s(qb,"SPAN",{});var Nb=i(za);m($o.$$.fragment,Nb),Nb.forEach(o),qb.forEach(o),rf=h(Wd),Ce=s(Wd,"SPAN",{});var Er=i(Ce);af=a(Er,"4. Switching the "),Pa=s(Er,"CODE",{});var Hb=i(Pa);lf=a(Hb,"return_dict"),Hb.forEach(o),sf=a(Er," argument to "),La=s(Er,"CODE",{});var Rb=i(La);nf=a(Rb,"True"),Rb.forEach(o),df=a(Er," by default"),Er.forEach(o),Wd.forEach(o),wn=h(t),ft=s(t,"P",{});var Gd=i(ft);cf=a(Gd,"The "),Co=s(Gd,"A",{href:!0});var OE=i(Co);Ia=s(OE,"CODE",{});var Wb=i(Ia);hf=a(Wb,"return_dict"),Wb.forEach(o),ff=a(OE," argument"),OE.forEach(o),pf=a(Gd," enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. This object is self-documented as keys can be used to retrieve values, while also behaving as a tuple as users may retrieve objects by index or by slice."),Gd.forEach(o),yn=h(t),pt=s(t,"P",{});var Ud=i(pt);mf=a(Ud,"This is a breaking change as the limitation of that tuple is that it cannot be unpacked: "),Sa=s(Ud,"CODE",{});var Gb=i(Sa);uf=a(Gb,"value0, value1 = outputs"),Gb.forEach(o),_f=a(Ud," will not work."),Ud.forEach(o),kn=h(t),Oe=s(t,"H5",{class:!0});var Xd=i(Oe);mt=s(Xd,"A",{id:!0,class:!0,href:!0});var Ub=i(mt);Ma=s(Ub,"SPAN",{});var Xb=i(Ma);m(Oo.$$.fragment,Xb),Xb.forEach(o),Ub.forEach(o),vf=h(Xd),ja=s(Xd,"SPAN",{});var Kb=i(ja);Ef=a(Kb,"How to obtain the same behavior as v3.x in v4.x"),Kb.forEach(o),Xd.forEach(o),Tn=h(t),P=s(t,"P",{});var Wt=i(P);bf=a(Wt,"In order to obtain the same behavior as version "),Fa=s(Wt,"CODE",{});var Jb=i(Fa);wf=a(Jb,"v3.x"),Jb.forEach(o),yf=a(Wt,", you should specify the "),Ba=s(Wt,"CODE",{});var Qb=i(Ba);kf=a(Qb,"return_dict"),Qb.forEach(o),Tf=a(Wt," argument to "),qa=s(Wt,"CODE",{});var Vb=i(qa);gf=a(Vb,"False"),Vb.forEach(o),$f=a(Wt,", either in the model configuration or during the forward pass."),Wt.forEach(o),gn=h(t),ut=s(t,"P",{});var Kd=i(ut);Cf=a(Kd,"In version "),Na=s(Kd,"CODE",{});var Yb=i(Na);Of=a(Yb,"v3.x"),Yb.forEach(o),Df=a(Kd,":"),Kd.forEach(o),$n=h(t),m(Do.$$.fragment,t),Cn=h(t),_t=s(t,"P",{});var Jd=i(_t);xf=a(Jd,"to obtain the same in version "),Ha=s(Jd,"CODE",{});var Zb=i(Ha);Af=a(Zb,"v4.x"),Zb.forEach(o),zf=a(Jd,":"),Jd.forEach(o),On=h(t),m(xo.$$.fragment,t),Dn=h(t),lr=s(t,"P",{});var e3=i(lr);Pf=a(e3,"or"),e3.forEach(o),xn=h(t),m(Ao.$$.fragment,t),An=h(t),De=s(t,"H4",{class:!0});var Qd=i(De);vt=s(Qd,"A",{id:!0,class:!0,href:!0});var t3=i(vt);Ra=s(t3,"SPAN",{});var o3=i(Ra);m(zo.$$.fragment,o3),o3.forEach(o),t3.forEach(o),Lf=h(Qd),Wa=s(Qd,"SPAN",{});var r3=i(Wa);If=a(r3,"5. Removed some deprecated attributes"),r3.forEach(o),Qd.forEach(o),zn=h(t),Et=s(t,"P",{});var Vd=i(Et);Sf=a(Vd,"Attributes that were deprecated have been removed if they had been deprecated for at least a month. The full list of deprecated attributes can be found in "),Po=s(Vd,"A",{href:!0,rel:!0});var a3=i(Po);Mf=a(a3,"#8604"),a3.forEach(o),jf=a(Vd,"."),Vd.forEach(o),Pn=h(t),sr=s(t,"P",{});var l3=i(sr);Ff=a(l3,"Here is a list of these attributes/methods/arguments and what their replacements should be:"),l3.forEach(o),Ln=h(t),ir=s(t,"P",{});var s3=i(ir);Bf=a(s3,"In several models, the labels become consistent with the other models:"),s3.forEach(o),In=h(t),b=s(t,"UL",{});var y=i(b);L=s(y,"LI",{});var Re=i(L);Ga=s(Re,"CODE",{});var i3=i(Ga);qf=a(i3,"masked_lm_labels"),i3.forEach(o),Nf=a(Re," becomes "),Ua=s(Re,"CODE",{});var n3=i(Ua);Hf=a(n3,"labels"),n3.forEach(o),Rf=a(Re," in "),Xa=s(Re,"CODE",{});var d3=i(Xa);Wf=a(d3,"AlbertForMaskedLM"),d3.forEach(o),Gf=a(Re," and "),Ka=s(Re,"CODE",{});var c3=i(Ka);Uf=a(c3,"AlbertForPreTraining"),c3.forEach(o),Xf=a(Re,"."),Re.forEach(o),Kf=h(y),I=s(y,"LI",{});var We=i(I);Ja=s(We,"CODE",{});var h3=i(Ja);Jf=a(h3,"masked_lm_labels"),h3.forEach(o),Qf=a(We," becomes "),Qa=s(We,"CODE",{});var f3=i(Qa);Vf=a(f3,"labels"),f3.forEach(o),Yf=a(We," in "),Va=s(We,"CODE",{});var p3=i(Va);Zf=a(p3,"BertForMaskedLM"),p3.forEach(o),ep=a(We," and "),Ya=s(We,"CODE",{});var m3=i(Ya);tp=a(m3,"BertForPreTraining"),m3.forEach(o),op=a(We,"."),We.forEach(o),rp=h(y),te=s(y,"LI",{});var Wo=i(te);Za=s(Wo,"CODE",{});var u3=i(Za);ap=a(u3,"masked_lm_labels"),u3.forEach(o),lp=a(Wo," becomes "),el=s(Wo,"CODE",{});var _3=i(el);sp=a(_3,"labels"),_3.forEach(o),ip=a(Wo," in "),tl=s(Wo,"CODE",{});var v3=i(tl);np=a(v3,"DistilBertForMaskedLM"),v3.forEach(o),dp=a(Wo,"."),Wo.forEach(o),cp=h(y),oe=s(y,"LI",{});var Go=i(oe);ol=s(Go,"CODE",{});var E3=i(ol);hp=a(E3,"masked_lm_labels"),E3.forEach(o),fp=a(Go," becomes "),rl=s(Go,"CODE",{});var b3=i(rl);pp=a(b3,"labels"),b3.forEach(o),mp=a(Go," in "),al=s(Go,"CODE",{});var w3=i(al);up=a(w3,"ElectraForMaskedLM"),w3.forEach(o),_p=a(Go,"."),Go.forEach(o),vp=h(y),re=s(y,"LI",{});var Uo=i(re);ll=s(Uo,"CODE",{});var y3=i(ll);Ep=a(y3,"masked_lm_labels"),y3.forEach(o),bp=a(Uo," becomes "),sl=s(Uo,"CODE",{});var k3=i(sl);wp=a(k3,"labels"),k3.forEach(o),yp=a(Uo," in "),il=s(Uo,"CODE",{});var T3=i(il);kp=a(T3,"LongformerForMaskedLM"),T3.forEach(o),Tp=a(Uo,"."),Uo.forEach(o),gp=h(y),ae=s(y,"LI",{});var Xo=i(ae);nl=s(Xo,"CODE",{});var g3=i(nl);$p=a(g3,"masked_lm_labels"),g3.forEach(o),Cp=a(Xo," becomes "),dl=s(Xo,"CODE",{});var $3=i(dl);Op=a($3,"labels"),$3.forEach(o),Dp=a(Xo," in "),cl=s(Xo,"CODE",{});var C3=i(cl);xp=a(C3,"MobileBertForMaskedLM"),C3.forEach(o),Ap=a(Xo,"."),Xo.forEach(o),zp=h(y),le=s(y,"LI",{});var Ko=i(le);hl=s(Ko,"CODE",{});var O3=i(hl);Pp=a(O3,"masked_lm_labels"),O3.forEach(o),Lp=a(Ko," becomes "),fl=s(Ko,"CODE",{});var D3=i(fl);Ip=a(D3,"labels"),D3.forEach(o),Sp=a(Ko," in "),pl=s(Ko,"CODE",{});var x3=i(pl);Mp=a(x3,"RobertaForMaskedLM"),x3.forEach(o),jp=a(Ko,"."),Ko.forEach(o),Fp=h(y),se=s(y,"LI",{});var Jo=i(se);ml=s(Jo,"CODE",{});var A3=i(ml);Bp=a(A3,"lm_labels"),A3.forEach(o),qp=a(Jo," becomes "),ul=s(Jo,"CODE",{});var z3=i(ul);Np=a(z3,"labels"),z3.forEach(o),Hp=a(Jo," in "),_l=s(Jo,"CODE",{});var P3=i(_l);Rp=a(P3,"BartForConditionalGeneration"),P3.forEach(o),Wp=a(Jo,"."),Jo.forEach(o),Gp=h(y),ie=s(y,"LI",{});var Qo=i(ie);vl=s(Qo,"CODE",{});var L3=i(vl);Up=a(L3,"lm_labels"),L3.forEach(o),Xp=a(Qo," becomes "),El=s(Qo,"CODE",{});var I3=i(El);Kp=a(I3,"labels"),I3.forEach(o),Jp=a(Qo," in "),bl=s(Qo,"CODE",{});var S3=i(bl);Qp=a(S3,"GPT2DoubleHeadsModel"),S3.forEach(o),Vp=a(Qo,"."),Qo.forEach(o),Yp=h(y),ne=s(y,"LI",{});var Vo=i(ne);wl=s(Vo,"CODE",{});var M3=i(wl);Zp=a(M3,"lm_labels"),M3.forEach(o),em=a(Vo," becomes "),yl=s(Vo,"CODE",{});var j3=i(yl);tm=a(j3,"labels"),j3.forEach(o),om=a(Vo," in "),kl=s(Vo,"CODE",{});var F3=i(kl);rm=a(F3,"OpenAIGPTDoubleHeadsModel"),F3.forEach(o),am=a(Vo,"."),Vo.forEach(o),lm=h(y),de=s(y,"LI",{});var Yo=i(de);Tl=s(Yo,"CODE",{});var B3=i(Tl);sm=a(B3,"lm_labels"),B3.forEach(o),im=a(Yo," becomes "),gl=s(Yo,"CODE",{});var q3=i(gl);nm=a(q3,"labels"),q3.forEach(o),dm=a(Yo," in "),$l=s(Yo,"CODE",{});var N3=i($l);cm=a(N3,"T5ForConditionalGeneration"),N3.forEach(o),hm=a(Yo,"."),Yo.forEach(o),y.forEach(o),Sn=h(t),nr=s(t,"P",{});var H3=i(nr);fm=a(H3,"In several models, the caching mechanism becomes consistent with the other models:"),H3.forEach(o),Mn=h(t),S=s(t,"UL",{});var Gt=i(S);bt=s(Gt,"LI",{});var Si=i(bt);Cl=s(Si,"CODE",{});var R3=i(Cl);pm=a(R3,"decoder_cached_states"),R3.forEach(o),mm=a(Si," becomes "),Ol=s(Si,"CODE",{});var W3=i(Ol);um=a(W3,"past_key_values"),W3.forEach(o),_m=a(Si," in all BART-like, FSMT and T5 models."),Si.forEach(o),vm=h(Gt),wt=s(Gt,"LI",{});var Mi=i(wt);Dl=s(Mi,"CODE",{});var G3=i(Dl);Em=a(G3,"decoder_past_key_values"),G3.forEach(o),bm=a(Mi," becomes "),xl=s(Mi,"CODE",{});var U3=i(xl);wm=a(U3,"past_key_values"),U3.forEach(o),ym=a(Mi," in all BART-like, FSMT and T5 models."),Mi.forEach(o),km=h(Gt),yt=s(Gt,"LI",{});var ji=i(yt);Al=s(ji,"CODE",{});var X3=i(Al);Tm=a(X3,"past"),X3.forEach(o),gm=a(ji," becomes "),zl=s(ji,"CODE",{});var K3=i(zl);$m=a(K3,"past_key_values"),K3.forEach(o),Cm=a(ji," in all CTRL models."),ji.forEach(o),Om=h(Gt),kt=s(Gt,"LI",{});var Fi=i(kt);Pl=s(Fi,"CODE",{});var J3=i(Pl);Dm=a(J3,"past"),J3.forEach(o),xm=a(Fi," becomes "),Ll=s(Fi,"CODE",{});var Q3=i(Ll);Am=a(Q3,"past_key_values"),Q3.forEach(o),zm=a(Fi," in all GPT-2 models."),Fi.forEach(o),Gt.forEach(o),jn=h(t),dr=s(t,"P",{});var V3=i(dr);Pm=a(V3,"Regarding the tokenizer classes:"),V3.forEach(o),Fn=h(t),ce=s(t,"UL",{});var br=i(ce);xe=s(br,"LI",{});var wr=i(xe);Lm=a(wr,"The tokenizer attribute "),Il=s(wr,"CODE",{});var Y3=i(Il);Im=a(Y3,"max_len"),Y3.forEach(o),Sm=a(wr," becomes "),Sl=s(wr,"CODE",{});var Z3=i(Sl);Mm=a(Z3,"model_max_length"),Z3.forEach(o),jm=a(wr,"."),wr.forEach(o),Fm=h(br),Ae=s(br,"LI",{});var yr=i(Ae);Bm=a(yr,"The tokenizer attribute "),Ml=s(yr,"CODE",{});var e4=i(Ml);qm=a(e4,"return_lengths"),e4.forEach(o),Nm=a(yr," becomes "),jl=s(yr,"CODE",{});var t4=i(jl);Hm=a(t4,"return_length"),t4.forEach(o),Rm=a(yr,"."),yr.forEach(o),Wm=h(br),ze=s(br,"LI",{});var kr=i(ze);Gm=a(kr,"The tokenizer encoding argument "),Fl=s(kr,"CODE",{});var o4=i(Fl);Um=a(o4,"is_pretokenized"),o4.forEach(o),Xm=a(kr," becomes "),Bl=s(kr,"CODE",{});var r4=i(Bl);Km=a(r4,"is_split_into_words"),r4.forEach(o),Jm=a(kr,"."),kr.forEach(o),br.forEach(o),Bn=h(t),Tt=s(t,"P",{});var Yd=i(Tt);Qm=a(Yd,"Regarding the "),ql=s(Yd,"CODE",{});var a4=i(ql);Vm=a(a4,"Trainer"),a4.forEach(o),Ym=a(Yd," class:"),Yd.forEach(o),qn=h(t),g=s(t,"UL",{});var O=i(g);B=s(O,"LI",{});var Ut=i(B);Zm=a(Ut,"The "),Nl=s(Ut,"CODE",{});var l4=i(Nl);eu=a(l4,"Trainer"),l4.forEach(o),tu=a(Ut," argument "),Hl=s(Ut,"CODE",{});var s4=i(Hl);ou=a(s4,"tb_writer"),s4.forEach(o),ru=a(Ut," is removed in favor of the callback "),Rl=s(Ut,"CODE",{});var i4=i(Rl);au=a(i4,"TensorBoardCallback(tb_writer=...)"),i4.forEach(o),lu=a(Ut,"."),Ut.forEach(o),su=h(O),q=s(O,"LI",{});var Xt=i(q);iu=a(Xt,"The "),Wl=s(Xt,"CODE",{});var n4=i(Wl);nu=a(n4,"Trainer"),n4.forEach(o),du=a(Xt," argument "),Gl=s(Xt,"CODE",{});var d4=i(Gl);cu=a(d4,"prediction_loss_only"),d4.forEach(o),hu=a(Xt," is removed in favor of the class argument "),Ul=s(Xt,"CODE",{});var c4=i(Ul);fu=a(c4,"args.prediction_loss_only"),c4.forEach(o),pu=a(Xt,"."),Xt.forEach(o),mu=h(O),Pe=s(O,"LI",{});var Tr=i(Pe);uu=a(Tr,"The "),Xl=s(Tr,"CODE",{});var h4=i(Xl);_u=a(h4,"Trainer"),h4.forEach(o),vu=a(Tr," attribute "),Kl=s(Tr,"CODE",{});var f4=i(Kl);Eu=a(f4,"data_collator"),f4.forEach(o),bu=a(Tr," should be a callable."),Tr.forEach(o),wu=h(O),N=s(O,"LI",{});var Kt=i(N);yu=a(Kt,"The "),Jl=s(Kt,"CODE",{});var p4=i(Jl);ku=a(p4,"Trainer"),p4.forEach(o),Tu=a(Kt," method "),Ql=s(Kt,"CODE",{});var m4=i(Ql);gu=a(m4,"_log"),m4.forEach(o),$u=a(Kt," is deprecated in favor of "),Vl=s(Kt,"CODE",{});var u4=i(Vl);Cu=a(u4,"log"),u4.forEach(o),Ou=a(Kt,"."),Kt.forEach(o),Du=h(O),H=s(O,"LI",{});var Jt=i(H);xu=a(Jt,"The "),Yl=s(Jt,"CODE",{});var _4=i(Yl);Au=a(_4,"Trainer"),_4.forEach(o),zu=a(Jt," method "),Zl=s(Jt,"CODE",{});var v4=i(Zl);Pu=a(v4,"_training_step"),v4.forEach(o),Lu=a(Jt," is deprecated in favor of "),es=s(Jt,"CODE",{});var E4=i(es);Iu=a(E4,"training_step"),E4.forEach(o),Su=a(Jt,"."),Jt.forEach(o),Mu=h(O),R=s(O,"LI",{});var Qt=i(R);ju=a(Qt,"The "),ts=s(Qt,"CODE",{});var b4=i(ts);Fu=a(b4,"Trainer"),b4.forEach(o),Bu=a(Qt," method "),os=s(Qt,"CODE",{});var w4=i(os);qu=a(w4,"_prediction_loop"),w4.forEach(o),Nu=a(Qt," is deprecated in favor of "),rs=s(Qt,"CODE",{});var y4=i(rs);Hu=a(y4,"prediction_loop"),y4.forEach(o),Ru=a(Qt,"."),Qt.forEach(o),Wu=h(O),W=s(O,"LI",{});var Vt=i(W);Gu=a(Vt,"The "),as=s(Vt,"CODE",{});var k4=i(as);Uu=a(k4,"Trainer"),k4.forEach(o),Xu=a(Vt," method "),ls=s(Vt,"CODE",{});var T4=i(ls);Ku=a(T4,"is_local_master"),T4.forEach(o),Ju=a(Vt," is deprecated in favor of "),ss=s(Vt,"CODE",{});var g4=i(ss);Qu=a(g4,"is_local_process_zero"),g4.forEach(o),Vu=a(Vt,"."),Vt.forEach(o),Yu=h(O),G=s(O,"LI",{});var Yt=i(G);Zu=a(Yt,"The "),is=s(Yt,"CODE",{});var $4=i(is);e_=a($4,"Trainer"),$4.forEach(o),t_=a(Yt," method "),ns=s(Yt,"CODE",{});var C4=i(ns);o_=a(C4,"is_world_master"),C4.forEach(o),r_=a(Yt," is deprecated in favor of "),ds=s(Yt,"CODE",{});var O4=i(ds);a_=a(O4,"is_world_process_zero"),O4.forEach(o),l_=a(Yt,"."),Yt.forEach(o),O.forEach(o),Nn=h(t),gt=s(t,"P",{});var Zd=i(gt);s_=a(Zd,"Regarding the "),cs=s(Zd,"CODE",{});var D4=i(cs);i_=a(D4,"TFTrainer"),D4.forEach(o),n_=a(Zd," class:"),Zd.forEach(o),Hn=h(t),D=s(t,"UL",{});var _e=i(D);U=s(_e,"LI",{});var Zt=i(U);d_=a(Zt,"The "),hs=s(Zt,"CODE",{});var x4=i(hs);c_=a(x4,"TFTrainer"),x4.forEach(o),h_=a(Zt," argument "),fs=s(Zt,"CODE",{});var A4=i(fs);f_=a(A4,"prediction_loss_only"),A4.forEach(o),p_=a(Zt," is removed in favor of the class argument "),ps=s(Zt,"CODE",{});var z4=i(ps);m_=a(z4,"args.prediction_loss_only"),z4.forEach(o),u_=a(Zt,"."),Zt.forEach(o),__=h(_e),X=s(_e,"LI",{});var eo=i(X);v_=a(eo,"The "),ms=s(eo,"CODE",{});var P4=i(ms);E_=a(P4,"Trainer"),P4.forEach(o),b_=a(eo," method "),us=s(eo,"CODE",{});var L4=i(us);w_=a(L4,"_log"),L4.forEach(o),y_=a(eo," is deprecated in favor of "),_s=s(eo,"CODE",{});var I4=i(_s);k_=a(I4,"log"),I4.forEach(o),T_=a(eo,"."),eo.forEach(o),g_=h(_e),K=s(_e,"LI",{});var to=i(K);$_=a(to,"The "),vs=s(to,"CODE",{});var S4=i(vs);C_=a(S4,"TFTrainer"),S4.forEach(o),O_=a(to," method "),Es=s(to,"CODE",{});var M4=i(Es);D_=a(M4,"_prediction_loop"),M4.forEach(o),x_=a(to," is deprecated in favor of "),bs=s(to,"CODE",{});var j4=i(bs);A_=a(j4,"prediction_loop"),j4.forEach(o),z_=a(to,"."),to.forEach(o),P_=h(_e),J=s(_e,"LI",{});var oo=i(J);L_=a(oo,"The "),ws=s(oo,"CODE",{});var F4=i(ws);I_=a(F4,"TFTrainer"),F4.forEach(o),S_=a(oo," method "),ys=s(oo,"CODE",{});var B4=i(ys);M_=a(B4,"_setup_wandb"),B4.forEach(o),j_=a(oo," is deprecated in favor of "),ks=s(oo,"CODE",{});var q4=i(ks);F_=a(q4,"setup_wandb"),q4.forEach(o),B_=a(oo,"."),oo.forEach(o),q_=h(_e),Q=s(_e,"LI",{});var ro=i(Q);N_=a(ro,"The "),Ts=s(ro,"CODE",{});var N4=i(Ts);H_=a(N4,"TFTrainer"),N4.forEach(o),R_=a(ro," method "),gs=s(ro,"CODE",{});var H4=i(gs);W_=a(H4,"_run_model"),H4.forEach(o),G_=a(ro," is deprecated in favor of "),$s=s(ro,"CODE",{});var R4=i($s);U_=a(R4,"run_model"),R4.forEach(o),X_=a(ro,"."),ro.forEach(o),_e.forEach(o),Rn=h(t),$t=s(t,"P",{});var ec=i($t);K_=a(ec,"Regarding the "),Cs=s(ec,"CODE",{});var W4=i(Cs);J_=a(W4,"TrainingArguments"),W4.forEach(o),Q_=a(ec," class:"),ec.forEach(o),Wn=h(t),cr=s(t,"UL",{});var G4=i(cr);V=s(G4,"LI",{});var ao=i(V);V_=a(ao,"The "),Os=s(ao,"CODE",{});var U4=i(Os);Y_=a(U4,"TrainingArguments"),U4.forEach(o),Z_=a(ao," argument "),Ds=s(ao,"CODE",{});var X4=i(Ds);ev=a(X4,"evaluate_during_training"),X4.forEach(o),tv=a(ao," is deprecated in favor of "),xs=s(ao,"CODE",{});var K4=i(xs);ov=a(K4,"evaluation_strategy"),K4.forEach(o),rv=a(ao,"."),ao.forEach(o),G4.forEach(o),Gn=h(t),hr=s(t,"P",{});var J4=i(hr);av=a(J4,"Regarding the Transfo-XL model:"),J4.forEach(o),Un=h(t),Ct=s(t,"UL",{});var tc=i(Ct);Le=s(tc,"LI",{});var gr=i(Le);lv=a(gr,"The Transfo-XL configuration attribute "),As=s(gr,"CODE",{});var Q4=i(As);sv=a(Q4,"tie_weight"),Q4.forEach(o),iv=a(gr," becomes "),zs=s(gr,"CODE",{});var V4=i(zs);nv=a(V4,"tie_words_embeddings"),V4.forEach(o),dv=a(gr,"."),gr.forEach(o),cv=h(tc),Ie=s(tc,"LI",{});var $r=i(Ie);hv=a($r,"The Transfo-XL modeling method "),Ps=s($r,"CODE",{});var Y4=i(Ps);fv=a(Y4,"reset_length"),Y4.forEach(o),pv=a($r," becomes "),Ls=s($r,"CODE",{});var Z4=i(Ls);mv=a(Z4,"reset_memory_length"),Z4.forEach(o),uv=a($r,"."),$r.forEach(o),tc.forEach(o),Xn=h(t),fr=s(t,"P",{});var e2=i(fr);_v=a(e2,"Regarding pipelines:"),e2.forEach(o),Kn=h(t),pr=s(t,"UL",{});var t2=i(pr);Y=s(t2,"LI",{});var lo=i(Y);vv=a(lo,"The "),Is=s(lo,"CODE",{});var o2=i(Is);Ev=a(o2,"FillMaskPipeline"),o2.forEach(o),bv=a(lo," argument "),Ss=s(lo,"CODE",{});var r2=i(Ss);wv=a(r2,"topk"),r2.forEach(o),yv=a(lo," becomes "),Ms=s(lo,"CODE",{});var a2=i(Ms);kv=a(a2,"top_k"),a2.forEach(o),Tv=a(lo,"."),lo.forEach(o),t2.forEach(o),Jn=h(t),Se=s(t,"H2",{class:!0});var oc=i(Se);Ot=s(oc,"A",{id:!0,class:!0,href:!0});var l2=i(Ot);js=s(l2,"SPAN",{});var s2=i(js);m(Lo.$$.fragment,s2),s2.forEach(o),l2.forEach(o),gv=h(oc),Fs=s(oc,"SPAN",{});var i2=i(Fs);$v=a(i2,"Migrating from pytorch-transformers to \u{1F917} Transformers"),i2.forEach(o),oc.forEach(o),Qn=h(t),Dt=s(t,"P",{});var rc=i(Dt);Cv=a(rc,"Here is a quick summary of what you should take care of when migrating from "),Bs=s(rc,"CODE",{});var n2=i(Bs);Ov=a(n2,"pytorch-transformers"),n2.forEach(o),Dv=a(rc," to \u{1F917} Transformers."),rc.forEach(o),Vn=h(t),Me=s(t,"H3",{class:!0});var ac=i(Me);xt=s(ac,"A",{id:!0,class:!0,href:!0});var d2=i(xt);qs=s(d2,"SPAN",{});var c2=i(qs);m(Io.$$.fragment,c2),c2.forEach(o),d2.forEach(o),xv=h(ac),je=s(ac,"SPAN",{});var Cr=i(je);Av=a(Cr,"Positional order of some models' keywords inputs ("),Ns=s(Cr,"CODE",{});var h2=i(Ns);zv=a(h2,"attention_mask"),h2.forEach(o),Pv=a(Cr,", "),Hs=s(Cr,"CODE",{});var f2=i(Hs);Lv=a(f2,"token_type_ids"),f2.forEach(o),Iv=a(Cr,"...) changed"),Cr.forEach(o),ac.forEach(o),Yn=h(t),M=s(t,"P",{});var so=i(M);Sv=a(so,"To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models "),Rs=s(so,"STRONG",{});var p2=i(Rs);Mv=a(p2,"keywords inputs"),p2.forEach(o),jv=a(so," ("),Ws=s(so,"CODE",{});var m2=i(Ws);Fv=a(m2,"attention_mask"),m2.forEach(o),Bv=a(so,", "),Gs=s(so,"CODE",{});var u2=i(Gs);qv=a(u2,"token_type_ids"),u2.forEach(o),Nv=a(so,"\u2026) has been changed."),so.forEach(o),Zn=h(t),At=s(t,"P",{});var lc=i(At);Hv=a(lc,"If you used to call the models with keyword names for keyword arguments, e.g. "),Us=s(lc,"CODE",{});var _2=i(Us);Rv=a(_2,"model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"),_2.forEach(o),Wv=a(lc,", this should not cause any change."),lc.forEach(o),ed=h(t),zt=s(t,"P",{});var sc=i(zt);Gv=a(sc,"If you used to call the models with positional inputs for keyword arguments, e.g. "),Xs=s(sc,"CODE",{});var v2=i(Xs);Uv=a(v2,"model(inputs_ids, attention_mask, token_type_ids)"),v2.forEach(o),Xv=a(sc,", you may have to double check the exact order of input arguments."),sc.forEach(o),td=h(t),Fe=s(t,"H2",{class:!0});var ic=i(Fe);Pt=s(ic,"A",{id:!0,class:!0,href:!0});var E2=i(Pt);Ks=s(E2,"SPAN",{});var b2=i(Ks);m(So.$$.fragment,b2),b2.forEach(o),E2.forEach(o),Kv=h(ic),Js=s(ic,"SPAN",{});var w2=i(Js);Jv=a(w2,"Migrating from pytorch-pretrained-bert"),w2.forEach(o),ic.forEach(o),od=h(t),Lt=s(t,"P",{});var nc=i(Lt);Qv=a(nc,"Here is a quick summary of what you should take care of when migrating from "),Qs=s(nc,"CODE",{});var y2=i(Qs);Vv=a(y2,"pytorch-pretrained-bert"),y2.forEach(o),Yv=a(nc," to \u{1F917} Transformers"),nc.forEach(o),rd=h(t),Be=s(t,"H3",{class:!0});var dc=i(Be);It=s(dc,"A",{id:!0,class:!0,href:!0});var k2=i(It);Vs=s(k2,"SPAN",{});var T2=i(Vs);m(Mo.$$.fragment,T2),T2.forEach(o),k2.forEach(o),Zv=h(dc),mr=s(dc,"SPAN",{});var DE=i(mr);e1=a(DE,"Models always output "),Ys=s(DE,"CODE",{});var g2=i(Ys);t1=a(g2,"tuples"),g2.forEach(o),DE.forEach(o),dc.forEach(o),ad=h(t),he=s(t,"P",{});var Or=i(he);o1=a(Or,"The main breaking change when migrating from "),Zs=s(Or,"CODE",{});var $2=i(Zs);r1=a($2,"pytorch-pretrained-bert"),$2.forEach(o),a1=a(Or," to \u{1F917} Transformers is that the models forward method always outputs a "),ei=s(Or,"CODE",{});var C2=i(ei);l1=a(C2,"tuple"),C2.forEach(o),s1=a(Or," with various elements depending on the model and the configuration parameters."),Or.forEach(o),ld=h(t),St=s(t,"P",{});var cc=i(St);i1=a(cc,"The exact content of the tuples for each model are detailed in the models\u2019 docstrings and the "),jo=s(cc,"A",{href:!0,rel:!0});var O2=i(jo);n1=a(O2,"documentation"),O2.forEach(o),d1=a(cc,"."),cc.forEach(o),sd=h(t),Mt=s(t,"P",{});var hc=i(Mt);c1=a(hc,"In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in "),ti=s(hc,"CODE",{});var D2=i(ti);h1=a(D2,"pytorch-pretrained-bert"),D2.forEach(o),f1=a(hc,"."),hc.forEach(o),id=h(t),fe=s(t,"P",{});var Dr=i(fe);p1=a(Dr,"Here is a "),oi=s(Dr,"CODE",{});var x2=i(oi);m1=a(x2,"pytorch-pretrained-bert"),x2.forEach(o),u1=a(Dr," to \u{1F917} Transformers conversion example for a "),ri=s(Dr,"CODE",{});var A2=i(ri);_1=a(A2,"BertForSequenceClassification"),A2.forEach(o),v1=a(Dr," classification model:"),Dr.forEach(o),nd=h(t),m(Fo.$$.fragment,t),dd=h(t),qe=s(t,"H3",{class:!0});var fc=i(qe);jt=s(fc,"A",{id:!0,class:!0,href:!0});var z2=i(jt);ai=s(z2,"SPAN",{});var P2=i(ai);m(Bo.$$.fragment,P2),P2.forEach(o),z2.forEach(o),E1=h(fc),li=s(fc,"SPAN",{});var L2=i(li);b1=a(L2,"Serialization"),L2.forEach(o),fc.forEach(o),cd=h(t),Ft=s(t,"P",{});var pc=i(Ft);w1=a(pc,"Breaking change in the "),si=s(pc,"CODE",{});var I2=i(si);y1=a(I2,"from_pretrained()"),I2.forEach(o),k1=a(pc,"method:"),pc.forEach(o),hd=h(t),Bt=s(t,"OL",{});var mc=i(Bt);ii=s(mc,"LI",{});var S2=i(ii);Ne=s(S2,"P",{});var xr=i(Ne);T1=a(xr,"Models are now set in evaluation mode by default when instantiated with the "),ni=s(xr,"CODE",{});var M2=i(ni);g1=a(M2,"from_pretrained()"),M2.forEach(o),$1=a(xr," method. To train them don\u2019t forget to set them back in training mode ("),di=s(xr,"CODE",{});var j2=i(di);C1=a(j2,"model.train()"),j2.forEach(o),O1=a(xr,") to activate the dropout modules."),xr.forEach(o),S2.forEach(o),D1=h(mc),ci=s(mc,"LI",{});var F2=i(ci);w=s(F2,"P",{});var k=i(w);x1=a(k,"The additional "),hi=s(k,"CODE",{});var B2=i(hi);A1=a(B2,"*inputs"),B2.forEach(o),z1=a(k," and "),fi=s(k,"CODE",{});var q2=i(fi);P1=a(q2,"**kwargs"),q2.forEach(o),L1=a(k," arguments supplied to the "),pi=s(k,"CODE",{});var N2=i(pi);I1=a(N2,"from_pretrained()"),N2.forEach(o),S1=a(k," method used to be directly passed to the underlying model\u2019s class "),mi=s(k,"CODE",{});var H2=i(mi);M1=a(H2,"__init__()"),H2.forEach(o),j1=a(k," method. They are now used to update the model configuration attribute first which can break derived model classes build based on the previous "),ui=s(k,"CODE",{});var R2=i(ui);F1=a(R2,"BertForSequenceClassification"),R2.forEach(o),B1=a(k," examples. More precisely, the positional arguments "),_i=s(k,"CODE",{});var W2=i(_i);q1=a(W2,"*inputs"),W2.forEach(o),N1=a(k," provided to "),vi=s(k,"CODE",{});var G2=i(vi);H1=a(G2,"from_pretrained()"),G2.forEach(o),R1=a(k," are directly forwarded the model "),Ei=s(k,"CODE",{});var U2=i(Ei);W1=a(U2,"__init__()"),U2.forEach(o),G1=a(k," method while the keyword arguments "),bi=s(k,"CODE",{});var X2=i(bi);U1=a(X2,"**kwargs"),X2.forEach(o),X1=a(k," (i) which match configuration class attributes are used to update said attributes (ii) which don\u2019t match any configuration class attributes are forwarded to the model "),wi=s(k,"CODE",{});var K2=i(wi);K1=a(K2,"__init__()"),K2.forEach(o),J1=a(k," method."),k.forEach(o),F2.forEach(o),mc.forEach(o),fd=h(t),qt=s(t,"P",{});var uc=i(qt);Q1=a(uc,"Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method "),yi=s(uc,"CODE",{});var J2=i(yi);V1=a(J2,"save_pretrained(save_directory)"),J2.forEach(o),Y1=a(uc," if you were using any other serialization method before."),uc.forEach(o),pd=h(t),ur=s(t,"P",{});var Q2=i(ur);Z1=a(Q2,"Here is an example:"),Q2.forEach(o),md=h(t),m(qo.$$.fragment,t),ud=h(t),He=s(t,"H3",{class:!0});var _c=i(He);Nt=s(_c,"A",{id:!0,class:!0,href:!0});var V2=i(Nt);ki=s(V2,"SPAN",{});var Y2=i(ki);m(No.$$.fragment,Y2),Y2.forEach(o),V2.forEach(o),eE=h(_c),Ti=s(_c,"SPAN",{});var Z2=i(Ti);tE=a(Z2,"Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"),Z2.forEach(o),_c.forEach(o),_d=h(t),j=s(t,"P",{});var io=i(j);oE=a(io,"The two optimizers previously included, "),gi=s(io,"CODE",{});var ew=i(gi);rE=a(ew,"BertAdam"),ew.forEach(o),aE=a(io," and "),$i=s(io,"CODE",{});var tw=i($i);lE=a(tw,"OpenAIAdam"),tw.forEach(o),sE=a(io,", have been replaced by a single "),Ci=s(io,"CODE",{});var ow=i(Ci);iE=a(ow,"AdamW"),ow.forEach(o),nE=a(io," optimizer which has a few differences:"),io.forEach(o),vd=h(t),pe=s(t,"UL",{});var Ar=i(pe);Oi=s(Ar,"LI",{});var rw=i(Oi);dE=a(rw,"it only implements weights decay correction,"),rw.forEach(o),cE=h(Ar),Di=s(Ar,"LI",{});var aw=i(Di);hE=a(aw,"schedules are now externals (see below),"),aw.forEach(o),fE=h(Ar),xi=s(Ar,"LI",{});var lw=i(xi);pE=a(lw,"gradient clipping is now also external (see below)."),lw.forEach(o),Ar.forEach(o),Ed=h(t),me=s(t,"P",{});var zr=i(me);mE=a(zr,"The new optimizer "),Ai=s(zr,"CODE",{});var sw=i(Ai);uE=a(sw,"AdamW"),sw.forEach(o),_E=a(zr," matches PyTorch "),zi=s(zr,"CODE",{});var iw=i(zi);vE=a(iw,"Adam"),iw.forEach(o),EE=a(zr," optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping."),zr.forEach(o),bd=h(t),Ht=s(t,"P",{});var vc=i(Ht);bE=a(vc,"The schedules are now standard "),Ho=s(vc,"A",{href:!0,rel:!0});var nw=i(Ho);wE=a(nw,"PyTorch learning rate schedulers"),nw.forEach(o),yE=a(vc," and not part of the optimizer anymore."),vc.forEach(o),wd=h(t),ue=s(t,"P",{});var Pr=i(ue);kE=a(Pr,"Here is a conversion examples from "),Pi=s(Pr,"CODE",{});var dw=i(Pi);TE=a(dw,"BertAdam"),dw.forEach(o),gE=a(Pr," with a linear warmup and decay schedule to "),Li=s(Pr,"CODE",{});var cw=i(Li);$E=a(cw,"AdamW"),cw.forEach(o),CE=a(Pr," and the same schedule:"),Pr.forEach(o),yd=h(t),m(Ro.$$.fragment,t),this.h()},h(){f(Z,"name","hf:doc:metadata"),f(Z,"content",JSON.stringify(vw)),f(F,"id","migrating-from-previous-packages"),f(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(F,"href","#migrating-from-previous-packages"),f(A,"class","relative group"),f(Ge,"id","migrating-from-transformers-v3x-to-v4x"),f(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ge,"href","#migrating-from-transformers-v3x-to-v4x"),f(ve,"class","relative group"),f(Xe,"id","1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default"),f(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xe,"href","#1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default"),f(Ee,"class","relative group"),f(Je,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Je,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(be,"class","relative group"),f(Ve,"href","main_classes/pipelines#transformers.TokenClassificationPipeline"),f(et,"id","2-sentencepiece-is-removed-from-the-required-dependencies"),f(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(et,"href","#2-sentencepiece-is-removed-from-the-required-dependencies"),f(ye,"class","relative group"),f(ot,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ot,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ke,"class","relative group"),f(lt,"id","3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder"),f(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(lt,"href","#3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder"),f(Te,"class","relative group"),f(it,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(it,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ge,"class","relative group"),f(ht,"id","4-switching-the-returndict-argument-to-true-by-default"),f(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ht,"href","#4-switching-the-returndict-argument-to-true-by-default"),f($e,"class","relative group"),f(Co,"href","main_classes/output"),f(mt,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(mt,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Oe,"class","relative group"),f(vt,"id","5-removed-some-deprecated-attributes"),f(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(vt,"href","#5-removed-some-deprecated-attributes"),f(De,"class","relative group"),f(Po,"href","https://github.com/huggingface/transformers/pull/8604"),f(Po,"rel","nofollow"),f(Ot,"id","migrating-from-pytorchtransformers-to-transformers"),f(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ot,"href","#migrating-from-pytorchtransformers-to-transformers"),f(Se,"class","relative group"),f(xt,"id","positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed"),f(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(xt,"href","#positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed"),f(Me,"class","relative group"),f(Pt,"id","migrating-from-pytorchpretrainedbert"),f(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Pt,"href","#migrating-from-pytorchpretrainedbert"),f(Fe,"class","relative group"),f(It,"id","models-always-output-tuples"),f(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(It,"href","#models-always-output-tuples"),f(Be,"class","relative group"),f(jo,"href","https://huggingface.co/transformers/"),f(jo,"rel","nofollow"),f(jt,"id","serialization"),f(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(jt,"href","#serialization"),f(qe,"class","relative group"),f(Nt,"id","optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"),f(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Nt,"href","#optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"),f(He,"class","relative group"),f(Ho,"href","https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"),f(Ho,"rel","nofollow")},m(t,n){e(document.head,Z),d(t,Zo,n),d(t,A,n),e(A,F),e(F,Lr),u(no,Lr,null),e(A,bc),e(A,Ir),e(Ir,wc),d(t,Bi,n),d(t,ve,n),e(ve,Ge),e(Ge,Sr),u(co,Sr,null),e(ve,yc),e(ve,Ue),e(Ue,kc),e(Ue,Mr),e(Mr,Tc),e(Ue,gc),e(Ue,jr),e(jr,$c),d(t,qi,n),d(t,er,n),e(er,Cc),d(t,Ni,n),d(t,Ee,n),e(Ee,Xe),e(Xe,Fr),u(ho,Fr,null),e(Ee,Oc),e(Ee,Br),e(Br,Dc),d(t,Hi,n),d(t,tr,n),e(tr,xc),d(t,Ri,n),d(t,or,n),e(or,Ac),d(t,Wi,n),d(t,Ke,n),e(Ke,qr),e(qr,zc),e(Ke,Pc),e(Ke,Nr),e(Nr,Lc),d(t,Gi,n),d(t,be,n),e(be,Je),e(Je,Hr),u(fo,Hr,null),e(be,Ic),e(be,Rr),e(Rr,Sc),d(t,Ui,n),d(t,Qe,n),e(Qe,po),e(po,Mc),e(po,Ve),e(Ve,jc),e(Ve,Wr),e(Wr,Fc),e(Ve,Bc),e(po,qc),e(Qe,Nc),e(Qe,we),e(we,Hc),e(we,Gr),e(Gr,Rc),e(we,Wc),e(we,Ur),e(Ur,Gc),e(we,Uc),d(t,Xi,n),d(t,Ye,n),e(Ye,Xc),e(Ye,Xr),e(Xr,Kc),e(Ye,Jc),d(t,Ki,n),u(mo,t,n),d(t,Ji,n),d(t,Ze,n),e(Ze,Qc),e(Ze,Kr),e(Kr,Vc),e(Ze,Yc),d(t,Qi,n),u(uo,t,n),d(t,Vi,n),d(t,ye,n),e(ye,et),e(et,Jr),u(_o,Jr,null),e(ye,Zc),e(ye,Qr),e(Qr,eh),d(t,Yi,n),d(t,z,n),e(z,th),e(z,Vr),e(Vr,oh),e(z,rh),e(z,Yr),e(Yr,ah),e(z,lh),e(z,Zr),e(Zr,sh),e(z,ih),d(t,Zi,n),d(t,tt,n),e(tt,nh),e(tt,ea),e(ea,dh),e(tt,ch),d(t,en,n),d(t,T,n),e(T,ta),e(ta,oa),e(oa,hh),e(T,fh),e(T,ra),e(ra,aa),e(aa,ph),e(T,mh),e(T,la),e(la,sa),e(sa,uh),e(T,_h),e(T,ia),e(ia,na),e(na,vh),e(T,Eh),e(T,da),e(da,ca),e(ca,bh),e(T,wh),e(T,ha),e(ha,fa),e(fa,yh),e(T,kh),e(T,pa),e(pa,ma),e(ma,Th),e(T,gh),e(T,ua),e(ua,_a),e(_a,$h),d(t,tn,n),d(t,ke,n),e(ke,ot),e(ot,va),u(vo,va,null),e(ke,Ch),e(ke,Ea),e(Ea,Oh),d(t,on,n),d(t,ee,n),e(ee,Dh),e(ee,ba),e(ba,xh),e(ee,Ah),e(ee,wa),e(wa,zh),e(ee,Ph),d(t,rn,n),d(t,rt,n),e(rt,Lh),e(rt,ya),e(ya,Ih),e(rt,Sh),d(t,an,n),u(Eo,t,n),d(t,ln,n),d(t,at,n),e(at,Mh),e(at,ka),e(ka,jh),e(at,Fh),d(t,sn,n),u(bo,t,n),d(t,nn,n),d(t,rr,n),e(rr,Bh),d(t,dn,n),u(wo,t,n),d(t,cn,n),d(t,Te,n),e(Te,lt),e(lt,Ta),u(yo,Ta,null),e(Te,qh),e(Te,ga),e(ga,Nh),d(t,hn,n),d(t,st,n),e(st,Hh),e(st,$a),e($a,Rh),e(st,Wh),d(t,fn,n),d(t,ar,n),e(ar,Gh),d(t,pn,n),d(t,ge,n),e(ge,it),e(it,Ca),u(ko,Ca,null),e(ge,Uh),e(ge,Oa),e(Oa,Xh),d(t,mn,n),d(t,nt,n),e(nt,Kh),e(nt,Da),e(Da,Jh),e(nt,Qh),d(t,un,n),d(t,dt,n),e(dt,Vh),e(dt,xa),e(xa,Yh),e(dt,Zh),d(t,_n,n),u(To,t,n),d(t,vn,n),d(t,ct,n),e(ct,ef),e(ct,Aa),e(Aa,tf),e(ct,of),d(t,En,n),u(go,t,n),d(t,bn,n),d(t,$e,n),e($e,ht),e(ht,za),u($o,za,null),e($e,rf),e($e,Ce),e(Ce,af),e(Ce,Pa),e(Pa,lf),e(Ce,sf),e(Ce,La),e(La,nf),e(Ce,df),d(t,wn,n),d(t,ft,n),e(ft,cf),e(ft,Co),e(Co,Ia),e(Ia,hf),e(Co,ff),e(ft,pf),d(t,yn,n),d(t,pt,n),e(pt,mf),e(pt,Sa),e(Sa,uf),e(pt,_f),d(t,kn,n),d(t,Oe,n),e(Oe,mt),e(mt,Ma),u(Oo,Ma,null),e(Oe,vf),e(Oe,ja),e(ja,Ef),d(t,Tn,n),d(t,P,n),e(P,bf),e(P,Fa),e(Fa,wf),e(P,yf),e(P,Ba),e(Ba,kf),e(P,Tf),e(P,qa),e(qa,gf),e(P,$f),d(t,gn,n),d(t,ut,n),e(ut,Cf),e(ut,Na),e(Na,Of),e(ut,Df),d(t,$n,n),u(Do,t,n),d(t,Cn,n),d(t,_t,n),e(_t,xf),e(_t,Ha),e(Ha,Af),e(_t,zf),d(t,On,n),u(xo,t,n),d(t,Dn,n),d(t,lr,n),e(lr,Pf),d(t,xn,n),u(Ao,t,n),d(t,An,n),d(t,De,n),e(De,vt),e(vt,Ra),u(zo,Ra,null),e(De,Lf),e(De,Wa),e(Wa,If),d(t,zn,n),d(t,Et,n),e(Et,Sf),e(Et,Po),e(Po,Mf),e(Et,jf),d(t,Pn,n),d(t,sr,n),e(sr,Ff),d(t,Ln,n),d(t,ir,n),e(ir,Bf),d(t,In,n),d(t,b,n),e(b,L),e(L,Ga),e(Ga,qf),e(L,Nf),e(L,Ua),e(Ua,Hf),e(L,Rf),e(L,Xa),e(Xa,Wf),e(L,Gf),e(L,Ka),e(Ka,Uf),e(L,Xf),e(b,Kf),e(b,I),e(I,Ja),e(Ja,Jf),e(I,Qf),e(I,Qa),e(Qa,Vf),e(I,Yf),e(I,Va),e(Va,Zf),e(I,ep),e(I,Ya),e(Ya,tp),e(I,op),e(b,rp),e(b,te),e(te,Za),e(Za,ap),e(te,lp),e(te,el),e(el,sp),e(te,ip),e(te,tl),e(tl,np),e(te,dp),e(b,cp),e(b,oe),e(oe,ol),e(ol,hp),e(oe,fp),e(oe,rl),e(rl,pp),e(oe,mp),e(oe,al),e(al,up),e(oe,_p),e(b,vp),e(b,re),e(re,ll),e(ll,Ep),e(re,bp),e(re,sl),e(sl,wp),e(re,yp),e(re,il),e(il,kp),e(re,Tp),e(b,gp),e(b,ae),e(ae,nl),e(nl,$p),e(ae,Cp),e(ae,dl),e(dl,Op),e(ae,Dp),e(ae,cl),e(cl,xp),e(ae,Ap),e(b,zp),e(b,le),e(le,hl),e(hl,Pp),e(le,Lp),e(le,fl),e(fl,Ip),e(le,Sp),e(le,pl),e(pl,Mp),e(le,jp),e(b,Fp),e(b,se),e(se,ml),e(ml,Bp),e(se,qp),e(se,ul),e(ul,Np),e(se,Hp),e(se,_l),e(_l,Rp),e(se,Wp),e(b,Gp),e(b,ie),e(ie,vl),e(vl,Up),e(ie,Xp),e(ie,El),e(El,Kp),e(ie,Jp),e(ie,bl),e(bl,Qp),e(ie,Vp),e(b,Yp),e(b,ne),e(ne,wl),e(wl,Zp),e(ne,em),e(ne,yl),e(yl,tm),e(ne,om),e(ne,kl),e(kl,rm),e(ne,am),e(b,lm),e(b,de),e(de,Tl),e(Tl,sm),e(de,im),e(de,gl),e(gl,nm),e(de,dm),e(de,$l),e($l,cm),e(de,hm),d(t,Sn,n),d(t,nr,n),e(nr,fm),d(t,Mn,n),d(t,S,n),e(S,bt),e(bt,Cl),e(Cl,pm),e(bt,mm),e(bt,Ol),e(Ol,um),e(bt,_m),e(S,vm),e(S,wt),e(wt,Dl),e(Dl,Em),e(wt,bm),e(wt,xl),e(xl,wm),e(wt,ym),e(S,km),e(S,yt),e(yt,Al),e(Al,Tm),e(yt,gm),e(yt,zl),e(zl,$m),e(yt,Cm),e(S,Om),e(S,kt),e(kt,Pl),e(Pl,Dm),e(kt,xm),e(kt,Ll),e(Ll,Am),e(kt,zm),d(t,jn,n),d(t,dr,n),e(dr,Pm),d(t,Fn,n),d(t,ce,n),e(ce,xe),e(xe,Lm),e(xe,Il),e(Il,Im),e(xe,Sm),e(xe,Sl),e(Sl,Mm),e(xe,jm),e(ce,Fm),e(ce,Ae),e(Ae,Bm),e(Ae,Ml),e(Ml,qm),e(Ae,Nm),e(Ae,jl),e(jl,Hm),e(Ae,Rm),e(ce,Wm),e(ce,ze),e(ze,Gm),e(ze,Fl),e(Fl,Um),e(ze,Xm),e(ze,Bl),e(Bl,Km),e(ze,Jm),d(t,Bn,n),d(t,Tt,n),e(Tt,Qm),e(Tt,ql),e(ql,Vm),e(Tt,Ym),d(t,qn,n),d(t,g,n),e(g,B),e(B,Zm),e(B,Nl),e(Nl,eu),e(B,tu),e(B,Hl),e(Hl,ou),e(B,ru),e(B,Rl),e(Rl,au),e(B,lu),e(g,su),e(g,q),e(q,iu),e(q,Wl),e(Wl,nu),e(q,du),e(q,Gl),e(Gl,cu),e(q,hu),e(q,Ul),e(Ul,fu),e(q,pu),e(g,mu),e(g,Pe),e(Pe,uu),e(Pe,Xl),e(Xl,_u),e(Pe,vu),e(Pe,Kl),e(Kl,Eu),e(Pe,bu),e(g,wu),e(g,N),e(N,yu),e(N,Jl),e(Jl,ku),e(N,Tu),e(N,Ql),e(Ql,gu),e(N,$u),e(N,Vl),e(Vl,Cu),e(N,Ou),e(g,Du),e(g,H),e(H,xu),e(H,Yl),e(Yl,Au),e(H,zu),e(H,Zl),e(Zl,Pu),e(H,Lu),e(H,es),e(es,Iu),e(H,Su),e(g,Mu),e(g,R),e(R,ju),e(R,ts),e(ts,Fu),e(R,Bu),e(R,os),e(os,qu),e(R,Nu),e(R,rs),e(rs,Hu),e(R,Ru),e(g,Wu),e(g,W),e(W,Gu),e(W,as),e(as,Uu),e(W,Xu),e(W,ls),e(ls,Ku),e(W,Ju),e(W,ss),e(ss,Qu),e(W,Vu),e(g,Yu),e(g,G),e(G,Zu),e(G,is),e(is,e_),e(G,t_),e(G,ns),e(ns,o_),e(G,r_),e(G,ds),e(ds,a_),e(G,l_),d(t,Nn,n),d(t,gt,n),e(gt,s_),e(gt,cs),e(cs,i_),e(gt,n_),d(t,Hn,n),d(t,D,n),e(D,U),e(U,d_),e(U,hs),e(hs,c_),e(U,h_),e(U,fs),e(fs,f_),e(U,p_),e(U,ps),e(ps,m_),e(U,u_),e(D,__),e(D,X),e(X,v_),e(X,ms),e(ms,E_),e(X,b_),e(X,us),e(us,w_),e(X,y_),e(X,_s),e(_s,k_),e(X,T_),e(D,g_),e(D,K),e(K,$_),e(K,vs),e(vs,C_),e(K,O_),e(K,Es),e(Es,D_),e(K,x_),e(K,bs),e(bs,A_),e(K,z_),e(D,P_),e(D,J),e(J,L_),e(J,ws),e(ws,I_),e(J,S_),e(J,ys),e(ys,M_),e(J,j_),e(J,ks),e(ks,F_),e(J,B_),e(D,q_),e(D,Q),e(Q,N_),e(Q,Ts),e(Ts,H_),e(Q,R_),e(Q,gs),e(gs,W_),e(Q,G_),e(Q,$s),e($s,U_),e(Q,X_),d(t,Rn,n),d(t,$t,n),e($t,K_),e($t,Cs),e(Cs,J_),e($t,Q_),d(t,Wn,n),d(t,cr,n),e(cr,V),e(V,V_),e(V,Os),e(Os,Y_),e(V,Z_),e(V,Ds),e(Ds,ev),e(V,tv),e(V,xs),e(xs,ov),e(V,rv),d(t,Gn,n),d(t,hr,n),e(hr,av),d(t,Un,n),d(t,Ct,n),e(Ct,Le),e(Le,lv),e(Le,As),e(As,sv),e(Le,iv),e(Le,zs),e(zs,nv),e(Le,dv),e(Ct,cv),e(Ct,Ie),e(Ie,hv),e(Ie,Ps),e(Ps,fv),e(Ie,pv),e(Ie,Ls),e(Ls,mv),e(Ie,uv),d(t,Xn,n),d(t,fr,n),e(fr,_v),d(t,Kn,n),d(t,pr,n),e(pr,Y),e(Y,vv),e(Y,Is),e(Is,Ev),e(Y,bv),e(Y,Ss),e(Ss,wv),e(Y,yv),e(Y,Ms),e(Ms,kv),e(Y,Tv),d(t,Jn,n),d(t,Se,n),e(Se,Ot),e(Ot,js),u(Lo,js,null),e(Se,gv),e(Se,Fs),e(Fs,$v),d(t,Qn,n),d(t,Dt,n),e(Dt,Cv),e(Dt,Bs),e(Bs,Ov),e(Dt,Dv),d(t,Vn,n),d(t,Me,n),e(Me,xt),e(xt,qs),u(Io,qs,null),e(Me,xv),e(Me,je),e(je,Av),e(je,Ns),e(Ns,zv),e(je,Pv),e(je,Hs),e(Hs,Lv),e(je,Iv),d(t,Yn,n),d(t,M,n),e(M,Sv),e(M,Rs),e(Rs,Mv),e(M,jv),e(M,Ws),e(Ws,Fv),e(M,Bv),e(M,Gs),e(Gs,qv),e(M,Nv),d(t,Zn,n),d(t,At,n),e(At,Hv),e(At,Us),e(Us,Rv),e(At,Wv),d(t,ed,n),d(t,zt,n),e(zt,Gv),e(zt,Xs),e(Xs,Uv),e(zt,Xv),d(t,td,n),d(t,Fe,n),e(Fe,Pt),e(Pt,Ks),u(So,Ks,null),e(Fe,Kv),e(Fe,Js),e(Js,Jv),d(t,od,n),d(t,Lt,n),e(Lt,Qv),e(Lt,Qs),e(Qs,Vv),e(Lt,Yv),d(t,rd,n),d(t,Be,n),e(Be,It),e(It,Vs),u(Mo,Vs,null),e(Be,Zv),e(Be,mr),e(mr,e1),e(mr,Ys),e(Ys,t1),d(t,ad,n),d(t,he,n),e(he,o1),e(he,Zs),e(Zs,r1),e(he,a1),e(he,ei),e(ei,l1),e(he,s1),d(t,ld,n),d(t,St,n),e(St,i1),e(St,jo),e(jo,n1),e(St,d1),d(t,sd,n),d(t,Mt,n),e(Mt,c1),e(Mt,ti),e(ti,h1),e(Mt,f1),d(t,id,n),d(t,fe,n),e(fe,p1),e(fe,oi),e(oi,m1),e(fe,u1),e(fe,ri),e(ri,_1),e(fe,v1),d(t,nd,n),u(Fo,t,n),d(t,dd,n),d(t,qe,n),e(qe,jt),e(jt,ai),u(Bo,ai,null),e(qe,E1),e(qe,li),e(li,b1),d(t,cd,n),d(t,Ft,n),e(Ft,w1),e(Ft,si),e(si,y1),e(Ft,k1),d(t,hd,n),d(t,Bt,n),e(Bt,ii),e(ii,Ne),e(Ne,T1),e(Ne,ni),e(ni,g1),e(Ne,$1),e(Ne,di),e(di,C1),e(Ne,O1),e(Bt,D1),e(Bt,ci),e(ci,w),e(w,x1),e(w,hi),e(hi,A1),e(w,z1),e(w,fi),e(fi,P1),e(w,L1),e(w,pi),e(pi,I1),e(w,S1),e(w,mi),e(mi,M1),e(w,j1),e(w,ui),e(ui,F1),e(w,B1),e(w,_i),e(_i,q1),e(w,N1),e(w,vi),e(vi,H1),e(w,R1),e(w,Ei),e(Ei,W1),e(w,G1),e(w,bi),e(bi,U1),e(w,X1),e(w,wi),e(wi,K1),e(w,J1),d(t,fd,n),d(t,qt,n),e(qt,Q1),e(qt,yi),e(yi,V1),e(qt,Y1),d(t,pd,n),d(t,ur,n),e(ur,Z1),d(t,md,n),u(qo,t,n),d(t,ud,n),d(t,He,n),e(He,Nt),e(Nt,ki),u(No,ki,null),e(He,eE),e(He,Ti),e(Ti,tE),d(t,_d,n),d(t,j,n),e(j,oE),e(j,gi),e(gi,rE),e(j,aE),e(j,$i),e($i,lE),e(j,sE),e(j,Ci),e(Ci,iE),e(j,nE),d(t,vd,n),d(t,pe,n),e(pe,Oi),e(Oi,dE),e(pe,cE),e(pe,Di),e(Di,hE),e(pe,fE),e(pe,xi),e(xi,pE),d(t,Ed,n),d(t,me,n),e(me,mE),e(me,Ai),e(Ai,uE),e(me,_E),e(me,zi),e(zi,vE),e(me,EE),d(t,bd,n),d(t,Ht,n),e(Ht,bE),e(Ht,Ho),e(Ho,wE),e(Ht,yE),d(t,wd,n),d(t,ue,n),e(ue,kE),e(ue,Pi),e(Pi,TE),e(ue,gE),e(ue,Li),e(Li,$E),e(ue,CE),d(t,yd,n),u(Ro,t,n),kd=!0},p:uw,i(t){kd||(_(no.$$.fragment,t),_(co.$$.fragment,t),_(ho.$$.fragment,t),_(fo.$$.fragment,t),_(mo.$$.fragment,t),_(uo.$$.fragment,t),_(_o.$$.fragment,t),_(vo.$$.fragment,t),_(Eo.$$.fragment,t),_(bo.$$.fragment,t),_(wo.$$.fragment,t),_(yo.$$.fragment,t),_(ko.$$.fragment,t),_(To.$$.fragment,t),_(go.$$.fragment,t),_($o.$$.fragment,t),_(Oo.$$.fragment,t),_(Do.$$.fragment,t),_(xo.$$.fragment,t),_(Ao.$$.fragment,t),_(zo.$$.fragment,t),_(Lo.$$.fragment,t),_(Io.$$.fragment,t),_(So.$$.fragment,t),_(Mo.$$.fragment,t),_(Fo.$$.fragment,t),_(Bo.$$.fragment,t),_(qo.$$.fragment,t),_(No.$$.fragment,t),_(Ro.$$.fragment,t),kd=!0)},o(t){v(no.$$.fragment,t),v(co.$$.fragment,t),v(ho.$$.fragment,t),v(fo.$$.fragment,t),v(mo.$$.fragment,t),v(uo.$$.fragment,t),v(_o.$$.fragment,t),v(vo.$$.fragment,t),v(Eo.$$.fragment,t),v(bo.$$.fragment,t),v(wo.$$.fragment,t),v(yo.$$.fragment,t),v(ko.$$.fragment,t),v(To.$$.fragment,t),v(go.$$.fragment,t),v($o.$$.fragment,t),v(Oo.$$.fragment,t),v(Do.$$.fragment,t),v(xo.$$.fragment,t),v(Ao.$$.fragment,t),v(zo.$$.fragment,t),v(Lo.$$.fragment,t),v(Io.$$.fragment,t),v(So.$$.fragment,t),v(Mo.$$.fragment,t),v(Fo.$$.fragment,t),v(Bo.$$.fragment,t),v(qo.$$.fragment,t),v(No.$$.fragment,t),v(Ro.$$.fragment,t),kd=!1},d(t){o(Z),t&&o(Zo),t&&o(A),E(no),t&&o(Bi),t&&o(ve),E(co),t&&o(qi),t&&o(er),t&&o(Ni),t&&o(Ee),E(ho),t&&o(Hi),t&&o(tr),t&&o(Ri),t&&o(or),t&&o(Wi),t&&o(Ke),t&&o(Gi),t&&o(be),E(fo),t&&o(Ui),t&&o(Qe),t&&o(Xi),t&&o(Ye),t&&o(Ki),E(mo,t),t&&o(Ji),t&&o(Ze),t&&o(Qi),E(uo,t),t&&o(Vi),t&&o(ye),E(_o),t&&o(Yi),t&&o(z),t&&o(Zi),t&&o(tt),t&&o(en),t&&o(T),t&&o(tn),t&&o(ke),E(vo),t&&o(on),t&&o(ee),t&&o(rn),t&&o(rt),t&&o(an),E(Eo,t),t&&o(ln),t&&o(at),t&&o(sn),E(bo,t),t&&o(nn),t&&o(rr),t&&o(dn),E(wo,t),t&&o(cn),t&&o(Te),E(yo),t&&o(hn),t&&o(st),t&&o(fn),t&&o(ar),t&&o(pn),t&&o(ge),E(ko),t&&o(mn),t&&o(nt),t&&o(un),t&&o(dt),t&&o(_n),E(To,t),t&&o(vn),t&&o(ct),t&&o(En),E(go,t),t&&o(bn),t&&o($e),E($o),t&&o(wn),t&&o(ft),t&&o(yn),t&&o(pt),t&&o(kn),t&&o(Oe),E(Oo),t&&o(Tn),t&&o(P),t&&o(gn),t&&o(ut),t&&o($n),E(Do,t),t&&o(Cn),t&&o(_t),t&&o(On),E(xo,t),t&&o(Dn),t&&o(lr),t&&o(xn),E(Ao,t),t&&o(An),t&&o(De),E(zo),t&&o(zn),t&&o(Et),t&&o(Pn),t&&o(sr),t&&o(Ln),t&&o(ir),t&&o(In),t&&o(b),t&&o(Sn),t&&o(nr),t&&o(Mn),t&&o(S),t&&o(jn),t&&o(dr),t&&o(Fn),t&&o(ce),t&&o(Bn),t&&o(Tt),t&&o(qn),t&&o(g),t&&o(Nn),t&&o(gt),t&&o(Hn),t&&o(D),t&&o(Rn),t&&o($t),t&&o(Wn),t&&o(cr),t&&o(Gn),t&&o(hr),t&&o(Un),t&&o(Ct),t&&o(Xn),t&&o(fr),t&&o(Kn),t&&o(pr),t&&o(Jn),t&&o(Se),E(Lo),t&&o(Qn),t&&o(Dt),t&&o(Vn),t&&o(Me),E(Io),t&&o(Yn),t&&o(M),t&&o(Zn),t&&o(At),t&&o(ed),t&&o(zt),t&&o(td),t&&o(Fe),E(So),t&&o(od),t&&o(Lt),t&&o(rd),t&&o(Be),E(Mo),t&&o(ad),t&&o(he),t&&o(ld),t&&o(St),t&&o(sd),t&&o(Mt),t&&o(id),t&&o(fe),t&&o(nd),E(Fo,t),t&&o(dd),t&&o(qe),E(Bo),t&&o(cd),t&&o(Ft),t&&o(hd),t&&o(Bt),t&&o(fd),t&&o(qt),t&&o(pd),t&&o(ur),t&&o(md),E(qo,t),t&&o(ud),t&&o(He),E(No),t&&o(_d),t&&o(j),t&&o(vd),t&&o(pe),t&&o(Ed),t&&o(me),t&&o(bd),t&&o(Ht),t&&o(wd),t&&o(ue),t&&o(yd),E(Ro,t)}}}const vw={local:"migrating-from-previous-packages",sections:[{local:"migrating-from-transformers-v3x-to-v4x",sections:[{local:"1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."},{local:"2-sentencepiece-is-removed-from-the-required-dependencies",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"2. SentencePiece is removed from the required dependencies"},{local:"3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"3. The architecture of the repo has been updated so that each model resides in its folder"},{local:"4-switching-the-returndict-argument-to-true-by-default",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"4. Switching the `return_dict` argument to `True` by default"},{local:"5-removed-some-deprecated-attributes",title:"5. Removed some deprecated attributes"}],title:"Migrating from transformers `v3.x` to `v4.x`"},{local:"migrating-from-pytorchtransformers-to-transformers",sections:[{local:"positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed",title:"Positional order of some models' keywords inputs (`attention_mask`, `token_type_ids`...) changed"}],title:"Migrating from pytorch-transformers to \u{1F917} Transformers"},{local:"migrating-from-pytorchpretrainedbert",sections:[{local:"models-always-output-tuples",title:"Models always output `tuples`"},{local:"serialization",title:"Serialization"},{local:"optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules",title:"Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"}],title:"Migrating from pytorch-pretrained-bert"}],title:"Migrating from previous packages"};function Ew(Ec,Z,Zo){let{fw:A}=Z;return Ec.$$set=F=>{"fw"in F&&Zo(0,A=F.fw)},[A]}class Tw extends hw{constructor(Z){super();fw(this,Z,Ew,_w,pw,{fw:0})}}export{Tw as default,vw as metadata};
