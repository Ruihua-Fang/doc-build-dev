import{S as No,i as Ro,s as Uo,e as r,k as h,w,t as a,M as Ho,c as l,d as e,m as f,a as i,x as j,h as n,b as c,F as t,g as p,y as $,q as v,o as x,B as q}from"../../chunks/vendor-4833417e.js";import{T as He}from"../../chunks/Tip-fffd6df1.js";import{Y as Bo}from"../../chunks/Youtube-27813aed.js";import{I as Nt}from"../../chunks/IconCopyLink-4b81c553.js";import{C}from"../../chunks/CodeBlock-6a3d1b46.js";import{C as Vo}from"../../chunks/CodeBlockFw-27a176a0.js";import"../../chunks/CopyButton-dacfbfaf.js";function Yo(F){let u,b,d,_,y;return{c(){u=r("p"),b=a("See the question answering "),d=r("a"),_=a("task page"),y=a(" for more information about other forms of question answering and their associated models, datasets, and metrics."),this.h()},l(m){u=l(m,"P",{});var g=i(u);b=n(g,"See the question answering "),d=l(g,"A",{href:!0,rel:!0});var E=i(d);_=n(E,"task page"),E.forEach(e),y=n(g," for more information about other forms of question answering and their associated models, datasets, and metrics."),g.forEach(e),this.h()},h(){c(d,"href","https://huggingface.co/tasks/question-answering"),c(d,"rel","nofollow")},m(m,g){p(m,u,g),t(u,b),t(u,d),t(d,_),t(u,y)},d(m){m&&e(u)}}}function Jo(F){let u,b,d,_,y,m,g,E;return{c(){u=r("p"),b=a("If you aren\u2019t familiar with fine-tuning a model with the "),d=r("a"),_=a("Trainer"),y=a(", take a look at the basic tutorial "),m=r("a"),g=a("here"),E=a("!"),this.h()},l(A){u=l(A,"P",{});var k=i(u);b=n(k,"If you aren\u2019t familiar with fine-tuning a model with the "),d=l(k,"A",{href:!0});var T=i(d);_=n(T,"Trainer"),T.forEach(e),y=n(k,", take a look at the basic tutorial "),m=l(k,"A",{href:!0});var O=i(m);g=n(O,"here"),O.forEach(e),E=n(k,"!"),k.forEach(e),this.h()},h(){c(d,"href","/docs/transformers/pr_16162/en/main_classes/trainer#transformers.Trainer"),c(m,"href","training#finetune-with-trainer")},m(A,k){p(A,u,k),t(u,b),t(u,d),t(d,_),t(u,y),t(u,m),t(m,g),t(u,E)},d(A){A&&e(u)}}}function Go(F){let u,b,d,_,y;return{c(){u=r("p"),b=a("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),d=r("a"),_=a("here"),y=a("!"),this.h()},l(m){u=l(m,"P",{});var g=i(u);b=n(g,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),d=l(g,"A",{href:!0});var E=i(d);_=n(E,"here"),E.forEach(e),y=n(g,"!"),g.forEach(e),this.h()},h(){c(d,"href","training#finetune-with-keras")},m(m,g){p(m,u,g),t(u,b),t(u,d),t(d,_),t(u,y)},d(m){m&&e(u)}}}function Ko(F){let u,b,d,_,y,m,g,E;return{c(){u=r("p"),b=a(`For a more in-depth example of how to fine-tune a model for question answering, take a look at the corresponding
`),d=r("a"),_=a("PyTorch notebook"),y=a(`
or `),m=r("a"),g=a("TensorFlow notebook"),E=a("."),this.h()},l(A){u=l(A,"P",{});var k=i(u);b=n(k,`For a more in-depth example of how to fine-tune a model for question answering, take a look at the corresponding
`),d=l(k,"A",{href:!0,rel:!0});var T=i(d);_=n(T,"PyTorch notebook"),T.forEach(e),y=n(k,`
or `),m=l(k,"A",{href:!0,rel:!0});var O=i(m);g=n(O,"TensorFlow notebook"),O.forEach(e),E=n(k,"."),k.forEach(e),this.h()},h(){c(d,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb"),c(d,"rel","nofollow"),c(m,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering-tf.ipynb"),c(m,"rel","nofollow")},m(A,k){p(A,u,k),t(u,b),t(u,d),t(d,_),t(u,y),t(u,m),t(m,g),t(u,E)},d(A){A&&e(u)}}}function Wo(F){let u,b,d,_,y,m,g,E,A,k,T,O,Bs,Ve,Rt,J,lt,Ye,Je,it,Ge,Ut,Q,Ke,cs,We,Xe,us,Ze,sa,Ht,G,Vt,R,K,pt,ds,ta,ht,ea,Yt,Ns,aa,Jt,ms,Gt,Rs,na,Kt,_s,Wt,L,oa,ft,ra,la,ct,ia,pa,Xt,U,W,ut,gs,ha,dt,fa,Zt,ws,se,M,ca,mt,ua,da,_t,ma,_a,te,js,ee,Us,ga,ae,I,P,wa,gt,ja,$a,wt,va,xa,jt,qa,ba,ka,H,ya,$t,Ea,Aa,vt,Ta,Da,za,S,Ca,$s,xt,Fa,Pa,qt,Sa,Oa,bt,Qa,La,ne,X,Ma,kt,Ia,Ba,oe,vs,re,D,Na,xs,yt,Ra,Ua,Et,Ha,Va,At,Ya,Ja,le,qs,ie,B,Ga,Hs,Ka,Wa,Tt,Xa,Za,pe,bs,he,V,Z,Dt,ks,sn,zt,tn,fe,ss,en,Vs,an,nn,ce,ys,ue,ts,de,Ys,on,me,N,Es,rn,Js,ln,pn,hn,As,fn,Gs,cn,un,dn,Ts,mn,Ks,_n,gn,_e,Ds,ge,Y,es,Ct,zs,wn,Ft,jn,we,Ws,$n,je,as,$e,z,vn,Pt,xn,qn,Cs,St,bn,kn,Ot,yn,En,ve,Fs,xe,Xs,An,qe,Ps,be,ns,Tn,Zs,Dn,zn,ke,Ss,ye,os,Cn,Os,Qt,Fn,Pn,Ee,Qs,Ae,rs,Sn,Ls,Lt,On,Qn,Te,Ms,De,ls,ze;return m=new Nt({}),T=new Bo({props:{id:"ajPx5LwJD-I"}}),G=new He({props:{$$slots:{default:[Yo]},$$scope:{ctx:F}}}),ds=new Nt({}),ms=new C({props:{code:`from datasets import load_dataset

squad = load_dataset("squad")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>squad = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>)`}}),_s=new C({props:{code:'squad["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>squad[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;answer_start&#x27;</span>: [<span class="hljs-number">515</span>], <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;Saint Bernadette Soubirous&#x27;</span>]},
 <span class="hljs-string">&#x27;context&#x27;</span>: <span class="hljs-string">&#x27;Architecturally, the school has a Catholic character. Atop the Main Building\\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;</span>,
 <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;5733be284776f41900661182&#x27;</span>,
 <span class="hljs-string">&#x27;question&#x27;</span>: <span class="hljs-string">&#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;University_of_Notre_Dame&#x27;</span>
}`}}),gs=new Nt({}),ws=new Bo({props:{id:"qgaM0weJHpA"}}),js=new C({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),vs=new C({props:{code:`def preprocess_function(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=384,
        truncation="only_second",
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    questions = [q.strip() <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;question&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = tokenizer(
<span class="hljs-meta">... </span>        questions,
<span class="hljs-meta">... </span>        examples[<span class="hljs-string">&quot;context&quot;</span>],
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">384</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
<span class="hljs-meta">... </span>        return_offsets_mapping=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-string">&quot;max_length&quot;</span>,
<span class="hljs-meta">... </span>    )

<span class="hljs-meta">... </span>    offset_mapping = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)
<span class="hljs-meta">... </span>    answers = examples[<span class="hljs-string">&quot;answers&quot;</span>]
<span class="hljs-meta">... </span>    start_positions = []
<span class="hljs-meta">... </span>    end_positions = []

<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(offset_mapping):
<span class="hljs-meta">... </span>        answer = answers[i]
<span class="hljs-meta">... </span>        start_char = answer[<span class="hljs-string">&quot;answer_start&quot;</span>][<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        end_char = answer[<span class="hljs-string">&quot;answer_start&quot;</span>][<span class="hljs-number">0</span>] + <span class="hljs-built_in">len</span>(answer[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
<span class="hljs-meta">... </span>        sequence_ids = inputs.sequence_ids(i)

<span class="hljs-meta">... </span>        <span class="hljs-comment"># Find the start and end of the context</span>
<span class="hljs-meta">... </span>        idx = <span class="hljs-number">0</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">while</span> sequence_ids[idx] != <span class="hljs-number">1</span>:
<span class="hljs-meta">... </span>            idx += <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>        context_start = idx
<span class="hljs-meta">... </span>        <span class="hljs-keyword">while</span> sequence_ids[idx] == <span class="hljs-number">1</span>:
<span class="hljs-meta">... </span>            idx += <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>        context_end = idx - <span class="hljs-number">1</span>

<span class="hljs-meta">... </span>        <span class="hljs-comment"># If the answer is not fully inside the context, label it (0, 0)</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> offset[context_start][<span class="hljs-number">0</span>] &gt; end_char <span class="hljs-keyword">or</span> offset[context_end][<span class="hljs-number">1</span>] &lt; start_char:
<span class="hljs-meta">... </span>            start_positions.append(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>            end_positions.append(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>        <span class="hljs-keyword">else</span>:
<span class="hljs-meta">... </span>            <span class="hljs-comment"># Otherwise it&#x27;s the start and end token positions</span>
<span class="hljs-meta">... </span>            idx = context_start
<span class="hljs-meta">... </span>            <span class="hljs-keyword">while</span> idx &lt;= context_end <span class="hljs-keyword">and</span> offset[idx][<span class="hljs-number">0</span>] &lt;= start_char:
<span class="hljs-meta">... </span>                idx += <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>            start_positions.append(idx - <span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>            idx = context_end
<span class="hljs-meta">... </span>            <span class="hljs-keyword">while</span> idx &gt;= context_start <span class="hljs-keyword">and</span> offset[idx][<span class="hljs-number">1</span>] &gt;= end_char:
<span class="hljs-meta">... </span>                idx -= <span class="hljs-number">1</span>
<span class="hljs-meta">... </span>            end_positions.append(idx + <span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;start_positions&quot;</span>] = start_positions
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;end_positions&quot;</span>] = end_positions
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`}}),qs=new C({props:{code:'tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_squad = squad.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>, remove_columns=squad[<span class="hljs-string">&quot;train&quot;</span>].column_names)'}}),bs=new Vo({props:{group1:{id:"pt",code:`from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator()`},group2:{id:"tf",code:`from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator(return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}}),ks=new Nt({}),ys=new C({props:{code:`from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForQuestionAnswering, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),ts=new He({props:{$$slots:{default:[Jo]},$$scope:{ctx:F}}}),Ds=new C({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_squad[<span class="hljs-string">&quot;validation&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),zs=new Nt({}),as=new He({props:{$$slots:{default:[Go]},$$scope:{ctx:F}}}),Fs=new C({props:{code:`tf_train_set = tokenized_squad["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "start_positions", "end_positions"],
    dummy_labels=True,
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_squad["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "start_positions", "end_positions"],
    dummy_labels=True,
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;start_positions&quot;</span>, <span class="hljs-string">&quot;end_positions&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = tokenized_squad[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;start_positions&quot;</span>, <span class="hljs-string">&quot;end_positions&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),Ps=new C({props:{code:`from transformers import create_optimizer

batch_size = 16
num_epochs = 2
total_train_steps = (len(tokenized_squad["train"]) // batch_size) * num_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=total_train_steps,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = (<span class="hljs-built_in">len</span>(tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size) * num_epochs
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(
<span class="hljs-meta">... </span>    init_lr=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_warmup_steps=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>    num_train_steps=total_train_steps,
<span class="hljs-meta">... </span>)`}}),Ss=new C({props:{code:`from transformers import TFAutoModelForQuestionAnswering

model = TFAutoModelForQuestionAnswering("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),Qs=new C({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),Ms=new C({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),ls=new He({props:{$$slots:{default:[Ko]},$$scope:{ctx:F}}}),{c(){u=r("meta"),b=h(),d=r("h1"),_=r("a"),y=r("span"),w(m.$$.fragment),g=h(),E=r("span"),A=a("Question answering"),k=h(),w(T.$$.fragment),O=h(),Bs=r("p"),Ve=a("Question answering tasks return an answer given a question. There are two common forms of question answering:"),Rt=h(),J=r("ul"),lt=r("li"),Ye=a("Extractive: extract the answer from the given context."),Je=h(),it=r("li"),Ge=a("Abstractive: generate an answer from the context that correctly answers the question."),Ut=h(),Q=r("p"),Ke=a("This guide will show you how to fine-tune "),cs=r("a"),We=a("DistilBERT"),Xe=a(" on the "),us=r("a"),Ze=a("SQuAD"),sa=a(" dataset for extractive question answering."),Ht=h(),w(G.$$.fragment),Vt=h(),R=r("h2"),K=r("a"),pt=r("span"),w(ds.$$.fragment),ta=h(),ht=r("span"),ea=a("Load SQuAD dataset"),Yt=h(),Ns=r("p"),aa=a("Load the SQuAD dataset from the \u{1F917} Datasets library:"),Jt=h(),w(ms.$$.fragment),Gt=h(),Rs=r("p"),na=a("Then take a look at an example:"),Kt=h(),w(_s.$$.fragment),Wt=h(),L=r("p"),oa=a("The "),ft=r("code"),ra=a("answers"),la=a(" field is a dictionary containing the starting position of the answer and the "),ct=r("code"),ia=a("text"),pa=a(" of the answer."),Xt=h(),U=r("h2"),W=r("a"),ut=r("span"),w(gs.$$.fragment),ha=h(),dt=r("span"),fa=a("Preprocess"),Zt=h(),w(ws.$$.fragment),se=h(),M=r("p"),ca=a("Load the DistilBERT tokenizer to process the "),mt=r("code"),ua=a("question"),da=a(" and "),_t=r("code"),ma=a("context"),_a=a(" fields:"),te=h(),w(js.$$.fragment),ee=h(),Us=r("p"),ga=a("There are a few preprocessing steps particular to question answering that you should be aware of:"),ae=h(),I=r("ol"),P=r("li"),wa=a("Some examples in a dataset may have a very long "),gt=r("code"),ja=a("context"),$a=a(" that exceeds the maximum input length of the model. Truncate only the "),wt=r("code"),va=a("context"),xa=a(" by setting "),jt=r("code"),qa=a('truncation="only_second"'),ba=a("."),ka=h(),H=r("li"),ya=a("Next, map the start and end positions of the answer to the original "),$t=r("code"),Ea=a("context"),Aa=a(` by setting
`),vt=r("code"),Ta=a("return_offset_mapping=True"),Da=a("."),za=h(),S=r("li"),Ca=a("With the mapping in hand, you can find the start and end tokens of the answer. Use the "),$s=r("a"),xt=r("code"),Fa=a("sequence_ids"),Pa=a(` method to
find which part of the offset corresponds to the `),qt=r("code"),Sa=a("question"),Oa=a(" and which corresponds to the "),bt=r("code"),Qa=a("context"),La=a("."),ne=h(),X=r("p"),Ma=a("Here is how you can create a function to truncate and map the start and end tokens of the answer to the "),kt=r("code"),Ia=a("context"),Ba=a(":"),oe=h(),w(vs.$$.fragment),re=h(),D=r("p"),Na=a("Use \u{1F917} Datasets "),xs=r("a"),yt=r("code"),Ra=a("map"),Ua=a(" function to apply the preprocessing function over the entire dataset. You can speed up the "),Et=r("code"),Ha=a("map"),Va=a(" function by setting "),At=r("code"),Ya=a("batched=True"),Ja=a(" to process multiple elements of the dataset at once. Remove the columns you don\u2019t need:"),le=h(),w(qs.$$.fragment),ie=h(),B=r("p"),Ga=a("Use "),Hs=r("a"),Ka=a("DefaultDataCollator"),Wa=a(" to create a batch of examples. Unlike other data collators in \u{1F917} Transformers, the "),Tt=r("code"),Xa=a("DefaultDataCollator"),Za=a(" does not apply additional preprocessing such as padding."),pe=h(),w(bs.$$.fragment),he=h(),V=r("h2"),Z=r("a"),Dt=r("span"),w(ks.$$.fragment),sn=h(),zt=r("span"),tn=a("Fine-tune with Trainer"),fe=h(),ss=r("p"),en=a("Load DistilBERT with "),Vs=r("a"),an=a("AutoModelForQuestionAnswering"),nn=a(":"),ce=h(),w(ys.$$.fragment),ue=h(),w(ts.$$.fragment),de=h(),Ys=r("p"),on=a("At this point, only three steps remain:"),me=h(),N=r("ol"),Es=r("li"),rn=a("Define your training hyperparameters in "),Js=r("a"),ln=a("TrainingArguments"),pn=a("."),hn=h(),As=r("li"),fn=a("Pass the training arguments to "),Gs=r("a"),cn=a("Trainer"),un=a(" along with the model, dataset, tokenizer, and data collator."),dn=h(),Ts=r("li"),mn=a("Call "),Ks=r("a"),_n=a("train()"),gn=a(" to fine-tune your model."),_e=h(),w(Ds.$$.fragment),ge=h(),Y=r("h2"),es=r("a"),Ct=r("span"),w(zs.$$.fragment),wn=h(),Ft=r("span"),jn=a("Fine-tune with TensorFlow"),we=h(),Ws=r("p"),$n=a("To fine-tune a model in TensorFlow is just as easy, with only a few differences."),je=h(),w(as.$$.fragment),$e=h(),z=r("p"),vn=a("Convert your datasets to the "),Pt=r("code"),xn=a("tf.data.Dataset"),qn=a(" format with "),Cs=r("a"),St=r("code"),bn=a("to_tf_dataset"),kn=a(". Specify inputs and the start and end positions of an answer in "),Ot=r("code"),yn=a("columns"),En=a(", whether to shuffle the dataset order, batch size, and the data collator:"),ve=h(),w(Fs.$$.fragment),xe=h(),Xs=r("p"),An=a("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),qe=h(),w(Ps.$$.fragment),be=h(),ns=r("p"),Tn=a("Load DistilBERT with "),Zs=r("a"),Dn=a("TFAutoModelForQuestionAnswering"),zn=a(":"),ke=h(),w(Ss.$$.fragment),ye=h(),os=r("p"),Cn=a("Configure the model for training with "),Os=r("a"),Qt=r("code"),Fn=a("compile"),Pn=a(":"),Ee=h(),w(Qs.$$.fragment),Ae=h(),rs=r("p"),Sn=a("Call "),Ls=r("a"),Lt=r("code"),On=a("fit"),Qn=a(" to fine-tune the model:"),Te=h(),w(Ms.$$.fragment),De=h(),w(ls.$$.fragment),this.h()},l(s){const o=Ho('[data-svelte="svelte-1phssyn"]',document.head);u=l(o,"META",{name:!0,content:!0}),o.forEach(e),b=f(s),d=l(s,"H1",{class:!0});var Is=i(d);_=l(Is,"A",{id:!0,class:!0,href:!0});var Mt=i(_);y=l(Mt,"SPAN",{});var It=i(y);j(m.$$.fragment,It),It.forEach(e),Mt.forEach(e),g=f(Is),E=l(Is,"SPAN",{});var Bt=i(E);A=n(Bt,"Question answering"),Bt.forEach(e),Is.forEach(e),k=f(s),j(T.$$.fragment,s),O=f(s),Bs=l(s,"P",{});var Ln=i(Bs);Ve=n(Ln,"Question answering tasks return an answer given a question. There are two common forms of question answering:"),Ln.forEach(e),Rt=f(s),J=l(s,"UL",{});var Ce=i(J);lt=l(Ce,"LI",{});var Mn=i(lt);Ye=n(Mn,"Extractive: extract the answer from the given context."),Mn.forEach(e),Je=f(Ce),it=l(Ce,"LI",{});var In=i(it);Ge=n(In,"Abstractive: generate an answer from the context that correctly answers the question."),In.forEach(e),Ce.forEach(e),Ut=f(s),Q=l(s,"P",{});var st=i(Q);Ke=n(st,"This guide will show you how to fine-tune "),cs=l(st,"A",{href:!0,rel:!0});var Bn=i(cs);We=n(Bn,"DistilBERT"),Bn.forEach(e),Xe=n(st," on the "),us=l(st,"A",{href:!0,rel:!0});var Nn=i(us);Ze=n(Nn,"SQuAD"),Nn.forEach(e),sa=n(st," dataset for extractive question answering."),st.forEach(e),Ht=f(s),j(G.$$.fragment,s),Vt=f(s),R=l(s,"H2",{class:!0});var Fe=i(R);K=l(Fe,"A",{id:!0,class:!0,href:!0});var Rn=i(K);pt=l(Rn,"SPAN",{});var Un=i(pt);j(ds.$$.fragment,Un),Un.forEach(e),Rn.forEach(e),ta=f(Fe),ht=l(Fe,"SPAN",{});var Hn=i(ht);ea=n(Hn,"Load SQuAD dataset"),Hn.forEach(e),Fe.forEach(e),Yt=f(s),Ns=l(s,"P",{});var Vn=i(Ns);aa=n(Vn,"Load the SQuAD dataset from the \u{1F917} Datasets library:"),Vn.forEach(e),Jt=f(s),j(ms.$$.fragment,s),Gt=f(s),Rs=l(s,"P",{});var Yn=i(Rs);na=n(Yn,"Then take a look at an example:"),Yn.forEach(e),Kt=f(s),j(_s.$$.fragment,s),Wt=f(s),L=l(s,"P",{});var tt=i(L);oa=n(tt,"The "),ft=l(tt,"CODE",{});var Jn=i(ft);ra=n(Jn,"answers"),Jn.forEach(e),la=n(tt," field is a dictionary containing the starting position of the answer and the "),ct=l(tt,"CODE",{});var Gn=i(ct);ia=n(Gn,"text"),Gn.forEach(e),pa=n(tt," of the answer."),tt.forEach(e),Xt=f(s),U=l(s,"H2",{class:!0});var Pe=i(U);W=l(Pe,"A",{id:!0,class:!0,href:!0});var Kn=i(W);ut=l(Kn,"SPAN",{});var Wn=i(ut);j(gs.$$.fragment,Wn),Wn.forEach(e),Kn.forEach(e),ha=f(Pe),dt=l(Pe,"SPAN",{});var Xn=i(dt);fa=n(Xn,"Preprocess"),Xn.forEach(e),Pe.forEach(e),Zt=f(s),j(ws.$$.fragment,s),se=f(s),M=l(s,"P",{});var et=i(M);ca=n(et,"Load the DistilBERT tokenizer to process the "),mt=l(et,"CODE",{});var Zn=i(mt);ua=n(Zn,"question"),Zn.forEach(e),da=n(et," and "),_t=l(et,"CODE",{});var so=i(_t);ma=n(so,"context"),so.forEach(e),_a=n(et," fields:"),et.forEach(e),te=f(s),j(js.$$.fragment,s),ee=f(s),Us=l(s,"P",{});var to=i(Us);ga=n(to,"There are a few preprocessing steps particular to question answering that you should be aware of:"),to.forEach(e),ae=f(s),I=l(s,"OL",{});var at=i(I);P=l(at,"LI",{});var is=i(P);wa=n(is,"Some examples in a dataset may have a very long "),gt=l(is,"CODE",{});var eo=i(gt);ja=n(eo,"context"),eo.forEach(e),$a=n(is," that exceeds the maximum input length of the model. Truncate only the "),wt=l(is,"CODE",{});var ao=i(wt);va=n(ao,"context"),ao.forEach(e),xa=n(is," by setting "),jt=l(is,"CODE",{});var no=i(jt);qa=n(no,'truncation="only_second"'),no.forEach(e),ba=n(is,"."),is.forEach(e),ka=f(at),H=l(at,"LI",{});var nt=i(H);ya=n(nt,"Next, map the start and end positions of the answer to the original "),$t=l(nt,"CODE",{});var oo=i($t);Ea=n(oo,"context"),oo.forEach(e),Aa=n(nt,` by setting
`),vt=l(nt,"CODE",{});var ro=i(vt);Ta=n(ro,"return_offset_mapping=True"),ro.forEach(e),Da=n(nt,"."),nt.forEach(e),za=f(at),S=l(at,"LI",{});var ps=i(S);Ca=n(ps,"With the mapping in hand, you can find the start and end tokens of the answer. Use the "),$s=l(ps,"A",{href:!0,rel:!0});var lo=i($s);xt=l(lo,"CODE",{});var io=i(xt);Fa=n(io,"sequence_ids"),io.forEach(e),lo.forEach(e),Pa=n(ps,` method to
find which part of the offset corresponds to the `),qt=l(ps,"CODE",{});var po=i(qt);Sa=n(po,"question"),po.forEach(e),Oa=n(ps," and which corresponds to the "),bt=l(ps,"CODE",{});var ho=i(bt);Qa=n(ho,"context"),ho.forEach(e),La=n(ps,"."),ps.forEach(e),at.forEach(e),ne=f(s),X=l(s,"P",{});var Se=i(X);Ma=n(Se,"Here is how you can create a function to truncate and map the start and end tokens of the answer to the "),kt=l(Se,"CODE",{});var fo=i(kt);Ia=n(fo,"context"),fo.forEach(e),Ba=n(Se,":"),Se.forEach(e),oe=f(s),j(vs.$$.fragment,s),re=f(s),D=l(s,"P",{});var hs=i(D);Na=n(hs,"Use \u{1F917} Datasets "),xs=l(hs,"A",{href:!0,rel:!0});var co=i(xs);yt=l(co,"CODE",{});var uo=i(yt);Ra=n(uo,"map"),uo.forEach(e),co.forEach(e),Ua=n(hs," function to apply the preprocessing function over the entire dataset. You can speed up the "),Et=l(hs,"CODE",{});var mo=i(Et);Ha=n(mo,"map"),mo.forEach(e),Va=n(hs," function by setting "),At=l(hs,"CODE",{});var _o=i(At);Ya=n(_o,"batched=True"),_o.forEach(e),Ja=n(hs," to process multiple elements of the dataset at once. Remove the columns you don\u2019t need:"),hs.forEach(e),le=f(s),j(qs.$$.fragment,s),ie=f(s),B=l(s,"P",{});var ot=i(B);Ga=n(ot,"Use "),Hs=l(ot,"A",{href:!0});var go=i(Hs);Ka=n(go,"DefaultDataCollator"),go.forEach(e),Wa=n(ot," to create a batch of examples. Unlike other data collators in \u{1F917} Transformers, the "),Tt=l(ot,"CODE",{});var wo=i(Tt);Xa=n(wo,"DefaultDataCollator"),wo.forEach(e),Za=n(ot," does not apply additional preprocessing such as padding."),ot.forEach(e),pe=f(s),j(bs.$$.fragment,s),he=f(s),V=l(s,"H2",{class:!0});var Oe=i(V);Z=l(Oe,"A",{id:!0,class:!0,href:!0});var jo=i(Z);Dt=l(jo,"SPAN",{});var $o=i(Dt);j(ks.$$.fragment,$o),$o.forEach(e),jo.forEach(e),sn=f(Oe),zt=l(Oe,"SPAN",{});var vo=i(zt);tn=n(vo,"Fine-tune with Trainer"),vo.forEach(e),Oe.forEach(e),fe=f(s),ss=l(s,"P",{});var Qe=i(ss);en=n(Qe,"Load DistilBERT with "),Vs=l(Qe,"A",{href:!0});var xo=i(Vs);an=n(xo,"AutoModelForQuestionAnswering"),xo.forEach(e),nn=n(Qe,":"),Qe.forEach(e),ce=f(s),j(ys.$$.fragment,s),ue=f(s),j(ts.$$.fragment,s),de=f(s),Ys=l(s,"P",{});var qo=i(Ys);on=n(qo,"At this point, only three steps remain:"),qo.forEach(e),me=f(s),N=l(s,"OL",{});var rt=i(N);Es=l(rt,"LI",{});var Le=i(Es);rn=n(Le,"Define your training hyperparameters in "),Js=l(Le,"A",{href:!0});var bo=i(Js);ln=n(bo,"TrainingArguments"),bo.forEach(e),pn=n(Le,"."),Le.forEach(e),hn=f(rt),As=l(rt,"LI",{});var Me=i(As);fn=n(Me,"Pass the training arguments to "),Gs=l(Me,"A",{href:!0});var ko=i(Gs);cn=n(ko,"Trainer"),ko.forEach(e),un=n(Me," along with the model, dataset, tokenizer, and data collator."),Me.forEach(e),dn=f(rt),Ts=l(rt,"LI",{});var Ie=i(Ts);mn=n(Ie,"Call "),Ks=l(Ie,"A",{href:!0});var yo=i(Ks);_n=n(yo,"train()"),yo.forEach(e),gn=n(Ie," to fine-tune your model."),Ie.forEach(e),rt.forEach(e),_e=f(s),j(Ds.$$.fragment,s),ge=f(s),Y=l(s,"H2",{class:!0});var Be=i(Y);es=l(Be,"A",{id:!0,class:!0,href:!0});var Eo=i(es);Ct=l(Eo,"SPAN",{});var Ao=i(Ct);j(zs.$$.fragment,Ao),Ao.forEach(e),Eo.forEach(e),wn=f(Be),Ft=l(Be,"SPAN",{});var To=i(Ft);jn=n(To,"Fine-tune with TensorFlow"),To.forEach(e),Be.forEach(e),we=f(s),Ws=l(s,"P",{});var Do=i(Ws);$n=n(Do,"To fine-tune a model in TensorFlow is just as easy, with only a few differences."),Do.forEach(e),je=f(s),j(as.$$.fragment,s),$e=f(s),z=l(s,"P",{});var fs=i(z);vn=n(fs,"Convert your datasets to the "),Pt=l(fs,"CODE",{});var zo=i(Pt);xn=n(zo,"tf.data.Dataset"),zo.forEach(e),qn=n(fs," format with "),Cs=l(fs,"A",{href:!0,rel:!0});var Co=i(Cs);St=l(Co,"CODE",{});var Fo=i(St);bn=n(Fo,"to_tf_dataset"),Fo.forEach(e),Co.forEach(e),kn=n(fs,". Specify inputs and the start and end positions of an answer in "),Ot=l(fs,"CODE",{});var Po=i(Ot);yn=n(Po,"columns"),Po.forEach(e),En=n(fs,", whether to shuffle the dataset order, batch size, and the data collator:"),fs.forEach(e),ve=f(s),j(Fs.$$.fragment,s),xe=f(s),Xs=l(s,"P",{});var So=i(Xs);An=n(So,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),So.forEach(e),qe=f(s),j(Ps.$$.fragment,s),be=f(s),ns=l(s,"P",{});var Ne=i(ns);Tn=n(Ne,"Load DistilBERT with "),Zs=l(Ne,"A",{href:!0});var Oo=i(Zs);Dn=n(Oo,"TFAutoModelForQuestionAnswering"),Oo.forEach(e),zn=n(Ne,":"),Ne.forEach(e),ke=f(s),j(Ss.$$.fragment,s),ye=f(s),os=l(s,"P",{});var Re=i(os);Cn=n(Re,"Configure the model for training with "),Os=l(Re,"A",{href:!0,rel:!0});var Qo=i(Os);Qt=l(Qo,"CODE",{});var Lo=i(Qt);Fn=n(Lo,"compile"),Lo.forEach(e),Qo.forEach(e),Pn=n(Re,":"),Re.forEach(e),Ee=f(s),j(Qs.$$.fragment,s),Ae=f(s),rs=l(s,"P",{});var Ue=i(rs);Sn=n(Ue,"Call "),Ls=l(Ue,"A",{href:!0,rel:!0});var Mo=i(Ls);Lt=l(Mo,"CODE",{});var Io=i(Lt);On=n(Io,"fit"),Io.forEach(e),Mo.forEach(e),Qn=n(Ue," to fine-tune the model:"),Ue.forEach(e),Te=f(s),j(Ms.$$.fragment,s),De=f(s),j(ls.$$.fragment,s),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(Xo)),c(_,"id","question-answering"),c(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_,"href","#question-answering"),c(d,"class","relative group"),c(cs,"href","https://huggingface.co/distilbert-base-uncased"),c(cs,"rel","nofollow"),c(us,"href","https://huggingface.co/datasets/squad"),c(us,"rel","nofollow"),c(K,"id","load-squad-dataset"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#load-squad-dataset"),c(R,"class","relative group"),c(W,"id","preprocess"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#preprocess"),c(U,"class","relative group"),c($s,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.Encoding.sequence_ids"),c($s,"rel","nofollow"),c(xs,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),c(xs,"rel","nofollow"),c(Hs,"href","/docs/transformers/pr_16162/en/main_classes/data_collator#transformers.DefaultDataCollator"),c(Z,"id","finetune-with-trainer"),c(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z,"href","#finetune-with-trainer"),c(V,"class","relative group"),c(Vs,"href","/docs/transformers/pr_16162/en/model_doc/auto#transformers.AutoModelForQuestionAnswering"),c(Js,"href","/docs/transformers/pr_16162/en/main_classes/trainer#transformers.TrainingArguments"),c(Gs,"href","/docs/transformers/pr_16162/en/main_classes/trainer#transformers.Trainer"),c(Ks,"href","/docs/transformers/pr_16162/en/main_classes/trainer#transformers.Trainer.train"),c(es,"id","finetune-with-tensorflow"),c(es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(es,"href","#finetune-with-tensorflow"),c(Y,"class","relative group"),c(Cs,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),c(Cs,"rel","nofollow"),c(Zs,"href","/docs/transformers/pr_16162/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering"),c(Os,"href","https://keras.io/api/models/model_training_apis/#compile-method"),c(Os,"rel","nofollow"),c(Ls,"href","https://keras.io/api/models/model_training_apis/#fit-method"),c(Ls,"rel","nofollow")},m(s,o){t(document.head,u),p(s,b,o),p(s,d,o),t(d,_),t(_,y),$(m,y,null),t(d,g),t(d,E),t(E,A),p(s,k,o),$(T,s,o),p(s,O,o),p(s,Bs,o),t(Bs,Ve),p(s,Rt,o),p(s,J,o),t(J,lt),t(lt,Ye),t(J,Je),t(J,it),t(it,Ge),p(s,Ut,o),p(s,Q,o),t(Q,Ke),t(Q,cs),t(cs,We),t(Q,Xe),t(Q,us),t(us,Ze),t(Q,sa),p(s,Ht,o),$(G,s,o),p(s,Vt,o),p(s,R,o),t(R,K),t(K,pt),$(ds,pt,null),t(R,ta),t(R,ht),t(ht,ea),p(s,Yt,o),p(s,Ns,o),t(Ns,aa),p(s,Jt,o),$(ms,s,o),p(s,Gt,o),p(s,Rs,o),t(Rs,na),p(s,Kt,o),$(_s,s,o),p(s,Wt,o),p(s,L,o),t(L,oa),t(L,ft),t(ft,ra),t(L,la),t(L,ct),t(ct,ia),t(L,pa),p(s,Xt,o),p(s,U,o),t(U,W),t(W,ut),$(gs,ut,null),t(U,ha),t(U,dt),t(dt,fa),p(s,Zt,o),$(ws,s,o),p(s,se,o),p(s,M,o),t(M,ca),t(M,mt),t(mt,ua),t(M,da),t(M,_t),t(_t,ma),t(M,_a),p(s,te,o),$(js,s,o),p(s,ee,o),p(s,Us,o),t(Us,ga),p(s,ae,o),p(s,I,o),t(I,P),t(P,wa),t(P,gt),t(gt,ja),t(P,$a),t(P,wt),t(wt,va),t(P,xa),t(P,jt),t(jt,qa),t(P,ba),t(I,ka),t(I,H),t(H,ya),t(H,$t),t($t,Ea),t(H,Aa),t(H,vt),t(vt,Ta),t(H,Da),t(I,za),t(I,S),t(S,Ca),t(S,$s),t($s,xt),t(xt,Fa),t(S,Pa),t(S,qt),t(qt,Sa),t(S,Oa),t(S,bt),t(bt,Qa),t(S,La),p(s,ne,o),p(s,X,o),t(X,Ma),t(X,kt),t(kt,Ia),t(X,Ba),p(s,oe,o),$(vs,s,o),p(s,re,o),p(s,D,o),t(D,Na),t(D,xs),t(xs,yt),t(yt,Ra),t(D,Ua),t(D,Et),t(Et,Ha),t(D,Va),t(D,At),t(At,Ya),t(D,Ja),p(s,le,o),$(qs,s,o),p(s,ie,o),p(s,B,o),t(B,Ga),t(B,Hs),t(Hs,Ka),t(B,Wa),t(B,Tt),t(Tt,Xa),t(B,Za),p(s,pe,o),$(bs,s,o),p(s,he,o),p(s,V,o),t(V,Z),t(Z,Dt),$(ks,Dt,null),t(V,sn),t(V,zt),t(zt,tn),p(s,fe,o),p(s,ss,o),t(ss,en),t(ss,Vs),t(Vs,an),t(ss,nn),p(s,ce,o),$(ys,s,o),p(s,ue,o),$(ts,s,o),p(s,de,o),p(s,Ys,o),t(Ys,on),p(s,me,o),p(s,N,o),t(N,Es),t(Es,rn),t(Es,Js),t(Js,ln),t(Es,pn),t(N,hn),t(N,As),t(As,fn),t(As,Gs),t(Gs,cn),t(As,un),t(N,dn),t(N,Ts),t(Ts,mn),t(Ts,Ks),t(Ks,_n),t(Ts,gn),p(s,_e,o),$(Ds,s,o),p(s,ge,o),p(s,Y,o),t(Y,es),t(es,Ct),$(zs,Ct,null),t(Y,wn),t(Y,Ft),t(Ft,jn),p(s,we,o),p(s,Ws,o),t(Ws,$n),p(s,je,o),$(as,s,o),p(s,$e,o),p(s,z,o),t(z,vn),t(z,Pt),t(Pt,xn),t(z,qn),t(z,Cs),t(Cs,St),t(St,bn),t(z,kn),t(z,Ot),t(Ot,yn),t(z,En),p(s,ve,o),$(Fs,s,o),p(s,xe,o),p(s,Xs,o),t(Xs,An),p(s,qe,o),$(Ps,s,o),p(s,be,o),p(s,ns,o),t(ns,Tn),t(ns,Zs),t(Zs,Dn),t(ns,zn),p(s,ke,o),$(Ss,s,o),p(s,ye,o),p(s,os,o),t(os,Cn),t(os,Os),t(Os,Qt),t(Qt,Fn),t(os,Pn),p(s,Ee,o),$(Qs,s,o),p(s,Ae,o),p(s,rs,o),t(rs,Sn),t(rs,Ls),t(Ls,Lt),t(Lt,On),t(rs,Qn),p(s,Te,o),$(Ms,s,o),p(s,De,o),$(ls,s,o),ze=!0},p(s,[o]){const Is={};o&2&&(Is.$$scope={dirty:o,ctx:s}),G.$set(Is);const Mt={};o&2&&(Mt.$$scope={dirty:o,ctx:s}),ts.$set(Mt);const It={};o&2&&(It.$$scope={dirty:o,ctx:s}),as.$set(It);const Bt={};o&2&&(Bt.$$scope={dirty:o,ctx:s}),ls.$set(Bt)},i(s){ze||(v(m.$$.fragment,s),v(T.$$.fragment,s),v(G.$$.fragment,s),v(ds.$$.fragment,s),v(ms.$$.fragment,s),v(_s.$$.fragment,s),v(gs.$$.fragment,s),v(ws.$$.fragment,s),v(js.$$.fragment,s),v(vs.$$.fragment,s),v(qs.$$.fragment,s),v(bs.$$.fragment,s),v(ks.$$.fragment,s),v(ys.$$.fragment,s),v(ts.$$.fragment,s),v(Ds.$$.fragment,s),v(zs.$$.fragment,s),v(as.$$.fragment,s),v(Fs.$$.fragment,s),v(Ps.$$.fragment,s),v(Ss.$$.fragment,s),v(Qs.$$.fragment,s),v(Ms.$$.fragment,s),v(ls.$$.fragment,s),ze=!0)},o(s){x(m.$$.fragment,s),x(T.$$.fragment,s),x(G.$$.fragment,s),x(ds.$$.fragment,s),x(ms.$$.fragment,s),x(_s.$$.fragment,s),x(gs.$$.fragment,s),x(ws.$$.fragment,s),x(js.$$.fragment,s),x(vs.$$.fragment,s),x(qs.$$.fragment,s),x(bs.$$.fragment,s),x(ks.$$.fragment,s),x(ys.$$.fragment,s),x(ts.$$.fragment,s),x(Ds.$$.fragment,s),x(zs.$$.fragment,s),x(as.$$.fragment,s),x(Fs.$$.fragment,s),x(Ps.$$.fragment,s),x(Ss.$$.fragment,s),x(Qs.$$.fragment,s),x(Ms.$$.fragment,s),x(ls.$$.fragment,s),ze=!1},d(s){e(u),s&&e(b),s&&e(d),q(m),s&&e(k),q(T,s),s&&e(O),s&&e(Bs),s&&e(Rt),s&&e(J),s&&e(Ut),s&&e(Q),s&&e(Ht),q(G,s),s&&e(Vt),s&&e(R),q(ds),s&&e(Yt),s&&e(Ns),s&&e(Jt),q(ms,s),s&&e(Gt),s&&e(Rs),s&&e(Kt),q(_s,s),s&&e(Wt),s&&e(L),s&&e(Xt),s&&e(U),q(gs),s&&e(Zt),q(ws,s),s&&e(se),s&&e(M),s&&e(te),q(js,s),s&&e(ee),s&&e(Us),s&&e(ae),s&&e(I),s&&e(ne),s&&e(X),s&&e(oe),q(vs,s),s&&e(re),s&&e(D),s&&e(le),q(qs,s),s&&e(ie),s&&e(B),s&&e(pe),q(bs,s),s&&e(he),s&&e(V),q(ks),s&&e(fe),s&&e(ss),s&&e(ce),q(ys,s),s&&e(ue),q(ts,s),s&&e(de),s&&e(Ys),s&&e(me),s&&e(N),s&&e(_e),q(Ds,s),s&&e(ge),s&&e(Y),q(zs),s&&e(we),s&&e(Ws),s&&e(je),q(as,s),s&&e($e),s&&e(z),s&&e(ve),q(Fs,s),s&&e(xe),s&&e(Xs),s&&e(qe),q(Ps,s),s&&e(be),s&&e(ns),s&&e(ke),q(Ss,s),s&&e(ye),s&&e(os),s&&e(Ee),q(Qs,s),s&&e(Ae),s&&e(rs),s&&e(Te),q(Ms,s),s&&e(De),q(ls,s)}}}const Xo={local:"question-answering",sections:[{local:"load-squad-dataset",title:"Load SQuAD dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-trainer",title:"Fine-tune with Trainer"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Question answering"};function Zo(F,u,b){let{fw:d}=u;return F.$$set=_=>{"fw"in _&&b(0,d=_.fw)},[d]}class lr extends No{constructor(u){super();Ro(this,u,Zo,Wo,Uo,{fw:0})}}export{lr as default,Xo as metadata};
