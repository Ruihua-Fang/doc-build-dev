import{S as Ya,i as Ja,s as Za,e as n,k as l,w as f,t as a,M as ei,c as s,d as o,m as c,a as r,x as u,h as i,b as d,F as e,g as p,y as _,q as g,o as T,B as k,v as ti}from"../../chunks/vendor-6b77c823.js";import{T as Ka}from"../../chunks/Tip-39098574.js";import{D as N}from"../../chunks/Docstring-1088f2fb.js";import{C as Ko}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ot}from"../../chunks/IconCopyLink-7a11ce68.js";function oi(nt){let m,P,v,F,C;return{c(){m=n("p"),P=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),F=a("Module"),C=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){m=s(y,"P",{});var w=r(m);P=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=s(w,"CODE",{});var A=r(v);F=i(A,"Module"),A.forEach(o),C=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(o)},m(y,w){p(y,m,w),e(m,P),e(m,v),e(v,F),e(m,C)},d(y){y&&o(m)}}}function ni(nt){let m,P,v,F,C;return{c(){m=n("p"),P=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),F=a("Module"),C=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){m=s(y,"P",{});var w=r(m);P=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=s(w,"CODE",{});var A=r(v);F=i(A,"Module"),A.forEach(o),C=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(o)},m(y,w){p(y,m,w),e(m,P),e(m,v),e(v,F),e(m,C)},d(y){y&&o(m)}}}function si(nt){let m,P,v,F,C,y,w,A,Yo,_o,U,qt,Jo,Zo,ke,en,tn,go,H,ae,Et,ve,on,xt,nn,To,ie,sn,be,rn,an,ko,st,dn,vo,rt,Ct,ln,bo,L,cn,ye,hn,pn,we,mn,fn,yo,Q,de,Pt,Me,un,It,_n,wo,at,X,gn,it,Tn,kn,dt,vn,bn,Mo,K,le,At,Fe,yn,Ot,wn,Fo,M,Se,Mn,Y,Fn,lt,Sn,$n,$e,zn,qn,En,J,xn,ct,Cn,Pn,ht,In,An,On,Dt,Dn,Nn,ze,Ln,ce,qe,jn,Z,Gn,Nt,Wn,Rn,Lt,Bn,Vn,So,ee,he,jt,Ee,Un,Gt,Hn,$o,b,xe,Qn,Wt,Xn,Kn,O,Rt,Yn,Jn,Bt,Zn,es,D,ts,Vt,os,ns,Ut,ss,rs,Ht,as,is,ds,Ce,ls,Qt,cs,hs,ps,Pe,ms,pt,fs,us,_s,j,Ie,gs,Xt,Ts,ks,Ae,mt,vs,Kt,bs,ys,ft,ws,Yt,Ms,Fs,pe,Oe,Ss,De,$s,Jt,zs,qs,Es,q,Ne,xs,Zt,Cs,Ps,Le,Is,te,As,eo,Os,Ds,to,Ns,Ls,js,oo,Gs,Ws,ut,je,zo,oe,me,no,Ge,Rs,so,Bs,qo,$,We,Vs,ro,Us,Hs,Re,Qs,_t,Xs,Ks,Ys,Be,Js,Ve,Zs,er,tr,E,Ue,or,ne,nr,gt,sr,rr,ao,ar,ir,dr,fe,lr,io,cr,hr,He,Eo,se,ue,lo,Qe,pr,co,mr,xo,z,Xe,fr,ho,ur,_r,Ke,gr,Tt,Tr,kr,vr,Ye,br,Je,yr,wr,Mr,x,Ze,Fr,re,Sr,kt,$r,zr,po,qr,Er,xr,_e,Cr,mo,Pr,Ir,et,Co;return y=new ot({}),ve=new ot({}),Me=new ot({}),Fe=new ot({}),Se=new N({props:{name:"class transformers.FSMTConfig",anchor:"transformers.FSMTConfig",parameters:[{name:"langs",val:" = ['en', 'de']"},{name:"src_vocab_size",val:" = 42024"},{name:"tgt_vocab_size",val:" = 42024"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 1024"},{name:"max_length",val:" = 200"},{name:"max_position_embeddings",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"encoder_layers",val:" = 12"},{name:"encoder_attention_heads",val:" = 16"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"decoder_layers",val:" = 12"},{name:"decoder_attention_heads",val:" = 16"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"is_encoder_decoder",val:" = True"},{name:"scale_embedding",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"num_beams",val:" = 5"},{name:"length_penalty",val:" = 1.0"},{name:"early_stopping",val:" = False"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"forced_eos_token_id",val:" = 2"},{name:"**common_kwargs",val:""}],parametersDescription:[{anchor:"transformers.FSMTConfig.langs",description:`<strong>langs</strong> (<code>List[str]</code>) &#x2014;
A list with source language and target_language (e.g., [&#x2018;en&#x2019;, &#x2018;ru&#x2019;]).`,name:"langs"},{anchor:"transformers.FSMTConfig.src_vocab_size",description:`<strong>src_vocab_size</strong> (<code>int</code>) &#x2014;
Vocabulary size of the encoder. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed to the forward method in the encoder.`,name:"src_vocab_size"},{anchor:"transformers.FSMTConfig.tgt_vocab_size",description:`<strong>tgt_vocab_size</strong> (<code>int</code>) &#x2014;
Vocabulary size of the decoder. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed to the forward method in the decoder.`,name:"tgt_vocab_size"},{anchor:"transformers.FSMTConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.FSMTConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.FSMTConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.FSMTConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FSMTConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FSMTConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.FSMTConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.FSMTConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.FSMTConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.FSMTConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FSMTConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.FSMTConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FSMTConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.FSMTConfig.scale_embedding",description:`<strong>scale_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Scale embeddings by diving by sqrt(d_model).`,name:"scale_embedding"},{anchor:"transformers.FSMTConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.FSMTConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.FSMTConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.FSMTConfig.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
This model starts decoding with <code>eos_token_id</code>
encoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
Google &#x201C;layerdrop arxiv&#x201D;, as its not explainable in one line.
decoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
Google &#x201C;layerdrop arxiv&#x201D;, as its not explainable in one line.`,name:"decoder_start_token_id"},{anchor:"transformers.FSMTConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.FSMTConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie input and output embeddings.`,name:"tie_word_embeddings"},{anchor:"transformers.FSMTConfig.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Number of beams for beam search that will be used by default in the <code>generate</code> method of the model. 1 means
no beam search.`,name:"num_beams"},{anchor:"transformers.FSMTConfig.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
Exponential penalty to the length that will be used by default in the <code>generate</code> method of the model.`,name:"length_penalty"},{anchor:"transformers.FSMTConfig.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Flag that will be used by default in the <code>generate</code> method of the model. Whether to stop the beam search
when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.FSMTConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.FSMTConfig.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached. Usually set to
<code>eos_token_id</code>.`,name:"forced_eos_token_id"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/configuration_fsmt.py#L41"}}),ze=new Ko({props:{code:`from transformers import FSMTConfig, FSMTModel

config = FSMTConfig.from_pretrained("facebook/wmt19-en-ru")
model = FSMTModel(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTConfig, FSMTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>config = FSMTConfig.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-en-ru&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTModel(config)`}}),qe=new N({props:{name:"to_dict",anchor:"transformers.FSMTConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/configuration_fsmt.py#L211",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Ee=new ot({}),xe=new N({props:{name:"class transformers.FSMTTokenizer",anchor:"transformers.FSMTTokenizer",parameters:[{name:"langs",val:" = None"},{name:"src_vocab_file",val:" = None"},{name:"tgt_vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"sep_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.langs",description:`<strong>langs</strong> (<code>List[str]</code>) &#x2014;
A list of two languages to translate from and to, for instance <code>[&quot;en&quot;, &quot;ru&quot;]</code>.`,name:"langs"},{anchor:"transformers.FSMTTokenizer.src_vocab_file",description:`<strong>src_vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary for the source language.`,name:"src_vocab_file"},{anchor:"transformers.FSMTTokenizer.tgt_vocab_file",description:`<strong>tgt_vocab_file</strong> (<code>st</code>) &#x2014;
File containing the vocabulary for the target language.`,name:"tgt_vocab_file"},{anchor:"transformers.FSMTTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
File containing the merges.`,name:"merges_file"},{anchor:"transformers.FSMTTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.FSMTTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.FSMTTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.FSMTTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.FSMTTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/tokenization_fsmt.py#L137"}}),Ie=new N({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/tokenization_fsmt.py#L397",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Oe=new N({props:{name:"get_special_tokens_mask",anchor:"transformers.FSMTTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/tokenization_fsmt.py#L423",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ne=new N({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/tokenization_fsmt.py#L451",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Le=new Ko({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),je=new N({props:{name:"save_vocabulary",anchor:"transformers.FSMTTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/tokenization_fsmt.py#L484"}}),Ge=new ot({}),We=new N({props:{name:"class transformers.FSMTModel",anchor:"transformers.FSMTModel",parameters:[{name:"config",val:": FSMTConfig"}],parametersDescription:[{anchor:"transformers.FSMTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16937/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/modeling_fsmt.py#L989"}}),Ue=new N({props:{name:"forward",anchor:"transformers.FSMTModel.forward",parameters:[{name:"input_ids",val:": LongTensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FSMTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>IIndices can be obtained using <code>FSTMTokenizer</code>. See <a href="/docs/transformers/pr_16937/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16937/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FSMTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FSMTModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a>. See <a href="/docs/transformers/pr_16937/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16937/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>FSMT uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.FSMTModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FSMTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FSMTModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.FSMTModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.FSMTModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code> is a sequence of hidden-states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.FSMTModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.FSMTModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.FSMTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FSMTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FSMTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16937/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/modeling_fsmt.py#L1003",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16937/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTConfig"
>FSMTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16937/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new Ka({props:{$$slots:{default:[oi]},$$scope:{ctx:nt}}}),He=new Ko({props:{code:`from transformers import FSMTTokenizer, FSMTModel
import torch

tokenizer = FSMTTokenizer.from_pretrained("facebook/wmt19-ru-en")
model = FSMTModel.from_pretrained("facebook/wmt19-ru-en")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTTokenizer, FSMTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FSMTTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTModel.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Qe=new ot({}),Xe=new N({props:{name:"class transformers.FSMTForConditionalGeneration",anchor:"transformers.FSMTForConditionalGeneration",parameters:[{name:"config",val:": FSMTConfig"}],parametersDescription:[{anchor:"transformers.FSMTForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16937/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/modeling_fsmt.py#L1113"}}),Ze=new N({props:{name:"forward",anchor:"transformers.FSMTForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": LongTensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FSMTForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>IIndices can be obtained using <code>FSTMTokenizer</code>. See <a href="/docs/transformers/pr_16937/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16937/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FSMTForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a>. See <a href="/docs/transformers/pr_16937/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16937/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>FSMT uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code> is a sequence of hidden-states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.FSMTForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.FSMTForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.FSMTForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FSMTForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FSMTForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16937/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FSMTForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/pr_16937/src/transformers/models/fsmt/modeling_fsmt.py#L1129",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16937/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTConfig"
>FSMTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16937/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),_e=new Ka({props:{$$slots:{default:[ni]},$$scope:{ctx:nt}}}),et=new Ko({props:{code:`from transformers import FSMTTokenizer, FSMTForConditionalGeneration

mname = "facebook/wmt19-ru-en"
model = FSMTForConditionalGeneration.from_pretrained(mname)
tokenizer = FSMTTokenizer.from_pretrained(mname)

src_text = "\u041C\u0430\u0448\u0438\u043D\u043D\u043E\u0435 \u043E\u0431\u0443\u0447\u0435\u043D\u0438\u0435 - \u044D\u0442\u043E \u0437\u0434\u043E\u0440\u043E\u0432\u043E, \u043D\u0435 \u0442\u0430\u043A \u043B\u0438?"
input_ids = tokenizer(src_text, return_tensors="pt").input_ids
outputs = model.generate(input_ids, num_beams=5, num_return_sequences=3)
tokenizer.decode(outputs[0], skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTTokenizer, FSMTForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>mname = <span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTForConditionalGeneration.from_pretrained(mname)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FSMTTokenizer.from_pretrained(mname)

<span class="hljs-meta">&gt;&gt;&gt; </span>src_text = <span class="hljs-string">&quot;\u041C\u0430\u0448\u0438\u043D\u043D\u043E\u0435 \u043E\u0431\u0443\u0447\u0435\u043D\u0438\u0435 - \u044D\u0442\u043E \u0437\u0434\u043E\u0440\u043E\u0432\u043E, \u043D\u0435 \u0442\u0430\u043A \u043B\u0438?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(src_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&quot;Machine learning is great, isn&#x27;t it?&quot;</span>`}}),{c(){m=n("meta"),P=l(),v=n("h1"),F=n("a"),C=n("span"),f(y.$$.fragment),w=l(),A=n("span"),Yo=a("FSMT"),_o=l(),U=n("p"),qt=n("strong"),Jo=a("DISCLAIMER:"),Zo=a(" If you see something strange, file a "),ke=n("a"),en=a("Github Issue"),tn=a(` and assign
@stas00.`),go=l(),H=n("h2"),ae=n("a"),Et=n("span"),f(ve.$$.fragment),on=l(),xt=n("span"),nn=a("Overview"),To=l(),ie=n("p"),sn=a("FSMT (FairSeq MachineTranslation) models were introduced in "),be=n("a"),rn=a("Facebook FAIR\u2019s WMT19 News Translation Task Submission"),an=a(" by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov."),ko=l(),st=n("p"),dn=a("The abstract of the paper is the following:"),vo=l(),rt=n("p"),Ct=n("em"),ln=a(`This paper describes Facebook FAIR\u2019s submission to the WMT19 shared news translation task. We participate in two
language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from
last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling
toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,
as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific
data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the
human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.
This system improves upon our WMT\u201918 submission by 4.5 BLEU points.`),bo=l(),L=n("p"),cn=a("This model was contributed by "),ye=n("a"),hn=a("stas"),pn=a(`. The original code can be found
`),we=n("a"),mn=a("here"),fn=a("."),yo=l(),Q=n("h2"),de=n("a"),Pt=n("span"),f(Me.$$.fragment),un=l(),It=n("span"),_n=a("Implementation Notes"),wo=l(),at=n("ul"),X=n("li"),gn=a(`FSMT uses source and target vocabulary pairs that aren\u2019t combined into one. It doesn\u2019t share embeddings tokens
either. Its tokenizer is very similar to `),it=n("a"),Tn=a("XLMTokenizer"),kn=a(` and the main model is derived from
`),dt=n("a"),vn=a("BartModel"),bn=a("."),Mo=l(),K=n("h2"),le=n("a"),At=n("span"),f(Fe.$$.fragment),yn=l(),Ot=n("span"),wn=a("FSMTConfig"),Fo=l(),M=n("div"),f(Se.$$.fragment),Mn=l(),Y=n("p"),Fn=a("This is the configuration class to store the configuration of a "),lt=n("a"),Sn=a("FSMTModel"),$n=a(`. It is used to instantiate a FSMT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the FSMT
`),$e=n("a"),zn=a("facebook/wmt19-en-ru"),qn=a(" architecture."),En=l(),J=n("p"),xn=a("Configuration objects inherit from "),ct=n("a"),Cn=a("PretrainedConfig"),Pn=a(` and can be used to control the model outputs. Read the
documentation from `),ht=n("a"),In=a("PretrainedConfig"),An=a(" for more information."),On=l(),Dt=n("p"),Dn=a("Examples:"),Nn=l(),f(ze.$$.fragment),Ln=l(),ce=n("div"),f(qe.$$.fragment),jn=l(),Z=n("p"),Gn=a("Serializes this instance to a Python dictionary. Override the default "),Nt=n("em"),Wn=a("to_dict()"),Rn=a(" from "),Lt=n("em"),Bn=a("PretrainedConfig"),Vn=a("."),So=l(),ee=n("h2"),he=n("a"),jt=n("span"),f(Ee.$$.fragment),Un=l(),Gt=n("span"),Hn=a("FSMTTokenizer"),$o=l(),b=n("div"),f(xe.$$.fragment),Qn=l(),Wt=n("p"),Xn=a("Construct an FAIRSEQ Transformer tokenizer. Based on Byte-Pair Encoding. The tokenization process is the following:"),Kn=l(),O=n("ul"),Rt=n("li"),Yn=a("Moses preprocessing and tokenization."),Jn=l(),Bt=n("li"),Zn=a("Normalizing all inputs text."),es=l(),D=n("li"),ts=a("The arguments "),Vt=n("code"),os=a("special_tokens"),ns=a(" and the function "),Ut=n("code"),ss=a("set_special_tokens"),rs=a(`, can be used to add additional symbols (like
\u201D`),Ht=n("strong"),as=a("classify"),is=a("\u201D) to a vocabulary."),ds=l(),Ce=n("li"),ls=a("The argument "),Qt=n("code"),cs=a("langs"),hs=a(" defines a pair of languages."),ps=l(),Pe=n("p"),ms=a("This tokenizer inherits from "),pt=n("a"),fs=a("PreTrainedTokenizer"),us=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),_s=l(),j=n("div"),f(Ie.$$.fragment),gs=l(),Xt=n("p"),Ts=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A FAIRSEQ Transformer sequence has the following format:`),ks=l(),Ae=n("ul"),mt=n("li"),vs=a("single sequence: "),Kt=n("code"),bs=a("<s> X </s>"),ys=l(),ft=n("li"),ws=a("pair of sequences: "),Yt=n("code"),Ms=a("<s> A </s> B </s>"),Fs=l(),pe=n("div"),f(Oe.$$.fragment),Ss=l(),De=n("p"),$s=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Jt=n("code"),zs=a("prepare_for_model"),qs=a(" method."),Es=l(),q=n("div"),f(Ne.$$.fragment),xs=l(),Zt=n("p"),Cs=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ
Transformer sequence pair mask has the following format:`),Ps=l(),f(Le.$$.fragment),Is=l(),te=n("p"),As=a("If "),eo=n("code"),Os=a("token_ids_1"),Ds=a(" is "),to=n("code"),Ns=a("None"),Ls=a(", this method only returns the first portion of the mask (0s)."),js=l(),oo=n("p"),Gs=a(`Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An
FAIRSEQ_TRANSFORMER sequence pair mask has the following format:`),Ws=l(),ut=n("div"),f(je.$$.fragment),zo=l(),oe=n("h2"),me=n("a"),no=n("span"),f(Ge.$$.fragment),Rs=l(),so=n("span"),Bs=a("FSMTModel"),qo=l(),$=n("div"),f(We.$$.fragment),Vs=l(),ro=n("p"),Us=a("The bare FSMT Model outputting raw hidden-states without any specific head on top."),Hs=l(),Re=n("p"),Qs=a("This model inherits from "),_t=n("a"),Xs=a("PreTrainedModel"),Ks=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ys=l(),Be=n("p"),Js=a("This model is also a PyTorch "),Ve=n("a"),Zs=a("torch.nn.Module"),er=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),tr=l(),E=n("div"),f(Ue.$$.fragment),or=l(),ne=n("p"),nr=a("The "),gt=n("a"),sr=a("FSMTModel"),rr=a(" forward method, overrides the "),ao=n("code"),ar=a("__call__"),ir=a(" special method."),dr=l(),f(fe.$$.fragment),lr=l(),io=n("p"),cr=a("Example:"),hr=l(),f(He.$$.fragment),Eo=l(),se=n("h2"),ue=n("a"),lo=n("span"),f(Qe.$$.fragment),pr=l(),co=n("span"),mr=a("FSMTForConditionalGeneration"),xo=l(),z=n("div"),f(Xe.$$.fragment),fr=l(),ho=n("p"),ur=a("The FSMT Model with a language modeling head. Can be used for summarization."),_r=l(),Ke=n("p"),gr=a("This model inherits from "),Tt=n("a"),Tr=a("PreTrainedModel"),kr=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vr=l(),Ye=n("p"),br=a("This model is also a PyTorch "),Je=n("a"),yr=a("torch.nn.Module"),wr=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Mr=l(),x=n("div"),f(Ze.$$.fragment),Fr=l(),re=n("p"),Sr=a("The "),kt=n("a"),$r=a("FSMTForConditionalGeneration"),zr=a(" forward method, overrides the "),po=n("code"),qr=a("__call__"),Er=a(" special method."),xr=l(),f(_e.$$.fragment),Cr=l(),mo=n("p"),Pr=a("Translation example::"),Ir=l(),f(et.$$.fragment),this.h()},l(t){const h=ei('[data-svelte="svelte-1phssyn"]',document.head);m=s(h,"META",{name:!0,content:!0}),h.forEach(o),P=c(t),v=s(t,"H1",{class:!0});var tt=r(v);F=s(tt,"A",{id:!0,class:!0,href:!0});var fo=r(F);C=s(fo,"SPAN",{});var Dr=r(C);u(y.$$.fragment,Dr),Dr.forEach(o),fo.forEach(o),w=c(tt),A=s(tt,"SPAN",{});var Nr=r(A);Yo=i(Nr,"FSMT"),Nr.forEach(o),tt.forEach(o),_o=c(t),U=s(t,"P",{});var uo=r(U);qt=s(uo,"STRONG",{});var Lr=r(qt);Jo=i(Lr,"DISCLAIMER:"),Lr.forEach(o),Zo=i(uo," If you see something strange, file a "),ke=s(uo,"A",{href:!0,rel:!0});var jr=r(ke);en=i(jr,"Github Issue"),jr.forEach(o),tn=i(uo,` and assign
@stas00.`),uo.forEach(o),go=c(t),H=s(t,"H2",{class:!0});var Po=r(H);ae=s(Po,"A",{id:!0,class:!0,href:!0});var Gr=r(ae);Et=s(Gr,"SPAN",{});var Wr=r(Et);u(ve.$$.fragment,Wr),Wr.forEach(o),Gr.forEach(o),on=c(Po),xt=s(Po,"SPAN",{});var Rr=r(xt);nn=i(Rr,"Overview"),Rr.forEach(o),Po.forEach(o),To=c(t),ie=s(t,"P",{});var Io=r(ie);sn=i(Io,"FSMT (FairSeq MachineTranslation) models were introduced in "),be=s(Io,"A",{href:!0,rel:!0});var Br=r(be);rn=i(Br,"Facebook FAIR\u2019s WMT19 News Translation Task Submission"),Br.forEach(o),an=i(Io," by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov."),Io.forEach(o),ko=c(t),st=s(t,"P",{});var Vr=r(st);dn=i(Vr,"The abstract of the paper is the following:"),Vr.forEach(o),vo=c(t),rt=s(t,"P",{});var Ur=r(rt);Ct=s(Ur,"EM",{});var Hr=r(Ct);ln=i(Hr,`This paper describes Facebook FAIR\u2019s submission to the WMT19 shared news translation task. We participate in two
language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from
last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling
toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,
as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific
data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the
human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.
This system improves upon our WMT\u201918 submission by 4.5 BLEU points.`),Hr.forEach(o),Ur.forEach(o),bo=c(t),L=s(t,"P",{});var vt=r(L);cn=i(vt,"This model was contributed by "),ye=s(vt,"A",{href:!0,rel:!0});var Qr=r(ye);hn=i(Qr,"stas"),Qr.forEach(o),pn=i(vt,`. The original code can be found
`),we=s(vt,"A",{href:!0,rel:!0});var Xr=r(we);mn=i(Xr,"here"),Xr.forEach(o),fn=i(vt,"."),vt.forEach(o),yo=c(t),Q=s(t,"H2",{class:!0});var Ao=r(Q);de=s(Ao,"A",{id:!0,class:!0,href:!0});var Kr=r(de);Pt=s(Kr,"SPAN",{});var Yr=r(Pt);u(Me.$$.fragment,Yr),Yr.forEach(o),Kr.forEach(o),un=c(Ao),It=s(Ao,"SPAN",{});var Jr=r(It);_n=i(Jr,"Implementation Notes"),Jr.forEach(o),Ao.forEach(o),wo=c(t),at=s(t,"UL",{});var Zr=r(at);X=s(Zr,"LI",{});var bt=r(X);gn=i(bt,`FSMT uses source and target vocabulary pairs that aren\u2019t combined into one. It doesn\u2019t share embeddings tokens
either. Its tokenizer is very similar to `),it=s(bt,"A",{href:!0});var ea=r(it);Tn=i(ea,"XLMTokenizer"),ea.forEach(o),kn=i(bt,` and the main model is derived from
`),dt=s(bt,"A",{href:!0});var ta=r(dt);vn=i(ta,"BartModel"),ta.forEach(o),bn=i(bt,"."),bt.forEach(o),Zr.forEach(o),Mo=c(t),K=s(t,"H2",{class:!0});var Oo=r(K);le=s(Oo,"A",{id:!0,class:!0,href:!0});var oa=r(le);At=s(oa,"SPAN",{});var na=r(At);u(Fe.$$.fragment,na),na.forEach(o),oa.forEach(o),yn=c(Oo),Ot=s(Oo,"SPAN",{});var sa=r(Ot);wn=i(sa,"FSMTConfig"),sa.forEach(o),Oo.forEach(o),Fo=c(t),M=s(t,"DIV",{class:!0});var I=r(M);u(Se.$$.fragment,I),Mn=c(I),Y=s(I,"P",{});var yt=r(Y);Fn=i(yt,"This is the configuration class to store the configuration of a "),lt=s(yt,"A",{href:!0});var ra=r(lt);Sn=i(ra,"FSMTModel"),ra.forEach(o),$n=i(yt,`. It is used to instantiate a FSMT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the FSMT
`),$e=s(yt,"A",{href:!0,rel:!0});var aa=r($e);zn=i(aa,"facebook/wmt19-en-ru"),aa.forEach(o),qn=i(yt," architecture."),yt.forEach(o),En=c(I),J=s(I,"P",{});var wt=r(J);xn=i(wt,"Configuration objects inherit from "),ct=s(wt,"A",{href:!0});var ia=r(ct);Cn=i(ia,"PretrainedConfig"),ia.forEach(o),Pn=i(wt,` and can be used to control the model outputs. Read the
documentation from `),ht=s(wt,"A",{href:!0});var da=r(ht);In=i(da,"PretrainedConfig"),da.forEach(o),An=i(wt," for more information."),wt.forEach(o),On=c(I),Dt=s(I,"P",{});var la=r(Dt);Dn=i(la,"Examples:"),la.forEach(o),Nn=c(I),u(ze.$$.fragment,I),Ln=c(I),ce=s(I,"DIV",{class:!0});var Do=r(ce);u(qe.$$.fragment,Do),jn=c(Do),Z=s(Do,"P",{});var Mt=r(Z);Gn=i(Mt,"Serializes this instance to a Python dictionary. Override the default "),Nt=s(Mt,"EM",{});var ca=r(Nt);Wn=i(ca,"to_dict()"),ca.forEach(o),Rn=i(Mt," from "),Lt=s(Mt,"EM",{});var ha=r(Lt);Bn=i(ha,"PretrainedConfig"),ha.forEach(o),Vn=i(Mt,"."),Mt.forEach(o),Do.forEach(o),I.forEach(o),So=c(t),ee=s(t,"H2",{class:!0});var No=r(ee);he=s(No,"A",{id:!0,class:!0,href:!0});var pa=r(he);jt=s(pa,"SPAN",{});var ma=r(jt);u(Ee.$$.fragment,ma),ma.forEach(o),pa.forEach(o),Un=c(No),Gt=s(No,"SPAN",{});var fa=r(Gt);Hn=i(fa,"FSMTTokenizer"),fa.forEach(o),No.forEach(o),$o=c(t),b=s(t,"DIV",{class:!0});var S=r(b);u(xe.$$.fragment,S),Qn=c(S),Wt=s(S,"P",{});var ua=r(Wt);Xn=i(ua,"Construct an FAIRSEQ Transformer tokenizer. Based on Byte-Pair Encoding. The tokenization process is the following:"),ua.forEach(o),Kn=c(S),O=s(S,"UL",{});var ge=r(O);Rt=s(ge,"LI",{});var _a=r(Rt);Yn=i(_a,"Moses preprocessing and tokenization."),_a.forEach(o),Jn=c(ge),Bt=s(ge,"LI",{});var ga=r(Bt);Zn=i(ga,"Normalizing all inputs text."),ga.forEach(o),es=c(ge),D=s(ge,"LI",{});var Te=r(D);ts=i(Te,"The arguments "),Vt=s(Te,"CODE",{});var Ta=r(Vt);os=i(Ta,"special_tokens"),Ta.forEach(o),ns=i(Te," and the function "),Ut=s(Te,"CODE",{});var ka=r(Ut);ss=i(ka,"set_special_tokens"),ka.forEach(o),rs=i(Te,`, can be used to add additional symbols (like
\u201D`),Ht=s(Te,"STRONG",{});var va=r(Ht);as=i(va,"classify"),va.forEach(o),is=i(Te,"\u201D) to a vocabulary."),Te.forEach(o),ds=c(ge),Ce=s(ge,"LI",{});var Lo=r(Ce);ls=i(Lo,"The argument "),Qt=s(Lo,"CODE",{});var ba=r(Qt);cs=i(ba,"langs"),ba.forEach(o),hs=i(Lo," defines a pair of languages."),Lo.forEach(o),ge.forEach(o),ps=c(S),Pe=s(S,"P",{});var jo=r(Pe);ms=i(jo,"This tokenizer inherits from "),pt=s(jo,"A",{href:!0});var ya=r(pt);fs=i(ya,"PreTrainedTokenizer"),ya.forEach(o),us=i(jo,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),jo.forEach(o),_s=c(S),j=s(S,"DIV",{class:!0});var Ft=r(j);u(Ie.$$.fragment,Ft),gs=c(Ft),Xt=s(Ft,"P",{});var wa=r(Xt);Ts=i(wa,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A FAIRSEQ Transformer sequence has the following format:`),wa.forEach(o),ks=c(Ft),Ae=s(Ft,"UL",{});var Go=r(Ae);mt=s(Go,"LI",{});var Ar=r(mt);vs=i(Ar,"single sequence: "),Kt=s(Ar,"CODE",{});var Ma=r(Kt);bs=i(Ma,"<s> X </s>"),Ma.forEach(o),Ar.forEach(o),ys=c(Go),ft=s(Go,"LI",{});var Or=r(ft);ws=i(Or,"pair of sequences: "),Yt=s(Or,"CODE",{});var Fa=r(Yt);Ms=i(Fa,"<s> A </s> B </s>"),Fa.forEach(o),Or.forEach(o),Go.forEach(o),Ft.forEach(o),Fs=c(S),pe=s(S,"DIV",{class:!0});var Wo=r(pe);u(Oe.$$.fragment,Wo),Ss=c(Wo),De=s(Wo,"P",{});var Ro=r(De);$s=i(Ro,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Jt=s(Ro,"CODE",{});var Sa=r(Jt);zs=i(Sa,"prepare_for_model"),Sa.forEach(o),qs=i(Ro," method."),Ro.forEach(o),Wo.forEach(o),Es=c(S),q=s(S,"DIV",{class:!0});var G=r(q);u(Ne.$$.fragment,G),xs=c(G),Zt=s(G,"P",{});var $a=r(Zt);Cs=i($a,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ
Transformer sequence pair mask has the following format:`),$a.forEach(o),Ps=c(G),u(Le.$$.fragment,G),Is=c(G),te=s(G,"P",{});var St=r(te);As=i(St,"If "),eo=s(St,"CODE",{});var za=r(eo);Os=i(za,"token_ids_1"),za.forEach(o),Ds=i(St," is "),to=s(St,"CODE",{});var qa=r(to);Ns=i(qa,"None"),qa.forEach(o),Ls=i(St,", this method only returns the first portion of the mask (0s)."),St.forEach(o),js=c(G),oo=s(G,"P",{});var Ea=r(oo);Gs=i(Ea,`Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An
FAIRSEQ_TRANSFORMER sequence pair mask has the following format:`),Ea.forEach(o),G.forEach(o),Ws=c(S),ut=s(S,"DIV",{class:!0});var xa=r(ut);u(je.$$.fragment,xa),xa.forEach(o),S.forEach(o),zo=c(t),oe=s(t,"H2",{class:!0});var Bo=r(oe);me=s(Bo,"A",{id:!0,class:!0,href:!0});var Ca=r(me);no=s(Ca,"SPAN",{});var Pa=r(no);u(Ge.$$.fragment,Pa),Pa.forEach(o),Ca.forEach(o),Rs=c(Bo),so=s(Bo,"SPAN",{});var Ia=r(so);Bs=i(Ia,"FSMTModel"),Ia.forEach(o),Bo.forEach(o),qo=c(t),$=s(t,"DIV",{class:!0});var W=r($);u(We.$$.fragment,W),Vs=c(W),ro=s(W,"P",{});var Aa=r(ro);Us=i(Aa,"The bare FSMT Model outputting raw hidden-states without any specific head on top."),Aa.forEach(o),Hs=c(W),Re=s(W,"P",{});var Vo=r(Re);Qs=i(Vo,"This model inherits from "),_t=s(Vo,"A",{href:!0});var Oa=r(_t);Xs=i(Oa,"PreTrainedModel"),Oa.forEach(o),Ks=i(Vo,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vo.forEach(o),Ys=c(W),Be=s(W,"P",{});var Uo=r(Be);Js=i(Uo,"This model is also a PyTorch "),Ve=s(Uo,"A",{href:!0,rel:!0});var Da=r(Ve);Zs=i(Da,"torch.nn.Module"),Da.forEach(o),er=i(Uo,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Uo.forEach(o),tr=c(W),E=s(W,"DIV",{class:!0});var R=r(E);u(Ue.$$.fragment,R),or=c(R),ne=s(R,"P",{});var $t=r(ne);nr=i($t,"The "),gt=s($t,"A",{href:!0});var Na=r(gt);sr=i(Na,"FSMTModel"),Na.forEach(o),rr=i($t," forward method, overrides the "),ao=s($t,"CODE",{});var La=r(ao);ar=i(La,"__call__"),La.forEach(o),ir=i($t," special method."),$t.forEach(o),dr=c(R),u(fe.$$.fragment,R),lr=c(R),io=s(R,"P",{});var ja=r(io);cr=i(ja,"Example:"),ja.forEach(o),hr=c(R),u(He.$$.fragment,R),R.forEach(o),W.forEach(o),Eo=c(t),se=s(t,"H2",{class:!0});var Ho=r(se);ue=s(Ho,"A",{id:!0,class:!0,href:!0});var Ga=r(ue);lo=s(Ga,"SPAN",{});var Wa=r(lo);u(Qe.$$.fragment,Wa),Wa.forEach(o),Ga.forEach(o),pr=c(Ho),co=s(Ho,"SPAN",{});var Ra=r(co);mr=i(Ra,"FSMTForConditionalGeneration"),Ra.forEach(o),Ho.forEach(o),xo=c(t),z=s(t,"DIV",{class:!0});var B=r(z);u(Xe.$$.fragment,B),fr=c(B),ho=s(B,"P",{});var Ba=r(ho);ur=i(Ba,"The FSMT Model with a language modeling head. Can be used for summarization."),Ba.forEach(o),_r=c(B),Ke=s(B,"P",{});var Qo=r(Ke);gr=i(Qo,"This model inherits from "),Tt=s(Qo,"A",{href:!0});var Va=r(Tt);Tr=i(Va,"PreTrainedModel"),Va.forEach(o),kr=i(Qo,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qo.forEach(o),vr=c(B),Ye=s(B,"P",{});var Xo=r(Ye);br=i(Xo,"This model is also a PyTorch "),Je=s(Xo,"A",{href:!0,rel:!0});var Ua=r(Je);yr=i(Ua,"torch.nn.Module"),Ua.forEach(o),wr=i(Xo,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xo.forEach(o),Mr=c(B),x=s(B,"DIV",{class:!0});var V=r(x);u(Ze.$$.fragment,V),Fr=c(V),re=s(V,"P",{});var zt=r(re);Sr=i(zt,"The "),kt=s(zt,"A",{href:!0});var Ha=r(kt);$r=i(Ha,"FSMTForConditionalGeneration"),Ha.forEach(o),zr=i(zt," forward method, overrides the "),po=s(zt,"CODE",{});var Qa=r(po);qr=i(Qa,"__call__"),Qa.forEach(o),Er=i(zt," special method."),zt.forEach(o),xr=c(V),u(_e.$$.fragment,V),Cr=c(V),mo=s(V,"P",{});var Xa=r(mo);Pr=i(Xa,"Translation example::"),Xa.forEach(o),Ir=c(V),u(et.$$.fragment,V),V.forEach(o),B.forEach(o),this.h()},h(){d(m,"name","hf:doc:metadata"),d(m,"content",JSON.stringify(ri)),d(F,"id","fsmt"),d(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(F,"href","#fsmt"),d(v,"class","relative group"),d(ke,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),d(ke,"rel","nofollow"),d(ae,"id","overview"),d(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ae,"href","#overview"),d(H,"class","relative group"),d(be,"href","https://arxiv.org/abs/1907.06616"),d(be,"rel","nofollow"),d(ye,"href","https://huggingface.co/stas"),d(ye,"rel","nofollow"),d(we,"href","https://github.com/pytorch/fairseq/tree/master/examples/wmt19"),d(we,"rel","nofollow"),d(de,"id","implementation-notes"),d(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(de,"href","#implementation-notes"),d(Q,"class","relative group"),d(it,"href","/docs/transformers/pr_16937/en/model_doc/xlm#transformers.XLMTokenizer"),d(dt,"href","/docs/transformers/pr_16937/en/model_doc/bart#transformers.BartModel"),d(le,"id","transformers.FSMTConfig"),d(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(le,"href","#transformers.FSMTConfig"),d(K,"class","relative group"),d(lt,"href","/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTModel"),d($e,"href","https://huggingface.co/facebook/wmt19-en-ru"),d($e,"rel","nofollow"),d(ct,"href","/docs/transformers/pr_16937/en/main_classes/configuration#transformers.PretrainedConfig"),d(ht,"href","/docs/transformers/pr_16937/en/main_classes/configuration#transformers.PretrainedConfig"),d(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(he,"id","transformers.FSMTTokenizer"),d(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(he,"href","#transformers.FSMTTokenizer"),d(ee,"class","relative group"),d(pt,"href","/docs/transformers/pr_16937/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(me,"id","transformers.FSMTModel"),d(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(me,"href","#transformers.FSMTModel"),d(oe,"class","relative group"),d(_t,"href","/docs/transformers/pr_16937/en/main_classes/model#transformers.PreTrainedModel"),d(Ve,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Ve,"rel","nofollow"),d(gt,"href","/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTModel"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ue,"id","transformers.FSMTForConditionalGeneration"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#transformers.FSMTForConditionalGeneration"),d(se,"class","relative group"),d(Tt,"href","/docs/transformers/pr_16937/en/main_classes/model#transformers.PreTrainedModel"),d(Je,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Je,"rel","nofollow"),d(kt,"href","/docs/transformers/pr_16937/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,m),p(t,P,h),p(t,v,h),e(v,F),e(F,C),_(y,C,null),e(v,w),e(v,A),e(A,Yo),p(t,_o,h),p(t,U,h),e(U,qt),e(qt,Jo),e(U,Zo),e(U,ke),e(ke,en),e(U,tn),p(t,go,h),p(t,H,h),e(H,ae),e(ae,Et),_(ve,Et,null),e(H,on),e(H,xt),e(xt,nn),p(t,To,h),p(t,ie,h),e(ie,sn),e(ie,be),e(be,rn),e(ie,an),p(t,ko,h),p(t,st,h),e(st,dn),p(t,vo,h),p(t,rt,h),e(rt,Ct),e(Ct,ln),p(t,bo,h),p(t,L,h),e(L,cn),e(L,ye),e(ye,hn),e(L,pn),e(L,we),e(we,mn),e(L,fn),p(t,yo,h),p(t,Q,h),e(Q,de),e(de,Pt),_(Me,Pt,null),e(Q,un),e(Q,It),e(It,_n),p(t,wo,h),p(t,at,h),e(at,X),e(X,gn),e(X,it),e(it,Tn),e(X,kn),e(X,dt),e(dt,vn),e(X,bn),p(t,Mo,h),p(t,K,h),e(K,le),e(le,At),_(Fe,At,null),e(K,yn),e(K,Ot),e(Ot,wn),p(t,Fo,h),p(t,M,h),_(Se,M,null),e(M,Mn),e(M,Y),e(Y,Fn),e(Y,lt),e(lt,Sn),e(Y,$n),e(Y,$e),e($e,zn),e(Y,qn),e(M,En),e(M,J),e(J,xn),e(J,ct),e(ct,Cn),e(J,Pn),e(J,ht),e(ht,In),e(J,An),e(M,On),e(M,Dt),e(Dt,Dn),e(M,Nn),_(ze,M,null),e(M,Ln),e(M,ce),_(qe,ce,null),e(ce,jn),e(ce,Z),e(Z,Gn),e(Z,Nt),e(Nt,Wn),e(Z,Rn),e(Z,Lt),e(Lt,Bn),e(Z,Vn),p(t,So,h),p(t,ee,h),e(ee,he),e(he,jt),_(Ee,jt,null),e(ee,Un),e(ee,Gt),e(Gt,Hn),p(t,$o,h),p(t,b,h),_(xe,b,null),e(b,Qn),e(b,Wt),e(Wt,Xn),e(b,Kn),e(b,O),e(O,Rt),e(Rt,Yn),e(O,Jn),e(O,Bt),e(Bt,Zn),e(O,es),e(O,D),e(D,ts),e(D,Vt),e(Vt,os),e(D,ns),e(D,Ut),e(Ut,ss),e(D,rs),e(D,Ht),e(Ht,as),e(D,is),e(O,ds),e(O,Ce),e(Ce,ls),e(Ce,Qt),e(Qt,cs),e(Ce,hs),e(b,ps),e(b,Pe),e(Pe,ms),e(Pe,pt),e(pt,fs),e(Pe,us),e(b,_s),e(b,j),_(Ie,j,null),e(j,gs),e(j,Xt),e(Xt,Ts),e(j,ks),e(j,Ae),e(Ae,mt),e(mt,vs),e(mt,Kt),e(Kt,bs),e(Ae,ys),e(Ae,ft),e(ft,ws),e(ft,Yt),e(Yt,Ms),e(b,Fs),e(b,pe),_(Oe,pe,null),e(pe,Ss),e(pe,De),e(De,$s),e(De,Jt),e(Jt,zs),e(De,qs),e(b,Es),e(b,q),_(Ne,q,null),e(q,xs),e(q,Zt),e(Zt,Cs),e(q,Ps),_(Le,q,null),e(q,Is),e(q,te),e(te,As),e(te,eo),e(eo,Os),e(te,Ds),e(te,to),e(to,Ns),e(te,Ls),e(q,js),e(q,oo),e(oo,Gs),e(b,Ws),e(b,ut),_(je,ut,null),p(t,zo,h),p(t,oe,h),e(oe,me),e(me,no),_(Ge,no,null),e(oe,Rs),e(oe,so),e(so,Bs),p(t,qo,h),p(t,$,h),_(We,$,null),e($,Vs),e($,ro),e(ro,Us),e($,Hs),e($,Re),e(Re,Qs),e(Re,_t),e(_t,Xs),e(Re,Ks),e($,Ys),e($,Be),e(Be,Js),e(Be,Ve),e(Ve,Zs),e(Be,er),e($,tr),e($,E),_(Ue,E,null),e(E,or),e(E,ne),e(ne,nr),e(ne,gt),e(gt,sr),e(ne,rr),e(ne,ao),e(ao,ar),e(ne,ir),e(E,dr),_(fe,E,null),e(E,lr),e(E,io),e(io,cr),e(E,hr),_(He,E,null),p(t,Eo,h),p(t,se,h),e(se,ue),e(ue,lo),_(Qe,lo,null),e(se,pr),e(se,co),e(co,mr),p(t,xo,h),p(t,z,h),_(Xe,z,null),e(z,fr),e(z,ho),e(ho,ur),e(z,_r),e(z,Ke),e(Ke,gr),e(Ke,Tt),e(Tt,Tr),e(Ke,kr),e(z,vr),e(z,Ye),e(Ye,br),e(Ye,Je),e(Je,yr),e(Ye,wr),e(z,Mr),e(z,x),_(Ze,x,null),e(x,Fr),e(x,re),e(re,Sr),e(re,kt),e(kt,$r),e(re,zr),e(re,po),e(po,qr),e(re,Er),e(x,xr),_(_e,x,null),e(x,Cr),e(x,mo),e(mo,Pr),e(x,Ir),_(et,x,null),Co=!0},p(t,[h]){const tt={};h&2&&(tt.$$scope={dirty:h,ctx:t}),fe.$set(tt);const fo={};h&2&&(fo.$$scope={dirty:h,ctx:t}),_e.$set(fo)},i(t){Co||(g(y.$$.fragment,t),g(ve.$$.fragment,t),g(Me.$$.fragment,t),g(Fe.$$.fragment,t),g(Se.$$.fragment,t),g(ze.$$.fragment,t),g(qe.$$.fragment,t),g(Ee.$$.fragment,t),g(xe.$$.fragment,t),g(Ie.$$.fragment,t),g(Oe.$$.fragment,t),g(Ne.$$.fragment,t),g(Le.$$.fragment,t),g(je.$$.fragment,t),g(Ge.$$.fragment,t),g(We.$$.fragment,t),g(Ue.$$.fragment,t),g(fe.$$.fragment,t),g(He.$$.fragment,t),g(Qe.$$.fragment,t),g(Xe.$$.fragment,t),g(Ze.$$.fragment,t),g(_e.$$.fragment,t),g(et.$$.fragment,t),Co=!0)},o(t){T(y.$$.fragment,t),T(ve.$$.fragment,t),T(Me.$$.fragment,t),T(Fe.$$.fragment,t),T(Se.$$.fragment,t),T(ze.$$.fragment,t),T(qe.$$.fragment,t),T(Ee.$$.fragment,t),T(xe.$$.fragment,t),T(Ie.$$.fragment,t),T(Oe.$$.fragment,t),T(Ne.$$.fragment,t),T(Le.$$.fragment,t),T(je.$$.fragment,t),T(Ge.$$.fragment,t),T(We.$$.fragment,t),T(Ue.$$.fragment,t),T(fe.$$.fragment,t),T(He.$$.fragment,t),T(Qe.$$.fragment,t),T(Xe.$$.fragment,t),T(Ze.$$.fragment,t),T(_e.$$.fragment,t),T(et.$$.fragment,t),Co=!1},d(t){o(m),t&&o(P),t&&o(v),k(y),t&&o(_o),t&&o(U),t&&o(go),t&&o(H),k(ve),t&&o(To),t&&o(ie),t&&o(ko),t&&o(st),t&&o(vo),t&&o(rt),t&&o(bo),t&&o(L),t&&o(yo),t&&o(Q),k(Me),t&&o(wo),t&&o(at),t&&o(Mo),t&&o(K),k(Fe),t&&o(Fo),t&&o(M),k(Se),k(ze),k(qe),t&&o(So),t&&o(ee),k(Ee),t&&o($o),t&&o(b),k(xe),k(Ie),k(Oe),k(Ne),k(Le),k(je),t&&o(zo),t&&o(oe),k(Ge),t&&o(qo),t&&o($),k(We),k(Ue),k(fe),k(He),t&&o(Eo),t&&o(se),k(Qe),t&&o(xo),t&&o(z),k(Xe),k(Ze),k(_e),k(et)}}}const ri={local:"fsmt",sections:[{local:"overview",title:"Overview"},{local:"implementation-notes",title:"Implementation Notes"},{local:"transformers.FSMTConfig",title:"FSMTConfig"},{local:"transformers.FSMTTokenizer",title:"FSMTTokenizer"},{local:"transformers.FSMTModel",title:"FSMTModel"},{local:"transformers.FSMTForConditionalGeneration",title:"FSMTForConditionalGeneration"}],title:"FSMT"};function ai(nt){return ti(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pi extends Ya{constructor(m){super();Ja(this,m,ai,si,Za,{})}}export{pi as default,ri as metadata};
