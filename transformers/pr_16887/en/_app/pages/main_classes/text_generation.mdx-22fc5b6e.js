import{S as Cm,i as Im,s as Bm,e as s,k as l,w as m,t,M as Wm,c as a,d as o,m as d,a as r,x as g,h as n,b as c,F as e,g as M,y as _,q as u,o as h,B as f,v as Hm}from"../../chunks/vendor-6b77c823.js";import{T as Nm}from"../../chunks/Tip-39098574.js";import{D as _e}from"../../chunks/Docstring-1088f2fb.js";import{C as be}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Bs}from"../../chunks/IconCopyLink-7a11ce68.js";function Rm(en){let p,C,L,q,D,w,ue,N,O,I,G;return{c(){p=s("p"),C=t("Apart from "),L=s("code"),q=t("inputs"),D=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),w=s("code"),ue=t("config.json"),N=t(`) which in turn defaults to the
`),O=s("a"),I=t("PretrainedConfig"),G=t(" of the model."),this.h()},l(S){p=a(S,"P",{});var x=r(p);C=n(x,"Apart from "),L=a(x,"CODE",{});var ye=r(L);q=n(ye,"inputs"),ye.forEach(o),D=n(x,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),w=a(x,"CODE",{});var je=r(w);ue=n(je,"config.json"),je.forEach(o),N=n(x,`) which in turn defaults to the
`),O=a(x,"A",{href:!0});var xe=r(O);I=n(xe,"PretrainedConfig"),xe.forEach(o),G=n(x," of the model."),x.forEach(o),this.h()},h(){c(O,"href","/docs/transformers/pr_16887/en/main_classes/configuration#transformers.PretrainedConfig")},m(S,x){M(S,p,x),e(p,C),e(p,L),e(L,q),e(p,D),e(p,w),e(w,ue),e(p,N),e(p,O),e(O,I),e(p,G)},d(S){S&&o(p)}}}function Um(en){let p,C,L,q,D,w,ue,N,O,I,G;return{c(){p=s("p"),C=t("Apart from "),L=s("code"),q=t("inputs"),D=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),w=s("code"),ue=t("config.json"),N=t(`) which in turn defaults to the
`),O=s("a"),I=t("PretrainedConfig"),G=t(" of the model."),this.h()},l(S){p=a(S,"P",{});var x=r(p);C=n(x,"Apart from "),L=a(x,"CODE",{});var ye=r(L);q=n(ye,"inputs"),ye.forEach(o),D=n(x,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),w=a(x,"CODE",{});var je=r(w);ue=n(je,"config.json"),je.forEach(o),N=n(x,`) which in turn defaults to the
`),O=a(x,"A",{href:!0});var xe=r(O);I=n(xe,"PretrainedConfig"),xe.forEach(o),G=n(x," of the model."),x.forEach(o),this.h()},h(){c(O,"href","/docs/transformers/pr_16887/en/main_classes/configuration#transformers.PretrainedConfig")},m(S,x){M(S,p,x),e(p,C),e(p,L),e(L,q),e(p,D),e(p,w),e(w,ue),e(p,N),e(p,O),e(O,I),e(p,G)},d(S){S&&o(p)}}}function Vm(en){let p,C,L,q,D,w,ue,N,O,I,G,S,x,ye,je,xe,ke,Le,Ws,tn,Hs,Rs,nn,Us,Vs,Ks,Me,Zs,on,Xs,Js,sn,Qs,Ys,ea,we,ta,an,na,oa,rn,sa,aa,hs,Te,Xe,$n,lt,ra,Fn,ia,fs,k,dt,la,ct,da,ln,ca,pa,ma,pt,ga,dn,_a,ua,ha,$,B,zn,fa,ba,cn,xa,ka,An,va,ya,Pn,ja,La,Ma,W,Dn,wa,Ta,pn,Ea,Oa,Nn,qa,Ga,Cn,Sa,$a,Fa,H,In,za,Aa,mn,Pa,Da,Bn,Na,Ca,Wn,Ia,Ba,Wa,R,Hn,Ha,Ra,gn,Ua,Va,Rn,Ka,Za,Un,Xa,Ja,Qa,U,Vn,Ya,er,_n,tr,nr,Kn,or,sr,Zn,ar,rr,ir,V,Xn,lr,dr,un,cr,pr,Jn,mr,gr,Qn,_r,ur,hr,b,mt,fr,Yn,br,xr,F,K,eo,kr,vr,hn,yr,jr,to,Lr,Mr,no,wr,Tr,Er,Z,oo,Or,qr,fn,Gr,Sr,so,$r,Fr,ao,zr,Ar,Pr,X,ro,Dr,Nr,bn,Cr,Ir,io,Br,Wr,lo,Hr,Rr,Ur,J,co,Vr,Kr,xn,Zr,Xr,po,Jr,Qr,mo,Yr,ei,ti,Q,go,ni,oi,kn,si,ai,_o,ri,ii,uo,li,di,ci,Y,ho,pi,mi,vn,gi,_i,fo,ui,hi,bo,fi,bi,xi,Je,ki,gt,vi,_t,yi,ji,Li,xo,Mi,wi,ko,Ti,Ei,ut,Oi,vo,qi,Gi,ht,Si,yo,$i,Fi,ft,zi,ee,bt,Ai,xt,Pi,jo,Di,Ni,Ci,Lo,Ii,Bi,kt,Wi,te,vt,Hi,yt,Ri,Mo,Ui,Vi,Ki,wo,Zi,Xi,jt,Ji,ne,Lt,Qi,Mt,Yi,To,el,tl,nl,Eo,ol,sl,wt,al,oe,Tt,rl,Et,il,Oo,ll,dl,cl,qo,pl,ml,Ot,gl,se,qt,_l,Gt,ul,Go,hl,fl,bl,So,xl,kl,St,vl,ae,$t,yl,Ft,jl,$o,Ll,Ml,wl,Fo,Tl,El,zt,bs,Ee,Qe,zo,At,Ol,Ao,ql,xs,he,Pt,Gl,Dt,Sl,yn,$l,Fl,zl,T,Nt,Al,Po,Pl,Dl,Ct,Nl,It,Cl,Il,Bl,fe,Wl,Do,Hl,Rl,No,Ul,Vl,jn,Kl,Zl,Xl,Bt,Jl,Wt,Ql,Yl,ed,Co,td,nd,Ht,ks,Oe,Ye,Io,Rt,od,Bo,sd,vs,z,Ut,ad,Vt,rd,Ln,id,ld,dd,Kt,cd,Mn,pd,md,gd,qe,re,Wo,_d,ud,Ho,hd,fd,Ro,bd,xd,Uo,kd,vd,yd,ie,Vo,jd,Ld,Ko,Md,wd,Zo,Td,Ed,Xo,Od,qd,Gd,le,Jo,Sd,$d,Qo,Fd,zd,Yo,Ad,Pd,es,Dd,Nd,Cd,E,Zt,Id,ts,Bd,Wd,Ge,de,ns,Hd,Rd,os,Ud,Vd,ss,Kd,Zd,as,Xd,Jd,Qd,ce,rs,Yd,ec,is,tc,nc,ls,oc,sc,ds,ac,rc,ic,pe,cs,lc,dc,ps,cc,pc,ms,mc,gc,gs,_c,uc,hc,et,fc,Xt,bc,Jt,xc,kc,vc,_s,yc,jc,Qt,ys;return w=new Bs({}),lt=new Bs({}),dt=new _e({props:{name:"class transformers.generation_utils.GenerationMixin",anchor:"transformers.generation_utils.GenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L379"}}),mt=new _e({props:{name:"generate",anchor:"transformers.generation_utils.GenerationMixin.generate",parameters:[{name:"inputs",val:": typing.Optional[torch.Tensor] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"typical_p",val:": typing.Optional[float] = None"},{name:"repetition_penalty",val:": typing.Optional[float] = None"},{name:"bad_words_ids",val:": typing.Optional[typing.Iterable[int]] = None"},{name:"force_words_ids",val:": typing.Union[typing.Iterable[int], typing.Iterable[typing.Iterable[int]], NoneType] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"encoder_no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"num_return_sequences",val:": typing.Optional[int] = None"},{name:"max_time",val:": typing.Optional[float] = None"},{name:"max_new_tokens",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"num_beam_groups",val:": typing.Optional[int] = None"},{name:"diversity_penalty",val:": typing.Optional[float] = None"},{name:"prefix_allowed_tokens_fn",val:": typing.Union[typing.Callable[[int, torch.Tensor], typing.List[int]], NoneType] = None"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = []"},{name:"renormalize_logits",val:": typing.Optional[bool] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = []"},{name:"constraints",val:": typing.Optional[typing.List[transformers.generation_beam_constraints.Constraint]] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"remove_invalid_values",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"exponential_decay_length_penalty",val:": typing.Union[typing.Tuple[typing.Union[int, float]], NoneType] = None"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.generate.inputs",description:`<strong>inputs</strong> (<code>torch.Tensor</code> of varying shape depending on the modality, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"inputs"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to <code>model.config.max_length</code>) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_new_tokens",description:`<strong>max_new_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
The maximum numbers of tokens to generate, ignore the current number of tokens. Use either
<code>max_new_tokens</code> or <code>max_length</code> but not both, they serve the same purpose.`,name:"max_new_tokens"},{anchor:"transformers.generation_utils.GenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_utils.GenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_utils.GenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_utils.GenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the
model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer
sequences.`,name:"length_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.encoder_no_repeat_ngram_size",description:`<strong>encoder_no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size that occur in the <code>encoder_input_ids</code> cannot occur in the
<code>decoder_input_ids</code>.`,name:"encoder_no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bad_words_ids(List[List[int]],",description:`<strong>bad_words_ids(<code>List[List[int]]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the token ids of the words that
should not appear in the generated text, use <code>tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids</code>.`,name:"bad_words_ids(List[List[int]],"},{anchor:"transformers.generation_utils.GenerationMixin.generate.force_words_ids(List[List[int]]",description:`<strong>force_words_ids(<code>List[List[int]]</code></strong> or <code>List[List[List[int]]]</code>, <em>optional</em>) &#x2014;
List of token ids that must be generated. If given a <code>List[List[int]]</code>, this is treated as a simple
list of words that must be included, the opposite to <code>bad_words_ids</code>. If given <code>List[List[List[int]]]</code>,
this triggers a <a href="https://github.com/huggingface/transformers/issues/14081" rel="nofollow">disjunctive constraint</a>,
where one can allow different forms of each word.`,name:"force_words_ids(List[List[int]]"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_time(float,",description:`<strong>max_time(<code>float</code>,</strong> <em>optional</em>, defaults to None) &#x2014;
The maximum amount of time you allow the computation to run for in seconds. generation will still
finish the current pass after allocated time has been passed.`,name:"max_time(float,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape
as <code>input_ids</code> that masks the pad token. <a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_utils.GenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beam_groups",description:`<strong>num_beam_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of groups to divide <code>num_beams</code> into in order to ensure diversity among different groups of
beams. <a href="https://arxiv.org/pdf/1610.02424.pdf" rel="nofollow">this paper</a> for more details.`,name:"num_beam_groups"},{anchor:"transformers.generation_utils.GenerationMixin.generate.diversity_penalty",description:`<strong>diversity_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
This value is subtracted from a beam&#x2019;s score if it generates a token same as any beam from other group
at a particular time. Note that <code>diversity_penalty</code> is only effective if <code>group beam search</code> is
enabled.`,name:"diversity_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.prefix_allowed_tokens_fn",description:`<strong>prefix_allowed_tokens_fn</strong> (<code>Callable[[int, torch.Tensor], List[int]]</code>, <em>optional</em>) &#x2014;
If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904" rel="nofollow">Autoregressive Entity
Retrieval</a>.`,name:"prefix_allowed_tokens_fn"},{anchor:"transformers.generation_utils.GenerationMixin.generate.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
Custom logits processors that complement the default logits processors built from arguments and a
model&#x2019;s config. If a logit processor is passed that is already created with the arguments or a model&#x2019;s
config an error is thrown. This feature is intended for advanced users.
renormalize_logits &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether to renormalize the logits after applying all the logits processors or warpers (including the
custom ones). It&#x2019;s highly recommended to set this flag to <code>True</code> as the search algorithms suppose the
score logits are normalized but some logit processors or warpers break the normalization.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.generate.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
Custom stopping criteria that complement the default stopping criteria built from arguments and a
model&#x2019;s config. If a stopping criteria is passed that is already created with the arguments or a
model&#x2019;s config an error is thrown. This feature is intended for advanced users.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.generate.constraints",description:`<strong>constraints</strong> (<code>List[Constraint]</code>, <em>optional</em>) &#x2014;
Custom constraints that can be added to the generation to ensure that the output will contain the use
of certain tokens as defined by <code>Constraint</code> objects, in the most sensible way possible.`,name:"constraints"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.`,name:"forced_eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.remove_invalid_values",description:`<strong>remove_invalid_values</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to remove possible <em>nan</em> and <em>inf</em> outputs of the model to prevent the generation method to
crash. Note that using <code>remove_invalid_values</code> can slow down generation.`,name:"remove_invalid_values"},{anchor:"transformers.generation_utils.GenerationMixin.generate.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)`,name:"synced_gpus"},{anchor:"transformers.generation_utils.GenerationMixin.generate.exponential_decay_length_penalty",description:`<strong>exponential_decay_length_penalty</strong> (<code>tuple(int, float)</code>, <em>optional</em>) &#x2014;
This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been
generated. The tuple shall consist of: <code>(start_index, decay_factor)</code> where <code>start_index</code> indicates
where penalty starts and <code>decay_factor</code> represents the factor of exponential decay</p>
<p>model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*.`,name:"exponential_decay_length_penalty"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L839",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> (if <code>return_dict_in_generate=True</code>
or when <code>config.return_dict_in_generate=True</code>) or a <code>torch.FloatTensor</code>.</p>
<p>If the model is <em>not</em> an encoder-decoder model (<code>model.config.is_encoder_decoder=False</code>), the possible
<a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a></li>
</ul>
<p>If the model is an encoder-decoder model (<code>model.config.is_encoder_decoder=True</code>), the possible
<a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a></li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> or <code>torch.LongTensor</code></p>
`}}),Je=new Nm({props:{warning:!0,$$slots:{default:[Rm]},$$scope:{ctx:en}}}),ut=new be({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# generate up to 30 tokens
outputs = model.generate(input_ids, do_sample=False, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n&#x27;</span>]`}}),ht=new be({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# sample up to 30 tokens
torch.manual_seed(0)
outputs = model.generate(input_ids, do_sample=True, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get rid of discrimination,&quot; said Rep. Mark Pocan (D-Wis.).\\n\\n&quot;Just look at the&#x27;</span>]`}}),ft=new be({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-de")

sentence = "Paris is one of the densest populated areas in Europe."
input_ids = tokenizer(sentence, return_tensors="pt").input_ids

outputs = model.generate(input_ids)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sentence = <span class="hljs-string">&quot;Paris is one of the densest populated areas in Europe.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Paris ist eines der dichtesten besiedelten Gebiete Europas.&#x27;</span>]`}}),bt=new _e({props:{name:"greedy_search",anchor:"transformers.generation_utils.GenerationMixin.greedy_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific keyword arguments will be forwarded to the <code>forward</code> function of the model.
If model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L1518",returnDescription:`
<p><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>
or <code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),kt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    StoppingCriteriaList,
    MaxLengthCriteria,
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "It might be possible to"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),
    ]
)
stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

outputs = model.greedy_search(
    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;It might be possible to&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">10</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.greedy_search(
<span class="hljs-meta">... </span>    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&quot;It might be possible to get a better understanding of the nature of the problem, but it&#x27;s not&quot;</span>]`}}),vt=new _e({props:{name:"sample",anchor:"transformers.generation_utils.GenerationMixin.sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L1750",returnDescription:`
<p><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),jt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    StoppingCriteriaList,
    MaxLengthCriteria,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
    ]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

torch.manual_seed(0)
outputs = model.sample(
    input_ids,
    logits_processor=logits_processor,
    logits_warper=logits_warper,
    stopping_criteria=stopping_criteria,
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;Today is a beautiful day, and&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">15</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.sample(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    logits_processor=logits_processor,
<span class="hljs-meta">... </span>    logits_warper=logits_warper,
<span class="hljs-meta">... </span>    stopping_criteria=stopping_criteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the&#x27;</span>]`}}),Lt=new _e({props:{name:"beam_search",anchor:"transformers.generation_utils.GenerationMixin.beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L2006",returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),wt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Tt=new _e({props:{name:"beam_sample",anchor:"transformers.generation_utils.GenerationMixin.beam_sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L2318",returnDescription:`
<p><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),Ot=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

outputs = model.beam_sample(
    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id)]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_sample(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),qt=new _e({props:{name:"group_beam_search",anchor:"transformers.generation_utils.GenerationMixin.group_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
<p>model_kwargs &#x2014;
Additional model specific kwargs that will be forwarded to the <code>forward</code> function of the model. If
model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L2640",returnDescription:`
<p><a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if
<code>model.config.is_encoder_decoder=False</code> and <code>return_dict_in_generate=True</code> or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if <code>model.config.is_encoder_decoder=True</code>.</p>
`}}),St=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    HammingDiversityLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
    num_beam_groups=3,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.group_beam_search(
    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    HammingDiversityLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run diverse beam search using 6 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>    num_beam_groups=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        HammingDiversityLogitsProcessor(<span class="hljs-number">5.5</span>, num_beams=<span class="hljs-number">6</span>, num_beam_groups=<span class="hljs-number">3</span>),
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.group_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),$t=new _e({props:{name:"constrained_beam_search",anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"constrained_beam_scorer",val:": ConstrainedBeamSearchScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = None"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.constrained_beam_scorer",description:`<strong>constrained_beam_scorer</strong> (<code>ConstrainedBeamSearchScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation, while satisfying a list of positive constraints. For more information, the
documentation of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer">ConstrainedBeamSearchScorer</a> should be read.`,name:"constrained_beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_utils.py#L3005",returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16887/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),zt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    ConstrainedBeamSearchScorer,
    PhrasalConstraint,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

constraint_str = "Sie"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.constrained_beam_search(
    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    ConstrainedBeamSearchScorer,
<span class="hljs-meta">... </span>    PhrasalConstraint,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_str = <span class="hljs-string">&quot;Sie&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_token_ids = tokenizer.encode(constraint_str)[:-<span class="hljs-number">1</span>]  <span class="hljs-comment"># slice to remove eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = ConstrainedBeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>, num_beams=num_beams, device=model.device, constraints=constraints
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.constrained_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt sind Sie?&#x27;</span>]`}}),At=new Bs({}),Pt=new _e({props:{name:"class transformers.generation_tf_utils.TFGenerationMixin",anchor:"transformers.generation_tf_utils.TFGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_tf_utils.py#L344"}}),Nt=new _e({props:{name:"generate",anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate",parameters:[{name:"input_ids",val:" = None"},{name:"max_length",val:" = None"},{name:"min_length",val:" = None"},{name:"do_sample",val:" = None"},{name:"early_stopping",val:" = None"},{name:"num_beams",val:" = None"},{name:"temperature",val:" = None"},{name:"top_k",val:" = None"},{name:"top_p",val:" = None"},{name:"repetition_penalty",val:" = None"},{name:"bad_words_ids",val:" = None"},{name:"bos_token_id",val:" = None"},{name:"pad_token_id",val:" = None"},{name:"eos_token_id",val:" = None"},{name:"length_penalty",val:" = None"},{name:"no_repeat_ngram_size",val:" = None"},{name:"num_return_sequences",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_start_token_id",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_scores",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict_in_generate",val:" = None"},{name:"forced_bos_token_id",val:" = None"},{name:"forced_eos_token_id",val:" = None"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.input_ids",description:"<strong>input_ids</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, `(batch_size, sequence_length, &#x2014;",name:"input_ids"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.feature_dim)`",description:`<strong>feature_dim)\`</strong> or <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"feature_dim)`"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.`,name:"length_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bad_words_ids(List[int],",description:`<strong>bad_words_ids(<code>List[int]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code>tokenizer.encode(bad_word, add_prefix_space=True)</code>.`,name:"bad_words_ids(List[int],"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>tf.Tensor</code> of <code>dtype=tf.int32</code> and shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code>input_ids</code> that masks the pad token.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.
model_specific_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model.`,name:"forced_eos_token_id"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_tf_utils.py#L366",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> (if <code>return_dict_in_generate=True</code> or when
<code>config.return_dict_in_generate=True</code>) or a <code>tf.Tensor</code>.</p>
<p>If the model is <em>not</em> an encoder-decoder model (<code>model.config.is_encoder_decoder=False</code>), the possible
<a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><code>TFGreedySearchDecoderOnlyOutput</code>,</li>
<li><code>TFSampleDecoderOnlyOutput</code>,</li>
<li><code>TFBeamSearchDecoderOnlyOutput</code>,</li>
<li><code>TFBeamSampleDecoderOnlyOutput</code></li>
</ul>
<p>If the model is an encoder-decoder model (<code>model.config.is_encoder_decoder=True</code>), the possible
<a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><code>TFGreedySearchEncoderDecoderOutput</code>,</li>
<li><code>TFSampleEncoderDecoderOutput</code>,</li>
<li><code>TFBeamSearchEncoderDecoderOutput</code>,</li>
<li><code>TFBeamSampleEncoderDecoderOutput</code></li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a> or <code>tf.Tensor</code></p>
`}}),Ht=new be({props:{code:`tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
outputs = model.generate(max_length=40)  # do greedy decoding
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("openai-gpt")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "openai-gpt"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5
)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True
)  # generate 3 candidates using sampling
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("ctrl")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "ctrl"
)  # Download model and configuration from huggingface.co and cache.
input_context = "Legal My neighbor is"  # "Legal" is one of the control codes for ctrl
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2
)  # generate sequences
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "gpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "My cute dog"
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ["idiot", "stupid", "shut up"]
]
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids
)  # generate sequences without allowing bad_words to be generated`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
outputs = model.generate(max_length=<span class="hljs-number">40</span>)  <span class="hljs-comment"># do greedy decoding</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-gpt&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;openai-gpt&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>, temperature=<span class="hljs-number">1.5</span>
)  <span class="hljs-comment"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#x27;The dog&#x27;</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">40</span>, temperature=<span class="hljs-number">0.7</span>, num_return_sequences=<span class="hljs-number">3</span>, do_sample=<span class="hljs-literal">True</span>
)  <span class="hljs-comment"># generate 3 candidates using sampling</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ctrl&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;ctrl&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;Legal My neighbor is&quot;</span>  <span class="hljs-comment"># &quot;Legal&quot; is one of the control codes for ctrl</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">0.7</span>, repetition_penalty=<span class="hljs-number">1.2</span>
)  <span class="hljs-comment"># generate sequences</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;My cute dog&quot;</span>
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=<span class="hljs-literal">True</span>) <span class="hljs-keyword">for</span> bad_word <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idiot&quot;</span>, <span class="hljs-string">&quot;stupid&quot;</span>, <span class="hljs-string">&quot;shut up&quot;</span>]
]
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>, bad_words_ids=bad_words_ids
)  <span class="hljs-comment"># generate sequences without allowing bad_words to be generated</span>`}}),Rt=new Bs({}),Ut=new _e({props:{name:"class transformers.generation_flax_utils.FlaxGenerationMixin",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_flax_utils.py#L118"}}),Zt=new _e({props:{name:"generate",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate",parameters:[{name:"input_ids",val:": ndarray"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"prng_key",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"trace",val:": bool = True"},{name:"params",val:": typing.Union[typing.Dict[str, jax._src.numpy.ndarray.ndarray], NoneType] = None"},{name:"**model_kwargs",val:""}],parametersDescription:[{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.trace",description:`<strong>trace</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trace generation. Setting <code>trace=False</code> should only be used for debugging and will lead to a
considerably slower runtime.`,name:"trace"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.params",description:`<strong>params</strong> (<code>Dict[str, jnp.ndarray]</code>, <em>optional</em>) &#x2014;
Optionally the model parameters can be passed. Can be useful for parallelized generation.
model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*. Also accepts <code>encoder_outputs</code> to skip encoder part.`,name:"params"}],source:"https://github.com/huggingface/transformers/blob/pr_16887/src/transformers/generation_flax_utils.py#L162",returnDescription:`
<p><a
  href="/docs/transformers/pr_16887/en/main_classes/output#transformers.utils.ModelOutput"
>ModelOutput</a>.</p>
`}}),et=new Nm({props:{warning:!0,$$slots:{default:[Um]},$$scope:{ctx:en}}}),Qt=new be({props:{code:`from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = FlaxAutoModelForCausalLM.from_pretrained("distilgpt2")
input_context = "The dog"
# encode input context
input_ids = tokenizer(input_context, return_tensors="np").input_ids
# generate candidates using sampling
outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_context = <span class="hljs-string">&quot;The dog&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># encode input context</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_context, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate candidates using sampling</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids=input_ids, max_length=<span class="hljs-number">20</span>, top_k=<span class="hljs-number">30</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),{c(){p=s("meta"),C=l(),L=s("h1"),q=s("a"),D=s("span"),m(w.$$.fragment),ue=l(),N=s("span"),O=t("Generation"),I=l(),G=s("p"),S=t("Each framework has a generate method for auto-regressive text generation implemented in their respective "),x=s("code"),ye=t("GenerationMixin"),je=t(" class:"),xe=l(),ke=s("ul"),Le=s("li"),Ws=t("PyTorch "),tn=s("a"),Hs=t("generate()"),Rs=t(" is implemented in "),nn=s("a"),Us=t("GenerationMixin"),Vs=t("."),Ks=l(),Me=s("li"),Zs=t("TensorFlow "),on=s("a"),Xs=t("generate()"),Js=t(" is implemented in "),sn=s("a"),Qs=t("TFGenerationMixin"),Ys=t("."),ea=l(),we=s("li"),ta=t("Flax/JAX "),an=s("a"),na=t("generate()"),oa=t(" is implemented in "),rn=s("a"),sa=t("FlaxGenerationMixin"),aa=t("."),hs=l(),Te=s("h2"),Xe=s("a"),$n=s("span"),m(lt.$$.fragment),ra=l(),Fn=s("span"),ia=t("GenerationMixin"),fs=l(),k=s("div"),m(dt.$$.fragment),la=l(),ct=s("p"),da=t("A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=s("a"),ca=t("PreTrainedModel"),pa=t("."),ma=l(),pt=s("p"),ga=t("The class exposes "),dn=s("a"),_a=t("generate()"),ua=t(", which can be used for:"),ha=l(),$=s("ul"),B=s("li"),zn=s("em"),fa=t("greedy decoding"),ba=t(" by calling "),cn=s("a"),xa=t("greedy_search()"),ka=t(" if "),An=s("code"),va=t("num_beams=1"),ya=t(` and
`),Pn=s("code"),ja=t("do_sample=False"),La=t("."),Ma=l(),W=s("li"),Dn=s("em"),wa=t("multinomial sampling"),Ta=t(" by calling "),pn=s("a"),Ea=t("sample()"),Oa=t(" if "),Nn=s("code"),qa=t("num_beams=1"),Ga=t(` and
`),Cn=s("code"),Sa=t("do_sample=True"),$a=t("."),Fa=l(),H=s("li"),In=s("em"),za=t("beam-search decoding"),Aa=t(" by calling "),mn=s("a"),Pa=t("beam_search()"),Da=t(" if "),Bn=s("code"),Na=t("num_beams>1"),Ca=t(` and
`),Wn=s("code"),Ia=t("do_sample=False"),Ba=t("."),Wa=l(),R=s("li"),Hn=s("em"),Ha=t("beam-search multinomial sampling"),Ra=t(" by calling "),gn=s("a"),Ua=t("beam_sample()"),Va=t(` if
`),Rn=s("code"),Ka=t("num_beams>1"),Za=t(" and "),Un=s("code"),Xa=t("do_sample=True"),Ja=t("."),Qa=l(),U=s("li"),Vn=s("em"),Ya=t("diverse beam-search decoding"),er=t(" by calling "),_n=s("a"),tr=t("group_beam_search()"),nr=t(`, if
`),Kn=s("code"),or=t("num_beams>1"),sr=t(" and "),Zn=s("code"),ar=t("num_beam_groups>1"),rr=t("."),ir=l(),V=s("li"),Xn=s("em"),lr=t("constrained beam-search decoding"),dr=t(" by calling "),un=s("a"),cr=t("constrained_beam_search()"),pr=t(`,
if `),Jn=s("code"),mr=t("constraints!=None"),gr=t(" or "),Qn=s("code"),_r=t("force_words_ids!=None"),ur=t("."),hr=l(),b=s("div"),m(mt.$$.fragment),fr=l(),Yn=s("p"),br=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),xr=l(),F=s("ul"),K=s("li"),eo=s("em"),kr=t("greedy decoding"),vr=t(" by calling "),hn=s("a"),yr=t("greedy_search()"),jr=t(" if "),to=s("code"),Lr=t("num_beams=1"),Mr=t(` and
`),no=s("code"),wr=t("do_sample=False"),Tr=t("."),Er=l(),Z=s("li"),oo=s("em"),Or=t("multinomial sampling"),qr=t(" by calling "),fn=s("a"),Gr=t("sample()"),Sr=t(" if "),so=s("code"),$r=t("num_beams=1"),Fr=t(` and
`),ao=s("code"),zr=t("do_sample=True"),Ar=t("."),Pr=l(),X=s("li"),ro=s("em"),Dr=t("beam-search decoding"),Nr=t(" by calling "),bn=s("a"),Cr=t("beam_search()"),Ir=t(" if "),io=s("code"),Br=t("num_beams>1"),Wr=t(` and
`),lo=s("code"),Hr=t("do_sample=False"),Rr=t("."),Ur=l(),J=s("li"),co=s("em"),Vr=t("beam-search multinomial sampling"),Kr=t(" by calling "),xn=s("a"),Zr=t("beam_sample()"),Xr=t(` if
`),po=s("code"),Jr=t("num_beams>1"),Qr=t(" and "),mo=s("code"),Yr=t("do_sample=True"),ei=t("."),ti=l(),Q=s("li"),go=s("em"),ni=t("diverse beam-search decoding"),oi=t(" by calling "),kn=s("a"),si=t("group_beam_search()"),ai=t(`, if
`),_o=s("code"),ri=t("num_beams>1"),ii=t(" and "),uo=s("code"),li=t("num_beam_groups>1"),di=t("."),ci=l(),Y=s("li"),ho=s("em"),pi=t("constrained beam-search decoding"),mi=t(` by calling
`),vn=s("a"),gi=t("constrained_beam_search()"),_i=t(", if "),fo=s("code"),ui=t("constraints!=None"),hi=t(` or
`),bo=s("code"),fi=t("force_words_ids!=None"),bi=t("."),xi=l(),m(Je.$$.fragment),ki=l(),gt=s("p"),vi=t("Most of these parameters are explained in more detail in "),_t=s("a"),yi=t(`this blog
post`),ji=t("."),Li=l(),xo=s("p"),Mi=t("Examples:"),wi=l(),ko=s("p"),Ti=t("Greedy Decoding:"),Ei=l(),m(ut.$$.fragment),Oi=l(),vo=s("p"),qi=t("Multinomial Sampling:"),Gi=l(),m(ht.$$.fragment),Si=l(),yo=s("p"),$i=t("Beam-search decoding:"),Fi=l(),m(ft.$$.fragment),zi=l(),ee=s("div"),m(bt.$$.fragment),Ai=l(),xt=s("p"),Pi=t("Generates sequences of token ids for models with a language modeling head using "),jo=s("strong"),Di=t("greedy decoding"),Ni=t(` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Ci=l(),Lo=s("p"),Ii=t("Examples:"),Bi=l(),m(kt.$$.fragment),Wi=l(),te=s("div"),m(vt.$$.fragment),Hi=l(),yt=s("p"),Ri=t("Generates sequences of token ids for models with a language modeling head using "),Mo=s("strong"),Ui=t("multinomial sampling"),Vi=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Ki=l(),wo=s("p"),Zi=t("Examples:"),Xi=l(),m(jt.$$.fragment),Ji=l(),ne=s("div"),m(Lt.$$.fragment),Qi=l(),Mt=s("p"),Yi=t("Generates sequences of token ids for models with a language modeling head using "),To=s("strong"),el=t("beam search decoding"),tl=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),nl=l(),Eo=s("p"),ol=t("Examples:"),sl=l(),m(wt.$$.fragment),al=l(),oe=s("div"),m(Tt.$$.fragment),rl=l(),Et=s("p"),il=t("Generates sequences of token ids for models with a language modeling head using "),Oo=s("strong"),ll=t(`beam search multinomial
sampling`),dl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),cl=l(),qo=s("p"),pl=t("Examples:"),ml=l(),m(Ot.$$.fragment),gl=l(),se=s("div"),m(qt.$$.fragment),_l=l(),Gt=s("p"),ul=t("Generates sequences of token ids for models with a language modeling head using "),Go=s("strong"),hl=t(`diverse beam search
decoding`),fl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),bl=l(),So=s("p"),xl=t("Examples:"),kl=l(),m(St.$$.fragment),vl=l(),ae=s("div"),m($t.$$.fragment),yl=l(),Ft=s("p"),jl=t("Generates sequences of token ids for models with a language modeling head using "),$o=s("strong"),Ll=t(`constrained beam search
decoding`),Ml=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),wl=l(),Fo=s("p"),Tl=t("Examples:"),El=l(),m(zt.$$.fragment),bs=l(),Ee=s("h2"),Qe=s("a"),zo=s("span"),m(At.$$.fragment),Ol=l(),Ao=s("span"),ql=t("TFGenerationMixin"),xs=l(),he=s("div"),m(Pt.$$.fragment),Gl=l(),Dt=s("p"),Sl=t("A class containing all of the functions supporting generation, to be used as a mixin in "),yn=s("a"),$l=t("TFPreTrainedModel"),Fl=t("."),zl=l(),T=s("div"),m(Nt.$$.fragment),Al=l(),Po=s("p"),Pl=t(`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),Dl=l(),Ct=s("p"),Nl=t("Adapted in part from "),It=s("a"),Cl=t(`Facebook\u2019s XLM beam search
code`),Il=t("."),Bl=l(),fe=s("p"),Wl=t("Apart from "),Do=s("code"),Hl=t("input_ids"),Rl=t(" and "),No=s("code"),Ul=t("attention_mask"),Vl=t(`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=s("a"),Kl=t("PretrainedConfig"),Zl=t(` of the model. The default values indicated are the default
values of those config.`),Xl=l(),Bt=s("p"),Jl=t("Most of these parameters are explained in more detail in "),Wt=s("a"),Ql=t(`this blog
post`),Yl=t("."),ed=l(),Co=s("p"),td=t("Examples:"),nd=l(),m(Ht.$$.fragment),ks=l(),Oe=s("h2"),Ye=s("a"),Io=s("span"),m(Rt.$$.fragment),od=l(),Bo=s("span"),sd=t("FlaxGenerationMixin"),vs=l(),z=s("div"),m(Ut.$$.fragment),ad=l(),Vt=s("p"),rd=t(`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Ln=s("a"),id=t("FlaxPreTrainedModel"),ld=t("."),dd=l(),Kt=s("p"),cd=t("The class exposes "),Mn=s("a"),pd=t("generate()"),md=t(", which can be used for:"),gd=l(),qe=s("ul"),re=s("li"),Wo=s("em"),_d=t("greedy decoding"),ud=t(" by calling "),Ho=s("code"),hd=t("_greedy_search()"),fd=t(` if
`),Ro=s("code"),bd=t("num_beams=1"),xd=t(" and "),Uo=s("code"),kd=t("do_sample=False"),vd=t("."),yd=l(),ie=s("li"),Vo=s("em"),jd=t("multinomial sampling"),Ld=t(" by calling "),Ko=s("code"),Md=t("_sample()"),wd=t(" if "),Zo=s("code"),Td=t("num_beams=1"),Ed=t(`
and `),Xo=s("code"),Od=t("do_sample=True"),qd=t("."),Gd=l(),le=s("li"),Jo=s("em"),Sd=t("beam-search decoding"),$d=t(" by calling "),Qo=s("code"),Fd=t("_beam_search"),zd=t(" if "),Yo=s("code"),Ad=t("num_beams>1"),Pd=t(`
and `),es=s("code"),Dd=t("do_sample=False"),Nd=t("."),Cd=l(),E=s("div"),m(Zt.$$.fragment),Id=l(),ts=s("p"),Bd=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Wd=l(),Ge=s("ul"),de=s("li"),ns=s("em"),Hd=t("greedy decoding"),Rd=t(" by calling "),os=s("code"),Ud=t("_greedy_search()"),Vd=t(` if
`),ss=s("code"),Kd=t("num_beams=1"),Zd=t(" and "),as=s("code"),Xd=t("do_sample=False"),Jd=t("."),Qd=l(),ce=s("li"),rs=s("em"),Yd=t("multinomial sampling"),ec=t(" by calling "),is=s("code"),tc=t("_sample()"),nc=t(" if "),ls=s("code"),oc=t("num_beams=1"),sc=t(`
and `),ds=s("code"),ac=t("do_sample=True"),rc=t("."),ic=l(),pe=s("li"),cs=s("em"),lc=t("beam-search decoding"),dc=t(" by calling "),ps=s("code"),cc=t("_beam_search"),pc=t(" if "),ms=s("code"),mc=t("num_beams>1"),gc=t(`
and `),gs=s("code"),_c=t("do_sample=False"),uc=t("."),hc=l(),m(et.$$.fragment),fc=l(),Xt=s("p"),bc=t("Most of these parameters are explained in more detail in "),Jt=s("a"),xc=t(`this blog
post`),kc=t("."),vc=l(),_s=s("p"),yc=t("Examples:"),jc=l(),m(Qt.$$.fragment),this.h()},l(i){const v=Wm('[data-svelte="svelte-1phssyn"]',document.head);p=a(v,"META",{name:!0,content:!0}),v.forEach(o),C=d(i),L=a(i,"H1",{class:!0});var Yt=r(L);q=a(Yt,"A",{id:!0,class:!0,href:!0});var us=r(q);D=a(us,"SPAN",{});var Lc=r(D);g(w.$$.fragment,Lc),Lc.forEach(o),us.forEach(o),ue=d(Yt),N=a(Yt,"SPAN",{});var Mc=r(N);O=n(Mc,"Generation"),Mc.forEach(o),Yt.forEach(o),I=d(i),G=a(i,"P",{});var js=r(G);S=n(js,"Each framework has a generate method for auto-regressive text generation implemented in their respective "),x=a(js,"CODE",{});var wc=r(x);ye=n(wc,"GenerationMixin"),wc.forEach(o),je=n(js," class:"),js.forEach(o),xe=d(i),ke=a(i,"UL",{});var wn=r(ke);Le=a(wn,"LI",{});var Tn=r(Le);Ws=n(Tn,"PyTorch "),tn=a(Tn,"A",{href:!0});var Tc=r(tn);Hs=n(Tc,"generate()"),Tc.forEach(o),Rs=n(Tn," is implemented in "),nn=a(Tn,"A",{href:!0});var Ec=r(nn);Us=n(Ec,"GenerationMixin"),Ec.forEach(o),Vs=n(Tn,"."),Tn.forEach(o),Ks=d(wn),Me=a(wn,"LI",{});var En=r(Me);Zs=n(En,"TensorFlow "),on=a(En,"A",{href:!0});var Oc=r(on);Xs=n(Oc,"generate()"),Oc.forEach(o),Js=n(En," is implemented in "),sn=a(En,"A",{href:!0});var qc=r(sn);Qs=n(qc,"TFGenerationMixin"),qc.forEach(o),Ys=n(En,"."),En.forEach(o),ea=d(wn),we=a(wn,"LI",{});var On=r(we);ta=n(On,"Flax/JAX "),an=a(On,"A",{href:!0});var Gc=r(an);na=n(Gc,"generate()"),Gc.forEach(o),oa=n(On," is implemented in "),rn=a(On,"A",{href:!0});var Sc=r(rn);sa=n(Sc,"FlaxGenerationMixin"),Sc.forEach(o),aa=n(On,"."),On.forEach(o),wn.forEach(o),hs=d(i),Te=a(i,"H2",{class:!0});var Ls=r(Te);Xe=a(Ls,"A",{id:!0,class:!0,href:!0});var $c=r(Xe);$n=a($c,"SPAN",{});var Fc=r($n);g(lt.$$.fragment,Fc),Fc.forEach(o),$c.forEach(o),ra=d(Ls),Fn=a(Ls,"SPAN",{});var zc=r(Fn);ia=n(zc,"GenerationMixin"),zc.forEach(o),Ls.forEach(o),fs=d(i),k=a(i,"DIV",{class:!0});var j=r(k);g(dt.$$.fragment,j),la=d(j),ct=a(j,"P",{});var Ms=r(ct);da=n(Ms,"A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=a(Ms,"A",{href:!0});var Ac=r(ln);ca=n(Ac,"PreTrainedModel"),Ac.forEach(o),pa=n(Ms,"."),Ms.forEach(o),ma=d(j),pt=a(j,"P",{});var ws=r(pt);ga=n(ws,"The class exposes "),dn=a(ws,"A",{href:!0});var Pc=r(dn);_a=n(Pc,"generate()"),Pc.forEach(o),ua=n(ws,", which can be used for:"),ws.forEach(o),ha=d(j),$=a(j,"UL",{});var me=r($);B=a(me,"LI",{});var Se=r(B);zn=a(Se,"EM",{});var Dc=r(zn);fa=n(Dc,"greedy decoding"),Dc.forEach(o),ba=n(Se," by calling "),cn=a(Se,"A",{href:!0});var Nc=r(cn);xa=n(Nc,"greedy_search()"),Nc.forEach(o),ka=n(Se," if "),An=a(Se,"CODE",{});var Cc=r(An);va=n(Cc,"num_beams=1"),Cc.forEach(o),ya=n(Se,` and
`),Pn=a(Se,"CODE",{});var Ic=r(Pn);ja=n(Ic,"do_sample=False"),Ic.forEach(o),La=n(Se,"."),Se.forEach(o),Ma=d(me),W=a(me,"LI",{});var $e=r(W);Dn=a($e,"EM",{});var Bc=r(Dn);wa=n(Bc,"multinomial sampling"),Bc.forEach(o),Ta=n($e," by calling "),pn=a($e,"A",{href:!0});var Wc=r(pn);Ea=n(Wc,"sample()"),Wc.forEach(o),Oa=n($e," if "),Nn=a($e,"CODE",{});var Hc=r(Nn);qa=n(Hc,"num_beams=1"),Hc.forEach(o),Ga=n($e,` and
`),Cn=a($e,"CODE",{});var Rc=r(Cn);Sa=n(Rc,"do_sample=True"),Rc.forEach(o),$a=n($e,"."),$e.forEach(o),Fa=d(me),H=a(me,"LI",{});var Fe=r(H);In=a(Fe,"EM",{});var Uc=r(In);za=n(Uc,"beam-search decoding"),Uc.forEach(o),Aa=n(Fe," by calling "),mn=a(Fe,"A",{href:!0});var Vc=r(mn);Pa=n(Vc,"beam_search()"),Vc.forEach(o),Da=n(Fe," if "),Bn=a(Fe,"CODE",{});var Kc=r(Bn);Na=n(Kc,"num_beams>1"),Kc.forEach(o),Ca=n(Fe,` and
`),Wn=a(Fe,"CODE",{});var Zc=r(Wn);Ia=n(Zc,"do_sample=False"),Zc.forEach(o),Ba=n(Fe,"."),Fe.forEach(o),Wa=d(me),R=a(me,"LI",{});var ze=r(R);Hn=a(ze,"EM",{});var Xc=r(Hn);Ha=n(Xc,"beam-search multinomial sampling"),Xc.forEach(o),Ra=n(ze," by calling "),gn=a(ze,"A",{href:!0});var Jc=r(gn);Ua=n(Jc,"beam_sample()"),Jc.forEach(o),Va=n(ze,` if
`),Rn=a(ze,"CODE",{});var Qc=r(Rn);Ka=n(Qc,"num_beams>1"),Qc.forEach(o),Za=n(ze," and "),Un=a(ze,"CODE",{});var Yc=r(Un);Xa=n(Yc,"do_sample=True"),Yc.forEach(o),Ja=n(ze,"."),ze.forEach(o),Qa=d(me),U=a(me,"LI",{});var Ae=r(U);Vn=a(Ae,"EM",{});var ep=r(Vn);Ya=n(ep,"diverse beam-search decoding"),ep.forEach(o),er=n(Ae," by calling "),_n=a(Ae,"A",{href:!0});var tp=r(_n);tr=n(tp,"group_beam_search()"),tp.forEach(o),nr=n(Ae,`, if
`),Kn=a(Ae,"CODE",{});var np=r(Kn);or=n(np,"num_beams>1"),np.forEach(o),sr=n(Ae," and "),Zn=a(Ae,"CODE",{});var op=r(Zn);ar=n(op,"num_beam_groups>1"),op.forEach(o),rr=n(Ae,"."),Ae.forEach(o),ir=d(me),V=a(me,"LI",{});var Pe=r(V);Xn=a(Pe,"EM",{});var sp=r(Xn);lr=n(sp,"constrained beam-search decoding"),sp.forEach(o),dr=n(Pe," by calling "),un=a(Pe,"A",{href:!0});var ap=r(un);cr=n(ap,"constrained_beam_search()"),ap.forEach(o),pr=n(Pe,`,
if `),Jn=a(Pe,"CODE",{});var rp=r(Jn);mr=n(rp,"constraints!=None"),rp.forEach(o),gr=n(Pe," or "),Qn=a(Pe,"CODE",{});var ip=r(Qn);_r=n(ip,"force_words_ids!=None"),ip.forEach(o),ur=n(Pe,"."),Pe.forEach(o),me.forEach(o),hr=d(j),b=a(j,"DIV",{class:!0});var y=r(b);g(mt.$$.fragment,y),fr=d(y),Yn=a(y,"P",{});var lp=r(Yn);br=n(lp,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),lp.forEach(o),xr=d(y),F=a(y,"UL",{});var ge=r(F);K=a(ge,"LI",{});var De=r(K);eo=a(De,"EM",{});var dp=r(eo);kr=n(dp,"greedy decoding"),dp.forEach(o),vr=n(De," by calling "),hn=a(De,"A",{href:!0});var cp=r(hn);yr=n(cp,"greedy_search()"),cp.forEach(o),jr=n(De," if "),to=a(De,"CODE",{});var pp=r(to);Lr=n(pp,"num_beams=1"),pp.forEach(o),Mr=n(De,` and
`),no=a(De,"CODE",{});var mp=r(no);wr=n(mp,"do_sample=False"),mp.forEach(o),Tr=n(De,"."),De.forEach(o),Er=d(ge),Z=a(ge,"LI",{});var Ne=r(Z);oo=a(Ne,"EM",{});var gp=r(oo);Or=n(gp,"multinomial sampling"),gp.forEach(o),qr=n(Ne," by calling "),fn=a(Ne,"A",{href:!0});var _p=r(fn);Gr=n(_p,"sample()"),_p.forEach(o),Sr=n(Ne," if "),so=a(Ne,"CODE",{});var up=r(so);$r=n(up,"num_beams=1"),up.forEach(o),Fr=n(Ne,` and
`),ao=a(Ne,"CODE",{});var hp=r(ao);zr=n(hp,"do_sample=True"),hp.forEach(o),Ar=n(Ne,"."),Ne.forEach(o),Pr=d(ge),X=a(ge,"LI",{});var Ce=r(X);ro=a(Ce,"EM",{});var fp=r(ro);Dr=n(fp,"beam-search decoding"),fp.forEach(o),Nr=n(Ce," by calling "),bn=a(Ce,"A",{href:!0});var bp=r(bn);Cr=n(bp,"beam_search()"),bp.forEach(o),Ir=n(Ce," if "),io=a(Ce,"CODE",{});var xp=r(io);Br=n(xp,"num_beams>1"),xp.forEach(o),Wr=n(Ce,` and
`),lo=a(Ce,"CODE",{});var kp=r(lo);Hr=n(kp,"do_sample=False"),kp.forEach(o),Rr=n(Ce,"."),Ce.forEach(o),Ur=d(ge),J=a(ge,"LI",{});var Ie=r(J);co=a(Ie,"EM",{});var vp=r(co);Vr=n(vp,"beam-search multinomial sampling"),vp.forEach(o),Kr=n(Ie," by calling "),xn=a(Ie,"A",{href:!0});var yp=r(xn);Zr=n(yp,"beam_sample()"),yp.forEach(o),Xr=n(Ie,` if
`),po=a(Ie,"CODE",{});var jp=r(po);Jr=n(jp,"num_beams>1"),jp.forEach(o),Qr=n(Ie," and "),mo=a(Ie,"CODE",{});var Lp=r(mo);Yr=n(Lp,"do_sample=True"),Lp.forEach(o),ei=n(Ie,"."),Ie.forEach(o),ti=d(ge),Q=a(ge,"LI",{});var Be=r(Q);go=a(Be,"EM",{});var Mp=r(go);ni=n(Mp,"diverse beam-search decoding"),Mp.forEach(o),oi=n(Be," by calling "),kn=a(Be,"A",{href:!0});var wp=r(kn);si=n(wp,"group_beam_search()"),wp.forEach(o),ai=n(Be,`, if
`),_o=a(Be,"CODE",{});var Tp=r(_o);ri=n(Tp,"num_beams>1"),Tp.forEach(o),ii=n(Be," and "),uo=a(Be,"CODE",{});var Ep=r(uo);li=n(Ep,"num_beam_groups>1"),Ep.forEach(o),di=n(Be,"."),Be.forEach(o),ci=d(ge),Y=a(ge,"LI",{});var We=r(Y);ho=a(We,"EM",{});var Op=r(ho);pi=n(Op,"constrained beam-search decoding"),Op.forEach(o),mi=n(We,` by calling
`),vn=a(We,"A",{href:!0});var qp=r(vn);gi=n(qp,"constrained_beam_search()"),qp.forEach(o),_i=n(We,", if "),fo=a(We,"CODE",{});var Gp=r(fo);ui=n(Gp,"constraints!=None"),Gp.forEach(o),hi=n(We,` or
`),bo=a(We,"CODE",{});var Sp=r(bo);fi=n(Sp,"force_words_ids!=None"),Sp.forEach(o),bi=n(We,"."),We.forEach(o),ge.forEach(o),xi=d(y),g(Je.$$.fragment,y),ki=d(y),gt=a(y,"P",{});var Ts=r(gt);vi=n(Ts,"Most of these parameters are explained in more detail in "),_t=a(Ts,"A",{href:!0,rel:!0});var $p=r(_t);yi=n($p,`this blog
post`),$p.forEach(o),ji=n(Ts,"."),Ts.forEach(o),Li=d(y),xo=a(y,"P",{});var Fp=r(xo);Mi=n(Fp,"Examples:"),Fp.forEach(o),wi=d(y),ko=a(y,"P",{});var zp=r(ko);Ti=n(zp,"Greedy Decoding:"),zp.forEach(o),Ei=d(y),g(ut.$$.fragment,y),Oi=d(y),vo=a(y,"P",{});var Ap=r(vo);qi=n(Ap,"Multinomial Sampling:"),Ap.forEach(o),Gi=d(y),g(ht.$$.fragment,y),Si=d(y),yo=a(y,"P",{});var Pp=r(yo);$i=n(Pp,"Beam-search decoding:"),Pp.forEach(o),Fi=d(y),g(ft.$$.fragment,y),y.forEach(o),zi=d(j),ee=a(j,"DIV",{class:!0});var tt=r(ee);g(bt.$$.fragment,tt),Ai=d(tt),xt=a(tt,"P",{});var Es=r(xt);Pi=n(Es,"Generates sequences of token ids for models with a language modeling head using "),jo=a(Es,"STRONG",{});var Dp=r(jo);Di=n(Dp,"greedy decoding"),Dp.forEach(o),Ni=n(Es,` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Es.forEach(o),Ci=d(tt),Lo=a(tt,"P",{});var Np=r(Lo);Ii=n(Np,"Examples:"),Np.forEach(o),Bi=d(tt),g(kt.$$.fragment,tt),tt.forEach(o),Wi=d(j),te=a(j,"DIV",{class:!0});var nt=r(te);g(vt.$$.fragment,nt),Hi=d(nt),yt=a(nt,"P",{});var Os=r(yt);Ri=n(Os,"Generates sequences of token ids for models with a language modeling head using "),Mo=a(Os,"STRONG",{});var Cp=r(Mo);Ui=n(Cp,"multinomial sampling"),Cp.forEach(o),Vi=n(Os,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Os.forEach(o),Ki=d(nt),wo=a(nt,"P",{});var Ip=r(wo);Zi=n(Ip,"Examples:"),Ip.forEach(o),Xi=d(nt),g(jt.$$.fragment,nt),nt.forEach(o),Ji=d(j),ne=a(j,"DIV",{class:!0});var ot=r(ne);g(Lt.$$.fragment,ot),Qi=d(ot),Mt=a(ot,"P",{});var qs=r(Mt);Yi=n(qs,"Generates sequences of token ids for models with a language modeling head using "),To=a(qs,"STRONG",{});var Bp=r(To);el=n(Bp,"beam search decoding"),Bp.forEach(o),tl=n(qs,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),qs.forEach(o),nl=d(ot),Eo=a(ot,"P",{});var Wp=r(Eo);ol=n(Wp,"Examples:"),Wp.forEach(o),sl=d(ot),g(wt.$$.fragment,ot),ot.forEach(o),al=d(j),oe=a(j,"DIV",{class:!0});var st=r(oe);g(Tt.$$.fragment,st),rl=d(st),Et=a(st,"P",{});var Gs=r(Et);il=n(Gs,"Generates sequences of token ids for models with a language modeling head using "),Oo=a(Gs,"STRONG",{});var Hp=r(Oo);ll=n(Hp,`beam search multinomial
sampling`),Hp.forEach(o),dl=n(Gs," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Gs.forEach(o),cl=d(st),qo=a(st,"P",{});var Rp=r(qo);pl=n(Rp,"Examples:"),Rp.forEach(o),ml=d(st),g(Ot.$$.fragment,st),st.forEach(o),gl=d(j),se=a(j,"DIV",{class:!0});var at=r(se);g(qt.$$.fragment,at),_l=d(at),Gt=a(at,"P",{});var Ss=r(Gt);ul=n(Ss,"Generates sequences of token ids for models with a language modeling head using "),Go=a(Ss,"STRONG",{});var Up=r(Go);hl=n(Up,`diverse beam search
decoding`),Up.forEach(o),fl=n(Ss," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Ss.forEach(o),bl=d(at),So=a(at,"P",{});var Vp=r(So);xl=n(Vp,"Examples:"),Vp.forEach(o),kl=d(at),g(St.$$.fragment,at),at.forEach(o),vl=d(j),ae=a(j,"DIV",{class:!0});var rt=r(ae);g($t.$$.fragment,rt),yl=d(rt),Ft=a(rt,"P",{});var $s=r(Ft);jl=n($s,"Generates sequences of token ids for models with a language modeling head using "),$o=a($s,"STRONG",{});var Kp=r($o);Ll=n(Kp,`constrained beam search
decoding`),Kp.forEach(o),Ml=n($s," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),$s.forEach(o),wl=d(rt),Fo=a(rt,"P",{});var Zp=r(Fo);Tl=n(Zp,"Examples:"),Zp.forEach(o),El=d(rt),g(zt.$$.fragment,rt),rt.forEach(o),j.forEach(o),bs=d(i),Ee=a(i,"H2",{class:!0});var Fs=r(Ee);Qe=a(Fs,"A",{id:!0,class:!0,href:!0});var Xp=r(Qe);zo=a(Xp,"SPAN",{});var Jp=r(zo);g(At.$$.fragment,Jp),Jp.forEach(o),Xp.forEach(o),Ol=d(Fs),Ao=a(Fs,"SPAN",{});var Qp=r(Ao);ql=n(Qp,"TFGenerationMixin"),Qp.forEach(o),Fs.forEach(o),xs=d(i),he=a(i,"DIV",{class:!0});var qn=r(he);g(Pt.$$.fragment,qn),Gl=d(qn),Dt=a(qn,"P",{});var zs=r(Dt);Sl=n(zs,"A class containing all of the functions supporting generation, to be used as a mixin in "),yn=a(zs,"A",{href:!0});var Yp=r(yn);$l=n(Yp,"TFPreTrainedModel"),Yp.forEach(o),Fl=n(zs,"."),zs.forEach(o),zl=d(qn),T=a(qn,"DIV",{class:!0});var A=r(T);g(Nt.$$.fragment,A),Al=d(A),Po=a(A,"P",{});var em=r(Po);Pl=n(em,`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),em.forEach(o),Dl=d(A),Ct=a(A,"P",{});var As=r(Ct);Nl=n(As,"Adapted in part from "),It=a(As,"A",{href:!0,rel:!0});var tm=r(It);Cl=n(tm,`Facebook\u2019s XLM beam search
code`),tm.forEach(o),Il=n(As,"."),As.forEach(o),Bl=d(A),fe=a(A,"P",{});var it=r(fe);Wl=n(it,"Apart from "),Do=a(it,"CODE",{});var nm=r(Do);Hl=n(nm,"input_ids"),nm.forEach(o),Rl=n(it," and "),No=a(it,"CODE",{});var om=r(No);Ul=n(om,"attention_mask"),om.forEach(o),Vl=n(it,`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=a(it,"A",{href:!0});var sm=r(jn);Kl=n(sm,"PretrainedConfig"),sm.forEach(o),Zl=n(it,` of the model. The default values indicated are the default
values of those config.`),it.forEach(o),Xl=d(A),Bt=a(A,"P",{});var Ps=r(Bt);Jl=n(Ps,"Most of these parameters are explained in more detail in "),Wt=a(Ps,"A",{href:!0,rel:!0});var am=r(Wt);Ql=n(am,`this blog
post`),am.forEach(o),Yl=n(Ps,"."),Ps.forEach(o),ed=d(A),Co=a(A,"P",{});var rm=r(Co);td=n(rm,"Examples:"),rm.forEach(o),nd=d(A),g(Ht.$$.fragment,A),A.forEach(o),qn.forEach(o),ks=d(i),Oe=a(i,"H2",{class:!0});var Ds=r(Oe);Ye=a(Ds,"A",{id:!0,class:!0,href:!0});var im=r(Ye);Io=a(im,"SPAN",{});var lm=r(Io);g(Rt.$$.fragment,lm),lm.forEach(o),im.forEach(o),od=d(Ds),Bo=a(Ds,"SPAN",{});var dm=r(Bo);sd=n(dm,"FlaxGenerationMixin"),dm.forEach(o),Ds.forEach(o),vs=d(i),z=a(i,"DIV",{class:!0});var ve=r(z);g(Ut.$$.fragment,ve),ad=d(ve),Vt=a(ve,"P",{});var Ns=r(Vt);rd=n(Ns,`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Ln=a(Ns,"A",{href:!0});var cm=r(Ln);id=n(cm,"FlaxPreTrainedModel"),cm.forEach(o),ld=n(Ns,"."),Ns.forEach(o),dd=d(ve),Kt=a(ve,"P",{});var Cs=r(Kt);cd=n(Cs,"The class exposes "),Mn=a(Cs,"A",{href:!0});var pm=r(Mn);pd=n(pm,"generate()"),pm.forEach(o),md=n(Cs,", which can be used for:"),Cs.forEach(o),gd=d(ve),qe=a(ve,"UL",{});var Gn=r(qe);re=a(Gn,"LI",{});var He=r(re);Wo=a(He,"EM",{});var mm=r(Wo);_d=n(mm,"greedy decoding"),mm.forEach(o),ud=n(He," by calling "),Ho=a(He,"CODE",{});var gm=r(Ho);hd=n(gm,"_greedy_search()"),gm.forEach(o),fd=n(He,` if
`),Ro=a(He,"CODE",{});var _m=r(Ro);bd=n(_m,"num_beams=1"),_m.forEach(o),xd=n(He," and "),Uo=a(He,"CODE",{});var um=r(Uo);kd=n(um,"do_sample=False"),um.forEach(o),vd=n(He,"."),He.forEach(o),yd=d(Gn),ie=a(Gn,"LI",{});var Re=r(ie);Vo=a(Re,"EM",{});var hm=r(Vo);jd=n(hm,"multinomial sampling"),hm.forEach(o),Ld=n(Re," by calling "),Ko=a(Re,"CODE",{});var fm=r(Ko);Md=n(fm,"_sample()"),fm.forEach(o),wd=n(Re," if "),Zo=a(Re,"CODE",{});var bm=r(Zo);Td=n(bm,"num_beams=1"),bm.forEach(o),Ed=n(Re,`
and `),Xo=a(Re,"CODE",{});var xm=r(Xo);Od=n(xm,"do_sample=True"),xm.forEach(o),qd=n(Re,"."),Re.forEach(o),Gd=d(Gn),le=a(Gn,"LI",{});var Ue=r(le);Jo=a(Ue,"EM",{});var km=r(Jo);Sd=n(km,"beam-search decoding"),km.forEach(o),$d=n(Ue," by calling "),Qo=a(Ue,"CODE",{});var vm=r(Qo);Fd=n(vm,"_beam_search"),vm.forEach(o),zd=n(Ue," if "),Yo=a(Ue,"CODE",{});var ym=r(Yo);Ad=n(ym,"num_beams>1"),ym.forEach(o),Pd=n(Ue,`
and `),es=a(Ue,"CODE",{});var jm=r(es);Dd=n(jm,"do_sample=False"),jm.forEach(o),Nd=n(Ue,"."),Ue.forEach(o),Gn.forEach(o),Cd=d(ve),E=a(ve,"DIV",{class:!0});var P=r(E);g(Zt.$$.fragment,P),Id=d(P),ts=a(P,"P",{});var Lm=r(ts);Bd=n(Lm,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Lm.forEach(o),Wd=d(P),Ge=a(P,"UL",{});var Sn=r(Ge);de=a(Sn,"LI",{});var Ve=r(de);ns=a(Ve,"EM",{});var Mm=r(ns);Hd=n(Mm,"greedy decoding"),Mm.forEach(o),Rd=n(Ve," by calling "),os=a(Ve,"CODE",{});var wm=r(os);Ud=n(wm,"_greedy_search()"),wm.forEach(o),Vd=n(Ve,` if
`),ss=a(Ve,"CODE",{});var Tm=r(ss);Kd=n(Tm,"num_beams=1"),Tm.forEach(o),Zd=n(Ve," and "),as=a(Ve,"CODE",{});var Em=r(as);Xd=n(Em,"do_sample=False"),Em.forEach(o),Jd=n(Ve,"."),Ve.forEach(o),Qd=d(Sn),ce=a(Sn,"LI",{});var Ke=r(ce);rs=a(Ke,"EM",{});var Om=r(rs);Yd=n(Om,"multinomial sampling"),Om.forEach(o),ec=n(Ke," by calling "),is=a(Ke,"CODE",{});var qm=r(is);tc=n(qm,"_sample()"),qm.forEach(o),nc=n(Ke," if "),ls=a(Ke,"CODE",{});var Gm=r(ls);oc=n(Gm,"num_beams=1"),Gm.forEach(o),sc=n(Ke,`
and `),ds=a(Ke,"CODE",{});var Sm=r(ds);ac=n(Sm,"do_sample=True"),Sm.forEach(o),rc=n(Ke,"."),Ke.forEach(o),ic=d(Sn),pe=a(Sn,"LI",{});var Ze=r(pe);cs=a(Ze,"EM",{});var $m=r(cs);lc=n($m,"beam-search decoding"),$m.forEach(o),dc=n(Ze," by calling "),ps=a(Ze,"CODE",{});var Fm=r(ps);cc=n(Fm,"_beam_search"),Fm.forEach(o),pc=n(Ze," if "),ms=a(Ze,"CODE",{});var zm=r(ms);mc=n(zm,"num_beams>1"),zm.forEach(o),gc=n(Ze,`
and `),gs=a(Ze,"CODE",{});var Am=r(gs);_c=n(Am,"do_sample=False"),Am.forEach(o),uc=n(Ze,"."),Ze.forEach(o),Sn.forEach(o),hc=d(P),g(et.$$.fragment,P),fc=d(P),Xt=a(P,"P",{});var Is=r(Xt);bc=n(Is,"Most of these parameters are explained in more detail in "),Jt=a(Is,"A",{href:!0,rel:!0});var Pm=r(Jt);xc=n(Pm,`this blog
post`),Pm.forEach(o),kc=n(Is,"."),Is.forEach(o),vc=d(P),_s=a(P,"P",{});var Dm=r(_s);yc=n(Dm,"Examples:"),Dm.forEach(o),jc=d(P),g(Qt.$$.fragment,P),P.forEach(o),ve.forEach(o),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Km)),c(q,"id","generation"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#generation"),c(L,"class","relative group"),c(tn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(nn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin"),c(on,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin.generate"),c(sn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin"),c(an,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(rn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Xe,"id","transformers.generation_utils.GenerationMixin"),c(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xe,"href","#transformers.generation_utils.GenerationMixin"),c(Te,"class","relative group"),c(ln,"href","/docs/transformers/pr_16887/en/main_classes/model#transformers.PreTrainedModel"),c(dn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(cn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(pn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(mn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(gn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(_n,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(un,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(hn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(fn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(bn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(xn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(kn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(vn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(_t,"href","https://huggingface.co/blog/how-to-generate"),c(_t,"rel","nofollow"),c(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qe,"id","transformers.generation_tf_utils.TFGenerationMixin"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#transformers.generation_tf_utils.TFGenerationMixin"),c(Ee,"class","relative group"),c(yn,"href","/docs/transformers/pr_16887/en/main_classes/model#transformers.TFPreTrainedModel"),c(It,"href","https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529"),c(It,"rel","nofollow"),c(jn,"href","/docs/transformers/pr_16887/en/main_classes/configuration#transformers.PretrainedConfig"),c(Wt,"href","https://huggingface.co/blog/how-to-generate"),c(Wt,"rel","nofollow"),c(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ye,"id","transformers.generation_flax_utils.FlaxGenerationMixin"),c(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ye,"href","#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Oe,"class","relative group"),c(Ln,"href","/docs/transformers/pr_16887/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Mn,"href","/docs/transformers/pr_16887/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(Jt,"href","https://huggingface.co/blog/how-to-generate"),c(Jt,"rel","nofollow"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(i,v){e(document.head,p),M(i,C,v),M(i,L,v),e(L,q),e(q,D),_(w,D,null),e(L,ue),e(L,N),e(N,O),M(i,I,v),M(i,G,v),e(G,S),e(G,x),e(x,ye),e(G,je),M(i,xe,v),M(i,ke,v),e(ke,Le),e(Le,Ws),e(Le,tn),e(tn,Hs),e(Le,Rs),e(Le,nn),e(nn,Us),e(Le,Vs),e(ke,Ks),e(ke,Me),e(Me,Zs),e(Me,on),e(on,Xs),e(Me,Js),e(Me,sn),e(sn,Qs),e(Me,Ys),e(ke,ea),e(ke,we),e(we,ta),e(we,an),e(an,na),e(we,oa),e(we,rn),e(rn,sa),e(we,aa),M(i,hs,v),M(i,Te,v),e(Te,Xe),e(Xe,$n),_(lt,$n,null),e(Te,ra),e(Te,Fn),e(Fn,ia),M(i,fs,v),M(i,k,v),_(dt,k,null),e(k,la),e(k,ct),e(ct,da),e(ct,ln),e(ln,ca),e(ct,pa),e(k,ma),e(k,pt),e(pt,ga),e(pt,dn),e(dn,_a),e(pt,ua),e(k,ha),e(k,$),e($,B),e(B,zn),e(zn,fa),e(B,ba),e(B,cn),e(cn,xa),e(B,ka),e(B,An),e(An,va),e(B,ya),e(B,Pn),e(Pn,ja),e(B,La),e($,Ma),e($,W),e(W,Dn),e(Dn,wa),e(W,Ta),e(W,pn),e(pn,Ea),e(W,Oa),e(W,Nn),e(Nn,qa),e(W,Ga),e(W,Cn),e(Cn,Sa),e(W,$a),e($,Fa),e($,H),e(H,In),e(In,za),e(H,Aa),e(H,mn),e(mn,Pa),e(H,Da),e(H,Bn),e(Bn,Na),e(H,Ca),e(H,Wn),e(Wn,Ia),e(H,Ba),e($,Wa),e($,R),e(R,Hn),e(Hn,Ha),e(R,Ra),e(R,gn),e(gn,Ua),e(R,Va),e(R,Rn),e(Rn,Ka),e(R,Za),e(R,Un),e(Un,Xa),e(R,Ja),e($,Qa),e($,U),e(U,Vn),e(Vn,Ya),e(U,er),e(U,_n),e(_n,tr),e(U,nr),e(U,Kn),e(Kn,or),e(U,sr),e(U,Zn),e(Zn,ar),e(U,rr),e($,ir),e($,V),e(V,Xn),e(Xn,lr),e(V,dr),e(V,un),e(un,cr),e(V,pr),e(V,Jn),e(Jn,mr),e(V,gr),e(V,Qn),e(Qn,_r),e(V,ur),e(k,hr),e(k,b),_(mt,b,null),e(b,fr),e(b,Yn),e(Yn,br),e(b,xr),e(b,F),e(F,K),e(K,eo),e(eo,kr),e(K,vr),e(K,hn),e(hn,yr),e(K,jr),e(K,to),e(to,Lr),e(K,Mr),e(K,no),e(no,wr),e(K,Tr),e(F,Er),e(F,Z),e(Z,oo),e(oo,Or),e(Z,qr),e(Z,fn),e(fn,Gr),e(Z,Sr),e(Z,so),e(so,$r),e(Z,Fr),e(Z,ao),e(ao,zr),e(Z,Ar),e(F,Pr),e(F,X),e(X,ro),e(ro,Dr),e(X,Nr),e(X,bn),e(bn,Cr),e(X,Ir),e(X,io),e(io,Br),e(X,Wr),e(X,lo),e(lo,Hr),e(X,Rr),e(F,Ur),e(F,J),e(J,co),e(co,Vr),e(J,Kr),e(J,xn),e(xn,Zr),e(J,Xr),e(J,po),e(po,Jr),e(J,Qr),e(J,mo),e(mo,Yr),e(J,ei),e(F,ti),e(F,Q),e(Q,go),e(go,ni),e(Q,oi),e(Q,kn),e(kn,si),e(Q,ai),e(Q,_o),e(_o,ri),e(Q,ii),e(Q,uo),e(uo,li),e(Q,di),e(F,ci),e(F,Y),e(Y,ho),e(ho,pi),e(Y,mi),e(Y,vn),e(vn,gi),e(Y,_i),e(Y,fo),e(fo,ui),e(Y,hi),e(Y,bo),e(bo,fi),e(Y,bi),e(b,xi),_(Je,b,null),e(b,ki),e(b,gt),e(gt,vi),e(gt,_t),e(_t,yi),e(gt,ji),e(b,Li),e(b,xo),e(xo,Mi),e(b,wi),e(b,ko),e(ko,Ti),e(b,Ei),_(ut,b,null),e(b,Oi),e(b,vo),e(vo,qi),e(b,Gi),_(ht,b,null),e(b,Si),e(b,yo),e(yo,$i),e(b,Fi),_(ft,b,null),e(k,zi),e(k,ee),_(bt,ee,null),e(ee,Ai),e(ee,xt),e(xt,Pi),e(xt,jo),e(jo,Di),e(xt,Ni),e(ee,Ci),e(ee,Lo),e(Lo,Ii),e(ee,Bi),_(kt,ee,null),e(k,Wi),e(k,te),_(vt,te,null),e(te,Hi),e(te,yt),e(yt,Ri),e(yt,Mo),e(Mo,Ui),e(yt,Vi),e(te,Ki),e(te,wo),e(wo,Zi),e(te,Xi),_(jt,te,null),e(k,Ji),e(k,ne),_(Lt,ne,null),e(ne,Qi),e(ne,Mt),e(Mt,Yi),e(Mt,To),e(To,el),e(Mt,tl),e(ne,nl),e(ne,Eo),e(Eo,ol),e(ne,sl),_(wt,ne,null),e(k,al),e(k,oe),_(Tt,oe,null),e(oe,rl),e(oe,Et),e(Et,il),e(Et,Oo),e(Oo,ll),e(Et,dl),e(oe,cl),e(oe,qo),e(qo,pl),e(oe,ml),_(Ot,oe,null),e(k,gl),e(k,se),_(qt,se,null),e(se,_l),e(se,Gt),e(Gt,ul),e(Gt,Go),e(Go,hl),e(Gt,fl),e(se,bl),e(se,So),e(So,xl),e(se,kl),_(St,se,null),e(k,vl),e(k,ae),_($t,ae,null),e(ae,yl),e(ae,Ft),e(Ft,jl),e(Ft,$o),e($o,Ll),e(Ft,Ml),e(ae,wl),e(ae,Fo),e(Fo,Tl),e(ae,El),_(zt,ae,null),M(i,bs,v),M(i,Ee,v),e(Ee,Qe),e(Qe,zo),_(At,zo,null),e(Ee,Ol),e(Ee,Ao),e(Ao,ql),M(i,xs,v),M(i,he,v),_(Pt,he,null),e(he,Gl),e(he,Dt),e(Dt,Sl),e(Dt,yn),e(yn,$l),e(Dt,Fl),e(he,zl),e(he,T),_(Nt,T,null),e(T,Al),e(T,Po),e(Po,Pl),e(T,Dl),e(T,Ct),e(Ct,Nl),e(Ct,It),e(It,Cl),e(Ct,Il),e(T,Bl),e(T,fe),e(fe,Wl),e(fe,Do),e(Do,Hl),e(fe,Rl),e(fe,No),e(No,Ul),e(fe,Vl),e(fe,jn),e(jn,Kl),e(fe,Zl),e(T,Xl),e(T,Bt),e(Bt,Jl),e(Bt,Wt),e(Wt,Ql),e(Bt,Yl),e(T,ed),e(T,Co),e(Co,td),e(T,nd),_(Ht,T,null),M(i,ks,v),M(i,Oe,v),e(Oe,Ye),e(Ye,Io),_(Rt,Io,null),e(Oe,od),e(Oe,Bo),e(Bo,sd),M(i,vs,v),M(i,z,v),_(Ut,z,null),e(z,ad),e(z,Vt),e(Vt,rd),e(Vt,Ln),e(Ln,id),e(Vt,ld),e(z,dd),e(z,Kt),e(Kt,cd),e(Kt,Mn),e(Mn,pd),e(Kt,md),e(z,gd),e(z,qe),e(qe,re),e(re,Wo),e(Wo,_d),e(re,ud),e(re,Ho),e(Ho,hd),e(re,fd),e(re,Ro),e(Ro,bd),e(re,xd),e(re,Uo),e(Uo,kd),e(re,vd),e(qe,yd),e(qe,ie),e(ie,Vo),e(Vo,jd),e(ie,Ld),e(ie,Ko),e(Ko,Md),e(ie,wd),e(ie,Zo),e(Zo,Td),e(ie,Ed),e(ie,Xo),e(Xo,Od),e(ie,qd),e(qe,Gd),e(qe,le),e(le,Jo),e(Jo,Sd),e(le,$d),e(le,Qo),e(Qo,Fd),e(le,zd),e(le,Yo),e(Yo,Ad),e(le,Pd),e(le,es),e(es,Dd),e(le,Nd),e(z,Cd),e(z,E),_(Zt,E,null),e(E,Id),e(E,ts),e(ts,Bd),e(E,Wd),e(E,Ge),e(Ge,de),e(de,ns),e(ns,Hd),e(de,Rd),e(de,os),e(os,Ud),e(de,Vd),e(de,ss),e(ss,Kd),e(de,Zd),e(de,as),e(as,Xd),e(de,Jd),e(Ge,Qd),e(Ge,ce),e(ce,rs),e(rs,Yd),e(ce,ec),e(ce,is),e(is,tc),e(ce,nc),e(ce,ls),e(ls,oc),e(ce,sc),e(ce,ds),e(ds,ac),e(ce,rc),e(Ge,ic),e(Ge,pe),e(pe,cs),e(cs,lc),e(pe,dc),e(pe,ps),e(ps,cc),e(pe,pc),e(pe,ms),e(ms,mc),e(pe,gc),e(pe,gs),e(gs,_c),e(pe,uc),e(E,hc),_(et,E,null),e(E,fc),e(E,Xt),e(Xt,bc),e(Xt,Jt),e(Jt,xc),e(Xt,kc),e(E,vc),e(E,_s),e(_s,yc),e(E,jc),_(Qt,E,null),ys=!0},p(i,[v]){const Yt={};v&2&&(Yt.$$scope={dirty:v,ctx:i}),Je.$set(Yt);const us={};v&2&&(us.$$scope={dirty:v,ctx:i}),et.$set(us)},i(i){ys||(u(w.$$.fragment,i),u(lt.$$.fragment,i),u(dt.$$.fragment,i),u(mt.$$.fragment,i),u(Je.$$.fragment,i),u(ut.$$.fragment,i),u(ht.$$.fragment,i),u(ft.$$.fragment,i),u(bt.$$.fragment,i),u(kt.$$.fragment,i),u(vt.$$.fragment,i),u(jt.$$.fragment,i),u(Lt.$$.fragment,i),u(wt.$$.fragment,i),u(Tt.$$.fragment,i),u(Ot.$$.fragment,i),u(qt.$$.fragment,i),u(St.$$.fragment,i),u($t.$$.fragment,i),u(zt.$$.fragment,i),u(At.$$.fragment,i),u(Pt.$$.fragment,i),u(Nt.$$.fragment,i),u(Ht.$$.fragment,i),u(Rt.$$.fragment,i),u(Ut.$$.fragment,i),u(Zt.$$.fragment,i),u(et.$$.fragment,i),u(Qt.$$.fragment,i),ys=!0)},o(i){h(w.$$.fragment,i),h(lt.$$.fragment,i),h(dt.$$.fragment,i),h(mt.$$.fragment,i),h(Je.$$.fragment,i),h(ut.$$.fragment,i),h(ht.$$.fragment,i),h(ft.$$.fragment,i),h(bt.$$.fragment,i),h(kt.$$.fragment,i),h(vt.$$.fragment,i),h(jt.$$.fragment,i),h(Lt.$$.fragment,i),h(wt.$$.fragment,i),h(Tt.$$.fragment,i),h(Ot.$$.fragment,i),h(qt.$$.fragment,i),h(St.$$.fragment,i),h($t.$$.fragment,i),h(zt.$$.fragment,i),h(At.$$.fragment,i),h(Pt.$$.fragment,i),h(Nt.$$.fragment,i),h(Ht.$$.fragment,i),h(Rt.$$.fragment,i),h(Ut.$$.fragment,i),h(Zt.$$.fragment,i),h(et.$$.fragment,i),h(Qt.$$.fragment,i),ys=!1},d(i){o(p),i&&o(C),i&&o(L),f(w),i&&o(I),i&&o(G),i&&o(xe),i&&o(ke),i&&o(hs),i&&o(Te),f(lt),i&&o(fs),i&&o(k),f(dt),f(mt),f(Je),f(ut),f(ht),f(ft),f(bt),f(kt),f(vt),f(jt),f(Lt),f(wt),f(Tt),f(Ot),f(qt),f(St),f($t),f(zt),i&&o(bs),i&&o(Ee),f(At),i&&o(xs),i&&o(he),f(Pt),f(Nt),f(Ht),i&&o(ks),i&&o(Oe),f(Rt),i&&o(vs),i&&o(z),f(Ut),f(Zt),f(et),f(Qt)}}}const Km={local:"generation",sections:[{local:"transformers.generation_utils.GenerationMixin",title:"GenerationMixin"},{local:"transformers.generation_tf_utils.TFGenerationMixin",title:"TFGenerationMixin"},{local:"transformers.generation_flax_utils.FlaxGenerationMixin",title:"FlaxGenerationMixin"}],title:"Generation"};function Zm(en){return Hm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class tg extends Cm{constructor(p){super();Im(this,p,Zm,Vm,Bm,{})}}export{tg as default,Km as metadata};
