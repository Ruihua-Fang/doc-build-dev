import{S as td,i as od,s as nd,e as n,k as d,w as _,t as r,M as sd,c as s,d as o,m as c,a,x as T,h as i,b as l,F as e,g as u,y as v,q as y,o as b,B as k,v as ad}from"../../chunks/vendor-6b77c823.js";import{T as dn}from"../../chunks/Tip-39098574.js";import{D as V}from"../../chunks/Docstring-abef54e3.js";import{C as oe}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Te}from"../../chunks/IconCopyLink-7a11ce68.js";function rd(W){let p,N,m,P,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),P=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var G=a(m);P=i(G,"Module"),G.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,P),e(p,w)},d(g){g&&o(p)}}}function id(W){let p,N,m,P,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),P=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var G=a(m);P=i(G,"Module"),G.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,P),e(p,w)},d(g){g&&o(p)}}}function ld(W){let p,N,m,P,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),P=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var G=a(m);P=i(G,"Module"),G.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,P),e(p,w)},d(g){g&&o(p)}}}function dd(W){let p,N,m,P,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),P=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var G=a(m);P=i(G,"Module"),G.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,P),e(p,w)},d(g){g&&o(p)}}}function cd(W){let p,N,m,P,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),P=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var G=a(m);P=i(G,"Module"),G.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,P),e(p,w)},d(g){g&&o(p)}}}function pd(W){let p,N,m,P,w,g,f,G,Xn,cn,ne,ve,lo,Ae,Kn,co,Qn,pn,J,Yn,Ie,Zn,es,Le,ts,os,hn,Ot,ns,un,ye,ss,Se,as,rs,mn,se,be,po,Oe,is,ho,ls,fn,ke,ds,uo,cs,ps,gn,De,_n,ae,Pe,mo,Be,hs,fo,us,Tn,C,We,ms,re,fs,Dt,gs,_s,He,Ts,vs,ys,ie,bs,Bt,ks,Ps,Wt,ws,Ns,$s,go,Gs,xs,Ue,vn,le,we,_o,Ve,Fs,To,Ms,yn,z,Je,Es,vo,Cs,zs,Re,qs,Ht,js,As,Is,Xe,Ls,Ke,Ss,Os,Ds,j,Qe,Bs,de,Ws,Ut,Hs,Us,yo,Vs,Js,Rs,Ne,Xs,bo,Ks,Qs,Ye,bn,ce,$e,ko,Ze,Ys,Po,Zs,kn,q,et,ea,wo,ta,oa,tt,na,Vt,sa,aa,ra,ot,ia,nt,la,da,ca,A,st,pa,pe,ha,Jt,ua,ma,No,fa,ga,_a,Ge,Ta,$o,va,ya,at,Pn,he,xe,Go,rt,ba,xo,ka,wn,x,it,Pa,Fo,wa,Na,Rt,Xt,$a,Ga,xa,B,Fa,Mo,Ma,Ea,Eo,Ca,za,Co,qa,ja,zo,Aa,Ia,La,lt,Sa,Kt,Oa,Da,Ba,dt,Wa,ct,Ha,Ua,Va,$,pt,Ja,ue,Ra,Qt,Xa,Ka,qo,Qa,Ya,Za,Fe,er,jo,tr,or,ht,nr,ut,sr,Ao,ar,rr,mt,ir,ft,Nn,me,Me,Io,gt,lr,Lo,dr,$n,F,_t,cr,So,pr,hr,Tt,ur,Yt,mr,fr,gr,vt,_r,yt,Tr,vr,yr,Oo,br,kr,H,Do,bt,Pr,wr,Bo,kt,Nr,$r,Wo,Pt,Gr,xr,Ho,wt,Fr,Mr,I,Nt,Er,fe,Cr,Uo,zr,qr,Vo,jr,Ar,Ir,Ee,Lr,Jo,Sr,Or,$t,Gn,ge,Ce,Ro,Gt,Dr,Xo,Br,xn,M,xt,Wr,Ko,Hr,Ur,Ft,Vr,Zt,Jr,Rr,Xr,Mt,Kr,Et,Qr,Yr,Zr,Qo,ei,ti,U,Yo,Ct,oi,ni,Zo,zt,si,ai,en,qt,ri,ii,tn,jt,li,di,L,At,ci,_e,pi,on,hi,ui,nn,mi,fi,gi,ze,_i,sn,Ti,vi,It,Fn;return g=new Te({}),Ae=new Te({}),Oe=new Te({}),De=new oe({props:{code:`from transformers import GPTNeoForCausalLM, GPT2Tokenizer

model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

prompt = (
    "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
    "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
    "researchers was the fact that the unicorns spoke perfect English."
)

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoForCausalLM, GPT2Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;In a shocking finding, scientists discovered a herd of unicorns living in a remote, &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;previously unexplored valley, in the Andes Mountains. Even more surprising to the &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;researchers was the fact that the unicorns spoke perfect English.&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    do_sample=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    temperature=<span class="hljs-number">0.9</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_text = tokenizer.batch_decode(gen_tokens)[<span class="hljs-number">0</span>]`}}),Be=new Te({}),We=new V({props:{name:"class transformers.GPTNeoConfig",anchor:"transformers.GPTNeoConfig",parameters:[{name:"vocab_size",val:" = 50257"},{name:"max_position_embeddings",val:" = 2048"},{name:"hidden_size",val:" = 2048"},{name:"num_layers",val:" = 24"},{name:"attention_types",val:" = [[['global', 'local'], 12]]"},{name:"num_heads",val:" = 16"},{name:"intermediate_size",val:" = None"},{name:"window_size",val:" = 256"},{name:"activation_function",val:" = 'gelu_new'"},{name:"resid_dropout",val:" = 0.0"},{name:"embed_dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"summary_type",val:" = 'cls_index'"},{name:"summary_use_proj",val:" = True"},{name:"summary_activation",val:" = None"},{name:"summary_proj_to_labels",val:" = True"},{name:"summary_first_dropout",val:" = 0.1"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 50256"},{name:"eos_token_id",val:" = 50256"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/configuration_gpt_neo.py#L34",parametersDescription:[{anchor:"transformers.GPTNeoConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50257) &#x2014;
Vocabulary size of the GPT Neo model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>. Vocabulary size of the model. Defines the different
tokens that can be represented by the <em>inputs_ids</em> passed to the forward method of <a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoConfig.attention_types",description:`<strong>attention_types</strong> (<code>List</code>, <em>optional</em>, defaults to <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code>) &#x2014;
The type of attention for each layer in a <code>List</code> of the following format <code>[[[&quot;attention_type&quot;], num_layerss]]</code> e.g. for a 24 layer model <code>[[[&quot;global&quot;], 24]]</code> or <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code> Choose the
value of <code>attention_type</code> from <code>[&quot;global&quot;, &quot;local&quot;]</code>`,name:"attention_types"},{anchor:"transformers.GPTNeoConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.GPTNeoConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.GPTNeoConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GPTNeoConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.GPTNeoConfig.embed_dropout",description:`<strong>embed_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"embed_dropout"},{anchor:"transformers.GPTNeoConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.GPTNeoConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.GPTNeoConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}]}}),Ue=new oe({props:{code:`from transformers import GPTNeoModel, GPTNeoConfig

# Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration
configuration = GPTNeoConfig()

# Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration
model = GPTNeoModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoModel, GPTNeoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GPTNeoConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Ve=new Te({}),Je=new V({props:{name:"class transformers.GPTNeoModel",anchor:"transformers.GPTNeoModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L474",parametersDescription:[{anchor:"transformers.GPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Qe=new V({props:{name:"forward",anchor:"transformers.GPTNeoModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L495",parametersDescription:[{anchor:"transformers.GPTNeoModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_10956/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new dn({props:{$$slots:{default:[rd]},$$scope:{ctx:W}}}),Ye=new oe({props:{code:`from transformers import GPT2Tokenizer, GPTNeoModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ze=new Te({}),et=new V({props:{name:"class transformers.GPTNeoForCausalLM",anchor:"transformers.GPTNeoForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L662",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),st=new V({props:{name:"forward",anchor:"transformers.GPTNeoForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L712",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_10956/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new dn({props:{$$slots:{default:[id]},$$scope:{ctx:W}}}),at=new oe({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),rt=new Te({}),it=new V({props:{name:"class transformers.GPTNeoForSequenceClassification",anchor:"transformers.GPTNeoForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L815",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),pt=new V({props:{name:"forward",anchor:"transformers.GPTNeoForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L827",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_10956/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Fe=new dn({props:{$$slots:{default:[ld]},$$scope:{ctx:W}}}),ht=new oe({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),ut=new oe({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B", num_labels=num_labels)

labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),mt=new oe({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),ft=new oe({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B", num_labels=num_labels)

num_labels = len(model.config.id2label)
labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),gt=new Te({}),_t=new V({props:{name:"class transformers.FlaxGPTNeoModel",anchor:"transformers.FlaxGPTNeoModel",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L580",parametersDescription:[{anchor:"transformers.FlaxGPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),Nt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L390",parametersDescription:[{anchor:"transformers.FlaxGPTNeoModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_10956/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ee=new dn({props:{$$slots:{default:[dd]},$$scope:{ctx:W}}}),$t=new oe({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoModel

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = FlaxGPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoModel.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Gt=new Te({}),xt=new V({props:{name:"class transformers.FlaxGPTNeoForCausalLM",anchor:"transformers.FlaxGPTNeoForCausalLM",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L645",parametersDescription:[{anchor:"transformers.FlaxGPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoForCausalLM.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),At=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoForCausalLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_10956/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L390",parametersDescription:[{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_10956/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoForCausalLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_10956/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_10956/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ze=new dn({props:{$$slots:{default:[cd]},$$scope:{ctx:W}}}),It=new oe({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = FlaxGPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
outputs = model(**inputs)

# retrieve logts for next token
next_token_logits = outputs.logits[:, -1]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`}}),{c(){p=n("meta"),N=d(),m=n("h1"),P=n("a"),w=n("span"),_(g.$$.fragment),f=d(),G=n("span"),Xn=r("GPT Neo"),cn=d(),ne=n("h2"),ve=n("a"),lo=n("span"),_(Ae.$$.fragment),Kn=d(),co=n("span"),Qn=r("Overview"),pn=d(),J=n("p"),Yn=r("The GPTNeo model was released in the "),Ie=n("a"),Zn=r("EleutherAI/gpt-neo"),es=r(` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Le=n("a"),ts=r("Pile"),os=r(" dataset."),hn=d(),Ot=n("p"),ns=r(`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),un=d(),ye=n("p"),ss=r("This model was contributed by "),Se=n("a"),as=r("valhalla"),rs=r("."),mn=d(),se=n("h3"),be=n("a"),po=n("span"),_(Oe.$$.fragment),is=d(),ho=n("span"),ls=r("Generation"),fn=d(),ke=n("p"),ds=r("The "),uo=n("code"),cs=r("generate()"),ps=r(" method can be used to generate text using GPT Neo model."),gn=d(),_(De.$$.fragment),_n=d(),ae=n("h2"),Pe=n("a"),mo=n("span"),_(Be.$$.fragment),hs=d(),fo=n("span"),us=r("GPTNeoConfig"),Tn=d(),C=n("div"),_(We.$$.fragment),ms=d(),re=n("p"),fs=r("This is the configuration class to store the configuration of a "),Dt=n("a"),gs=r("GPTNeoModel"),_s=r(`. It is used to instantiate a GPT
Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the GPTNeo
`),He=n("a"),Ts=r("gpt-neo-1.3B"),vs=r(" architecture."),ys=d(),ie=n("p"),bs=r("Configuration objects inherit from "),Bt=n("a"),ks=r("PretrainedConfig"),Ps=r(` and can be used to control the model outputs. Read the
documentation from `),Wt=n("a"),ws=r("PretrainedConfig"),Ns=r(" for more information."),$s=d(),go=n("p"),Gs=r("Example:"),xs=d(),_(Ue.$$.fragment),vn=d(),le=n("h2"),we=n("a"),_o=n("span"),_(Ve.$$.fragment),Fs=d(),To=n("span"),Ms=r("GPTNeoModel"),yn=d(),z=n("div"),_(Je.$$.fragment),Es=d(),vo=n("p"),Cs=r("The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),zs=d(),Re=n("p"),qs=r("This model inherits from "),Ht=n("a"),js=r("PreTrainedModel"),As=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Is=d(),Xe=n("p"),Ls=r("This model is also a PyTorch "),Ke=n("a"),Ss=r("torch.nn.Module"),Os=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ds=d(),j=n("div"),_(Qe.$$.fragment),Bs=d(),de=n("p"),Ws=r("The "),Ut=n("a"),Hs=r("GPTNeoModel"),Us=r(" forward method, overrides the "),yo=n("code"),Vs=r("__call__"),Js=r(" special method."),Rs=d(),_(Ne.$$.fragment),Xs=d(),bo=n("p"),Ks=r("Example:"),Qs=d(),_(Ye.$$.fragment),bn=d(),ce=n("h2"),$e=n("a"),ko=n("span"),_(Ze.$$.fragment),Ys=d(),Po=n("span"),Zs=r("GPTNeoForCausalLM"),kn=d(),q=n("div"),_(et.$$.fragment),ea=d(),wo=n("p"),ta=r(`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),oa=d(),tt=n("p"),na=r("This model inherits from "),Vt=n("a"),sa=r("PreTrainedModel"),aa=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ra=d(),ot=n("p"),ia=r("This model is also a PyTorch "),nt=n("a"),la=r("torch.nn.Module"),da=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ca=d(),A=n("div"),_(st.$$.fragment),pa=d(),pe=n("p"),ha=r("The "),Jt=n("a"),ua=r("GPTNeoForCausalLM"),ma=r(" forward method, overrides the "),No=n("code"),fa=r("__call__"),ga=r(" special method."),_a=d(),_(Ge.$$.fragment),Ta=d(),$o=n("p"),va=r("Example:"),ya=d(),_(at.$$.fragment),Pn=d(),he=n("h2"),xe=n("a"),Go=n("span"),_(rt.$$.fragment),ba=d(),xo=n("span"),ka=r("GPTNeoForSequenceClassification"),wn=d(),x=n("div"),_(it.$$.fragment),Pa=d(),Fo=n("p"),wa=r("The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),Na=d(),Rt=n("p"),Xt=n("a"),$a=r("GPTNeoForSequenceClassification"),Ga=r(` uses the last token in order to do the classification, as other causal models
(e.g. GPT-1) do.`),xa=d(),B=n("p"),Fa=r(`Since it does classification on the last token, it requires to know the position of the last token. If a
`),Mo=n("code"),Ma=r("pad_token_id"),Ea=r(` is defined in the configuration, it finds the last token that is not a padding token in each row. If
no `),Eo=n("code"),Ca=r("pad_token_id"),za=r(` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when `),Co=n("code"),qa=r("inputs_embeds"),ja=r(" are passed instead of "),zo=n("code"),Aa=r("input_ids"),Ia=r(`, it does the same (take the last value in
each row of the batch).`),La=d(),lt=n("p"),Sa=r("This model inherits from "),Kt=n("a"),Oa=r("PreTrainedModel"),Da=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ba=d(),dt=n("p"),Wa=r("This model is also a PyTorch "),ct=n("a"),Ha=r("torch.nn.Module"),Ua=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Va=d(),$=n("div"),_(pt.$$.fragment),Ja=d(),ue=n("p"),Ra=r("The "),Qt=n("a"),Xa=r("GPTNeoForSequenceClassification"),Ka=r(" forward method, overrides the "),qo=n("code"),Qa=r("__call__"),Ya=r(" special method."),Za=d(),_(Fe.$$.fragment),er=d(),jo=n("p"),tr=r("Example of single-label classification:"),or=d(),_(ht.$$.fragment),nr=d(),_(ut.$$.fragment),sr=d(),Ao=n("p"),ar=r("Example of multi-label classification:"),rr=d(),_(mt.$$.fragment),ir=d(),_(ft.$$.fragment),Nn=d(),me=n("h2"),Me=n("a"),Io=n("span"),_(gt.$$.fragment),lr=d(),Lo=n("span"),dr=r("FlaxGPTNeoModel"),$n=d(),F=n("div"),_(_t.$$.fragment),cr=d(),So=n("p"),pr=r("The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),hr=d(),Tt=n("p"),ur=r("This model inherits from "),Yt=n("a"),mr=r("FlaxPreTrainedModel"),fr=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gr=d(),vt=n("p"),_r=r(`This model is also a Flax Linen
`),yt=n("a"),Tr=r("flax.nn.Module"),vr=r(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),yr=d(),Oo=n("p"),br=r("Finally, this model supports inherent JAX features such as:"),kr=d(),H=n("ul"),Do=n("li"),bt=n("a"),Pr=r("Just-In-Time (JIT) compilation"),wr=d(),Bo=n("li"),kt=n("a"),Nr=r("Automatic Differentiation"),$r=d(),Wo=n("li"),Pt=n("a"),Gr=r("Vectorization"),xr=d(),Ho=n("li"),wt=n("a"),Fr=r("Parallelization"),Mr=d(),I=n("div"),_(Nt.$$.fragment),Er=d(),fe=n("p"),Cr=r("The "),Uo=n("code"),zr=r("FlaxGPTNeoPreTrainedModel"),qr=r("forward method, overrides the "),Vo=n("code"),jr=r("__call__"),Ar=r(" special method."),Ir=d(),_(Ee.$$.fragment),Lr=d(),Jo=n("p"),Sr=r("Example:"),Or=d(),_($t.$$.fragment),Gn=d(),ge=n("h2"),Ce=n("a"),Ro=n("span"),_(Gt.$$.fragment),Dr=d(),Xo=n("span"),Br=r("FlaxGPTNeoForCausalLM"),xn=d(),M=n("div"),_(xt.$$.fragment),Wr=d(),Ko=n("p"),Hr=r(`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Ur=d(),Ft=n("p"),Vr=r("This model inherits from "),Zt=n("a"),Jr=r("FlaxPreTrainedModel"),Rr=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xr=d(),Mt=n("p"),Kr=r(`This model is also a Flax Linen
`),Et=n("a"),Qr=r("flax.nn.Module"),Yr=r(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Zr=d(),Qo=n("p"),ei=r("Finally, this model supports inherent JAX features such as:"),ti=d(),U=n("ul"),Yo=n("li"),Ct=n("a"),oi=r("Just-In-Time (JIT) compilation"),ni=d(),Zo=n("li"),zt=n("a"),si=r("Automatic Differentiation"),ai=d(),en=n("li"),qt=n("a"),ri=r("Vectorization"),ii=d(),tn=n("li"),jt=n("a"),li=r("Parallelization"),di=d(),L=n("div"),_(At.$$.fragment),ci=d(),_e=n("p"),pi=r("The "),on=n("code"),hi=r("FlaxGPTNeoPreTrainedModel"),ui=r("forward method, overrides the "),nn=n("code"),mi=r("__call__"),fi=r(" special method."),gi=d(),_(ze.$$.fragment),_i=d(),sn=n("p"),Ti=r("Example:"),vi=d(),_(It.$$.fragment),this.h()},l(t){const h=sd('[data-svelte="svelte-1phssyn"]',document.head);p=s(h,"META",{name:!0,content:!0}),h.forEach(o),N=c(t),m=s(t,"H1",{class:!0});var Lt=a(m);P=s(Lt,"A",{id:!0,class:!0,href:!0});var an=a(P);w=s(an,"SPAN",{});var rn=a(w);T(g.$$.fragment,rn),rn.forEach(o),an.forEach(o),f=c(Lt),G=s(Lt,"SPAN",{});var ln=a(G);Xn=i(ln,"GPT Neo"),ln.forEach(o),Lt.forEach(o),cn=c(t),ne=s(t,"H2",{class:!0});var St=a(ne);ve=s(St,"A",{id:!0,class:!0,href:!0});var bi=a(ve);lo=s(bi,"SPAN",{});var ki=a(lo);T(Ae.$$.fragment,ki),ki.forEach(o),bi.forEach(o),Kn=c(St),co=s(St,"SPAN",{});var Pi=a(co);Qn=i(Pi,"Overview"),Pi.forEach(o),St.forEach(o),pn=c(t),J=s(t,"P",{});var eo=a(J);Yn=i(eo,"The GPTNeo model was released in the "),Ie=s(eo,"A",{href:!0,rel:!0});var wi=a(Ie);Zn=i(wi,"EleutherAI/gpt-neo"),wi.forEach(o),es=i(eo,` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Le=s(eo,"A",{href:!0,rel:!0});var Ni=a(Le);ts=i(Ni,"Pile"),Ni.forEach(o),os=i(eo," dataset."),eo.forEach(o),hn=c(t),Ot=s(t,"P",{});var $i=a(Ot);ns=i($i,`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),$i.forEach(o),un=c(t),ye=s(t,"P",{});var Mn=a(ye);ss=i(Mn,"This model was contributed by "),Se=s(Mn,"A",{href:!0,rel:!0});var Gi=a(Se);as=i(Gi,"valhalla"),Gi.forEach(o),rs=i(Mn,"."),Mn.forEach(o),mn=c(t),se=s(t,"H3",{class:!0});var En=a(se);be=s(En,"A",{id:!0,class:!0,href:!0});var xi=a(be);po=s(xi,"SPAN",{});var Fi=a(po);T(Oe.$$.fragment,Fi),Fi.forEach(o),xi.forEach(o),is=c(En),ho=s(En,"SPAN",{});var Mi=a(ho);ls=i(Mi,"Generation"),Mi.forEach(o),En.forEach(o),fn=c(t),ke=s(t,"P",{});var Cn=a(ke);ds=i(Cn,"The "),uo=s(Cn,"CODE",{});var Ei=a(uo);cs=i(Ei,"generate()"),Ei.forEach(o),ps=i(Cn," method can be used to generate text using GPT Neo model."),Cn.forEach(o),gn=c(t),T(De.$$.fragment,t),_n=c(t),ae=s(t,"H2",{class:!0});var zn=a(ae);Pe=s(zn,"A",{id:!0,class:!0,href:!0});var Ci=a(Pe);mo=s(Ci,"SPAN",{});var zi=a(mo);T(Be.$$.fragment,zi),zi.forEach(o),Ci.forEach(o),hs=c(zn),fo=s(zn,"SPAN",{});var qi=a(fo);us=i(qi,"GPTNeoConfig"),qi.forEach(o),zn.forEach(o),Tn=c(t),C=s(t,"DIV",{class:!0});var R=a(C);T(We.$$.fragment,R),ms=c(R),re=s(R,"P",{});var to=a(re);fs=i(to,"This is the configuration class to store the configuration of a "),Dt=s(to,"A",{href:!0});var ji=a(Dt);gs=i(ji,"GPTNeoModel"),ji.forEach(o),_s=i(to,`. It is used to instantiate a GPT
Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the GPTNeo
`),He=s(to,"A",{href:!0,rel:!0});var Ai=a(He);Ts=i(Ai,"gpt-neo-1.3B"),Ai.forEach(o),vs=i(to," architecture."),to.forEach(o),ys=c(R),ie=s(R,"P",{});var oo=a(ie);bs=i(oo,"Configuration objects inherit from "),Bt=s(oo,"A",{href:!0});var Ii=a(Bt);ks=i(Ii,"PretrainedConfig"),Ii.forEach(o),Ps=i(oo,` and can be used to control the model outputs. Read the
documentation from `),Wt=s(oo,"A",{href:!0});var Li=a(Wt);ws=i(Li,"PretrainedConfig"),Li.forEach(o),Ns=i(oo," for more information."),oo.forEach(o),$s=c(R),go=s(R,"P",{});var Si=a(go);Gs=i(Si,"Example:"),Si.forEach(o),xs=c(R),T(Ue.$$.fragment,R),R.forEach(o),vn=c(t),le=s(t,"H2",{class:!0});var qn=a(le);we=s(qn,"A",{id:!0,class:!0,href:!0});var Oi=a(we);_o=s(Oi,"SPAN",{});var Di=a(_o);T(Ve.$$.fragment,Di),Di.forEach(o),Oi.forEach(o),Fs=c(qn),To=s(qn,"SPAN",{});var Bi=a(To);Ms=i(Bi,"GPTNeoModel"),Bi.forEach(o),qn.forEach(o),yn=c(t),z=s(t,"DIV",{class:!0});var X=a(z);T(Je.$$.fragment,X),Es=c(X),vo=s(X,"P",{});var Wi=a(vo);Cs=i(Wi,"The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),Wi.forEach(o),zs=c(X),Re=s(X,"P",{});var jn=a(Re);qs=i(jn,"This model inherits from "),Ht=s(jn,"A",{href:!0});var Hi=a(Ht);js=i(Hi,"PreTrainedModel"),Hi.forEach(o),As=i(jn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jn.forEach(o),Is=c(X),Xe=s(X,"P",{});var An=a(Xe);Ls=i(An,"This model is also a PyTorch "),Ke=s(An,"A",{href:!0,rel:!0});var Ui=a(Ke);Ss=i(Ui,"torch.nn.Module"),Ui.forEach(o),Os=i(An,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),An.forEach(o),Ds=c(X),j=s(X,"DIV",{class:!0});var K=a(j);T(Qe.$$.fragment,K),Bs=c(K),de=s(K,"P",{});var no=a(de);Ws=i(no,"The "),Ut=s(no,"A",{href:!0});var Vi=a(Ut);Hs=i(Vi,"GPTNeoModel"),Vi.forEach(o),Us=i(no," forward method, overrides the "),yo=s(no,"CODE",{});var Ji=a(yo);Vs=i(Ji,"__call__"),Ji.forEach(o),Js=i(no," special method."),no.forEach(o),Rs=c(K),T(Ne.$$.fragment,K),Xs=c(K),bo=s(K,"P",{});var Ri=a(bo);Ks=i(Ri,"Example:"),Ri.forEach(o),Qs=c(K),T(Ye.$$.fragment,K),K.forEach(o),X.forEach(o),bn=c(t),ce=s(t,"H2",{class:!0});var In=a(ce);$e=s(In,"A",{id:!0,class:!0,href:!0});var Xi=a($e);ko=s(Xi,"SPAN",{});var Ki=a(ko);T(Ze.$$.fragment,Ki),Ki.forEach(o),Xi.forEach(o),Ys=c(In),Po=s(In,"SPAN",{});var Qi=a(Po);Zs=i(Qi,"GPTNeoForCausalLM"),Qi.forEach(o),In.forEach(o),kn=c(t),q=s(t,"DIV",{class:!0});var Q=a(q);T(et.$$.fragment,Q),ea=c(Q),wo=s(Q,"P",{});var Yi=a(wo);ta=i(Yi,`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Yi.forEach(o),oa=c(Q),tt=s(Q,"P",{});var Ln=a(tt);na=i(Ln,"This model inherits from "),Vt=s(Ln,"A",{href:!0});var Zi=a(Vt);sa=i(Zi,"PreTrainedModel"),Zi.forEach(o),aa=i(Ln,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ln.forEach(o),ra=c(Q),ot=s(Q,"P",{});var Sn=a(ot);ia=i(Sn,"This model is also a PyTorch "),nt=s(Sn,"A",{href:!0,rel:!0});var el=a(nt);la=i(el,"torch.nn.Module"),el.forEach(o),da=i(Sn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Sn.forEach(o),ca=c(Q),A=s(Q,"DIV",{class:!0});var Y=a(A);T(st.$$.fragment,Y),pa=c(Y),pe=s(Y,"P",{});var so=a(pe);ha=i(so,"The "),Jt=s(so,"A",{href:!0});var tl=a(Jt);ua=i(tl,"GPTNeoForCausalLM"),tl.forEach(o),ma=i(so," forward method, overrides the "),No=s(so,"CODE",{});var ol=a(No);fa=i(ol,"__call__"),ol.forEach(o),ga=i(so," special method."),so.forEach(o),_a=c(Y),T(Ge.$$.fragment,Y),Ta=c(Y),$o=s(Y,"P",{});var nl=a($o);va=i(nl,"Example:"),nl.forEach(o),ya=c(Y),T(at.$$.fragment,Y),Y.forEach(o),Q.forEach(o),Pn=c(t),he=s(t,"H2",{class:!0});var On=a(he);xe=s(On,"A",{id:!0,class:!0,href:!0});var sl=a(xe);Go=s(sl,"SPAN",{});var al=a(Go);T(rt.$$.fragment,al),al.forEach(o),sl.forEach(o),ba=c(On),xo=s(On,"SPAN",{});var rl=a(xo);ka=i(rl,"GPTNeoForSequenceClassification"),rl.forEach(o),On.forEach(o),wn=c(t),x=s(t,"DIV",{class:!0});var S=a(x);T(it.$$.fragment,S),Pa=c(S),Fo=s(S,"P",{});var il=a(Fo);wa=i(il,"The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),il.forEach(o),Na=c(S),Rt=s(S,"P",{});var yi=a(Rt);Xt=s(yi,"A",{href:!0});var ll=a(Xt);$a=i(ll,"GPTNeoForSequenceClassification"),ll.forEach(o),Ga=i(yi,` uses the last token in order to do the classification, as other causal models
(e.g. GPT-1) do.`),yi.forEach(o),xa=c(S),B=s(S,"P",{});var Z=a(B);Fa=i(Z,`Since it does classification on the last token, it requires to know the position of the last token. If a
`),Mo=s(Z,"CODE",{});var dl=a(Mo);Ma=i(dl,"pad_token_id"),dl.forEach(o),Ea=i(Z,` is defined in the configuration, it finds the last token that is not a padding token in each row. If
no `),Eo=s(Z,"CODE",{});var cl=a(Eo);Ca=i(cl,"pad_token_id"),cl.forEach(o),za=i(Z,` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when `),Co=s(Z,"CODE",{});var pl=a(Co);qa=i(pl,"inputs_embeds"),pl.forEach(o),ja=i(Z," are passed instead of "),zo=s(Z,"CODE",{});var hl=a(zo);Aa=i(hl,"input_ids"),hl.forEach(o),Ia=i(Z,`, it does the same (take the last value in
each row of the batch).`),Z.forEach(o),La=c(S),lt=s(S,"P",{});var Dn=a(lt);Sa=i(Dn,"This model inherits from "),Kt=s(Dn,"A",{href:!0});var ul=a(Kt);Oa=i(ul,"PreTrainedModel"),ul.forEach(o),Da=i(Dn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Dn.forEach(o),Ba=c(S),dt=s(S,"P",{});var Bn=a(dt);Wa=i(Bn,"This model is also a PyTorch "),ct=s(Bn,"A",{href:!0,rel:!0});var ml=a(ct);Ha=i(ml,"torch.nn.Module"),ml.forEach(o),Ua=i(Bn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bn.forEach(o),Va=c(S),$=s(S,"DIV",{class:!0});var E=a($);T(pt.$$.fragment,E),Ja=c(E),ue=s(E,"P",{});var ao=a(ue);Ra=i(ao,"The "),Qt=s(ao,"A",{href:!0});var fl=a(Qt);Xa=i(fl,"GPTNeoForSequenceClassification"),fl.forEach(o),Ka=i(ao," forward method, overrides the "),qo=s(ao,"CODE",{});var gl=a(qo);Qa=i(gl,"__call__"),gl.forEach(o),Ya=i(ao," special method."),ao.forEach(o),Za=c(E),T(Fe.$$.fragment,E),er=c(E),jo=s(E,"P",{});var _l=a(jo);tr=i(_l,"Example of single-label classification:"),_l.forEach(o),or=c(E),T(ht.$$.fragment,E),nr=c(E),T(ut.$$.fragment,E),sr=c(E),Ao=s(E,"P",{});var Tl=a(Ao);ar=i(Tl,"Example of multi-label classification:"),Tl.forEach(o),rr=c(E),T(mt.$$.fragment,E),ir=c(E),T(ft.$$.fragment,E),E.forEach(o),S.forEach(o),Nn=c(t),me=s(t,"H2",{class:!0});var Wn=a(me);Me=s(Wn,"A",{id:!0,class:!0,href:!0});var vl=a(Me);Io=s(vl,"SPAN",{});var yl=a(Io);T(gt.$$.fragment,yl),yl.forEach(o),vl.forEach(o),lr=c(Wn),Lo=s(Wn,"SPAN",{});var bl=a(Lo);dr=i(bl,"FlaxGPTNeoModel"),bl.forEach(o),Wn.forEach(o),$n=c(t),F=s(t,"DIV",{class:!0});var O=a(F);T(_t.$$.fragment,O),cr=c(O),So=s(O,"P",{});var kl=a(So);pr=i(kl,"The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),kl.forEach(o),hr=c(O),Tt=s(O,"P",{});var Hn=a(Tt);ur=i(Hn,"This model inherits from "),Yt=s(Hn,"A",{href:!0});var Pl=a(Yt);mr=i(Pl,"FlaxPreTrainedModel"),Pl.forEach(o),fr=i(Hn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hn.forEach(o),gr=c(O),vt=s(O,"P",{});var Un=a(vt);_r=i(Un,`This model is also a Flax Linen
`),yt=s(Un,"A",{href:!0,rel:!0});var wl=a(yt);Tr=i(wl,"flax.nn.Module"),wl.forEach(o),vr=i(Un,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Un.forEach(o),yr=c(O),Oo=s(O,"P",{});var Nl=a(Oo);br=i(Nl,"Finally, this model supports inherent JAX features such as:"),Nl.forEach(o),kr=c(O),H=s(O,"UL",{});var qe=a(H);Do=s(qe,"LI",{});var $l=a(Do);bt=s($l,"A",{href:!0,rel:!0});var Gl=a(bt);Pr=i(Gl,"Just-In-Time (JIT) compilation"),Gl.forEach(o),$l.forEach(o),wr=c(qe),Bo=s(qe,"LI",{});var xl=a(Bo);kt=s(xl,"A",{href:!0,rel:!0});var Fl=a(kt);Nr=i(Fl,"Automatic Differentiation"),Fl.forEach(o),xl.forEach(o),$r=c(qe),Wo=s(qe,"LI",{});var Ml=a(Wo);Pt=s(Ml,"A",{href:!0,rel:!0});var El=a(Pt);Gr=i(El,"Vectorization"),El.forEach(o),Ml.forEach(o),xr=c(qe),Ho=s(qe,"LI",{});var Cl=a(Ho);wt=s(Cl,"A",{href:!0,rel:!0});var zl=a(wt);Fr=i(zl,"Parallelization"),zl.forEach(o),Cl.forEach(o),qe.forEach(o),Mr=c(O),I=s(O,"DIV",{class:!0});var ee=a(I);T(Nt.$$.fragment,ee),Er=c(ee),fe=s(ee,"P",{});var ro=a(fe);Cr=i(ro,"The "),Uo=s(ro,"CODE",{});var ql=a(Uo);zr=i(ql,"FlaxGPTNeoPreTrainedModel"),ql.forEach(o),qr=i(ro,"forward method, overrides the "),Vo=s(ro,"CODE",{});var jl=a(Vo);jr=i(jl,"__call__"),jl.forEach(o),Ar=i(ro," special method."),ro.forEach(o),Ir=c(ee),T(Ee.$$.fragment,ee),Lr=c(ee),Jo=s(ee,"P",{});var Al=a(Jo);Sr=i(Al,"Example:"),Al.forEach(o),Or=c(ee),T($t.$$.fragment,ee),ee.forEach(o),O.forEach(o),Gn=c(t),ge=s(t,"H2",{class:!0});var Vn=a(ge);Ce=s(Vn,"A",{id:!0,class:!0,href:!0});var Il=a(Ce);Ro=s(Il,"SPAN",{});var Ll=a(Ro);T(Gt.$$.fragment,Ll),Ll.forEach(o),Il.forEach(o),Dr=c(Vn),Xo=s(Vn,"SPAN",{});var Sl=a(Xo);Br=i(Sl,"FlaxGPTNeoForCausalLM"),Sl.forEach(o),Vn.forEach(o),xn=c(t),M=s(t,"DIV",{class:!0});var D=a(M);T(xt.$$.fragment,D),Wr=c(D),Ko=s(D,"P",{});var Ol=a(Ko);Hr=i(Ol,`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Ol.forEach(o),Ur=c(D),Ft=s(D,"P",{});var Jn=a(Ft);Vr=i(Jn,"This model inherits from "),Zt=s(Jn,"A",{href:!0});var Dl=a(Zt);Jr=i(Dl,"FlaxPreTrainedModel"),Dl.forEach(o),Rr=i(Jn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jn.forEach(o),Xr=c(D),Mt=s(D,"P",{});var Rn=a(Mt);Kr=i(Rn,`This model is also a Flax Linen
`),Et=s(Rn,"A",{href:!0,rel:!0});var Bl=a(Et);Qr=i(Bl,"flax.nn.Module"),Bl.forEach(o),Yr=i(Rn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Rn.forEach(o),Zr=c(D),Qo=s(D,"P",{});var Wl=a(Qo);ei=i(Wl,"Finally, this model supports inherent JAX features such as:"),Wl.forEach(o),ti=c(D),U=s(D,"UL",{});var je=a(U);Yo=s(je,"LI",{});var Hl=a(Yo);Ct=s(Hl,"A",{href:!0,rel:!0});var Ul=a(Ct);oi=i(Ul,"Just-In-Time (JIT) compilation"),Ul.forEach(o),Hl.forEach(o),ni=c(je),Zo=s(je,"LI",{});var Vl=a(Zo);zt=s(Vl,"A",{href:!0,rel:!0});var Jl=a(zt);si=i(Jl,"Automatic Differentiation"),Jl.forEach(o),Vl.forEach(o),ai=c(je),en=s(je,"LI",{});var Rl=a(en);qt=s(Rl,"A",{href:!0,rel:!0});var Xl=a(qt);ri=i(Xl,"Vectorization"),Xl.forEach(o),Rl.forEach(o),ii=c(je),tn=s(je,"LI",{});var Kl=a(tn);jt=s(Kl,"A",{href:!0,rel:!0});var Ql=a(jt);li=i(Ql,"Parallelization"),Ql.forEach(o),Kl.forEach(o),je.forEach(o),di=c(D),L=s(D,"DIV",{class:!0});var te=a(L);T(At.$$.fragment,te),ci=c(te),_e=s(te,"P",{});var io=a(_e);pi=i(io,"The "),on=s(io,"CODE",{});var Yl=a(on);hi=i(Yl,"FlaxGPTNeoPreTrainedModel"),Yl.forEach(o),ui=i(io,"forward method, overrides the "),nn=s(io,"CODE",{});var Zl=a(nn);mi=i(Zl,"__call__"),Zl.forEach(o),fi=i(io," special method."),io.forEach(o),gi=c(te),T(ze.$$.fragment,te),_i=c(te),sn=s(te,"P",{});var ed=a(sn);Ti=i(ed,"Example:"),ed.forEach(o),vi=c(te),T(It.$$.fragment,te),te.forEach(o),D.forEach(o),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(hd)),l(P,"id","gpt-neo"),l(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(P,"href","#gpt-neo"),l(m,"class","relative group"),l(ve,"id","overview"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#overview"),l(ne,"class","relative group"),l(Ie,"href","https://github.com/EleutherAI/gpt-neo"),l(Ie,"rel","nofollow"),l(Le,"href","https://pile.eleuther.ai/"),l(Le,"rel","nofollow"),l(Se,"href","https://huggingface.co/valhalla"),l(Se,"rel","nofollow"),l(be,"id","generation"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#generation"),l(se,"class","relative group"),l(Pe,"id","transformers.GPTNeoConfig"),l(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Pe,"href","#transformers.GPTNeoConfig"),l(ae,"class","relative group"),l(Dt,"href","/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(He,"href","https://huggingface.co/EleutherAI/gpt-neo-1.3B"),l(He,"rel","nofollow"),l(Bt,"href","/docs/transformers/pr_10956/en/main_classes/configuration#transformers.PretrainedConfig"),l(Wt,"href","/docs/transformers/pr_10956/en/main_classes/configuration#transformers.PretrainedConfig"),l(C,"class","docstring"),l(we,"id","transformers.GPTNeoModel"),l(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(we,"href","#transformers.GPTNeoModel"),l(le,"class","relative group"),l(Ht,"href","/docs/transformers/pr_10956/en/main_classes/model#transformers.PreTrainedModel"),l(Ke,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ke,"rel","nofollow"),l(Ut,"href","/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(j,"class","docstring"),l(z,"class","docstring"),l($e,"id","transformers.GPTNeoForCausalLM"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#transformers.GPTNeoForCausalLM"),l(ce,"class","relative group"),l(Vt,"href","/docs/transformers/pr_10956/en/main_classes/model#transformers.PreTrainedModel"),l(nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(nt,"rel","nofollow"),l(Jt,"href","/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),l(A,"class","docstring"),l(q,"class","docstring"),l(xe,"id","transformers.GPTNeoForSequenceClassification"),l(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(xe,"href","#transformers.GPTNeoForSequenceClassification"),l(he,"class","relative group"),l(Xt,"href","/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(Kt,"href","/docs/transformers/pr_10956/en/main_classes/model#transformers.PreTrainedModel"),l(ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(ct,"rel","nofollow"),l(Qt,"href","/docs/transformers/pr_10956/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l($,"class","docstring"),l(x,"class","docstring"),l(Me,"id","transformers.FlaxGPTNeoModel"),l(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Me,"href","#transformers.FlaxGPTNeoModel"),l(me,"class","relative group"),l(Yt,"href","/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(yt,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(yt,"rel","nofollow"),l(bt,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(bt,"rel","nofollow"),l(kt,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(kt,"rel","nofollow"),l(Pt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(Pt,"rel","nofollow"),l(wt,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(wt,"rel","nofollow"),l(I,"class","docstring"),l(F,"class","docstring"),l(Ce,"id","transformers.FlaxGPTNeoForCausalLM"),l(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ce,"href","#transformers.FlaxGPTNeoForCausalLM"),l(ge,"class","relative group"),l(Zt,"href","/docs/transformers/pr_10956/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(Et,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(Et,"rel","nofollow"),l(Ct,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(Ct,"rel","nofollow"),l(zt,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(zt,"rel","nofollow"),l(qt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(qt,"rel","nofollow"),l(jt,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(jt,"rel","nofollow"),l(L,"class","docstring"),l(M,"class","docstring")},m(t,h){e(document.head,p),u(t,N,h),u(t,m,h),e(m,P),e(P,w),v(g,w,null),e(m,f),e(m,G),e(G,Xn),u(t,cn,h),u(t,ne,h),e(ne,ve),e(ve,lo),v(Ae,lo,null),e(ne,Kn),e(ne,co),e(co,Qn),u(t,pn,h),u(t,J,h),e(J,Yn),e(J,Ie),e(Ie,Zn),e(J,es),e(J,Le),e(Le,ts),e(J,os),u(t,hn,h),u(t,Ot,h),e(Ot,ns),u(t,un,h),u(t,ye,h),e(ye,ss),e(ye,Se),e(Se,as),e(ye,rs),u(t,mn,h),u(t,se,h),e(se,be),e(be,po),v(Oe,po,null),e(se,is),e(se,ho),e(ho,ls),u(t,fn,h),u(t,ke,h),e(ke,ds),e(ke,uo),e(uo,cs),e(ke,ps),u(t,gn,h),v(De,t,h),u(t,_n,h),u(t,ae,h),e(ae,Pe),e(Pe,mo),v(Be,mo,null),e(ae,hs),e(ae,fo),e(fo,us),u(t,Tn,h),u(t,C,h),v(We,C,null),e(C,ms),e(C,re),e(re,fs),e(re,Dt),e(Dt,gs),e(re,_s),e(re,He),e(He,Ts),e(re,vs),e(C,ys),e(C,ie),e(ie,bs),e(ie,Bt),e(Bt,ks),e(ie,Ps),e(ie,Wt),e(Wt,ws),e(ie,Ns),e(C,$s),e(C,go),e(go,Gs),e(C,xs),v(Ue,C,null),u(t,vn,h),u(t,le,h),e(le,we),e(we,_o),v(Ve,_o,null),e(le,Fs),e(le,To),e(To,Ms),u(t,yn,h),u(t,z,h),v(Je,z,null),e(z,Es),e(z,vo),e(vo,Cs),e(z,zs),e(z,Re),e(Re,qs),e(Re,Ht),e(Ht,js),e(Re,As),e(z,Is),e(z,Xe),e(Xe,Ls),e(Xe,Ke),e(Ke,Ss),e(Xe,Os),e(z,Ds),e(z,j),v(Qe,j,null),e(j,Bs),e(j,de),e(de,Ws),e(de,Ut),e(Ut,Hs),e(de,Us),e(de,yo),e(yo,Vs),e(de,Js),e(j,Rs),v(Ne,j,null),e(j,Xs),e(j,bo),e(bo,Ks),e(j,Qs),v(Ye,j,null),u(t,bn,h),u(t,ce,h),e(ce,$e),e($e,ko),v(Ze,ko,null),e(ce,Ys),e(ce,Po),e(Po,Zs),u(t,kn,h),u(t,q,h),v(et,q,null),e(q,ea),e(q,wo),e(wo,ta),e(q,oa),e(q,tt),e(tt,na),e(tt,Vt),e(Vt,sa),e(tt,aa),e(q,ra),e(q,ot),e(ot,ia),e(ot,nt),e(nt,la),e(ot,da),e(q,ca),e(q,A),v(st,A,null),e(A,pa),e(A,pe),e(pe,ha),e(pe,Jt),e(Jt,ua),e(pe,ma),e(pe,No),e(No,fa),e(pe,ga),e(A,_a),v(Ge,A,null),e(A,Ta),e(A,$o),e($o,va),e(A,ya),v(at,A,null),u(t,Pn,h),u(t,he,h),e(he,xe),e(xe,Go),v(rt,Go,null),e(he,ba),e(he,xo),e(xo,ka),u(t,wn,h),u(t,x,h),v(it,x,null),e(x,Pa),e(x,Fo),e(Fo,wa),e(x,Na),e(x,Rt),e(Rt,Xt),e(Xt,$a),e(Rt,Ga),e(x,xa),e(x,B),e(B,Fa),e(B,Mo),e(Mo,Ma),e(B,Ea),e(B,Eo),e(Eo,Ca),e(B,za),e(B,Co),e(Co,qa),e(B,ja),e(B,zo),e(zo,Aa),e(B,Ia),e(x,La),e(x,lt),e(lt,Sa),e(lt,Kt),e(Kt,Oa),e(lt,Da),e(x,Ba),e(x,dt),e(dt,Wa),e(dt,ct),e(ct,Ha),e(dt,Ua),e(x,Va),e(x,$),v(pt,$,null),e($,Ja),e($,ue),e(ue,Ra),e(ue,Qt),e(Qt,Xa),e(ue,Ka),e(ue,qo),e(qo,Qa),e(ue,Ya),e($,Za),v(Fe,$,null),e($,er),e($,jo),e(jo,tr),e($,or),v(ht,$,null),e($,nr),v(ut,$,null),e($,sr),e($,Ao),e(Ao,ar),e($,rr),v(mt,$,null),e($,ir),v(ft,$,null),u(t,Nn,h),u(t,me,h),e(me,Me),e(Me,Io),v(gt,Io,null),e(me,lr),e(me,Lo),e(Lo,dr),u(t,$n,h),u(t,F,h),v(_t,F,null),e(F,cr),e(F,So),e(So,pr),e(F,hr),e(F,Tt),e(Tt,ur),e(Tt,Yt),e(Yt,mr),e(Tt,fr),e(F,gr),e(F,vt),e(vt,_r),e(vt,yt),e(yt,Tr),e(vt,vr),e(F,yr),e(F,Oo),e(Oo,br),e(F,kr),e(F,H),e(H,Do),e(Do,bt),e(bt,Pr),e(H,wr),e(H,Bo),e(Bo,kt),e(kt,Nr),e(H,$r),e(H,Wo),e(Wo,Pt),e(Pt,Gr),e(H,xr),e(H,Ho),e(Ho,wt),e(wt,Fr),e(F,Mr),e(F,I),v(Nt,I,null),e(I,Er),e(I,fe),e(fe,Cr),e(fe,Uo),e(Uo,zr),e(fe,qr),e(fe,Vo),e(Vo,jr),e(fe,Ar),e(I,Ir),v(Ee,I,null),e(I,Lr),e(I,Jo),e(Jo,Sr),e(I,Or),v($t,I,null),u(t,Gn,h),u(t,ge,h),e(ge,Ce),e(Ce,Ro),v(Gt,Ro,null),e(ge,Dr),e(ge,Xo),e(Xo,Br),u(t,xn,h),u(t,M,h),v(xt,M,null),e(M,Wr),e(M,Ko),e(Ko,Hr),e(M,Ur),e(M,Ft),e(Ft,Vr),e(Ft,Zt),e(Zt,Jr),e(Ft,Rr),e(M,Xr),e(M,Mt),e(Mt,Kr),e(Mt,Et),e(Et,Qr),e(Mt,Yr),e(M,Zr),e(M,Qo),e(Qo,ei),e(M,ti),e(M,U),e(U,Yo),e(Yo,Ct),e(Ct,oi),e(U,ni),e(U,Zo),e(Zo,zt),e(zt,si),e(U,ai),e(U,en),e(en,qt),e(qt,ri),e(U,ii),e(U,tn),e(tn,jt),e(jt,li),e(M,di),e(M,L),v(At,L,null),e(L,ci),e(L,_e),e(_e,pi),e(_e,on),e(on,hi),e(_e,ui),e(_e,nn),e(nn,mi),e(_e,fi),e(L,gi),v(ze,L,null),e(L,_i),e(L,sn),e(sn,Ti),e(L,vi),v(It,L,null),Fn=!0},p(t,[h]){const Lt={};h&2&&(Lt.$$scope={dirty:h,ctx:t}),Ne.$set(Lt);const an={};h&2&&(an.$$scope={dirty:h,ctx:t}),Ge.$set(an);const rn={};h&2&&(rn.$$scope={dirty:h,ctx:t}),Fe.$set(rn);const ln={};h&2&&(ln.$$scope={dirty:h,ctx:t}),Ee.$set(ln);const St={};h&2&&(St.$$scope={dirty:h,ctx:t}),ze.$set(St)},i(t){Fn||(y(g.$$.fragment,t),y(Ae.$$.fragment,t),y(Oe.$$.fragment,t),y(De.$$.fragment,t),y(Be.$$.fragment,t),y(We.$$.fragment,t),y(Ue.$$.fragment,t),y(Ve.$$.fragment,t),y(Je.$$.fragment,t),y(Qe.$$.fragment,t),y(Ne.$$.fragment,t),y(Ye.$$.fragment,t),y(Ze.$$.fragment,t),y(et.$$.fragment,t),y(st.$$.fragment,t),y(Ge.$$.fragment,t),y(at.$$.fragment,t),y(rt.$$.fragment,t),y(it.$$.fragment,t),y(pt.$$.fragment,t),y(Fe.$$.fragment,t),y(ht.$$.fragment,t),y(ut.$$.fragment,t),y(mt.$$.fragment,t),y(ft.$$.fragment,t),y(gt.$$.fragment,t),y(_t.$$.fragment,t),y(Nt.$$.fragment,t),y(Ee.$$.fragment,t),y($t.$$.fragment,t),y(Gt.$$.fragment,t),y(xt.$$.fragment,t),y(At.$$.fragment,t),y(ze.$$.fragment,t),y(It.$$.fragment,t),Fn=!0)},o(t){b(g.$$.fragment,t),b(Ae.$$.fragment,t),b(Oe.$$.fragment,t),b(De.$$.fragment,t),b(Be.$$.fragment,t),b(We.$$.fragment,t),b(Ue.$$.fragment,t),b(Ve.$$.fragment,t),b(Je.$$.fragment,t),b(Qe.$$.fragment,t),b(Ne.$$.fragment,t),b(Ye.$$.fragment,t),b(Ze.$$.fragment,t),b(et.$$.fragment,t),b(st.$$.fragment,t),b(Ge.$$.fragment,t),b(at.$$.fragment,t),b(rt.$$.fragment,t),b(it.$$.fragment,t),b(pt.$$.fragment,t),b(Fe.$$.fragment,t),b(ht.$$.fragment,t),b(ut.$$.fragment,t),b(mt.$$.fragment,t),b(ft.$$.fragment,t),b(gt.$$.fragment,t),b(_t.$$.fragment,t),b(Nt.$$.fragment,t),b(Ee.$$.fragment,t),b($t.$$.fragment,t),b(Gt.$$.fragment,t),b(xt.$$.fragment,t),b(At.$$.fragment,t),b(ze.$$.fragment,t),b(It.$$.fragment,t),Fn=!1},d(t){o(p),t&&o(N),t&&o(m),k(g),t&&o(cn),t&&o(ne),k(Ae),t&&o(pn),t&&o(J),t&&o(hn),t&&o(Ot),t&&o(un),t&&o(ye),t&&o(mn),t&&o(se),k(Oe),t&&o(fn),t&&o(ke),t&&o(gn),k(De,t),t&&o(_n),t&&o(ae),k(Be),t&&o(Tn),t&&o(C),k(We),k(Ue),t&&o(vn),t&&o(le),k(Ve),t&&o(yn),t&&o(z),k(Je),k(Qe),k(Ne),k(Ye),t&&o(bn),t&&o(ce),k(Ze),t&&o(kn),t&&o(q),k(et),k(st),k(Ge),k(at),t&&o(Pn),t&&o(he),k(rt),t&&o(wn),t&&o(x),k(it),k(pt),k(Fe),k(ht),k(ut),k(mt),k(ft),t&&o(Nn),t&&o(me),k(gt),t&&o($n),t&&o(F),k(_t),k(Nt),k(Ee),k($t),t&&o(Gn),t&&o(ge),k(Gt),t&&o(xn),t&&o(M),k(xt),k(At),k(ze),k(It)}}}const hd={local:"gpt-neo",sections:[{local:"overview",sections:[{local:"generation",title:"Generation"}],title:"Overview"},{local:"transformers.GPTNeoConfig",title:"GPTNeoConfig"},{local:"transformers.GPTNeoModel",title:"GPTNeoModel"},{local:"transformers.GPTNeoForCausalLM",title:"GPTNeoForCausalLM"},{local:"transformers.GPTNeoForSequenceClassification",title:"GPTNeoForSequenceClassification"},{local:"transformers.FlaxGPTNeoModel",title:"FlaxGPTNeoModel"},{local:"transformers.FlaxGPTNeoForCausalLM",title:"FlaxGPTNeoForCausalLM"}],title:"GPT Neo"};function ud(W){return ad(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vd extends td{constructor(p){super();od(this,p,ud,pd,nd,{})}}export{vd as default,hd as metadata};
