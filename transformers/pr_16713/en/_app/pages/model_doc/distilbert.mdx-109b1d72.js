import{S as yD,i as $D,s as FD,e as o,k as l,w as b,t as a,M as DD,c as n,d as t,m as d,a as r,x as k,h as i,b as c,F as e,g as u,y as T,q as w,o as y,B as $,v as BD}from"../../chunks/vendor-6b77c823.js";import{T as ge}from"../../chunks/Tip-39098574.js";import{D as H}from"../../chunks/Docstring-af1d0ae0.js";import{C as re}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as be}from"../../chunks/IconCopyLink-7a11ce68.js";function MD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function ED(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function xD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function zD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function jD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function CD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function qD(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de;return{c(){p=o("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),F=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),B=o("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),E=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),K=l(),x=o("ul"),z=o("li"),ue=a("a single Tensor with "),W=o("code"),se=a("input_ids"),me=a(" only and nothing else: "),R=o("code"),ie=a("model(inputs_ids)"),ee=l(),A=o("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),q=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o("code"),de=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),v=n(h,"UL",{});var J=r(v);F=n(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),m=d(J),B=n(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),V=d(h),E=n(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=n(O,"CODE",{});var ve=r(S);X=i(ve,"tf.keras.Model.fit"),ve.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=n(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),K=d(h),x=n(h,"UL",{});var C=r(x);z=n(C,"LI",{});var Q=r(z);ue=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),me=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=n(C,"LI",{});var ne=r(q);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n(ne,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),ne.forEach(t),C.forEach(t)},m(h,M){u(h,p,M),e(p,D),u(h,g,M),u(h,v,M),e(v,F),e(F,_),e(v,m),e(v,B),e(B,ce),u(h,V,M),u(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),u(h,N,M),u(h,P,M),e(P,Y),u(h,K,M),u(h,x,M),e(x,z),e(z,ue),e(z,W),e(W,se),e(z,me),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,oe),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(E),h&&t(N),h&&t(P),h&&t(K),h&&t(x)}}}function PD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function AD(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de;return{c(){p=o("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),F=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),B=o("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),E=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),K=l(),x=o("ul"),z=o("li"),ue=a("a single Tensor with "),W=o("code"),se=a("input_ids"),me=a(" only and nothing else: "),R=o("code"),ie=a("model(inputs_ids)"),ee=l(),A=o("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),q=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o("code"),de=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),v=n(h,"UL",{});var J=r(v);F=n(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),m=d(J),B=n(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),V=d(h),E=n(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=n(O,"CODE",{});var ve=r(S);X=i(ve,"tf.keras.Model.fit"),ve.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=n(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),K=d(h),x=n(h,"UL",{});var C=r(x);z=n(C,"LI",{});var Q=r(z);ue=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),me=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=n(C,"LI",{});var ne=r(q);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n(ne,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),ne.forEach(t),C.forEach(t)},m(h,M){u(h,p,M),e(p,D),u(h,g,M),u(h,v,M),e(v,F),e(F,_),e(v,m),e(v,B),e(B,ce),u(h,V,M),u(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),u(h,N,M),u(h,P,M),e(P,Y),u(h,K,M),u(h,x,M),e(x,z),e(z,ue),e(z,W),e(W,se),e(z,me),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,oe),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(E),h&&t(N),h&&t(P),h&&t(K),h&&t(x)}}}function OD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function ND(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de;return{c(){p=o("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),F=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),B=o("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),E=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),K=l(),x=o("ul"),z=o("li"),ue=a("a single Tensor with "),W=o("code"),se=a("input_ids"),me=a(" only and nothing else: "),R=o("code"),ie=a("model(inputs_ids)"),ee=l(),A=o("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),q=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o("code"),de=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),v=n(h,"UL",{});var J=r(v);F=n(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),m=d(J),B=n(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),V=d(h),E=n(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=n(O,"CODE",{});var ve=r(S);X=i(ve,"tf.keras.Model.fit"),ve.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=n(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),K=d(h),x=n(h,"UL",{});var C=r(x);z=n(C,"LI",{});var Q=r(z);ue=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),me=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=n(C,"LI",{});var ne=r(q);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n(ne,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),ne.forEach(t),C.forEach(t)},m(h,M){u(h,p,M),e(p,D),u(h,g,M),u(h,v,M),e(v,F),e(F,_),e(v,m),e(v,B),e(B,ce),u(h,V,M),u(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),u(h,N,M),u(h,P,M),e(P,Y),u(h,K,M),u(h,x,M),e(x,z),e(z,ue),e(z,W),e(W,se),e(z,me),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,oe),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(E),h&&t(N),h&&t(P),h&&t(K),h&&t(x)}}}function LD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function SD(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de;return{c(){p=o("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),F=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),B=o("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),E=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),K=l(),x=o("ul"),z=o("li"),ue=a("a single Tensor with "),W=o("code"),se=a("input_ids"),me=a(" only and nothing else: "),R=o("code"),ie=a("model(inputs_ids)"),ee=l(),A=o("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),q=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o("code"),de=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),v=n(h,"UL",{});var J=r(v);F=n(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),m=d(J),B=n(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),V=d(h),E=n(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=n(O,"CODE",{});var ve=r(S);X=i(ve,"tf.keras.Model.fit"),ve.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=n(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),K=d(h),x=n(h,"UL",{});var C=r(x);z=n(C,"LI",{});var Q=r(z);ue=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),me=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=n(C,"LI",{});var ne=r(q);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n(ne,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),ne.forEach(t),C.forEach(t)},m(h,M){u(h,p,M),e(p,D),u(h,g,M),u(h,v,M),e(v,F),e(F,_),e(v,m),e(v,B),e(B,ce),u(h,V,M),u(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),u(h,N,M),u(h,P,M),e(P,Y),u(h,K,M),u(h,x,M),e(x,z),e(z,ue),e(z,W),e(W,se),e(z,me),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,oe),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(E),h&&t(N),h&&t(P),h&&t(K),h&&t(x)}}}function ID(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function WD(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de;return{c(){p=o("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),F=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),B=o("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),E=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),K=l(),x=o("ul"),z=o("li"),ue=a("a single Tensor with "),W=o("code"),se=a("input_ids"),me=a(" only and nothing else: "),R=o("code"),ie=a("model(inputs_ids)"),ee=l(),A=o("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),q=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o("code"),de=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),v=n(h,"UL",{});var J=r(v);F=n(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),m=d(J),B=n(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),V=d(h),E=n(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=n(O,"CODE",{});var ve=r(S);X=i(ve,"tf.keras.Model.fit"),ve.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=n(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),K=d(h),x=n(h,"UL",{});var C=r(x);z=n(C,"LI",{});var Q=r(z);ue=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),me=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=n(C,"LI",{});var ne=r(q);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n(ne,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),ne.forEach(t),C.forEach(t)},m(h,M){u(h,p,M),e(p,D),u(h,g,M),u(h,v,M),e(v,F),e(F,_),e(v,m),e(v,B),e(B,ce),u(h,V,M),u(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),u(h,N,M),u(h,P,M),e(P,Y),u(h,K,M),u(h,x,M),e(x,z),e(z,ue),e(z,W),e(W,se),e(z,me),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,oe),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(E),h&&t(N),h&&t(P),h&&t(K),h&&t(x)}}}function RD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function UD(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de;return{c(){p=o("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),F=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),B=o("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),E=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),K=l(),x=o("ul"),z=o("li"),ue=a("a single Tensor with "),W=o("code"),se=a("input_ids"),me=a(" only and nothing else: "),R=o("code"),ie=a("model(inputs_ids)"),ee=l(),A=o("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),q=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o("code"),de=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),v=n(h,"UL",{});var J=r(v);F=n(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),m=d(J),B=n(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),V=d(h),E=n(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=n(O,"CODE",{});var ve=r(S);X=i(ve,"tf.keras.Model.fit"),ve.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=n(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),K=d(h),x=n(h,"UL",{});var C=r(x);z=n(C,"LI",{});var Q=r(z);ue=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),me=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=n(C,"LI",{});var ne=r(q);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n(ne,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),ne.forEach(t),C.forEach(t)},m(h,M){u(h,p,M),e(p,D),u(h,g,M),u(h,v,M),e(v,F),e(F,_),e(v,m),e(v,B),e(B,ce),u(h,V,M),u(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),u(h,N,M),u(h,P,M),e(P,Y),u(h,K,M),u(h,x,M),e(x,z),e(z,ue),e(z,W),e(W,se),e(z,me),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,oe),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(E),h&&t(N),h&&t(P),h&&t(K),h&&t(x)}}}function QD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function HD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function VD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function KD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function JD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function GD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function XD(j){let p,D,g,v,F;return{c(){p=o("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var m=r(p);D=i(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),F=i(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(t)},m(_,m){u(_,p,m),e(p,D),e(p,g),e(g,v),e(p,F)},d(_){_&&t(p)}}}function YD(j){let p,D,g,v,F,_,m,B,ce,V,E,G,S,X,pe,I,he,ae,N,P,Y,K,x,z,ue,W,se,me,R,ie,ee,A,le,L,oe,fe,q,te,U,de,h,M,J,_e,Te,O,ve,we,ye,C,Q,$e,Fe,Z,De,ne,Be,Tu,wp,bt,wu,Yo,yu,$u,Zo,Fu,Du,en,Bu,Mu,yp,as,Js,Dl,tn,Eu,Bl,xu,$p,Ke,sn,zu,Ct,ju,vi,Cu,qu,bi,Pu,Au,on,Ou,Nu,Lu,is,Su,ki,Iu,Wu,Ti,Ru,Uu,Qu,Ml,Hu,Vu,nn,Fp,ls,Gs,El,rn,Ku,xl,Ju,Dp,_t,an,Gu,zl,Xu,Yu,Xs,wi,Zu,em,yi,tm,sm,om,ln,nm,$i,rm,am,Bp,ds,Ys,jl,dn,im,Cl,lm,Mp,vt,cn,dm,pn,cm,ql,pm,hm,um,Zs,Fi,mm,fm,Di,gm,_m,vm,hn,bm,Bi,km,Tm,Ep,cs,eo,Pl,un,wm,Al,ym,xp,Je,mn,$m,Ol,Fm,Dm,fn,Bm,Mi,Mm,Em,xm,gn,zm,_n,jm,Cm,qm,tt,vn,Pm,ps,Am,Ei,Om,Nm,Nl,Lm,Sm,Im,to,Wm,Ll,Rm,Um,bn,zp,hs,so,Sl,kn,Qm,Il,Hm,jp,Ge,Tn,Vm,wn,Km,Wl,Jm,Gm,Xm,yn,Ym,xi,Zm,ef,tf,$n,sf,Fn,of,nf,rf,Ie,Dn,af,us,lf,zi,df,cf,Rl,pf,hf,uf,oo,mf,Ul,ff,gf,Bn,_f,Mn,Cp,ms,no,Ql,En,vf,Hl,bf,qp,Xe,xn,kf,Vl,Tf,wf,zn,yf,ji,$f,Ff,Df,jn,Bf,Cn,Mf,Ef,xf,ke,qn,zf,fs,jf,Ci,Cf,qf,Kl,Pf,Af,Of,ro,Nf,Jl,Lf,Sf,Pn,If,An,Wf,Gl,Rf,Uf,On,Qf,Nn,Pp,gs,ao,Xl,Ln,Hf,Yl,Vf,Ap,Ye,Sn,Kf,Zl,Jf,Gf,In,Xf,qi,Yf,Zf,eg,Wn,tg,Rn,sg,og,ng,st,Un,rg,_s,ag,Pi,ig,lg,ed,dg,cg,pg,io,hg,td,ug,mg,Qn,Op,vs,lo,sd,Hn,fg,od,gg,Np,Ze,Vn,_g,nd,vg,bg,Kn,kg,Ai,Tg,wg,yg,Jn,$g,Gn,Fg,Dg,Bg,We,Xn,Mg,bs,Eg,Oi,xg,zg,rd,jg,Cg,qg,co,Pg,ad,Ag,Og,Yn,Ng,Zn,Lp,ks,po,id,er,Lg,ld,Sg,Sp,et,tr,Ig,Ts,Wg,dd,Rg,Ug,cd,Qg,Hg,Vg,sr,Kg,Ni,Jg,Gg,Xg,or,Yg,nr,Zg,e_,t_,Re,rr,s_,ws,o_,Li,n_,r_,pd,a_,i_,l_,ho,d_,hd,c_,p_,ar,h_,ir,Ip,ys,uo,ud,lr,u_,md,m_,Wp,Pe,dr,f_,fd,g_,__,cr,v_,Si,b_,k_,T_,pr,w_,hr,y_,$_,F_,mo,D_,ot,ur,B_,$s,M_,Ii,E_,x_,gd,z_,j_,C_,fo,q_,_d,P_,A_,mr,Rp,Fs,go,vd,fr,O_,bd,N_,Up,Ae,gr,L_,_r,S_,kd,I_,W_,R_,vr,U_,Wi,Q_,H_,V_,br,K_,kr,J_,G_,X_,_o,Y_,Ue,Tr,Z_,Ds,ev,Ri,tv,sv,Td,ov,nv,rv,vo,av,wd,iv,lv,wr,dv,yr,Qp,Bs,bo,yd,$r,cv,$d,pv,Hp,Oe,Fr,hv,Fd,uv,mv,Dr,fv,Ui,gv,_v,vv,Br,bv,Mr,kv,Tv,wv,ko,yv,Qe,Er,$v,Ms,Fv,Qi,Dv,Bv,Dd,Mv,Ev,xv,To,zv,Bd,jv,Cv,xr,qv,zr,Vp,Es,wo,Md,jr,Pv,Ed,Av,Kp,Ne,Cr,Ov,xd,Nv,Lv,qr,Sv,Hi,Iv,Wv,Rv,Pr,Uv,Ar,Qv,Hv,Vv,yo,Kv,nt,Or,Jv,xs,Gv,Vi,Xv,Yv,zd,Zv,e1,t1,$o,s1,jd,o1,n1,Nr,Jp,zs,Fo,Cd,Lr,r1,qd,a1,Gp,Le,Sr,i1,Pd,l1,d1,Ir,c1,Ki,p1,h1,u1,Wr,m1,Rr,f1,g1,_1,Do,v1,He,Ur,b1,js,k1,Ji,T1,w1,Ad,y1,$1,F1,Bo,D1,Od,B1,M1,Qr,E1,Hr,Xp,Cs,Mo,Nd,Vr,x1,Ld,z1,Yp,Se,Kr,j1,qs,C1,Sd,q1,P1,Id,A1,O1,N1,Jr,L1,Gi,S1,I1,W1,Gr,R1,Xr,U1,Q1,H1,Eo,V1,Ve,Yr,K1,Ps,J1,Xi,G1,X1,Wd,Y1,Z1,eb,xo,tb,Rd,sb,ob,Zr,nb,ea,Zp,As,zo,Ud,ta,rb,Qd,ab,eh,Me,sa,ib,Hd,lb,db,oa,cb,Yi,pb,hb,ub,na,mb,ra,fb,gb,_b,Vd,vb,bb,qt,Kd,aa,kb,Tb,Jd,ia,wb,yb,Gd,la,$b,Fb,Xd,da,Db,Bb,rt,ca,Mb,Os,Eb,Yd,xb,zb,Zd,jb,Cb,qb,jo,Pb,ec,Ab,Ob,pa,th,Ns,Co,tc,ha,Nb,sc,Lb,sh,Ee,ua,Sb,ma,Ib,oc,Wb,Rb,Ub,fa,Qb,Zi,Hb,Vb,Kb,ga,Jb,_a,Gb,Xb,Yb,nc,Zb,ek,Pt,rc,va,tk,sk,ac,ba,ok,nk,ic,ka,rk,ak,lc,Ta,ik,lk,at,wa,dk,Ls,ck,dc,pk,hk,cc,uk,mk,fk,qo,gk,pc,_k,vk,ya,oh,Ss,Po,hc,$a,bk,uc,kk,nh,xe,Fa,Tk,mc,wk,yk,Da,$k,el,Fk,Dk,Bk,Ba,Mk,Ma,Ek,xk,zk,fc,jk,Ck,At,gc,Ea,qk,Pk,_c,xa,Ak,Ok,vc,za,Nk,Lk,bc,ja,Sk,Ik,it,Ca,Wk,Is,Rk,kc,Uk,Qk,Tc,Hk,Vk,Kk,Ao,Jk,wc,Gk,Xk,qa,rh,Ws,Oo,yc,Pa,Yk,$c,Zk,ah,ze,Aa,eT,Fc,tT,sT,Oa,oT,tl,nT,rT,aT,Na,iT,La,lT,dT,cT,Dc,pT,hT,Ot,Bc,Sa,uT,mT,Mc,Ia,fT,gT,Ec,Wa,_T,vT,xc,Ra,bT,kT,lt,Ua,TT,Rs,wT,zc,yT,$T,jc,FT,DT,BT,No,MT,Cc,ET,xT,Qa,ih,Us,Lo,qc,Ha,zT,Pc,jT,lh,je,Va,CT,Ac,qT,PT,Ka,AT,sl,OT,NT,LT,Ja,ST,Ga,IT,WT,RT,Oc,UT,QT,Nt,Nc,Xa,HT,VT,Lc,Ya,KT,JT,Sc,Za,GT,XT,Ic,ei,YT,ZT,dt,ti,ew,Qs,tw,Wc,sw,ow,Rc,nw,rw,aw,So,iw,Uc,lw,dw,si,dh,Hs,Io,Qc,oi,cw,Hc,pw,ch,Ce,ni,hw,Vs,uw,Vc,mw,fw,Kc,gw,_w,vw,ri,bw,ol,kw,Tw,ww,ai,yw,ii,$w,Fw,Dw,Jc,Bw,Mw,Lt,Gc,li,Ew,xw,Xc,di,zw,jw,Yc,ci,Cw,qw,Zc,pi,Pw,Aw,ct,hi,Ow,Ks,Nw,ep,Lw,Sw,tp,Iw,Ww,Rw,Wo,Uw,sp,Qw,Hw,ui,ph;return _=new be({}),X=new be({}),tn=new be({}),sn=new H({props:{name:"class transformers.DistilBertConfig",anchor:"transformers.DistilBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"max_position_embeddings",val:" = 512"},{name:"sinusoidal_pos_embds",val:" = False"},{name:"n_layers",val:" = 6"},{name:"n_heads",val:" = 12"},{name:"dim",val:" = 768"},{name:"hidden_dim",val:" = 3072"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"qa_dropout",val:" = 0.1"},{name:"seq_classif_dropout",val:" = 0.2"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/configuration_distilbert.py#L37",parametersDescription:[{anchor:"transformers.DistilBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> or <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DistilBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DistilBertConfig.sinusoidal_pos_embds",description:`<strong>sinusoidal_pos_embds</strong> (<code>boolean</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sinusoidal positional embeddings.`,name:"sinusoidal_pos_embds"},{anchor:"transformers.DistilBertConfig.n_layers",description:`<strong>n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layers"},{anchor:"transformers.DistilBertConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_heads"},{anchor:"transformers.DistilBertConfig.dim",description:`<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"dim"},{anchor:"transformers.DistilBertConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
The size of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"hidden_dim"},{anchor:"transformers.DistilBertConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DistilBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DistilBertConfig.activation",description:`<strong>activation</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation"},{anchor:"transformers.DistilBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DistilBertConfig.qa_dropout",description:`<strong>qa_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilities used in the question answering model <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a>.`,name:"qa_dropout"},{anchor:"transformers.DistilBertConfig.seq_classif_dropout",description:`<strong>seq_classif_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The dropout probabilities used in the sequence classification and the multiple choice model
<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>.`,name:"seq_classif_dropout"}]}}),nn=new re({props:{code:`from transformers import DistilBertModel, DistilBertConfig

# Initializing a DistilBERT configuration
configuration = DistilBertConfig()

# Initializing a model from the configuration
model = DistilBertModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertModel, DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DistilBERT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DistilBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),rn=new be({}),an=new H({props:{name:"class transformers.DistilBertTokenizer",anchor:"transformers.DistilBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/tokenization_distilbert.py#L56"}}),dn=new be({}),cn=new H({props:{name:"class transformers.DistilBertTokenizerFast",anchor:"transformers.DistilBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/tokenization_distilbert_fast.py#L65"}}),un=new be({}),mn=new H({props:{name:"class transformers.DistilBertModel",anchor:"transformers.DistilBertModel",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L457",parametersDescription:[{anchor:"transformers.DistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vn=new H({props:{name:"forward",anchor:"transformers.DistilBertModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L529",parametersDescription:[{anchor:"transformers.DistilBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),to=new ge({props:{$$slots:{default:[MD]},$$scope:{ctx:j}}}),bn=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertModel
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),kn=new be({}),Tn=new H({props:{name:"class transformers.DistilBertForMaskedLM",anchor:"transformers.DistilBertForMaskedLM",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L585",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Dn=new H({props:{name:"forward",anchor:"transformers.DistilBertForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L627",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new ge({props:{$$slots:{default:[ED]},$$scope:{ctx:j}}}),Bn=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMaskedLM
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),Mn=new re({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-[MASK] tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
`}}),En=new be({}),xn=new H({props:{name:"class transformers.DistilBertForSequenceClassification",anchor:"transformers.DistilBertForSequenceClassification",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L691",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qn=new H({props:{name:"forward",anchor:"transformers.DistilBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L725",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new ge({props:{$$slots:{default:[xD]},$$scope:{ctx:j}}}),Pn=new re({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),An=new re({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),On=new re({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),Nn=new re({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=num_labels, problem_type="multi_label_classification"
)

labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),Ln=new be({}),Sn=new H({props:{name:"class transformers.DistilBertForMultipleChoice",anchor:"transformers.DistilBertForMultipleChoice",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L1021",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Un=new H({props:{name:"forward",anchor:"transformers.DistilBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L1053",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new ge({props:{$$slots:{default:[zD]},$$scope:{ctx:j}}}),Qn=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMultipleChoice
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")
model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Hn=new be({}),Vn=new H({props:{name:"class transformers.DistilBertForTokenClassification",anchor:"transformers.DistilBertForTokenClassification",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L926",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Xn=new H({props:{name:"forward",anchor:"transformers.DistilBertForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L958",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new ge({props:{$$slots:{default:[jD]},$$scope:{ctx:j}}}),Yn=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForTokenClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),Zn=new re({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),er=new be({}),tr=new H({props:{name:"class transformers.DistilBertForQuestionAnswering",anchor:"transformers.DistilBertForQuestionAnswering",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L809",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new H({props:{name:"forward",anchor:"transformers.DistilBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_distilbert.py#L841",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ho=new ge({props:{$$slots:{default:[CD]},$$scope:{ctx:j}}}),ar=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),ir=new re({props:{code:`# target is "nice puppet"
target_start_index = torch.tensor([14])
target_end_index = torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = torch.tensor([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),lr=new be({}),dr=new H({props:{name:"class transformers.TFDistilBertModel",anchor:"transformers.TFDistilBertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L522",parametersDescription:[{anchor:"transformers.TFDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ge({props:{$$slots:{default:[qD]},$$scope:{ctx:j}}}),ur=new H({props:{name:"call",anchor:"transformers.TFDistilBertModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L527",parametersDescription:[{anchor:"transformers.TFDistilBertModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),fo=new ge({props:{$$slots:{default:[PD]},$$scope:{ctx:j}}}),mr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),fr=new be({}),gr=new H({props:{name:"class transformers.TFDistilBertForMaskedLM",anchor:"transformers.TFDistilBertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L609",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_o=new ge({props:{$$slots:{default:[AD]},$$scope:{ctx:j}}}),Tr=new H({props:{name:"call",anchor:"transformers.TFDistilBertForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L629",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),vo=new ge({props:{$$slots:{default:[OD]},$$scope:{ctx:j}}}),wr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>selected_logits = tf.gather_nd(logits[<span class="hljs-number">0</span>], indices=mask_token_index)

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = tf.math.argmax(selected_logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),yr=new re({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
# mask labels of non-[MASK] tokens
labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(float(outputs.loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(outputs.loss), <span class="hljs-number">2</span>)
`}}),$r=new be({}),Fr=new H({props:{name:"class transformers.TFDistilBertForSequenceClassification",anchor:"transformers.TFDistilBertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L699",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ko=new ge({props:{$$slots:{default:[ND]},$$scope:{ctx:j}}}),Er=new H({props:{name:"call",anchor:"transformers.TFDistilBertForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L716",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),To=new ge({props:{$$slots:{default:[LD]},$$scope:{ctx:j}}}),xr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

logits = model(**inputs).logits

predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),zr=new re({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = tf.constant(1)
loss = model(**inputs, labels=labels).loss
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.constant(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),jr=new be({}),Cr=new H({props:{name:"class transformers.TFDistilBertForMultipleChoice",anchor:"transformers.TFDistilBertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L862",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yo=new ge({props:{$$slots:{default:[SD]},$$scope:{ctx:j}}}),Or=new H({props:{name:"call",anchor:"transformers.TFDistilBertForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L888",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$o=new ge({props:{$$slots:{default:[ID]},$$scope:{ctx:j}}}),Nr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMultipleChoice
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Lr=new be({}),Sr=new H({props:{name:"class transformers.TFDistilBertForTokenClassification",anchor:"transformers.TFDistilBertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L786",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Do=new ge({props:{$$slots:{default:[WD]},$$scope:{ctx:j}}}),Ur=new H({props:{name:"call",anchor:"transformers.TFDistilBertForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L797",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Bo=new ge({props:{$$slots:{default:[RD]},$$scope:{ctx:j}}}),Qr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
)

logits = model(**inputs).logits
predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>].numpy().tolist()]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),Hr=new re({props:{code:`labels = predicted_token_class_ids
loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),Vr=new be({}),Kr=new H({props:{name:"class transformers.TFDistilBertForQuestionAnswering",anchor:"transformers.TFDistilBertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L987",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Eo=new ge({props:{$$slots:{default:[UD]},$$scope:{ctx:j}}}),Yr=new H({props:{name:"call",anchor:"transformers.TFDistilBertForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_tf_distilbert.py#L998",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),xo=new ge({props:{$$slots:{default:[QD]},$$scope:{ctx:j}}}),Zr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),ea=new re({props:{code:`# target is "nice puppet"
target_start_index = tf.constant([14])
target_end_index = tf.constant([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = tf.math.reduce_mean(outputs.loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = tf.constant([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = tf.constant([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(outputs.loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),ta=new be({}),sa=new H({props:{name:"class transformers.FlaxDistilBertModel",anchor:"transformers.FlaxDistilBertModel",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L523",parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ca=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}]}}),jo=new ge({props:{$$slots:{default:[HD]},$$scope:{ctx:j}}}),pa=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),ha=new be({}),ua=new H({props:{name:"class transformers.FlaxDistilBertForMaskedLM",anchor:"transformers.FlaxDistilBertForMaskedLM",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L596",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),wa=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMaskedLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),qo=new ge({props:{$$slots:{default:[VD]},$$scope:{ctx:j}}}),ya=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMaskedLM

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),$a=new be({}),Fa=new H({props:{name:"class transformers.FlaxDistilBertForSequenceClassification",anchor:"transformers.FlaxDistilBertForSequenceClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L665",parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ca=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ao=new ge({props:{$$slots:{default:[KD]},$$scope:{ctx:j}}}),qa=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Pa=new be({}),Aa=new H({props:{name:"class transformers.FlaxDistilBertForMultipleChoice",anchor:"transformers.FlaxDistilBertForMultipleChoice",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L745",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ua=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),No=new ge({props:{$$slots:{default:[JD]},$$scope:{ctx:j}}}),Qa=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMultipleChoice

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ha=new be({}),Va=new H({props:{name:"class transformers.FlaxDistilBertForTokenClassification",anchor:"transformers.FlaxDistilBertForTokenClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L811",parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ti=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForTokenClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),So=new ge({props:{$$slots:{default:[GD]},$$scope:{ctx:j}}}),si=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),oi=new be({}),ni=new H({props:{name:"class transformers.FlaxDistilBertForQuestionAnswering",anchor:"transformers.FlaxDistilBertForQuestionAnswering",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L881",parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),hi=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16713/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16713/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16713/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16713/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16713/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Wo=new ge({props:{$$slots:{default:[XD]},$$scope:{ctx:j}}}),ui=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){p=o("meta"),D=l(),g=o("h1"),v=o("a"),F=o("span"),b(_.$$.fragment),m=l(),B=o("span"),ce=a("DistilBERT"),V=l(),E=o("h2"),G=o("a"),S=o("span"),b(X.$$.fragment),pe=l(),I=o("span"),he=a("Overview"),ae=l(),N=o("p"),P=a("The DistilBERT model was proposed in the blog post "),Y=o("a"),K=a(`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),x=a(", and the paper "),z=o("a"),ue=a(`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),W=a(`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),se=o("em"),me=a("bert-base-uncased"),R=a(`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),ie=l(),ee=o("p"),A=a("The abstract from the paper is the following:"),le=l(),L=o("p"),oe=o("em"),fe=a(`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),q=l(),te=o("p"),U=a("Tips:"),de=l(),h=o("ul"),M=o("li"),J=a("DistilBERT doesn\u2019t have "),_e=o("code"),Te=a("token_type_ids"),O=a(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),ve=o("code"),we=a("tokenizer.sep_token"),ye=a(" (or "),C=o("code"),Q=a("[SEP]"),$e=a(")."),Fe=l(),Z=o("li"),De=a("DistilBERT doesn\u2019t have options to select the input positions ("),ne=o("code"),Be=a("position_ids"),Tu=a(` input). This could be added if
necessary though, just let us know if you need this option.`),wp=l(),bt=o("p"),wu=a("This model was contributed by "),Yo=o("a"),yu=a("victorsanh"),$u=a(`. This model jax version was
contributed by `),Zo=o("a"),Fu=a("kamalkraj"),Du=a(". The original code can be found "),en=o("a"),Bu=a("here"),Mu=a("."),yp=l(),as=o("h2"),Js=o("a"),Dl=o("span"),b(tn.$$.fragment),Eu=l(),Bl=o("span"),xu=a("DistilBertConfig"),$p=l(),Ke=o("div"),b(sn.$$.fragment),zu=l(),Ct=o("p"),ju=a("This is the configuration class to store the configuration of a "),vi=o("a"),Cu=a("DistilBertModel"),qu=a(" or a "),bi=o("a"),Pu=a("TFDistilBertModel"),Au=a(`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),on=o("a"),Ou=a("distilbert-base-uncased"),Nu=a(" architecture."),Lu=l(),is=o("p"),Su=a("Configuration objects inherit from "),ki=o("a"),Iu=a("PretrainedConfig"),Wu=a(` and can be used to control the model outputs. Read the
documentation from `),Ti=o("a"),Ru=a("PretrainedConfig"),Uu=a(" for more information."),Qu=l(),Ml=o("p"),Hu=a("Examples:"),Vu=l(),b(nn.$$.fragment),Fp=l(),ls=o("h2"),Gs=o("a"),El=o("span"),b(rn.$$.fragment),Ku=l(),xl=o("span"),Ju=a("DistilBertTokenizer"),Dp=l(),_t=o("div"),b(an.$$.fragment),Gu=l(),zl=o("p"),Xu=a("Construct a DistilBERT tokenizer."),Yu=l(),Xs=o("p"),wi=o("a"),Zu=a("DistilBertTokenizer"),em=a(" is identical to "),yi=o("a"),tm=a("BertTokenizer"),sm=a(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),om=l(),ln=o("p"),nm=a("Refer to superclass "),$i=o("a"),rm=a("BertTokenizer"),am=a(" for usage examples and documentation concerning parameters."),Bp=l(),ds=o("h2"),Ys=o("a"),jl=o("span"),b(dn.$$.fragment),im=l(),Cl=o("span"),lm=a("DistilBertTokenizerFast"),Mp=l(),vt=o("div"),b(cn.$$.fragment),dm=l(),pn=o("p"),cm=a("Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),ql=o("em"),pm=a("tokenizers"),hm=a(" library)."),um=l(),Zs=o("p"),Fi=o("a"),mm=a("DistilBertTokenizerFast"),fm=a(" is identical to "),Di=o("a"),gm=a("BertTokenizerFast"),_m=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),vm=l(),hn=o("p"),bm=a("Refer to superclass "),Bi=o("a"),km=a("BertTokenizerFast"),Tm=a(" for usage examples and documentation concerning parameters."),Ep=l(),cs=o("h2"),eo=o("a"),Pl=o("span"),b(un.$$.fragment),wm=l(),Al=o("span"),ym=a("DistilBertModel"),xp=l(),Je=o("div"),b(mn.$$.fragment),$m=l(),Ol=o("p"),Fm=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),Dm=l(),fn=o("p"),Bm=a("This model inherits from "),Mi=o("a"),Mm=a("PreTrainedModel"),Em=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xm=l(),gn=o("p"),zm=a("This model is also a PyTorch "),_n=o("a"),jm=a("torch.nn.Module"),Cm=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),qm=l(),tt=o("div"),b(vn.$$.fragment),Pm=l(),ps=o("p"),Am=a("The "),Ei=o("a"),Om=a("DistilBertModel"),Nm=a(" forward method, overrides the "),Nl=o("code"),Lm=a("__call__"),Sm=a(" special method."),Im=l(),b(to.$$.fragment),Wm=l(),Ll=o("p"),Rm=a("Example:"),Um=l(),b(bn.$$.fragment),zp=l(),hs=o("h2"),so=o("a"),Sl=o("span"),b(kn.$$.fragment),Qm=l(),Il=o("span"),Hm=a("DistilBertForMaskedLM"),jp=l(),Ge=o("div"),b(Tn.$$.fragment),Vm=l(),wn=o("p"),Km=a("DistilBert Model with a "),Wl=o("code"),Jm=a("masked language modeling"),Gm=a(" head on top."),Xm=l(),yn=o("p"),Ym=a("This model inherits from "),xi=o("a"),Zm=a("PreTrainedModel"),ef=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tf=l(),$n=o("p"),sf=a("This model is also a PyTorch "),Fn=o("a"),of=a("torch.nn.Module"),nf=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rf=l(),Ie=o("div"),b(Dn.$$.fragment),af=l(),us=o("p"),lf=a("The "),zi=o("a"),df=a("DistilBertForMaskedLM"),cf=a(" forward method, overrides the "),Rl=o("code"),pf=a("__call__"),hf=a(" special method."),uf=l(),b(oo.$$.fragment),mf=l(),Ul=o("p"),ff=a("Example:"),gf=l(),b(Bn.$$.fragment),_f=l(),b(Mn.$$.fragment),Cp=l(),ms=o("h2"),no=o("a"),Ql=o("span"),b(En.$$.fragment),vf=l(),Hl=o("span"),bf=a("DistilBertForSequenceClassification"),qp=l(),Xe=o("div"),b(xn.$$.fragment),kf=l(),Vl=o("p"),Tf=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),wf=l(),zn=o("p"),yf=a("This model inherits from "),ji=o("a"),$f=a("PreTrainedModel"),Ff=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Df=l(),jn=o("p"),Bf=a("This model is also a PyTorch "),Cn=o("a"),Mf=a("torch.nn.Module"),Ef=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xf=l(),ke=o("div"),b(qn.$$.fragment),zf=l(),fs=o("p"),jf=a("The "),Ci=o("a"),Cf=a("DistilBertForSequenceClassification"),qf=a(" forward method, overrides the "),Kl=o("code"),Pf=a("__call__"),Af=a(" special method."),Of=l(),b(ro.$$.fragment),Nf=l(),Jl=o("p"),Lf=a("Example of single-label classification:"),Sf=l(),b(Pn.$$.fragment),If=l(),b(An.$$.fragment),Wf=l(),Gl=o("p"),Rf=a("Example of multi-label classification:"),Uf=l(),b(On.$$.fragment),Qf=l(),b(Nn.$$.fragment),Pp=l(),gs=o("h2"),ao=o("a"),Xl=o("span"),b(Ln.$$.fragment),Hf=l(),Yl=o("span"),Vf=a("DistilBertForMultipleChoice"),Ap=l(),Ye=o("div"),b(Sn.$$.fragment),Kf=l(),Zl=o("p"),Jf=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Gf=l(),In=o("p"),Xf=a("This model inherits from "),qi=o("a"),Yf=a("PreTrainedModel"),Zf=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eg=l(),Wn=o("p"),tg=a("This model is also a PyTorch "),Rn=o("a"),sg=a("torch.nn.Module"),og=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ng=l(),st=o("div"),b(Un.$$.fragment),rg=l(),_s=o("p"),ag=a("The "),Pi=o("a"),ig=a("DistilBertForMultipleChoice"),lg=a(" forward method, overrides the "),ed=o("code"),dg=a("__call__"),cg=a(" special method."),pg=l(),b(io.$$.fragment),hg=l(),td=o("p"),ug=a("Examples:"),mg=l(),b(Qn.$$.fragment),Op=l(),vs=o("h2"),lo=o("a"),sd=o("span"),b(Hn.$$.fragment),fg=l(),od=o("span"),gg=a("DistilBertForTokenClassification"),Np=l(),Ze=o("div"),b(Vn.$$.fragment),_g=l(),nd=o("p"),vg=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),bg=l(),Kn=o("p"),kg=a("This model inherits from "),Ai=o("a"),Tg=a("PreTrainedModel"),wg=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yg=l(),Jn=o("p"),$g=a("This model is also a PyTorch "),Gn=o("a"),Fg=a("torch.nn.Module"),Dg=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bg=l(),We=o("div"),b(Xn.$$.fragment),Mg=l(),bs=o("p"),Eg=a("The "),Oi=o("a"),xg=a("DistilBertForTokenClassification"),zg=a(" forward method, overrides the "),rd=o("code"),jg=a("__call__"),Cg=a(" special method."),qg=l(),b(co.$$.fragment),Pg=l(),ad=o("p"),Ag=a("Example:"),Og=l(),b(Yn.$$.fragment),Ng=l(),b(Zn.$$.fragment),Lp=l(),ks=o("h2"),po=o("a"),id=o("span"),b(er.$$.fragment),Lg=l(),ld=o("span"),Sg=a("DistilBertForQuestionAnswering"),Sp=l(),et=o("div"),b(tr.$$.fragment),Ig=l(),Ts=o("p"),Wg=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),dd=o("code"),Rg=a("span start logits"),Ug=a(" and "),cd=o("code"),Qg=a("span end logits"),Hg=a(")."),Vg=l(),sr=o("p"),Kg=a("This model inherits from "),Ni=o("a"),Jg=a("PreTrainedModel"),Gg=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xg=l(),or=o("p"),Yg=a("This model is also a PyTorch "),nr=o("a"),Zg=a("torch.nn.Module"),e_=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),t_=l(),Re=o("div"),b(rr.$$.fragment),s_=l(),ws=o("p"),o_=a("The "),Li=o("a"),n_=a("DistilBertForQuestionAnswering"),r_=a(" forward method, overrides the "),pd=o("code"),a_=a("__call__"),i_=a(" special method."),l_=l(),b(ho.$$.fragment),d_=l(),hd=o("p"),c_=a("Example:"),p_=l(),b(ar.$$.fragment),h_=l(),b(ir.$$.fragment),Ip=l(),ys=o("h2"),uo=o("a"),ud=o("span"),b(lr.$$.fragment),u_=l(),md=o("span"),m_=a("TFDistilBertModel"),Wp=l(),Pe=o("div"),b(dr.$$.fragment),f_=l(),fd=o("p"),g_=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),__=l(),cr=o("p"),v_=a("This model inherits from "),Si=o("a"),b_=a("TFPreTrainedModel"),k_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),T_=l(),pr=o("p"),w_=a("This model is also a "),hr=o("a"),y_=a("tf.keras.Model"),$_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),F_=l(),b(mo.$$.fragment),D_=l(),ot=o("div"),b(ur.$$.fragment),B_=l(),$s=o("p"),M_=a("The "),Ii=o("a"),E_=a("TFDistilBertModel"),x_=a(" forward method, overrides the "),gd=o("code"),z_=a("__call__"),j_=a(" special method."),C_=l(),b(fo.$$.fragment),q_=l(),_d=o("p"),P_=a("Example:"),A_=l(),b(mr.$$.fragment),Rp=l(),Fs=o("h2"),go=o("a"),vd=o("span"),b(fr.$$.fragment),O_=l(),bd=o("span"),N_=a("TFDistilBertForMaskedLM"),Up=l(),Ae=o("div"),b(gr.$$.fragment),L_=l(),_r=o("p"),S_=a("DistilBert Model with a "),kd=o("code"),I_=a("masked language modeling"),W_=a(" head on top."),R_=l(),vr=o("p"),U_=a("This model inherits from "),Wi=o("a"),Q_=a("TFPreTrainedModel"),H_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),V_=l(),br=o("p"),K_=a("This model is also a "),kr=o("a"),J_=a("tf.keras.Model"),G_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),X_=l(),b(_o.$$.fragment),Y_=l(),Ue=o("div"),b(Tr.$$.fragment),Z_=l(),Ds=o("p"),ev=a("The "),Ri=o("a"),tv=a("TFDistilBertForMaskedLM"),sv=a(" forward method, overrides the "),Td=o("code"),ov=a("__call__"),nv=a(" special method."),rv=l(),b(vo.$$.fragment),av=l(),wd=o("p"),iv=a("Example:"),lv=l(),b(wr.$$.fragment),dv=l(),b(yr.$$.fragment),Qp=l(),Bs=o("h2"),bo=o("a"),yd=o("span"),b($r.$$.fragment),cv=l(),$d=o("span"),pv=a("TFDistilBertForSequenceClassification"),Hp=l(),Oe=o("div"),b(Fr.$$.fragment),hv=l(),Fd=o("p"),uv=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),mv=l(),Dr=o("p"),fv=a("This model inherits from "),Ui=o("a"),gv=a("TFPreTrainedModel"),_v=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vv=l(),Br=o("p"),bv=a("This model is also a "),Mr=o("a"),kv=a("tf.keras.Model"),Tv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wv=l(),b(ko.$$.fragment),yv=l(),Qe=o("div"),b(Er.$$.fragment),$v=l(),Ms=o("p"),Fv=a("The "),Qi=o("a"),Dv=a("TFDistilBertForSequenceClassification"),Bv=a(" forward method, overrides the "),Dd=o("code"),Mv=a("__call__"),Ev=a(" special method."),xv=l(),b(To.$$.fragment),zv=l(),Bd=o("p"),jv=a("Example:"),Cv=l(),b(xr.$$.fragment),qv=l(),b(zr.$$.fragment),Vp=l(),Es=o("h2"),wo=o("a"),Md=o("span"),b(jr.$$.fragment),Pv=l(),Ed=o("span"),Av=a("TFDistilBertForMultipleChoice"),Kp=l(),Ne=o("div"),b(Cr.$$.fragment),Ov=l(),xd=o("p"),Nv=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Lv=l(),qr=o("p"),Sv=a("This model inherits from "),Hi=o("a"),Iv=a("TFPreTrainedModel"),Wv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rv=l(),Pr=o("p"),Uv=a("This model is also a "),Ar=o("a"),Qv=a("tf.keras.Model"),Hv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Vv=l(),b(yo.$$.fragment),Kv=l(),nt=o("div"),b(Or.$$.fragment),Jv=l(),xs=o("p"),Gv=a("The "),Vi=o("a"),Xv=a("TFDistilBertForMultipleChoice"),Yv=a(" forward method, overrides the "),zd=o("code"),Zv=a("__call__"),e1=a(" special method."),t1=l(),b($o.$$.fragment),s1=l(),jd=o("p"),o1=a("Example:"),n1=l(),b(Nr.$$.fragment),Jp=l(),zs=o("h2"),Fo=o("a"),Cd=o("span"),b(Lr.$$.fragment),r1=l(),qd=o("span"),a1=a("TFDistilBertForTokenClassification"),Gp=l(),Le=o("div"),b(Sr.$$.fragment),i1=l(),Pd=o("p"),l1=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),d1=l(),Ir=o("p"),c1=a("This model inherits from "),Ki=o("a"),p1=a("TFPreTrainedModel"),h1=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),u1=l(),Wr=o("p"),m1=a("This model is also a "),Rr=o("a"),f1=a("tf.keras.Model"),g1=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_1=l(),b(Do.$$.fragment),v1=l(),He=o("div"),b(Ur.$$.fragment),b1=l(),js=o("p"),k1=a("The "),Ji=o("a"),T1=a("TFDistilBertForTokenClassification"),w1=a(" forward method, overrides the "),Ad=o("code"),y1=a("__call__"),$1=a(" special method."),F1=l(),b(Bo.$$.fragment),D1=l(),Od=o("p"),B1=a("Example:"),M1=l(),b(Qr.$$.fragment),E1=l(),b(Hr.$$.fragment),Xp=l(),Cs=o("h2"),Mo=o("a"),Nd=o("span"),b(Vr.$$.fragment),x1=l(),Ld=o("span"),z1=a("TFDistilBertForQuestionAnswering"),Yp=l(),Se=o("div"),b(Kr.$$.fragment),j1=l(),qs=o("p"),C1=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),Sd=o("code"),q1=a("span start logits"),P1=a(" and "),Id=o("code"),A1=a("span end logits"),O1=a(")."),N1=l(),Jr=o("p"),L1=a("This model inherits from "),Gi=o("a"),S1=a("TFPreTrainedModel"),I1=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),W1=l(),Gr=o("p"),R1=a("This model is also a "),Xr=o("a"),U1=a("tf.keras.Model"),Q1=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),H1=l(),b(Eo.$$.fragment),V1=l(),Ve=o("div"),b(Yr.$$.fragment),K1=l(),Ps=o("p"),J1=a("The "),Xi=o("a"),G1=a("TFDistilBertForQuestionAnswering"),X1=a(" forward method, overrides the "),Wd=o("code"),Y1=a("__call__"),Z1=a(" special method."),eb=l(),b(xo.$$.fragment),tb=l(),Rd=o("p"),sb=a("Example:"),ob=l(),b(Zr.$$.fragment),nb=l(),b(ea.$$.fragment),Zp=l(),As=o("h2"),zo=o("a"),Ud=o("span"),b(ta.$$.fragment),rb=l(),Qd=o("span"),ab=a("FlaxDistilBertModel"),eh=l(),Me=o("div"),b(sa.$$.fragment),ib=l(),Hd=o("p"),lb=a("The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),db=l(),oa=o("p"),cb=a("This model inherits from "),Yi=o("a"),pb=a("FlaxPreTrainedModel"),hb=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),ub=l(),na=o("p"),mb=a("This model is also a Flax Linen "),ra=o("a"),fb=a("flax.linen.Module"),gb=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),_b=l(),Vd=o("p"),vb=a("Finally, this model supports inherent JAX features such as:"),bb=l(),qt=o("ul"),Kd=o("li"),aa=o("a"),kb=a("Just-In-Time (JIT) compilation"),Tb=l(),Jd=o("li"),ia=o("a"),wb=a("Automatic Differentiation"),yb=l(),Gd=o("li"),la=o("a"),$b=a("Vectorization"),Fb=l(),Xd=o("li"),da=o("a"),Db=a("Parallelization"),Bb=l(),rt=o("div"),b(ca.$$.fragment),Mb=l(),Os=o("p"),Eb=a("The "),Yd=o("code"),xb=a("FlaxDistilBertPreTrainedModel"),zb=a(" forward method, overrides the "),Zd=o("code"),jb=a("__call__"),Cb=a(" special method."),qb=l(),b(jo.$$.fragment),Pb=l(),ec=o("p"),Ab=a("Example:"),Ob=l(),b(pa.$$.fragment),th=l(),Ns=o("h2"),Co=o("a"),tc=o("span"),b(ha.$$.fragment),Nb=l(),sc=o("span"),Lb=a("FlaxDistilBertForMaskedLM"),sh=l(),Ee=o("div"),b(ua.$$.fragment),Sb=l(),ma=o("p"),Ib=a("DistilBert Model with a "),oc=o("code"),Wb=a("language modeling"),Rb=a(" head on top."),Ub=l(),fa=o("p"),Qb=a("This model inherits from "),Zi=o("a"),Hb=a("FlaxPreTrainedModel"),Vb=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Kb=l(),ga=o("p"),Jb=a("This model is also a Flax Linen "),_a=o("a"),Gb=a("flax.linen.Module"),Xb=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Yb=l(),nc=o("p"),Zb=a("Finally, this model supports inherent JAX features such as:"),ek=l(),Pt=o("ul"),rc=o("li"),va=o("a"),tk=a("Just-In-Time (JIT) compilation"),sk=l(),ac=o("li"),ba=o("a"),ok=a("Automatic Differentiation"),nk=l(),ic=o("li"),ka=o("a"),rk=a("Vectorization"),ak=l(),lc=o("li"),Ta=o("a"),ik=a("Parallelization"),lk=l(),at=o("div"),b(wa.$$.fragment),dk=l(),Ls=o("p"),ck=a("The "),dc=o("code"),pk=a("FlaxDistilBertPreTrainedModel"),hk=a(" forward method, overrides the "),cc=o("code"),uk=a("__call__"),mk=a(" special method."),fk=l(),b(qo.$$.fragment),gk=l(),pc=o("p"),_k=a("Example:"),vk=l(),b(ya.$$.fragment),oh=l(),Ss=o("h2"),Po=o("a"),hc=o("span"),b($a.$$.fragment),bk=l(),uc=o("span"),kk=a("FlaxDistilBertForSequenceClassification"),nh=l(),xe=o("div"),b(Fa.$$.fragment),Tk=l(),mc=o("p"),wk=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),yk=l(),Da=o("p"),$k=a("This model inherits from "),el=o("a"),Fk=a("FlaxPreTrainedModel"),Dk=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Bk=l(),Ba=o("p"),Mk=a("This model is also a Flax Linen "),Ma=o("a"),Ek=a("flax.linen.Module"),xk=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),zk=l(),fc=o("p"),jk=a("Finally, this model supports inherent JAX features such as:"),Ck=l(),At=o("ul"),gc=o("li"),Ea=o("a"),qk=a("Just-In-Time (JIT) compilation"),Pk=l(),_c=o("li"),xa=o("a"),Ak=a("Automatic Differentiation"),Ok=l(),vc=o("li"),za=o("a"),Nk=a("Vectorization"),Lk=l(),bc=o("li"),ja=o("a"),Sk=a("Parallelization"),Ik=l(),it=o("div"),b(Ca.$$.fragment),Wk=l(),Is=o("p"),Rk=a("The "),kc=o("code"),Uk=a("FlaxDistilBertPreTrainedModel"),Qk=a(" forward method, overrides the "),Tc=o("code"),Hk=a("__call__"),Vk=a(" special method."),Kk=l(),b(Ao.$$.fragment),Jk=l(),wc=o("p"),Gk=a("Example:"),Xk=l(),b(qa.$$.fragment),rh=l(),Ws=o("h2"),Oo=o("a"),yc=o("span"),b(Pa.$$.fragment),Yk=l(),$c=o("span"),Zk=a("FlaxDistilBertForMultipleChoice"),ah=l(),ze=o("div"),b(Aa.$$.fragment),eT=l(),Fc=o("p"),tT=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),sT=l(),Oa=o("p"),oT=a("This model inherits from "),tl=o("a"),nT=a("FlaxPreTrainedModel"),rT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),aT=l(),Na=o("p"),iT=a("This model is also a Flax Linen "),La=o("a"),lT=a("flax.linen.Module"),dT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),cT=l(),Dc=o("p"),pT=a("Finally, this model supports inherent JAX features such as:"),hT=l(),Ot=o("ul"),Bc=o("li"),Sa=o("a"),uT=a("Just-In-Time (JIT) compilation"),mT=l(),Mc=o("li"),Ia=o("a"),fT=a("Automatic Differentiation"),gT=l(),Ec=o("li"),Wa=o("a"),_T=a("Vectorization"),vT=l(),xc=o("li"),Ra=o("a"),bT=a("Parallelization"),kT=l(),lt=o("div"),b(Ua.$$.fragment),TT=l(),Rs=o("p"),wT=a("The "),zc=o("code"),yT=a("FlaxDistilBertPreTrainedModel"),$T=a(" forward method, overrides the "),jc=o("code"),FT=a("__call__"),DT=a(" special method."),BT=l(),b(No.$$.fragment),MT=l(),Cc=o("p"),ET=a("Example:"),xT=l(),b(Qa.$$.fragment),ih=l(),Us=o("h2"),Lo=o("a"),qc=o("span"),b(Ha.$$.fragment),zT=l(),Pc=o("span"),jT=a("FlaxDistilBertForTokenClassification"),lh=l(),je=o("div"),b(Va.$$.fragment),CT=l(),Ac=o("p"),qT=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),PT=l(),Ka=o("p"),AT=a("This model inherits from "),sl=o("a"),OT=a("FlaxPreTrainedModel"),NT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),LT=l(),Ja=o("p"),ST=a("This model is also a Flax Linen "),Ga=o("a"),IT=a("flax.linen.Module"),WT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),RT=l(),Oc=o("p"),UT=a("Finally, this model supports inherent JAX features such as:"),QT=l(),Nt=o("ul"),Nc=o("li"),Xa=o("a"),HT=a("Just-In-Time (JIT) compilation"),VT=l(),Lc=o("li"),Ya=o("a"),KT=a("Automatic Differentiation"),JT=l(),Sc=o("li"),Za=o("a"),GT=a("Vectorization"),XT=l(),Ic=o("li"),ei=o("a"),YT=a("Parallelization"),ZT=l(),dt=o("div"),b(ti.$$.fragment),ew=l(),Qs=o("p"),tw=a("The "),Wc=o("code"),sw=a("FlaxDistilBertPreTrainedModel"),ow=a(" forward method, overrides the "),Rc=o("code"),nw=a("__call__"),rw=a(" special method."),aw=l(),b(So.$$.fragment),iw=l(),Uc=o("p"),lw=a("Example:"),dw=l(),b(si.$$.fragment),dh=l(),Hs=o("h2"),Io=o("a"),Qc=o("span"),b(oi.$$.fragment),cw=l(),Hc=o("span"),pw=a("FlaxDistilBertForQuestionAnswering"),ch=l(),Ce=o("div"),b(ni.$$.fragment),hw=l(),Vs=o("p"),uw=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Vc=o("code"),mw=a("span start logits"),fw=a(" and "),Kc=o("code"),gw=a("span end logits"),_w=a(")."),vw=l(),ri=o("p"),bw=a("This model inherits from "),ol=o("a"),kw=a("FlaxPreTrainedModel"),Tw=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),ww=l(),ai=o("p"),yw=a("This model is also a Flax Linen "),ii=o("a"),$w=a("flax.linen.Module"),Fw=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Dw=l(),Jc=o("p"),Bw=a("Finally, this model supports inherent JAX features such as:"),Mw=l(),Lt=o("ul"),Gc=o("li"),li=o("a"),Ew=a("Just-In-Time (JIT) compilation"),xw=l(),Xc=o("li"),di=o("a"),zw=a("Automatic Differentiation"),jw=l(),Yc=o("li"),ci=o("a"),Cw=a("Vectorization"),qw=l(),Zc=o("li"),pi=o("a"),Pw=a("Parallelization"),Aw=l(),ct=o("div"),b(hi.$$.fragment),Ow=l(),Ks=o("p"),Nw=a("The "),ep=o("code"),Lw=a("FlaxDistilBertPreTrainedModel"),Sw=a(" forward method, overrides the "),tp=o("code"),Iw=a("__call__"),Ww=a(" special method."),Rw=l(),b(Wo.$$.fragment),Uw=l(),sp=o("p"),Qw=a("Example:"),Hw=l(),b(ui.$$.fragment),this.h()},l(s){const f=DD('[data-svelte="svelte-1phssyn"]',document.head);p=n(f,"META",{name:!0,content:!0}),f.forEach(t),D=d(s),g=n(s,"H1",{class:!0});var mi=r(g);v=n(mi,"A",{id:!0,class:!0,href:!0});var op=r(v);F=n(op,"SPAN",{});var np=r(F);k(_.$$.fragment,np),np.forEach(t),op.forEach(t),m=d(mi),B=n(mi,"SPAN",{});var rp=r(B);ce=i(rp,"DistilBERT"),rp.forEach(t),mi.forEach(t),V=d(s),E=n(s,"H2",{class:!0});var fi=r(E);G=n(fi,"A",{id:!0,class:!0,href:!0});var ap=r(G);S=n(ap,"SPAN",{});var ip=r(S);k(X.$$.fragment,ip),ip.forEach(t),ap.forEach(t),pe=d(fi),I=n(fi,"SPAN",{});var lp=r(I);he=i(lp,"Overview"),lp.forEach(t),fi.forEach(t),ae=d(s),N=n(s,"P",{});var St=r(N);P=i(St,"The DistilBERT model was proposed in the blog post "),Y=n(St,"A",{href:!0,rel:!0});var dp=r(Y);K=i(dp,`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),dp.forEach(t),x=i(St,", and the paper "),z=n(St,"A",{href:!0,rel:!0});var cp=r(z);ue=i(cp,`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),cp.forEach(t),W=i(St,`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),se=n(St,"EM",{});var pp=r(se);me=i(pp,"bert-base-uncased"),pp.forEach(t),R=i(St,`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),St.forEach(t),ie=d(s),ee=n(s,"P",{});var hp=r(ee);A=i(hp,"The abstract from the paper is the following:"),hp.forEach(t),le=d(s),L=n(s,"P",{});var up=r(L);oe=n(up,"EM",{});var mp=r(oe);fe=i(mp,`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),mp.forEach(t),up.forEach(t),q=d(s),te=n(s,"P",{});var fp=r(te);U=i(fp,"Tips:"),fp.forEach(t),de=d(s),h=n(s,"UL",{});var gi=r(h);M=n(gi,"LI",{});var It=r(M);J=i(It,"DistilBERT doesn\u2019t have "),_e=n(It,"CODE",{});var gp=r(_e);Te=i(gp,"token_type_ids"),gp.forEach(t),O=i(It,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),ve=n(It,"CODE",{});var _p=r(ve);we=i(_p,"tokenizer.sep_token"),_p.forEach(t),ye=i(It," (or "),C=n(It,"CODE",{});var vp=r(C);Q=i(vp,"[SEP]"),vp.forEach(t),$e=i(It,")."),It.forEach(t),Fe=d(gi),Z=n(gi,"LI",{});var _i=r(Z);De=i(_i,"DistilBERT doesn\u2019t have options to select the input positions ("),ne=n(_i,"CODE",{});var bp=r(ne);Be=i(bp,"position_ids"),bp.forEach(t),Tu=i(_i,` input). This could be added if
necessary though, just let us know if you need this option.`),_i.forEach(t),gi.forEach(t),wp=d(s),bt=n(s,"P",{});var Wt=r(bt);wu=i(Wt,"This model was contributed by "),Yo=n(Wt,"A",{href:!0,rel:!0});var Vw=r(Yo);yu=i(Vw,"victorsanh"),Vw.forEach(t),$u=i(Wt,`. This model jax version was
contributed by `),Zo=n(Wt,"A",{href:!0,rel:!0});var Kw=r(Zo);Fu=i(Kw,"kamalkraj"),Kw.forEach(t),Du=i(Wt,". The original code can be found "),en=n(Wt,"A",{href:!0,rel:!0});var Jw=r(en);Bu=i(Jw,"here"),Jw.forEach(t),Mu=i(Wt,"."),Wt.forEach(t),yp=d(s),as=n(s,"H2",{class:!0});var hh=r(as);Js=n(hh,"A",{id:!0,class:!0,href:!0});var Gw=r(Js);Dl=n(Gw,"SPAN",{});var Xw=r(Dl);k(tn.$$.fragment,Xw),Xw.forEach(t),Gw.forEach(t),Eu=d(hh),Bl=n(hh,"SPAN",{});var Yw=r(Bl);xu=i(Yw,"DistilBertConfig"),Yw.forEach(t),hh.forEach(t),$p=d(s),Ke=n(s,"DIV",{class:!0});var Rt=r(Ke);k(sn.$$.fragment,Rt),zu=d(Rt),Ct=n(Rt,"P",{});var Ro=r(Ct);ju=i(Ro,"This is the configuration class to store the configuration of a "),vi=n(Ro,"A",{href:!0});var Zw=r(vi);Cu=i(Zw,"DistilBertModel"),Zw.forEach(t),qu=i(Ro," or a "),bi=n(Ro,"A",{href:!0});var ey=r(bi);Pu=i(ey,"TFDistilBertModel"),ey.forEach(t),Au=i(Ro,`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),on=n(Ro,"A",{href:!0,rel:!0});var ty=r(on);Ou=i(ty,"distilbert-base-uncased"),ty.forEach(t),Nu=i(Ro," architecture."),Ro.forEach(t),Lu=d(Rt),is=n(Rt,"P",{});var nl=r(is);Su=i(nl,"Configuration objects inherit from "),ki=n(nl,"A",{href:!0});var sy=r(ki);Iu=i(sy,"PretrainedConfig"),sy.forEach(t),Wu=i(nl,` and can be used to control the model outputs. Read the
documentation from `),Ti=n(nl,"A",{href:!0});var oy=r(Ti);Ru=i(oy,"PretrainedConfig"),oy.forEach(t),Uu=i(nl," for more information."),nl.forEach(t),Qu=d(Rt),Ml=n(Rt,"P",{});var ny=r(Ml);Hu=i(ny,"Examples:"),ny.forEach(t),Vu=d(Rt),k(nn.$$.fragment,Rt),Rt.forEach(t),Fp=d(s),ls=n(s,"H2",{class:!0});var uh=r(ls);Gs=n(uh,"A",{id:!0,class:!0,href:!0});var ry=r(Gs);El=n(ry,"SPAN",{});var ay=r(El);k(rn.$$.fragment,ay),ay.forEach(t),ry.forEach(t),Ku=d(uh),xl=n(uh,"SPAN",{});var iy=r(xl);Ju=i(iy,"DistilBertTokenizer"),iy.forEach(t),uh.forEach(t),Dp=d(s),_t=n(s,"DIV",{class:!0});var Uo=r(_t);k(an.$$.fragment,Uo),Gu=d(Uo),zl=n(Uo,"P",{});var ly=r(zl);Xu=i(ly,"Construct a DistilBERT tokenizer."),ly.forEach(t),Yu=d(Uo),Xs=n(Uo,"P",{});var kp=r(Xs);wi=n(kp,"A",{href:!0});var dy=r(wi);Zu=i(dy,"DistilBertTokenizer"),dy.forEach(t),em=i(kp," is identical to "),yi=n(kp,"A",{href:!0});var cy=r(yi);tm=i(cy,"BertTokenizer"),cy.forEach(t),sm=i(kp,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),kp.forEach(t),om=d(Uo),ln=n(Uo,"P",{});var mh=r(ln);nm=i(mh,"Refer to superclass "),$i=n(mh,"A",{href:!0});var py=r($i);rm=i(py,"BertTokenizer"),py.forEach(t),am=i(mh," for usage examples and documentation concerning parameters."),mh.forEach(t),Uo.forEach(t),Bp=d(s),ds=n(s,"H2",{class:!0});var fh=r(ds);Ys=n(fh,"A",{id:!0,class:!0,href:!0});var hy=r(Ys);jl=n(hy,"SPAN",{});var uy=r(jl);k(dn.$$.fragment,uy),uy.forEach(t),hy.forEach(t),im=d(fh),Cl=n(fh,"SPAN",{});var my=r(Cl);lm=i(my,"DistilBertTokenizerFast"),my.forEach(t),fh.forEach(t),Mp=d(s),vt=n(s,"DIV",{class:!0});var Qo=r(vt);k(cn.$$.fragment,Qo),dm=d(Qo),pn=n(Qo,"P",{});var gh=r(pn);cm=i(gh,"Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),ql=n(gh,"EM",{});var fy=r(ql);pm=i(fy,"tokenizers"),fy.forEach(t),hm=i(gh," library)."),gh.forEach(t),um=d(Qo),Zs=n(Qo,"P",{});var Tp=r(Zs);Fi=n(Tp,"A",{href:!0});var gy=r(Fi);mm=i(gy,"DistilBertTokenizerFast"),gy.forEach(t),fm=i(Tp," is identical to "),Di=n(Tp,"A",{href:!0});var _y=r(Di);gm=i(_y,"BertTokenizerFast"),_y.forEach(t),_m=i(Tp,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Tp.forEach(t),vm=d(Qo),hn=n(Qo,"P",{});var _h=r(hn);bm=i(_h,"Refer to superclass "),Bi=n(_h,"A",{href:!0});var vy=r(Bi);km=i(vy,"BertTokenizerFast"),vy.forEach(t),Tm=i(_h," for usage examples and documentation concerning parameters."),_h.forEach(t),Qo.forEach(t),Ep=d(s),cs=n(s,"H2",{class:!0});var vh=r(cs);eo=n(vh,"A",{id:!0,class:!0,href:!0});var by=r(eo);Pl=n(by,"SPAN",{});var ky=r(Pl);k(un.$$.fragment,ky),ky.forEach(t),by.forEach(t),wm=d(vh),Al=n(vh,"SPAN",{});var Ty=r(Al);ym=i(Ty,"DistilBertModel"),Ty.forEach(t),vh.forEach(t),xp=d(s),Je=n(s,"DIV",{class:!0});var Ut=r(Je);k(mn.$$.fragment,Ut),$m=d(Ut),Ol=n(Ut,"P",{});var wy=r(Ol);Fm=i(wy,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),wy.forEach(t),Dm=d(Ut),fn=n(Ut,"P",{});var bh=r(fn);Bm=i(bh,"This model inherits from "),Mi=n(bh,"A",{href:!0});var yy=r(Mi);Mm=i(yy,"PreTrainedModel"),yy.forEach(t),Em=i(bh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),bh.forEach(t),xm=d(Ut),gn=n(Ut,"P",{});var kh=r(gn);zm=i(kh,"This model is also a PyTorch "),_n=n(kh,"A",{href:!0,rel:!0});var $y=r(_n);jm=i($y,"torch.nn.Module"),$y.forEach(t),Cm=i(kh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kh.forEach(t),qm=d(Ut),tt=n(Ut,"DIV",{class:!0});var Qt=r(tt);k(vn.$$.fragment,Qt),Pm=d(Qt),ps=n(Qt,"P",{});var rl=r(ps);Am=i(rl,"The "),Ei=n(rl,"A",{href:!0});var Fy=r(Ei);Om=i(Fy,"DistilBertModel"),Fy.forEach(t),Nm=i(rl," forward method, overrides the "),Nl=n(rl,"CODE",{});var Dy=r(Nl);Lm=i(Dy,"__call__"),Dy.forEach(t),Sm=i(rl," special method."),rl.forEach(t),Im=d(Qt),k(to.$$.fragment,Qt),Wm=d(Qt),Ll=n(Qt,"P",{});var By=r(Ll);Rm=i(By,"Example:"),By.forEach(t),Um=d(Qt),k(bn.$$.fragment,Qt),Qt.forEach(t),Ut.forEach(t),zp=d(s),hs=n(s,"H2",{class:!0});var Th=r(hs);so=n(Th,"A",{id:!0,class:!0,href:!0});var My=r(so);Sl=n(My,"SPAN",{});var Ey=r(Sl);k(kn.$$.fragment,Ey),Ey.forEach(t),My.forEach(t),Qm=d(Th),Il=n(Th,"SPAN",{});var xy=r(Il);Hm=i(xy,"DistilBertForMaskedLM"),xy.forEach(t),Th.forEach(t),jp=d(s),Ge=n(s,"DIV",{class:!0});var Ht=r(Ge);k(Tn.$$.fragment,Ht),Vm=d(Ht),wn=n(Ht,"P",{});var wh=r(wn);Km=i(wh,"DistilBert Model with a "),Wl=n(wh,"CODE",{});var zy=r(Wl);Jm=i(zy,"masked language modeling"),zy.forEach(t),Gm=i(wh," head on top."),wh.forEach(t),Xm=d(Ht),yn=n(Ht,"P",{});var yh=r(yn);Ym=i(yh,"This model inherits from "),xi=n(yh,"A",{href:!0});var jy=r(xi);Zm=i(jy,"PreTrainedModel"),jy.forEach(t),ef=i(yh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yh.forEach(t),tf=d(Ht),$n=n(Ht,"P",{});var $h=r($n);sf=i($h,"This model is also a PyTorch "),Fn=n($h,"A",{href:!0,rel:!0});var Cy=r(Fn);of=i(Cy,"torch.nn.Module"),Cy.forEach(t),nf=i($h,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),$h.forEach(t),rf=d(Ht),Ie=n(Ht,"DIV",{class:!0});var kt=r(Ie);k(Dn.$$.fragment,kt),af=d(kt),us=n(kt,"P",{});var al=r(us);lf=i(al,"The "),zi=n(al,"A",{href:!0});var qy=r(zi);df=i(qy,"DistilBertForMaskedLM"),qy.forEach(t),cf=i(al," forward method, overrides the "),Rl=n(al,"CODE",{});var Py=r(Rl);pf=i(Py,"__call__"),Py.forEach(t),hf=i(al," special method."),al.forEach(t),uf=d(kt),k(oo.$$.fragment,kt),mf=d(kt),Ul=n(kt,"P",{});var Ay=r(Ul);ff=i(Ay,"Example:"),Ay.forEach(t),gf=d(kt),k(Bn.$$.fragment,kt),_f=d(kt),k(Mn.$$.fragment,kt),kt.forEach(t),Ht.forEach(t),Cp=d(s),ms=n(s,"H2",{class:!0});var Fh=r(ms);no=n(Fh,"A",{id:!0,class:!0,href:!0});var Oy=r(no);Ql=n(Oy,"SPAN",{});var Ny=r(Ql);k(En.$$.fragment,Ny),Ny.forEach(t),Oy.forEach(t),vf=d(Fh),Hl=n(Fh,"SPAN",{});var Ly=r(Hl);bf=i(Ly,"DistilBertForSequenceClassification"),Ly.forEach(t),Fh.forEach(t),qp=d(s),Xe=n(s,"DIV",{class:!0});var Vt=r(Xe);k(xn.$$.fragment,Vt),kf=d(Vt),Vl=n(Vt,"P",{});var Sy=r(Vl);Tf=i(Sy,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Sy.forEach(t),wf=d(Vt),zn=n(Vt,"P",{});var Dh=r(zn);yf=i(Dh,"This model inherits from "),ji=n(Dh,"A",{href:!0});var Iy=r(ji);$f=i(Iy,"PreTrainedModel"),Iy.forEach(t),Ff=i(Dh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Dh.forEach(t),Df=d(Vt),jn=n(Vt,"P",{});var Bh=r(jn);Bf=i(Bh,"This model is also a PyTorch "),Cn=n(Bh,"A",{href:!0,rel:!0});var Wy=r(Cn);Mf=i(Wy,"torch.nn.Module"),Wy.forEach(t),Ef=i(Bh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bh.forEach(t),xf=d(Vt),ke=n(Vt,"DIV",{class:!0});var qe=r(ke);k(qn.$$.fragment,qe),zf=d(qe),fs=n(qe,"P",{});var il=r(fs);jf=i(il,"The "),Ci=n(il,"A",{href:!0});var Ry=r(Ci);Cf=i(Ry,"DistilBertForSequenceClassification"),Ry.forEach(t),qf=i(il," forward method, overrides the "),Kl=n(il,"CODE",{});var Uy=r(Kl);Pf=i(Uy,"__call__"),Uy.forEach(t),Af=i(il," special method."),il.forEach(t),Of=d(qe),k(ro.$$.fragment,qe),Nf=d(qe),Jl=n(qe,"P",{});var Qy=r(Jl);Lf=i(Qy,"Example of single-label classification:"),Qy.forEach(t),Sf=d(qe),k(Pn.$$.fragment,qe),If=d(qe),k(An.$$.fragment,qe),Wf=d(qe),Gl=n(qe,"P",{});var Hy=r(Gl);Rf=i(Hy,"Example of multi-label classification:"),Hy.forEach(t),Uf=d(qe),k(On.$$.fragment,qe),Qf=d(qe),k(Nn.$$.fragment,qe),qe.forEach(t),Vt.forEach(t),Pp=d(s),gs=n(s,"H2",{class:!0});var Mh=r(gs);ao=n(Mh,"A",{id:!0,class:!0,href:!0});var Vy=r(ao);Xl=n(Vy,"SPAN",{});var Ky=r(Xl);k(Ln.$$.fragment,Ky),Ky.forEach(t),Vy.forEach(t),Hf=d(Mh),Yl=n(Mh,"SPAN",{});var Jy=r(Yl);Vf=i(Jy,"DistilBertForMultipleChoice"),Jy.forEach(t),Mh.forEach(t),Ap=d(s),Ye=n(s,"DIV",{class:!0});var Kt=r(Ye);k(Sn.$$.fragment,Kt),Kf=d(Kt),Zl=n(Kt,"P",{});var Gy=r(Zl);Jf=i(Gy,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Gy.forEach(t),Gf=d(Kt),In=n(Kt,"P",{});var Eh=r(In);Xf=i(Eh,"This model inherits from "),qi=n(Eh,"A",{href:!0});var Xy=r(qi);Yf=i(Xy,"PreTrainedModel"),Xy.forEach(t),Zf=i(Eh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Eh.forEach(t),eg=d(Kt),Wn=n(Kt,"P",{});var xh=r(Wn);tg=i(xh,"This model is also a PyTorch "),Rn=n(xh,"A",{href:!0,rel:!0});var Yy=r(Rn);sg=i(Yy,"torch.nn.Module"),Yy.forEach(t),og=i(xh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xh.forEach(t),ng=d(Kt),st=n(Kt,"DIV",{class:!0});var Jt=r(st);k(Un.$$.fragment,Jt),rg=d(Jt),_s=n(Jt,"P",{});var ll=r(_s);ag=i(ll,"The "),Pi=n(ll,"A",{href:!0});var Zy=r(Pi);ig=i(Zy,"DistilBertForMultipleChoice"),Zy.forEach(t),lg=i(ll," forward method, overrides the "),ed=n(ll,"CODE",{});var e$=r(ed);dg=i(e$,"__call__"),e$.forEach(t),cg=i(ll," special method."),ll.forEach(t),pg=d(Jt),k(io.$$.fragment,Jt),hg=d(Jt),td=n(Jt,"P",{});var t$=r(td);ug=i(t$,"Examples:"),t$.forEach(t),mg=d(Jt),k(Qn.$$.fragment,Jt),Jt.forEach(t),Kt.forEach(t),Op=d(s),vs=n(s,"H2",{class:!0});var zh=r(vs);lo=n(zh,"A",{id:!0,class:!0,href:!0});var s$=r(lo);sd=n(s$,"SPAN",{});var o$=r(sd);k(Hn.$$.fragment,o$),o$.forEach(t),s$.forEach(t),fg=d(zh),od=n(zh,"SPAN",{});var n$=r(od);gg=i(n$,"DistilBertForTokenClassification"),n$.forEach(t),zh.forEach(t),Np=d(s),Ze=n(s,"DIV",{class:!0});var Gt=r(Ze);k(Vn.$$.fragment,Gt),_g=d(Gt),nd=n(Gt,"P",{});var r$=r(nd);vg=i(r$,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),r$.forEach(t),bg=d(Gt),Kn=n(Gt,"P",{});var jh=r(Kn);kg=i(jh,"This model inherits from "),Ai=n(jh,"A",{href:!0});var a$=r(Ai);Tg=i(a$,"PreTrainedModel"),a$.forEach(t),wg=i(jh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jh.forEach(t),yg=d(Gt),Jn=n(Gt,"P",{});var Ch=r(Jn);$g=i(Ch,"This model is also a PyTorch "),Gn=n(Ch,"A",{href:!0,rel:!0});var i$=r(Gn);Fg=i(i$,"torch.nn.Module"),i$.forEach(t),Dg=i(Ch,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ch.forEach(t),Bg=d(Gt),We=n(Gt,"DIV",{class:!0});var Tt=r(We);k(Xn.$$.fragment,Tt),Mg=d(Tt),bs=n(Tt,"P",{});var dl=r(bs);Eg=i(dl,"The "),Oi=n(dl,"A",{href:!0});var l$=r(Oi);xg=i(l$,"DistilBertForTokenClassification"),l$.forEach(t),zg=i(dl," forward method, overrides the "),rd=n(dl,"CODE",{});var d$=r(rd);jg=i(d$,"__call__"),d$.forEach(t),Cg=i(dl," special method."),dl.forEach(t),qg=d(Tt),k(co.$$.fragment,Tt),Pg=d(Tt),ad=n(Tt,"P",{});var c$=r(ad);Ag=i(c$,"Example:"),c$.forEach(t),Og=d(Tt),k(Yn.$$.fragment,Tt),Ng=d(Tt),k(Zn.$$.fragment,Tt),Tt.forEach(t),Gt.forEach(t),Lp=d(s),ks=n(s,"H2",{class:!0});var qh=r(ks);po=n(qh,"A",{id:!0,class:!0,href:!0});var p$=r(po);id=n(p$,"SPAN",{});var h$=r(id);k(er.$$.fragment,h$),h$.forEach(t),p$.forEach(t),Lg=d(qh),ld=n(qh,"SPAN",{});var u$=r(ld);Sg=i(u$,"DistilBertForQuestionAnswering"),u$.forEach(t),qh.forEach(t),Sp=d(s),et=n(s,"DIV",{class:!0});var Xt=r(et);k(tr.$$.fragment,Xt),Ig=d(Xt),Ts=n(Xt,"P",{});var cl=r(Ts);Wg=i(cl,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),dd=n(cl,"CODE",{});var m$=r(dd);Rg=i(m$,"span start logits"),m$.forEach(t),Ug=i(cl," and "),cd=n(cl,"CODE",{});var f$=r(cd);Qg=i(f$,"span end logits"),f$.forEach(t),Hg=i(cl,")."),cl.forEach(t),Vg=d(Xt),sr=n(Xt,"P",{});var Ph=r(sr);Kg=i(Ph,"This model inherits from "),Ni=n(Ph,"A",{href:!0});var g$=r(Ni);Jg=i(g$,"PreTrainedModel"),g$.forEach(t),Gg=i(Ph,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ph.forEach(t),Xg=d(Xt),or=n(Xt,"P",{});var Ah=r(or);Yg=i(Ah,"This model is also a PyTorch "),nr=n(Ah,"A",{href:!0,rel:!0});var _$=r(nr);Zg=i(_$,"torch.nn.Module"),_$.forEach(t),e_=i(Ah,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ah.forEach(t),t_=d(Xt),Re=n(Xt,"DIV",{class:!0});var wt=r(Re);k(rr.$$.fragment,wt),s_=d(wt),ws=n(wt,"P",{});var pl=r(ws);o_=i(pl,"The "),Li=n(pl,"A",{href:!0});var v$=r(Li);n_=i(v$,"DistilBertForQuestionAnswering"),v$.forEach(t),r_=i(pl," forward method, overrides the "),pd=n(pl,"CODE",{});var b$=r(pd);a_=i(b$,"__call__"),b$.forEach(t),i_=i(pl," special method."),pl.forEach(t),l_=d(wt),k(ho.$$.fragment,wt),d_=d(wt),hd=n(wt,"P",{});var k$=r(hd);c_=i(k$,"Example:"),k$.forEach(t),p_=d(wt),k(ar.$$.fragment,wt),h_=d(wt),k(ir.$$.fragment,wt),wt.forEach(t),Xt.forEach(t),Ip=d(s),ys=n(s,"H2",{class:!0});var Oh=r(ys);uo=n(Oh,"A",{id:!0,class:!0,href:!0});var T$=r(uo);ud=n(T$,"SPAN",{});var w$=r(ud);k(lr.$$.fragment,w$),w$.forEach(t),T$.forEach(t),u_=d(Oh),md=n(Oh,"SPAN",{});var y$=r(md);m_=i(y$,"TFDistilBertModel"),y$.forEach(t),Oh.forEach(t),Wp=d(s),Pe=n(s,"DIV",{class:!0});var yt=r(Pe);k(dr.$$.fragment,yt),f_=d(yt),fd=n(yt,"P",{});var $$=r(fd);g_=i($$,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),$$.forEach(t),__=d(yt),cr=n(yt,"P",{});var Nh=r(cr);v_=i(Nh,"This model inherits from "),Si=n(Nh,"A",{href:!0});var F$=r(Si);b_=i(F$,"TFPreTrainedModel"),F$.forEach(t),k_=i(Nh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Nh.forEach(t),T_=d(yt),pr=n(yt,"P",{});var Lh=r(pr);w_=i(Lh,"This model is also a "),hr=n(Lh,"A",{href:!0,rel:!0});var D$=r(hr);y_=i(D$,"tf.keras.Model"),D$.forEach(t),$_=i(Lh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lh.forEach(t),F_=d(yt),k(mo.$$.fragment,yt),D_=d(yt),ot=n(yt,"DIV",{class:!0});var Yt=r(ot);k(ur.$$.fragment,Yt),B_=d(Yt),$s=n(Yt,"P",{});var hl=r($s);M_=i(hl,"The "),Ii=n(hl,"A",{href:!0});var B$=r(Ii);E_=i(B$,"TFDistilBertModel"),B$.forEach(t),x_=i(hl," forward method, overrides the "),gd=n(hl,"CODE",{});var M$=r(gd);z_=i(M$,"__call__"),M$.forEach(t),j_=i(hl," special method."),hl.forEach(t),C_=d(Yt),k(fo.$$.fragment,Yt),q_=d(Yt),_d=n(Yt,"P",{});var E$=r(_d);P_=i(E$,"Example:"),E$.forEach(t),A_=d(Yt),k(mr.$$.fragment,Yt),Yt.forEach(t),yt.forEach(t),Rp=d(s),Fs=n(s,"H2",{class:!0});var Sh=r(Fs);go=n(Sh,"A",{id:!0,class:!0,href:!0});var x$=r(go);vd=n(x$,"SPAN",{});var z$=r(vd);k(fr.$$.fragment,z$),z$.forEach(t),x$.forEach(t),O_=d(Sh),bd=n(Sh,"SPAN",{});var j$=r(bd);N_=i(j$,"TFDistilBertForMaskedLM"),j$.forEach(t),Sh.forEach(t),Up=d(s),Ae=n(s,"DIV",{class:!0});var $t=r(Ae);k(gr.$$.fragment,$t),L_=d($t),_r=n($t,"P",{});var Ih=r(_r);S_=i(Ih,"DistilBert Model with a "),kd=n(Ih,"CODE",{});var C$=r(kd);I_=i(C$,"masked language modeling"),C$.forEach(t),W_=i(Ih," head on top."),Ih.forEach(t),R_=d($t),vr=n($t,"P",{});var Wh=r(vr);U_=i(Wh,"This model inherits from "),Wi=n(Wh,"A",{href:!0});var q$=r(Wi);Q_=i(q$,"TFPreTrainedModel"),q$.forEach(t),H_=i(Wh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wh.forEach(t),V_=d($t),br=n($t,"P",{});var Rh=r(br);K_=i(Rh,"This model is also a "),kr=n(Rh,"A",{href:!0,rel:!0});var P$=r(kr);J_=i(P$,"tf.keras.Model"),P$.forEach(t),G_=i(Rh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Rh.forEach(t),X_=d($t),k(_o.$$.fragment,$t),Y_=d($t),Ue=n($t,"DIV",{class:!0});var Ft=r(Ue);k(Tr.$$.fragment,Ft),Z_=d(Ft),Ds=n(Ft,"P",{});var ul=r(Ds);ev=i(ul,"The "),Ri=n(ul,"A",{href:!0});var A$=r(Ri);tv=i(A$,"TFDistilBertForMaskedLM"),A$.forEach(t),sv=i(ul," forward method, overrides the "),Td=n(ul,"CODE",{});var O$=r(Td);ov=i(O$,"__call__"),O$.forEach(t),nv=i(ul," special method."),ul.forEach(t),rv=d(Ft),k(vo.$$.fragment,Ft),av=d(Ft),wd=n(Ft,"P",{});var N$=r(wd);iv=i(N$,"Example:"),N$.forEach(t),lv=d(Ft),k(wr.$$.fragment,Ft),dv=d(Ft),k(yr.$$.fragment,Ft),Ft.forEach(t),$t.forEach(t),Qp=d(s),Bs=n(s,"H2",{class:!0});var Uh=r(Bs);bo=n(Uh,"A",{id:!0,class:!0,href:!0});var L$=r(bo);yd=n(L$,"SPAN",{});var S$=r(yd);k($r.$$.fragment,S$),S$.forEach(t),L$.forEach(t),cv=d(Uh),$d=n(Uh,"SPAN",{});var I$=r($d);pv=i(I$,"TFDistilBertForSequenceClassification"),I$.forEach(t),Uh.forEach(t),Hp=d(s),Oe=n(s,"DIV",{class:!0});var Dt=r(Oe);k(Fr.$$.fragment,Dt),hv=d(Dt),Fd=n(Dt,"P",{});var W$=r(Fd);uv=i(W$,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),W$.forEach(t),mv=d(Dt),Dr=n(Dt,"P",{});var Qh=r(Dr);fv=i(Qh,"This model inherits from "),Ui=n(Qh,"A",{href:!0});var R$=r(Ui);gv=i(R$,"TFPreTrainedModel"),R$.forEach(t),_v=i(Qh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qh.forEach(t),vv=d(Dt),Br=n(Dt,"P",{});var Hh=r(Br);bv=i(Hh,"This model is also a "),Mr=n(Hh,"A",{href:!0,rel:!0});var U$=r(Mr);kv=i(U$,"tf.keras.Model"),U$.forEach(t),Tv=i(Hh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Hh.forEach(t),wv=d(Dt),k(ko.$$.fragment,Dt),yv=d(Dt),Qe=n(Dt,"DIV",{class:!0});var Bt=r(Qe);k(Er.$$.fragment,Bt),$v=d(Bt),Ms=n(Bt,"P",{});var ml=r(Ms);Fv=i(ml,"The "),Qi=n(ml,"A",{href:!0});var Q$=r(Qi);Dv=i(Q$,"TFDistilBertForSequenceClassification"),Q$.forEach(t),Bv=i(ml," forward method, overrides the "),Dd=n(ml,"CODE",{});var H$=r(Dd);Mv=i(H$,"__call__"),H$.forEach(t),Ev=i(ml," special method."),ml.forEach(t),xv=d(Bt),k(To.$$.fragment,Bt),zv=d(Bt),Bd=n(Bt,"P",{});var V$=r(Bd);jv=i(V$,"Example:"),V$.forEach(t),Cv=d(Bt),k(xr.$$.fragment,Bt),qv=d(Bt),k(zr.$$.fragment,Bt),Bt.forEach(t),Dt.forEach(t),Vp=d(s),Es=n(s,"H2",{class:!0});var Vh=r(Es);wo=n(Vh,"A",{id:!0,class:!0,href:!0});var K$=r(wo);Md=n(K$,"SPAN",{});var J$=r(Md);k(jr.$$.fragment,J$),J$.forEach(t),K$.forEach(t),Pv=d(Vh),Ed=n(Vh,"SPAN",{});var G$=r(Ed);Av=i(G$,"TFDistilBertForMultipleChoice"),G$.forEach(t),Vh.forEach(t),Kp=d(s),Ne=n(s,"DIV",{class:!0});var Mt=r(Ne);k(Cr.$$.fragment,Mt),Ov=d(Mt),xd=n(Mt,"P",{});var X$=r(xd);Nv=i(X$,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),X$.forEach(t),Lv=d(Mt),qr=n(Mt,"P",{});var Kh=r(qr);Sv=i(Kh,"This model inherits from "),Hi=n(Kh,"A",{href:!0});var Y$=r(Hi);Iv=i(Y$,"TFPreTrainedModel"),Y$.forEach(t),Wv=i(Kh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Kh.forEach(t),Rv=d(Mt),Pr=n(Mt,"P",{});var Jh=r(Pr);Uv=i(Jh,"This model is also a "),Ar=n(Jh,"A",{href:!0,rel:!0});var Z$=r(Ar);Qv=i(Z$,"tf.keras.Model"),Z$.forEach(t),Hv=i(Jh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Jh.forEach(t),Vv=d(Mt),k(yo.$$.fragment,Mt),Kv=d(Mt),nt=n(Mt,"DIV",{class:!0});var Zt=r(nt);k(Or.$$.fragment,Zt),Jv=d(Zt),xs=n(Zt,"P",{});var fl=r(xs);Gv=i(fl,"The "),Vi=n(fl,"A",{href:!0});var e2=r(Vi);Xv=i(e2,"TFDistilBertForMultipleChoice"),e2.forEach(t),Yv=i(fl," forward method, overrides the "),zd=n(fl,"CODE",{});var t2=r(zd);Zv=i(t2,"__call__"),t2.forEach(t),e1=i(fl," special method."),fl.forEach(t),t1=d(Zt),k($o.$$.fragment,Zt),s1=d(Zt),jd=n(Zt,"P",{});var s2=r(jd);o1=i(s2,"Example:"),s2.forEach(t),n1=d(Zt),k(Nr.$$.fragment,Zt),Zt.forEach(t),Mt.forEach(t),Jp=d(s),zs=n(s,"H2",{class:!0});var Gh=r(zs);Fo=n(Gh,"A",{id:!0,class:!0,href:!0});var o2=r(Fo);Cd=n(o2,"SPAN",{});var n2=r(Cd);k(Lr.$$.fragment,n2),n2.forEach(t),o2.forEach(t),r1=d(Gh),qd=n(Gh,"SPAN",{});var r2=r(qd);a1=i(r2,"TFDistilBertForTokenClassification"),r2.forEach(t),Gh.forEach(t),Gp=d(s),Le=n(s,"DIV",{class:!0});var Et=r(Le);k(Sr.$$.fragment,Et),i1=d(Et),Pd=n(Et,"P",{});var a2=r(Pd);l1=i(a2,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),a2.forEach(t),d1=d(Et),Ir=n(Et,"P",{});var Xh=r(Ir);c1=i(Xh,"This model inherits from "),Ki=n(Xh,"A",{href:!0});var i2=r(Ki);p1=i(i2,"TFPreTrainedModel"),i2.forEach(t),h1=i(Xh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xh.forEach(t),u1=d(Et),Wr=n(Et,"P",{});var Yh=r(Wr);m1=i(Yh,"This model is also a "),Rr=n(Yh,"A",{href:!0,rel:!0});var l2=r(Rr);f1=i(l2,"tf.keras.Model"),l2.forEach(t),g1=i(Yh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Yh.forEach(t),_1=d(Et),k(Do.$$.fragment,Et),v1=d(Et),He=n(Et,"DIV",{class:!0});var xt=r(He);k(Ur.$$.fragment,xt),b1=d(xt),js=n(xt,"P",{});var gl=r(js);k1=i(gl,"The "),Ji=n(gl,"A",{href:!0});var d2=r(Ji);T1=i(d2,"TFDistilBertForTokenClassification"),d2.forEach(t),w1=i(gl," forward method, overrides the "),Ad=n(gl,"CODE",{});var c2=r(Ad);y1=i(c2,"__call__"),c2.forEach(t),$1=i(gl," special method."),gl.forEach(t),F1=d(xt),k(Bo.$$.fragment,xt),D1=d(xt),Od=n(xt,"P",{});var p2=r(Od);B1=i(p2,"Example:"),p2.forEach(t),M1=d(xt),k(Qr.$$.fragment,xt),E1=d(xt),k(Hr.$$.fragment,xt),xt.forEach(t),Et.forEach(t),Xp=d(s),Cs=n(s,"H2",{class:!0});var Zh=r(Cs);Mo=n(Zh,"A",{id:!0,class:!0,href:!0});var h2=r(Mo);Nd=n(h2,"SPAN",{});var u2=r(Nd);k(Vr.$$.fragment,u2),u2.forEach(t),h2.forEach(t),x1=d(Zh),Ld=n(Zh,"SPAN",{});var m2=r(Ld);z1=i(m2,"TFDistilBertForQuestionAnswering"),m2.forEach(t),Zh.forEach(t),Yp=d(s),Se=n(s,"DIV",{class:!0});var zt=r(Se);k(Kr.$$.fragment,zt),j1=d(zt),qs=n(zt,"P",{});var _l=r(qs);C1=i(_l,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),Sd=n(_l,"CODE",{});var f2=r(Sd);q1=i(f2,"span start logits"),f2.forEach(t),P1=i(_l," and "),Id=n(_l,"CODE",{});var g2=r(Id);A1=i(g2,"span end logits"),g2.forEach(t),O1=i(_l,")."),_l.forEach(t),N1=d(zt),Jr=n(zt,"P",{});var eu=r(Jr);L1=i(eu,"This model inherits from "),Gi=n(eu,"A",{href:!0});var _2=r(Gi);S1=i(_2,"TFPreTrainedModel"),_2.forEach(t),I1=i(eu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eu.forEach(t),W1=d(zt),Gr=n(zt,"P",{});var tu=r(Gr);R1=i(tu,"This model is also a "),Xr=n(tu,"A",{href:!0,rel:!0});var v2=r(Xr);U1=i(v2,"tf.keras.Model"),v2.forEach(t),Q1=i(tu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),tu.forEach(t),H1=d(zt),k(Eo.$$.fragment,zt),V1=d(zt),Ve=n(zt,"DIV",{class:!0});var jt=r(Ve);k(Yr.$$.fragment,jt),K1=d(jt),Ps=n(jt,"P",{});var vl=r(Ps);J1=i(vl,"The "),Xi=n(vl,"A",{href:!0});var b2=r(Xi);G1=i(b2,"TFDistilBertForQuestionAnswering"),b2.forEach(t),X1=i(vl," forward method, overrides the "),Wd=n(vl,"CODE",{});var k2=r(Wd);Y1=i(k2,"__call__"),k2.forEach(t),Z1=i(vl," special method."),vl.forEach(t),eb=d(jt),k(xo.$$.fragment,jt),tb=d(jt),Rd=n(jt,"P",{});var T2=r(Rd);sb=i(T2,"Example:"),T2.forEach(t),ob=d(jt),k(Zr.$$.fragment,jt),nb=d(jt),k(ea.$$.fragment,jt),jt.forEach(t),zt.forEach(t),Zp=d(s),As=n(s,"H2",{class:!0});var su=r(As);zo=n(su,"A",{id:!0,class:!0,href:!0});var w2=r(zo);Ud=n(w2,"SPAN",{});var y2=r(Ud);k(ta.$$.fragment,y2),y2.forEach(t),w2.forEach(t),rb=d(su),Qd=n(su,"SPAN",{});var $2=r(Qd);ab=i($2,"FlaxDistilBertModel"),$2.forEach(t),su.forEach(t),eh=d(s),Me=n(s,"DIV",{class:!0});var pt=r(Me);k(sa.$$.fragment,pt),ib=d(pt),Hd=n(pt,"P",{});var F2=r(Hd);lb=i(F2,"The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),F2.forEach(t),db=d(pt),oa=n(pt,"P",{});var ou=r(oa);cb=i(ou,"This model inherits from "),Yi=n(ou,"A",{href:!0});var D2=r(Yi);pb=i(D2,"FlaxPreTrainedModel"),D2.forEach(t),hb=i(ou,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),ou.forEach(t),ub=d(pt),na=n(pt,"P",{});var nu=r(na);mb=i(nu,"This model is also a Flax Linen "),ra=n(nu,"A",{href:!0,rel:!0});var B2=r(ra);fb=i(B2,"flax.linen.Module"),B2.forEach(t),gb=i(nu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),nu.forEach(t),_b=d(pt),Vd=n(pt,"P",{});var M2=r(Vd);vb=i(M2,"Finally, this model supports inherent JAX features such as:"),M2.forEach(t),bb=d(pt),qt=n(pt,"UL",{});var Ho=r(qt);Kd=n(Ho,"LI",{});var E2=r(Kd);aa=n(E2,"A",{href:!0,rel:!0});var x2=r(aa);kb=i(x2,"Just-In-Time (JIT) compilation"),x2.forEach(t),E2.forEach(t),Tb=d(Ho),Jd=n(Ho,"LI",{});var z2=r(Jd);ia=n(z2,"A",{href:!0,rel:!0});var j2=r(ia);wb=i(j2,"Automatic Differentiation"),j2.forEach(t),z2.forEach(t),yb=d(Ho),Gd=n(Ho,"LI",{});var C2=r(Gd);la=n(C2,"A",{href:!0,rel:!0});var q2=r(la);$b=i(q2,"Vectorization"),q2.forEach(t),C2.forEach(t),Fb=d(Ho),Xd=n(Ho,"LI",{});var P2=r(Xd);da=n(P2,"A",{href:!0,rel:!0});var A2=r(da);Db=i(A2,"Parallelization"),A2.forEach(t),P2.forEach(t),Ho.forEach(t),Bb=d(pt),rt=n(pt,"DIV",{class:!0});var es=r(rt);k(ca.$$.fragment,es),Mb=d(es),Os=n(es,"P",{});var bl=r(Os);Eb=i(bl,"The "),Yd=n(bl,"CODE",{});var O2=r(Yd);xb=i(O2,"FlaxDistilBertPreTrainedModel"),O2.forEach(t),zb=i(bl," forward method, overrides the "),Zd=n(bl,"CODE",{});var N2=r(Zd);jb=i(N2,"__call__"),N2.forEach(t),Cb=i(bl," special method."),bl.forEach(t),qb=d(es),k(jo.$$.fragment,es),Pb=d(es),ec=n(es,"P",{});var L2=r(ec);Ab=i(L2,"Example:"),L2.forEach(t),Ob=d(es),k(pa.$$.fragment,es),es.forEach(t),pt.forEach(t),th=d(s),Ns=n(s,"H2",{class:!0});var ru=r(Ns);Co=n(ru,"A",{id:!0,class:!0,href:!0});var S2=r(Co);tc=n(S2,"SPAN",{});var I2=r(tc);k(ha.$$.fragment,I2),I2.forEach(t),S2.forEach(t),Nb=d(ru),sc=n(ru,"SPAN",{});var W2=r(sc);Lb=i(W2,"FlaxDistilBertForMaskedLM"),W2.forEach(t),ru.forEach(t),sh=d(s),Ee=n(s,"DIV",{class:!0});var ht=r(Ee);k(ua.$$.fragment,ht),Sb=d(ht),ma=n(ht,"P",{});var au=r(ma);Ib=i(au,"DistilBert Model with a "),oc=n(au,"CODE",{});var R2=r(oc);Wb=i(R2,"language modeling"),R2.forEach(t),Rb=i(au," head on top."),au.forEach(t),Ub=d(ht),fa=n(ht,"P",{});var iu=r(fa);Qb=i(iu,"This model inherits from "),Zi=n(iu,"A",{href:!0});var U2=r(Zi);Hb=i(U2,"FlaxPreTrainedModel"),U2.forEach(t),Vb=i(iu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),iu.forEach(t),Kb=d(ht),ga=n(ht,"P",{});var lu=r(ga);Jb=i(lu,"This model is also a Flax Linen "),_a=n(lu,"A",{href:!0,rel:!0});var Q2=r(_a);Gb=i(Q2,"flax.linen.Module"),Q2.forEach(t),Xb=i(lu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),lu.forEach(t),Yb=d(ht),nc=n(ht,"P",{});var H2=r(nc);Zb=i(H2,"Finally, this model supports inherent JAX features such as:"),H2.forEach(t),ek=d(ht),Pt=n(ht,"UL",{});var Vo=r(Pt);rc=n(Vo,"LI",{});var V2=r(rc);va=n(V2,"A",{href:!0,rel:!0});var K2=r(va);tk=i(K2,"Just-In-Time (JIT) compilation"),K2.forEach(t),V2.forEach(t),sk=d(Vo),ac=n(Vo,"LI",{});var J2=r(ac);ba=n(J2,"A",{href:!0,rel:!0});var G2=r(ba);ok=i(G2,"Automatic Differentiation"),G2.forEach(t),J2.forEach(t),nk=d(Vo),ic=n(Vo,"LI",{});var X2=r(ic);ka=n(X2,"A",{href:!0,rel:!0});var Y2=r(ka);rk=i(Y2,"Vectorization"),Y2.forEach(t),X2.forEach(t),ak=d(Vo),lc=n(Vo,"LI",{});var Z2=r(lc);Ta=n(Z2,"A",{href:!0,rel:!0});var eF=r(Ta);ik=i(eF,"Parallelization"),eF.forEach(t),Z2.forEach(t),Vo.forEach(t),lk=d(ht),at=n(ht,"DIV",{class:!0});var ts=r(at);k(wa.$$.fragment,ts),dk=d(ts),Ls=n(ts,"P",{});var kl=r(Ls);ck=i(kl,"The "),dc=n(kl,"CODE",{});var tF=r(dc);pk=i(tF,"FlaxDistilBertPreTrainedModel"),tF.forEach(t),hk=i(kl," forward method, overrides the "),cc=n(kl,"CODE",{});var sF=r(cc);uk=i(sF,"__call__"),sF.forEach(t),mk=i(kl," special method."),kl.forEach(t),fk=d(ts),k(qo.$$.fragment,ts),gk=d(ts),pc=n(ts,"P",{});var oF=r(pc);_k=i(oF,"Example:"),oF.forEach(t),vk=d(ts),k(ya.$$.fragment,ts),ts.forEach(t),ht.forEach(t),oh=d(s),Ss=n(s,"H2",{class:!0});var du=r(Ss);Po=n(du,"A",{id:!0,class:!0,href:!0});var nF=r(Po);hc=n(nF,"SPAN",{});var rF=r(hc);k($a.$$.fragment,rF),rF.forEach(t),nF.forEach(t),bk=d(du),uc=n(du,"SPAN",{});var aF=r(uc);kk=i(aF,"FlaxDistilBertForSequenceClassification"),aF.forEach(t),du.forEach(t),nh=d(s),xe=n(s,"DIV",{class:!0});var ut=r(xe);k(Fa.$$.fragment,ut),Tk=d(ut),mc=n(ut,"P",{});var iF=r(mc);wk=i(iF,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),iF.forEach(t),yk=d(ut),Da=n(ut,"P",{});var cu=r(Da);$k=i(cu,"This model inherits from "),el=n(cu,"A",{href:!0});var lF=r(el);Fk=i(lF,"FlaxPreTrainedModel"),lF.forEach(t),Dk=i(cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),cu.forEach(t),Bk=d(ut),Ba=n(ut,"P",{});var pu=r(Ba);Mk=i(pu,"This model is also a Flax Linen "),Ma=n(pu,"A",{href:!0,rel:!0});var dF=r(Ma);Ek=i(dF,"flax.linen.Module"),dF.forEach(t),xk=i(pu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),pu.forEach(t),zk=d(ut),fc=n(ut,"P",{});var cF=r(fc);jk=i(cF,"Finally, this model supports inherent JAX features such as:"),cF.forEach(t),Ck=d(ut),At=n(ut,"UL",{});var Ko=r(At);gc=n(Ko,"LI",{});var pF=r(gc);Ea=n(pF,"A",{href:!0,rel:!0});var hF=r(Ea);qk=i(hF,"Just-In-Time (JIT) compilation"),hF.forEach(t),pF.forEach(t),Pk=d(Ko),_c=n(Ko,"LI",{});var uF=r(_c);xa=n(uF,"A",{href:!0,rel:!0});var mF=r(xa);Ak=i(mF,"Automatic Differentiation"),mF.forEach(t),uF.forEach(t),Ok=d(Ko),vc=n(Ko,"LI",{});var fF=r(vc);za=n(fF,"A",{href:!0,rel:!0});var gF=r(za);Nk=i(gF,"Vectorization"),gF.forEach(t),fF.forEach(t),Lk=d(Ko),bc=n(Ko,"LI",{});var _F=r(bc);ja=n(_F,"A",{href:!0,rel:!0});var vF=r(ja);Sk=i(vF,"Parallelization"),vF.forEach(t),_F.forEach(t),Ko.forEach(t),Ik=d(ut),it=n(ut,"DIV",{class:!0});var ss=r(it);k(Ca.$$.fragment,ss),Wk=d(ss),Is=n(ss,"P",{});var Tl=r(Is);Rk=i(Tl,"The "),kc=n(Tl,"CODE",{});var bF=r(kc);Uk=i(bF,"FlaxDistilBertPreTrainedModel"),bF.forEach(t),Qk=i(Tl," forward method, overrides the "),Tc=n(Tl,"CODE",{});var kF=r(Tc);Hk=i(kF,"__call__"),kF.forEach(t),Vk=i(Tl," special method."),Tl.forEach(t),Kk=d(ss),k(Ao.$$.fragment,ss),Jk=d(ss),wc=n(ss,"P",{});var TF=r(wc);Gk=i(TF,"Example:"),TF.forEach(t),Xk=d(ss),k(qa.$$.fragment,ss),ss.forEach(t),ut.forEach(t),rh=d(s),Ws=n(s,"H2",{class:!0});var hu=r(Ws);Oo=n(hu,"A",{id:!0,class:!0,href:!0});var wF=r(Oo);yc=n(wF,"SPAN",{});var yF=r(yc);k(Pa.$$.fragment,yF),yF.forEach(t),wF.forEach(t),Yk=d(hu),$c=n(hu,"SPAN",{});var $F=r($c);Zk=i($F,"FlaxDistilBertForMultipleChoice"),$F.forEach(t),hu.forEach(t),ah=d(s),ze=n(s,"DIV",{class:!0});var mt=r(ze);k(Aa.$$.fragment,mt),eT=d(mt),Fc=n(mt,"P",{});var FF=r(Fc);tT=i(FF,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),FF.forEach(t),sT=d(mt),Oa=n(mt,"P",{});var uu=r(Oa);oT=i(uu,"This model inherits from "),tl=n(uu,"A",{href:!0});var DF=r(tl);nT=i(DF,"FlaxPreTrainedModel"),DF.forEach(t),rT=i(uu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),uu.forEach(t),aT=d(mt),Na=n(mt,"P",{});var mu=r(Na);iT=i(mu,"This model is also a Flax Linen "),La=n(mu,"A",{href:!0,rel:!0});var BF=r(La);lT=i(BF,"flax.linen.Module"),BF.forEach(t),dT=i(mu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),mu.forEach(t),cT=d(mt),Dc=n(mt,"P",{});var MF=r(Dc);pT=i(MF,"Finally, this model supports inherent JAX features such as:"),MF.forEach(t),hT=d(mt),Ot=n(mt,"UL",{});var Jo=r(Ot);Bc=n(Jo,"LI",{});var EF=r(Bc);Sa=n(EF,"A",{href:!0,rel:!0});var xF=r(Sa);uT=i(xF,"Just-In-Time (JIT) compilation"),xF.forEach(t),EF.forEach(t),mT=d(Jo),Mc=n(Jo,"LI",{});var zF=r(Mc);Ia=n(zF,"A",{href:!0,rel:!0});var jF=r(Ia);fT=i(jF,"Automatic Differentiation"),jF.forEach(t),zF.forEach(t),gT=d(Jo),Ec=n(Jo,"LI",{});var CF=r(Ec);Wa=n(CF,"A",{href:!0,rel:!0});var qF=r(Wa);_T=i(qF,"Vectorization"),qF.forEach(t),CF.forEach(t),vT=d(Jo),xc=n(Jo,"LI",{});var PF=r(xc);Ra=n(PF,"A",{href:!0,rel:!0});var AF=r(Ra);bT=i(AF,"Parallelization"),AF.forEach(t),PF.forEach(t),Jo.forEach(t),kT=d(mt),lt=n(mt,"DIV",{class:!0});var os=r(lt);k(Ua.$$.fragment,os),TT=d(os),Rs=n(os,"P",{});var wl=r(Rs);wT=i(wl,"The "),zc=n(wl,"CODE",{});var OF=r(zc);yT=i(OF,"FlaxDistilBertPreTrainedModel"),OF.forEach(t),$T=i(wl," forward method, overrides the "),jc=n(wl,"CODE",{});var NF=r(jc);FT=i(NF,"__call__"),NF.forEach(t),DT=i(wl," special method."),wl.forEach(t),BT=d(os),k(No.$$.fragment,os),MT=d(os),Cc=n(os,"P",{});var LF=r(Cc);ET=i(LF,"Example:"),LF.forEach(t),xT=d(os),k(Qa.$$.fragment,os),os.forEach(t),mt.forEach(t),ih=d(s),Us=n(s,"H2",{class:!0});var fu=r(Us);Lo=n(fu,"A",{id:!0,class:!0,href:!0});var SF=r(Lo);qc=n(SF,"SPAN",{});var IF=r(qc);k(Ha.$$.fragment,IF),IF.forEach(t),SF.forEach(t),zT=d(fu),Pc=n(fu,"SPAN",{});var WF=r(Pc);jT=i(WF,"FlaxDistilBertForTokenClassification"),WF.forEach(t),fu.forEach(t),lh=d(s),je=n(s,"DIV",{class:!0});var ft=r(je);k(Va.$$.fragment,ft),CT=d(ft),Ac=n(ft,"P",{});var RF=r(Ac);qT=i(RF,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),RF.forEach(t),PT=d(ft),Ka=n(ft,"P",{});var gu=r(Ka);AT=i(gu,"This model inherits from "),sl=n(gu,"A",{href:!0});var UF=r(sl);OT=i(UF,"FlaxPreTrainedModel"),UF.forEach(t),NT=i(gu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),gu.forEach(t),LT=d(ft),Ja=n(ft,"P",{});var _u=r(Ja);ST=i(_u,"This model is also a Flax Linen "),Ga=n(_u,"A",{href:!0,rel:!0});var QF=r(Ga);IT=i(QF,"flax.linen.Module"),QF.forEach(t),WT=i(_u,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),_u.forEach(t),RT=d(ft),Oc=n(ft,"P",{});var HF=r(Oc);UT=i(HF,"Finally, this model supports inherent JAX features such as:"),HF.forEach(t),QT=d(ft),Nt=n(ft,"UL",{});var Go=r(Nt);Nc=n(Go,"LI",{});var VF=r(Nc);Xa=n(VF,"A",{href:!0,rel:!0});var KF=r(Xa);HT=i(KF,"Just-In-Time (JIT) compilation"),KF.forEach(t),VF.forEach(t),VT=d(Go),Lc=n(Go,"LI",{});var JF=r(Lc);Ya=n(JF,"A",{href:!0,rel:!0});var GF=r(Ya);KT=i(GF,"Automatic Differentiation"),GF.forEach(t),JF.forEach(t),JT=d(Go),Sc=n(Go,"LI",{});var XF=r(Sc);Za=n(XF,"A",{href:!0,rel:!0});var YF=r(Za);GT=i(YF,"Vectorization"),YF.forEach(t),XF.forEach(t),XT=d(Go),Ic=n(Go,"LI",{});var ZF=r(Ic);ei=n(ZF,"A",{href:!0,rel:!0});var eD=r(ei);YT=i(eD,"Parallelization"),eD.forEach(t),ZF.forEach(t),Go.forEach(t),ZT=d(ft),dt=n(ft,"DIV",{class:!0});var ns=r(dt);k(ti.$$.fragment,ns),ew=d(ns),Qs=n(ns,"P",{});var yl=r(Qs);tw=i(yl,"The "),Wc=n(yl,"CODE",{});var tD=r(Wc);sw=i(tD,"FlaxDistilBertPreTrainedModel"),tD.forEach(t),ow=i(yl," forward method, overrides the "),Rc=n(yl,"CODE",{});var sD=r(Rc);nw=i(sD,"__call__"),sD.forEach(t),rw=i(yl," special method."),yl.forEach(t),aw=d(ns),k(So.$$.fragment,ns),iw=d(ns),Uc=n(ns,"P",{});var oD=r(Uc);lw=i(oD,"Example:"),oD.forEach(t),dw=d(ns),k(si.$$.fragment,ns),ns.forEach(t),ft.forEach(t),dh=d(s),Hs=n(s,"H2",{class:!0});var vu=r(Hs);Io=n(vu,"A",{id:!0,class:!0,href:!0});var nD=r(Io);Qc=n(nD,"SPAN",{});var rD=r(Qc);k(oi.$$.fragment,rD),rD.forEach(t),nD.forEach(t),cw=d(vu),Hc=n(vu,"SPAN",{});var aD=r(Hc);pw=i(aD,"FlaxDistilBertForQuestionAnswering"),aD.forEach(t),vu.forEach(t),ch=d(s),Ce=n(s,"DIV",{class:!0});var gt=r(Ce);k(ni.$$.fragment,gt),hw=d(gt),Vs=n(gt,"P",{});var $l=r(Vs);uw=i($l,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Vc=n($l,"CODE",{});var iD=r(Vc);mw=i(iD,"span start logits"),iD.forEach(t),fw=i($l," and "),Kc=n($l,"CODE",{});var lD=r(Kc);gw=i(lD,"span end logits"),lD.forEach(t),_w=i($l,")."),$l.forEach(t),vw=d(gt),ri=n(gt,"P",{});var bu=r(ri);bw=i(bu,"This model inherits from "),ol=n(bu,"A",{href:!0});var dD=r(ol);kw=i(dD,"FlaxPreTrainedModel"),dD.forEach(t),Tw=i(bu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),bu.forEach(t),ww=d(gt),ai=n(gt,"P",{});var ku=r(ai);yw=i(ku,"This model is also a Flax Linen "),ii=n(ku,"A",{href:!0,rel:!0});var cD=r(ii);$w=i(cD,"flax.linen.Module"),cD.forEach(t),Fw=i(ku,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),ku.forEach(t),Dw=d(gt),Jc=n(gt,"P",{});var pD=r(Jc);Bw=i(pD,"Finally, this model supports inherent JAX features such as:"),pD.forEach(t),Mw=d(gt),Lt=n(gt,"UL",{});var Xo=r(Lt);Gc=n(Xo,"LI",{});var hD=r(Gc);li=n(hD,"A",{href:!0,rel:!0});var uD=r(li);Ew=i(uD,"Just-In-Time (JIT) compilation"),uD.forEach(t),hD.forEach(t),xw=d(Xo),Xc=n(Xo,"LI",{});var mD=r(Xc);di=n(mD,"A",{href:!0,rel:!0});var fD=r(di);zw=i(fD,"Automatic Differentiation"),fD.forEach(t),mD.forEach(t),jw=d(Xo),Yc=n(Xo,"LI",{});var gD=r(Yc);ci=n(gD,"A",{href:!0,rel:!0});var _D=r(ci);Cw=i(_D,"Vectorization"),_D.forEach(t),gD.forEach(t),qw=d(Xo),Zc=n(Xo,"LI",{});var vD=r(Zc);pi=n(vD,"A",{href:!0,rel:!0});var bD=r(pi);Pw=i(bD,"Parallelization"),bD.forEach(t),vD.forEach(t),Xo.forEach(t),Aw=d(gt),ct=n(gt,"DIV",{class:!0});var rs=r(ct);k(hi.$$.fragment,rs),Ow=d(rs),Ks=n(rs,"P",{});var Fl=r(Ks);Nw=i(Fl,"The "),ep=n(Fl,"CODE",{});var kD=r(ep);Lw=i(kD,"FlaxDistilBertPreTrainedModel"),kD.forEach(t),Sw=i(Fl," forward method, overrides the "),tp=n(Fl,"CODE",{});var TD=r(tp);Iw=i(TD,"__call__"),TD.forEach(t),Ww=i(Fl," special method."),Fl.forEach(t),Rw=d(rs),k(Wo.$$.fragment,rs),Uw=d(rs),sp=n(rs,"P",{});var wD=r(sp);Qw=i(wD,"Example:"),wD.forEach(t),Hw=d(rs),k(ui.$$.fragment,rs),rs.forEach(t),gt.forEach(t),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(ZD)),c(v,"id","distilbert"),c(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(v,"href","#distilbert"),c(g,"class","relative group"),c(G,"id","overview"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#overview"),c(E,"class","relative group"),c(Y,"href","https://medium.com/huggingface/distilbert-8cf3380435b5"),c(Y,"rel","nofollow"),c(z,"href","https://arxiv.org/abs/1910.01108"),c(z,"rel","nofollow"),c(Yo,"href","https://huggingface.co/victorsanh"),c(Yo,"rel","nofollow"),c(Zo,"href","https://huggingface.co/kamalkraj"),c(Zo,"rel","nofollow"),c(en,"href","https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"),c(en,"rel","nofollow"),c(Js,"id","transformers.DistilBertConfig"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#transformers.DistilBertConfig"),c(as,"class","relative group"),c(vi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertModel"),c(bi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(on,"href","https://huggingface.co/distilbert-base-uncased"),c(on,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_16713/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ti,"href","/docs/transformers/pr_16713/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ke,"class","docstring"),c(Gs,"id","transformers.DistilBertTokenizer"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#transformers.DistilBertTokenizer"),c(ls,"class","relative group"),c(wi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(yi,"href","/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer"),c($i,"href","/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizer"),c(_t,"class","docstring"),c(Ys,"id","transformers.DistilBertTokenizerFast"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#transformers.DistilBertTokenizerFast"),c(ds,"class","relative group"),c(Fi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(Di,"href","/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizerFast"),c(Bi,"href","/docs/transformers/pr_16713/en/model_doc/bert#transformers.BertTokenizerFast"),c(vt,"class","docstring"),c(eo,"id","transformers.DistilBertModel"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.DistilBertModel"),c(cs,"class","relative group"),c(Mi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel"),c(_n,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(_n,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertModel"),c(tt,"class","docstring"),c(Je,"class","docstring"),c(so,"id","transformers.DistilBertForMaskedLM"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.DistilBertForMaskedLM"),c(hs,"class","relative group"),c(xi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel"),c(Fn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Fn,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(Ie,"class","docstring"),c(Ge,"class","docstring"),c(no,"id","transformers.DistilBertForSequenceClassification"),c(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(no,"href","#transformers.DistilBertForSequenceClassification"),c(ms,"class","relative group"),c(ji,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel"),c(Cn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Cn,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(ke,"class","docstring"),c(Xe,"class","docstring"),c(ao,"id","transformers.DistilBertForMultipleChoice"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.DistilBertForMultipleChoice"),c(gs,"class","relative group"),c(qi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel"),c(Rn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Rn,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(st,"class","docstring"),c(Ye,"class","docstring"),c(lo,"id","transformers.DistilBertForTokenClassification"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.DistilBertForTokenClassification"),c(vs,"class","relative group"),c(Ai,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel"),c(Gn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Gn,"rel","nofollow"),c(Oi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(We,"class","docstring"),c(Ze,"class","docstring"),c(po,"id","transformers.DistilBertForQuestionAnswering"),c(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(po,"href","#transformers.DistilBertForQuestionAnswering"),c(ks,"class","relative group"),c(Ni,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.PreTrainedModel"),c(nr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(nr,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Re,"class","docstring"),c(et,"class","docstring"),c(uo,"id","transformers.TFDistilBertModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFDistilBertModel"),c(ys,"class","relative group"),c(Si,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.TFPreTrainedModel"),c(hr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(hr,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(ot,"class","docstring"),c(Pe,"class","docstring"),c(go,"id","transformers.TFDistilBertForMaskedLM"),c(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(go,"href","#transformers.TFDistilBertForMaskedLM"),c(Fs,"class","relative group"),c(Wi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.TFPreTrainedModel"),c(kr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(kr,"rel","nofollow"),c(Ri,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(Ue,"class","docstring"),c(Ae,"class","docstring"),c(bo,"id","transformers.TFDistilBertForSequenceClassification"),c(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bo,"href","#transformers.TFDistilBertForSequenceClassification"),c(Bs,"class","relative group"),c(Ui,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.TFPreTrainedModel"),c(Mr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Mr,"rel","nofollow"),c(Qi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Qe,"class","docstring"),c(Oe,"class","docstring"),c(wo,"id","transformers.TFDistilBertForMultipleChoice"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFDistilBertForMultipleChoice"),c(Es,"class","relative group"),c(Hi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ar,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ar,"rel","nofollow"),c(Vi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(nt,"class","docstring"),c(Ne,"class","docstring"),c(Fo,"id","transformers.TFDistilBertForTokenClassification"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFDistilBertForTokenClassification"),c(zs,"class","relative group"),c(Ki,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rr,"rel","nofollow"),c(Ji,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(He,"class","docstring"),c(Le,"class","docstring"),c(Mo,"id","transformers.TFDistilBertForQuestionAnswering"),c(Mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mo,"href","#transformers.TFDistilBertForQuestionAnswering"),c(Cs,"class","relative group"),c(Gi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.TFPreTrainedModel"),c(Xr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Xr,"rel","nofollow"),c(Xi,"href","/docs/transformers/pr_16713/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(Ve,"class","docstring"),c(Se,"class","docstring"),c(zo,"id","transformers.FlaxDistilBertModel"),c(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zo,"href","#transformers.FlaxDistilBertModel"),c(As,"class","relative group"),c(Yi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(ra,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(ra,"rel","nofollow"),c(aa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(aa,"rel","nofollow"),c(ia,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ia,"rel","nofollow"),c(la,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(la,"rel","nofollow"),c(da,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(da,"rel","nofollow"),c(rt,"class","docstring"),c(Me,"class","docstring"),c(Co,"id","transformers.FlaxDistilBertForMaskedLM"),c(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Co,"href","#transformers.FlaxDistilBertForMaskedLM"),c(Ns,"class","relative group"),c(Zi,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(_a,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(_a,"rel","nofollow"),c(va,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(va,"rel","nofollow"),c(ba,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ba,"rel","nofollow"),c(ka,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ka,"rel","nofollow"),c(Ta,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Ta,"rel","nofollow"),c(at,"class","docstring"),c(Ee,"class","docstring"),c(Po,"id","transformers.FlaxDistilBertForSequenceClassification"),c(Po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Po,"href","#transformers.FlaxDistilBertForSequenceClassification"),c(Ss,"class","relative group"),c(el,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ma,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ma,"rel","nofollow"),c(Ea,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Ea,"rel","nofollow"),c(xa,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(xa,"rel","nofollow"),c(za,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(za,"rel","nofollow"),c(ja,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ja,"rel","nofollow"),c(it,"class","docstring"),c(xe,"class","docstring"),c(Oo,"id","transformers.FlaxDistilBertForMultipleChoice"),c(Oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oo,"href","#transformers.FlaxDistilBertForMultipleChoice"),c(Ws,"class","relative group"),c(tl,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(La,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(La,"rel","nofollow"),c(Sa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Sa,"rel","nofollow"),c(Ia,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ia,"rel","nofollow"),c(Wa,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Wa,"rel","nofollow"),c(Ra,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Ra,"rel","nofollow"),c(lt,"class","docstring"),c(ze,"class","docstring"),c(Lo,"id","transformers.FlaxDistilBertForTokenClassification"),c(Lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lo,"href","#transformers.FlaxDistilBertForTokenClassification"),c(Us,"class","relative group"),c(sl,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ga,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ga,"rel","nofollow"),c(Xa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Xa,"rel","nofollow"),c(Ya,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ya,"rel","nofollow"),c(Za,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Za,"rel","nofollow"),c(ei,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ei,"rel","nofollow"),c(dt,"class","docstring"),c(je,"class","docstring"),c(Io,"id","transformers.FlaxDistilBertForQuestionAnswering"),c(Io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Io,"href","#transformers.FlaxDistilBertForQuestionAnswering"),c(Hs,"class","relative group"),c(ol,"href","/docs/transformers/pr_16713/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(ii,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(ii,"rel","nofollow"),c(li,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(li,"rel","nofollow"),c(di,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(di,"rel","nofollow"),c(ci,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ci,"rel","nofollow"),c(pi,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(pi,"rel","nofollow"),c(ct,"class","docstring"),c(Ce,"class","docstring")},m(s,f){e(document.head,p),u(s,D,f),u(s,g,f),e(g,v),e(v,F),T(_,F,null),e(g,m),e(g,B),e(B,ce),u(s,V,f),u(s,E,f),e(E,G),e(G,S),T(X,S,null),e(E,pe),e(E,I),e(I,he),u(s,ae,f),u(s,N,f),e(N,P),e(N,Y),e(Y,K),e(N,x),e(N,z),e(z,ue),e(N,W),e(N,se),e(se,me),e(N,R),u(s,ie,f),u(s,ee,f),e(ee,A),u(s,le,f),u(s,L,f),e(L,oe),e(oe,fe),u(s,q,f),u(s,te,f),e(te,U),u(s,de,f),u(s,h,f),e(h,M),e(M,J),e(M,_e),e(_e,Te),e(M,O),e(M,ve),e(ve,we),e(M,ye),e(M,C),e(C,Q),e(M,$e),e(h,Fe),e(h,Z),e(Z,De),e(Z,ne),e(ne,Be),e(Z,Tu),u(s,wp,f),u(s,bt,f),e(bt,wu),e(bt,Yo),e(Yo,yu),e(bt,$u),e(bt,Zo),e(Zo,Fu),e(bt,Du),e(bt,en),e(en,Bu),e(bt,Mu),u(s,yp,f),u(s,as,f),e(as,Js),e(Js,Dl),T(tn,Dl,null),e(as,Eu),e(as,Bl),e(Bl,xu),u(s,$p,f),u(s,Ke,f),T(sn,Ke,null),e(Ke,zu),e(Ke,Ct),e(Ct,ju),e(Ct,vi),e(vi,Cu),e(Ct,qu),e(Ct,bi),e(bi,Pu),e(Ct,Au),e(Ct,on),e(on,Ou),e(Ct,Nu),e(Ke,Lu),e(Ke,is),e(is,Su),e(is,ki),e(ki,Iu),e(is,Wu),e(is,Ti),e(Ti,Ru),e(is,Uu),e(Ke,Qu),e(Ke,Ml),e(Ml,Hu),e(Ke,Vu),T(nn,Ke,null),u(s,Fp,f),u(s,ls,f),e(ls,Gs),e(Gs,El),T(rn,El,null),e(ls,Ku),e(ls,xl),e(xl,Ju),u(s,Dp,f),u(s,_t,f),T(an,_t,null),e(_t,Gu),e(_t,zl),e(zl,Xu),e(_t,Yu),e(_t,Xs),e(Xs,wi),e(wi,Zu),e(Xs,em),e(Xs,yi),e(yi,tm),e(Xs,sm),e(_t,om),e(_t,ln),e(ln,nm),e(ln,$i),e($i,rm),e(ln,am),u(s,Bp,f),u(s,ds,f),e(ds,Ys),e(Ys,jl),T(dn,jl,null),e(ds,im),e(ds,Cl),e(Cl,lm),u(s,Mp,f),u(s,vt,f),T(cn,vt,null),e(vt,dm),e(vt,pn),e(pn,cm),e(pn,ql),e(ql,pm),e(pn,hm),e(vt,um),e(vt,Zs),e(Zs,Fi),e(Fi,mm),e(Zs,fm),e(Zs,Di),e(Di,gm),e(Zs,_m),e(vt,vm),e(vt,hn),e(hn,bm),e(hn,Bi),e(Bi,km),e(hn,Tm),u(s,Ep,f),u(s,cs,f),e(cs,eo),e(eo,Pl),T(un,Pl,null),e(cs,wm),e(cs,Al),e(Al,ym),u(s,xp,f),u(s,Je,f),T(mn,Je,null),e(Je,$m),e(Je,Ol),e(Ol,Fm),e(Je,Dm),e(Je,fn),e(fn,Bm),e(fn,Mi),e(Mi,Mm),e(fn,Em),e(Je,xm),e(Je,gn),e(gn,zm),e(gn,_n),e(_n,jm),e(gn,Cm),e(Je,qm),e(Je,tt),T(vn,tt,null),e(tt,Pm),e(tt,ps),e(ps,Am),e(ps,Ei),e(Ei,Om),e(ps,Nm),e(ps,Nl),e(Nl,Lm),e(ps,Sm),e(tt,Im),T(to,tt,null),e(tt,Wm),e(tt,Ll),e(Ll,Rm),e(tt,Um),T(bn,tt,null),u(s,zp,f),u(s,hs,f),e(hs,so),e(so,Sl),T(kn,Sl,null),e(hs,Qm),e(hs,Il),e(Il,Hm),u(s,jp,f),u(s,Ge,f),T(Tn,Ge,null),e(Ge,Vm),e(Ge,wn),e(wn,Km),e(wn,Wl),e(Wl,Jm),e(wn,Gm),e(Ge,Xm),e(Ge,yn),e(yn,Ym),e(yn,xi),e(xi,Zm),e(yn,ef),e(Ge,tf),e(Ge,$n),e($n,sf),e($n,Fn),e(Fn,of),e($n,nf),e(Ge,rf),e(Ge,Ie),T(Dn,Ie,null),e(Ie,af),e(Ie,us),e(us,lf),e(us,zi),e(zi,df),e(us,cf),e(us,Rl),e(Rl,pf),e(us,hf),e(Ie,uf),T(oo,Ie,null),e(Ie,mf),e(Ie,Ul),e(Ul,ff),e(Ie,gf),T(Bn,Ie,null),e(Ie,_f),T(Mn,Ie,null),u(s,Cp,f),u(s,ms,f),e(ms,no),e(no,Ql),T(En,Ql,null),e(ms,vf),e(ms,Hl),e(Hl,bf),u(s,qp,f),u(s,Xe,f),T(xn,Xe,null),e(Xe,kf),e(Xe,Vl),e(Vl,Tf),e(Xe,wf),e(Xe,zn),e(zn,yf),e(zn,ji),e(ji,$f),e(zn,Ff),e(Xe,Df),e(Xe,jn),e(jn,Bf),e(jn,Cn),e(Cn,Mf),e(jn,Ef),e(Xe,xf),e(Xe,ke),T(qn,ke,null),e(ke,zf),e(ke,fs),e(fs,jf),e(fs,Ci),e(Ci,Cf),e(fs,qf),e(fs,Kl),e(Kl,Pf),e(fs,Af),e(ke,Of),T(ro,ke,null),e(ke,Nf),e(ke,Jl),e(Jl,Lf),e(ke,Sf),T(Pn,ke,null),e(ke,If),T(An,ke,null),e(ke,Wf),e(ke,Gl),e(Gl,Rf),e(ke,Uf),T(On,ke,null),e(ke,Qf),T(Nn,ke,null),u(s,Pp,f),u(s,gs,f),e(gs,ao),e(ao,Xl),T(Ln,Xl,null),e(gs,Hf),e(gs,Yl),e(Yl,Vf),u(s,Ap,f),u(s,Ye,f),T(Sn,Ye,null),e(Ye,Kf),e(Ye,Zl),e(Zl,Jf),e(Ye,Gf),e(Ye,In),e(In,Xf),e(In,qi),e(qi,Yf),e(In,Zf),e(Ye,eg),e(Ye,Wn),e(Wn,tg),e(Wn,Rn),e(Rn,sg),e(Wn,og),e(Ye,ng),e(Ye,st),T(Un,st,null),e(st,rg),e(st,_s),e(_s,ag),e(_s,Pi),e(Pi,ig),e(_s,lg),e(_s,ed),e(ed,dg),e(_s,cg),e(st,pg),T(io,st,null),e(st,hg),e(st,td),e(td,ug),e(st,mg),T(Qn,st,null),u(s,Op,f),u(s,vs,f),e(vs,lo),e(lo,sd),T(Hn,sd,null),e(vs,fg),e(vs,od),e(od,gg),u(s,Np,f),u(s,Ze,f),T(Vn,Ze,null),e(Ze,_g),e(Ze,nd),e(nd,vg),e(Ze,bg),e(Ze,Kn),e(Kn,kg),e(Kn,Ai),e(Ai,Tg),e(Kn,wg),e(Ze,yg),e(Ze,Jn),e(Jn,$g),e(Jn,Gn),e(Gn,Fg),e(Jn,Dg),e(Ze,Bg),e(Ze,We),T(Xn,We,null),e(We,Mg),e(We,bs),e(bs,Eg),e(bs,Oi),e(Oi,xg),e(bs,zg),e(bs,rd),e(rd,jg),e(bs,Cg),e(We,qg),T(co,We,null),e(We,Pg),e(We,ad),e(ad,Ag),e(We,Og),T(Yn,We,null),e(We,Ng),T(Zn,We,null),u(s,Lp,f),u(s,ks,f),e(ks,po),e(po,id),T(er,id,null),e(ks,Lg),e(ks,ld),e(ld,Sg),u(s,Sp,f),u(s,et,f),T(tr,et,null),e(et,Ig),e(et,Ts),e(Ts,Wg),e(Ts,dd),e(dd,Rg),e(Ts,Ug),e(Ts,cd),e(cd,Qg),e(Ts,Hg),e(et,Vg),e(et,sr),e(sr,Kg),e(sr,Ni),e(Ni,Jg),e(sr,Gg),e(et,Xg),e(et,or),e(or,Yg),e(or,nr),e(nr,Zg),e(or,e_),e(et,t_),e(et,Re),T(rr,Re,null),e(Re,s_),e(Re,ws),e(ws,o_),e(ws,Li),e(Li,n_),e(ws,r_),e(ws,pd),e(pd,a_),e(ws,i_),e(Re,l_),T(ho,Re,null),e(Re,d_),e(Re,hd),e(hd,c_),e(Re,p_),T(ar,Re,null),e(Re,h_),T(ir,Re,null),u(s,Ip,f),u(s,ys,f),e(ys,uo),e(uo,ud),T(lr,ud,null),e(ys,u_),e(ys,md),e(md,m_),u(s,Wp,f),u(s,Pe,f),T(dr,Pe,null),e(Pe,f_),e(Pe,fd),e(fd,g_),e(Pe,__),e(Pe,cr),e(cr,v_),e(cr,Si),e(Si,b_),e(cr,k_),e(Pe,T_),e(Pe,pr),e(pr,w_),e(pr,hr),e(hr,y_),e(pr,$_),e(Pe,F_),T(mo,Pe,null),e(Pe,D_),e(Pe,ot),T(ur,ot,null),e(ot,B_),e(ot,$s),e($s,M_),e($s,Ii),e(Ii,E_),e($s,x_),e($s,gd),e(gd,z_),e($s,j_),e(ot,C_),T(fo,ot,null),e(ot,q_),e(ot,_d),e(_d,P_),e(ot,A_),T(mr,ot,null),u(s,Rp,f),u(s,Fs,f),e(Fs,go),e(go,vd),T(fr,vd,null),e(Fs,O_),e(Fs,bd),e(bd,N_),u(s,Up,f),u(s,Ae,f),T(gr,Ae,null),e(Ae,L_),e(Ae,_r),e(_r,S_),e(_r,kd),e(kd,I_),e(_r,W_),e(Ae,R_),e(Ae,vr),e(vr,U_),e(vr,Wi),e(Wi,Q_),e(vr,H_),e(Ae,V_),e(Ae,br),e(br,K_),e(br,kr),e(kr,J_),e(br,G_),e(Ae,X_),T(_o,Ae,null),e(Ae,Y_),e(Ae,Ue),T(Tr,Ue,null),e(Ue,Z_),e(Ue,Ds),e(Ds,ev),e(Ds,Ri),e(Ri,tv),e(Ds,sv),e(Ds,Td),e(Td,ov),e(Ds,nv),e(Ue,rv),T(vo,Ue,null),e(Ue,av),e(Ue,wd),e(wd,iv),e(Ue,lv),T(wr,Ue,null),e(Ue,dv),T(yr,Ue,null),u(s,Qp,f),u(s,Bs,f),e(Bs,bo),e(bo,yd),T($r,yd,null),e(Bs,cv),e(Bs,$d),e($d,pv),u(s,Hp,f),u(s,Oe,f),T(Fr,Oe,null),e(Oe,hv),e(Oe,Fd),e(Fd,uv),e(Oe,mv),e(Oe,Dr),e(Dr,fv),e(Dr,Ui),e(Ui,gv),e(Dr,_v),e(Oe,vv),e(Oe,Br),e(Br,bv),e(Br,Mr),e(Mr,kv),e(Br,Tv),e(Oe,wv),T(ko,Oe,null),e(Oe,yv),e(Oe,Qe),T(Er,Qe,null),e(Qe,$v),e(Qe,Ms),e(Ms,Fv),e(Ms,Qi),e(Qi,Dv),e(Ms,Bv),e(Ms,Dd),e(Dd,Mv),e(Ms,Ev),e(Qe,xv),T(To,Qe,null),e(Qe,zv),e(Qe,Bd),e(Bd,jv),e(Qe,Cv),T(xr,Qe,null),e(Qe,qv),T(zr,Qe,null),u(s,Vp,f),u(s,Es,f),e(Es,wo),e(wo,Md),T(jr,Md,null),e(Es,Pv),e(Es,Ed),e(Ed,Av),u(s,Kp,f),u(s,Ne,f),T(Cr,Ne,null),e(Ne,Ov),e(Ne,xd),e(xd,Nv),e(Ne,Lv),e(Ne,qr),e(qr,Sv),e(qr,Hi),e(Hi,Iv),e(qr,Wv),e(Ne,Rv),e(Ne,Pr),e(Pr,Uv),e(Pr,Ar),e(Ar,Qv),e(Pr,Hv),e(Ne,Vv),T(yo,Ne,null),e(Ne,Kv),e(Ne,nt),T(Or,nt,null),e(nt,Jv),e(nt,xs),e(xs,Gv),e(xs,Vi),e(Vi,Xv),e(xs,Yv),e(xs,zd),e(zd,Zv),e(xs,e1),e(nt,t1),T($o,nt,null),e(nt,s1),e(nt,jd),e(jd,o1),e(nt,n1),T(Nr,nt,null),u(s,Jp,f),u(s,zs,f),e(zs,Fo),e(Fo,Cd),T(Lr,Cd,null),e(zs,r1),e(zs,qd),e(qd,a1),u(s,Gp,f),u(s,Le,f),T(Sr,Le,null),e(Le,i1),e(Le,Pd),e(Pd,l1),e(Le,d1),e(Le,Ir),e(Ir,c1),e(Ir,Ki),e(Ki,p1),e(Ir,h1),e(Le,u1),e(Le,Wr),e(Wr,m1),e(Wr,Rr),e(Rr,f1),e(Wr,g1),e(Le,_1),T(Do,Le,null),e(Le,v1),e(Le,He),T(Ur,He,null),e(He,b1),e(He,js),e(js,k1),e(js,Ji),e(Ji,T1),e(js,w1),e(js,Ad),e(Ad,y1),e(js,$1),e(He,F1),T(Bo,He,null),e(He,D1),e(He,Od),e(Od,B1),e(He,M1),T(Qr,He,null),e(He,E1),T(Hr,He,null),u(s,Xp,f),u(s,Cs,f),e(Cs,Mo),e(Mo,Nd),T(Vr,Nd,null),e(Cs,x1),e(Cs,Ld),e(Ld,z1),u(s,Yp,f),u(s,Se,f),T(Kr,Se,null),e(Se,j1),e(Se,qs),e(qs,C1),e(qs,Sd),e(Sd,q1),e(qs,P1),e(qs,Id),e(Id,A1),e(qs,O1),e(Se,N1),e(Se,Jr),e(Jr,L1),e(Jr,Gi),e(Gi,S1),e(Jr,I1),e(Se,W1),e(Se,Gr),e(Gr,R1),e(Gr,Xr),e(Xr,U1),e(Gr,Q1),e(Se,H1),T(Eo,Se,null),e(Se,V1),e(Se,Ve),T(Yr,Ve,null),e(Ve,K1),e(Ve,Ps),e(Ps,J1),e(Ps,Xi),e(Xi,G1),e(Ps,X1),e(Ps,Wd),e(Wd,Y1),e(Ps,Z1),e(Ve,eb),T(xo,Ve,null),e(Ve,tb),e(Ve,Rd),e(Rd,sb),e(Ve,ob),T(Zr,Ve,null),e(Ve,nb),T(ea,Ve,null),u(s,Zp,f),u(s,As,f),e(As,zo),e(zo,Ud),T(ta,Ud,null),e(As,rb),e(As,Qd),e(Qd,ab),u(s,eh,f),u(s,Me,f),T(sa,Me,null),e(Me,ib),e(Me,Hd),e(Hd,lb),e(Me,db),e(Me,oa),e(oa,cb),e(oa,Yi),e(Yi,pb),e(oa,hb),e(Me,ub),e(Me,na),e(na,mb),e(na,ra),e(ra,fb),e(na,gb),e(Me,_b),e(Me,Vd),e(Vd,vb),e(Me,bb),e(Me,qt),e(qt,Kd),e(Kd,aa),e(aa,kb),e(qt,Tb),e(qt,Jd),e(Jd,ia),e(ia,wb),e(qt,yb),e(qt,Gd),e(Gd,la),e(la,$b),e(qt,Fb),e(qt,Xd),e(Xd,da),e(da,Db),e(Me,Bb),e(Me,rt),T(ca,rt,null),e(rt,Mb),e(rt,Os),e(Os,Eb),e(Os,Yd),e(Yd,xb),e(Os,zb),e(Os,Zd),e(Zd,jb),e(Os,Cb),e(rt,qb),T(jo,rt,null),e(rt,Pb),e(rt,ec),e(ec,Ab),e(rt,Ob),T(pa,rt,null),u(s,th,f),u(s,Ns,f),e(Ns,Co),e(Co,tc),T(ha,tc,null),e(Ns,Nb),e(Ns,sc),e(sc,Lb),u(s,sh,f),u(s,Ee,f),T(ua,Ee,null),e(Ee,Sb),e(Ee,ma),e(ma,Ib),e(ma,oc),e(oc,Wb),e(ma,Rb),e(Ee,Ub),e(Ee,fa),e(fa,Qb),e(fa,Zi),e(Zi,Hb),e(fa,Vb),e(Ee,Kb),e(Ee,ga),e(ga,Jb),e(ga,_a),e(_a,Gb),e(ga,Xb),e(Ee,Yb),e(Ee,nc),e(nc,Zb),e(Ee,ek),e(Ee,Pt),e(Pt,rc),e(rc,va),e(va,tk),e(Pt,sk),e(Pt,ac),e(ac,ba),e(ba,ok),e(Pt,nk),e(Pt,ic),e(ic,ka),e(ka,rk),e(Pt,ak),e(Pt,lc),e(lc,Ta),e(Ta,ik),e(Ee,lk),e(Ee,at),T(wa,at,null),e(at,dk),e(at,Ls),e(Ls,ck),e(Ls,dc),e(dc,pk),e(Ls,hk),e(Ls,cc),e(cc,uk),e(Ls,mk),e(at,fk),T(qo,at,null),e(at,gk),e(at,pc),e(pc,_k),e(at,vk),T(ya,at,null),u(s,oh,f),u(s,Ss,f),e(Ss,Po),e(Po,hc),T($a,hc,null),e(Ss,bk),e(Ss,uc),e(uc,kk),u(s,nh,f),u(s,xe,f),T(Fa,xe,null),e(xe,Tk),e(xe,mc),e(mc,wk),e(xe,yk),e(xe,Da),e(Da,$k),e(Da,el),e(el,Fk),e(Da,Dk),e(xe,Bk),e(xe,Ba),e(Ba,Mk),e(Ba,Ma),e(Ma,Ek),e(Ba,xk),e(xe,zk),e(xe,fc),e(fc,jk),e(xe,Ck),e(xe,At),e(At,gc),e(gc,Ea),e(Ea,qk),e(At,Pk),e(At,_c),e(_c,xa),e(xa,Ak),e(At,Ok),e(At,vc),e(vc,za),e(za,Nk),e(At,Lk),e(At,bc),e(bc,ja),e(ja,Sk),e(xe,Ik),e(xe,it),T(Ca,it,null),e(it,Wk),e(it,Is),e(Is,Rk),e(Is,kc),e(kc,Uk),e(Is,Qk),e(Is,Tc),e(Tc,Hk),e(Is,Vk),e(it,Kk),T(Ao,it,null),e(it,Jk),e(it,wc),e(wc,Gk),e(it,Xk),T(qa,it,null),u(s,rh,f),u(s,Ws,f),e(Ws,Oo),e(Oo,yc),T(Pa,yc,null),e(Ws,Yk),e(Ws,$c),e($c,Zk),u(s,ah,f),u(s,ze,f),T(Aa,ze,null),e(ze,eT),e(ze,Fc),e(Fc,tT),e(ze,sT),e(ze,Oa),e(Oa,oT),e(Oa,tl),e(tl,nT),e(Oa,rT),e(ze,aT),e(ze,Na),e(Na,iT),e(Na,La),e(La,lT),e(Na,dT),e(ze,cT),e(ze,Dc),e(Dc,pT),e(ze,hT),e(ze,Ot),e(Ot,Bc),e(Bc,Sa),e(Sa,uT),e(Ot,mT),e(Ot,Mc),e(Mc,Ia),e(Ia,fT),e(Ot,gT),e(Ot,Ec),e(Ec,Wa),e(Wa,_T),e(Ot,vT),e(Ot,xc),e(xc,Ra),e(Ra,bT),e(ze,kT),e(ze,lt),T(Ua,lt,null),e(lt,TT),e(lt,Rs),e(Rs,wT),e(Rs,zc),e(zc,yT),e(Rs,$T),e(Rs,jc),e(jc,FT),e(Rs,DT),e(lt,BT),T(No,lt,null),e(lt,MT),e(lt,Cc),e(Cc,ET),e(lt,xT),T(Qa,lt,null),u(s,ih,f),u(s,Us,f),e(Us,Lo),e(Lo,qc),T(Ha,qc,null),e(Us,zT),e(Us,Pc),e(Pc,jT),u(s,lh,f),u(s,je,f),T(Va,je,null),e(je,CT),e(je,Ac),e(Ac,qT),e(je,PT),e(je,Ka),e(Ka,AT),e(Ka,sl),e(sl,OT),e(Ka,NT),e(je,LT),e(je,Ja),e(Ja,ST),e(Ja,Ga),e(Ga,IT),e(Ja,WT),e(je,RT),e(je,Oc),e(Oc,UT),e(je,QT),e(je,Nt),e(Nt,Nc),e(Nc,Xa),e(Xa,HT),e(Nt,VT),e(Nt,Lc),e(Lc,Ya),e(Ya,KT),e(Nt,JT),e(Nt,Sc),e(Sc,Za),e(Za,GT),e(Nt,XT),e(Nt,Ic),e(Ic,ei),e(ei,YT),e(je,ZT),e(je,dt),T(ti,dt,null),e(dt,ew),e(dt,Qs),e(Qs,tw),e(Qs,Wc),e(Wc,sw),e(Qs,ow),e(Qs,Rc),e(Rc,nw),e(Qs,rw),e(dt,aw),T(So,dt,null),e(dt,iw),e(dt,Uc),e(Uc,lw),e(dt,dw),T(si,dt,null),u(s,dh,f),u(s,Hs,f),e(Hs,Io),e(Io,Qc),T(oi,Qc,null),e(Hs,cw),e(Hs,Hc),e(Hc,pw),u(s,ch,f),u(s,Ce,f),T(ni,Ce,null),e(Ce,hw),e(Ce,Vs),e(Vs,uw),e(Vs,Vc),e(Vc,mw),e(Vs,fw),e(Vs,Kc),e(Kc,gw),e(Vs,_w),e(Ce,vw),e(Ce,ri),e(ri,bw),e(ri,ol),e(ol,kw),e(ri,Tw),e(Ce,ww),e(Ce,ai),e(ai,yw),e(ai,ii),e(ii,$w),e(ai,Fw),e(Ce,Dw),e(Ce,Jc),e(Jc,Bw),e(Ce,Mw),e(Ce,Lt),e(Lt,Gc),e(Gc,li),e(li,Ew),e(Lt,xw),e(Lt,Xc),e(Xc,di),e(di,zw),e(Lt,jw),e(Lt,Yc),e(Yc,ci),e(ci,Cw),e(Lt,qw),e(Lt,Zc),e(Zc,pi),e(pi,Pw),e(Ce,Aw),e(Ce,ct),T(hi,ct,null),e(ct,Ow),e(ct,Ks),e(Ks,Nw),e(Ks,ep),e(ep,Lw),e(Ks,Sw),e(Ks,tp),e(tp,Iw),e(Ks,Ww),e(ct,Rw),T(Wo,ct,null),e(ct,Uw),e(ct,sp),e(sp,Qw),e(ct,Hw),T(ui,ct,null),ph=!0},p(s,[f]){const mi={};f&2&&(mi.$$scope={dirty:f,ctx:s}),to.$set(mi);const op={};f&2&&(op.$$scope={dirty:f,ctx:s}),oo.$set(op);const np={};f&2&&(np.$$scope={dirty:f,ctx:s}),ro.$set(np);const rp={};f&2&&(rp.$$scope={dirty:f,ctx:s}),io.$set(rp);const fi={};f&2&&(fi.$$scope={dirty:f,ctx:s}),co.$set(fi);const ap={};f&2&&(ap.$$scope={dirty:f,ctx:s}),ho.$set(ap);const ip={};f&2&&(ip.$$scope={dirty:f,ctx:s}),mo.$set(ip);const lp={};f&2&&(lp.$$scope={dirty:f,ctx:s}),fo.$set(lp);const St={};f&2&&(St.$$scope={dirty:f,ctx:s}),_o.$set(St);const dp={};f&2&&(dp.$$scope={dirty:f,ctx:s}),vo.$set(dp);const cp={};f&2&&(cp.$$scope={dirty:f,ctx:s}),ko.$set(cp);const pp={};f&2&&(pp.$$scope={dirty:f,ctx:s}),To.$set(pp);const hp={};f&2&&(hp.$$scope={dirty:f,ctx:s}),yo.$set(hp);const up={};f&2&&(up.$$scope={dirty:f,ctx:s}),$o.$set(up);const mp={};f&2&&(mp.$$scope={dirty:f,ctx:s}),Do.$set(mp);const fp={};f&2&&(fp.$$scope={dirty:f,ctx:s}),Bo.$set(fp);const gi={};f&2&&(gi.$$scope={dirty:f,ctx:s}),Eo.$set(gi);const It={};f&2&&(It.$$scope={dirty:f,ctx:s}),xo.$set(It);const gp={};f&2&&(gp.$$scope={dirty:f,ctx:s}),jo.$set(gp);const _p={};f&2&&(_p.$$scope={dirty:f,ctx:s}),qo.$set(_p);const vp={};f&2&&(vp.$$scope={dirty:f,ctx:s}),Ao.$set(vp);const _i={};f&2&&(_i.$$scope={dirty:f,ctx:s}),No.$set(_i);const bp={};f&2&&(bp.$$scope={dirty:f,ctx:s}),So.$set(bp);const Wt={};f&2&&(Wt.$$scope={dirty:f,ctx:s}),Wo.$set(Wt)},i(s){ph||(w(_.$$.fragment,s),w(X.$$.fragment,s),w(tn.$$.fragment,s),w(sn.$$.fragment,s),w(nn.$$.fragment,s),w(rn.$$.fragment,s),w(an.$$.fragment,s),w(dn.$$.fragment,s),w(cn.$$.fragment,s),w(un.$$.fragment,s),w(mn.$$.fragment,s),w(vn.$$.fragment,s),w(to.$$.fragment,s),w(bn.$$.fragment,s),w(kn.$$.fragment,s),w(Tn.$$.fragment,s),w(Dn.$$.fragment,s),w(oo.$$.fragment,s),w(Bn.$$.fragment,s),w(Mn.$$.fragment,s),w(En.$$.fragment,s),w(xn.$$.fragment,s),w(qn.$$.fragment,s),w(ro.$$.fragment,s),w(Pn.$$.fragment,s),w(An.$$.fragment,s),w(On.$$.fragment,s),w(Nn.$$.fragment,s),w(Ln.$$.fragment,s),w(Sn.$$.fragment,s),w(Un.$$.fragment,s),w(io.$$.fragment,s),w(Qn.$$.fragment,s),w(Hn.$$.fragment,s),w(Vn.$$.fragment,s),w(Xn.$$.fragment,s),w(co.$$.fragment,s),w(Yn.$$.fragment,s),w(Zn.$$.fragment,s),w(er.$$.fragment,s),w(tr.$$.fragment,s),w(rr.$$.fragment,s),w(ho.$$.fragment,s),w(ar.$$.fragment,s),w(ir.$$.fragment,s),w(lr.$$.fragment,s),w(dr.$$.fragment,s),w(mo.$$.fragment,s),w(ur.$$.fragment,s),w(fo.$$.fragment,s),w(mr.$$.fragment,s),w(fr.$$.fragment,s),w(gr.$$.fragment,s),w(_o.$$.fragment,s),w(Tr.$$.fragment,s),w(vo.$$.fragment,s),w(wr.$$.fragment,s),w(yr.$$.fragment,s),w($r.$$.fragment,s),w(Fr.$$.fragment,s),w(ko.$$.fragment,s),w(Er.$$.fragment,s),w(To.$$.fragment,s),w(xr.$$.fragment,s),w(zr.$$.fragment,s),w(jr.$$.fragment,s),w(Cr.$$.fragment,s),w(yo.$$.fragment,s),w(Or.$$.fragment,s),w($o.$$.fragment,s),w(Nr.$$.fragment,s),w(Lr.$$.fragment,s),w(Sr.$$.fragment,s),w(Do.$$.fragment,s),w(Ur.$$.fragment,s),w(Bo.$$.fragment,s),w(Qr.$$.fragment,s),w(Hr.$$.fragment,s),w(Vr.$$.fragment,s),w(Kr.$$.fragment,s),w(Eo.$$.fragment,s),w(Yr.$$.fragment,s),w(xo.$$.fragment,s),w(Zr.$$.fragment,s),w(ea.$$.fragment,s),w(ta.$$.fragment,s),w(sa.$$.fragment,s),w(ca.$$.fragment,s),w(jo.$$.fragment,s),w(pa.$$.fragment,s),w(ha.$$.fragment,s),w(ua.$$.fragment,s),w(wa.$$.fragment,s),w(qo.$$.fragment,s),w(ya.$$.fragment,s),w($a.$$.fragment,s),w(Fa.$$.fragment,s),w(Ca.$$.fragment,s),w(Ao.$$.fragment,s),w(qa.$$.fragment,s),w(Pa.$$.fragment,s),w(Aa.$$.fragment,s),w(Ua.$$.fragment,s),w(No.$$.fragment,s),w(Qa.$$.fragment,s),w(Ha.$$.fragment,s),w(Va.$$.fragment,s),w(ti.$$.fragment,s),w(So.$$.fragment,s),w(si.$$.fragment,s),w(oi.$$.fragment,s),w(ni.$$.fragment,s),w(hi.$$.fragment,s),w(Wo.$$.fragment,s),w(ui.$$.fragment,s),ph=!0)},o(s){y(_.$$.fragment,s),y(X.$$.fragment,s),y(tn.$$.fragment,s),y(sn.$$.fragment,s),y(nn.$$.fragment,s),y(rn.$$.fragment,s),y(an.$$.fragment,s),y(dn.$$.fragment,s),y(cn.$$.fragment,s),y(un.$$.fragment,s),y(mn.$$.fragment,s),y(vn.$$.fragment,s),y(to.$$.fragment,s),y(bn.$$.fragment,s),y(kn.$$.fragment,s),y(Tn.$$.fragment,s),y(Dn.$$.fragment,s),y(oo.$$.fragment,s),y(Bn.$$.fragment,s),y(Mn.$$.fragment,s),y(En.$$.fragment,s),y(xn.$$.fragment,s),y(qn.$$.fragment,s),y(ro.$$.fragment,s),y(Pn.$$.fragment,s),y(An.$$.fragment,s),y(On.$$.fragment,s),y(Nn.$$.fragment,s),y(Ln.$$.fragment,s),y(Sn.$$.fragment,s),y(Un.$$.fragment,s),y(io.$$.fragment,s),y(Qn.$$.fragment,s),y(Hn.$$.fragment,s),y(Vn.$$.fragment,s),y(Xn.$$.fragment,s),y(co.$$.fragment,s),y(Yn.$$.fragment,s),y(Zn.$$.fragment,s),y(er.$$.fragment,s),y(tr.$$.fragment,s),y(rr.$$.fragment,s),y(ho.$$.fragment,s),y(ar.$$.fragment,s),y(ir.$$.fragment,s),y(lr.$$.fragment,s),y(dr.$$.fragment,s),y(mo.$$.fragment,s),y(ur.$$.fragment,s),y(fo.$$.fragment,s),y(mr.$$.fragment,s),y(fr.$$.fragment,s),y(gr.$$.fragment,s),y(_o.$$.fragment,s),y(Tr.$$.fragment,s),y(vo.$$.fragment,s),y(wr.$$.fragment,s),y(yr.$$.fragment,s),y($r.$$.fragment,s),y(Fr.$$.fragment,s),y(ko.$$.fragment,s),y(Er.$$.fragment,s),y(To.$$.fragment,s),y(xr.$$.fragment,s),y(zr.$$.fragment,s),y(jr.$$.fragment,s),y(Cr.$$.fragment,s),y(yo.$$.fragment,s),y(Or.$$.fragment,s),y($o.$$.fragment,s),y(Nr.$$.fragment,s),y(Lr.$$.fragment,s),y(Sr.$$.fragment,s),y(Do.$$.fragment,s),y(Ur.$$.fragment,s),y(Bo.$$.fragment,s),y(Qr.$$.fragment,s),y(Hr.$$.fragment,s),y(Vr.$$.fragment,s),y(Kr.$$.fragment,s),y(Eo.$$.fragment,s),y(Yr.$$.fragment,s),y(xo.$$.fragment,s),y(Zr.$$.fragment,s),y(ea.$$.fragment,s),y(ta.$$.fragment,s),y(sa.$$.fragment,s),y(ca.$$.fragment,s),y(jo.$$.fragment,s),y(pa.$$.fragment,s),y(ha.$$.fragment,s),y(ua.$$.fragment,s),y(wa.$$.fragment,s),y(qo.$$.fragment,s),y(ya.$$.fragment,s),y($a.$$.fragment,s),y(Fa.$$.fragment,s),y(Ca.$$.fragment,s),y(Ao.$$.fragment,s),y(qa.$$.fragment,s),y(Pa.$$.fragment,s),y(Aa.$$.fragment,s),y(Ua.$$.fragment,s),y(No.$$.fragment,s),y(Qa.$$.fragment,s),y(Ha.$$.fragment,s),y(Va.$$.fragment,s),y(ti.$$.fragment,s),y(So.$$.fragment,s),y(si.$$.fragment,s),y(oi.$$.fragment,s),y(ni.$$.fragment,s),y(hi.$$.fragment,s),y(Wo.$$.fragment,s),y(ui.$$.fragment,s),ph=!1},d(s){t(p),s&&t(D),s&&t(g),$(_),s&&t(V),s&&t(E),$(X),s&&t(ae),s&&t(N),s&&t(ie),s&&t(ee),s&&t(le),s&&t(L),s&&t(q),s&&t(te),s&&t(de),s&&t(h),s&&t(wp),s&&t(bt),s&&t(yp),s&&t(as),$(tn),s&&t($p),s&&t(Ke),$(sn),$(nn),s&&t(Fp),s&&t(ls),$(rn),s&&t(Dp),s&&t(_t),$(an),s&&t(Bp),s&&t(ds),$(dn),s&&t(Mp),s&&t(vt),$(cn),s&&t(Ep),s&&t(cs),$(un),s&&t(xp),s&&t(Je),$(mn),$(vn),$(to),$(bn),s&&t(zp),s&&t(hs),$(kn),s&&t(jp),s&&t(Ge),$(Tn),$(Dn),$(oo),$(Bn),$(Mn),s&&t(Cp),s&&t(ms),$(En),s&&t(qp),s&&t(Xe),$(xn),$(qn),$(ro),$(Pn),$(An),$(On),$(Nn),s&&t(Pp),s&&t(gs),$(Ln),s&&t(Ap),s&&t(Ye),$(Sn),$(Un),$(io),$(Qn),s&&t(Op),s&&t(vs),$(Hn),s&&t(Np),s&&t(Ze),$(Vn),$(Xn),$(co),$(Yn),$(Zn),s&&t(Lp),s&&t(ks),$(er),s&&t(Sp),s&&t(et),$(tr),$(rr),$(ho),$(ar),$(ir),s&&t(Ip),s&&t(ys),$(lr),s&&t(Wp),s&&t(Pe),$(dr),$(mo),$(ur),$(fo),$(mr),s&&t(Rp),s&&t(Fs),$(fr),s&&t(Up),s&&t(Ae),$(gr),$(_o),$(Tr),$(vo),$(wr),$(yr),s&&t(Qp),s&&t(Bs),$($r),s&&t(Hp),s&&t(Oe),$(Fr),$(ko),$(Er),$(To),$(xr),$(zr),s&&t(Vp),s&&t(Es),$(jr),s&&t(Kp),s&&t(Ne),$(Cr),$(yo),$(Or),$($o),$(Nr),s&&t(Jp),s&&t(zs),$(Lr),s&&t(Gp),s&&t(Le),$(Sr),$(Do),$(Ur),$(Bo),$(Qr),$(Hr),s&&t(Xp),s&&t(Cs),$(Vr),s&&t(Yp),s&&t(Se),$(Kr),$(Eo),$(Yr),$(xo),$(Zr),$(ea),s&&t(Zp),s&&t(As),$(ta),s&&t(eh),s&&t(Me),$(sa),$(ca),$(jo),$(pa),s&&t(th),s&&t(Ns),$(ha),s&&t(sh),s&&t(Ee),$(ua),$(wa),$(qo),$(ya),s&&t(oh),s&&t(Ss),$($a),s&&t(nh),s&&t(xe),$(Fa),$(Ca),$(Ao),$(qa),s&&t(rh),s&&t(Ws),$(Pa),s&&t(ah),s&&t(ze),$(Aa),$(Ua),$(No),$(Qa),s&&t(ih),s&&t(Us),$(Ha),s&&t(lh),s&&t(je),$(Va),$(ti),$(So),$(si),s&&t(dh),s&&t(Hs),$(oi),s&&t(ch),s&&t(Ce),$(ni),$(hi),$(Wo),$(ui)}}}const ZD={local:"distilbert",sections:[{local:"overview",title:"Overview"},{local:"transformers.DistilBertConfig",title:"DistilBertConfig"},{local:"transformers.DistilBertTokenizer",title:"DistilBertTokenizer"},{local:"transformers.DistilBertTokenizerFast",title:"DistilBertTokenizerFast"},{local:"transformers.DistilBertModel",title:"DistilBertModel"},{local:"transformers.DistilBertForMaskedLM",title:"DistilBertForMaskedLM"},{local:"transformers.DistilBertForSequenceClassification",title:"DistilBertForSequenceClassification"},{local:"transformers.DistilBertForMultipleChoice",title:"DistilBertForMultipleChoice"},{local:"transformers.DistilBertForTokenClassification",title:"DistilBertForTokenClassification"},{local:"transformers.DistilBertForQuestionAnswering",title:"DistilBertForQuestionAnswering"},{local:"transformers.TFDistilBertModel",title:"TFDistilBertModel"},{local:"transformers.TFDistilBertForMaskedLM",title:"TFDistilBertForMaskedLM"},{local:"transformers.TFDistilBertForSequenceClassification",title:"TFDistilBertForSequenceClassification"},{local:"transformers.TFDistilBertForMultipleChoice",title:"TFDistilBertForMultipleChoice"},{local:"transformers.TFDistilBertForTokenClassification",title:"TFDistilBertForTokenClassification"},{local:"transformers.TFDistilBertForQuestionAnswering",title:"TFDistilBertForQuestionAnswering"},{local:"transformers.FlaxDistilBertModel",title:"FlaxDistilBertModel"},{local:"transformers.FlaxDistilBertForMaskedLM",title:"FlaxDistilBertForMaskedLM"},{local:"transformers.FlaxDistilBertForSequenceClassification",title:"FlaxDistilBertForSequenceClassification"},{local:"transformers.FlaxDistilBertForMultipleChoice",title:"FlaxDistilBertForMultipleChoice"},{local:"transformers.FlaxDistilBertForTokenClassification",title:"FlaxDistilBertForTokenClassification"},{local:"transformers.FlaxDistilBertForQuestionAnswering",title:"FlaxDistilBertForQuestionAnswering"}],title:"DistilBERT"};function e0(j){return BD(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class a0 extends yD{constructor(p){super();$D(this,p,e0,YD,FD,{})}}export{a0 as default,ZD as metadata};
