import{S as Zl,i as ed,s as td,e as n,k as d,w as _,t as r,M as od,c as s,d as o,m as c,a,x as T,h as i,b as l,F as e,g as u,y as v,q as y,o as b,B as P,v as nd}from"../../chunks/vendor-6b77c823.js";import{T as ln}from"../../chunks/Tip-39098574.js";import{D as V}from"../../chunks/Docstring-abef54e3.js";import{C as _e}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Te}from"../../chunks/IconCopyLink-7a11ce68.js";function sd(W){let p,N,m,k,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,k),e(p,w)},d(g){g&&o(p)}}}function ad(W){let p,N,m,k,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,k),e(p,w)},d(g){g&&o(p)}}}function rd(W){let p,N,m,k,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,k),e(p,w)},d(g){g&&o(p)}}}function id(W){let p,N,m,k,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,k),e(p,w)},d(g){g&&o(p)}}}function ld(W){let p,N,m,k,w;return{c(){p=n("p"),N=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),w=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);N=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),w=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,N),e(p,m),e(m,k),e(p,w)},d(g){g&&o(p)}}}function dd(W){let p,N,m,k,w,g,f,$,Rn,dn,oe,ve,io,Ae,Xn,lo,Kn,cn,J,Qn,Ie,Yn,Zn,Le,es,ts,pn,St,os,hn,ye,ns,Se,ss,as,un,ne,be,co,Oe,rs,po,is,mn,Pe,ls,ho,ds,cs,fn,De,gn,se,ke,uo,Be,ps,mo,hs,_n,z,We,us,ae,ms,Ot,fs,gs,He,_s,Ts,vs,re,ys,Dt,bs,Ps,Bt,ks,ws,Ns,fo,$s,Gs,Ue,Tn,ie,we,go,Ve,xs,_o,Ms,vn,C,Je,Fs,To,Es,zs,Re,Cs,Wt,qs,js,As,Xe,Is,Ke,Ls,Ss,Os,j,Qe,Ds,le,Bs,Ht,Ws,Hs,vo,Us,Vs,Js,Ne,Rs,yo,Xs,Ks,Ye,yn,de,$e,bo,Ze,Qs,Po,Ys,bn,q,et,Zs,ko,ea,ta,tt,oa,Ut,na,sa,aa,ot,ra,nt,ia,la,da,A,st,ca,ce,pa,Vt,ha,ua,wo,ma,fa,ga,Ge,_a,No,Ta,va,at,Pn,pe,xe,$o,rt,ya,Go,ba,kn,x,it,Pa,xo,ka,wa,Jt,Rt,Na,$a,Ga,B,xa,Mo,Ma,Fa,Fo,Ea,za,Eo,Ca,qa,zo,ja,Aa,Ia,lt,La,Xt,Sa,Oa,Da,dt,Ba,ct,Wa,Ha,Ua,G,pt,Va,he,Ja,Kt,Ra,Xa,Co,Ka,Qa,Ya,Me,Za,qo,er,tr,ht,or,ut,nr,jo,sr,ar,mt,wn,ue,Fe,Ao,ft,rr,Io,ir,Nn,M,gt,lr,Lo,dr,cr,_t,pr,Qt,hr,ur,mr,Tt,fr,vt,gr,_r,Tr,So,vr,yr,H,Oo,yt,br,Pr,Do,bt,kr,wr,Bo,Pt,Nr,$r,Wo,kt,Gr,xr,I,wt,Mr,me,Fr,Ho,Er,zr,Uo,Cr,qr,jr,Ee,Ar,Vo,Ir,Lr,Nt,$n,fe,ze,Jo,$t,Sr,Ro,Or,Gn,F,Gt,Dr,Xo,Br,Wr,xt,Hr,Yt,Ur,Vr,Jr,Mt,Rr,Ft,Xr,Kr,Qr,Ko,Yr,Zr,U,Qo,Et,ei,ti,Yo,zt,oi,ni,Zo,Ct,si,ai,en,qt,ri,ii,L,jt,li,ge,di,tn,ci,pi,on,hi,ui,mi,Ce,fi,nn,gi,_i,At,xn;return g=new Te({}),Ae=new Te({}),Oe=new Te({}),De=new _e({props:{code:`from transformers import GPTNeoForCausalLM, GPT2Tokenizer

model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

prompt = (
    "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
    "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
    "researchers was the fact that the unicorns spoke perfect English."
)

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoForCausalLM, GPT2Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;In a shocking finding, scientists discovered a herd of unicorns living in a remote, &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;previously unexplored valley, in the Andes Mountains. Even more surprising to the &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;researchers was the fact that the unicorns spoke perfect English.&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    do_sample=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    temperature=<span class="hljs-number">0.9</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_text = tokenizer.batch_decode(gen_tokens)[<span class="hljs-number">0</span>]`}}),Be=new Te({}),We=new V({props:{name:"class transformers.GPTNeoConfig",anchor:"transformers.GPTNeoConfig",parameters:[{name:"vocab_size",val:" = 50257"},{name:"max_position_embeddings",val:" = 2048"},{name:"hidden_size",val:" = 2048"},{name:"num_layers",val:" = 24"},{name:"attention_types",val:" = [[['global', 'local'], 12]]"},{name:"num_heads",val:" = 16"},{name:"intermediate_size",val:" = None"},{name:"window_size",val:" = 256"},{name:"activation_function",val:" = 'gelu_new'"},{name:"resid_dropout",val:" = 0.0"},{name:"embed_dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"summary_type",val:" = 'cls_index'"},{name:"summary_use_proj",val:" = True"},{name:"summary_activation",val:" = None"},{name:"summary_proj_to_labels",val:" = True"},{name:"summary_first_dropout",val:" = 0.1"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 50256"},{name:"eos_token_id",val:" = 50256"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/configuration_gpt_neo.py#L34",parametersDescription:[{anchor:"transformers.GPTNeoConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50257) &#x2014;
Vocabulary size of the GPT Neo model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>. Vocabulary size of the model. Defines the different
tokens that can be represented by the <em>inputs_ids</em> passed to the forward method of <a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoConfig.attention_types",description:`<strong>attention_types</strong> (<code>List</code>, <em>optional</em>, defaults to <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code>) &#x2014;
The type of attention for each layer in a <code>List</code> of the following format <code>[[[&quot;attention_type&quot;], num_layerss]]</code> e.g. for a 24 layer model <code>[[[&quot;global&quot;], 24]]</code> or <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code> Choose the
value of <code>attention_type</code> from <code>[&quot;global&quot;, &quot;local&quot;]</code>`,name:"attention_types"},{anchor:"transformers.GPTNeoConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.GPTNeoConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.GPTNeoConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GPTNeoConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.GPTNeoConfig.embed_dropout",description:`<strong>embed_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"embed_dropout"},{anchor:"transformers.GPTNeoConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.GPTNeoConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.GPTNeoConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}]}}),Ue=new _e({props:{code:`from transformers import GPTNeoModel, GPTNeoConfig

# Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration
configuration = GPTNeoConfig()

# Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration
model = GPTNeoModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoModel, GPTNeoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GPTNeoConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Ve=new Te({}),Je=new V({props:{name:"class transformers.GPTNeoModel",anchor:"transformers.GPTNeoModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L474",parametersDescription:[{anchor:"transformers.GPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Qe=new V({props:{name:"forward",anchor:"transformers.GPTNeoModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L495",parametersDescription:[{anchor:"transformers.GPTNeoModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new ln({props:{$$slots:{default:[sd]},$$scope:{ctx:W}}}),Ye=new _e({props:{code:`from transformers import GPT2Tokenizer, GPTNeoModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ze=new Te({}),et=new V({props:{name:"class transformers.GPTNeoForCausalLM",anchor:"transformers.GPTNeoForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L662",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),st=new V({props:{name:"forward",anchor:"transformers.GPTNeoForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L712",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new ln({props:{$$slots:{default:[ad]},$$scope:{ctx:W}}}),at=new _e({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),rt=new Te({}),it=new V({props:{name:"class transformers.GPTNeoForSequenceClassification",anchor:"transformers.GPTNeoForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L815",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),pt=new V({props:{name:"forward",anchor:"transformers.GPTNeoForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L827",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new ln({props:{$$slots:{default:[rd]},$$scope:{ctx:W}}}),ht=new _e({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),ut=new _e({props:{code:`labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),mt=new _e({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]


num_labels = len(model.config.id2label)
labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]


<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),ft=new Te({}),gt=new V({props:{name:"class transformers.FlaxGPTNeoModel",anchor:"transformers.FlaxGPTNeoModel",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L580",parametersDescription:[{anchor:"transformers.FlaxGPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),wt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L390",parametersDescription:[{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ee=new ln({props:{$$slots:{default:[id]},$$scope:{ctx:W}}}),Nt=new _e({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoModel

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = FlaxGPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoModel.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),$t=new Te({}),Gt=new V({props:{name:"class transformers.FlaxGPTNeoForCausalLM",anchor:"transformers.FlaxGPTNeoForCausalLM",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L645",parametersDescription:[{anchor:"transformers.FlaxGPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoForCausalLM.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),jt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L390",parametersDescription:[{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ce=new ln({props:{$$slots:{default:[ld]},$$scope:{ctx:W}}}),At=new _e({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = FlaxGPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
outputs = model(**inputs)

# retrieve logts for next token
next_token_logits = outputs.logits[:, -1]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`}}),{c(){p=n("meta"),N=d(),m=n("h1"),k=n("a"),w=n("span"),_(g.$$.fragment),f=d(),$=n("span"),Rn=r("GPT Neo"),dn=d(),oe=n("h2"),ve=n("a"),io=n("span"),_(Ae.$$.fragment),Xn=d(),lo=n("span"),Kn=r("Overview"),cn=d(),J=n("p"),Qn=r("The GPTNeo model was released in the "),Ie=n("a"),Yn=r("EleutherAI/gpt-neo"),Zn=r(` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Le=n("a"),es=r("Pile"),ts=r(" dataset."),pn=d(),St=n("p"),os=r(`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),hn=d(),ye=n("p"),ns=r("This model was contributed by "),Se=n("a"),ss=r("valhalla"),as=r("."),un=d(),ne=n("h3"),be=n("a"),co=n("span"),_(Oe.$$.fragment),rs=d(),po=n("span"),is=r("Generation"),mn=d(),Pe=n("p"),ls=r("The "),ho=n("code"),ds=r("generate()"),cs=r(" method can be used to generate text using GPT Neo model."),fn=d(),_(De.$$.fragment),gn=d(),se=n("h2"),ke=n("a"),uo=n("span"),_(Be.$$.fragment),ps=d(),mo=n("span"),hs=r("GPTNeoConfig"),_n=d(),z=n("div"),_(We.$$.fragment),us=d(),ae=n("p"),ms=r("This is the configuration class to store the configuration of a "),Ot=n("a"),fs=r("GPTNeoModel"),gs=r(`. It is used to instantiate a GPT
Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the GPTNeo
`),He=n("a"),_s=r("gpt-neo-1.3B"),Ts=r(" architecture."),vs=d(),re=n("p"),ys=r("Configuration objects inherit from "),Dt=n("a"),bs=r("PretrainedConfig"),Ps=r(` and can be used to control the model outputs. Read the
documentation from `),Bt=n("a"),ks=r("PretrainedConfig"),ws=r(" for more information."),Ns=d(),fo=n("p"),$s=r("Example:"),Gs=d(),_(Ue.$$.fragment),Tn=d(),ie=n("h2"),we=n("a"),go=n("span"),_(Ve.$$.fragment),xs=d(),_o=n("span"),Ms=r("GPTNeoModel"),vn=d(),C=n("div"),_(Je.$$.fragment),Fs=d(),To=n("p"),Es=r("The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),zs=d(),Re=n("p"),Cs=r("This model inherits from "),Wt=n("a"),qs=r("PreTrainedModel"),js=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),As=d(),Xe=n("p"),Is=r("This model is also a PyTorch "),Ke=n("a"),Ls=r("torch.nn.Module"),Ss=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Os=d(),j=n("div"),_(Qe.$$.fragment),Ds=d(),le=n("p"),Bs=r("The "),Ht=n("a"),Ws=r("GPTNeoModel"),Hs=r(" forward method, overrides the "),vo=n("code"),Us=r("__call__"),Vs=r(" special method."),Js=d(),_(Ne.$$.fragment),Rs=d(),yo=n("p"),Xs=r("Example:"),Ks=d(),_(Ye.$$.fragment),yn=d(),de=n("h2"),$e=n("a"),bo=n("span"),_(Ze.$$.fragment),Qs=d(),Po=n("span"),Ys=r("GPTNeoForCausalLM"),bn=d(),q=n("div"),_(et.$$.fragment),Zs=d(),ko=n("p"),ea=r(`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),ta=d(),tt=n("p"),oa=r("This model inherits from "),Ut=n("a"),na=r("PreTrainedModel"),sa=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),aa=d(),ot=n("p"),ra=r("This model is also a PyTorch "),nt=n("a"),ia=r("torch.nn.Module"),la=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),da=d(),A=n("div"),_(st.$$.fragment),ca=d(),ce=n("p"),pa=r("The "),Vt=n("a"),ha=r("GPTNeoForCausalLM"),ua=r(" forward method, overrides the "),wo=n("code"),ma=r("__call__"),fa=r(" special method."),ga=d(),_(Ge.$$.fragment),_a=d(),No=n("p"),Ta=r("Example:"),va=d(),_(at.$$.fragment),Pn=d(),pe=n("h2"),xe=n("a"),$o=n("span"),_(rt.$$.fragment),ya=d(),Go=n("span"),ba=r("GPTNeoForSequenceClassification"),kn=d(),x=n("div"),_(it.$$.fragment),Pa=d(),xo=n("p"),ka=r("The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),wa=d(),Jt=n("p"),Rt=n("a"),Na=r("GPTNeoForSequenceClassification"),$a=r(` uses the last token in order to do the classification, as other causal models
(e.g. GPT-1) do.`),Ga=d(),B=n("p"),xa=r(`Since it does classification on the last token, it requires to know the position of the last token. If a
`),Mo=n("code"),Ma=r("pad_token_id"),Fa=r(` is defined in the configuration, it finds the last token that is not a padding token in each row. If
no `),Fo=n("code"),Ea=r("pad_token_id"),za=r(` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when `),Eo=n("code"),Ca=r("inputs_embeds"),qa=r(" are passed instead of "),zo=n("code"),ja=r("input_ids"),Aa=r(`, it does the same (take the last value in
each row of the batch).`),Ia=d(),lt=n("p"),La=r("This model inherits from "),Xt=n("a"),Sa=r("PreTrainedModel"),Oa=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Da=d(),dt=n("p"),Ba=r("This model is also a PyTorch "),ct=n("a"),Wa=r("torch.nn.Module"),Ha=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ua=d(),G=n("div"),_(pt.$$.fragment),Va=d(),he=n("p"),Ja=r("The "),Kt=n("a"),Ra=r("GPTNeoForSequenceClassification"),Xa=r(" forward method, overrides the "),Co=n("code"),Ka=r("__call__"),Qa=r(" special method."),Ya=d(),_(Me.$$.fragment),Za=d(),qo=n("p"),er=r("Example of single-label classification:"),tr=d(),_(ht.$$.fragment),or=d(),_(ut.$$.fragment),nr=d(),jo=n("p"),sr=r("Example of multi-label classification:"),ar=d(),_(mt.$$.fragment),wn=d(),ue=n("h2"),Fe=n("a"),Ao=n("span"),_(ft.$$.fragment),rr=d(),Io=n("span"),ir=r("FlaxGPTNeoModel"),Nn=d(),M=n("div"),_(gt.$$.fragment),lr=d(),Lo=n("p"),dr=r("The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),cr=d(),_t=n("p"),pr=r("This model inherits from "),Qt=n("a"),hr=r("FlaxPreTrainedModel"),ur=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mr=d(),Tt=n("p"),fr=r(`This model is also a Flax Linen
`),vt=n("a"),gr=r("flax.nn.Module"),_r=r(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Tr=d(),So=n("p"),vr=r("Finally, this model supports inherent JAX features such as:"),yr=d(),H=n("ul"),Oo=n("li"),yt=n("a"),br=r("Just-In-Time (JIT) compilation"),Pr=d(),Do=n("li"),bt=n("a"),kr=r("Automatic Differentiation"),wr=d(),Bo=n("li"),Pt=n("a"),Nr=r("Vectorization"),$r=d(),Wo=n("li"),kt=n("a"),Gr=r("Parallelization"),xr=d(),I=n("div"),_(wt.$$.fragment),Mr=d(),me=n("p"),Fr=r("The "),Ho=n("code"),Er=r("FlaxGPTNeoPreTrainedModel"),zr=r("forward method, overrides the "),Uo=n("code"),Cr=r("__call__"),qr=r(" special method."),jr=d(),_(Ee.$$.fragment),Ar=d(),Vo=n("p"),Ir=r("Example:"),Lr=d(),_(Nt.$$.fragment),$n=d(),fe=n("h2"),ze=n("a"),Jo=n("span"),_($t.$$.fragment),Sr=d(),Ro=n("span"),Or=r("FlaxGPTNeoForCausalLM"),Gn=d(),F=n("div"),_(Gt.$$.fragment),Dr=d(),Xo=n("p"),Br=r(`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Wr=d(),xt=n("p"),Hr=r("This model inherits from "),Yt=n("a"),Ur=r("FlaxPreTrainedModel"),Vr=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jr=d(),Mt=n("p"),Rr=r(`This model is also a Flax Linen
`),Ft=n("a"),Xr=r("flax.nn.Module"),Kr=r(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Qr=d(),Ko=n("p"),Yr=r("Finally, this model supports inherent JAX features such as:"),Zr=d(),U=n("ul"),Qo=n("li"),Et=n("a"),ei=r("Just-In-Time (JIT) compilation"),ti=d(),Yo=n("li"),zt=n("a"),oi=r("Automatic Differentiation"),ni=d(),Zo=n("li"),Ct=n("a"),si=r("Vectorization"),ai=d(),en=n("li"),qt=n("a"),ri=r("Parallelization"),ii=d(),L=n("div"),_(jt.$$.fragment),li=d(),ge=n("p"),di=r("The "),tn=n("code"),ci=r("FlaxGPTNeoPreTrainedModel"),pi=r("forward method, overrides the "),on=n("code"),hi=r("__call__"),ui=r(" special method."),mi=d(),_(Ce.$$.fragment),fi=d(),nn=n("p"),gi=r("Example:"),_i=d(),_(At.$$.fragment),this.h()},l(t){const h=od('[data-svelte="svelte-1phssyn"]',document.head);p=s(h,"META",{name:!0,content:!0}),h.forEach(o),N=c(t),m=s(t,"H1",{class:!0});var It=a(m);k=s(It,"A",{id:!0,class:!0,href:!0});var sn=a(k);w=s(sn,"SPAN",{});var an=a(w);T(g.$$.fragment,an),an.forEach(o),sn.forEach(o),f=c(It),$=s(It,"SPAN",{});var rn=a($);Rn=i(rn,"GPT Neo"),rn.forEach(o),It.forEach(o),dn=c(t),oe=s(t,"H2",{class:!0});var Lt=a(oe);ve=s(Lt,"A",{id:!0,class:!0,href:!0});var vi=a(ve);io=s(vi,"SPAN",{});var yi=a(io);T(Ae.$$.fragment,yi),yi.forEach(o),vi.forEach(o),Xn=c(Lt),lo=s(Lt,"SPAN",{});var bi=a(lo);Kn=i(bi,"Overview"),bi.forEach(o),Lt.forEach(o),cn=c(t),J=s(t,"P",{});var Zt=a(J);Qn=i(Zt,"The GPTNeo model was released in the "),Ie=s(Zt,"A",{href:!0,rel:!0});var Pi=a(Ie);Yn=i(Pi,"EleutherAI/gpt-neo"),Pi.forEach(o),Zn=i(Zt,` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Le=s(Zt,"A",{href:!0,rel:!0});var ki=a(Le);es=i(ki,"Pile"),ki.forEach(o),ts=i(Zt," dataset."),Zt.forEach(o),pn=c(t),St=s(t,"P",{});var wi=a(St);os=i(wi,`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),wi.forEach(o),hn=c(t),ye=s(t,"P",{});var Mn=a(ye);ns=i(Mn,"This model was contributed by "),Se=s(Mn,"A",{href:!0,rel:!0});var Ni=a(Se);ss=i(Ni,"valhalla"),Ni.forEach(o),as=i(Mn,"."),Mn.forEach(o),un=c(t),ne=s(t,"H3",{class:!0});var Fn=a(ne);be=s(Fn,"A",{id:!0,class:!0,href:!0});var $i=a(be);co=s($i,"SPAN",{});var Gi=a(co);T(Oe.$$.fragment,Gi),Gi.forEach(o),$i.forEach(o),rs=c(Fn),po=s(Fn,"SPAN",{});var xi=a(po);is=i(xi,"Generation"),xi.forEach(o),Fn.forEach(o),mn=c(t),Pe=s(t,"P",{});var En=a(Pe);ls=i(En,"The "),ho=s(En,"CODE",{});var Mi=a(ho);ds=i(Mi,"generate()"),Mi.forEach(o),cs=i(En," method can be used to generate text using GPT Neo model."),En.forEach(o),fn=c(t),T(De.$$.fragment,t),gn=c(t),se=s(t,"H2",{class:!0});var zn=a(se);ke=s(zn,"A",{id:!0,class:!0,href:!0});var Fi=a(ke);uo=s(Fi,"SPAN",{});var Ei=a(uo);T(Be.$$.fragment,Ei),Ei.forEach(o),Fi.forEach(o),ps=c(zn),mo=s(zn,"SPAN",{});var zi=a(mo);hs=i(zi,"GPTNeoConfig"),zi.forEach(o),zn.forEach(o),_n=c(t),z=s(t,"DIV",{class:!0});var R=a(z);T(We.$$.fragment,R),us=c(R),ae=s(R,"P",{});var eo=a(ae);ms=i(eo,"This is the configuration class to store the configuration of a "),Ot=s(eo,"A",{href:!0});var Ci=a(Ot);fs=i(Ci,"GPTNeoModel"),Ci.forEach(o),gs=i(eo,`. It is used to instantiate a GPT
Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the GPTNeo
`),He=s(eo,"A",{href:!0,rel:!0});var qi=a(He);_s=i(qi,"gpt-neo-1.3B"),qi.forEach(o),Ts=i(eo," architecture."),eo.forEach(o),vs=c(R),re=s(R,"P",{});var to=a(re);ys=i(to,"Configuration objects inherit from "),Dt=s(to,"A",{href:!0});var ji=a(Dt);bs=i(ji,"PretrainedConfig"),ji.forEach(o),Ps=i(to,` and can be used to control the model outputs. Read the
documentation from `),Bt=s(to,"A",{href:!0});var Ai=a(Bt);ks=i(Ai,"PretrainedConfig"),Ai.forEach(o),ws=i(to," for more information."),to.forEach(o),Ns=c(R),fo=s(R,"P",{});var Ii=a(fo);$s=i(Ii,"Example:"),Ii.forEach(o),Gs=c(R),T(Ue.$$.fragment,R),R.forEach(o),Tn=c(t),ie=s(t,"H2",{class:!0});var Cn=a(ie);we=s(Cn,"A",{id:!0,class:!0,href:!0});var Li=a(we);go=s(Li,"SPAN",{});var Si=a(go);T(Ve.$$.fragment,Si),Si.forEach(o),Li.forEach(o),xs=c(Cn),_o=s(Cn,"SPAN",{});var Oi=a(_o);Ms=i(Oi,"GPTNeoModel"),Oi.forEach(o),Cn.forEach(o),vn=c(t),C=s(t,"DIV",{class:!0});var X=a(C);T(Je.$$.fragment,X),Fs=c(X),To=s(X,"P",{});var Di=a(To);Es=i(Di,"The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),Di.forEach(o),zs=c(X),Re=s(X,"P",{});var qn=a(Re);Cs=i(qn,"This model inherits from "),Wt=s(qn,"A",{href:!0});var Bi=a(Wt);qs=i(Bi,"PreTrainedModel"),Bi.forEach(o),js=i(qn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qn.forEach(o),As=c(X),Xe=s(X,"P",{});var jn=a(Xe);Is=i(jn,"This model is also a PyTorch "),Ke=s(jn,"A",{href:!0,rel:!0});var Wi=a(Ke);Ls=i(Wi,"torch.nn.Module"),Wi.forEach(o),Ss=i(jn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jn.forEach(o),Os=c(X),j=s(X,"DIV",{class:!0});var K=a(j);T(Qe.$$.fragment,K),Ds=c(K),le=s(K,"P",{});var oo=a(le);Bs=i(oo,"The "),Ht=s(oo,"A",{href:!0});var Hi=a(Ht);Ws=i(Hi,"GPTNeoModel"),Hi.forEach(o),Hs=i(oo," forward method, overrides the "),vo=s(oo,"CODE",{});var Ui=a(vo);Us=i(Ui,"__call__"),Ui.forEach(o),Vs=i(oo," special method."),oo.forEach(o),Js=c(K),T(Ne.$$.fragment,K),Rs=c(K),yo=s(K,"P",{});var Vi=a(yo);Xs=i(Vi,"Example:"),Vi.forEach(o),Ks=c(K),T(Ye.$$.fragment,K),K.forEach(o),X.forEach(o),yn=c(t),de=s(t,"H2",{class:!0});var An=a(de);$e=s(An,"A",{id:!0,class:!0,href:!0});var Ji=a($e);bo=s(Ji,"SPAN",{});var Ri=a(bo);T(Ze.$$.fragment,Ri),Ri.forEach(o),Ji.forEach(o),Qs=c(An),Po=s(An,"SPAN",{});var Xi=a(Po);Ys=i(Xi,"GPTNeoForCausalLM"),Xi.forEach(o),An.forEach(o),bn=c(t),q=s(t,"DIV",{class:!0});var Q=a(q);T(et.$$.fragment,Q),Zs=c(Q),ko=s(Q,"P",{});var Ki=a(ko);ea=i(Ki,`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Ki.forEach(o),ta=c(Q),tt=s(Q,"P",{});var In=a(tt);oa=i(In,"This model inherits from "),Ut=s(In,"A",{href:!0});var Qi=a(Ut);na=i(Qi,"PreTrainedModel"),Qi.forEach(o),sa=i(In,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),In.forEach(o),aa=c(Q),ot=s(Q,"P",{});var Ln=a(ot);ra=i(Ln,"This model is also a PyTorch "),nt=s(Ln,"A",{href:!0,rel:!0});var Yi=a(nt);ia=i(Yi,"torch.nn.Module"),Yi.forEach(o),la=i(Ln,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ln.forEach(o),da=c(Q),A=s(Q,"DIV",{class:!0});var Y=a(A);T(st.$$.fragment,Y),ca=c(Y),ce=s(Y,"P",{});var no=a(ce);pa=i(no,"The "),Vt=s(no,"A",{href:!0});var Zi=a(Vt);ha=i(Zi,"GPTNeoForCausalLM"),Zi.forEach(o),ua=i(no," forward method, overrides the "),wo=s(no,"CODE",{});var el=a(wo);ma=i(el,"__call__"),el.forEach(o),fa=i(no," special method."),no.forEach(o),ga=c(Y),T(Ge.$$.fragment,Y),_a=c(Y),No=s(Y,"P",{});var tl=a(No);Ta=i(tl,"Example:"),tl.forEach(o),va=c(Y),T(at.$$.fragment,Y),Y.forEach(o),Q.forEach(o),Pn=c(t),pe=s(t,"H2",{class:!0});var Sn=a(pe);xe=s(Sn,"A",{id:!0,class:!0,href:!0});var ol=a(xe);$o=s(ol,"SPAN",{});var nl=a($o);T(rt.$$.fragment,nl),nl.forEach(o),ol.forEach(o),ya=c(Sn),Go=s(Sn,"SPAN",{});var sl=a(Go);ba=i(sl,"GPTNeoForSequenceClassification"),sl.forEach(o),Sn.forEach(o),kn=c(t),x=s(t,"DIV",{class:!0});var S=a(x);T(it.$$.fragment,S),Pa=c(S),xo=s(S,"P",{});var al=a(xo);ka=i(al,"The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),al.forEach(o),wa=c(S),Jt=s(S,"P",{});var Ti=a(Jt);Rt=s(Ti,"A",{href:!0});var rl=a(Rt);Na=i(rl,"GPTNeoForSequenceClassification"),rl.forEach(o),$a=i(Ti,` uses the last token in order to do the classification, as other causal models
(e.g. GPT-1) do.`),Ti.forEach(o),Ga=c(S),B=s(S,"P",{});var Z=a(B);xa=i(Z,`Since it does classification on the last token, it requires to know the position of the last token. If a
`),Mo=s(Z,"CODE",{});var il=a(Mo);Ma=i(il,"pad_token_id"),il.forEach(o),Fa=i(Z,` is defined in the configuration, it finds the last token that is not a padding token in each row. If
no `),Fo=s(Z,"CODE",{});var ll=a(Fo);Ea=i(ll,"pad_token_id"),ll.forEach(o),za=i(Z,` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when `),Eo=s(Z,"CODE",{});var dl=a(Eo);Ca=i(dl,"inputs_embeds"),dl.forEach(o),qa=i(Z," are passed instead of "),zo=s(Z,"CODE",{});var cl=a(zo);ja=i(cl,"input_ids"),cl.forEach(o),Aa=i(Z,`, it does the same (take the last value in
each row of the batch).`),Z.forEach(o),Ia=c(S),lt=s(S,"P",{});var On=a(lt);La=i(On,"This model inherits from "),Xt=s(On,"A",{href:!0});var pl=a(Xt);Sa=i(pl,"PreTrainedModel"),pl.forEach(o),Oa=i(On,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),On.forEach(o),Da=c(S),dt=s(S,"P",{});var Dn=a(dt);Ba=i(Dn,"This model is also a PyTorch "),ct=s(Dn,"A",{href:!0,rel:!0});var hl=a(ct);Wa=i(hl,"torch.nn.Module"),hl.forEach(o),Ha=i(Dn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Dn.forEach(o),Ua=c(S),G=s(S,"DIV",{class:!0});var E=a(G);T(pt.$$.fragment,E),Va=c(E),he=s(E,"P",{});var so=a(he);Ja=i(so,"The "),Kt=s(so,"A",{href:!0});var ul=a(Kt);Ra=i(ul,"GPTNeoForSequenceClassification"),ul.forEach(o),Xa=i(so," forward method, overrides the "),Co=s(so,"CODE",{});var ml=a(Co);Ka=i(ml,"__call__"),ml.forEach(o),Qa=i(so," special method."),so.forEach(o),Ya=c(E),T(Me.$$.fragment,E),Za=c(E),qo=s(E,"P",{});var fl=a(qo);er=i(fl,"Example of single-label classification:"),fl.forEach(o),tr=c(E),T(ht.$$.fragment,E),or=c(E),T(ut.$$.fragment,E),nr=c(E),jo=s(E,"P",{});var gl=a(jo);sr=i(gl,"Example of multi-label classification:"),gl.forEach(o),ar=c(E),T(mt.$$.fragment,E),E.forEach(o),S.forEach(o),wn=c(t),ue=s(t,"H2",{class:!0});var Bn=a(ue);Fe=s(Bn,"A",{id:!0,class:!0,href:!0});var _l=a(Fe);Ao=s(_l,"SPAN",{});var Tl=a(Ao);T(ft.$$.fragment,Tl),Tl.forEach(o),_l.forEach(o),rr=c(Bn),Io=s(Bn,"SPAN",{});var vl=a(Io);ir=i(vl,"FlaxGPTNeoModel"),vl.forEach(o),Bn.forEach(o),Nn=c(t),M=s(t,"DIV",{class:!0});var O=a(M);T(gt.$$.fragment,O),lr=c(O),Lo=s(O,"P",{});var yl=a(Lo);dr=i(yl,"The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),yl.forEach(o),cr=c(O),_t=s(O,"P",{});var Wn=a(_t);pr=i(Wn,"This model inherits from "),Qt=s(Wn,"A",{href:!0});var bl=a(Qt);hr=i(bl,"FlaxPreTrainedModel"),bl.forEach(o),ur=i(Wn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wn.forEach(o),mr=c(O),Tt=s(O,"P",{});var Hn=a(Tt);fr=i(Hn,`This model is also a Flax Linen
`),vt=s(Hn,"A",{href:!0,rel:!0});var Pl=a(vt);gr=i(Pl,"flax.nn.Module"),Pl.forEach(o),_r=i(Hn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Hn.forEach(o),Tr=c(O),So=s(O,"P",{});var kl=a(So);vr=i(kl,"Finally, this model supports inherent JAX features such as:"),kl.forEach(o),yr=c(O),H=s(O,"UL",{});var qe=a(H);Oo=s(qe,"LI",{});var wl=a(Oo);yt=s(wl,"A",{href:!0,rel:!0});var Nl=a(yt);br=i(Nl,"Just-In-Time (JIT) compilation"),Nl.forEach(o),wl.forEach(o),Pr=c(qe),Do=s(qe,"LI",{});var $l=a(Do);bt=s($l,"A",{href:!0,rel:!0});var Gl=a(bt);kr=i(Gl,"Automatic Differentiation"),Gl.forEach(o),$l.forEach(o),wr=c(qe),Bo=s(qe,"LI",{});var xl=a(Bo);Pt=s(xl,"A",{href:!0,rel:!0});var Ml=a(Pt);Nr=i(Ml,"Vectorization"),Ml.forEach(o),xl.forEach(o),$r=c(qe),Wo=s(qe,"LI",{});var Fl=a(Wo);kt=s(Fl,"A",{href:!0,rel:!0});var El=a(kt);Gr=i(El,"Parallelization"),El.forEach(o),Fl.forEach(o),qe.forEach(o),xr=c(O),I=s(O,"DIV",{class:!0});var ee=a(I);T(wt.$$.fragment,ee),Mr=c(ee),me=s(ee,"P",{});var ao=a(me);Fr=i(ao,"The "),Ho=s(ao,"CODE",{});var zl=a(Ho);Er=i(zl,"FlaxGPTNeoPreTrainedModel"),zl.forEach(o),zr=i(ao,"forward method, overrides the "),Uo=s(ao,"CODE",{});var Cl=a(Uo);Cr=i(Cl,"__call__"),Cl.forEach(o),qr=i(ao," special method."),ao.forEach(o),jr=c(ee),T(Ee.$$.fragment,ee),Ar=c(ee),Vo=s(ee,"P",{});var ql=a(Vo);Ir=i(ql,"Example:"),ql.forEach(o),Lr=c(ee),T(Nt.$$.fragment,ee),ee.forEach(o),O.forEach(o),$n=c(t),fe=s(t,"H2",{class:!0});var Un=a(fe);ze=s(Un,"A",{id:!0,class:!0,href:!0});var jl=a(ze);Jo=s(jl,"SPAN",{});var Al=a(Jo);T($t.$$.fragment,Al),Al.forEach(o),jl.forEach(o),Sr=c(Un),Ro=s(Un,"SPAN",{});var Il=a(Ro);Or=i(Il,"FlaxGPTNeoForCausalLM"),Il.forEach(o),Un.forEach(o),Gn=c(t),F=s(t,"DIV",{class:!0});var D=a(F);T(Gt.$$.fragment,D),Dr=c(D),Xo=s(D,"P",{});var Ll=a(Xo);Br=i(Ll,`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Ll.forEach(o),Wr=c(D),xt=s(D,"P",{});var Vn=a(xt);Hr=i(Vn,"This model inherits from "),Yt=s(Vn,"A",{href:!0});var Sl=a(Yt);Ur=i(Sl,"FlaxPreTrainedModel"),Sl.forEach(o),Vr=i(Vn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vn.forEach(o),Jr=c(D),Mt=s(D,"P",{});var Jn=a(Mt);Rr=i(Jn,`This model is also a Flax Linen
`),Ft=s(Jn,"A",{href:!0,rel:!0});var Ol=a(Ft);Xr=i(Ol,"flax.nn.Module"),Ol.forEach(o),Kr=i(Jn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Jn.forEach(o),Qr=c(D),Ko=s(D,"P",{});var Dl=a(Ko);Yr=i(Dl,"Finally, this model supports inherent JAX features such as:"),Dl.forEach(o),Zr=c(D),U=s(D,"UL",{});var je=a(U);Qo=s(je,"LI",{});var Bl=a(Qo);Et=s(Bl,"A",{href:!0,rel:!0});var Wl=a(Et);ei=i(Wl,"Just-In-Time (JIT) compilation"),Wl.forEach(o),Bl.forEach(o),ti=c(je),Yo=s(je,"LI",{});var Hl=a(Yo);zt=s(Hl,"A",{href:!0,rel:!0});var Ul=a(zt);oi=i(Ul,"Automatic Differentiation"),Ul.forEach(o),Hl.forEach(o),ni=c(je),Zo=s(je,"LI",{});var Vl=a(Zo);Ct=s(Vl,"A",{href:!0,rel:!0});var Jl=a(Ct);si=i(Jl,"Vectorization"),Jl.forEach(o),Vl.forEach(o),ai=c(je),en=s(je,"LI",{});var Rl=a(en);qt=s(Rl,"A",{href:!0,rel:!0});var Xl=a(qt);ri=i(Xl,"Parallelization"),Xl.forEach(o),Rl.forEach(o),je.forEach(o),ii=c(D),L=s(D,"DIV",{class:!0});var te=a(L);T(jt.$$.fragment,te),li=c(te),ge=s(te,"P",{});var ro=a(ge);di=i(ro,"The "),tn=s(ro,"CODE",{});var Kl=a(tn);ci=i(Kl,"FlaxGPTNeoPreTrainedModel"),Kl.forEach(o),pi=i(ro,"forward method, overrides the "),on=s(ro,"CODE",{});var Ql=a(on);hi=i(Ql,"__call__"),Ql.forEach(o),ui=i(ro," special method."),ro.forEach(o),mi=c(te),T(Ce.$$.fragment,te),fi=c(te),nn=s(te,"P",{});var Yl=a(nn);gi=i(Yl,"Example:"),Yl.forEach(o),_i=c(te),T(At.$$.fragment,te),te.forEach(o),D.forEach(o),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(cd)),l(k,"id","gpt-neo"),l(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(k,"href","#gpt-neo"),l(m,"class","relative group"),l(ve,"id","overview"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#overview"),l(oe,"class","relative group"),l(Ie,"href","https://github.com/EleutherAI/gpt-neo"),l(Ie,"rel","nofollow"),l(Le,"href","https://pile.eleuther.ai/"),l(Le,"rel","nofollow"),l(Se,"href","https://huggingface.co/valhalla"),l(Se,"rel","nofollow"),l(be,"id","generation"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#generation"),l(ne,"class","relative group"),l(ke,"id","transformers.GPTNeoConfig"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.GPTNeoConfig"),l(se,"class","relative group"),l(Ot,"href","/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(He,"href","https://huggingface.co/EleutherAI/gpt-neo-1.3B"),l(He,"rel","nofollow"),l(Dt,"href","/docs/transformers/pr_16363/en/main_classes/configuration#transformers.PretrainedConfig"),l(Bt,"href","/docs/transformers/pr_16363/en/main_classes/configuration#transformers.PretrainedConfig"),l(z,"class","docstring"),l(we,"id","transformers.GPTNeoModel"),l(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(we,"href","#transformers.GPTNeoModel"),l(ie,"class","relative group"),l(Wt,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),l(Ke,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ke,"rel","nofollow"),l(Ht,"href","/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(j,"class","docstring"),l(C,"class","docstring"),l($e,"id","transformers.GPTNeoForCausalLM"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#transformers.GPTNeoForCausalLM"),l(de,"class","relative group"),l(Ut,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),l(nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(nt,"rel","nofollow"),l(Vt,"href","/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),l(A,"class","docstring"),l(q,"class","docstring"),l(xe,"id","transformers.GPTNeoForSequenceClassification"),l(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(xe,"href","#transformers.GPTNeoForSequenceClassification"),l(pe,"class","relative group"),l(Rt,"href","/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(Xt,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),l(ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(ct,"rel","nofollow"),l(Kt,"href","/docs/transformers/pr_16363/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(G,"class","docstring"),l(x,"class","docstring"),l(Fe,"id","transformers.FlaxGPTNeoModel"),l(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Fe,"href","#transformers.FlaxGPTNeoModel"),l(ue,"class","relative group"),l(Qt,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(vt,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(vt,"rel","nofollow"),l(yt,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(yt,"rel","nofollow"),l(bt,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(bt,"rel","nofollow"),l(Pt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(Pt,"rel","nofollow"),l(kt,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(kt,"rel","nofollow"),l(I,"class","docstring"),l(M,"class","docstring"),l(ze,"id","transformers.FlaxGPTNeoForCausalLM"),l(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ze,"href","#transformers.FlaxGPTNeoForCausalLM"),l(fe,"class","relative group"),l(Yt,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(Ft,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(Ft,"rel","nofollow"),l(Et,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(Et,"rel","nofollow"),l(zt,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(zt,"rel","nofollow"),l(Ct,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(Ct,"rel","nofollow"),l(qt,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(qt,"rel","nofollow"),l(L,"class","docstring"),l(F,"class","docstring")},m(t,h){e(document.head,p),u(t,N,h),u(t,m,h),e(m,k),e(k,w),v(g,w,null),e(m,f),e(m,$),e($,Rn),u(t,dn,h),u(t,oe,h),e(oe,ve),e(ve,io),v(Ae,io,null),e(oe,Xn),e(oe,lo),e(lo,Kn),u(t,cn,h),u(t,J,h),e(J,Qn),e(J,Ie),e(Ie,Yn),e(J,Zn),e(J,Le),e(Le,es),e(J,ts),u(t,pn,h),u(t,St,h),e(St,os),u(t,hn,h),u(t,ye,h),e(ye,ns),e(ye,Se),e(Se,ss),e(ye,as),u(t,un,h),u(t,ne,h),e(ne,be),e(be,co),v(Oe,co,null),e(ne,rs),e(ne,po),e(po,is),u(t,mn,h),u(t,Pe,h),e(Pe,ls),e(Pe,ho),e(ho,ds),e(Pe,cs),u(t,fn,h),v(De,t,h),u(t,gn,h),u(t,se,h),e(se,ke),e(ke,uo),v(Be,uo,null),e(se,ps),e(se,mo),e(mo,hs),u(t,_n,h),u(t,z,h),v(We,z,null),e(z,us),e(z,ae),e(ae,ms),e(ae,Ot),e(Ot,fs),e(ae,gs),e(ae,He),e(He,_s),e(ae,Ts),e(z,vs),e(z,re),e(re,ys),e(re,Dt),e(Dt,bs),e(re,Ps),e(re,Bt),e(Bt,ks),e(re,ws),e(z,Ns),e(z,fo),e(fo,$s),e(z,Gs),v(Ue,z,null),u(t,Tn,h),u(t,ie,h),e(ie,we),e(we,go),v(Ve,go,null),e(ie,xs),e(ie,_o),e(_o,Ms),u(t,vn,h),u(t,C,h),v(Je,C,null),e(C,Fs),e(C,To),e(To,Es),e(C,zs),e(C,Re),e(Re,Cs),e(Re,Wt),e(Wt,qs),e(Re,js),e(C,As),e(C,Xe),e(Xe,Is),e(Xe,Ke),e(Ke,Ls),e(Xe,Ss),e(C,Os),e(C,j),v(Qe,j,null),e(j,Ds),e(j,le),e(le,Bs),e(le,Ht),e(Ht,Ws),e(le,Hs),e(le,vo),e(vo,Us),e(le,Vs),e(j,Js),v(Ne,j,null),e(j,Rs),e(j,yo),e(yo,Xs),e(j,Ks),v(Ye,j,null),u(t,yn,h),u(t,de,h),e(de,$e),e($e,bo),v(Ze,bo,null),e(de,Qs),e(de,Po),e(Po,Ys),u(t,bn,h),u(t,q,h),v(et,q,null),e(q,Zs),e(q,ko),e(ko,ea),e(q,ta),e(q,tt),e(tt,oa),e(tt,Ut),e(Ut,na),e(tt,sa),e(q,aa),e(q,ot),e(ot,ra),e(ot,nt),e(nt,ia),e(ot,la),e(q,da),e(q,A),v(st,A,null),e(A,ca),e(A,ce),e(ce,pa),e(ce,Vt),e(Vt,ha),e(ce,ua),e(ce,wo),e(wo,ma),e(ce,fa),e(A,ga),v(Ge,A,null),e(A,_a),e(A,No),e(No,Ta),e(A,va),v(at,A,null),u(t,Pn,h),u(t,pe,h),e(pe,xe),e(xe,$o),v(rt,$o,null),e(pe,ya),e(pe,Go),e(Go,ba),u(t,kn,h),u(t,x,h),v(it,x,null),e(x,Pa),e(x,xo),e(xo,ka),e(x,wa),e(x,Jt),e(Jt,Rt),e(Rt,Na),e(Jt,$a),e(x,Ga),e(x,B),e(B,xa),e(B,Mo),e(Mo,Ma),e(B,Fa),e(B,Fo),e(Fo,Ea),e(B,za),e(B,Eo),e(Eo,Ca),e(B,qa),e(B,zo),e(zo,ja),e(B,Aa),e(x,Ia),e(x,lt),e(lt,La),e(lt,Xt),e(Xt,Sa),e(lt,Oa),e(x,Da),e(x,dt),e(dt,Ba),e(dt,ct),e(ct,Wa),e(dt,Ha),e(x,Ua),e(x,G),v(pt,G,null),e(G,Va),e(G,he),e(he,Ja),e(he,Kt),e(Kt,Ra),e(he,Xa),e(he,Co),e(Co,Ka),e(he,Qa),e(G,Ya),v(Me,G,null),e(G,Za),e(G,qo),e(qo,er),e(G,tr),v(ht,G,null),e(G,or),v(ut,G,null),e(G,nr),e(G,jo),e(jo,sr),e(G,ar),v(mt,G,null),u(t,wn,h),u(t,ue,h),e(ue,Fe),e(Fe,Ao),v(ft,Ao,null),e(ue,rr),e(ue,Io),e(Io,ir),u(t,Nn,h),u(t,M,h),v(gt,M,null),e(M,lr),e(M,Lo),e(Lo,dr),e(M,cr),e(M,_t),e(_t,pr),e(_t,Qt),e(Qt,hr),e(_t,ur),e(M,mr),e(M,Tt),e(Tt,fr),e(Tt,vt),e(vt,gr),e(Tt,_r),e(M,Tr),e(M,So),e(So,vr),e(M,yr),e(M,H),e(H,Oo),e(Oo,yt),e(yt,br),e(H,Pr),e(H,Do),e(Do,bt),e(bt,kr),e(H,wr),e(H,Bo),e(Bo,Pt),e(Pt,Nr),e(H,$r),e(H,Wo),e(Wo,kt),e(kt,Gr),e(M,xr),e(M,I),v(wt,I,null),e(I,Mr),e(I,me),e(me,Fr),e(me,Ho),e(Ho,Er),e(me,zr),e(me,Uo),e(Uo,Cr),e(me,qr),e(I,jr),v(Ee,I,null),e(I,Ar),e(I,Vo),e(Vo,Ir),e(I,Lr),v(Nt,I,null),u(t,$n,h),u(t,fe,h),e(fe,ze),e(ze,Jo),v($t,Jo,null),e(fe,Sr),e(fe,Ro),e(Ro,Or),u(t,Gn,h),u(t,F,h),v(Gt,F,null),e(F,Dr),e(F,Xo),e(Xo,Br),e(F,Wr),e(F,xt),e(xt,Hr),e(xt,Yt),e(Yt,Ur),e(xt,Vr),e(F,Jr),e(F,Mt),e(Mt,Rr),e(Mt,Ft),e(Ft,Xr),e(Mt,Kr),e(F,Qr),e(F,Ko),e(Ko,Yr),e(F,Zr),e(F,U),e(U,Qo),e(Qo,Et),e(Et,ei),e(U,ti),e(U,Yo),e(Yo,zt),e(zt,oi),e(U,ni),e(U,Zo),e(Zo,Ct),e(Ct,si),e(U,ai),e(U,en),e(en,qt),e(qt,ri),e(F,ii),e(F,L),v(jt,L,null),e(L,li),e(L,ge),e(ge,di),e(ge,tn),e(tn,ci),e(ge,pi),e(ge,on),e(on,hi),e(ge,ui),e(L,mi),v(Ce,L,null),e(L,fi),e(L,nn),e(nn,gi),e(L,_i),v(At,L,null),xn=!0},p(t,[h]){const It={};h&2&&(It.$$scope={dirty:h,ctx:t}),Ne.$set(It);const sn={};h&2&&(sn.$$scope={dirty:h,ctx:t}),Ge.$set(sn);const an={};h&2&&(an.$$scope={dirty:h,ctx:t}),Me.$set(an);const rn={};h&2&&(rn.$$scope={dirty:h,ctx:t}),Ee.$set(rn);const Lt={};h&2&&(Lt.$$scope={dirty:h,ctx:t}),Ce.$set(Lt)},i(t){xn||(y(g.$$.fragment,t),y(Ae.$$.fragment,t),y(Oe.$$.fragment,t),y(De.$$.fragment,t),y(Be.$$.fragment,t),y(We.$$.fragment,t),y(Ue.$$.fragment,t),y(Ve.$$.fragment,t),y(Je.$$.fragment,t),y(Qe.$$.fragment,t),y(Ne.$$.fragment,t),y(Ye.$$.fragment,t),y(Ze.$$.fragment,t),y(et.$$.fragment,t),y(st.$$.fragment,t),y(Ge.$$.fragment,t),y(at.$$.fragment,t),y(rt.$$.fragment,t),y(it.$$.fragment,t),y(pt.$$.fragment,t),y(Me.$$.fragment,t),y(ht.$$.fragment,t),y(ut.$$.fragment,t),y(mt.$$.fragment,t),y(ft.$$.fragment,t),y(gt.$$.fragment,t),y(wt.$$.fragment,t),y(Ee.$$.fragment,t),y(Nt.$$.fragment,t),y($t.$$.fragment,t),y(Gt.$$.fragment,t),y(jt.$$.fragment,t),y(Ce.$$.fragment,t),y(At.$$.fragment,t),xn=!0)},o(t){b(g.$$.fragment,t),b(Ae.$$.fragment,t),b(Oe.$$.fragment,t),b(De.$$.fragment,t),b(Be.$$.fragment,t),b(We.$$.fragment,t),b(Ue.$$.fragment,t),b(Ve.$$.fragment,t),b(Je.$$.fragment,t),b(Qe.$$.fragment,t),b(Ne.$$.fragment,t),b(Ye.$$.fragment,t),b(Ze.$$.fragment,t),b(et.$$.fragment,t),b(st.$$.fragment,t),b(Ge.$$.fragment,t),b(at.$$.fragment,t),b(rt.$$.fragment,t),b(it.$$.fragment,t),b(pt.$$.fragment,t),b(Me.$$.fragment,t),b(ht.$$.fragment,t),b(ut.$$.fragment,t),b(mt.$$.fragment,t),b(ft.$$.fragment,t),b(gt.$$.fragment,t),b(wt.$$.fragment,t),b(Ee.$$.fragment,t),b(Nt.$$.fragment,t),b($t.$$.fragment,t),b(Gt.$$.fragment,t),b(jt.$$.fragment,t),b(Ce.$$.fragment,t),b(At.$$.fragment,t),xn=!1},d(t){o(p),t&&o(N),t&&o(m),P(g),t&&o(dn),t&&o(oe),P(Ae),t&&o(cn),t&&o(J),t&&o(pn),t&&o(St),t&&o(hn),t&&o(ye),t&&o(un),t&&o(ne),P(Oe),t&&o(mn),t&&o(Pe),t&&o(fn),P(De,t),t&&o(gn),t&&o(se),P(Be),t&&o(_n),t&&o(z),P(We),P(Ue),t&&o(Tn),t&&o(ie),P(Ve),t&&o(vn),t&&o(C),P(Je),P(Qe),P(Ne),P(Ye),t&&o(yn),t&&o(de),P(Ze),t&&o(bn),t&&o(q),P(et),P(st),P(Ge),P(at),t&&o(Pn),t&&o(pe),P(rt),t&&o(kn),t&&o(x),P(it),P(pt),P(Me),P(ht),P(ut),P(mt),t&&o(wn),t&&o(ue),P(ft),t&&o(Nn),t&&o(M),P(gt),P(wt),P(Ee),P(Nt),t&&o($n),t&&o(fe),P($t),t&&o(Gn),t&&o(F),P(Gt),P(jt),P(Ce),P(At)}}}const cd={local:"gpt-neo",sections:[{local:"overview",sections:[{local:"generation",title:"Generation"}],title:"Overview"},{local:"transformers.GPTNeoConfig",title:"GPTNeoConfig"},{local:"transformers.GPTNeoModel",title:"GPTNeoModel"},{local:"transformers.GPTNeoForCausalLM",title:"GPTNeoForCausalLM"},{local:"transformers.GPTNeoForSequenceClassification",title:"GPTNeoForSequenceClassification"},{local:"transformers.FlaxGPTNeoModel",title:"FlaxGPTNeoModel"},{local:"transformers.FlaxGPTNeoForCausalLM",title:"FlaxGPTNeoForCausalLM"}],title:"GPT Neo"};function pd(W){return nd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _d extends Zl{constructor(p){super();ed(this,p,pd,dd,td,{})}}export{_d as default,cd as metadata};
