import{S as C$,i as j$,s as x$,e as r,k as l,w as F,t,M as L$,c as a,d as n,m as d,a as i,x as v,h as o,b as c,F as e,g as h,y,q as w,o as b,B as $,v as O$}from"../../chunks/vendor-6b77c823.js";import{T as ze}from"../../chunks/Tip-39098574.js";import{D as X}from"../../chunks/Docstring-abef54e3.js";import{C as qe}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Pe}from"../../chunks/IconCopyLink-7a11ce68.js";function D$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function A$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function N$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function I$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function S$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function B$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function W$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function Q$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function U$(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function R$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function H$(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function V$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function Y$(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function K$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function G$(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function Z$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function X$(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function J$(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function e2(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function n2(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function t2(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function o2(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function s2(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){u=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),pe=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),ue=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(p){u=a(p,"P",{});var E=i(u);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(p),_=a(p,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(p),q=a(p,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),pe=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);ue=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(p),L=a(p,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(p),P=a(p,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(p,E){h(p,u,E),e(u,z),h(p,g,E),h(p,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(p,K,E),h(p,q,E),e(q,J),e(q,A),e(A,ne),e(q,pe),e(q,N),e(N,ue),e(q,ie),h(p,Y,E),h(p,L,E),e(L,te),h(p,G,E),h(p,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(p){p&&n(u),p&&n(g),p&&n(_),p&&n(K),p&&n(q),p&&n(Y),p&&n(L),p&&n(G),p&&n(P)}}}function r2(V){let u,z,g,_,k;return{c(){u=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){u=a(T,"P",{});var m=i(u);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,u,m),e(u,z),e(u,g),e(g,_),e(u,k)},d(T){T&&n(u)}}}function a2(V){let u,z,g,_,k,T,m,M,ce,K,q,J,A,ne,pe,N,ue,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge,p,E,Z,Te,ye,D,ke,we,be,x,R,$e,Fe,H,Ee,ve,_e,Me,Xa,Qu,Uu,Pc,Dn,Ru,No,Hu,Vu,Io,Yu,Ku,Cc,Zn,Bt,ml,So,Gu,gl,Zu,jc,Ln,Bo,Xu,On,Ju,Ja,eh,nh,ei,th,oh,Wo,sh,rh,ah,Xn,ih,ni,lh,dh,ti,ch,ph,xc,Jn,Wt,_l,Qo,uh,Tl,hh,Lc,Ce,Uo,fh,kl,mh,gh,Qt,oi,_h,Th,si,kh,Fh,vh,Ro,yh,ri,wh,bh,$h,An,Ho,Eh,Fl,Mh,zh,Vo,ai,qh,vl,Ph,Ch,ii,jh,yl,xh,Lh,Ut,Yo,Oh,Ko,Dh,wl,Ah,Nh,Ih,yn,Go,Sh,bl,Bh,Wh,Zo,Qh,et,Uh,$l,Rh,Hh,El,Vh,Yh,Kh,li,Xo,Oc,nt,Rt,Ml,Jo,Gh,zl,Zh,Dc,en,es,Xh,ns,Jh,ql,ef,nf,tf,Ht,di,of,sf,ci,rf,af,lf,ts,df,pi,cf,pf,uf,wn,os,hf,Pl,ff,mf,ss,gf,tt,_f,Cl,Tf,kf,jl,Ff,vf,Ac,ot,Vt,xl,rs,yf,Ll,wf,Nc,st,as,bf,is,$f,ui,Ef,Mf,Ic,rt,ls,zf,ds,qf,hi,Pf,Cf,Sc,at,Yt,Ol,cs,jf,Dl,xf,Bc,We,ps,Lf,Al,Of,Df,us,Af,hs,Nf,If,Sf,fs,Bf,fi,Wf,Qf,Uf,ms,Rf,gs,Hf,Vf,Yf,nn,_s,Kf,it,Gf,mi,Zf,Xf,Nl,Jf,em,nm,Kt,tm,Il,om,sm,Ts,Wc,lt,Gt,Sl,ks,rm,Bl,am,Qc,Qe,Fs,im,Wl,lm,dm,vs,cm,ys,pm,um,hm,ws,fm,gi,mm,gm,_m,bs,Tm,$s,km,Fm,vm,tn,Es,ym,dt,wm,_i,bm,$m,Ql,Em,Mm,zm,Zt,qm,Ul,Pm,Cm,Ms,Uc,ct,Xt,Rl,zs,jm,Hl,xm,Rc,pt,qs,Lm,on,Ps,Om,ut,Dm,Ti,Am,Nm,Vl,Im,Sm,Bm,Jt,Wm,Yl,Qm,Um,Cs,Hc,ht,eo,Kl,js,Rm,Gl,Hm,Vc,Ue,xs,Vm,Ls,Ym,Zl,Km,Gm,Zm,Os,Xm,Ds,Jm,eg,ng,As,tg,ki,og,sg,rg,Ns,ag,Is,ig,lg,dg,Ke,Ss,cg,ft,pg,Fi,ug,hg,Xl,fg,mg,gg,no,_g,Jl,Tg,kg,Bs,Fg,Ws,Yc,mt,to,ed,Qs,vg,nd,yg,Kc,Re,Us,wg,td,bg,$g,Rs,Eg,Hs,Mg,zg,qg,Vs,Pg,vi,Cg,jg,xg,Ys,Lg,Ks,Og,Dg,Ag,xe,Gs,Ng,gt,Ig,yi,Sg,Bg,od,Wg,Qg,Ug,oo,Rg,sd,Hg,Vg,Zs,Yg,Xs,Kg,rd,Gg,Zg,Js,Gc,_t,so,ad,er,Xg,id,Jg,Zc,He,nr,e_,ld,n_,t_,tr,o_,or,s_,r_,a_,sr,i_,wi,l_,d_,c_,rr,p_,ar,u_,h_,f_,sn,ir,m_,Tt,g_,bi,__,T_,dd,k_,F_,v_,ro,y_,cd,w_,b_,lr,Xc,kt,ao,pd,dr,$_,ud,E_,Jc,Ve,cr,M_,hd,z_,q_,pr,P_,ur,C_,j_,x_,hr,L_,$i,O_,D_,A_,fr,N_,mr,I_,S_,B_,Ge,gr,W_,Ft,Q_,Ei,U_,R_,fd,H_,V_,Y_,io,K_,md,G_,Z_,_r,X_,Tr,ep,vt,lo,gd,kr,J_,_d,eT,np,Ye,Fr,nT,yt,tT,Td,oT,sT,kd,rT,aT,iT,vr,lT,yr,dT,cT,pT,wr,uT,Mi,hT,fT,mT,br,gT,$r,_T,TT,kT,Ze,Er,FT,wt,vT,zi,yT,wT,Fd,bT,$T,ET,co,MT,vd,zT,qT,Mr,PT,zr,tp,bt,po,yd,qr,CT,wd,jT,op,Le,Pr,xT,bd,LT,OT,Cr,DT,jr,AT,NT,IT,xr,ST,qi,BT,WT,QT,Lr,UT,Or,RT,HT,VT,uo,YT,rn,Dr,KT,$t,GT,Pi,ZT,XT,$d,JT,ek,nk,ho,tk,Ed,ok,sk,Ar,sp,Et,fo,Md,Nr,rk,zd,ak,rp,Oe,Ir,ik,qd,lk,dk,Sr,ck,Br,pk,uk,hk,Wr,fk,Ci,mk,gk,_k,Qr,Tk,Ur,kk,Fk,vk,mo,yk,an,Rr,wk,Mt,bk,ji,$k,Ek,Pd,Mk,zk,qk,go,Pk,Cd,Ck,jk,Hr,ap,zt,_o,jd,Vr,xk,xd,Lk,ip,De,Yr,Ok,Ld,Dk,Ak,Kr,Nk,Gr,Ik,Sk,Bk,Zr,Wk,xi,Qk,Uk,Rk,Xr,Hk,Jr,Vk,Yk,Kk,To,Gk,ln,ea,Zk,qt,Xk,Li,Jk,eF,Od,nF,tF,oF,ko,sF,Dd,rF,aF,na,lp,Pt,Fo,Ad,ta,iF,Nd,lF,dp,Ae,oa,dF,sa,cF,Id,pF,uF,hF,ra,fF,aa,mF,gF,_F,ia,TF,Oi,kF,FF,vF,la,yF,da,wF,bF,$F,vo,EF,dn,ca,MF,Ct,zF,Di,qF,PF,Sd,CF,jF,xF,yo,LF,Bd,OF,DF,pa,cp,jt,wo,Wd,ua,AF,Qd,NF,pp,Ne,ha,IF,Ud,SF,BF,fa,WF,ma,QF,UF,RF,ga,HF,Ai,VF,YF,KF,_a,GF,Ta,ZF,XF,JF,bo,ev,cn,ka,nv,xt,tv,Ni,ov,sv,Rd,rv,av,iv,$o,lv,Hd,dv,cv,Fa,up,Lt,Eo,Vd,va,pv,Yd,uv,hp,Ie,ya,hv,Kd,fv,mv,wa,gv,ba,_v,Tv,kv,$a,Fv,Ii,vv,yv,wv,Ea,bv,Ma,$v,Ev,Mv,Mo,zv,pn,za,qv,Ot,Pv,Si,Cv,jv,Gd,xv,Lv,Ov,zo,Dv,Zd,Av,Nv,qa,fp,Dt,qo,Xd,Pa,Iv,Jd,Sv,mp,Se,Ca,Bv,ec,Wv,Qv,ja,Uv,xa,Rv,Hv,Vv,La,Yv,Bi,Kv,Gv,Zv,Oa,Xv,Da,Jv,ey,ny,Po,ty,un,Aa,oy,At,sy,Wi,ry,ay,nc,iy,ly,dy,Co,cy,tc,py,uy,Na,gp,Nt,jo,oc,Ia,hy,sc,fy,_p,Be,Sa,my,It,gy,rc,_y,Ty,ac,ky,Fy,vy,Ba,yy,Wa,wy,by,$y,Qa,Ey,Qi,My,zy,qy,Ua,Py,Ra,Cy,jy,xy,xo,Ly,hn,Ha,Oy,St,Dy,Ui,Ay,Ny,ic,Iy,Sy,By,Lo,Wy,lc,Qy,Uy,Va,Tp;return T=new Pe({}),ne=new Pe({}),So=new Pe({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Qo=new Pe({}),Uo=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new qe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new Pe({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new qe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new Pe({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L834",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L980",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new Pe({}),ps=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L927",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L943",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new ze({props:{$$slots:{default:[D$]},$$scope:{ctx:V}}}),Ts=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),ks=new Pe({}),Fs=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1004",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1021",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new ze({props:{$$slots:{default:[A$]},$$scope:{ctx:V}}}),Ms=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new Pe({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1112"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1121",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new ze({props:{$$slots:{default:[N$]},$$scope:{ctx:V}}}),Cs=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),js=new Pe({}),xs=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1195",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ss=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1211",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new ze({props:{$$slots:{default:[I$]},$$scope:{ctx:V}}}),Bs=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of <mask>
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of &lt;mask&gt;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),Ws=new qe({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-<mask> tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-&lt;mask&gt; tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
`}}),Qs=new Pe({}),Us=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1275",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gs=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1286",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new ze({props:{$$slots:{default:[S$]},$$scope:{ctx:V}}}),Zs=new qe({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),Xs=new qe({props:{code:`labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),Js=new qe({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]


num_labels = len(model.config.id2label)
labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]


<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),er=new Pe({}),nr=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1368",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ir=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1377",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new ze({props:{$$slots:{default:[B$]},$$scope:{ctx:V}}}),lr=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),dr=new Pe({}),cr=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1452",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),gr=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1464",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new ze({props:{$$slots:{default:[W$]},$$scope:{ctx:V}}}),_r=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),Tr=new qe({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),kr=new Pe({}),Fr=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1526",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Er=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_funnel.py#L1537",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new ze({props:{$$slots:{default:[Q$]},$$scope:{ctx:V}}}),Mr=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),zr=new qe({props:{code:`# target is "nice puppet"
target_start_index, target_end_index = torch.tensor([14]), torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index, target_end_index = torch.tensor([<span class="hljs-number">14</span>]), torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),qr=new Pe({}),Pr=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1097",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),uo=new ze({props:{$$slots:{default:[U$]},$$scope:{ctx:V}}}),Dr=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1102",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ho=new ze({props:{$$slots:{default:[R$]},$$scope:{ctx:V}}}),Ar=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Nr=new Pe({}),Ir=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1145",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[H$]},$$scope:{ctx:V}}}),Rr=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1150",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),go=new ze({props:{$$slots:{default:[V$]},$$scope:{ctx:V}}}),Hr=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Vr=new Pe({}),Yr=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1196",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),To=new ze({props:{$$slots:{default:[Y$]},$$scope:{ctx:V}}}),ea=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1203",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ko=new ze({props:{$$slots:{default:[K$]},$$scope:{ctx:V}}}),na=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),ta=new Pe({}),oa=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1263",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[G$]},$$scope:{ctx:V}}}),ca=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1277",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[Z$]},$$scope:{ctx:V}}}),pa=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ua=new Pe({}),ha=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1345",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bo=new ze({props:{$$slots:{default:[X$]},$$scope:{ctx:V}}}),ka=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1353",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$o=new ze({props:{$$slots:{default:[J$]},$$scope:{ctx:V}}}),Fa=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),va=new Pe({}),ya=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1422",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[e2]},$$scope:{ctx:V}}}),za=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1439",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),zo=new ze({props:{$$slots:{default:[n2]},$$scope:{ctx:V}}}),qa=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Pa=new Pe({}),Ca=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1539",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[t2]},$$scope:{ctx:V}}}),Aa=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1550",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Co=new ze({props:{$$slots:{default:[o2]},$$scope:{ctx:V}}}),Na=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ia=new Pe({}),Sa=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1618",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[s2]},$$scope:{ctx:V}}}),Ha=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16363/src/transformers/models/funnel/modeling_tf_funnel.py#L1628",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_16363/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16363/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16363/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Lo=new ze({props:{$$slots:{default:[r2]},$$scope:{ctx:V}}}),Va=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){u=r("meta"),z=l(),g=r("h1"),_=r("a"),k=r("span"),F(T.$$.fragment),m=l(),M=r("span"),ce=t("Funnel Transformer"),K=l(),q=r("h2"),J=r("a"),A=r("span"),F(ne.$$.fragment),pe=l(),N=r("span"),ue=t("Overview"),ie=l(),Y=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),G=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),j=l(),oe=r("p"),W=t("The abstract from the paper is the following:"),le=l(),se=r("p"),I=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),Q=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),S=r("li"),O=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),U=t("FunnelModel"),ge=t(", "),p=r("a"),E=t("FunnelForPreTraining"),Z=t(`,
`),Te=r("a"),ye=t("FunnelForMaskedLM"),D=t(", "),ke=r("a"),we=t("FunnelForTokenClassification"),be=t(` and
class:`),x=r("em"),R=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),Fe=r("a"),H=t("FunnelBaseModel"),Ee=t(", "),ve=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Xa=r("a"),Qu=t("FunnelForMultipleChoice"),Uu=t("."),Pc=l(),Dn=r("p"),Ru=t("This model was contributed by "),No=r("a"),Hu=t("sgugger"),Vu=t(". The original code can be found "),Io=r("a"),Yu=t("here"),Ku=t("."),Cc=l(),Zn=r("h2"),Bt=r("a"),ml=r("span"),F(So.$$.fragment),Gu=l(),gl=r("span"),Zu=t("FunnelConfig"),jc=l(),Ln=r("div"),F(Bo.$$.fragment),Xu=l(),On=r("p"),Ju=t("This is the configuration class to store the configuration of a "),Ja=r("a"),eh=t("FunnelModel"),nh=t(" or a "),ei=r("a"),th=t("TFBertModel"),oh=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),sh=t("funnel-transformer/small"),rh=t(" architecture."),ah=l(),Xn=r("p"),ih=t("Configuration objects inherit from "),ni=r("a"),lh=t("PretrainedConfig"),dh=t(` and can be used to control the model outputs. Read the
documentation from `),ti=r("a"),ch=t("PretrainedConfig"),ph=t(" for more information."),xc=l(),Jn=r("h2"),Wt=r("a"),_l=r("span"),F(Qo.$$.fragment),uh=l(),Tl=r("span"),hh=t("FunnelTokenizer"),Lc=l(),Ce=r("div"),F(Uo.$$.fragment),fh=l(),kl=r("p"),mh=t("Construct a Funnel Transformer tokenizer."),gh=l(),Qt=r("p"),oi=r("a"),_h=t("FunnelTokenizer"),Th=t(" is identical to "),si=r("a"),kh=t("BertTokenizer"),Fh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),vh=l(),Ro=r("p"),yh=t("Refer to superclass "),ri=r("a"),wh=t("BertTokenizer"),bh=t(" for usage examples and documentation concerning parameters."),$h=l(),An=r("div"),F(Ho.$$.fragment),Eh=l(),Fl=r("p"),Mh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),zh=l(),Vo=r("ul"),ai=r("li"),qh=t("single sequence: "),vl=r("code"),Ph=t("[CLS] X [SEP]"),Ch=l(),ii=r("li"),jh=t("pair of sequences: "),yl=r("code"),xh=t("[CLS] A [SEP] B [SEP]"),Lh=l(),Ut=r("div"),F(Yo.$$.fragment),Oh=l(),Ko=r("p"),Dh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),wl=r("code"),Ah=t("prepare_for_model"),Nh=t(" method."),Ih=l(),yn=r("div"),F(Go.$$.fragment),Sh=l(),bl=r("p"),Bh=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Wh=l(),F(Zo.$$.fragment),Qh=l(),et=r("p"),Uh=t("If "),$l=r("code"),Rh=t("token_ids_1"),Hh=t(" is "),El=r("code"),Vh=t("None"),Yh=t(", this method only returns the first portion of the mask (0s)."),Kh=l(),li=r("div"),F(Xo.$$.fragment),Oc=l(),nt=r("h2"),Rt=r("a"),Ml=r("span"),F(Jo.$$.fragment),Gh=l(),zl=r("span"),Zh=t("FunnelTokenizerFast"),Dc=l(),en=r("div"),F(es.$$.fragment),Xh=l(),ns=r("p"),Jh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),ql=r("em"),ef=t("tokenizers"),nf=t(" library)."),tf=l(),Ht=r("p"),di=r("a"),of=t("FunnelTokenizerFast"),sf=t(" is identical to "),ci=r("a"),rf=t("BertTokenizerFast"),af=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),lf=l(),ts=r("p"),df=t("Refer to superclass "),pi=r("a"),cf=t("BertTokenizerFast"),pf=t(" for usage examples and documentation concerning parameters."),uf=l(),wn=r("div"),F(os.$$.fragment),hf=l(),Pl=r("p"),ff=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),mf=l(),F(ss.$$.fragment),gf=l(),tt=r("p"),_f=t("If "),Cl=r("code"),Tf=t("token_ids_1"),kf=t(" is "),jl=r("code"),Ff=t("None"),vf=t(", this method only returns the first portion of the mask (0s)."),Ac=l(),ot=r("h2"),Vt=r("a"),xl=r("span"),F(rs.$$.fragment),yf=l(),Ll=r("span"),wf=t("Funnel specific outputs"),Nc=l(),st=r("div"),F(as.$$.fragment),bf=l(),is=r("p"),$f=t("Output type of "),ui=r("a"),Ef=t("FunnelForPreTraining"),Mf=t("."),Ic=l(),rt=r("div"),F(ls.$$.fragment),zf=l(),ds=r("p"),qf=t("Output type of "),hi=r("a"),Pf=t("FunnelForPreTraining"),Cf=t("."),Sc=l(),at=r("h2"),Yt=r("a"),Ol=r("span"),F(cs.$$.fragment),jf=l(),Dl=r("span"),xf=t("FunnelBaseModel"),Bc=l(),We=r("div"),F(ps.$$.fragment),Lf=l(),Al=r("p"),Of=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Df=l(),us=r("p"),Af=t("The Funnel Transformer model was proposed in "),hs=r("a"),Nf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),If=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Sf=l(),fs=r("p"),Bf=t("This model inherits from "),fi=r("a"),Wf=t("PreTrainedModel"),Qf=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Uf=l(),ms=r("p"),Rf=t("This model is also a PyTorch "),gs=r("a"),Hf=t("torch.nn.Module"),Vf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Yf=l(),nn=r("div"),F(_s.$$.fragment),Kf=l(),it=r("p"),Gf=t("The "),mi=r("a"),Zf=t("FunnelBaseModel"),Xf=t(" forward method, overrides the "),Nl=r("code"),Jf=t("__call__"),em=t(" special method."),nm=l(),F(Kt.$$.fragment),tm=l(),Il=r("p"),om=t("Example:"),sm=l(),F(Ts.$$.fragment),Wc=l(),lt=r("h2"),Gt=r("a"),Sl=r("span"),F(ks.$$.fragment),rm=l(),Bl=r("span"),am=t("FunnelModel"),Qc=l(),Qe=r("div"),F(Fs.$$.fragment),im=l(),Wl=r("p"),lm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),dm=l(),vs=r("p"),cm=t("The Funnel Transformer model was proposed in "),ys=r("a"),pm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),um=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),hm=l(),ws=r("p"),fm=t("This model inherits from "),gi=r("a"),mm=t("PreTrainedModel"),gm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_m=l(),bs=r("p"),Tm=t("This model is also a PyTorch "),$s=r("a"),km=t("torch.nn.Module"),Fm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),vm=l(),tn=r("div"),F(Es.$$.fragment),ym=l(),dt=r("p"),wm=t("The "),_i=r("a"),bm=t("FunnelModel"),$m=t(" forward method, overrides the "),Ql=r("code"),Em=t("__call__"),Mm=t(" special method."),zm=l(),F(Zt.$$.fragment),qm=l(),Ul=r("p"),Pm=t("Example:"),Cm=l(),F(Ms.$$.fragment),Uc=l(),ct=r("h2"),Xt=r("a"),Rl=r("span"),F(zs.$$.fragment),jm=l(),Hl=r("span"),xm=t("FunnelModelForPreTraining"),Rc=l(),pt=r("div"),F(qs.$$.fragment),Lm=l(),on=r("div"),F(Ps.$$.fragment),Om=l(),ut=r("p"),Dm=t("The "),Ti=r("a"),Am=t("FunnelForPreTraining"),Nm=t(" forward method, overrides the "),Vl=r("code"),Im=t("__call__"),Sm=t(" special method."),Bm=l(),F(Jt.$$.fragment),Wm=l(),Yl=r("p"),Qm=t("Examples:"),Um=l(),F(Cs.$$.fragment),Hc=l(),ht=r("h2"),eo=r("a"),Kl=r("span"),F(js.$$.fragment),Rm=l(),Gl=r("span"),Hm=t("FunnelForMaskedLM"),Vc=l(),Ue=r("div"),F(xs.$$.fragment),Vm=l(),Ls=r("p"),Ym=t("Funnel Transformer Model with a "),Zl=r("code"),Km=t("language modeling"),Gm=t(" head on top."),Zm=l(),Os=r("p"),Xm=t("The Funnel Transformer model was proposed in "),Ds=r("a"),Jm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),eg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ng=l(),As=r("p"),tg=t("This model inherits from "),ki=r("a"),og=t("PreTrainedModel"),sg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rg=l(),Ns=r("p"),ag=t("This model is also a PyTorch "),Is=r("a"),ig=t("torch.nn.Module"),lg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),dg=l(),Ke=r("div"),F(Ss.$$.fragment),cg=l(),ft=r("p"),pg=t("The "),Fi=r("a"),ug=t("FunnelForMaskedLM"),hg=t(" forward method, overrides the "),Xl=r("code"),fg=t("__call__"),mg=t(" special method."),gg=l(),F(no.$$.fragment),_g=l(),Jl=r("p"),Tg=t("Example:"),kg=l(),F(Bs.$$.fragment),Fg=l(),F(Ws.$$.fragment),Yc=l(),mt=r("h2"),to=r("a"),ed=r("span"),F(Qs.$$.fragment),vg=l(),nd=r("span"),yg=t("FunnelForSequenceClassification"),Kc=l(),Re=r("div"),F(Us.$$.fragment),wg=l(),td=r("p"),bg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),$g=l(),Rs=r("p"),Eg=t("The Funnel Transformer model was proposed in "),Hs=r("a"),Mg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),zg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),qg=l(),Vs=r("p"),Pg=t("This model inherits from "),vi=r("a"),Cg=t("PreTrainedModel"),jg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xg=l(),Ys=r("p"),Lg=t("This model is also a PyTorch "),Ks=r("a"),Og=t("torch.nn.Module"),Dg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ag=l(),xe=r("div"),F(Gs.$$.fragment),Ng=l(),gt=r("p"),Ig=t("The "),yi=r("a"),Sg=t("FunnelForSequenceClassification"),Bg=t(" forward method, overrides the "),od=r("code"),Wg=t("__call__"),Qg=t(" special method."),Ug=l(),F(oo.$$.fragment),Rg=l(),sd=r("p"),Hg=t("Example of single-label classification:"),Vg=l(),F(Zs.$$.fragment),Yg=l(),F(Xs.$$.fragment),Kg=l(),rd=r("p"),Gg=t("Example of multi-label classification:"),Zg=l(),F(Js.$$.fragment),Gc=l(),_t=r("h2"),so=r("a"),ad=r("span"),F(er.$$.fragment),Xg=l(),id=r("span"),Jg=t("FunnelForMultipleChoice"),Zc=l(),He=r("div"),F(nr.$$.fragment),e_=l(),ld=r("p"),n_=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),t_=l(),tr=r("p"),o_=t("The Funnel Transformer model was proposed in "),or=r("a"),s_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),r_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),a_=l(),sr=r("p"),i_=t("This model inherits from "),wi=r("a"),l_=t("PreTrainedModel"),d_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),c_=l(),rr=r("p"),p_=t("This model is also a PyTorch "),ar=r("a"),u_=t("torch.nn.Module"),h_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),f_=l(),sn=r("div"),F(ir.$$.fragment),m_=l(),Tt=r("p"),g_=t("The "),bi=r("a"),__=t("FunnelForMultipleChoice"),T_=t(" forward method, overrides the "),dd=r("code"),k_=t("__call__"),F_=t(" special method."),v_=l(),F(ro.$$.fragment),y_=l(),cd=r("p"),w_=t("Example:"),b_=l(),F(lr.$$.fragment),Xc=l(),kt=r("h2"),ao=r("a"),pd=r("span"),F(dr.$$.fragment),$_=l(),ud=r("span"),E_=t("FunnelForTokenClassification"),Jc=l(),Ve=r("div"),F(cr.$$.fragment),M_=l(),hd=r("p"),z_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),q_=l(),pr=r("p"),P_=t("The Funnel Transformer model was proposed in "),ur=r("a"),C_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),j_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),x_=l(),hr=r("p"),L_=t("This model inherits from "),$i=r("a"),O_=t("PreTrainedModel"),D_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),A_=l(),fr=r("p"),N_=t("This model is also a PyTorch "),mr=r("a"),I_=t("torch.nn.Module"),S_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),B_=l(),Ge=r("div"),F(gr.$$.fragment),W_=l(),Ft=r("p"),Q_=t("The "),Ei=r("a"),U_=t("FunnelForTokenClassification"),R_=t(" forward method, overrides the "),fd=r("code"),H_=t("__call__"),V_=t(" special method."),Y_=l(),F(io.$$.fragment),K_=l(),md=r("p"),G_=t("Example:"),Z_=l(),F(_r.$$.fragment),X_=l(),F(Tr.$$.fragment),ep=l(),vt=r("h2"),lo=r("a"),gd=r("span"),F(kr.$$.fragment),J_=l(),_d=r("span"),eT=t("FunnelForQuestionAnswering"),np=l(),Ye=r("div"),F(Fr.$$.fragment),nT=l(),yt=r("p"),tT=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),Td=r("code"),oT=t("span start logits"),sT=t(" and "),kd=r("code"),rT=t("span end logits"),aT=t(")."),iT=l(),vr=r("p"),lT=t("The Funnel Transformer model was proposed in "),yr=r("a"),dT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),cT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pT=l(),wr=r("p"),uT=t("This model inherits from "),Mi=r("a"),hT=t("PreTrainedModel"),fT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mT=l(),br=r("p"),gT=t("This model is also a PyTorch "),$r=r("a"),_T=t("torch.nn.Module"),TT=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kT=l(),Ze=r("div"),F(Er.$$.fragment),FT=l(),wt=r("p"),vT=t("The "),zi=r("a"),yT=t("FunnelForQuestionAnswering"),wT=t(" forward method, overrides the "),Fd=r("code"),bT=t("__call__"),$T=t(" special method."),ET=l(),F(co.$$.fragment),MT=l(),vd=r("p"),zT=t("Example:"),qT=l(),F(Mr.$$.fragment),PT=l(),F(zr.$$.fragment),tp=l(),bt=r("h2"),po=r("a"),yd=r("span"),F(qr.$$.fragment),CT=l(),wd=r("span"),jT=t("TFFunnelBaseModel"),op=l(),Le=r("div"),F(Pr.$$.fragment),xT=l(),bd=r("p"),LT=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),OT=l(),Cr=r("p"),DT=t("The Funnel Transformer model was proposed in "),jr=r("a"),AT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),NT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),IT=l(),xr=r("p"),ST=t("This model inherits from "),qi=r("a"),BT=t("TFPreTrainedModel"),WT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),QT=l(),Lr=r("p"),UT=t("This model is also a "),Or=r("a"),RT=t("tf.keras.Model"),HT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),VT=l(),F(uo.$$.fragment),YT=l(),rn=r("div"),F(Dr.$$.fragment),KT=l(),$t=r("p"),GT=t("The "),Pi=r("a"),ZT=t("TFFunnelBaseModel"),XT=t(" forward method, overrides the "),$d=r("code"),JT=t("__call__"),ek=t(" special method."),nk=l(),F(ho.$$.fragment),tk=l(),Ed=r("p"),ok=t("Example:"),sk=l(),F(Ar.$$.fragment),sp=l(),Et=r("h2"),fo=r("a"),Md=r("span"),F(Nr.$$.fragment),rk=l(),zd=r("span"),ak=t("TFFunnelModel"),rp=l(),Oe=r("div"),F(Ir.$$.fragment),ik=l(),qd=r("p"),lk=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),dk=l(),Sr=r("p"),ck=t("The Funnel Transformer model was proposed in "),Br=r("a"),pk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),uk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),hk=l(),Wr=r("p"),fk=t("This model inherits from "),Ci=r("a"),mk=t("TFPreTrainedModel"),gk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_k=l(),Qr=r("p"),Tk=t("This model is also a "),Ur=r("a"),kk=t("tf.keras.Model"),Fk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),vk=l(),F(mo.$$.fragment),yk=l(),an=r("div"),F(Rr.$$.fragment),wk=l(),Mt=r("p"),bk=t("The "),ji=r("a"),$k=t("TFFunnelModel"),Ek=t(" forward method, overrides the "),Pd=r("code"),Mk=t("__call__"),zk=t(" special method."),qk=l(),F(go.$$.fragment),Pk=l(),Cd=r("p"),Ck=t("Example:"),jk=l(),F(Hr.$$.fragment),ap=l(),zt=r("h2"),_o=r("a"),jd=r("span"),F(Vr.$$.fragment),xk=l(),xd=r("span"),Lk=t("TFFunnelModelForPreTraining"),ip=l(),De=r("div"),F(Yr.$$.fragment),Ok=l(),Ld=r("p"),Dk=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),Ak=l(),Kr=r("p"),Nk=t("The Funnel Transformer model was proposed in "),Gr=r("a"),Ik=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bk=l(),Zr=r("p"),Wk=t("This model inherits from "),xi=r("a"),Qk=t("TFPreTrainedModel"),Uk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rk=l(),Xr=r("p"),Hk=t("This model is also a "),Jr=r("a"),Vk=t("tf.keras.Model"),Yk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Kk=l(),F(To.$$.fragment),Gk=l(),ln=r("div"),F(ea.$$.fragment),Zk=l(),qt=r("p"),Xk=t("The "),Li=r("a"),Jk=t("TFFunnelForPreTraining"),eF=t(" forward method, overrides the "),Od=r("code"),nF=t("__call__"),tF=t(" special method."),oF=l(),F(ko.$$.fragment),sF=l(),Dd=r("p"),rF=t("Examples:"),aF=l(),F(na.$$.fragment),lp=l(),Pt=r("h2"),Fo=r("a"),Ad=r("span"),F(ta.$$.fragment),iF=l(),Nd=r("span"),lF=t("TFFunnelForMaskedLM"),dp=l(),Ae=r("div"),F(oa.$$.fragment),dF=l(),sa=r("p"),cF=t("Funnel Model with a "),Id=r("code"),pF=t("language modeling"),uF=t(" head on top."),hF=l(),ra=r("p"),fF=t("The Funnel Transformer model was proposed in "),aa=r("a"),mF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),gF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_F=l(),ia=r("p"),TF=t("This model inherits from "),Oi=r("a"),kF=t("TFPreTrainedModel"),FF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vF=l(),la=r("p"),yF=t("This model is also a "),da=r("a"),wF=t("tf.keras.Model"),bF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),$F=l(),F(vo.$$.fragment),EF=l(),dn=r("div"),F(ca.$$.fragment),MF=l(),Ct=r("p"),zF=t("The "),Di=r("a"),qF=t("TFFunnelForMaskedLM"),PF=t(" forward method, overrides the "),Sd=r("code"),CF=t("__call__"),jF=t(" special method."),xF=l(),F(yo.$$.fragment),LF=l(),Bd=r("p"),OF=t("Example:"),DF=l(),F(pa.$$.fragment),cp=l(),jt=r("h2"),wo=r("a"),Wd=r("span"),F(ua.$$.fragment),AF=l(),Qd=r("span"),NF=t("TFFunnelForSequenceClassification"),pp=l(),Ne=r("div"),F(ha.$$.fragment),IF=l(),Ud=r("p"),SF=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),BF=l(),fa=r("p"),WF=t("The Funnel Transformer model was proposed in "),ma=r("a"),QF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),UF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),RF=l(),ga=r("p"),HF=t("This model inherits from "),Ai=r("a"),VF=t("TFPreTrainedModel"),YF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),KF=l(),_a=r("p"),GF=t("This model is also a "),Ta=r("a"),ZF=t("tf.keras.Model"),XF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),JF=l(),F(bo.$$.fragment),ev=l(),cn=r("div"),F(ka.$$.fragment),nv=l(),xt=r("p"),tv=t("The "),Ni=r("a"),ov=t("TFFunnelForSequenceClassification"),sv=t(" forward method, overrides the "),Rd=r("code"),rv=t("__call__"),av=t(" special method."),iv=l(),F($o.$$.fragment),lv=l(),Hd=r("p"),dv=t("Example:"),cv=l(),F(Fa.$$.fragment),up=l(),Lt=r("h2"),Eo=r("a"),Vd=r("span"),F(va.$$.fragment),pv=l(),Yd=r("span"),uv=t("TFFunnelForMultipleChoice"),hp=l(),Ie=r("div"),F(ya.$$.fragment),hv=l(),Kd=r("p"),fv=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),mv=l(),wa=r("p"),gv=t("The Funnel Transformer model was proposed in "),ba=r("a"),_v=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Tv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),kv=l(),$a=r("p"),Fv=t("This model inherits from "),Ii=r("a"),vv=t("TFPreTrainedModel"),yv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wv=l(),Ea=r("p"),bv=t("This model is also a "),Ma=r("a"),$v=t("tf.keras.Model"),Ev=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Mv=l(),F(Mo.$$.fragment),zv=l(),pn=r("div"),F(za.$$.fragment),qv=l(),Ot=r("p"),Pv=t("The "),Si=r("a"),Cv=t("TFFunnelForMultipleChoice"),jv=t(" forward method, overrides the "),Gd=r("code"),xv=t("__call__"),Lv=t(" special method."),Ov=l(),F(zo.$$.fragment),Dv=l(),Zd=r("p"),Av=t("Example:"),Nv=l(),F(qa.$$.fragment),fp=l(),Dt=r("h2"),qo=r("a"),Xd=r("span"),F(Pa.$$.fragment),Iv=l(),Jd=r("span"),Sv=t("TFFunnelForTokenClassification"),mp=l(),Se=r("div"),F(Ca.$$.fragment),Bv=l(),ec=r("p"),Wv=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Qv=l(),ja=r("p"),Uv=t("The Funnel Transformer model was proposed in "),xa=r("a"),Rv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Hv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Vv=l(),La=r("p"),Yv=t("This model inherits from "),Bi=r("a"),Kv=t("TFPreTrainedModel"),Gv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zv=l(),Oa=r("p"),Xv=t("This model is also a "),Da=r("a"),Jv=t("tf.keras.Model"),ey=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ny=l(),F(Po.$$.fragment),ty=l(),un=r("div"),F(Aa.$$.fragment),oy=l(),At=r("p"),sy=t("The "),Wi=r("a"),ry=t("TFFunnelForTokenClassification"),ay=t(" forward method, overrides the "),nc=r("code"),iy=t("__call__"),ly=t(" special method."),dy=l(),F(Co.$$.fragment),cy=l(),tc=r("p"),py=t("Example:"),uy=l(),F(Na.$$.fragment),gp=l(),Nt=r("h2"),jo=r("a"),oc=r("span"),F(Ia.$$.fragment),hy=l(),sc=r("span"),fy=t("TFFunnelForQuestionAnswering"),_p=l(),Be=r("div"),F(Sa.$$.fragment),my=l(),It=r("p"),gy=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),rc=r("code"),_y=t("span start logits"),Ty=t(" and "),ac=r("code"),ky=t("span end logits"),Fy=t(")."),vy=l(),Ba=r("p"),yy=t("The Funnel Transformer model was proposed in "),Wa=r("a"),wy=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),by=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),$y=l(),Qa=r("p"),Ey=t("This model inherits from "),Qi=r("a"),My=t("TFPreTrainedModel"),zy=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qy=l(),Ua=r("p"),Py=t("This model is also a "),Ra=r("a"),Cy=t("tf.keras.Model"),jy=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),xy=l(),F(xo.$$.fragment),Ly=l(),hn=r("div"),F(Ha.$$.fragment),Oy=l(),St=r("p"),Dy=t("The "),Ui=r("a"),Ay=t("TFFunnelForQuestionAnswering"),Ny=t(" forward method, overrides the "),ic=r("code"),Iy=t("__call__"),Sy=t(" special method."),By=l(),F(Lo.$$.fragment),Wy=l(),lc=r("p"),Qy=t("Example:"),Uy=l(),F(Va.$$.fragment),this.h()},l(s){const f=L$('[data-svelte="svelte-1phssyn"]',document.head);u=a(f,"META",{name:!0,content:!0}),f.forEach(n),z=d(s),g=a(s,"H1",{class:!0});var Ya=i(g);_=a(Ya,"A",{id:!0,class:!0,href:!0});var dc=i(_);k=a(dc,"SPAN",{});var cc=i(k);v(T.$$.fragment,cc),cc.forEach(n),dc.forEach(n),m=d(Ya),M=a(Ya,"SPAN",{});var pc=i(M);ce=o(pc,"Funnel Transformer"),pc.forEach(n),Ya.forEach(n),K=d(s),q=a(s,"H2",{class:!0});var Ka=i(q);J=a(Ka,"A",{id:!0,class:!0,href:!0});var uc=i(J);A=a(uc,"SPAN",{});var hc=i(A);v(ne.$$.fragment,hc),hc.forEach(n),uc.forEach(n),pe=d(Ka),N=a(Ka,"SPAN",{});var fc=i(N);ue=o(fc,"Overview"),fc.forEach(n),Ka.forEach(n),ie=d(s),Y=a(s,"P",{});var Ga=i(Y);L=o(Ga,"The Funnel Transformer model was proposed in the paper "),te=a(Ga,"A",{href:!0,rel:!0});var mc=i(te);G=o(mc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),mc.forEach(n),P=o(Ga,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Ga.forEach(n),j=d(s),oe=a(s,"P",{});var gc=i(oe);W=o(gc,"The abstract from the paper is the following:"),gc.forEach(n),le=d(s),se=a(s,"P",{});var _c=i(se);I=a(_c,"EM",{});var Tc=i(I);he=o(Tc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),Tc.forEach(n),_c.forEach(n),de=d(s),C=a(s,"P",{});var kc=i(C);fe=o(kc,"Tips:"),kc.forEach(n),B=d(s),ee=a(s,"UL",{});var Za=i(ee);ae=a(Za,"LI",{});var Fc=i(ae);Q=o(Fc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),Fc.forEach(n),me=d(Za),S=a(Za,"LI",{});var je=i(S);O=o(je,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(je,"A",{href:!0});var vc=i(re);U=o(vc,"FunnelModel"),vc.forEach(n),ge=o(je,", "),p=a(je,"A",{href:!0});var yc=i(p);E=o(yc,"FunnelForPreTraining"),yc.forEach(n),Z=o(je,`,
`),Te=a(je,"A",{href:!0});var wc=i(Te);ye=o(wc,"FunnelForMaskedLM"),wc.forEach(n),D=o(je,", "),ke=a(je,"A",{href:!0});var bc=i(ke);we=o(bc,"FunnelForTokenClassification"),bc.forEach(n),be=o(je,` and
class:`),x=a(je,"EM",{});var $c=i(x);R=o($c,"~transformers.FunnelForQuestionAnswering"),$c.forEach(n),$e=o(je,`. The second ones should be used for
`),Fe=a(je,"A",{href:!0});var Ec=i(Fe);H=o(Ec,"FunnelBaseModel"),Ec.forEach(n),Ee=o(je,", "),ve=a(je,"A",{href:!0});var Mc=i(ve);_e=o(Mc,"FunnelForSequenceClassification"),Mc.forEach(n),Me=o(je,` and
`),Xa=a(je,"A",{href:!0});var Vy=i(Xa);Qu=o(Vy,"FunnelForMultipleChoice"),Vy.forEach(n),Uu=o(je,"."),je.forEach(n),Za.forEach(n),Pc=d(s),Dn=a(s,"P",{});var Ri=i(Dn);Ru=o(Ri,"This model was contributed by "),No=a(Ri,"A",{href:!0,rel:!0});var Yy=i(No);Hu=o(Yy,"sgugger"),Yy.forEach(n),Vu=o(Ri,". The original code can be found "),Io=a(Ri,"A",{href:!0,rel:!0});var Ky=i(Io);Yu=o(Ky,"here"),Ky.forEach(n),Ku=o(Ri,"."),Ri.forEach(n),Cc=d(s),Zn=a(s,"H2",{class:!0});var kp=i(Zn);Bt=a(kp,"A",{id:!0,class:!0,href:!0});var Gy=i(Bt);ml=a(Gy,"SPAN",{});var Zy=i(ml);v(So.$$.fragment,Zy),Zy.forEach(n),Gy.forEach(n),Gu=d(kp),gl=a(kp,"SPAN",{});var Xy=i(gl);Zu=o(Xy,"FunnelConfig"),Xy.forEach(n),kp.forEach(n),jc=d(s),Ln=a(s,"DIV",{class:!0});var Hi=i(Ln);v(Bo.$$.fragment,Hi),Xu=d(Hi),On=a(Hi,"P",{});var Oo=i(On);Ju=o(Oo,"This is the configuration class to store the configuration of a "),Ja=a(Oo,"A",{href:!0});var Jy=i(Ja);eh=o(Jy,"FunnelModel"),Jy.forEach(n),nh=o(Oo," or a "),ei=a(Oo,"A",{href:!0});var e1=i(ei);th=o(e1,"TFBertModel"),e1.forEach(n),oh=o(Oo,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Oo,"A",{href:!0,rel:!0});var n1=i(Wo);sh=o(n1,"funnel-transformer/small"),n1.forEach(n),rh=o(Oo," architecture."),Oo.forEach(n),ah=d(Hi),Xn=a(Hi,"P",{});var Vi=i(Xn);ih=o(Vi,"Configuration objects inherit from "),ni=a(Vi,"A",{href:!0});var t1=i(ni);lh=o(t1,"PretrainedConfig"),t1.forEach(n),dh=o(Vi,` and can be used to control the model outputs. Read the
documentation from `),ti=a(Vi,"A",{href:!0});var o1=i(ti);ch=o(o1,"PretrainedConfig"),o1.forEach(n),ph=o(Vi," for more information."),Vi.forEach(n),Hi.forEach(n),xc=d(s),Jn=a(s,"H2",{class:!0});var Fp=i(Jn);Wt=a(Fp,"A",{id:!0,class:!0,href:!0});var s1=i(Wt);_l=a(s1,"SPAN",{});var r1=i(_l);v(Qo.$$.fragment,r1),r1.forEach(n),s1.forEach(n),uh=d(Fp),Tl=a(Fp,"SPAN",{});var a1=i(Tl);hh=o(a1,"FunnelTokenizer"),a1.forEach(n),Fp.forEach(n),Lc=d(s),Ce=a(s,"DIV",{class:!0});var Xe=i(Ce);v(Uo.$$.fragment,Xe),fh=d(Xe),kl=a(Xe,"P",{});var i1=i(kl);mh=o(i1,"Construct a Funnel Transformer tokenizer."),i1.forEach(n),gh=d(Xe),Qt=a(Xe,"P",{});var zc=i(Qt);oi=a(zc,"A",{href:!0});var l1=i(oi);_h=o(l1,"FunnelTokenizer"),l1.forEach(n),Th=o(zc," is identical to "),si=a(zc,"A",{href:!0});var d1=i(si);kh=o(d1,"BertTokenizer"),d1.forEach(n),Fh=o(zc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),zc.forEach(n),vh=d(Xe),Ro=a(Xe,"P",{});var vp=i(Ro);yh=o(vp,"Refer to superclass "),ri=a(vp,"A",{href:!0});var c1=i(ri);wh=o(c1,"BertTokenizer"),c1.forEach(n),bh=o(vp," for usage examples and documentation concerning parameters."),vp.forEach(n),$h=d(Xe),An=a(Xe,"DIV",{class:!0});var Yi=i(An);v(Ho.$$.fragment,Yi),Eh=d(Yi),Fl=a(Yi,"P",{});var p1=i(Fl);Mh=o(p1,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),p1.forEach(n),zh=d(Yi),Vo=a(Yi,"UL",{});var yp=i(Vo);ai=a(yp,"LI",{});var Ry=i(ai);qh=o(Ry,"single sequence: "),vl=a(Ry,"CODE",{});var u1=i(vl);Ph=o(u1,"[CLS] X [SEP]"),u1.forEach(n),Ry.forEach(n),Ch=d(yp),ii=a(yp,"LI",{});var Hy=i(ii);jh=o(Hy,"pair of sequences: "),yl=a(Hy,"CODE",{});var h1=i(yl);xh=o(h1,"[CLS] A [SEP] B [SEP]"),h1.forEach(n),Hy.forEach(n),yp.forEach(n),Yi.forEach(n),Lh=d(Xe),Ut=a(Xe,"DIV",{class:!0});var wp=i(Ut);v(Yo.$$.fragment,wp),Oh=d(wp),Ko=a(wp,"P",{});var bp=i(Ko);Dh=o(bp,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),wl=a(bp,"CODE",{});var f1=i(wl);Ah=o(f1,"prepare_for_model"),f1.forEach(n),Nh=o(bp," method."),bp.forEach(n),wp.forEach(n),Ih=d(Xe),yn=a(Xe,"DIV",{class:!0});var Do=i(yn);v(Go.$$.fragment,Do),Sh=d(Do),bl=a(Do,"P",{});var m1=i(bl);Bh=o(m1,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),m1.forEach(n),Wh=d(Do),v(Zo.$$.fragment,Do),Qh=d(Do),et=a(Do,"P",{});var Ki=i(et);Uh=o(Ki,"If "),$l=a(Ki,"CODE",{});var g1=i($l);Rh=o(g1,"token_ids_1"),g1.forEach(n),Hh=o(Ki," is "),El=a(Ki,"CODE",{});var _1=i(El);Vh=o(_1,"None"),_1.forEach(n),Yh=o(Ki,", this method only returns the first portion of the mask (0s)."),Ki.forEach(n),Do.forEach(n),Kh=d(Xe),li=a(Xe,"DIV",{class:!0});var T1=i(li);v(Xo.$$.fragment,T1),T1.forEach(n),Xe.forEach(n),Oc=d(s),nt=a(s,"H2",{class:!0});var $p=i(nt);Rt=a($p,"A",{id:!0,class:!0,href:!0});var k1=i(Rt);Ml=a(k1,"SPAN",{});var F1=i(Ml);v(Jo.$$.fragment,F1),F1.forEach(n),k1.forEach(n),Gh=d($p),zl=a($p,"SPAN",{});var v1=i(zl);Zh=o(v1,"FunnelTokenizerFast"),v1.forEach(n),$p.forEach(n),Dc=d(s),en=a(s,"DIV",{class:!0});var Nn=i(en);v(es.$$.fragment,Nn),Xh=d(Nn),ns=a(Nn,"P",{});var Ep=i(ns);Jh=o(Ep,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),ql=a(Ep,"EM",{});var y1=i(ql);ef=o(y1,"tokenizers"),y1.forEach(n),nf=o(Ep," library)."),Ep.forEach(n),tf=d(Nn),Ht=a(Nn,"P",{});var qc=i(Ht);di=a(qc,"A",{href:!0});var w1=i(di);of=o(w1,"FunnelTokenizerFast"),w1.forEach(n),sf=o(qc," is identical to "),ci=a(qc,"A",{href:!0});var b1=i(ci);rf=o(b1,"BertTokenizerFast"),b1.forEach(n),af=o(qc,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),qc.forEach(n),lf=d(Nn),ts=a(Nn,"P",{});var Mp=i(ts);df=o(Mp,"Refer to superclass "),pi=a(Mp,"A",{href:!0});var $1=i(pi);cf=o($1,"BertTokenizerFast"),$1.forEach(n),pf=o(Mp," for usage examples and documentation concerning parameters."),Mp.forEach(n),uf=d(Nn),wn=a(Nn,"DIV",{class:!0});var Ao=i(wn);v(os.$$.fragment,Ao),hf=d(Ao),Pl=a(Ao,"P",{});var E1=i(Pl);ff=o(E1,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),E1.forEach(n),mf=d(Ao),v(ss.$$.fragment,Ao),gf=d(Ao),tt=a(Ao,"P",{});var Gi=i(tt);_f=o(Gi,"If "),Cl=a(Gi,"CODE",{});var M1=i(Cl);Tf=o(M1,"token_ids_1"),M1.forEach(n),kf=o(Gi," is "),jl=a(Gi,"CODE",{});var z1=i(jl);Ff=o(z1,"None"),z1.forEach(n),vf=o(Gi,", this method only returns the first portion of the mask (0s)."),Gi.forEach(n),Ao.forEach(n),Nn.forEach(n),Ac=d(s),ot=a(s,"H2",{class:!0});var zp=i(ot);Vt=a(zp,"A",{id:!0,class:!0,href:!0});var q1=i(Vt);xl=a(q1,"SPAN",{});var P1=i(xl);v(rs.$$.fragment,P1),P1.forEach(n),q1.forEach(n),yf=d(zp),Ll=a(zp,"SPAN",{});var C1=i(Ll);wf=o(C1,"Funnel specific outputs"),C1.forEach(n),zp.forEach(n),Nc=d(s),st=a(s,"DIV",{class:!0});var qp=i(st);v(as.$$.fragment,qp),bf=d(qp),is=a(qp,"P",{});var Pp=i(is);$f=o(Pp,"Output type of "),ui=a(Pp,"A",{href:!0});var j1=i(ui);Ef=o(j1,"FunnelForPreTraining"),j1.forEach(n),Mf=o(Pp,"."),Pp.forEach(n),qp.forEach(n),Ic=d(s),rt=a(s,"DIV",{class:!0});var Cp=i(rt);v(ls.$$.fragment,Cp),zf=d(Cp),ds=a(Cp,"P",{});var jp=i(ds);qf=o(jp,"Output type of "),hi=a(jp,"A",{href:!0});var x1=i(hi);Pf=o(x1,"FunnelForPreTraining"),x1.forEach(n),Cf=o(jp,"."),jp.forEach(n),Cp.forEach(n),Sc=d(s),at=a(s,"H2",{class:!0});var xp=i(at);Yt=a(xp,"A",{id:!0,class:!0,href:!0});var L1=i(Yt);Ol=a(L1,"SPAN",{});var O1=i(Ol);v(cs.$$.fragment,O1),O1.forEach(n),L1.forEach(n),jf=d(xp),Dl=a(xp,"SPAN",{});var D1=i(Dl);xf=o(D1,"FunnelBaseModel"),D1.forEach(n),xp.forEach(n),Bc=d(s),We=a(s,"DIV",{class:!0});var bn=i(We);v(ps.$$.fragment,bn),Lf=d(bn),Al=a(bn,"P",{});var A1=i(Al);Of=o(A1,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),A1.forEach(n),Df=d(bn),us=a(bn,"P",{});var Lp=i(us);Af=o(Lp,"The Funnel Transformer model was proposed in "),hs=a(Lp,"A",{href:!0,rel:!0});var N1=i(hs);Nf=o(N1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),N1.forEach(n),If=o(Lp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Lp.forEach(n),Sf=d(bn),fs=a(bn,"P",{});var Op=i(fs);Bf=o(Op,"This model inherits from "),fi=a(Op,"A",{href:!0});var I1=i(fi);Wf=o(I1,"PreTrainedModel"),I1.forEach(n),Qf=o(Op,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Op.forEach(n),Uf=d(bn),ms=a(bn,"P",{});var Dp=i(ms);Rf=o(Dp,"This model is also a PyTorch "),gs=a(Dp,"A",{href:!0,rel:!0});var S1=i(gs);Hf=o(S1,"torch.nn.Module"),S1.forEach(n),Vf=o(Dp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Dp.forEach(n),Yf=d(bn),nn=a(bn,"DIV",{class:!0});var In=i(nn);v(_s.$$.fragment,In),Kf=d(In),it=a(In,"P",{});var Zi=i(it);Gf=o(Zi,"The "),mi=a(Zi,"A",{href:!0});var B1=i(mi);Zf=o(B1,"FunnelBaseModel"),B1.forEach(n),Xf=o(Zi," forward method, overrides the "),Nl=a(Zi,"CODE",{});var W1=i(Nl);Jf=o(W1,"__call__"),W1.forEach(n),em=o(Zi," special method."),Zi.forEach(n),nm=d(In),v(Kt.$$.fragment,In),tm=d(In),Il=a(In,"P",{});var Q1=i(Il);om=o(Q1,"Example:"),Q1.forEach(n),sm=d(In),v(Ts.$$.fragment,In),In.forEach(n),bn.forEach(n),Wc=d(s),lt=a(s,"H2",{class:!0});var Ap=i(lt);Gt=a(Ap,"A",{id:!0,class:!0,href:!0});var U1=i(Gt);Sl=a(U1,"SPAN",{});var R1=i(Sl);v(ks.$$.fragment,R1),R1.forEach(n),U1.forEach(n),rm=d(Ap),Bl=a(Ap,"SPAN",{});var H1=i(Bl);am=o(H1,"FunnelModel"),H1.forEach(n),Ap.forEach(n),Qc=d(s),Qe=a(s,"DIV",{class:!0});var $n=i(Qe);v(Fs.$$.fragment,$n),im=d($n),Wl=a($n,"P",{});var V1=i(Wl);lm=o(V1,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),V1.forEach(n),dm=d($n),vs=a($n,"P",{});var Np=i(vs);cm=o(Np,"The Funnel Transformer model was proposed in "),ys=a(Np,"A",{href:!0,rel:!0});var Y1=i(ys);pm=o(Y1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Y1.forEach(n),um=o(Np," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Np.forEach(n),hm=d($n),ws=a($n,"P",{});var Ip=i(ws);fm=o(Ip,"This model inherits from "),gi=a(Ip,"A",{href:!0});var K1=i(gi);mm=o(K1,"PreTrainedModel"),K1.forEach(n),gm=o(Ip,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ip.forEach(n),_m=d($n),bs=a($n,"P",{});var Sp=i(bs);Tm=o(Sp,"This model is also a PyTorch "),$s=a(Sp,"A",{href:!0,rel:!0});var G1=i($s);km=o(G1,"torch.nn.Module"),G1.forEach(n),Fm=o(Sp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Sp.forEach(n),vm=d($n),tn=a($n,"DIV",{class:!0});var Sn=i(tn);v(Es.$$.fragment,Sn),ym=d(Sn),dt=a(Sn,"P",{});var Xi=i(dt);wm=o(Xi,"The "),_i=a(Xi,"A",{href:!0});var Z1=i(_i);bm=o(Z1,"FunnelModel"),Z1.forEach(n),$m=o(Xi," forward method, overrides the "),Ql=a(Xi,"CODE",{});var X1=i(Ql);Em=o(X1,"__call__"),X1.forEach(n),Mm=o(Xi," special method."),Xi.forEach(n),zm=d(Sn),v(Zt.$$.fragment,Sn),qm=d(Sn),Ul=a(Sn,"P",{});var J1=i(Ul);Pm=o(J1,"Example:"),J1.forEach(n),Cm=d(Sn),v(Ms.$$.fragment,Sn),Sn.forEach(n),$n.forEach(n),Uc=d(s),ct=a(s,"H2",{class:!0});var Bp=i(ct);Xt=a(Bp,"A",{id:!0,class:!0,href:!0});var ew=i(Xt);Rl=a(ew,"SPAN",{});var nw=i(Rl);v(zs.$$.fragment,nw),nw.forEach(n),ew.forEach(n),jm=d(Bp),Hl=a(Bp,"SPAN",{});var tw=i(Hl);xm=o(tw,"FunnelModelForPreTraining"),tw.forEach(n),Bp.forEach(n),Rc=d(s),pt=a(s,"DIV",{class:!0});var Wp=i(pt);v(qs.$$.fragment,Wp),Lm=d(Wp),on=a(Wp,"DIV",{class:!0});var Bn=i(on);v(Ps.$$.fragment,Bn),Om=d(Bn),ut=a(Bn,"P",{});var Ji=i(ut);Dm=o(Ji,"The "),Ti=a(Ji,"A",{href:!0});var ow=i(Ti);Am=o(ow,"FunnelForPreTraining"),ow.forEach(n),Nm=o(Ji," forward method, overrides the "),Vl=a(Ji,"CODE",{});var sw=i(Vl);Im=o(sw,"__call__"),sw.forEach(n),Sm=o(Ji," special method."),Ji.forEach(n),Bm=d(Bn),v(Jt.$$.fragment,Bn),Wm=d(Bn),Yl=a(Bn,"P",{});var rw=i(Yl);Qm=o(rw,"Examples:"),rw.forEach(n),Um=d(Bn),v(Cs.$$.fragment,Bn),Bn.forEach(n),Wp.forEach(n),Hc=d(s),ht=a(s,"H2",{class:!0});var Qp=i(ht);eo=a(Qp,"A",{id:!0,class:!0,href:!0});var aw=i(eo);Kl=a(aw,"SPAN",{});var iw=i(Kl);v(js.$$.fragment,iw),iw.forEach(n),aw.forEach(n),Rm=d(Qp),Gl=a(Qp,"SPAN",{});var lw=i(Gl);Hm=o(lw,"FunnelForMaskedLM"),lw.forEach(n),Qp.forEach(n),Vc=d(s),Ue=a(s,"DIV",{class:!0});var En=i(Ue);v(xs.$$.fragment,En),Vm=d(En),Ls=a(En,"P",{});var Up=i(Ls);Ym=o(Up,"Funnel Transformer Model with a "),Zl=a(Up,"CODE",{});var dw=i(Zl);Km=o(dw,"language modeling"),dw.forEach(n),Gm=o(Up," head on top."),Up.forEach(n),Zm=d(En),Os=a(En,"P",{});var Rp=i(Os);Xm=o(Rp,"The Funnel Transformer model was proposed in "),Ds=a(Rp,"A",{href:!0,rel:!0});var cw=i(Ds);Jm=o(cw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),cw.forEach(n),eg=o(Rp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Rp.forEach(n),ng=d(En),As=a(En,"P",{});var Hp=i(As);tg=o(Hp,"This model inherits from "),ki=a(Hp,"A",{href:!0});var pw=i(ki);og=o(pw,"PreTrainedModel"),pw.forEach(n),sg=o(Hp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hp.forEach(n),rg=d(En),Ns=a(En,"P",{});var Vp=i(Ns);ag=o(Vp,"This model is also a PyTorch "),Is=a(Vp,"A",{href:!0,rel:!0});var uw=i(Is);ig=o(uw,"torch.nn.Module"),uw.forEach(n),lg=o(Vp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vp.forEach(n),dg=d(En),Ke=a(En,"DIV",{class:!0});var Mn=i(Ke);v(Ss.$$.fragment,Mn),cg=d(Mn),ft=a(Mn,"P",{});var el=i(ft);pg=o(el,"The "),Fi=a(el,"A",{href:!0});var hw=i(Fi);ug=o(hw,"FunnelForMaskedLM"),hw.forEach(n),hg=o(el," forward method, overrides the "),Xl=a(el,"CODE",{});var fw=i(Xl);fg=o(fw,"__call__"),fw.forEach(n),mg=o(el," special method."),el.forEach(n),gg=d(Mn),v(no.$$.fragment,Mn),_g=d(Mn),Jl=a(Mn,"P",{});var mw=i(Jl);Tg=o(mw,"Example:"),mw.forEach(n),kg=d(Mn),v(Bs.$$.fragment,Mn),Fg=d(Mn),v(Ws.$$.fragment,Mn),Mn.forEach(n),En.forEach(n),Yc=d(s),mt=a(s,"H2",{class:!0});var Yp=i(mt);to=a(Yp,"A",{id:!0,class:!0,href:!0});var gw=i(to);ed=a(gw,"SPAN",{});var _w=i(ed);v(Qs.$$.fragment,_w),_w.forEach(n),gw.forEach(n),vg=d(Yp),nd=a(Yp,"SPAN",{});var Tw=i(nd);yg=o(Tw,"FunnelForSequenceClassification"),Tw.forEach(n),Yp.forEach(n),Kc=d(s),Re=a(s,"DIV",{class:!0});var zn=i(Re);v(Us.$$.fragment,zn),wg=d(zn),td=a(zn,"P",{});var kw=i(td);bg=o(kw,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),kw.forEach(n),$g=d(zn),Rs=a(zn,"P",{});var Kp=i(Rs);Eg=o(Kp,"The Funnel Transformer model was proposed in "),Hs=a(Kp,"A",{href:!0,rel:!0});var Fw=i(Hs);Mg=o(Fw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fw.forEach(n),zg=o(Kp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Kp.forEach(n),qg=d(zn),Vs=a(zn,"P",{});var Gp=i(Vs);Pg=o(Gp,"This model inherits from "),vi=a(Gp,"A",{href:!0});var vw=i(vi);Cg=o(vw,"PreTrainedModel"),vw.forEach(n),jg=o(Gp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Gp.forEach(n),xg=d(zn),Ys=a(zn,"P",{});var Zp=i(Ys);Lg=o(Zp,"This model is also a PyTorch "),Ks=a(Zp,"A",{href:!0,rel:!0});var yw=i(Ks);Og=o(yw,"torch.nn.Module"),yw.forEach(n),Dg=o(Zp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zp.forEach(n),Ag=d(zn),xe=a(zn,"DIV",{class:!0});var Je=i(xe);v(Gs.$$.fragment,Je),Ng=d(Je),gt=a(Je,"P",{});var nl=i(gt);Ig=o(nl,"The "),yi=a(nl,"A",{href:!0});var ww=i(yi);Sg=o(ww,"FunnelForSequenceClassification"),ww.forEach(n),Bg=o(nl," forward method, overrides the "),od=a(nl,"CODE",{});var bw=i(od);Wg=o(bw,"__call__"),bw.forEach(n),Qg=o(nl," special method."),nl.forEach(n),Ug=d(Je),v(oo.$$.fragment,Je),Rg=d(Je),sd=a(Je,"P",{});var $w=i(sd);Hg=o($w,"Example of single-label classification:"),$w.forEach(n),Vg=d(Je),v(Zs.$$.fragment,Je),Yg=d(Je),v(Xs.$$.fragment,Je),Kg=d(Je),rd=a(Je,"P",{});var Ew=i(rd);Gg=o(Ew,"Example of multi-label classification:"),Ew.forEach(n),Zg=d(Je),v(Js.$$.fragment,Je),Je.forEach(n),zn.forEach(n),Gc=d(s),_t=a(s,"H2",{class:!0});var Xp=i(_t);so=a(Xp,"A",{id:!0,class:!0,href:!0});var Mw=i(so);ad=a(Mw,"SPAN",{});var zw=i(ad);v(er.$$.fragment,zw),zw.forEach(n),Mw.forEach(n),Xg=d(Xp),id=a(Xp,"SPAN",{});var qw=i(id);Jg=o(qw,"FunnelForMultipleChoice"),qw.forEach(n),Xp.forEach(n),Zc=d(s),He=a(s,"DIV",{class:!0});var qn=i(He);v(nr.$$.fragment,qn),e_=d(qn),ld=a(qn,"P",{});var Pw=i(ld);n_=o(Pw,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Pw.forEach(n),t_=d(qn),tr=a(qn,"P",{});var Jp=i(tr);o_=o(Jp,"The Funnel Transformer model was proposed in "),or=a(Jp,"A",{href:!0,rel:!0});var Cw=i(or);s_=o(Cw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Cw.forEach(n),r_=o(Jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Jp.forEach(n),a_=d(qn),sr=a(qn,"P",{});var eu=i(sr);i_=o(eu,"This model inherits from "),wi=a(eu,"A",{href:!0});var jw=i(wi);l_=o(jw,"PreTrainedModel"),jw.forEach(n),d_=o(eu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eu.forEach(n),c_=d(qn),rr=a(qn,"P",{});var nu=i(rr);p_=o(nu,"This model is also a PyTorch "),ar=a(nu,"A",{href:!0,rel:!0});var xw=i(ar);u_=o(xw,"torch.nn.Module"),xw.forEach(n),h_=o(nu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),nu.forEach(n),f_=d(qn),sn=a(qn,"DIV",{class:!0});var Wn=i(sn);v(ir.$$.fragment,Wn),m_=d(Wn),Tt=a(Wn,"P",{});var tl=i(Tt);g_=o(tl,"The "),bi=a(tl,"A",{href:!0});var Lw=i(bi);__=o(Lw,"FunnelForMultipleChoice"),Lw.forEach(n),T_=o(tl," forward method, overrides the "),dd=a(tl,"CODE",{});var Ow=i(dd);k_=o(Ow,"__call__"),Ow.forEach(n),F_=o(tl," special method."),tl.forEach(n),v_=d(Wn),v(ro.$$.fragment,Wn),y_=d(Wn),cd=a(Wn,"P",{});var Dw=i(cd);w_=o(Dw,"Example:"),Dw.forEach(n),b_=d(Wn),v(lr.$$.fragment,Wn),Wn.forEach(n),qn.forEach(n),Xc=d(s),kt=a(s,"H2",{class:!0});var tu=i(kt);ao=a(tu,"A",{id:!0,class:!0,href:!0});var Aw=i(ao);pd=a(Aw,"SPAN",{});var Nw=i(pd);v(dr.$$.fragment,Nw),Nw.forEach(n),Aw.forEach(n),$_=d(tu),ud=a(tu,"SPAN",{});var Iw=i(ud);E_=o(Iw,"FunnelForTokenClassification"),Iw.forEach(n),tu.forEach(n),Jc=d(s),Ve=a(s,"DIV",{class:!0});var Pn=i(Ve);v(cr.$$.fragment,Pn),M_=d(Pn),hd=a(Pn,"P",{});var Sw=i(hd);z_=o(Sw,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),Sw.forEach(n),q_=d(Pn),pr=a(Pn,"P",{});var ou=i(pr);P_=o(ou,"The Funnel Transformer model was proposed in "),ur=a(ou,"A",{href:!0,rel:!0});var Bw=i(ur);C_=o(Bw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Bw.forEach(n),j_=o(ou," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ou.forEach(n),x_=d(Pn),hr=a(Pn,"P",{});var su=i(hr);L_=o(su,"This model inherits from "),$i=a(su,"A",{href:!0});var Ww=i($i);O_=o(Ww,"PreTrainedModel"),Ww.forEach(n),D_=o(su,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),su.forEach(n),A_=d(Pn),fr=a(Pn,"P",{});var ru=i(fr);N_=o(ru,"This model is also a PyTorch "),mr=a(ru,"A",{href:!0,rel:!0});var Qw=i(mr);I_=o(Qw,"torch.nn.Module"),Qw.forEach(n),S_=o(ru,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ru.forEach(n),B_=d(Pn),Ge=a(Pn,"DIV",{class:!0});var Cn=i(Ge);v(gr.$$.fragment,Cn),W_=d(Cn),Ft=a(Cn,"P",{});var ol=i(Ft);Q_=o(ol,"The "),Ei=a(ol,"A",{href:!0});var Uw=i(Ei);U_=o(Uw,"FunnelForTokenClassification"),Uw.forEach(n),R_=o(ol," forward method, overrides the "),fd=a(ol,"CODE",{});var Rw=i(fd);H_=o(Rw,"__call__"),Rw.forEach(n),V_=o(ol," special method."),ol.forEach(n),Y_=d(Cn),v(io.$$.fragment,Cn),K_=d(Cn),md=a(Cn,"P",{});var Hw=i(md);G_=o(Hw,"Example:"),Hw.forEach(n),Z_=d(Cn),v(_r.$$.fragment,Cn),X_=d(Cn),v(Tr.$$.fragment,Cn),Cn.forEach(n),Pn.forEach(n),ep=d(s),vt=a(s,"H2",{class:!0});var au=i(vt);lo=a(au,"A",{id:!0,class:!0,href:!0});var Vw=i(lo);gd=a(Vw,"SPAN",{});var Yw=i(gd);v(kr.$$.fragment,Yw),Yw.forEach(n),Vw.forEach(n),J_=d(au),_d=a(au,"SPAN",{});var Kw=i(_d);eT=o(Kw,"FunnelForQuestionAnswering"),Kw.forEach(n),au.forEach(n),np=d(s),Ye=a(s,"DIV",{class:!0});var jn=i(Ye);v(Fr.$$.fragment,jn),nT=d(jn),yt=a(jn,"P",{});var sl=i(yt);tT=o(sl,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),Td=a(sl,"CODE",{});var Gw=i(Td);oT=o(Gw,"span start logits"),Gw.forEach(n),sT=o(sl," and "),kd=a(sl,"CODE",{});var Zw=i(kd);rT=o(Zw,"span end logits"),Zw.forEach(n),aT=o(sl,")."),sl.forEach(n),iT=d(jn),vr=a(jn,"P",{});var iu=i(vr);lT=o(iu,"The Funnel Transformer model was proposed in "),yr=a(iu,"A",{href:!0,rel:!0});var Xw=i(yr);dT=o(Xw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Xw.forEach(n),cT=o(iu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),iu.forEach(n),pT=d(jn),wr=a(jn,"P",{});var lu=i(wr);uT=o(lu,"This model inherits from "),Mi=a(lu,"A",{href:!0});var Jw=i(Mi);hT=o(Jw,"PreTrainedModel"),Jw.forEach(n),fT=o(lu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lu.forEach(n),mT=d(jn),br=a(jn,"P",{});var du=i(br);gT=o(du,"This model is also a PyTorch "),$r=a(du,"A",{href:!0,rel:!0});var eb=i($r);_T=o(eb,"torch.nn.Module"),eb.forEach(n),TT=o(du,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),du.forEach(n),kT=d(jn),Ze=a(jn,"DIV",{class:!0});var xn=i(Ze);v(Er.$$.fragment,xn),FT=d(xn),wt=a(xn,"P",{});var rl=i(wt);vT=o(rl,"The "),zi=a(rl,"A",{href:!0});var nb=i(zi);yT=o(nb,"FunnelForQuestionAnswering"),nb.forEach(n),wT=o(rl," forward method, overrides the "),Fd=a(rl,"CODE",{});var tb=i(Fd);bT=o(tb,"__call__"),tb.forEach(n),$T=o(rl," special method."),rl.forEach(n),ET=d(xn),v(co.$$.fragment,xn),MT=d(xn),vd=a(xn,"P",{});var ob=i(vd);zT=o(ob,"Example:"),ob.forEach(n),qT=d(xn),v(Mr.$$.fragment,xn),PT=d(xn),v(zr.$$.fragment,xn),xn.forEach(n),jn.forEach(n),tp=d(s),bt=a(s,"H2",{class:!0});var cu=i(bt);po=a(cu,"A",{id:!0,class:!0,href:!0});var sb=i(po);yd=a(sb,"SPAN",{});var rb=i(yd);v(qr.$$.fragment,rb),rb.forEach(n),sb.forEach(n),CT=d(cu),wd=a(cu,"SPAN",{});var ab=i(wd);jT=o(ab,"TFFunnelBaseModel"),ab.forEach(n),cu.forEach(n),op=d(s),Le=a(s,"DIV",{class:!0});var fn=i(Le);v(Pr.$$.fragment,fn),xT=d(fn),bd=a(fn,"P",{});var ib=i(bd);LT=o(ib,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),ib.forEach(n),OT=d(fn),Cr=a(fn,"P",{});var pu=i(Cr);DT=o(pu,"The Funnel Transformer model was proposed in "),jr=a(pu,"A",{href:!0,rel:!0});var lb=i(jr);AT=o(lb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lb.forEach(n),NT=o(pu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pu.forEach(n),IT=d(fn),xr=a(fn,"P",{});var uu=i(xr);ST=o(uu,"This model inherits from "),qi=a(uu,"A",{href:!0});var db=i(qi);BT=o(db,"TFPreTrainedModel"),db.forEach(n),WT=o(uu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),uu.forEach(n),QT=d(fn),Lr=a(fn,"P",{});var hu=i(Lr);UT=o(hu,"This model is also a "),Or=a(hu,"A",{href:!0,rel:!0});var cb=i(Or);RT=o(cb,"tf.keras.Model"),cb.forEach(n),HT=o(hu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hu.forEach(n),VT=d(fn),v(uo.$$.fragment,fn),YT=d(fn),rn=a(fn,"DIV",{class:!0});var Qn=i(rn);v(Dr.$$.fragment,Qn),KT=d(Qn),$t=a(Qn,"P",{});var al=i($t);GT=o(al,"The "),Pi=a(al,"A",{href:!0});var pb=i(Pi);ZT=o(pb,"TFFunnelBaseModel"),pb.forEach(n),XT=o(al," forward method, overrides the "),$d=a(al,"CODE",{});var ub=i($d);JT=o(ub,"__call__"),ub.forEach(n),ek=o(al," special method."),al.forEach(n),nk=d(Qn),v(ho.$$.fragment,Qn),tk=d(Qn),Ed=a(Qn,"P",{});var hb=i(Ed);ok=o(hb,"Example:"),hb.forEach(n),sk=d(Qn),v(Ar.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),sp=d(s),Et=a(s,"H2",{class:!0});var fu=i(Et);fo=a(fu,"A",{id:!0,class:!0,href:!0});var fb=i(fo);Md=a(fb,"SPAN",{});var mb=i(Md);v(Nr.$$.fragment,mb),mb.forEach(n),fb.forEach(n),rk=d(fu),zd=a(fu,"SPAN",{});var gb=i(zd);ak=o(gb,"TFFunnelModel"),gb.forEach(n),fu.forEach(n),rp=d(s),Oe=a(s,"DIV",{class:!0});var mn=i(Oe);v(Ir.$$.fragment,mn),ik=d(mn),qd=a(mn,"P",{});var _b=i(qd);lk=o(_b,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),_b.forEach(n),dk=d(mn),Sr=a(mn,"P",{});var mu=i(Sr);ck=o(mu,"The Funnel Transformer model was proposed in "),Br=a(mu,"A",{href:!0,rel:!0});var Tb=i(Br);pk=o(Tb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Tb.forEach(n),uk=o(mu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mu.forEach(n),hk=d(mn),Wr=a(mn,"P",{});var gu=i(Wr);fk=o(gu,"This model inherits from "),Ci=a(gu,"A",{href:!0});var kb=i(Ci);mk=o(kb,"TFPreTrainedModel"),kb.forEach(n),gk=o(gu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gu.forEach(n),_k=d(mn),Qr=a(mn,"P",{});var _u=i(Qr);Tk=o(_u,"This model is also a "),Ur=a(_u,"A",{href:!0,rel:!0});var Fb=i(Ur);kk=o(Fb,"tf.keras.Model"),Fb.forEach(n),Fk=o(_u,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_u.forEach(n),vk=d(mn),v(mo.$$.fragment,mn),yk=d(mn),an=a(mn,"DIV",{class:!0});var Un=i(an);v(Rr.$$.fragment,Un),wk=d(Un),Mt=a(Un,"P",{});var il=i(Mt);bk=o(il,"The "),ji=a(il,"A",{href:!0});var vb=i(ji);$k=o(vb,"TFFunnelModel"),vb.forEach(n),Ek=o(il," forward method, overrides the "),Pd=a(il,"CODE",{});var yb=i(Pd);Mk=o(yb,"__call__"),yb.forEach(n),zk=o(il," special method."),il.forEach(n),qk=d(Un),v(go.$$.fragment,Un),Pk=d(Un),Cd=a(Un,"P",{});var wb=i(Cd);Ck=o(wb,"Example:"),wb.forEach(n),jk=d(Un),v(Hr.$$.fragment,Un),Un.forEach(n),mn.forEach(n),ap=d(s),zt=a(s,"H2",{class:!0});var Tu=i(zt);_o=a(Tu,"A",{id:!0,class:!0,href:!0});var bb=i(_o);jd=a(bb,"SPAN",{});var $b=i(jd);v(Vr.$$.fragment,$b),$b.forEach(n),bb.forEach(n),xk=d(Tu),xd=a(Tu,"SPAN",{});var Eb=i(xd);Lk=o(Eb,"TFFunnelModelForPreTraining"),Eb.forEach(n),Tu.forEach(n),ip=d(s),De=a(s,"DIV",{class:!0});var gn=i(De);v(Yr.$$.fragment,gn),Ok=d(gn),Ld=a(gn,"P",{});var Mb=i(Ld);Dk=o(Mb,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),Mb.forEach(n),Ak=d(gn),Kr=a(gn,"P",{});var ku=i(Kr);Nk=o(ku,"The Funnel Transformer model was proposed in "),Gr=a(ku,"A",{href:!0,rel:!0});var zb=i(Gr);Ik=o(zb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),zb.forEach(n),Sk=o(ku," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ku.forEach(n),Bk=d(gn),Zr=a(gn,"P",{});var Fu=i(Zr);Wk=o(Fu,"This model inherits from "),xi=a(Fu,"A",{href:!0});var qb=i(xi);Qk=o(qb,"TFPreTrainedModel"),qb.forEach(n),Uk=o(Fu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Fu.forEach(n),Rk=d(gn),Xr=a(gn,"P",{});var vu=i(Xr);Hk=o(vu,"This model is also a "),Jr=a(vu,"A",{href:!0,rel:!0});var Pb=i(Jr);Vk=o(Pb,"tf.keras.Model"),Pb.forEach(n),Yk=o(vu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),vu.forEach(n),Kk=d(gn),v(To.$$.fragment,gn),Gk=d(gn),ln=a(gn,"DIV",{class:!0});var Rn=i(ln);v(ea.$$.fragment,Rn),Zk=d(Rn),qt=a(Rn,"P",{});var ll=i(qt);Xk=o(ll,"The "),Li=a(ll,"A",{href:!0});var Cb=i(Li);Jk=o(Cb,"TFFunnelForPreTraining"),Cb.forEach(n),eF=o(ll," forward method, overrides the "),Od=a(ll,"CODE",{});var jb=i(Od);nF=o(jb,"__call__"),jb.forEach(n),tF=o(ll," special method."),ll.forEach(n),oF=d(Rn),v(ko.$$.fragment,Rn),sF=d(Rn),Dd=a(Rn,"P",{});var xb=i(Dd);rF=o(xb,"Examples:"),xb.forEach(n),aF=d(Rn),v(na.$$.fragment,Rn),Rn.forEach(n),gn.forEach(n),lp=d(s),Pt=a(s,"H2",{class:!0});var yu=i(Pt);Fo=a(yu,"A",{id:!0,class:!0,href:!0});var Lb=i(Fo);Ad=a(Lb,"SPAN",{});var Ob=i(Ad);v(ta.$$.fragment,Ob),Ob.forEach(n),Lb.forEach(n),iF=d(yu),Nd=a(yu,"SPAN",{});var Db=i(Nd);lF=o(Db,"TFFunnelForMaskedLM"),Db.forEach(n),yu.forEach(n),dp=d(s),Ae=a(s,"DIV",{class:!0});var _n=i(Ae);v(oa.$$.fragment,_n),dF=d(_n),sa=a(_n,"P",{});var wu=i(sa);cF=o(wu,"Funnel Model with a "),Id=a(wu,"CODE",{});var Ab=i(Id);pF=o(Ab,"language modeling"),Ab.forEach(n),uF=o(wu," head on top."),wu.forEach(n),hF=d(_n),ra=a(_n,"P",{});var bu=i(ra);fF=o(bu,"The Funnel Transformer model was proposed in "),aa=a(bu,"A",{href:!0,rel:!0});var Nb=i(aa);mF=o(Nb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Nb.forEach(n),gF=o(bu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),bu.forEach(n),_F=d(_n),ia=a(_n,"P",{});var $u=i(ia);TF=o($u,"This model inherits from "),Oi=a($u,"A",{href:!0});var Ib=i(Oi);kF=o(Ib,"TFPreTrainedModel"),Ib.forEach(n),FF=o($u,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$u.forEach(n),vF=d(_n),la=a(_n,"P",{});var Eu=i(la);yF=o(Eu,"This model is also a "),da=a(Eu,"A",{href:!0,rel:!0});var Sb=i(da);wF=o(Sb,"tf.keras.Model"),Sb.forEach(n),bF=o(Eu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Eu.forEach(n),$F=d(_n),v(vo.$$.fragment,_n),EF=d(_n),dn=a(_n,"DIV",{class:!0});var Hn=i(dn);v(ca.$$.fragment,Hn),MF=d(Hn),Ct=a(Hn,"P",{});var dl=i(Ct);zF=o(dl,"The "),Di=a(dl,"A",{href:!0});var Bb=i(Di);qF=o(Bb,"TFFunnelForMaskedLM"),Bb.forEach(n),PF=o(dl," forward method, overrides the "),Sd=a(dl,"CODE",{});var Wb=i(Sd);CF=o(Wb,"__call__"),Wb.forEach(n),jF=o(dl," special method."),dl.forEach(n),xF=d(Hn),v(yo.$$.fragment,Hn),LF=d(Hn),Bd=a(Hn,"P",{});var Qb=i(Bd);OF=o(Qb,"Example:"),Qb.forEach(n),DF=d(Hn),v(pa.$$.fragment,Hn),Hn.forEach(n),_n.forEach(n),cp=d(s),jt=a(s,"H2",{class:!0});var Mu=i(jt);wo=a(Mu,"A",{id:!0,class:!0,href:!0});var Ub=i(wo);Wd=a(Ub,"SPAN",{});var Rb=i(Wd);v(ua.$$.fragment,Rb),Rb.forEach(n),Ub.forEach(n),AF=d(Mu),Qd=a(Mu,"SPAN",{});var Hb=i(Qd);NF=o(Hb,"TFFunnelForSequenceClassification"),Hb.forEach(n),Mu.forEach(n),pp=d(s),Ne=a(s,"DIV",{class:!0});var Tn=i(Ne);v(ha.$$.fragment,Tn),IF=d(Tn),Ud=a(Tn,"P",{});var Vb=i(Ud);SF=o(Vb,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Vb.forEach(n),BF=d(Tn),fa=a(Tn,"P",{});var zu=i(fa);WF=o(zu,"The Funnel Transformer model was proposed in "),ma=a(zu,"A",{href:!0,rel:!0});var Yb=i(ma);QF=o(Yb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Yb.forEach(n),UF=o(zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zu.forEach(n),RF=d(Tn),ga=a(Tn,"P",{});var qu=i(ga);HF=o(qu,"This model inherits from "),Ai=a(qu,"A",{href:!0});var Kb=i(Ai);VF=o(Kb,"TFPreTrainedModel"),Kb.forEach(n),YF=o(qu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qu.forEach(n),KF=d(Tn),_a=a(Tn,"P",{});var Pu=i(_a);GF=o(Pu,"This model is also a "),Ta=a(Pu,"A",{href:!0,rel:!0});var Gb=i(Ta);ZF=o(Gb,"tf.keras.Model"),Gb.forEach(n),XF=o(Pu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pu.forEach(n),JF=d(Tn),v(bo.$$.fragment,Tn),ev=d(Tn),cn=a(Tn,"DIV",{class:!0});var Vn=i(cn);v(ka.$$.fragment,Vn),nv=d(Vn),xt=a(Vn,"P",{});var cl=i(xt);tv=o(cl,"The "),Ni=a(cl,"A",{href:!0});var Zb=i(Ni);ov=o(Zb,"TFFunnelForSequenceClassification"),Zb.forEach(n),sv=o(cl," forward method, overrides the "),Rd=a(cl,"CODE",{});var Xb=i(Rd);rv=o(Xb,"__call__"),Xb.forEach(n),av=o(cl," special method."),cl.forEach(n),iv=d(Vn),v($o.$$.fragment,Vn),lv=d(Vn),Hd=a(Vn,"P",{});var Jb=i(Hd);dv=o(Jb,"Example:"),Jb.forEach(n),cv=d(Vn),v(Fa.$$.fragment,Vn),Vn.forEach(n),Tn.forEach(n),up=d(s),Lt=a(s,"H2",{class:!0});var Cu=i(Lt);Eo=a(Cu,"A",{id:!0,class:!0,href:!0});var e$=i(Eo);Vd=a(e$,"SPAN",{});var n$=i(Vd);v(va.$$.fragment,n$),n$.forEach(n),e$.forEach(n),pv=d(Cu),Yd=a(Cu,"SPAN",{});var t$=i(Yd);uv=o(t$,"TFFunnelForMultipleChoice"),t$.forEach(n),Cu.forEach(n),hp=d(s),Ie=a(s,"DIV",{class:!0});var kn=i(Ie);v(ya.$$.fragment,kn),hv=d(kn),Kd=a(kn,"P",{});var o$=i(Kd);fv=o(o$,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),o$.forEach(n),mv=d(kn),wa=a(kn,"P",{});var ju=i(wa);gv=o(ju,"The Funnel Transformer model was proposed in "),ba=a(ju,"A",{href:!0,rel:!0});var s$=i(ba);_v=o(s$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),s$.forEach(n),Tv=o(ju," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ju.forEach(n),kv=d(kn),$a=a(kn,"P",{});var xu=i($a);Fv=o(xu,"This model inherits from "),Ii=a(xu,"A",{href:!0});var r$=i(Ii);vv=o(r$,"TFPreTrainedModel"),r$.forEach(n),yv=o(xu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xu.forEach(n),wv=d(kn),Ea=a(kn,"P",{});var Lu=i(Ea);bv=o(Lu,"This model is also a "),Ma=a(Lu,"A",{href:!0,rel:!0});var a$=i(Ma);$v=o(a$,"tf.keras.Model"),a$.forEach(n),Ev=o(Lu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lu.forEach(n),Mv=d(kn),v(Mo.$$.fragment,kn),zv=d(kn),pn=a(kn,"DIV",{class:!0});var Yn=i(pn);v(za.$$.fragment,Yn),qv=d(Yn),Ot=a(Yn,"P",{});var pl=i(Ot);Pv=o(pl,"The "),Si=a(pl,"A",{href:!0});var i$=i(Si);Cv=o(i$,"TFFunnelForMultipleChoice"),i$.forEach(n),jv=o(pl," forward method, overrides the "),Gd=a(pl,"CODE",{});var l$=i(Gd);xv=o(l$,"__call__"),l$.forEach(n),Lv=o(pl," special method."),pl.forEach(n),Ov=d(Yn),v(zo.$$.fragment,Yn),Dv=d(Yn),Zd=a(Yn,"P",{});var d$=i(Zd);Av=o(d$,"Example:"),d$.forEach(n),Nv=d(Yn),v(qa.$$.fragment,Yn),Yn.forEach(n),kn.forEach(n),fp=d(s),Dt=a(s,"H2",{class:!0});var Ou=i(Dt);qo=a(Ou,"A",{id:!0,class:!0,href:!0});var c$=i(qo);Xd=a(c$,"SPAN",{});var p$=i(Xd);v(Pa.$$.fragment,p$),p$.forEach(n),c$.forEach(n),Iv=d(Ou),Jd=a(Ou,"SPAN",{});var u$=i(Jd);Sv=o(u$,"TFFunnelForTokenClassification"),u$.forEach(n),Ou.forEach(n),mp=d(s),Se=a(s,"DIV",{class:!0});var Fn=i(Se);v(Ca.$$.fragment,Fn),Bv=d(Fn),ec=a(Fn,"P",{});var h$=i(ec);Wv=o(h$,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),h$.forEach(n),Qv=d(Fn),ja=a(Fn,"P",{});var Du=i(ja);Uv=o(Du,"The Funnel Transformer model was proposed in "),xa=a(Du,"A",{href:!0,rel:!0});var f$=i(xa);Rv=o(f$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),f$.forEach(n),Hv=o(Du," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Du.forEach(n),Vv=d(Fn),La=a(Fn,"P",{});var Au=i(La);Yv=o(Au,"This model inherits from "),Bi=a(Au,"A",{href:!0});var m$=i(Bi);Kv=o(m$,"TFPreTrainedModel"),m$.forEach(n),Gv=o(Au,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Au.forEach(n),Zv=d(Fn),Oa=a(Fn,"P",{});var Nu=i(Oa);Xv=o(Nu,"This model is also a "),Da=a(Nu,"A",{href:!0,rel:!0});var g$=i(Da);Jv=o(g$,"tf.keras.Model"),g$.forEach(n),ey=o(Nu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Nu.forEach(n),ny=d(Fn),v(Po.$$.fragment,Fn),ty=d(Fn),un=a(Fn,"DIV",{class:!0});var Kn=i(un);v(Aa.$$.fragment,Kn),oy=d(Kn),At=a(Kn,"P",{});var ul=i(At);sy=o(ul,"The "),Wi=a(ul,"A",{href:!0});var _$=i(Wi);ry=o(_$,"TFFunnelForTokenClassification"),_$.forEach(n),ay=o(ul," forward method, overrides the "),nc=a(ul,"CODE",{});var T$=i(nc);iy=o(T$,"__call__"),T$.forEach(n),ly=o(ul," special method."),ul.forEach(n),dy=d(Kn),v(Co.$$.fragment,Kn),cy=d(Kn),tc=a(Kn,"P",{});var k$=i(tc);py=o(k$,"Example:"),k$.forEach(n),uy=d(Kn),v(Na.$$.fragment,Kn),Kn.forEach(n),Fn.forEach(n),gp=d(s),Nt=a(s,"H2",{class:!0});var Iu=i(Nt);jo=a(Iu,"A",{id:!0,class:!0,href:!0});var F$=i(jo);oc=a(F$,"SPAN",{});var v$=i(oc);v(Ia.$$.fragment,v$),v$.forEach(n),F$.forEach(n),hy=d(Iu),sc=a(Iu,"SPAN",{});var y$=i(sc);fy=o(y$,"TFFunnelForQuestionAnswering"),y$.forEach(n),Iu.forEach(n),_p=d(s),Be=a(s,"DIV",{class:!0});var vn=i(Be);v(Sa.$$.fragment,vn),my=d(vn),It=a(vn,"P",{});var hl=i(It);gy=o(hl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),rc=a(hl,"CODE",{});var w$=i(rc);_y=o(w$,"span start logits"),w$.forEach(n),Ty=o(hl," and "),ac=a(hl,"CODE",{});var b$=i(ac);ky=o(b$,"span end logits"),b$.forEach(n),Fy=o(hl,")."),hl.forEach(n),vy=d(vn),Ba=a(vn,"P",{});var Su=i(Ba);yy=o(Su,"The Funnel Transformer model was proposed in "),Wa=a(Su,"A",{href:!0,rel:!0});var $$=i(Wa);wy=o($$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),$$.forEach(n),by=o(Su," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Su.forEach(n),$y=d(vn),Qa=a(vn,"P",{});var Bu=i(Qa);Ey=o(Bu,"This model inherits from "),Qi=a(Bu,"A",{href:!0});var E$=i(Qi);My=o(E$,"TFPreTrainedModel"),E$.forEach(n),zy=o(Bu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bu.forEach(n),qy=d(vn),Ua=a(vn,"P",{});var Wu=i(Ua);Py=o(Wu,"This model is also a "),Ra=a(Wu,"A",{href:!0,rel:!0});var M$=i(Ra);Cy=o(M$,"tf.keras.Model"),M$.forEach(n),jy=o(Wu,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Wu.forEach(n),xy=d(vn),v(xo.$$.fragment,vn),Ly=d(vn),hn=a(vn,"DIV",{class:!0});var Gn=i(hn);v(Ha.$$.fragment,Gn),Oy=d(Gn),St=a(Gn,"P",{});var fl=i(St);Dy=o(fl,"The "),Ui=a(fl,"A",{href:!0});var z$=i(Ui);Ay=o(z$,"TFFunnelForQuestionAnswering"),z$.forEach(n),Ny=o(fl," forward method, overrides the "),ic=a(fl,"CODE",{});var q$=i(ic);Iy=o(q$,"__call__"),q$.forEach(n),Sy=o(fl," special method."),fl.forEach(n),By=d(Gn),v(Lo.$$.fragment,Gn),Wy=d(Gn),lc=a(Gn,"P",{});var P$=i(lc);Qy=o(P$,"Example:"),P$.forEach(n),Uy=d(Gn),v(Va.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(i2)),c(_,"id","funnel-transformer"),c(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_,"href","#funnel-transformer"),c(g,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelModel"),c(p,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(ke,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(Fe,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ve,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Xa,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(No,"href","https://huggingface.co/sgugger"),c(No,"rel","nofollow"),c(Io,"href","https://github.com/laiguokun/Funnel-Transformer"),c(Io,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Zn,"class","relative group"),c(Ja,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelModel"),c(ei,"href","/docs/transformers/pr_16363/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(ni,"href","/docs/transformers/pr_16363/en/main_classes/configuration#transformers.PretrainedConfig"),c(ti,"href","/docs/transformers/pr_16363/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ln,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(oi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizer"),c(si,"href","/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer"),c(ri,"href","/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizer"),c(An,"class","docstring"),c(Ut,"class","docstring"),c(yn,"class","docstring"),c(li,"class","docstring"),c(Ce,"class","docstring"),c(Rt,"id","transformers.FunnelTokenizerFast"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(di,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ci,"href","/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizerFast"),c(pi,"href","/docs/transformers/pr_16363/en/model_doc/bert#transformers.BertTokenizerFast"),c(wn,"class","docstring"),c(en,"class","docstring"),c(Vt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(ui,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(hi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(fi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(mi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelBaseModel"),c(nn,"class","docstring"),c(We,"class","docstring"),c(Gt,"id","transformers.FunnelModel"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ys,"href","https://arxiv.org/abs/2006.03236"),c(ys,"rel","nofollow"),c(gi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(_i,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelModel"),c(tn,"class","docstring"),c(Qe,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(Ti,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(on,"class","docstring"),c(pt,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(Fi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Ke,"class","docstring"),c(Ue,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Hs,"href","https://arxiv.org/abs/2006.03236"),c(Hs,"rel","nofollow"),c(vi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c(Ks,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ks,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(xe,"class","docstring"),c(Re,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(or,"href","https://arxiv.org/abs/2006.03236"),c(or,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c(ar,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ar,"rel","nofollow"),c(bi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(sn,"class","docstring"),c(He,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(kt,"class","relative group"),c(ur,"href","https://arxiv.org/abs/2006.03236"),c(ur,"rel","nofollow"),c($i,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c(mr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(mr,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(Ge,"class","docstring"),c(Ve,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(vt,"class","relative group"),c(yr,"href","https://arxiv.org/abs/2006.03236"),c(yr,"rel","nofollow"),c(Mi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.PreTrainedModel"),c($r,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($r,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(Ze,"class","docstring"),c(Ye,"class","docstring"),c(po,"id","transformers.TFFunnelBaseModel"),c(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(po,"href","#transformers.TFFunnelBaseModel"),c(bt,"class","relative group"),c(jr,"href","https://arxiv.org/abs/2006.03236"),c(jr,"rel","nofollow"),c(qi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Or,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Or,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(rn,"class","docstring"),c(Le,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Br,"href","https://arxiv.org/abs/2006.03236"),c(Br,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ur,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ur,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelModel"),c(an,"class","docstring"),c(Oe,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Gr,"href","https://arxiv.org/abs/2006.03236"),c(Gr,"rel","nofollow"),c(xi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Jr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Jr,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(ln,"class","docstring"),c(De,"class","docstring"),c(Fo,"id","transformers.TFFunnelForMaskedLM"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(aa,"href","https://arxiv.org/abs/2006.03236"),c(aa,"rel","nofollow"),c(Oi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(da,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(da,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(dn,"class","docstring"),c(Ae,"class","docstring"),c(wo,"id","transformers.TFFunnelForSequenceClassification"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFFunnelForSequenceClassification"),c(jt,"class","relative group"),c(ma,"href","https://arxiv.org/abs/2006.03236"),c(ma,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ta,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ta,"rel","nofollow"),c(Ni,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(cn,"class","docstring"),c(Ne,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c(ba,"href","https://arxiv.org/abs/2006.03236"),c(ba,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ma,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ma,"rel","nofollow"),c(Si,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(pn,"class","docstring"),c(Ie,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(xa,"href","https://arxiv.org/abs/2006.03236"),c(xa,"rel","nofollow"),c(Bi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Da,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Da,"rel","nofollow"),c(Wi,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Se,"class","docstring"),c(jo,"id","transformers.TFFunnelForQuestionAnswering"),c(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jo,"href","#transformers.TFFunnelForQuestionAnswering"),c(Nt,"class","relative group"),c(Wa,"href","https://arxiv.org/abs/2006.03236"),c(Wa,"rel","nofollow"),c(Qi,"href","/docs/transformers/pr_16363/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ra,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ra,"rel","nofollow"),c(Ui,"href","/docs/transformers/pr_16363/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(hn,"class","docstring"),c(Be,"class","docstring")},m(s,f){e(document.head,u),h(s,z,f),h(s,g,f),e(g,_),e(_,k),y(T,k,null),e(g,m),e(g,M),e(M,ce),h(s,K,f),h(s,q,f),e(q,J),e(J,A),y(ne,A,null),e(q,pe),e(q,N),e(N,ue),h(s,ie,f),h(s,Y,f),e(Y,L),e(Y,te),e(te,G),e(Y,P),h(s,j,f),h(s,oe,f),e(oe,W),h(s,le,f),h(s,se,f),e(se,I),e(I,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,Q),e(ee,me),e(ee,S),e(S,O),e(S,re),e(re,U),e(S,ge),e(S,p),e(p,E),e(S,Z),e(S,Te),e(Te,ye),e(S,D),e(S,ke),e(ke,we),e(S,be),e(S,x),e(x,R),e(S,$e),e(S,Fe),e(Fe,H),e(S,Ee),e(S,ve),e(ve,_e),e(S,Me),e(S,Xa),e(Xa,Qu),e(S,Uu),h(s,Pc,f),h(s,Dn,f),e(Dn,Ru),e(Dn,No),e(No,Hu),e(Dn,Vu),e(Dn,Io),e(Io,Yu),e(Dn,Ku),h(s,Cc,f),h(s,Zn,f),e(Zn,Bt),e(Bt,ml),y(So,ml,null),e(Zn,Gu),e(Zn,gl),e(gl,Zu),h(s,jc,f),h(s,Ln,f),y(Bo,Ln,null),e(Ln,Xu),e(Ln,On),e(On,Ju),e(On,Ja),e(Ja,eh),e(On,nh),e(On,ei),e(ei,th),e(On,oh),e(On,Wo),e(Wo,sh),e(On,rh),e(Ln,ah),e(Ln,Xn),e(Xn,ih),e(Xn,ni),e(ni,lh),e(Xn,dh),e(Xn,ti),e(ti,ch),e(Xn,ph),h(s,xc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,_l),y(Qo,_l,null),e(Jn,uh),e(Jn,Tl),e(Tl,hh),h(s,Lc,f),h(s,Ce,f),y(Uo,Ce,null),e(Ce,fh),e(Ce,kl),e(kl,mh),e(Ce,gh),e(Ce,Qt),e(Qt,oi),e(oi,_h),e(Qt,Th),e(Qt,si),e(si,kh),e(Qt,Fh),e(Ce,vh),e(Ce,Ro),e(Ro,yh),e(Ro,ri),e(ri,wh),e(Ro,bh),e(Ce,$h),e(Ce,An),y(Ho,An,null),e(An,Eh),e(An,Fl),e(Fl,Mh),e(An,zh),e(An,Vo),e(Vo,ai),e(ai,qh),e(ai,vl),e(vl,Ph),e(Vo,Ch),e(Vo,ii),e(ii,jh),e(ii,yl),e(yl,xh),e(Ce,Lh),e(Ce,Ut),y(Yo,Ut,null),e(Ut,Oh),e(Ut,Ko),e(Ko,Dh),e(Ko,wl),e(wl,Ah),e(Ko,Nh),e(Ce,Ih),e(Ce,yn),y(Go,yn,null),e(yn,Sh),e(yn,bl),e(bl,Bh),e(yn,Wh),y(Zo,yn,null),e(yn,Qh),e(yn,et),e(et,Uh),e(et,$l),e($l,Rh),e(et,Hh),e(et,El),e(El,Vh),e(et,Yh),e(Ce,Kh),e(Ce,li),y(Xo,li,null),h(s,Oc,f),h(s,nt,f),e(nt,Rt),e(Rt,Ml),y(Jo,Ml,null),e(nt,Gh),e(nt,zl),e(zl,Zh),h(s,Dc,f),h(s,en,f),y(es,en,null),e(en,Xh),e(en,ns),e(ns,Jh),e(ns,ql),e(ql,ef),e(ns,nf),e(en,tf),e(en,Ht),e(Ht,di),e(di,of),e(Ht,sf),e(Ht,ci),e(ci,rf),e(Ht,af),e(en,lf),e(en,ts),e(ts,df),e(ts,pi),e(pi,cf),e(ts,pf),e(en,uf),e(en,wn),y(os,wn,null),e(wn,hf),e(wn,Pl),e(Pl,ff),e(wn,mf),y(ss,wn,null),e(wn,gf),e(wn,tt),e(tt,_f),e(tt,Cl),e(Cl,Tf),e(tt,kf),e(tt,jl),e(jl,Ff),e(tt,vf),h(s,Ac,f),h(s,ot,f),e(ot,Vt),e(Vt,xl),y(rs,xl,null),e(ot,yf),e(ot,Ll),e(Ll,wf),h(s,Nc,f),h(s,st,f),y(as,st,null),e(st,bf),e(st,is),e(is,$f),e(is,ui),e(ui,Ef),e(is,Mf),h(s,Ic,f),h(s,rt,f),y(ls,rt,null),e(rt,zf),e(rt,ds),e(ds,qf),e(ds,hi),e(hi,Pf),e(ds,Cf),h(s,Sc,f),h(s,at,f),e(at,Yt),e(Yt,Ol),y(cs,Ol,null),e(at,jf),e(at,Dl),e(Dl,xf),h(s,Bc,f),h(s,We,f),y(ps,We,null),e(We,Lf),e(We,Al),e(Al,Of),e(We,Df),e(We,us),e(us,Af),e(us,hs),e(hs,Nf),e(us,If),e(We,Sf),e(We,fs),e(fs,Bf),e(fs,fi),e(fi,Wf),e(fs,Qf),e(We,Uf),e(We,ms),e(ms,Rf),e(ms,gs),e(gs,Hf),e(ms,Vf),e(We,Yf),e(We,nn),y(_s,nn,null),e(nn,Kf),e(nn,it),e(it,Gf),e(it,mi),e(mi,Zf),e(it,Xf),e(it,Nl),e(Nl,Jf),e(it,em),e(nn,nm),y(Kt,nn,null),e(nn,tm),e(nn,Il),e(Il,om),e(nn,sm),y(Ts,nn,null),h(s,Wc,f),h(s,lt,f),e(lt,Gt),e(Gt,Sl),y(ks,Sl,null),e(lt,rm),e(lt,Bl),e(Bl,am),h(s,Qc,f),h(s,Qe,f),y(Fs,Qe,null),e(Qe,im),e(Qe,Wl),e(Wl,lm),e(Qe,dm),e(Qe,vs),e(vs,cm),e(vs,ys),e(ys,pm),e(vs,um),e(Qe,hm),e(Qe,ws),e(ws,fm),e(ws,gi),e(gi,mm),e(ws,gm),e(Qe,_m),e(Qe,bs),e(bs,Tm),e(bs,$s),e($s,km),e(bs,Fm),e(Qe,vm),e(Qe,tn),y(Es,tn,null),e(tn,ym),e(tn,dt),e(dt,wm),e(dt,_i),e(_i,bm),e(dt,$m),e(dt,Ql),e(Ql,Em),e(dt,Mm),e(tn,zm),y(Zt,tn,null),e(tn,qm),e(tn,Ul),e(Ul,Pm),e(tn,Cm),y(Ms,tn,null),h(s,Uc,f),h(s,ct,f),e(ct,Xt),e(Xt,Rl),y(zs,Rl,null),e(ct,jm),e(ct,Hl),e(Hl,xm),h(s,Rc,f),h(s,pt,f),y(qs,pt,null),e(pt,Lm),e(pt,on),y(Ps,on,null),e(on,Om),e(on,ut),e(ut,Dm),e(ut,Ti),e(Ti,Am),e(ut,Nm),e(ut,Vl),e(Vl,Im),e(ut,Sm),e(on,Bm),y(Jt,on,null),e(on,Wm),e(on,Yl),e(Yl,Qm),e(on,Um),y(Cs,on,null),h(s,Hc,f),h(s,ht,f),e(ht,eo),e(eo,Kl),y(js,Kl,null),e(ht,Rm),e(ht,Gl),e(Gl,Hm),h(s,Vc,f),h(s,Ue,f),y(xs,Ue,null),e(Ue,Vm),e(Ue,Ls),e(Ls,Ym),e(Ls,Zl),e(Zl,Km),e(Ls,Gm),e(Ue,Zm),e(Ue,Os),e(Os,Xm),e(Os,Ds),e(Ds,Jm),e(Os,eg),e(Ue,ng),e(Ue,As),e(As,tg),e(As,ki),e(ki,og),e(As,sg),e(Ue,rg),e(Ue,Ns),e(Ns,ag),e(Ns,Is),e(Is,ig),e(Ns,lg),e(Ue,dg),e(Ue,Ke),y(Ss,Ke,null),e(Ke,cg),e(Ke,ft),e(ft,pg),e(ft,Fi),e(Fi,ug),e(ft,hg),e(ft,Xl),e(Xl,fg),e(ft,mg),e(Ke,gg),y(no,Ke,null),e(Ke,_g),e(Ke,Jl),e(Jl,Tg),e(Ke,kg),y(Bs,Ke,null),e(Ke,Fg),y(Ws,Ke,null),h(s,Yc,f),h(s,mt,f),e(mt,to),e(to,ed),y(Qs,ed,null),e(mt,vg),e(mt,nd),e(nd,yg),h(s,Kc,f),h(s,Re,f),y(Us,Re,null),e(Re,wg),e(Re,td),e(td,bg),e(Re,$g),e(Re,Rs),e(Rs,Eg),e(Rs,Hs),e(Hs,Mg),e(Rs,zg),e(Re,qg),e(Re,Vs),e(Vs,Pg),e(Vs,vi),e(vi,Cg),e(Vs,jg),e(Re,xg),e(Re,Ys),e(Ys,Lg),e(Ys,Ks),e(Ks,Og),e(Ys,Dg),e(Re,Ag),e(Re,xe),y(Gs,xe,null),e(xe,Ng),e(xe,gt),e(gt,Ig),e(gt,yi),e(yi,Sg),e(gt,Bg),e(gt,od),e(od,Wg),e(gt,Qg),e(xe,Ug),y(oo,xe,null),e(xe,Rg),e(xe,sd),e(sd,Hg),e(xe,Vg),y(Zs,xe,null),e(xe,Yg),y(Xs,xe,null),e(xe,Kg),e(xe,rd),e(rd,Gg),e(xe,Zg),y(Js,xe,null),h(s,Gc,f),h(s,_t,f),e(_t,so),e(so,ad),y(er,ad,null),e(_t,Xg),e(_t,id),e(id,Jg),h(s,Zc,f),h(s,He,f),y(nr,He,null),e(He,e_),e(He,ld),e(ld,n_),e(He,t_),e(He,tr),e(tr,o_),e(tr,or),e(or,s_),e(tr,r_),e(He,a_),e(He,sr),e(sr,i_),e(sr,wi),e(wi,l_),e(sr,d_),e(He,c_),e(He,rr),e(rr,p_),e(rr,ar),e(ar,u_),e(rr,h_),e(He,f_),e(He,sn),y(ir,sn,null),e(sn,m_),e(sn,Tt),e(Tt,g_),e(Tt,bi),e(bi,__),e(Tt,T_),e(Tt,dd),e(dd,k_),e(Tt,F_),e(sn,v_),y(ro,sn,null),e(sn,y_),e(sn,cd),e(cd,w_),e(sn,b_),y(lr,sn,null),h(s,Xc,f),h(s,kt,f),e(kt,ao),e(ao,pd),y(dr,pd,null),e(kt,$_),e(kt,ud),e(ud,E_),h(s,Jc,f),h(s,Ve,f),y(cr,Ve,null),e(Ve,M_),e(Ve,hd),e(hd,z_),e(Ve,q_),e(Ve,pr),e(pr,P_),e(pr,ur),e(ur,C_),e(pr,j_),e(Ve,x_),e(Ve,hr),e(hr,L_),e(hr,$i),e($i,O_),e(hr,D_),e(Ve,A_),e(Ve,fr),e(fr,N_),e(fr,mr),e(mr,I_),e(fr,S_),e(Ve,B_),e(Ve,Ge),y(gr,Ge,null),e(Ge,W_),e(Ge,Ft),e(Ft,Q_),e(Ft,Ei),e(Ei,U_),e(Ft,R_),e(Ft,fd),e(fd,H_),e(Ft,V_),e(Ge,Y_),y(io,Ge,null),e(Ge,K_),e(Ge,md),e(md,G_),e(Ge,Z_),y(_r,Ge,null),e(Ge,X_),y(Tr,Ge,null),h(s,ep,f),h(s,vt,f),e(vt,lo),e(lo,gd),y(kr,gd,null),e(vt,J_),e(vt,_d),e(_d,eT),h(s,np,f),h(s,Ye,f),y(Fr,Ye,null),e(Ye,nT),e(Ye,yt),e(yt,tT),e(yt,Td),e(Td,oT),e(yt,sT),e(yt,kd),e(kd,rT),e(yt,aT),e(Ye,iT),e(Ye,vr),e(vr,lT),e(vr,yr),e(yr,dT),e(vr,cT),e(Ye,pT),e(Ye,wr),e(wr,uT),e(wr,Mi),e(Mi,hT),e(wr,fT),e(Ye,mT),e(Ye,br),e(br,gT),e(br,$r),e($r,_T),e(br,TT),e(Ye,kT),e(Ye,Ze),y(Er,Ze,null),e(Ze,FT),e(Ze,wt),e(wt,vT),e(wt,zi),e(zi,yT),e(wt,wT),e(wt,Fd),e(Fd,bT),e(wt,$T),e(Ze,ET),y(co,Ze,null),e(Ze,MT),e(Ze,vd),e(vd,zT),e(Ze,qT),y(Mr,Ze,null),e(Ze,PT),y(zr,Ze,null),h(s,tp,f),h(s,bt,f),e(bt,po),e(po,yd),y(qr,yd,null),e(bt,CT),e(bt,wd),e(wd,jT),h(s,op,f),h(s,Le,f),y(Pr,Le,null),e(Le,xT),e(Le,bd),e(bd,LT),e(Le,OT),e(Le,Cr),e(Cr,DT),e(Cr,jr),e(jr,AT),e(Cr,NT),e(Le,IT),e(Le,xr),e(xr,ST),e(xr,qi),e(qi,BT),e(xr,WT),e(Le,QT),e(Le,Lr),e(Lr,UT),e(Lr,Or),e(Or,RT),e(Lr,HT),e(Le,VT),y(uo,Le,null),e(Le,YT),e(Le,rn),y(Dr,rn,null),e(rn,KT),e(rn,$t),e($t,GT),e($t,Pi),e(Pi,ZT),e($t,XT),e($t,$d),e($d,JT),e($t,ek),e(rn,nk),y(ho,rn,null),e(rn,tk),e(rn,Ed),e(Ed,ok),e(rn,sk),y(Ar,rn,null),h(s,sp,f),h(s,Et,f),e(Et,fo),e(fo,Md),y(Nr,Md,null),e(Et,rk),e(Et,zd),e(zd,ak),h(s,rp,f),h(s,Oe,f),y(Ir,Oe,null),e(Oe,ik),e(Oe,qd),e(qd,lk),e(Oe,dk),e(Oe,Sr),e(Sr,ck),e(Sr,Br),e(Br,pk),e(Sr,uk),e(Oe,hk),e(Oe,Wr),e(Wr,fk),e(Wr,Ci),e(Ci,mk),e(Wr,gk),e(Oe,_k),e(Oe,Qr),e(Qr,Tk),e(Qr,Ur),e(Ur,kk),e(Qr,Fk),e(Oe,vk),y(mo,Oe,null),e(Oe,yk),e(Oe,an),y(Rr,an,null),e(an,wk),e(an,Mt),e(Mt,bk),e(Mt,ji),e(ji,$k),e(Mt,Ek),e(Mt,Pd),e(Pd,Mk),e(Mt,zk),e(an,qk),y(go,an,null),e(an,Pk),e(an,Cd),e(Cd,Ck),e(an,jk),y(Hr,an,null),h(s,ap,f),h(s,zt,f),e(zt,_o),e(_o,jd),y(Vr,jd,null),e(zt,xk),e(zt,xd),e(xd,Lk),h(s,ip,f),h(s,De,f),y(Yr,De,null),e(De,Ok),e(De,Ld),e(Ld,Dk),e(De,Ak),e(De,Kr),e(Kr,Nk),e(Kr,Gr),e(Gr,Ik),e(Kr,Sk),e(De,Bk),e(De,Zr),e(Zr,Wk),e(Zr,xi),e(xi,Qk),e(Zr,Uk),e(De,Rk),e(De,Xr),e(Xr,Hk),e(Xr,Jr),e(Jr,Vk),e(Xr,Yk),e(De,Kk),y(To,De,null),e(De,Gk),e(De,ln),y(ea,ln,null),e(ln,Zk),e(ln,qt),e(qt,Xk),e(qt,Li),e(Li,Jk),e(qt,eF),e(qt,Od),e(Od,nF),e(qt,tF),e(ln,oF),y(ko,ln,null),e(ln,sF),e(ln,Dd),e(Dd,rF),e(ln,aF),y(na,ln,null),h(s,lp,f),h(s,Pt,f),e(Pt,Fo),e(Fo,Ad),y(ta,Ad,null),e(Pt,iF),e(Pt,Nd),e(Nd,lF),h(s,dp,f),h(s,Ae,f),y(oa,Ae,null),e(Ae,dF),e(Ae,sa),e(sa,cF),e(sa,Id),e(Id,pF),e(sa,uF),e(Ae,hF),e(Ae,ra),e(ra,fF),e(ra,aa),e(aa,mF),e(ra,gF),e(Ae,_F),e(Ae,ia),e(ia,TF),e(ia,Oi),e(Oi,kF),e(ia,FF),e(Ae,vF),e(Ae,la),e(la,yF),e(la,da),e(da,wF),e(la,bF),e(Ae,$F),y(vo,Ae,null),e(Ae,EF),e(Ae,dn),y(ca,dn,null),e(dn,MF),e(dn,Ct),e(Ct,zF),e(Ct,Di),e(Di,qF),e(Ct,PF),e(Ct,Sd),e(Sd,CF),e(Ct,jF),e(dn,xF),y(yo,dn,null),e(dn,LF),e(dn,Bd),e(Bd,OF),e(dn,DF),y(pa,dn,null),h(s,cp,f),h(s,jt,f),e(jt,wo),e(wo,Wd),y(ua,Wd,null),e(jt,AF),e(jt,Qd),e(Qd,NF),h(s,pp,f),h(s,Ne,f),y(ha,Ne,null),e(Ne,IF),e(Ne,Ud),e(Ud,SF),e(Ne,BF),e(Ne,fa),e(fa,WF),e(fa,ma),e(ma,QF),e(fa,UF),e(Ne,RF),e(Ne,ga),e(ga,HF),e(ga,Ai),e(Ai,VF),e(ga,YF),e(Ne,KF),e(Ne,_a),e(_a,GF),e(_a,Ta),e(Ta,ZF),e(_a,XF),e(Ne,JF),y(bo,Ne,null),e(Ne,ev),e(Ne,cn),y(ka,cn,null),e(cn,nv),e(cn,xt),e(xt,tv),e(xt,Ni),e(Ni,ov),e(xt,sv),e(xt,Rd),e(Rd,rv),e(xt,av),e(cn,iv),y($o,cn,null),e(cn,lv),e(cn,Hd),e(Hd,dv),e(cn,cv),y(Fa,cn,null),h(s,up,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Vd),y(va,Vd,null),e(Lt,pv),e(Lt,Yd),e(Yd,uv),h(s,hp,f),h(s,Ie,f),y(ya,Ie,null),e(Ie,hv),e(Ie,Kd),e(Kd,fv),e(Ie,mv),e(Ie,wa),e(wa,gv),e(wa,ba),e(ba,_v),e(wa,Tv),e(Ie,kv),e(Ie,$a),e($a,Fv),e($a,Ii),e(Ii,vv),e($a,yv),e(Ie,wv),e(Ie,Ea),e(Ea,bv),e(Ea,Ma),e(Ma,$v),e(Ea,Ev),e(Ie,Mv),y(Mo,Ie,null),e(Ie,zv),e(Ie,pn),y(za,pn,null),e(pn,qv),e(pn,Ot),e(Ot,Pv),e(Ot,Si),e(Si,Cv),e(Ot,jv),e(Ot,Gd),e(Gd,xv),e(Ot,Lv),e(pn,Ov),y(zo,pn,null),e(pn,Dv),e(pn,Zd),e(Zd,Av),e(pn,Nv),y(qa,pn,null),h(s,fp,f),h(s,Dt,f),e(Dt,qo),e(qo,Xd),y(Pa,Xd,null),e(Dt,Iv),e(Dt,Jd),e(Jd,Sv),h(s,mp,f),h(s,Se,f),y(Ca,Se,null),e(Se,Bv),e(Se,ec),e(ec,Wv),e(Se,Qv),e(Se,ja),e(ja,Uv),e(ja,xa),e(xa,Rv),e(ja,Hv),e(Se,Vv),e(Se,La),e(La,Yv),e(La,Bi),e(Bi,Kv),e(La,Gv),e(Se,Zv),e(Se,Oa),e(Oa,Xv),e(Oa,Da),e(Da,Jv),e(Oa,ey),e(Se,ny),y(Po,Se,null),e(Se,ty),e(Se,un),y(Aa,un,null),e(un,oy),e(un,At),e(At,sy),e(At,Wi),e(Wi,ry),e(At,ay),e(At,nc),e(nc,iy),e(At,ly),e(un,dy),y(Co,un,null),e(un,cy),e(un,tc),e(tc,py),e(un,uy),y(Na,un,null),h(s,gp,f),h(s,Nt,f),e(Nt,jo),e(jo,oc),y(Ia,oc,null),e(Nt,hy),e(Nt,sc),e(sc,fy),h(s,_p,f),h(s,Be,f),y(Sa,Be,null),e(Be,my),e(Be,It),e(It,gy),e(It,rc),e(rc,_y),e(It,Ty),e(It,ac),e(ac,ky),e(It,Fy),e(Be,vy),e(Be,Ba),e(Ba,yy),e(Ba,Wa),e(Wa,wy),e(Ba,by),e(Be,$y),e(Be,Qa),e(Qa,Ey),e(Qa,Qi),e(Qi,My),e(Qa,zy),e(Be,qy),e(Be,Ua),e(Ua,Py),e(Ua,Ra),e(Ra,Cy),e(Ua,jy),e(Be,xy),y(xo,Be,null),e(Be,Ly),e(Be,hn),y(Ha,hn,null),e(hn,Oy),e(hn,St),e(St,Dy),e(St,Ui),e(Ui,Ay),e(St,Ny),e(St,ic),e(ic,Iy),e(St,Sy),e(hn,By),y(Lo,hn,null),e(hn,Wy),e(hn,lc),e(lc,Qy),e(hn,Uy),y(Va,hn,null),Tp=!0},p(s,[f]){const Ya={};f&2&&(Ya.$$scope={dirty:f,ctx:s}),Kt.$set(Ya);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:s}),Zt.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),Jt.$set(cc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),no.$set(pc);const Ka={};f&2&&(Ka.$$scope={dirty:f,ctx:s}),oo.$set(Ka);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),ro.$set(uc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),io.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),co.$set(fc);const Ga={};f&2&&(Ga.$$scope={dirty:f,ctx:s}),uo.$set(Ga);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),ho.$set(mc);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),mo.$set(gc);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),go.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),To.$set(Tc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),ko.$set(kc);const Za={};f&2&&(Za.$$scope={dirty:f,ctx:s}),vo.$set(Za);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),yo.$set(Fc);const je={};f&2&&(je.$$scope={dirty:f,ctx:s}),bo.$set(je);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),$o.$set(vc);const yc={};f&2&&(yc.$$scope={dirty:f,ctx:s}),Mo.$set(yc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),zo.$set(wc);const bc={};f&2&&(bc.$$scope={dirty:f,ctx:s}),Po.$set(bc);const $c={};f&2&&($c.$$scope={dirty:f,ctx:s}),Co.$set($c);const Ec={};f&2&&(Ec.$$scope={dirty:f,ctx:s}),xo.$set(Ec);const Mc={};f&2&&(Mc.$$scope={dirty:f,ctx:s}),Lo.$set(Mc)},i(s){Tp||(w(T.$$.fragment,s),w(ne.$$.fragment,s),w(So.$$.fragment,s),w(Bo.$$.fragment,s),w(Qo.$$.fragment,s),w(Uo.$$.fragment,s),w(Ho.$$.fragment,s),w(Yo.$$.fragment,s),w(Go.$$.fragment,s),w(Zo.$$.fragment,s),w(Xo.$$.fragment,s),w(Jo.$$.fragment,s),w(es.$$.fragment,s),w(os.$$.fragment,s),w(ss.$$.fragment,s),w(rs.$$.fragment,s),w(as.$$.fragment,s),w(ls.$$.fragment,s),w(cs.$$.fragment,s),w(ps.$$.fragment,s),w(_s.$$.fragment,s),w(Kt.$$.fragment,s),w(Ts.$$.fragment,s),w(ks.$$.fragment,s),w(Fs.$$.fragment,s),w(Es.$$.fragment,s),w(Zt.$$.fragment,s),w(Ms.$$.fragment,s),w(zs.$$.fragment,s),w(qs.$$.fragment,s),w(Ps.$$.fragment,s),w(Jt.$$.fragment,s),w(Cs.$$.fragment,s),w(js.$$.fragment,s),w(xs.$$.fragment,s),w(Ss.$$.fragment,s),w(no.$$.fragment,s),w(Bs.$$.fragment,s),w(Ws.$$.fragment,s),w(Qs.$$.fragment,s),w(Us.$$.fragment,s),w(Gs.$$.fragment,s),w(oo.$$.fragment,s),w(Zs.$$.fragment,s),w(Xs.$$.fragment,s),w(Js.$$.fragment,s),w(er.$$.fragment,s),w(nr.$$.fragment,s),w(ir.$$.fragment,s),w(ro.$$.fragment,s),w(lr.$$.fragment,s),w(dr.$$.fragment,s),w(cr.$$.fragment,s),w(gr.$$.fragment,s),w(io.$$.fragment,s),w(_r.$$.fragment,s),w(Tr.$$.fragment,s),w(kr.$$.fragment,s),w(Fr.$$.fragment,s),w(Er.$$.fragment,s),w(co.$$.fragment,s),w(Mr.$$.fragment,s),w(zr.$$.fragment,s),w(qr.$$.fragment,s),w(Pr.$$.fragment,s),w(uo.$$.fragment,s),w(Dr.$$.fragment,s),w(ho.$$.fragment,s),w(Ar.$$.fragment,s),w(Nr.$$.fragment,s),w(Ir.$$.fragment,s),w(mo.$$.fragment,s),w(Rr.$$.fragment,s),w(go.$$.fragment,s),w(Hr.$$.fragment,s),w(Vr.$$.fragment,s),w(Yr.$$.fragment,s),w(To.$$.fragment,s),w(ea.$$.fragment,s),w(ko.$$.fragment,s),w(na.$$.fragment,s),w(ta.$$.fragment,s),w(oa.$$.fragment,s),w(vo.$$.fragment,s),w(ca.$$.fragment,s),w(yo.$$.fragment,s),w(pa.$$.fragment,s),w(ua.$$.fragment,s),w(ha.$$.fragment,s),w(bo.$$.fragment,s),w(ka.$$.fragment,s),w($o.$$.fragment,s),w(Fa.$$.fragment,s),w(va.$$.fragment,s),w(ya.$$.fragment,s),w(Mo.$$.fragment,s),w(za.$$.fragment,s),w(zo.$$.fragment,s),w(qa.$$.fragment,s),w(Pa.$$.fragment,s),w(Ca.$$.fragment,s),w(Po.$$.fragment,s),w(Aa.$$.fragment,s),w(Co.$$.fragment,s),w(Na.$$.fragment,s),w(Ia.$$.fragment,s),w(Sa.$$.fragment,s),w(xo.$$.fragment,s),w(Ha.$$.fragment,s),w(Lo.$$.fragment,s),w(Va.$$.fragment,s),Tp=!0)},o(s){b(T.$$.fragment,s),b(ne.$$.fragment,s),b(So.$$.fragment,s),b(Bo.$$.fragment,s),b(Qo.$$.fragment,s),b(Uo.$$.fragment,s),b(Ho.$$.fragment,s),b(Yo.$$.fragment,s),b(Go.$$.fragment,s),b(Zo.$$.fragment,s),b(Xo.$$.fragment,s),b(Jo.$$.fragment,s),b(es.$$.fragment,s),b(os.$$.fragment,s),b(ss.$$.fragment,s),b(rs.$$.fragment,s),b(as.$$.fragment,s),b(ls.$$.fragment,s),b(cs.$$.fragment,s),b(ps.$$.fragment,s),b(_s.$$.fragment,s),b(Kt.$$.fragment,s),b(Ts.$$.fragment,s),b(ks.$$.fragment,s),b(Fs.$$.fragment,s),b(Es.$$.fragment,s),b(Zt.$$.fragment,s),b(Ms.$$.fragment,s),b(zs.$$.fragment,s),b(qs.$$.fragment,s),b(Ps.$$.fragment,s),b(Jt.$$.fragment,s),b(Cs.$$.fragment,s),b(js.$$.fragment,s),b(xs.$$.fragment,s),b(Ss.$$.fragment,s),b(no.$$.fragment,s),b(Bs.$$.fragment,s),b(Ws.$$.fragment,s),b(Qs.$$.fragment,s),b(Us.$$.fragment,s),b(Gs.$$.fragment,s),b(oo.$$.fragment,s),b(Zs.$$.fragment,s),b(Xs.$$.fragment,s),b(Js.$$.fragment,s),b(er.$$.fragment,s),b(nr.$$.fragment,s),b(ir.$$.fragment,s),b(ro.$$.fragment,s),b(lr.$$.fragment,s),b(dr.$$.fragment,s),b(cr.$$.fragment,s),b(gr.$$.fragment,s),b(io.$$.fragment,s),b(_r.$$.fragment,s),b(Tr.$$.fragment,s),b(kr.$$.fragment,s),b(Fr.$$.fragment,s),b(Er.$$.fragment,s),b(co.$$.fragment,s),b(Mr.$$.fragment,s),b(zr.$$.fragment,s),b(qr.$$.fragment,s),b(Pr.$$.fragment,s),b(uo.$$.fragment,s),b(Dr.$$.fragment,s),b(ho.$$.fragment,s),b(Ar.$$.fragment,s),b(Nr.$$.fragment,s),b(Ir.$$.fragment,s),b(mo.$$.fragment,s),b(Rr.$$.fragment,s),b(go.$$.fragment,s),b(Hr.$$.fragment,s),b(Vr.$$.fragment,s),b(Yr.$$.fragment,s),b(To.$$.fragment,s),b(ea.$$.fragment,s),b(ko.$$.fragment,s),b(na.$$.fragment,s),b(ta.$$.fragment,s),b(oa.$$.fragment,s),b(vo.$$.fragment,s),b(ca.$$.fragment,s),b(yo.$$.fragment,s),b(pa.$$.fragment,s),b(ua.$$.fragment,s),b(ha.$$.fragment,s),b(bo.$$.fragment,s),b(ka.$$.fragment,s),b($o.$$.fragment,s),b(Fa.$$.fragment,s),b(va.$$.fragment,s),b(ya.$$.fragment,s),b(Mo.$$.fragment,s),b(za.$$.fragment,s),b(zo.$$.fragment,s),b(qa.$$.fragment,s),b(Pa.$$.fragment,s),b(Ca.$$.fragment,s),b(Po.$$.fragment,s),b(Aa.$$.fragment,s),b(Co.$$.fragment,s),b(Na.$$.fragment,s),b(Ia.$$.fragment,s),b(Sa.$$.fragment,s),b(xo.$$.fragment,s),b(Ha.$$.fragment,s),b(Lo.$$.fragment,s),b(Va.$$.fragment,s),Tp=!1},d(s){n(u),s&&n(z),s&&n(g),$(T),s&&n(K),s&&n(q),$(ne),s&&n(ie),s&&n(Y),s&&n(j),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Pc),s&&n(Dn),s&&n(Cc),s&&n(Zn),$(So),s&&n(jc),s&&n(Ln),$(Bo),s&&n(xc),s&&n(Jn),$(Qo),s&&n(Lc),s&&n(Ce),$(Uo),$(Ho),$(Yo),$(Go),$(Zo),$(Xo),s&&n(Oc),s&&n(nt),$(Jo),s&&n(Dc),s&&n(en),$(es),$(os),$(ss),s&&n(Ac),s&&n(ot),$(rs),s&&n(Nc),s&&n(st),$(as),s&&n(Ic),s&&n(rt),$(ls),s&&n(Sc),s&&n(at),$(cs),s&&n(Bc),s&&n(We),$(ps),$(_s),$(Kt),$(Ts),s&&n(Wc),s&&n(lt),$(ks),s&&n(Qc),s&&n(Qe),$(Fs),$(Es),$(Zt),$(Ms),s&&n(Uc),s&&n(ct),$(zs),s&&n(Rc),s&&n(pt),$(qs),$(Ps),$(Jt),$(Cs),s&&n(Hc),s&&n(ht),$(js),s&&n(Vc),s&&n(Ue),$(xs),$(Ss),$(no),$(Bs),$(Ws),s&&n(Yc),s&&n(mt),$(Qs),s&&n(Kc),s&&n(Re),$(Us),$(Gs),$(oo),$(Zs),$(Xs),$(Js),s&&n(Gc),s&&n(_t),$(er),s&&n(Zc),s&&n(He),$(nr),$(ir),$(ro),$(lr),s&&n(Xc),s&&n(kt),$(dr),s&&n(Jc),s&&n(Ve),$(cr),$(gr),$(io),$(_r),$(Tr),s&&n(ep),s&&n(vt),$(kr),s&&n(np),s&&n(Ye),$(Fr),$(Er),$(co),$(Mr),$(zr),s&&n(tp),s&&n(bt),$(qr),s&&n(op),s&&n(Le),$(Pr),$(uo),$(Dr),$(ho),$(Ar),s&&n(sp),s&&n(Et),$(Nr),s&&n(rp),s&&n(Oe),$(Ir),$(mo),$(Rr),$(go),$(Hr),s&&n(ap),s&&n(zt),$(Vr),s&&n(ip),s&&n(De),$(Yr),$(To),$(ea),$(ko),$(na),s&&n(lp),s&&n(Pt),$(ta),s&&n(dp),s&&n(Ae),$(oa),$(vo),$(ca),$(yo),$(pa),s&&n(cp),s&&n(jt),$(ua),s&&n(pp),s&&n(Ne),$(ha),$(bo),$(ka),$($o),$(Fa),s&&n(up),s&&n(Lt),$(va),s&&n(hp),s&&n(Ie),$(ya),$(Mo),$(za),$(zo),$(qa),s&&n(fp),s&&n(Dt),$(Pa),s&&n(mp),s&&n(Se),$(Ca),$(Po),$(Aa),$(Co),$(Na),s&&n(gp),s&&n(Nt),$(Ia),s&&n(_p),s&&n(Be),$(Sa),$(xo),$(Ha),$(Lo),$(Va)}}}const i2={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function l2(V){return O$(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class f2 extends C${constructor(u){super();j$(this,u,l2,a2,x$,{})}}export{f2 as default,i2 as metadata};
