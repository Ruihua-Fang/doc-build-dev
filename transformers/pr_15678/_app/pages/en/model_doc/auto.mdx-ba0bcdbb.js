import{S as Dft,i as qft,s as Gft,e as a,k as l,w as f,t as o,L as Oft,c as n,d as t,m as i,a as s,x as m,h as r,b as c,J as e,g as b,y as g,q as h,o as p,B as _}from"../../../chunks/vendor-9e2b328e.js";import{T as ACr}from"../../../chunks/Tip-76f97a76.js";import{D as E}from"../../../chunks/Docstring-50fd6873.js";import{C as w}from"../../../chunks/CodeBlock-b9ff96e9.js";import{I as z}from"../../../chunks/IconCopyLink-fd0e58fd.js";import"../../../chunks/CopyButton-4b97cbf7.js";function Xft(bi){let J,Ae,le,me,oo,ce,ue,No,vi,gf,ra,Ti,Fi,LC,hf,Ee,so,Ci,Bn,BC,xn,kn,xC,Mi,Rn,kC,Ei,pf,ka;return{c(){J=a("p"),Ae=o("If your "),le=a("code"),me=o("NewModelConfig"),oo=o(" is a subclass of "),ce=a("code"),ue=o("PretrainedConfig"),No=o(`, make sure its
`),vi=a("code"),gf=o("model_type"),ra=o(" attribute is set to the same key you use when registering the config (here "),Ti=a("code"),Fi=o('"new-model"'),LC=o(")."),hf=l(),Ee=a("p"),so=o("Likewise, if your "),Ci=a("code"),Bn=o("NewModel"),BC=o(" is a subclass of "),xn=a("a"),kn=o("PreTrainedModel"),xC=o(`, make sure its
`),Mi=a("code"),Rn=o("config_class"),kC=o(` attribute is set to the same class you use when registering the model (here
`),Ei=a("code"),pf=o("NewModelConfig"),ka=o(")."),this.h()},l(lo){J=n(lo,"P",{});var ge=s(J);Ae=r(ge,"If your "),le=n(ge,"CODE",{});var n0=s(le);me=r(n0,"NewModelConfig"),n0.forEach(t),oo=r(ge," is a subclass of "),ce=n(ge,"CODE",{});var yi=s(ce);ue=r(yi,"PretrainedConfig"),yi.forEach(t),No=r(ge,`, make sure its
`),vi=n(ge,"CODE",{});var s0=s(vi);gf=r(s0,"model_type"),s0.forEach(t),ra=r(ge," attribute is set to the same key you use when registering the config (here "),Ti=n(ge,"CODE",{});var l0=s(Ti);Fi=r(l0,'"new-model"'),l0.forEach(t),LC=r(ge,")."),ge.forEach(t),hf=i(lo),Ee=n(lo,"P",{});var Do=s(Ee);so=r(Do,"Likewise, if your "),Ci=n(Do,"CODE",{});var Ra=s(Ci);Bn=r(Ra,"NewModel"),Ra.forEach(t),BC=r(Do," is a subclass of "),xn=n(Do,"A",{href:!0});var i0=s(xn);kn=r(i0,"PreTrainedModel"),i0.forEach(t),xC=r(Do,`, make sure its
`),Mi=n(Do,"CODE",{});var _f=s(Mi);Rn=r(_f,"config_class"),_f.forEach(t),kC=r(Do,` attribute is set to the same class you use when registering the model (here
`),Ei=n(Do,"CODE",{});var d0=s(Ei);pf=r(d0,"NewModelConfig"),d0.forEach(t),ka=r(Do,")."),Do.forEach(t),this.h()},h(){c(xn,"href","/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel")},m(lo,ge){b(lo,J,ge),e(J,Ae),e(J,le),e(le,me),e(J,oo),e(J,ce),e(ce,ue),e(J,No),e(J,vi),e(vi,gf),e(J,ra),e(J,Ti),e(Ti,Fi),e(J,LC),b(lo,hf,ge),b(lo,Ee,ge),e(Ee,so),e(Ee,Ci),e(Ci,Bn),e(Ee,BC),e(Ee,xn),e(xn,kn),e(Ee,xC),e(Ee,Mi),e(Mi,Rn),e(Ee,kC),e(Ee,Ei),e(Ei,pf),e(Ee,ka)},d(lo){lo&&t(J),lo&&t(hf),lo&&t(Ee)}}}function zft(bi){let J,Ae,le,me,oo;return{c(){J=a("p"),Ae=o("Passing "),le=a("code"),me=o("use_auth_token=True"),oo=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),le=n(ue,"CODE",{});var No=s(le);me=r(No,"use_auth_token=True"),No.forEach(t),oo=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,le),e(le,me),e(J,oo)},d(ce){ce&&t(J)}}}function Vft(bi){let J,Ae,le,me,oo;return{c(){J=a("p"),Ae=o("Passing "),le=a("code"),me=o("use_auth_token=True"),oo=o(" is required when you want to use a private model.")},l(ce){J=n(ce,"P",{});var ue=s(J);Ae=r(ue,"Passing "),le=n(ue,"CODE",{});var No=s(le);me=r(No,"use_auth_token=True"),No.forEach(t),oo=r(ue," is required when you want to use a private model."),ue.forEach(t)},m(ce,ue){b(ce,J,ue),e(J,Ae),e(J,le),e(le,me),e(J,oo)},d(ce){ce&&t(J)}}}function Wft(bi){let J,Ae,le,me,oo,ce,ue,No,vi,gf,ra,Ti,Fi,LC,hf,Ee,so,Ci,Bn,BC,xn,kn,xC,Mi,Rn,kC,Ei,pf,ka,lo,ge,n0,yi,s0,l0,Do,Ra,i0,_f,d0,MBe,bAe,wi,uf,Dz,RC,EBe,qz,yBe,vAe,Sn,wBe,Gz,ABe,LBe,Oz,BBe,xBe,TAe,SC,FAe,c0,kBe,CAe,bf,MAe,Ai,vf,Xz,PC,RBe,zz,SBe,EAe,qo,$C,PBe,IC,$Be,f0,IBe,jBe,NBe,jC,DBe,Vz,qBe,GBe,OBe,io,NC,XBe,Wz,zBe,VBe,Li,WBe,Qz,QBe,HBe,Hz,UBe,JBe,YBe,v,Tf,Uz,KBe,ZBe,m0,exe,oxe,rxe,Ff,Jz,txe,axe,g0,nxe,sxe,lxe,Cf,Yz,ixe,dxe,h0,cxe,fxe,mxe,Mf,Kz,gxe,hxe,p0,pxe,_xe,uxe,Ef,Zz,bxe,vxe,_0,Txe,Fxe,Cxe,yf,eV,Mxe,Exe,u0,yxe,wxe,Axe,wf,oV,Lxe,Bxe,b0,xxe,kxe,Rxe,Af,rV,Sxe,Pxe,v0,$xe,Ixe,jxe,Lf,tV,Nxe,Dxe,T0,qxe,Gxe,Oxe,Bf,aV,Xxe,zxe,F0,Vxe,Wxe,Qxe,xf,nV,Hxe,Uxe,C0,Jxe,Yxe,Kxe,kf,sV,Zxe,eke,M0,oke,rke,tke,Rf,lV,ake,nke,E0,ske,lke,ike,Sf,iV,dke,cke,y0,fke,mke,gke,Pf,dV,hke,pke,w0,_ke,uke,bke,$f,cV,vke,Tke,A0,Fke,Cke,Mke,If,fV,Eke,yke,L0,wke,Ake,Lke,jf,mV,Bke,xke,B0,kke,Rke,Ske,Nf,gV,Pke,$ke,x0,Ike,jke,Nke,Df,hV,Dke,qke,k0,Gke,Oke,Xke,qf,pV,zke,Vke,R0,Wke,Qke,Hke,Gf,_V,Uke,Jke,S0,Yke,Kke,Zke,Of,uV,eRe,oRe,P0,rRe,tRe,aRe,Xf,bV,nRe,sRe,$0,lRe,iRe,dRe,zf,vV,cRe,fRe,I0,mRe,gRe,hRe,Vf,TV,pRe,_Re,j0,uRe,bRe,vRe,Wf,FV,TRe,FRe,N0,CRe,MRe,ERe,Qf,CV,yRe,wRe,D0,ARe,LRe,BRe,Hf,MV,xRe,kRe,q0,RRe,SRe,PRe,Uf,EV,$Re,IRe,G0,jRe,NRe,DRe,Jf,yV,qRe,GRe,O0,ORe,XRe,zRe,Yf,wV,VRe,WRe,X0,QRe,HRe,URe,Kf,AV,JRe,YRe,z0,KRe,ZRe,eSe,Zf,LV,oSe,rSe,V0,tSe,aSe,nSe,em,BV,sSe,lSe,W0,iSe,dSe,cSe,om,xV,fSe,mSe,Q0,gSe,hSe,pSe,rm,kV,_Se,uSe,H0,bSe,vSe,TSe,tm,RV,FSe,CSe,U0,MSe,ESe,ySe,am,SV,wSe,ASe,J0,LSe,BSe,xSe,nm,PV,kSe,RSe,Y0,SSe,PSe,$Se,sm,$V,ISe,jSe,K0,NSe,DSe,qSe,lm,IV,GSe,OSe,Z0,XSe,zSe,VSe,im,jV,WSe,QSe,eL,HSe,USe,JSe,dm,NV,YSe,KSe,oL,ZSe,ePe,oPe,cm,DV,rPe,tPe,rL,aPe,nPe,sPe,fm,qV,lPe,iPe,tL,dPe,cPe,fPe,mm,GV,mPe,gPe,aL,hPe,pPe,_Pe,gm,OV,uPe,bPe,nL,vPe,TPe,FPe,hm,XV,CPe,MPe,sL,EPe,yPe,wPe,pm,zV,APe,LPe,lL,BPe,xPe,kPe,_m,VV,RPe,SPe,iL,PPe,$Pe,IPe,um,WV,jPe,NPe,dL,DPe,qPe,GPe,bm,QV,OPe,XPe,cL,zPe,VPe,WPe,vm,HV,QPe,HPe,fL,UPe,JPe,YPe,Tm,UV,KPe,ZPe,mL,e$e,o$e,r$e,Fm,JV,t$e,a$e,gL,n$e,s$e,l$e,Cm,YV,i$e,d$e,hL,c$e,f$e,m$e,Mm,KV,g$e,h$e,pL,p$e,_$e,u$e,Em,ZV,b$e,v$e,_L,T$e,F$e,C$e,ym,eW,M$e,E$e,uL,y$e,w$e,A$e,wm,oW,L$e,B$e,bL,x$e,k$e,R$e,Am,rW,S$e,P$e,vL,$$e,I$e,j$e,Lm,tW,N$e,D$e,TL,q$e,G$e,O$e,Bm,aW,X$e,z$e,FL,V$e,W$e,Q$e,xm,nW,H$e,U$e,CL,J$e,Y$e,K$e,km,sW,Z$e,eIe,ML,oIe,rIe,tIe,Rm,lW,aIe,nIe,EL,sIe,lIe,iIe,Sm,iW,dIe,cIe,yL,fIe,mIe,gIe,Pm,dW,hIe,pIe,wL,_Ie,uIe,bIe,$m,cW,vIe,TIe,AL,FIe,CIe,MIe,Im,fW,EIe,yIe,LL,wIe,AIe,LIe,jm,mW,BIe,xIe,BL,kIe,RIe,SIe,Nm,gW,PIe,$Ie,xL,IIe,jIe,NIe,Dm,hW,DIe,qIe,kL,GIe,OIe,XIe,qm,pW,zIe,VIe,RL,WIe,QIe,HIe,Gm,_W,UIe,JIe,SL,YIe,KIe,ZIe,Om,uW,eje,oje,PL,rje,tje,aje,Xm,bW,nje,sje,$L,lje,ije,dje,zm,vW,cje,fje,IL,mje,gje,hje,Vm,TW,pje,_je,jL,uje,bje,vje,Wm,FW,Tje,Fje,NL,Cje,Mje,Eje,Qm,CW,yje,wje,DL,Aje,Lje,Bje,Hm,MW,xje,kje,qL,Rje,Sje,Pje,Um,EW,$je,Ije,GL,jje,Nje,Dje,Jm,yW,qje,Gje,OL,Oje,Xje,zje,Ym,wW,Vje,Wje,XL,Qje,Hje,Uje,Km,AW,Jje,Yje,zL,Kje,Zje,eNe,Zm,LW,oNe,rNe,VL,tNe,aNe,nNe,eg,BW,sNe,lNe,WL,iNe,dNe,cNe,xW,fNe,mNe,DC,gNe,og,qC,hNe,kW,pNe,yAe,Bi,rg,RW,GC,_Ne,SW,uNe,wAe,Go,OC,bNe,XC,vNe,QL,TNe,FNe,CNe,zC,MNe,PW,ENe,yNe,wNe,co,VC,ANe,$W,LNe,BNe,Sa,xNe,IW,kNe,RNe,jW,SNe,PNe,NW,$Ne,INe,jNe,M,Pn,DW,NNe,DNe,HL,qNe,GNe,UL,ONe,XNe,zNe,$n,qW,VNe,WNe,JL,QNe,HNe,YL,UNe,JNe,YNe,In,GW,KNe,ZNe,KL,eDe,oDe,ZL,rDe,tDe,aDe,tg,OW,nDe,sDe,e9,lDe,iDe,dDe,jn,XW,cDe,fDe,o9,mDe,gDe,r9,hDe,pDe,_De,ag,zW,uDe,bDe,t9,vDe,TDe,FDe,ng,VW,CDe,MDe,a9,EDe,yDe,wDe,sg,WW,ADe,LDe,n9,BDe,xDe,kDe,Nn,QW,RDe,SDe,s9,PDe,$De,l9,IDe,jDe,NDe,Dn,HW,DDe,qDe,i9,GDe,ODe,d9,XDe,zDe,VDe,qn,UW,WDe,QDe,c9,HDe,UDe,f9,JDe,YDe,KDe,lg,JW,ZDe,eqe,m9,oqe,rqe,tqe,ig,YW,aqe,nqe,g9,sqe,lqe,iqe,Gn,KW,dqe,cqe,h9,fqe,mqe,p9,gqe,hqe,pqe,dg,ZW,_qe,uqe,_9,bqe,vqe,Tqe,On,eQ,Fqe,Cqe,u9,Mqe,Eqe,b9,yqe,wqe,Aqe,Xn,oQ,Lqe,Bqe,v9,xqe,kqe,T9,Rqe,Sqe,Pqe,zn,rQ,$qe,Iqe,F9,jqe,Nqe,tQ,Dqe,qqe,Gqe,cg,aQ,Oqe,Xqe,C9,zqe,Vqe,Wqe,Vn,nQ,Qqe,Hqe,M9,Uqe,Jqe,E9,Yqe,Kqe,Zqe,fg,sQ,eGe,oGe,y9,rGe,tGe,aGe,Wn,lQ,nGe,sGe,w9,lGe,iGe,A9,dGe,cGe,fGe,Qn,iQ,mGe,gGe,L9,hGe,pGe,B9,_Ge,uGe,bGe,Hn,dQ,vGe,TGe,x9,FGe,CGe,k9,MGe,EGe,yGe,mg,cQ,wGe,AGe,R9,LGe,BGe,xGe,Un,fQ,kGe,RGe,S9,SGe,PGe,P9,$Ge,IGe,jGe,gg,mQ,NGe,DGe,$9,qGe,GGe,OGe,Jn,gQ,XGe,zGe,I9,VGe,WGe,j9,QGe,HGe,UGe,Yn,hQ,JGe,YGe,N9,KGe,ZGe,D9,eOe,oOe,rOe,Kn,pQ,tOe,aOe,q9,nOe,sOe,G9,lOe,iOe,dOe,Zn,_Q,cOe,fOe,O9,mOe,gOe,X9,hOe,pOe,_Oe,hg,uQ,uOe,bOe,z9,vOe,TOe,FOe,es,bQ,COe,MOe,V9,EOe,yOe,W9,wOe,AOe,LOe,os,vQ,BOe,xOe,Q9,kOe,ROe,H9,SOe,POe,$Oe,rs,TQ,IOe,jOe,U9,NOe,DOe,J9,qOe,GOe,OOe,ts,FQ,XOe,zOe,Y9,VOe,WOe,K9,QOe,HOe,UOe,as,CQ,JOe,YOe,Z9,KOe,ZOe,eB,eXe,oXe,rXe,ns,MQ,tXe,aXe,oB,nXe,sXe,rB,lXe,iXe,dXe,pg,EQ,cXe,fXe,tB,mXe,gXe,hXe,ss,yQ,pXe,_Xe,aB,uXe,bXe,nB,vXe,TXe,FXe,_g,wQ,CXe,MXe,sB,EXe,yXe,wXe,ug,AQ,AXe,LXe,lB,BXe,xXe,kXe,ls,LQ,RXe,SXe,iB,PXe,$Xe,dB,IXe,jXe,NXe,is,BQ,DXe,qXe,cB,GXe,OXe,fB,XXe,zXe,VXe,bg,xQ,WXe,QXe,mB,HXe,UXe,JXe,ds,kQ,YXe,KXe,gB,ZXe,eze,hB,oze,rze,tze,cs,RQ,aze,nze,pB,sze,lze,_B,ize,dze,cze,fs,SQ,fze,mze,uB,gze,hze,bB,pze,_ze,uze,ms,PQ,bze,vze,vB,Tze,Fze,TB,Cze,Mze,Eze,gs,$Q,yze,wze,FB,Aze,Lze,CB,Bze,xze,kze,vg,IQ,Rze,Sze,MB,Pze,$ze,Ize,Tg,jQ,jze,Nze,EB,Dze,qze,Gze,Fg,NQ,Oze,Xze,yB,zze,Vze,Wze,hs,DQ,Qze,Hze,wB,Uze,Jze,AB,Yze,Kze,Zze,Cg,qQ,eVe,oVe,LB,rVe,tVe,aVe,ps,GQ,nVe,sVe,BB,lVe,iVe,xB,dVe,cVe,fVe,_s,OQ,mVe,gVe,kB,hVe,pVe,RB,_Ve,uVe,bVe,us,XQ,vVe,TVe,SB,FVe,CVe,PB,MVe,EVe,yVe,bs,zQ,wVe,AVe,$B,LVe,BVe,IB,xVe,kVe,RVe,vs,VQ,SVe,PVe,jB,$Ve,IVe,NB,jVe,NVe,DVe,Mg,WQ,qVe,GVe,DB,OVe,XVe,zVe,Eg,QQ,VVe,WVe,qB,QVe,HVe,UVe,Ts,HQ,JVe,YVe,GB,KVe,ZVe,OB,eWe,oWe,rWe,Fs,UQ,tWe,aWe,XB,nWe,sWe,zB,lWe,iWe,dWe,Cs,JQ,cWe,fWe,VB,mWe,gWe,WB,hWe,pWe,_We,yg,YQ,uWe,bWe,QB,vWe,TWe,FWe,wg,KQ,CWe,MWe,HB,EWe,yWe,wWe,Ag,ZQ,AWe,LWe,UB,BWe,xWe,kWe,Lg,eH,RWe,SWe,JB,PWe,$We,IWe,Ms,oH,jWe,NWe,YB,DWe,qWe,KB,GWe,OWe,XWe,Bg,rH,zWe,VWe,ZB,WWe,QWe,HWe,xg,tH,UWe,JWe,ex,YWe,KWe,ZWe,Es,aH,eQe,oQe,ox,rQe,tQe,rx,aQe,nQe,sQe,ys,nH,lQe,iQe,tx,dQe,cQe,ax,fQe,mQe,gQe,sH,hQe,pQe,WC,_Qe,kg,QC,uQe,lH,bQe,AAe,xi,Rg,iH,HC,vQe,dH,TQe,LAe,Oo,UC,FQe,JC,CQe,nx,MQe,EQe,yQe,YC,wQe,cH,AQe,LQe,BQe,Le,KC,xQe,fH,kQe,RQe,Pa,SQe,mH,PQe,$Qe,gH,IQe,jQe,hH,NQe,DQe,qQe,se,Sg,pH,GQe,OQe,sx,XQe,zQe,VQe,Pg,_H,WQe,QQe,lx,HQe,UQe,JQe,$g,uH,YQe,KQe,ix,ZQe,eHe,oHe,Ig,bH,rHe,tHe,dx,aHe,nHe,sHe,jg,vH,lHe,iHe,cx,dHe,cHe,fHe,Ng,TH,mHe,gHe,fx,hHe,pHe,_He,Dg,FH,uHe,bHe,mx,vHe,THe,FHe,qg,CH,CHe,MHe,gx,EHe,yHe,wHe,Gg,MH,AHe,LHe,hx,BHe,xHe,kHe,Og,EH,RHe,SHe,px,PHe,$He,IHe,Xg,yH,jHe,NHe,_x,DHe,qHe,GHe,zg,wH,OHe,XHe,ux,zHe,VHe,WHe,Vg,AH,QHe,HHe,bx,UHe,JHe,YHe,Wg,LH,KHe,ZHe,vx,eUe,oUe,rUe,Qg,tUe,BH,aUe,nUe,ZC,sUe,Hg,e4,lUe,xH,iUe,BAe,ki,Ug,kH,o4,dUe,RH,cUe,xAe,Xo,r4,fUe,t4,mUe,Tx,gUe,hUe,pUe,a4,_Ue,SH,uUe,bUe,vUe,Be,n4,TUe,PH,FUe,CUe,Ri,MUe,$H,EUe,yUe,IH,wUe,AUe,LUe,ye,Jg,jH,BUe,xUe,Fx,kUe,RUe,SUe,Yg,NH,PUe,$Ue,Cx,IUe,jUe,NUe,Kg,DH,DUe,qUe,Mx,GUe,OUe,XUe,Zg,qH,zUe,VUe,Ex,WUe,QUe,HUe,eh,GH,UUe,JUe,yx,YUe,KUe,ZUe,oh,OH,eJe,oJe,wx,rJe,tJe,aJe,rh,XH,nJe,sJe,Ax,lJe,iJe,dJe,th,zH,cJe,fJe,Lx,mJe,gJe,hJe,ah,pJe,VH,_Je,uJe,s4,bJe,nh,l4,vJe,WH,TJe,kAe,Si,sh,QH,i4,FJe,HH,CJe,RAe,zo,d4,MJe,Pi,EJe,UH,yJe,wJe,JH,AJe,LJe,BJe,c4,xJe,YH,kJe,RJe,SJe,Ir,f4,PJe,KH,$Je,IJe,$i,jJe,ZH,NJe,DJe,eU,qJe,GJe,OJe,oU,XJe,zJe,m4,VJe,xe,g4,WJe,rU,QJe,HJe,$a,UJe,tU,JJe,YJe,aU,KJe,ZJe,nU,eYe,oYe,rYe,F,lh,sU,tYe,aYe,Bx,nYe,sYe,lYe,ih,lU,iYe,dYe,xx,cYe,fYe,mYe,dh,iU,gYe,hYe,kx,pYe,_Ye,uYe,ch,dU,bYe,vYe,Rx,TYe,FYe,CYe,fh,cU,MYe,EYe,Sx,yYe,wYe,AYe,mh,fU,LYe,BYe,Px,xYe,kYe,RYe,gh,mU,SYe,PYe,$x,$Ye,IYe,jYe,hh,gU,NYe,DYe,Ix,qYe,GYe,OYe,ph,hU,XYe,zYe,jx,VYe,WYe,QYe,_h,pU,HYe,UYe,Nx,JYe,YYe,KYe,uh,_U,ZYe,eKe,Dx,oKe,rKe,tKe,bh,uU,aKe,nKe,qx,sKe,lKe,iKe,vh,bU,dKe,cKe,Gx,fKe,mKe,gKe,Th,vU,hKe,pKe,Ox,_Ke,uKe,bKe,Fh,TU,vKe,TKe,Xx,FKe,CKe,MKe,Ch,FU,EKe,yKe,zx,wKe,AKe,LKe,Mh,CU,BKe,xKe,Vx,kKe,RKe,SKe,Eh,MU,PKe,$Ke,Wx,IKe,jKe,NKe,yh,EU,DKe,qKe,Qx,GKe,OKe,XKe,wh,yU,zKe,VKe,Hx,WKe,QKe,HKe,Ah,wU,UKe,JKe,Ux,YKe,KKe,ZKe,Lh,AU,eZe,oZe,Jx,rZe,tZe,aZe,Bh,LU,nZe,sZe,Yx,lZe,iZe,dZe,xh,BU,cZe,fZe,Kx,mZe,gZe,hZe,kh,xU,pZe,_Ze,Zx,uZe,bZe,vZe,ws,kU,TZe,FZe,ek,CZe,MZe,ok,EZe,yZe,wZe,Rh,RU,AZe,LZe,rk,BZe,xZe,kZe,Sh,SU,RZe,SZe,tk,PZe,$Ze,IZe,Ph,PU,jZe,NZe,ak,DZe,qZe,GZe,$h,$U,OZe,XZe,nk,zZe,VZe,WZe,Ih,IU,QZe,HZe,sk,UZe,JZe,YZe,jh,jU,KZe,ZZe,lk,eeo,oeo,reo,Nh,NU,teo,aeo,ik,neo,seo,leo,Dh,DU,ieo,deo,dk,ceo,feo,meo,qh,qU,geo,heo,ck,peo,_eo,ueo,Gh,GU,beo,veo,fk,Teo,Feo,Ceo,Oh,OU,Meo,Eeo,mk,yeo,weo,Aeo,Xh,XU,Leo,Beo,gk,xeo,keo,Reo,zh,zU,Seo,Peo,hk,$eo,Ieo,jeo,Vh,VU,Neo,Deo,pk,qeo,Geo,Oeo,Wh,WU,Xeo,zeo,_k,Veo,Weo,Qeo,Qh,QU,Heo,Ueo,uk,Jeo,Yeo,Keo,Hh,HU,Zeo,eoo,bk,ooo,roo,too,Uh,UU,aoo,noo,vk,soo,loo,ioo,Jh,JU,doo,coo,Tk,foo,moo,goo,Yh,YU,hoo,poo,Fk,_oo,uoo,boo,Kh,KU,voo,Too,Ck,Foo,Coo,Moo,Zh,ZU,Eoo,yoo,Mk,woo,Aoo,Loo,ep,eJ,Boo,xoo,Ek,koo,Roo,Soo,op,oJ,Poo,$oo,yk,Ioo,joo,Noo,rp,rJ,Doo,qoo,wk,Goo,Ooo,Xoo,tp,tJ,zoo,Voo,Ak,Woo,Qoo,Hoo,ap,aJ,Uoo,Joo,Lk,Yoo,Koo,Zoo,np,nJ,ero,oro,Bk,rro,tro,aro,sp,sJ,nro,sro,xk,lro,iro,dro,lp,lJ,cro,fro,kk,mro,gro,hro,ip,iJ,pro,_ro,Rk,uro,bro,vro,dp,dJ,Tro,Fro,Sk,Cro,Mro,Ero,cp,cJ,yro,wro,Pk,Aro,Lro,Bro,fp,fJ,xro,kro,$k,Rro,Sro,Pro,mp,mJ,$ro,Iro,Ik,jro,Nro,Dro,gp,gJ,qro,Gro,jk,Oro,Xro,zro,hp,hJ,Vro,Wro,Nk,Qro,Hro,Uro,pp,pJ,Jro,Yro,Dk,Kro,Zro,eto,_p,_J,oto,rto,qk,tto,ato,nto,up,uJ,sto,lto,Gk,ito,dto,cto,bp,bJ,fto,mto,Ok,gto,hto,pto,vp,vJ,_to,uto,Xk,bto,vto,Tto,Tp,TJ,Fto,Cto,zk,Mto,Eto,yto,Fp,FJ,wto,Ato,Vk,Lto,Bto,xto,Cp,CJ,kto,Rto,Wk,Sto,Pto,$to,Mp,MJ,Ito,jto,Qk,Nto,Dto,qto,Ep,EJ,Gto,Oto,Hk,Xto,zto,Vto,yp,yJ,Wto,Qto,Uk,Hto,Uto,Jto,wp,wJ,Yto,Kto,Jk,Zto,eao,oao,Ap,AJ,rao,tao,Yk,aao,nao,sao,Lp,LJ,lao,iao,Kk,dao,cao,fao,Bp,BJ,mao,gao,Zk,hao,pao,_ao,xp,xJ,uao,bao,eR,vao,Tao,Fao,kp,kJ,Cao,Mao,oR,Eao,yao,wao,Rp,RJ,Aao,Lao,rR,Bao,xao,kao,Sp,SJ,Rao,Sao,tR,Pao,$ao,Iao,Pp,jao,PJ,Nao,Dao,$J,qao,Gao,IJ,Oao,Xao,h4,SAe,Ii,$p,jJ,p4,zao,NJ,Vao,PAe,Vo,_4,Wao,ji,Qao,DJ,Hao,Uao,qJ,Jao,Yao,Kao,u4,Zao,GJ,eno,ono,rno,jr,b4,tno,OJ,ano,nno,Ni,sno,XJ,lno,ino,zJ,dno,cno,fno,VJ,mno,gno,v4,hno,ke,T4,pno,WJ,_no,uno,Ia,bno,QJ,vno,Tno,HJ,Fno,Cno,UJ,Mno,Eno,yno,k,Ip,JJ,wno,Ano,aR,Lno,Bno,xno,jp,YJ,kno,Rno,nR,Sno,Pno,$no,Np,KJ,Ino,jno,sR,Nno,Dno,qno,Dp,ZJ,Gno,Ono,lR,Xno,zno,Vno,qp,eY,Wno,Qno,iR,Hno,Uno,Jno,Gp,oY,Yno,Kno,dR,Zno,eso,oso,Op,rY,rso,tso,cR,aso,nso,sso,Xp,tY,lso,iso,fR,dso,cso,fso,zp,aY,mso,gso,mR,hso,pso,_so,Vp,nY,uso,bso,gR,vso,Tso,Fso,Wp,sY,Cso,Mso,hR,Eso,yso,wso,Qp,lY,Aso,Lso,pR,Bso,xso,kso,Hp,iY,Rso,Sso,_R,Pso,$so,Iso,Up,dY,jso,Nso,uR,Dso,qso,Gso,Jp,cY,Oso,Xso,bR,zso,Vso,Wso,Yp,fY,Qso,Hso,vR,Uso,Jso,Yso,Kp,mY,Kso,Zso,TR,elo,olo,rlo,Zp,gY,tlo,alo,FR,nlo,slo,llo,e_,hY,ilo,dlo,CR,clo,flo,mlo,o_,pY,glo,hlo,MR,plo,_lo,ulo,r_,_Y,blo,vlo,ER,Tlo,Flo,Clo,t_,uY,Mlo,Elo,yR,ylo,wlo,Alo,a_,bY,Llo,Blo,wR,xlo,klo,Rlo,n_,vY,Slo,Plo,AR,$lo,Ilo,jlo,s_,TY,Nlo,Dlo,LR,qlo,Glo,Olo,l_,FY,Xlo,zlo,BR,Vlo,Wlo,Qlo,i_,CY,Hlo,Ulo,xR,Jlo,Ylo,Klo,d_,MY,Zlo,eio,kR,oio,rio,tio,c_,EY,aio,nio,RR,sio,lio,iio,f_,yY,dio,cio,SR,fio,mio,gio,m_,wY,hio,pio,PR,_io,uio,bio,g_,AY,vio,Tio,$R,Fio,Cio,Mio,h_,LY,Eio,yio,IR,wio,Aio,Lio,p_,BY,Bio,xio,jR,kio,Rio,Sio,__,xY,Pio,$io,NR,Iio,jio,Nio,u_,kY,Dio,qio,DR,Gio,Oio,Xio,b_,RY,zio,Vio,qR,Wio,Qio,Hio,v_,SY,Uio,Jio,GR,Yio,Kio,Zio,T_,edo,PY,odo,rdo,$Y,tdo,ado,IY,ndo,sdo,F4,$Ae,Di,F_,jY,C4,ldo,NY,ido,IAe,Wo,M4,ddo,qi,cdo,DY,fdo,mdo,qY,gdo,hdo,pdo,E4,_do,GY,udo,bdo,vdo,Nr,y4,Tdo,OY,Fdo,Cdo,Gi,Mdo,XY,Edo,ydo,zY,wdo,Ado,Ldo,VY,Bdo,xdo,w4,kdo,Re,A4,Rdo,WY,Sdo,Pdo,ja,$do,QY,Ido,jdo,HY,Ndo,Ddo,UY,qdo,Gdo,Odo,I,C_,JY,Xdo,zdo,OR,Vdo,Wdo,Qdo,M_,YY,Hdo,Udo,XR,Jdo,Ydo,Kdo,E_,KY,Zdo,eco,zR,oco,rco,tco,y_,ZY,aco,nco,VR,sco,lco,ico,w_,eK,dco,cco,WR,fco,mco,gco,A_,oK,hco,pco,QR,_co,uco,bco,L_,rK,vco,Tco,HR,Fco,Cco,Mco,B_,tK,Eco,yco,UR,wco,Aco,Lco,x_,aK,Bco,xco,JR,kco,Rco,Sco,k_,nK,Pco,$co,YR,Ico,jco,Nco,R_,sK,Dco,qco,KR,Gco,Oco,Xco,S_,lK,zco,Vco,ZR,Wco,Qco,Hco,P_,iK,Uco,Jco,eS,Yco,Kco,Zco,$_,dK,efo,ofo,oS,rfo,tfo,afo,I_,cK,nfo,sfo,rS,lfo,ifo,dfo,j_,fK,cfo,ffo,tS,mfo,gfo,hfo,N_,mK,pfo,_fo,aS,ufo,bfo,vfo,D_,gK,Tfo,Ffo,nS,Cfo,Mfo,Efo,q_,hK,yfo,wfo,sS,Afo,Lfo,Bfo,G_,pK,xfo,kfo,lS,Rfo,Sfo,Pfo,O_,_K,$fo,Ifo,iS,jfo,Nfo,Dfo,X_,uK,qfo,Gfo,dS,Ofo,Xfo,zfo,z_,bK,Vfo,Wfo,cS,Qfo,Hfo,Ufo,V_,vK,Jfo,Yfo,fS,Kfo,Zfo,emo,W_,TK,omo,rmo,mS,tmo,amo,nmo,Q_,FK,smo,lmo,gS,imo,dmo,cmo,H_,CK,fmo,mmo,hS,gmo,hmo,pmo,U_,MK,_mo,umo,pS,bmo,vmo,Tmo,J_,EK,Fmo,Cmo,_S,Mmo,Emo,ymo,Y_,yK,wmo,Amo,uS,Lmo,Bmo,xmo,K_,wK,kmo,Rmo,bS,Smo,Pmo,$mo,Z_,AK,Imo,jmo,vS,Nmo,Dmo,qmo,eu,LK,Gmo,Omo,TS,Xmo,zmo,Vmo,ou,Wmo,BK,Qmo,Hmo,xK,Umo,Jmo,kK,Ymo,Kmo,L4,jAe,Oi,ru,RK,B4,Zmo,SK,ego,NAe,Qo,x4,ogo,Xi,rgo,PK,tgo,ago,$K,ngo,sgo,lgo,k4,igo,IK,dgo,cgo,fgo,Dr,R4,mgo,jK,ggo,hgo,zi,pgo,NK,_go,ugo,DK,bgo,vgo,Tgo,qK,Fgo,Cgo,S4,Mgo,Se,P4,Ego,GK,ygo,wgo,Na,Ago,OK,Lgo,Bgo,XK,xgo,kgo,zK,Rgo,Sgo,Pgo,$,tu,VK,$go,Igo,FS,jgo,Ngo,Dgo,au,WK,qgo,Ggo,CS,Ogo,Xgo,zgo,nu,QK,Vgo,Wgo,MS,Qgo,Hgo,Ugo,su,HK,Jgo,Ygo,ES,Kgo,Zgo,eho,lu,UK,oho,rho,yS,tho,aho,nho,iu,JK,sho,lho,wS,iho,dho,cho,du,YK,fho,mho,AS,gho,hho,pho,cu,KK,_ho,uho,LS,bho,vho,Tho,fu,ZK,Fho,Cho,BS,Mho,Eho,yho,mu,eZ,who,Aho,xS,Lho,Bho,xho,gu,oZ,kho,Rho,kS,Sho,Pho,$ho,hu,rZ,Iho,jho,RS,Nho,Dho,qho,pu,tZ,Gho,Oho,SS,Xho,zho,Vho,_u,aZ,Who,Qho,PS,Hho,Uho,Jho,uu,nZ,Yho,Kho,$S,Zho,epo,opo,bu,sZ,rpo,tpo,IS,apo,npo,spo,vu,lZ,lpo,ipo,jS,dpo,cpo,fpo,Tu,iZ,mpo,gpo,NS,hpo,ppo,_po,Fu,dZ,upo,bpo,DS,vpo,Tpo,Fpo,Cu,cZ,Cpo,Mpo,qS,Epo,ypo,wpo,Mu,fZ,Apo,Lpo,GS,Bpo,xpo,kpo,Eu,mZ,Rpo,Spo,OS,Ppo,$po,Ipo,yu,gZ,jpo,Npo,XS,Dpo,qpo,Gpo,wu,hZ,Opo,Xpo,zS,zpo,Vpo,Wpo,Au,pZ,Qpo,Hpo,VS,Upo,Jpo,Ypo,Lu,_Z,Kpo,Zpo,WS,e_o,o_o,r_o,Bu,uZ,t_o,a_o,QS,n_o,s_o,l_o,xu,bZ,i_o,d_o,HS,c_o,f_o,m_o,ku,vZ,g_o,h_o,US,p_o,__o,u_o,Ru,TZ,b_o,v_o,FZ,T_o,F_o,C_o,Su,CZ,M_o,E_o,JS,y_o,w_o,A_o,Pu,MZ,L_o,B_o,YS,x_o,k_o,R_o,$u,EZ,S_o,P_o,KS,$_o,I_o,j_o,Iu,yZ,N_o,D_o,ZS,q_o,G_o,O_o,ju,X_o,wZ,z_o,V_o,AZ,W_o,Q_o,LZ,H_o,U_o,$4,DAe,Vi,Nu,BZ,I4,J_o,xZ,Y_o,qAe,Ho,j4,K_o,Wi,Z_o,kZ,euo,ouo,RZ,ruo,tuo,auo,N4,nuo,SZ,suo,luo,iuo,qr,D4,duo,PZ,cuo,fuo,Qi,muo,$Z,guo,huo,IZ,puo,_uo,uuo,jZ,buo,vuo,q4,Tuo,Pe,G4,Fuo,NZ,Cuo,Muo,Da,Euo,DZ,yuo,wuo,qZ,Auo,Luo,GZ,Buo,xuo,kuo,ne,Du,OZ,Ruo,Suo,eP,Puo,$uo,Iuo,qu,XZ,juo,Nuo,oP,Duo,quo,Guo,Gu,zZ,Ouo,Xuo,rP,zuo,Vuo,Wuo,Ou,VZ,Quo,Huo,tP,Uuo,Juo,Yuo,Xu,WZ,Kuo,Zuo,aP,e1o,o1o,r1o,zu,QZ,t1o,a1o,nP,n1o,s1o,l1o,Vu,HZ,i1o,d1o,sP,c1o,f1o,m1o,Wu,UZ,g1o,h1o,lP,p1o,_1o,u1o,Qu,JZ,b1o,v1o,iP,T1o,F1o,C1o,Hu,YZ,M1o,E1o,dP,y1o,w1o,A1o,Uu,KZ,L1o,B1o,cP,x1o,k1o,R1o,Ju,ZZ,S1o,P1o,fP,$1o,I1o,j1o,Yu,eee,N1o,D1o,mP,q1o,G1o,O1o,Ku,oee,X1o,z1o,gP,V1o,W1o,Q1o,Zu,ree,H1o,U1o,hP,J1o,Y1o,K1o,e1,Z1o,tee,ebo,obo,aee,rbo,tbo,nee,abo,nbo,O4,GAe,Hi,o1,see,X4,sbo,lee,lbo,OAe,Uo,z4,ibo,Ui,dbo,iee,cbo,fbo,dee,mbo,gbo,hbo,V4,pbo,cee,_bo,ubo,bbo,Gr,W4,vbo,fee,Tbo,Fbo,Ji,Cbo,mee,Mbo,Ebo,gee,ybo,wbo,Abo,hee,Lbo,Bbo,Q4,xbo,$e,H4,kbo,pee,Rbo,Sbo,qa,Pbo,_ee,$bo,Ibo,uee,jbo,Nbo,bee,Dbo,qbo,Gbo,A,r1,vee,Obo,Xbo,pP,zbo,Vbo,Wbo,t1,Tee,Qbo,Hbo,_P,Ubo,Jbo,Ybo,a1,Fee,Kbo,Zbo,uP,e5o,o5o,r5o,n1,Cee,t5o,a5o,bP,n5o,s5o,l5o,s1,Mee,i5o,d5o,vP,c5o,f5o,m5o,l1,Eee,g5o,h5o,TP,p5o,_5o,u5o,i1,yee,b5o,v5o,FP,T5o,F5o,C5o,d1,wee,M5o,E5o,CP,y5o,w5o,A5o,c1,Aee,L5o,B5o,MP,x5o,k5o,R5o,f1,Lee,S5o,P5o,EP,$5o,I5o,j5o,m1,Bee,N5o,D5o,yP,q5o,G5o,O5o,g1,xee,X5o,z5o,wP,V5o,W5o,Q5o,h1,kee,H5o,U5o,AP,J5o,Y5o,K5o,p1,Ree,Z5o,e2o,LP,o2o,r2o,t2o,_1,See,a2o,n2o,BP,s2o,l2o,i2o,u1,Pee,d2o,c2o,xP,f2o,m2o,g2o,b1,$ee,h2o,p2o,kP,_2o,u2o,b2o,v1,Iee,v2o,T2o,RP,F2o,C2o,M2o,T1,jee,E2o,y2o,SP,w2o,A2o,L2o,F1,Nee,B2o,x2o,PP,k2o,R2o,S2o,C1,Dee,P2o,$2o,$P,I2o,j2o,N2o,M1,qee,D2o,q2o,IP,G2o,O2o,X2o,E1,Gee,z2o,V2o,jP,W2o,Q2o,H2o,y1,Oee,U2o,J2o,NP,Y2o,K2o,Z2o,w1,Xee,evo,ovo,DP,rvo,tvo,avo,A1,zee,nvo,svo,qP,lvo,ivo,dvo,L1,Vee,cvo,fvo,GP,mvo,gvo,hvo,B1,Wee,pvo,_vo,OP,uvo,bvo,vvo,x1,Qee,Tvo,Fvo,XP,Cvo,Mvo,Evo,k1,Hee,yvo,wvo,zP,Avo,Lvo,Bvo,R1,Uee,xvo,kvo,VP,Rvo,Svo,Pvo,S1,Jee,$vo,Ivo,WP,jvo,Nvo,Dvo,P1,Yee,qvo,Gvo,QP,Ovo,Xvo,zvo,$1,Kee,Vvo,Wvo,HP,Qvo,Hvo,Uvo,I1,Zee,Jvo,Yvo,UP,Kvo,Zvo,e6o,j1,eoe,o6o,r6o,JP,t6o,a6o,n6o,N1,ooe,s6o,l6o,YP,i6o,d6o,c6o,D1,roe,f6o,m6o,KP,g6o,h6o,p6o,q1,toe,_6o,u6o,ZP,b6o,v6o,T6o,G1,aoe,F6o,C6o,e$,M6o,E6o,y6o,O1,noe,w6o,A6o,o$,L6o,B6o,x6o,X1,soe,k6o,R6o,r$,S6o,P6o,$6o,z1,loe,I6o,j6o,t$,N6o,D6o,q6o,V1,ioe,G6o,O6o,a$,X6o,z6o,V6o,W1,W6o,doe,Q6o,H6o,coe,U6o,J6o,foe,Y6o,K6o,U4,XAe,Yi,Q1,moe,J4,Z6o,goe,eTo,zAe,Jo,Y4,oTo,Ki,rTo,hoe,tTo,aTo,poe,nTo,sTo,lTo,K4,iTo,_oe,dTo,cTo,fTo,Or,Z4,mTo,uoe,gTo,hTo,Zi,pTo,boe,_To,uTo,voe,bTo,vTo,TTo,Toe,FTo,CTo,eM,MTo,Ie,oM,ETo,Foe,yTo,wTo,Ga,ATo,Coe,LTo,BTo,Moe,xTo,kTo,Eoe,RTo,STo,PTo,G,H1,yoe,$To,ITo,n$,jTo,NTo,DTo,U1,woe,qTo,GTo,s$,OTo,XTo,zTo,J1,Aoe,VTo,WTo,l$,QTo,HTo,UTo,Y1,Loe,JTo,YTo,i$,KTo,ZTo,e7o,K1,Boe,o7o,r7o,d$,t7o,a7o,n7o,Z1,xoe,s7o,l7o,c$,i7o,d7o,c7o,eb,koe,f7o,m7o,f$,g7o,h7o,p7o,ob,Roe,_7o,u7o,m$,b7o,v7o,T7o,rb,Soe,F7o,C7o,g$,M7o,E7o,y7o,tb,Poe,w7o,A7o,h$,L7o,B7o,x7o,ab,$oe,k7o,R7o,p$,S7o,P7o,$7o,nb,Ioe,I7o,j7o,_$,N7o,D7o,q7o,sb,joe,G7o,O7o,u$,X7o,z7o,V7o,lb,Noe,W7o,Q7o,b$,H7o,U7o,J7o,ib,Doe,Y7o,K7o,v$,Z7o,e8o,o8o,db,qoe,r8o,t8o,T$,a8o,n8o,s8o,cb,Goe,l8o,i8o,F$,d8o,c8o,f8o,fb,Ooe,m8o,g8o,C$,h8o,p8o,_8o,mb,Xoe,u8o,b8o,M$,v8o,T8o,F8o,gb,zoe,C8o,M8o,E$,E8o,y8o,w8o,hb,Voe,A8o,L8o,y$,B8o,x8o,k8o,pb,Woe,R8o,S8o,w$,P8o,$8o,I8o,_b,Qoe,j8o,N8o,A$,D8o,q8o,G8o,ub,Hoe,O8o,X8o,L$,z8o,V8o,W8o,bb,Uoe,Q8o,H8o,B$,U8o,J8o,Y8o,vb,Joe,K8o,Z8o,x$,eFo,oFo,rFo,Tb,Yoe,tFo,aFo,k$,nFo,sFo,lFo,Fb,iFo,Koe,dFo,cFo,Zoe,fFo,mFo,ere,gFo,hFo,rM,VAe,ed,Cb,ore,tM,pFo,rre,_Fo,WAe,Yo,aM,uFo,od,bFo,tre,vFo,TFo,are,FFo,CFo,MFo,nM,EFo,nre,yFo,wFo,AFo,Xr,sM,LFo,sre,BFo,xFo,rd,kFo,lre,RFo,SFo,ire,PFo,$Fo,IFo,dre,jFo,NFo,lM,DFo,je,iM,qFo,cre,GFo,OFo,Oa,XFo,fre,zFo,VFo,mre,WFo,QFo,gre,HFo,UFo,JFo,oa,Mb,hre,YFo,KFo,R$,ZFo,eCo,oCo,Eb,pre,rCo,tCo,S$,aCo,nCo,sCo,yb,_re,lCo,iCo,P$,dCo,cCo,fCo,wb,ure,mCo,gCo,$$,hCo,pCo,_Co,Ab,bre,uCo,bCo,I$,vCo,TCo,FCo,Lb,CCo,vre,MCo,ECo,Tre,yCo,wCo,Fre,ACo,LCo,dM,QAe,td,Bb,Cre,cM,BCo,Mre,xCo,HAe,Ko,fM,kCo,ad,RCo,Ere,SCo,PCo,yre,$Co,ICo,jCo,mM,NCo,wre,DCo,qCo,GCo,zr,gM,OCo,Are,XCo,zCo,nd,VCo,Lre,WCo,QCo,Bre,HCo,UCo,JCo,xre,YCo,KCo,hM,ZCo,Ne,pM,e4o,kre,o4o,r4o,Xa,t4o,Rre,a4o,n4o,Sre,s4o,l4o,Pre,i4o,d4o,c4o,N,xb,$re,f4o,m4o,j$,g4o,h4o,p4o,kb,Ire,_4o,u4o,N$,b4o,v4o,T4o,Rb,jre,F4o,C4o,D$,M4o,E4o,y4o,Sb,Nre,w4o,A4o,q$,L4o,B4o,x4o,Pb,Dre,k4o,R4o,G$,S4o,P4o,$4o,$b,qre,I4o,j4o,O$,N4o,D4o,q4o,Ib,Gre,G4o,O4o,X$,X4o,z4o,V4o,jb,Ore,W4o,Q4o,z$,H4o,U4o,J4o,Nb,Xre,Y4o,K4o,V$,Z4o,eMo,oMo,Db,zre,rMo,tMo,W$,aMo,nMo,sMo,qb,Vre,lMo,iMo,Q$,dMo,cMo,fMo,Gb,Wre,mMo,gMo,H$,hMo,pMo,_Mo,Ob,Qre,uMo,bMo,U$,vMo,TMo,FMo,Xb,Hre,CMo,MMo,J$,EMo,yMo,wMo,zb,Ure,AMo,LMo,Y$,BMo,xMo,kMo,Vb,Jre,RMo,SMo,K$,PMo,$Mo,IMo,Wb,Yre,jMo,NMo,Z$,DMo,qMo,GMo,Qb,Kre,OMo,XMo,eI,zMo,VMo,WMo,Hb,Zre,QMo,HMo,oI,UMo,JMo,YMo,Ub,ete,KMo,ZMo,rI,eEo,oEo,rEo,Jb,ote,tEo,aEo,tI,nEo,sEo,lEo,Yb,rte,iEo,dEo,aI,cEo,fEo,mEo,Kb,tte,gEo,hEo,nI,pEo,_Eo,uEo,Zb,ate,bEo,vEo,sI,TEo,FEo,CEo,e5,nte,MEo,EEo,lI,yEo,wEo,AEo,o5,ste,LEo,BEo,iI,xEo,kEo,REo,r5,lte,SEo,PEo,dI,$Eo,IEo,jEo,t5,ite,NEo,DEo,cI,qEo,GEo,OEo,a5,dte,XEo,zEo,fI,VEo,WEo,QEo,n5,cte,HEo,UEo,mI,JEo,YEo,KEo,s5,fte,ZEo,e3o,gI,o3o,r3o,t3o,l5,mte,a3o,n3o,hI,s3o,l3o,i3o,i5,d3o,gte,c3o,f3o,hte,m3o,g3o,pte,h3o,p3o,_M,UAe,sd,d5,_te,uM,_3o,ute,u3o,JAe,Zo,bM,b3o,ld,v3o,bte,T3o,F3o,vte,C3o,M3o,E3o,vM,y3o,Tte,w3o,A3o,L3o,Vr,TM,B3o,Fte,x3o,k3o,id,R3o,Cte,S3o,P3o,Mte,$3o,I3o,j3o,Ete,N3o,D3o,FM,q3o,De,CM,G3o,yte,O3o,X3o,za,z3o,wte,V3o,W3o,Ate,Q3o,H3o,Lte,U3o,J3o,Y3o,R,c5,Bte,K3o,Z3o,pI,eyo,oyo,ryo,f5,xte,tyo,ayo,_I,nyo,syo,lyo,m5,kte,iyo,dyo,uI,cyo,fyo,myo,g5,Rte,gyo,hyo,bI,pyo,_yo,uyo,h5,Ste,byo,vyo,vI,Tyo,Fyo,Cyo,p5,Pte,Myo,Eyo,TI,yyo,wyo,Ayo,_5,$te,Lyo,Byo,FI,xyo,kyo,Ryo,u5,Ite,Syo,Pyo,CI,$yo,Iyo,jyo,b5,jte,Nyo,Dyo,MI,qyo,Gyo,Oyo,v5,Nte,Xyo,zyo,EI,Vyo,Wyo,Qyo,T5,Dte,Hyo,Uyo,yI,Jyo,Yyo,Kyo,F5,qte,Zyo,ewo,wI,owo,rwo,two,C5,Gte,awo,nwo,AI,swo,lwo,iwo,M5,Ote,dwo,cwo,LI,fwo,mwo,gwo,E5,Xte,hwo,pwo,BI,_wo,uwo,bwo,y5,zte,vwo,Two,xI,Fwo,Cwo,Mwo,w5,Vte,Ewo,ywo,kI,wwo,Awo,Lwo,A5,Wte,Bwo,xwo,RI,kwo,Rwo,Swo,L5,Qte,Pwo,$wo,SI,Iwo,jwo,Nwo,B5,Hte,Dwo,qwo,PI,Gwo,Owo,Xwo,x5,Ute,zwo,Vwo,$I,Wwo,Qwo,Hwo,k5,Jte,Uwo,Jwo,II,Ywo,Kwo,Zwo,R5,Yte,eAo,oAo,jI,rAo,tAo,aAo,S5,Kte,nAo,sAo,NI,lAo,iAo,dAo,P5,Zte,cAo,fAo,DI,mAo,gAo,hAo,$5,eae,pAo,_Ao,qI,uAo,bAo,vAo,I5,oae,TAo,FAo,GI,CAo,MAo,EAo,j5,rae,yAo,wAo,OI,AAo,LAo,BAo,N5,tae,xAo,kAo,XI,RAo,SAo,PAo,D5,aae,$Ao,IAo,zI,jAo,NAo,DAo,q5,nae,qAo,GAo,VI,OAo,XAo,zAo,G5,sae,VAo,WAo,WI,QAo,HAo,UAo,O5,lae,JAo,YAo,QI,KAo,ZAo,e0o,X5,iae,o0o,r0o,HI,t0o,a0o,n0o,z5,dae,s0o,l0o,UI,i0o,d0o,c0o,V5,cae,f0o,m0o,JI,g0o,h0o,p0o,W5,fae,_0o,u0o,YI,b0o,v0o,T0o,Q5,mae,F0o,C0o,KI,M0o,E0o,y0o,H5,w0o,gae,A0o,L0o,hae,B0o,x0o,pae,k0o,R0o,MM,YAe,dd,U5,_ae,EM,S0o,uae,P0o,KAe,er,yM,$0o,cd,I0o,bae,j0o,N0o,vae,D0o,q0o,G0o,wM,O0o,Tae,X0o,z0o,V0o,Wr,AM,W0o,Fae,Q0o,H0o,fd,U0o,Cae,J0o,Y0o,Mae,K0o,Z0o,eLo,Eae,oLo,rLo,LM,tLo,qe,BM,aLo,yae,nLo,sLo,Va,lLo,wae,iLo,dLo,Aae,cLo,fLo,Lae,mLo,gLo,hLo,Bae,J5,xae,pLo,_Lo,ZI,uLo,bLo,vLo,Y5,TLo,kae,FLo,CLo,Rae,MLo,ELo,Sae,yLo,wLo,xM,ZAe,md,K5,Pae,kM,ALo,$ae,LLo,e0e,or,RM,BLo,gd,xLo,Iae,kLo,RLo,jae,SLo,PLo,$Lo,SM,ILo,Nae,jLo,NLo,DLo,Qr,PM,qLo,Dae,GLo,OLo,hd,XLo,qae,zLo,VLo,Gae,WLo,QLo,HLo,Oae,ULo,JLo,$M,YLo,Ge,IM,KLo,Xae,ZLo,e9o,Wa,o9o,zae,r9o,t9o,Vae,a9o,n9o,Wae,s9o,l9o,i9o,we,Z5,Qae,d9o,c9o,ej,f9o,m9o,g9o,e2,Hae,h9o,p9o,oj,_9o,u9o,b9o,As,Uae,v9o,T9o,rj,F9o,C9o,tj,M9o,E9o,y9o,o2,Jae,w9o,A9o,aj,L9o,B9o,x9o,ta,Yae,k9o,R9o,nj,S9o,P9o,sj,$9o,I9o,lj,j9o,N9o,D9o,r2,Kae,q9o,G9o,ij,O9o,X9o,z9o,t2,Zae,V9o,W9o,dj,Q9o,H9o,U9o,a2,ene,J9o,Y9o,cj,K9o,Z9o,eBo,n2,oBo,one,rBo,tBo,rne,aBo,nBo,tne,sBo,lBo,jM,o0e,pd,s2,ane,NM,iBo,nne,dBo,r0e,rr,DM,cBo,_d,fBo,sne,mBo,gBo,lne,hBo,pBo,_Bo,qM,uBo,ine,bBo,vBo,TBo,Hr,GM,FBo,dne,CBo,MBo,ud,EBo,cne,yBo,wBo,fne,ABo,LBo,BBo,mne,xBo,kBo,OM,RBo,Oe,XM,SBo,gne,PBo,$Bo,Qa,IBo,hne,jBo,NBo,pne,DBo,qBo,_ne,GBo,OBo,XBo,une,l2,bne,zBo,VBo,fj,WBo,QBo,HBo,i2,UBo,vne,JBo,YBo,Tne,KBo,ZBo,Fne,exo,oxo,zM,t0e,bd,d2,Cne,VM,rxo,Mne,txo,a0e,tr,WM,axo,vd,nxo,Ene,sxo,lxo,yne,ixo,dxo,cxo,QM,fxo,wne,mxo,gxo,hxo,Ur,HM,pxo,Ane,_xo,uxo,Td,bxo,Lne,vxo,Txo,Bne,Fxo,Cxo,Mxo,xne,Exo,yxo,UM,wxo,Xe,JM,Axo,kne,Lxo,Bxo,Ha,xxo,Rne,kxo,Rxo,Sne,Sxo,Pxo,Pne,$xo,Ixo,jxo,ro,c2,$ne,Nxo,Dxo,mj,qxo,Gxo,Oxo,f2,Ine,Xxo,zxo,gj,Vxo,Wxo,Qxo,m2,jne,Hxo,Uxo,hj,Jxo,Yxo,Kxo,g2,Nne,Zxo,eko,pj,oko,rko,tko,h2,Dne,ako,nko,_j,sko,lko,iko,p2,qne,dko,cko,uj,fko,mko,gko,_2,Gne,hko,pko,bj,_ko,uko,bko,u2,vko,One,Tko,Fko,Xne,Cko,Mko,zne,Eko,yko,YM,n0e,Fd,b2,Vne,KM,wko,Wne,Ako,s0e,ar,ZM,Lko,Cd,Bko,Qne,xko,kko,Hne,Rko,Sko,Pko,eE,$ko,Une,Iko,jko,Nko,Jr,oE,Dko,Jne,qko,Gko,Md,Oko,Yne,Xko,zko,Kne,Vko,Wko,Qko,Zne,Hko,Uko,rE,Jko,ze,tE,Yko,ese,Kko,Zko,Ua,eRo,ose,oRo,rRo,rse,tRo,aRo,tse,nRo,sRo,lRo,Ed,v2,ase,iRo,dRo,vj,cRo,fRo,mRo,T2,nse,gRo,hRo,Tj,pRo,_Ro,uRo,F2,sse,bRo,vRo,Fj,TRo,FRo,CRo,C2,MRo,lse,ERo,yRo,ise,wRo,ARo,dse,LRo,BRo,aE,l0e,yd,M2,cse,nE,xRo,fse,kRo,i0e,nr,sE,RRo,wd,SRo,mse,PRo,$Ro,gse,IRo,jRo,NRo,lE,DRo,hse,qRo,GRo,ORo,Yr,iE,XRo,pse,zRo,VRo,Ad,WRo,_se,QRo,HRo,use,URo,JRo,YRo,bse,KRo,ZRo,dE,eSo,Ve,cE,oSo,vse,rSo,tSo,Ja,aSo,Tse,nSo,sSo,Fse,lSo,iSo,Cse,dSo,cSo,fSo,to,E2,Mse,mSo,gSo,Cj,hSo,pSo,_So,y2,Ese,uSo,bSo,Mj,vSo,TSo,FSo,w2,yse,CSo,MSo,Ej,ESo,ySo,wSo,A2,wse,ASo,LSo,yj,BSo,xSo,kSo,L2,Ase,RSo,SSo,wj,PSo,$So,ISo,B2,Lse,jSo,NSo,Aj,DSo,qSo,GSo,x2,Bse,OSo,XSo,Lj,zSo,VSo,WSo,k2,QSo,xse,HSo,USo,kse,JSo,YSo,Rse,KSo,ZSo,fE,d0e,Ld,R2,Sse,mE,ePo,Pse,oPo,c0e,sr,gE,rPo,Bd,tPo,$se,aPo,nPo,Ise,sPo,lPo,iPo,hE,dPo,jse,cPo,fPo,mPo,Kr,pE,gPo,Nse,hPo,pPo,xd,_Po,Dse,uPo,bPo,qse,vPo,TPo,FPo,Gse,CPo,MPo,_E,EPo,We,uE,yPo,Ose,wPo,APo,Ya,LPo,Xse,BPo,xPo,zse,kPo,RPo,Vse,SPo,PPo,$Po,bE,S2,Wse,IPo,jPo,Bj,NPo,DPo,qPo,P2,Qse,GPo,OPo,xj,XPo,zPo,VPo,$2,WPo,Hse,QPo,HPo,Use,UPo,JPo,Jse,YPo,KPo,vE,f0e,kd,I2,Yse,TE,ZPo,Kse,e$o,m0e,lr,FE,o$o,Rd,r$o,Zse,t$o,a$o,ele,n$o,s$o,l$o,CE,i$o,ole,d$o,c$o,f$o,Zr,ME,m$o,rle,g$o,h$o,Sd,p$o,tle,_$o,u$o,ale,b$o,v$o,T$o,nle,F$o,C$o,EE,M$o,Qe,yE,E$o,sle,y$o,w$o,Ka,A$o,lle,L$o,B$o,ile,x$o,k$o,dle,R$o,S$o,P$o,Pd,j2,cle,$$o,I$o,kj,j$o,N$o,D$o,N2,fle,q$o,G$o,Rj,O$o,X$o,z$o,D2,mle,V$o,W$o,Sj,Q$o,H$o,U$o,q2,J$o,gle,Y$o,K$o,hle,Z$o,eIo,ple,oIo,rIo,wE,g0e,$d,G2,_le,AE,tIo,ule,aIo,h0e,ir,LE,nIo,Id,sIo,ble,lIo,iIo,vle,dIo,cIo,fIo,BE,mIo,Tle,gIo,hIo,pIo,et,xE,_Io,Fle,uIo,bIo,jd,vIo,Cle,TIo,FIo,Mle,CIo,MIo,EIo,Ele,yIo,wIo,kE,AIo,He,RE,LIo,yle,BIo,xIo,Za,kIo,wle,RIo,SIo,Ale,PIo,$Io,Lle,IIo,jIo,NIo,Ble,O2,xle,DIo,qIo,Pj,GIo,OIo,XIo,X2,zIo,kle,VIo,WIo,Rle,QIo,HIo,Sle,UIo,JIo,SE,p0e,Nd,z2,Ple,PE,YIo,$le,KIo,_0e,dr,$E,ZIo,Dd,ejo,Ile,ojo,rjo,jle,tjo,ajo,njo,IE,sjo,Nle,ljo,ijo,djo,ot,jE,cjo,Dle,fjo,mjo,qd,gjo,qle,hjo,pjo,Gle,_jo,ujo,bjo,Ole,vjo,Tjo,NE,Fjo,Ue,DE,Cjo,Xle,Mjo,Ejo,en,yjo,zle,wjo,Ajo,Vle,Ljo,Bjo,Wle,xjo,kjo,Rjo,Qle,V2,Hle,Sjo,Pjo,$j,$jo,Ijo,jjo,W2,Njo,Ule,Djo,qjo,Jle,Gjo,Ojo,Yle,Xjo,zjo,qE,u0e,Gd,Q2,Kle,GE,Vjo,Zle,Wjo,b0e,cr,OE,Qjo,Od,Hjo,eie,Ujo,Jjo,oie,Yjo,Kjo,Zjo,XE,eNo,rie,oNo,rNo,tNo,rt,zE,aNo,tie,nNo,sNo,Xd,lNo,aie,iNo,dNo,nie,cNo,fNo,mNo,sie,gNo,hNo,VE,pNo,Je,WE,_No,lie,uNo,bNo,on,vNo,iie,TNo,FNo,die,CNo,MNo,cie,ENo,yNo,wNo,QE,H2,fie,ANo,LNo,Ij,BNo,xNo,kNo,U2,mie,RNo,SNo,jj,PNo,$No,INo,J2,jNo,gie,NNo,DNo,hie,qNo,GNo,pie,ONo,XNo,HE,v0e,zd,Y2,_ie,UE,zNo,uie,VNo,T0e,fr,JE,WNo,Vd,QNo,bie,HNo,UNo,vie,JNo,YNo,KNo,YE,ZNo,Tie,eDo,oDo,rDo,tt,KE,tDo,Fie,aDo,nDo,Wd,sDo,Cie,lDo,iDo,Mie,dDo,cDo,fDo,Eie,mDo,gDo,ZE,hDo,fo,e3,pDo,yie,_Do,uDo,rn,bDo,wie,vDo,TDo,Aie,FDo,CDo,Lie,MDo,EDo,yDo,B,K2,Bie,wDo,ADo,Nj,LDo,BDo,xDo,Z2,xie,kDo,RDo,Dj,SDo,PDo,$Do,ev,kie,IDo,jDo,qj,NDo,DDo,qDo,ov,Rie,GDo,ODo,Gj,XDo,zDo,VDo,rv,Sie,WDo,QDo,Oj,HDo,UDo,JDo,tv,Pie,YDo,KDo,Xj,ZDo,eqo,oqo,av,$ie,rqo,tqo,zj,aqo,nqo,sqo,nv,Iie,lqo,iqo,Vj,dqo,cqo,fqo,sv,jie,mqo,gqo,Wj,hqo,pqo,_qo,lv,Nie,uqo,bqo,Qj,vqo,Tqo,Fqo,iv,Die,Cqo,Mqo,Hj,Eqo,yqo,wqo,dv,qie,Aqo,Lqo,Uj,Bqo,xqo,kqo,cv,Gie,Rqo,Sqo,Jj,Pqo,$qo,Iqo,fv,Oie,jqo,Nqo,Yj,Dqo,qqo,Gqo,mv,Xie,Oqo,Xqo,Kj,zqo,Vqo,Wqo,Ls,zie,Qqo,Hqo,Zj,Uqo,Jqo,eN,Yqo,Kqo,Zqo,gv,Vie,eGo,oGo,oN,rGo,tGo,aGo,hv,Wie,nGo,sGo,rN,lGo,iGo,dGo,pv,Qie,cGo,fGo,tN,mGo,gGo,hGo,_v,Hie,pGo,_Go,aN,uGo,bGo,vGo,uv,Uie,TGo,FGo,nN,CGo,MGo,EGo,bv,Jie,yGo,wGo,sN,AGo,LGo,BGo,vv,Yie,xGo,kGo,lN,RGo,SGo,PGo,Tv,Kie,$Go,IGo,iN,jGo,NGo,DGo,Fv,Zie,qGo,GGo,dN,OGo,XGo,zGo,Cv,ede,VGo,WGo,cN,QGo,HGo,UGo,Mv,ode,JGo,YGo,fN,KGo,ZGo,eOo,Ev,rde,oOo,rOo,mN,tOo,aOo,nOo,yv,tde,sOo,lOo,gN,iOo,dOo,cOo,wv,ade,fOo,mOo,hN,gOo,hOo,pOo,Av,nde,_Oo,uOo,pN,bOo,vOo,TOo,Lv,sde,FOo,COo,_N,MOo,EOo,yOo,Bv,lde,wOo,AOo,uN,LOo,BOo,xOo,xv,ide,kOo,ROo,bN,SOo,POo,$Oo,kv,dde,IOo,jOo,vN,NOo,DOo,qOo,Rv,cde,GOo,OOo,TN,XOo,zOo,VOo,Sv,fde,WOo,QOo,FN,HOo,UOo,JOo,Pv,mde,YOo,KOo,CN,ZOo,eXo,oXo,$v,gde,rXo,tXo,MN,aXo,nXo,sXo,Iv,hde,lXo,iXo,EN,dXo,cXo,fXo,jv,pde,mXo,gXo,yN,hXo,pXo,_Xo,_de,uXo,bXo,o3,F0e,Qd,Nv,ude,r3,vXo,bde,TXo,C0e,mr,t3,FXo,Hd,CXo,vde,MXo,EXo,Tde,yXo,wXo,AXo,a3,LXo,Fde,BXo,xXo,kXo,at,n3,RXo,Cde,SXo,PXo,Ud,$Xo,Mde,IXo,jXo,Ede,NXo,DXo,qXo,yde,GXo,OXo,s3,XXo,mo,l3,zXo,wde,VXo,WXo,tn,QXo,Ade,HXo,UXo,Lde,JXo,YXo,Bde,KXo,ZXo,ezo,H,Dv,xde,ozo,rzo,wN,tzo,azo,nzo,qv,kde,szo,lzo,AN,izo,dzo,czo,Gv,Rde,fzo,mzo,LN,gzo,hzo,pzo,Ov,Sde,_zo,uzo,BN,bzo,vzo,Tzo,Xv,Pde,Fzo,Czo,xN,Mzo,Ezo,yzo,zv,$de,wzo,Azo,kN,Lzo,Bzo,xzo,Vv,Ide,kzo,Rzo,RN,Szo,Pzo,$zo,Wv,jde,Izo,jzo,SN,Nzo,Dzo,qzo,Qv,Nde,Gzo,Ozo,PN,Xzo,zzo,Vzo,Hv,Dde,Wzo,Qzo,$N,Hzo,Uzo,Jzo,Uv,qde,Yzo,Kzo,IN,Zzo,eVo,oVo,Jv,Gde,rVo,tVo,jN,aVo,nVo,sVo,Yv,Ode,lVo,iVo,NN,dVo,cVo,fVo,Kv,Xde,mVo,gVo,DN,hVo,pVo,_Vo,Zv,zde,uVo,bVo,qN,vVo,TVo,FVo,e6,Vde,CVo,MVo,GN,EVo,yVo,wVo,o6,Wde,AVo,LVo,ON,BVo,xVo,kVo,r6,Qde,RVo,SVo,XN,PVo,$Vo,IVo,t6,Hde,jVo,NVo,zN,DVo,qVo,GVo,a6,Ude,OVo,XVo,VN,zVo,VVo,WVo,n6,Jde,QVo,HVo,WN,UVo,JVo,YVo,s6,Yde,KVo,ZVo,QN,eWo,oWo,rWo,Kde,tWo,aWo,i3,M0e,Jd,l6,Zde,d3,nWo,ece,sWo,E0e,gr,c3,lWo,Yd,iWo,oce,dWo,cWo,rce,fWo,mWo,gWo,f3,hWo,tce,pWo,_Wo,uWo,nt,m3,bWo,ace,vWo,TWo,Kd,FWo,nce,CWo,MWo,sce,EWo,yWo,wWo,lce,AWo,LWo,g3,BWo,go,h3,xWo,ice,kWo,RWo,an,SWo,dce,PWo,$Wo,cce,IWo,jWo,fce,NWo,DWo,qWo,he,i6,mce,GWo,OWo,HN,XWo,zWo,VWo,d6,gce,WWo,QWo,UN,HWo,UWo,JWo,c6,hce,YWo,KWo,JN,ZWo,eQo,oQo,f6,pce,rQo,tQo,YN,aQo,nQo,sQo,m6,_ce,lQo,iQo,KN,dQo,cQo,fQo,g6,uce,mQo,gQo,ZN,hQo,pQo,_Qo,h6,bce,uQo,bQo,eD,vQo,TQo,FQo,p6,vce,CQo,MQo,oD,EQo,yQo,wQo,_6,Tce,AQo,LQo,rD,BQo,xQo,kQo,u6,Fce,RQo,SQo,tD,PQo,$Qo,IQo,Cce,jQo,NQo,p3,y0e,Zd,b6,Mce,_3,DQo,Ece,qQo,w0e,hr,u3,GQo,ec,OQo,yce,XQo,zQo,wce,VQo,WQo,QQo,b3,HQo,Ace,UQo,JQo,YQo,st,v3,KQo,Lce,ZQo,eHo,oc,oHo,Bce,rHo,tHo,xce,aHo,nHo,sHo,kce,lHo,iHo,T3,dHo,ho,F3,cHo,Rce,fHo,mHo,nn,gHo,Sce,hHo,pHo,Pce,_Ho,uHo,$ce,bHo,vHo,THo,Ice,v6,jce,FHo,CHo,aD,MHo,EHo,yHo,Nce,wHo,AHo,C3,A0e,rc,T6,Dce,M3,LHo,qce,BHo,L0e,pr,E3,xHo,tc,kHo,Gce,RHo,SHo,Oce,PHo,$Ho,IHo,y3,jHo,Xce,NHo,DHo,qHo,lt,w3,GHo,zce,OHo,XHo,ac,zHo,Vce,VHo,WHo,Wce,QHo,HHo,UHo,Qce,JHo,YHo,A3,KHo,po,L3,ZHo,Hce,eUo,oUo,sn,rUo,Uce,tUo,aUo,Jce,nUo,sUo,Yce,lUo,iUo,dUo,Y,F6,Kce,cUo,fUo,nD,mUo,gUo,hUo,C6,Zce,pUo,_Uo,sD,uUo,bUo,vUo,M6,efe,TUo,FUo,lD,CUo,MUo,EUo,E6,ofe,yUo,wUo,iD,AUo,LUo,BUo,y6,rfe,xUo,kUo,dD,RUo,SUo,PUo,w6,tfe,$Uo,IUo,cD,jUo,NUo,DUo,A6,afe,qUo,GUo,fD,OUo,XUo,zUo,L6,nfe,VUo,WUo,mD,QUo,HUo,UUo,B6,sfe,JUo,YUo,gD,KUo,ZUo,eJo,x6,lfe,oJo,rJo,hD,tJo,aJo,nJo,k6,ife,sJo,lJo,pD,iJo,dJo,cJo,R6,dfe,fJo,mJo,_D,gJo,hJo,pJo,S6,cfe,_Jo,uJo,uD,bJo,vJo,TJo,P6,ffe,FJo,CJo,bD,MJo,EJo,yJo,$6,mfe,wJo,AJo,vD,LJo,BJo,xJo,I6,gfe,kJo,RJo,TD,SJo,PJo,$Jo,j6,hfe,IJo,jJo,FD,NJo,DJo,qJo,N6,pfe,GJo,OJo,CD,XJo,zJo,VJo,D6,_fe,WJo,QJo,MD,HJo,UJo,JJo,q6,ufe,YJo,KJo,ED,ZJo,eYo,oYo,bfe,rYo,tYo,B3,B0e,nc,G6,vfe,x3,aYo,Tfe,nYo,x0e,_r,k3,sYo,sc,lYo,Ffe,iYo,dYo,Cfe,cYo,fYo,mYo,R3,gYo,Mfe,hYo,pYo,_Yo,it,S3,uYo,Efe,bYo,vYo,lc,TYo,yfe,FYo,CYo,wfe,MYo,EYo,yYo,Afe,wYo,AYo,P3,LYo,_o,$3,BYo,Lfe,xYo,kYo,ln,RYo,Bfe,SYo,PYo,xfe,$Yo,IYo,kfe,jYo,NYo,DYo,pe,O6,Rfe,qYo,GYo,yD,OYo,XYo,zYo,X6,Sfe,VYo,WYo,wD,QYo,HYo,UYo,z6,Pfe,JYo,YYo,AD,KYo,ZYo,eKo,V6,$fe,oKo,rKo,LD,tKo,aKo,nKo,W6,Ife,sKo,lKo,BD,iKo,dKo,cKo,Q6,jfe,fKo,mKo,xD,gKo,hKo,pKo,H6,Nfe,_Ko,uKo,kD,bKo,vKo,TKo,U6,Dfe,FKo,CKo,RD,MKo,EKo,yKo,J6,qfe,wKo,AKo,SD,LKo,BKo,xKo,Y6,Gfe,kKo,RKo,PD,SKo,PKo,$Ko,Ofe,IKo,jKo,I3,k0e,ic,K6,Xfe,j3,NKo,zfe,DKo,R0e,ur,N3,qKo,dc,GKo,Vfe,OKo,XKo,Wfe,zKo,VKo,WKo,D3,QKo,Qfe,HKo,UKo,JKo,dt,q3,YKo,Hfe,KKo,ZKo,cc,eZo,Ufe,oZo,rZo,Jfe,tZo,aZo,nZo,Yfe,sZo,lZo,G3,iZo,uo,O3,dZo,Kfe,cZo,fZo,dn,mZo,Zfe,gZo,hZo,eme,pZo,_Zo,ome,uZo,bZo,vZo,X,Z6,rme,TZo,FZo,$D,CZo,MZo,EZo,eT,tme,yZo,wZo,ID,AZo,LZo,BZo,oT,ame,xZo,kZo,jD,RZo,SZo,PZo,rT,nme,$Zo,IZo,ND,jZo,NZo,DZo,tT,sme,qZo,GZo,DD,OZo,XZo,zZo,aT,lme,VZo,WZo,qD,QZo,HZo,UZo,nT,ime,JZo,YZo,GD,KZo,ZZo,eer,sT,dme,oer,rer,OD,ter,aer,ner,lT,cme,ser,ler,XD,ier,der,cer,iT,fme,fer,mer,zD,ger,her,per,dT,mme,_er,uer,VD,ber,ver,Ter,cT,gme,Fer,Cer,WD,Mer,Eer,yer,fT,hme,wer,Aer,QD,Ler,Ber,xer,mT,pme,ker,Rer,HD,Ser,Per,$er,gT,_me,Ier,jer,UD,Ner,Der,qer,hT,ume,Ger,Oer,JD,Xer,zer,Ver,pT,bme,Wer,Qer,YD,Her,Uer,Jer,_T,vme,Yer,Ker,KD,Zer,eor,oor,uT,Tme,ror,tor,ZD,aor,nor,sor,bT,Fme,lor,ior,eq,dor,cor,mor,vT,Cme,gor,hor,oq,por,_or,uor,TT,Mme,bor,vor,rq,Tor,For,Cor,FT,Eme,Mor,Eor,tq,yor,wor,Aor,CT,yme,Lor,Bor,aq,xor,kor,Ror,MT,wme,Sor,Por,nq,$or,Ior,jor,Ame,Nor,Dor,X3,S0e,fc,ET,Lme,z3,qor,Bme,Gor,P0e,br,V3,Oor,mc,Xor,xme,zor,Vor,kme,Wor,Qor,Hor,W3,Uor,Rme,Jor,Yor,Kor,ct,Q3,Zor,Sme,err,orr,gc,rrr,Pme,trr,arr,$me,nrr,srr,lrr,Ime,irr,drr,H3,crr,bo,U3,frr,jme,mrr,grr,cn,hrr,Nme,prr,_rr,Dme,urr,brr,qme,vrr,Trr,Frr,te,yT,Gme,Crr,Mrr,sq,Err,yrr,wrr,wT,Ome,Arr,Lrr,lq,Brr,xrr,krr,AT,Xme,Rrr,Srr,iq,Prr,$rr,Irr,LT,zme,jrr,Nrr,dq,Drr,qrr,Grr,BT,Vme,Orr,Xrr,cq,zrr,Vrr,Wrr,xT,Wme,Qrr,Hrr,fq,Urr,Jrr,Yrr,kT,Qme,Krr,Zrr,mq,etr,otr,rtr,RT,Hme,ttr,atr,gq,ntr,str,ltr,ST,Ume,itr,dtr,hq,ctr,ftr,mtr,PT,Jme,gtr,htr,pq,ptr,_tr,utr,$T,Yme,btr,vtr,_q,Ttr,Ftr,Ctr,IT,Kme,Mtr,Etr,uq,ytr,wtr,Atr,jT,Zme,Ltr,Btr,bq,xtr,ktr,Rtr,NT,ege,Str,Ptr,vq,$tr,Itr,jtr,DT,oge,Ntr,Dtr,Tq,qtr,Gtr,Otr,qT,rge,Xtr,ztr,Fq,Vtr,Wtr,Qtr,GT,tge,Htr,Utr,Cq,Jtr,Ytr,Ktr,age,Ztr,ear,J3,$0e,hc,OT,nge,Y3,oar,sge,rar,I0e,vr,K3,tar,pc,aar,lge,nar,sar,ige,lar,iar,dar,Z3,car,dge,far,mar,gar,ft,ey,har,cge,par,_ar,_c,uar,fge,bar,Tar,mge,Far,Car,Mar,gge,Ear,yar,oy,war,vo,ry,Aar,hge,Lar,Bar,fn,xar,pge,kar,Rar,_ge,Sar,Par,uge,$ar,Iar,jar,bge,XT,vge,Nar,Dar,Mq,qar,Gar,Oar,Tge,Xar,zar,ty,j0e,uc,zT,Fge,ay,Var,Cge,War,N0e,Tr,ny,Qar,bc,Har,Mge,Uar,Jar,Ege,Yar,Kar,Zar,sy,enr,yge,onr,rnr,tnr,mt,ly,anr,wge,nnr,snr,vc,lnr,Age,inr,dnr,Lge,cnr,fnr,mnr,Bge,gnr,hnr,iy,pnr,To,dy,_nr,xge,unr,bnr,mn,vnr,kge,Tnr,Fnr,Rge,Cnr,Mnr,Sge,Enr,ynr,wnr,K,VT,Pge,Anr,Lnr,Eq,Bnr,xnr,knr,WT,$ge,Rnr,Snr,yq,Pnr,$nr,Inr,QT,Ige,jnr,Nnr,wq,Dnr,qnr,Gnr,HT,jge,Onr,Xnr,Aq,znr,Vnr,Wnr,UT,Nge,Qnr,Hnr,Lq,Unr,Jnr,Ynr,JT,Dge,Knr,Znr,Bq,esr,osr,rsr,YT,qge,tsr,asr,xq,nsr,ssr,lsr,KT,Gge,isr,dsr,kq,csr,fsr,msr,ZT,Oge,gsr,hsr,Rq,psr,_sr,usr,e7,Xge,bsr,vsr,Sq,Tsr,Fsr,Csr,o7,zge,Msr,Esr,Pq,ysr,wsr,Asr,r7,Vge,Lsr,Bsr,$q,xsr,ksr,Rsr,t7,Wge,Ssr,Psr,Iq,$sr,Isr,jsr,a7,Qge,Nsr,Dsr,jq,qsr,Gsr,Osr,n7,Hge,Xsr,zsr,Nq,Vsr,Wsr,Qsr,s7,Uge,Hsr,Usr,Dq,Jsr,Ysr,Ksr,l7,Jge,Zsr,elr,qq,olr,rlr,tlr,i7,Yge,alr,nlr,Gq,slr,llr,ilr,d7,Kge,dlr,clr,Oq,flr,mlr,glr,c7,Zge,hlr,plr,Xq,_lr,ulr,blr,ehe,vlr,Tlr,cy,D0e,Tc,f7,ohe,fy,Flr,rhe,Clr,q0e,Fr,my,Mlr,Fc,Elr,the,ylr,wlr,ahe,Alr,Llr,Blr,gy,xlr,nhe,klr,Rlr,Slr,gt,hy,Plr,she,$lr,Ilr,Cc,jlr,lhe,Nlr,Dlr,ihe,qlr,Glr,Olr,dhe,Xlr,zlr,py,Vlr,Fo,_y,Wlr,che,Qlr,Hlr,gn,Ulr,fhe,Jlr,Ylr,mhe,Klr,Zlr,ghe,eir,oir,rir,Z,m7,hhe,tir,air,zq,nir,sir,lir,g7,phe,iir,dir,Vq,cir,fir,mir,h7,_he,gir,hir,Wq,pir,_ir,uir,p7,uhe,bir,vir,Qq,Tir,Fir,Cir,_7,bhe,Mir,Eir,Hq,yir,wir,Air,u7,vhe,Lir,Bir,Uq,xir,kir,Rir,b7,The,Sir,Pir,Jq,$ir,Iir,jir,v7,Fhe,Nir,Dir,Yq,qir,Gir,Oir,T7,Che,Xir,zir,Kq,Vir,Wir,Qir,F7,Mhe,Hir,Uir,Zq,Jir,Yir,Kir,C7,Ehe,Zir,edr,eG,odr,rdr,tdr,M7,yhe,adr,ndr,oG,sdr,ldr,idr,E7,whe,ddr,cdr,rG,fdr,mdr,gdr,y7,Ahe,hdr,pdr,tG,_dr,udr,bdr,w7,Lhe,vdr,Tdr,aG,Fdr,Cdr,Mdr,A7,Bhe,Edr,ydr,nG,wdr,Adr,Ldr,L7,xhe,Bdr,xdr,sG,kdr,Rdr,Sdr,B7,khe,Pdr,$dr,lG,Idr,jdr,Ndr,x7,Rhe,Ddr,qdr,iG,Gdr,Odr,Xdr,She,zdr,Vdr,uy,G0e,Mc,k7,Phe,by,Wdr,$he,Qdr,O0e,Cr,vy,Hdr,Ec,Udr,Ihe,Jdr,Ydr,jhe,Kdr,Zdr,ecr,Ty,ocr,Nhe,rcr,tcr,acr,ht,Fy,ncr,Dhe,scr,lcr,yc,icr,qhe,dcr,ccr,Ghe,fcr,mcr,gcr,Ohe,hcr,pcr,Cy,_cr,Co,My,ucr,Xhe,bcr,vcr,hn,Tcr,zhe,Fcr,Ccr,Vhe,Mcr,Ecr,Whe,ycr,wcr,Acr,Qhe,R7,Hhe,Lcr,Bcr,dG,xcr,kcr,Rcr,Uhe,Scr,Pcr,Ey,X0e,wc,S7,Jhe,yy,$cr,Yhe,Icr,z0e,Mr,wy,jcr,Ac,Ncr,Khe,Dcr,qcr,Zhe,Gcr,Ocr,Xcr,Ay,zcr,epe,Vcr,Wcr,Qcr,pt,Ly,Hcr,ope,Ucr,Jcr,Lc,Ycr,rpe,Kcr,Zcr,tpe,efr,ofr,rfr,ape,tfr,afr,By,nfr,Mo,xy,sfr,npe,lfr,ifr,pn,dfr,spe,cfr,ffr,lpe,mfr,gfr,ipe,hfr,pfr,_fr,dpe,P7,cpe,ufr,bfr,cG,vfr,Tfr,Ffr,fpe,Cfr,Mfr,ky,V0e,Bc,$7,mpe,Ry,Efr,gpe,yfr,W0e,Er,Sy,wfr,xc,Afr,hpe,Lfr,Bfr,ppe,xfr,kfr,Rfr,Py,Sfr,_pe,Pfr,$fr,Ifr,_t,$y,jfr,upe,Nfr,Dfr,kc,qfr,bpe,Gfr,Ofr,vpe,Xfr,zfr,Vfr,Tpe,Wfr,Qfr,Iy,Hfr,Eo,jy,Ufr,Fpe,Jfr,Yfr,_n,Kfr,Cpe,Zfr,emr,Mpe,omr,rmr,Epe,tmr,amr,nmr,V,I7,ype,smr,lmr,fG,imr,dmr,cmr,j7,wpe,fmr,mmr,mG,gmr,hmr,pmr,N7,Ape,_mr,umr,gG,bmr,vmr,Tmr,D7,Lpe,Fmr,Cmr,hG,Mmr,Emr,ymr,q7,Bpe,wmr,Amr,pG,Lmr,Bmr,xmr,G7,xpe,kmr,Rmr,_G,Smr,Pmr,$mr,O7,kpe,Imr,jmr,uG,Nmr,Dmr,qmr,X7,Rpe,Gmr,Omr,bG,Xmr,zmr,Vmr,z7,Spe,Wmr,Qmr,vG,Hmr,Umr,Jmr,V7,Ppe,Ymr,Kmr,TG,Zmr,egr,ogr,W7,$pe,rgr,tgr,FG,agr,ngr,sgr,Q7,Ipe,lgr,igr,CG,dgr,cgr,fgr,H7,jpe,mgr,ggr,MG,hgr,pgr,_gr,U7,Npe,ugr,bgr,EG,vgr,Tgr,Fgr,J7,Dpe,Cgr,Mgr,yG,Egr,ygr,wgr,Y7,qpe,Agr,Lgr,wG,Bgr,xgr,kgr,K7,Gpe,Rgr,Sgr,AG,Pgr,$gr,Igr,Z7,Ope,jgr,Ngr,LG,Dgr,qgr,Ggr,e8,Xpe,Ogr,Xgr,BG,zgr,Vgr,Wgr,o8,zpe,Qgr,Hgr,xG,Ugr,Jgr,Ygr,r8,Vpe,Kgr,Zgr,kG,ehr,ohr,rhr,t8,Wpe,thr,ahr,RG,nhr,shr,lhr,a8,Qpe,ihr,dhr,SG,chr,fhr,mhr,n8,Hpe,ghr,hhr,PG,phr,_hr,uhr,Upe,bhr,vhr,Ny,Q0e,Rc,s8,Jpe,Dy,Thr,Ype,Fhr,H0e,yr,qy,Chr,Sc,Mhr,Kpe,Ehr,yhr,Zpe,whr,Ahr,Lhr,Gy,Bhr,e_e,xhr,khr,Rhr,ut,Oy,Shr,o_e,Phr,$hr,Pc,Ihr,r_e,jhr,Nhr,t_e,Dhr,qhr,Ghr,a_e,Ohr,Xhr,Xy,zhr,yo,zy,Vhr,n_e,Whr,Qhr,un,Hhr,s_e,Uhr,Jhr,l_e,Yhr,Khr,i_e,Zhr,epr,opr,bn,l8,d_e,rpr,tpr,$G,apr,npr,spr,i8,c_e,lpr,ipr,IG,dpr,cpr,fpr,d8,f_e,mpr,gpr,jG,hpr,ppr,_pr,c8,m_e,upr,bpr,NG,vpr,Tpr,Fpr,g_e,Cpr,Mpr,Vy,U0e,$c,f8,h_e,Wy,Epr,p_e,ypr,J0e,wr,Qy,wpr,Ic,Apr,__e,Lpr,Bpr,u_e,xpr,kpr,Rpr,Hy,Spr,b_e,Ppr,$pr,Ipr,bt,Uy,jpr,v_e,Npr,Dpr,jc,qpr,T_e,Gpr,Opr,F_e,Xpr,zpr,Vpr,C_e,Wpr,Qpr,Jy,Hpr,wo,Yy,Upr,M_e,Jpr,Ypr,vn,Kpr,E_e,Zpr,e_r,y_e,o_r,r_r,w_e,t_r,a_r,n_r,fe,m8,A_e,s_r,l_r,DG,i_r,d_r,c_r,g8,L_e,f_r,m_r,qG,g_r,h_r,p_r,h8,B_e,__r,u_r,GG,b_r,v_r,T_r,p8,x_e,F_r,C_r,OG,M_r,E_r,y_r,_8,k_e,w_r,A_r,XG,L_r,B_r,x_r,u8,R_e,k_r,R_r,zG,S_r,P_r,$_r,b8,S_e,I_r,j_r,VG,N_r,D_r,q_r,v8,P_e,G_r,O_r,WG,X_r,z_r,V_r,T8,$_e,W_r,Q_r,QG,H_r,U_r,J_r,F8,I_e,Y_r,K_r,HG,Z_r,eur,our,C8,j_e,rur,tur,UG,aur,nur,sur,N_e,lur,iur,Ky,Y0e,Nc,M8,D_e,Zy,dur,q_e,cur,K0e,Ar,ew,fur,Dc,mur,G_e,gur,hur,O_e,pur,_ur,uur,ow,bur,X_e,vur,Tur,Fur,vt,rw,Cur,z_e,Mur,Eur,qc,yur,V_e,wur,Aur,W_e,Lur,Bur,xur,Q_e,kur,Rur,tw,Sur,Ao,aw,Pur,H_e,$ur,Iur,Tn,jur,U_e,Nur,Dur,J_e,qur,Gur,Y_e,Our,Xur,zur,be,E8,K_e,Vur,Wur,JG,Qur,Hur,Uur,y8,Z_e,Jur,Yur,YG,Kur,Zur,e1r,w8,eue,o1r,r1r,KG,t1r,a1r,n1r,A8,oue,s1r,l1r,ZG,i1r,d1r,c1r,L8,rue,f1r,m1r,eO,g1r,h1r,p1r,B8,tue,_1r,u1r,oO,b1r,v1r,T1r,x8,aue,F1r,C1r,rO,M1r,E1r,y1r,k8,nue,w1r,A1r,tO,L1r,B1r,x1r,R8,sue,k1r,R1r,aO,S1r,P1r,$1r,lue,I1r,j1r,nw,Z0e,Gc,S8,iue,sw,N1r,due,D1r,eLe,Lr,lw,q1r,Oc,G1r,cue,O1r,X1r,fue,z1r,V1r,W1r,iw,Q1r,mue,H1r,U1r,J1r,Tt,dw,Y1r,gue,K1r,Z1r,Xc,ebr,hue,obr,rbr,pue,tbr,abr,nbr,_ue,sbr,lbr,cw,ibr,Lo,fw,dbr,uue,cbr,fbr,Fn,mbr,bue,gbr,hbr,vue,pbr,_br,Tue,ubr,bbr,vbr,ve,P8,Fue,Tbr,Fbr,nO,Cbr,Mbr,Ebr,$8,Cue,ybr,wbr,sO,Abr,Lbr,Bbr,I8,Mue,xbr,kbr,lO,Rbr,Sbr,Pbr,j8,Eue,$br,Ibr,iO,jbr,Nbr,Dbr,N8,yue,qbr,Gbr,dO,Obr,Xbr,zbr,D8,wue,Vbr,Wbr,cO,Qbr,Hbr,Ubr,q8,Aue,Jbr,Ybr,fO,Kbr,Zbr,e5r,G8,Lue,o5r,r5r,mO,t5r,a5r,n5r,O8,Bue,s5r,l5r,gO,i5r,d5r,c5r,xue,f5r,m5r,mw,oLe,zc,X8,kue,gw,g5r,Rue,h5r,rLe,Br,hw,p5r,Vc,_5r,Sue,u5r,b5r,Pue,v5r,T5r,F5r,pw,C5r,$ue,M5r,E5r,y5r,Ft,_w,w5r,Iue,A5r,L5r,Wc,B5r,jue,x5r,k5r,Nue,R5r,S5r,P5r,Due,$5r,I5r,uw,j5r,Bo,bw,N5r,que,D5r,q5r,Cn,G5r,Gue,O5r,X5r,Oue,z5r,V5r,Xue,W5r,Q5r,H5r,Te,z8,zue,U5r,J5r,hO,Y5r,K5r,Z5r,V8,Vue,e2r,o2r,pO,r2r,t2r,a2r,W8,Wue,n2r,s2r,_O,l2r,i2r,d2r,Q8,Que,c2r,f2r,uO,m2r,g2r,h2r,H8,Hue,p2r,_2r,bO,u2r,b2r,v2r,U8,Uue,T2r,F2r,vO,C2r,M2r,E2r,J8,Jue,y2r,w2r,TO,A2r,L2r,B2r,Y8,Yue,x2r,k2r,FO,R2r,S2r,P2r,K8,Kue,$2r,I2r,CO,j2r,N2r,D2r,Zue,q2r,G2r,vw,tLe,Qc,Z8,e1e,Tw,O2r,o1e,X2r,aLe,xr,Fw,z2r,Hc,V2r,r1e,W2r,Q2r,t1e,H2r,U2r,J2r,Cw,Y2r,a1e,K2r,Z2r,evr,Ct,Mw,ovr,n1e,rvr,tvr,Uc,avr,s1e,nvr,svr,l1e,lvr,ivr,dvr,i1e,cvr,fvr,Ew,mvr,xo,yw,gvr,d1e,hvr,pvr,Mn,_vr,c1e,uvr,bvr,f1e,vvr,Tvr,m1e,Fvr,Cvr,Mvr,Fe,eF,g1e,Evr,yvr,MO,wvr,Avr,Lvr,oF,h1e,Bvr,xvr,EO,kvr,Rvr,Svr,rF,p1e,Pvr,$vr,yO,Ivr,jvr,Nvr,tF,_1e,Dvr,qvr,wO,Gvr,Ovr,Xvr,aF,u1e,zvr,Vvr,AO,Wvr,Qvr,Hvr,nF,b1e,Uvr,Jvr,LO,Yvr,Kvr,Zvr,sF,v1e,e6r,o6r,BO,r6r,t6r,a6r,lF,T1e,n6r,s6r,xO,l6r,i6r,d6r,iF,F1e,c6r,f6r,kO,m6r,g6r,h6r,C1e,p6r,_6r,ww,nLe,Jc,dF,M1e,Aw,u6r,E1e,b6r,sLe,kr,Lw,v6r,Yc,T6r,y1e,F6r,C6r,w1e,M6r,E6r,y6r,Bw,w6r,A1e,A6r,L6r,B6r,Mt,xw,x6r,L1e,k6r,R6r,Kc,S6r,B1e,P6r,$6r,x1e,I6r,j6r,N6r,k1e,D6r,q6r,kw,G6r,ko,Rw,O6r,R1e,X6r,z6r,En,V6r,S1e,W6r,Q6r,P1e,H6r,U6r,$1e,J6r,Y6r,K6r,ao,cF,I1e,Z6r,eTr,RO,oTr,rTr,tTr,fF,j1e,aTr,nTr,SO,sTr,lTr,iTr,mF,N1e,dTr,cTr,PO,fTr,mTr,gTr,gF,D1e,hTr,pTr,$O,_Tr,uTr,bTr,hF,q1e,vTr,TTr,IO,FTr,CTr,MTr,pF,G1e,ETr,yTr,jO,wTr,ATr,LTr,_F,O1e,BTr,xTr,NO,kTr,RTr,STr,X1e,PTr,$Tr,Sw,lLe,Zc,uF,z1e,Pw,ITr,V1e,jTr,iLe,Rr,$w,NTr,ef,DTr,W1e,qTr,GTr,Q1e,OTr,XTr,zTr,Iw,VTr,H1e,WTr,QTr,HTr,Et,jw,UTr,U1e,JTr,YTr,of,KTr,J1e,ZTr,e7r,Y1e,o7r,r7r,t7r,K1e,a7r,n7r,Nw,s7r,Ro,Dw,l7r,Z1e,i7r,d7r,yn,c7r,ebe,f7r,m7r,obe,g7r,h7r,rbe,p7r,_7r,u7r,no,bF,tbe,b7r,v7r,DO,T7r,F7r,C7r,vF,abe,M7r,E7r,qO,y7r,w7r,A7r,TF,nbe,L7r,B7r,GO,x7r,k7r,R7r,FF,sbe,S7r,P7r,OO,$7r,I7r,j7r,CF,lbe,N7r,D7r,XO,q7r,G7r,O7r,MF,ibe,X7r,z7r,zO,V7r,W7r,Q7r,EF,dbe,H7r,U7r,VO,J7r,Y7r,K7r,cbe,Z7r,e8r,qw,dLe,rf,yF,fbe,Gw,o8r,mbe,r8r,cLe,Sr,Ow,t8r,tf,a8r,gbe,n8r,s8r,hbe,l8r,i8r,d8r,Xw,c8r,pbe,f8r,m8r,g8r,yt,zw,h8r,_be,p8r,_8r,af,u8r,ube,b8r,v8r,bbe,T8r,F8r,C8r,vbe,M8r,E8r,Vw,y8r,So,Ww,w8r,Tbe,A8r,L8r,wn,B8r,Fbe,x8r,k8r,Cbe,R8r,S8r,Mbe,P8r,$8r,I8r,Ebe,wF,ybe,j8r,N8r,WO,D8r,q8r,G8r,wbe,O8r,X8r,Qw,fLe,nf,AF,Abe,Hw,z8r,Lbe,V8r,mLe,Pr,Uw,W8r,sf,Q8r,Bbe,H8r,U8r,xbe,J8r,Y8r,K8r,Jw,Z8r,kbe,eFr,oFr,rFr,wt,Yw,tFr,Rbe,aFr,nFr,lf,sFr,Sbe,lFr,iFr,Pbe,dFr,cFr,fFr,$be,mFr,gFr,Kw,hFr,Po,Zw,pFr,Ibe,_Fr,uFr,An,bFr,jbe,vFr,TFr,Nbe,FFr,CFr,Dbe,MFr,EFr,yFr,eA,LF,qbe,wFr,AFr,QO,LFr,BFr,xFr,BF,Gbe,kFr,RFr,HO,SFr,PFr,$Fr,Obe,IFr,jFr,oA,gLe,df,xF,Xbe,rA,NFr,zbe,DFr,hLe,$r,tA,qFr,cf,GFr,Vbe,OFr,XFr,Wbe,zFr,VFr,WFr,aA,QFr,Qbe,HFr,UFr,JFr,At,nA,YFr,Hbe,KFr,ZFr,ff,eCr,Ube,oCr,rCr,Jbe,tCr,aCr,nCr,Ybe,sCr,lCr,sA,iCr,$o,lA,dCr,Kbe,cCr,fCr,Ln,mCr,Zbe,gCr,hCr,e5e,pCr,_Cr,o5e,uCr,bCr,vCr,r5e,kF,t5e,TCr,FCr,UO,CCr,MCr,ECr,a5e,yCr,wCr,iA,pLe;return ce=new z({}),ka=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),RC=new z({}),SC=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),bf=new ACr({props:{warning:"&lcub;true}",$$slots:{default:[Xft]},$$scope:{ctx:bi}}}),PC=new z({}),$C=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/configuration_auto.py#L509"}}),NC=new E({props:{name:"from\\_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/configuration_auto.py#L532",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),DC=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),qC=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/configuration_auto.py#L654",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),GC=new z({}),OC=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/tokenization_auto.py#L350"}}),VC=new E({props:{name:"from\\_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/tokenization_auto.py#L364",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15678/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),WC=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),QC=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/tokenization_auto.py#L560",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),HC=new z({}),UC=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/feature_extraction_auto.py#L168"}}),KC=new E({props:{name:"from\\_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/feature_extraction_auto.py#L182",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15678/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Qg=new ACr({props:{$$slots:{default:[zft]},$$scope:{ctx:bi}}}),ZC=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),e4=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/feature_extraction_auto.py#L309",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),o4=new z({}),r4=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/processing_auto.py#L71"}}),n4=new E({props:{name:"from\\_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ah=new ACr({props:{$$slots:{default:[Vft]},$$scope:{ctx:bi}}}),s4=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),l4=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),i4=new z({}),d4=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L653"}}),f4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),m4=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),g4=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h4=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),p4=new z({}),_4=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L660"}}),b4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),v4=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),T4=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F4=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C4=new z({}),M4=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L675"}}),y4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),w4=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),A4=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L4=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B4=new z({}),x4=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L682"}}),R4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),S4=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),P4=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$4=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I4=new z({}),j4=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L689"}}),D4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),q4=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),G4=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O4=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),X4=new z({}),z4=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L698"}}),W4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Q4=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),H4=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),U4=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),J4=new z({}),Y4=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L732"}}),Z4=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),eM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),oM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tM=new z({}),aM=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L739"}}),sM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),lM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),iM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cM=new z({}),fM=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L725"}}),gM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),hM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),pM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_M=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uM=new z({}),bM=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L707"}}),TM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),FM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),CM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),MM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),EM=new z({}),yM=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L714"}}),AM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),LM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),BM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),kM=new z({}),RM=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L748"}}),PM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),$M=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),IM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jM=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),NM=new z({}),DM=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L778"}}),GM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),OM=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),XM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zM=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VM=new z({}),WM=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L785"}}),HM=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),UM=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),JM=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),YM=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),KM=new z({}),ZM=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L808"}}),oE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),rE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),tE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nE=new z({}),sE=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L792"}}),iE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),dE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),cE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mE=new z({}),gE=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L799"}}),pE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),_E=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),uE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),TE=new z({}),FE=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L817"}}),ME=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),EE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),yE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wE=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),AE=new z({}),LE=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L771"}}),xE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),kE=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),RE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),SE=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),PE=new z({}),$E=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L755"}}),jE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),NE=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),DE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qE=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),GE=new z({}),OE=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_auto.py#L762"}}),zE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),VE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),WE=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),HE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),UE=new z({}),JE=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),KE=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),ZE=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),e3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),o3=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),r3=new z({}),t3=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),n3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),s3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),l3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),i3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),d3=new z({}),c3=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),m3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),g3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),h3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),p3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_3=new z({}),u3=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),v3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),T3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),F3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),C3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),M3=new z({}),E3=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),w3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),A3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),L3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),B3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),x3=new z({}),k3=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),S3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),P3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),$3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),I3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),j3=new z({}),N3=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),q3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),G3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),O3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),z3=new z({}),V3=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),Q3=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),H3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),U3=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J3=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y3=new z({}),K3=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),ey=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),oy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),ry=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ty=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ay=new z({}),ny=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),ly=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),iy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),dy=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fy=new z({}),my=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),hy=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),py=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),_y=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),by=new z({}),vy=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),Fy=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),My=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ey=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yy=new z({}),wy=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),Ly=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),By=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),xy=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ky=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ry=new z({}),Sy=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),$y=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),Iy=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),jy=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ny=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Dy=new z({}),qy=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),Oy=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),Xy=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),zy=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Vy=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Wy=new z({}),Qy=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),Uy=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),Jy=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),Yy=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ky=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Zy=new z({}),ew=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),rw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),tw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),aw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sw=new z({}),lw=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),dw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),cw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),fw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gw=new z({}),hw=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),_w=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),bw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Tw=new z({}),Fw=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),Mw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Ew=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),yw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ww=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Aw=new z({}),Lw=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),xw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),kw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),Rw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Sw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Pw=new z({}),$w=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),jw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Nw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),Dw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Gw=new z({}),Ow=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),zw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),Vw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),Ww=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Qw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Hw=new z({}),Uw=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),Yw=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Kw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),Zw=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rA=new z({}),tA=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),nA=new E({props:{name:"from\\_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),sA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),lA=new E({props:{name:"from\\_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15678/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15678/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15678/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),le=a("h1"),me=a("a"),oo=a("span"),f(ce.$$.fragment),ue=l(),No=a("span"),vi=o("Auto Classes"),gf=l(),ra=a("p"),Ti=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Fi=a("code"),LC=o("from_pretrained()"),hf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Ee=l(),so=a("p"),Ci=o("Instantiating one of "),Bn=a("a"),BC=o("AutoConfig"),xn=o(", "),kn=a("a"),xC=o("AutoModel"),Mi=o(`, and
`),Rn=a("a"),kC=o("AutoTokenizer"),Ei=o(" will directly create a class of the relevant architecture. For instance"),pf=l(),f(ka.$$.fragment),lo=l(),ge=a("p"),n0=o("will create a model that is an instance of "),yi=a("a"),s0=o("BertModel"),l0=o("."),Do=l(),Ra=a("p"),i0=o("There is one class of "),_f=a("code"),d0=o("AutoModel"),MBe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),bAe=l(),wi=a("h2"),uf=a("a"),Dz=a("span"),f(RC.$$.fragment),EBe=l(),qz=a("span"),yBe=o("Extending the Auto Classes"),vAe=l(),Sn=a("p"),wBe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),Gz=a("code"),ABe=o("NewModel"),LBe=o(", make sure you have a "),Oz=a("code"),BBe=o("NewModelConfig"),xBe=o(` then you can add those to the auto
classes like this:`),TAe=l(),f(SC.$$.fragment),FAe=l(),c0=a("p"),kBe=o("You will then be able to use the auto classes like you would usually do!"),CAe=l(),f(bf.$$.fragment),MAe=l(),Ai=a("h2"),vf=a("a"),Xz=a("span"),f(PC.$$.fragment),RBe=l(),zz=a("span"),SBe=o("AutoConfig"),EAe=l(),qo=a("div"),f($C.$$.fragment),PBe=l(),IC=a("p"),$Be=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),f0=a("a"),IBe=o("from_pretrained()"),jBe=o(" class method."),NBe=l(),jC=a("p"),DBe=o("This class cannot be instantiated directly using "),Vz=a("code"),qBe=o("__init__()"),GBe=o(" (throws an error)."),OBe=l(),io=a("div"),f(NC.$$.fragment),XBe=l(),Wz=a("p"),zBe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),VBe=l(),Li=a("p"),WBe=o("The configuration class to instantiate is selected based on the "),Qz=a("code"),QBe=o("model_type"),HBe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),Hz=a("code"),UBe=o("pretrained_model_name_or_path"),JBe=o(":"),YBe=l(),v=a("ul"),Tf=a("li"),Uz=a("strong"),KBe=o("albert"),ZBe=o(" \u2014 "),m0=a("a"),exe=o("AlbertConfig"),oxe=o(" (ALBERT model)"),rxe=l(),Ff=a("li"),Jz=a("strong"),txe=o("bart"),axe=o(" \u2014 "),g0=a("a"),nxe=o("BartConfig"),sxe=o(" (BART model)"),lxe=l(),Cf=a("li"),Yz=a("strong"),ixe=o("beit"),dxe=o(" \u2014 "),h0=a("a"),cxe=o("BeitConfig"),fxe=o(" (BEiT model)"),mxe=l(),Mf=a("li"),Kz=a("strong"),gxe=o("bert"),hxe=o(" \u2014 "),p0=a("a"),pxe=o("BertConfig"),_xe=o(" (BERT model)"),uxe=l(),Ef=a("li"),Zz=a("strong"),bxe=o("bert-generation"),vxe=o(" \u2014 "),_0=a("a"),Txe=o("BertGenerationConfig"),Fxe=o(" (Bert Generation model)"),Cxe=l(),yf=a("li"),eV=a("strong"),Mxe=o("big_bird"),Exe=o(" \u2014 "),u0=a("a"),yxe=o("BigBirdConfig"),wxe=o(" (BigBird model)"),Axe=l(),wf=a("li"),oV=a("strong"),Lxe=o("bigbird_pegasus"),Bxe=o(" \u2014 "),b0=a("a"),xxe=o("BigBirdPegasusConfig"),kxe=o(" (BigBirdPegasus model)"),Rxe=l(),Af=a("li"),rV=a("strong"),Sxe=o("blenderbot"),Pxe=o(" \u2014 "),v0=a("a"),$xe=o("BlenderbotConfig"),Ixe=o(" (Blenderbot model)"),jxe=l(),Lf=a("li"),tV=a("strong"),Nxe=o("blenderbot-small"),Dxe=o(" \u2014 "),T0=a("a"),qxe=o("BlenderbotSmallConfig"),Gxe=o(" (BlenderbotSmall model)"),Oxe=l(),Bf=a("li"),aV=a("strong"),Xxe=o("camembert"),zxe=o(" \u2014 "),F0=a("a"),Vxe=o("CamembertConfig"),Wxe=o(" (CamemBERT model)"),Qxe=l(),xf=a("li"),nV=a("strong"),Hxe=o("canine"),Uxe=o(" \u2014 "),C0=a("a"),Jxe=o("CanineConfig"),Yxe=o(" (Canine model)"),Kxe=l(),kf=a("li"),sV=a("strong"),Zxe=o("clip"),eke=o(" \u2014 "),M0=a("a"),oke=o("CLIPConfig"),rke=o(" (CLIP model)"),tke=l(),Rf=a("li"),lV=a("strong"),ake=o("convbert"),nke=o(" \u2014 "),E0=a("a"),ske=o("ConvBertConfig"),lke=o(" (ConvBERT model)"),ike=l(),Sf=a("li"),iV=a("strong"),dke=o("convnext"),cke=o(" \u2014 "),y0=a("a"),fke=o("ConvNextConfig"),mke=o(" (ConvNext model)"),gke=l(),Pf=a("li"),dV=a("strong"),hke=o("ctrl"),pke=o(" \u2014 "),w0=a("a"),_ke=o("CTRLConfig"),uke=o(" (CTRL model)"),bke=l(),$f=a("li"),cV=a("strong"),vke=o("deberta"),Tke=o(" \u2014 "),A0=a("a"),Fke=o("DebertaConfig"),Cke=o(" (DeBERTa model)"),Mke=l(),If=a("li"),fV=a("strong"),Eke=o("deberta-v2"),yke=o(" \u2014 "),L0=a("a"),wke=o("DebertaV2Config"),Ake=o(" (DeBERTa-v2 model)"),Lke=l(),jf=a("li"),mV=a("strong"),Bke=o("deit"),xke=o(" \u2014 "),B0=a("a"),kke=o("DeiTConfig"),Rke=o(" (DeiT model)"),Ske=l(),Nf=a("li"),gV=a("strong"),Pke=o("detr"),$ke=o(" \u2014 "),x0=a("a"),Ike=o("DetrConfig"),jke=o(" (DETR model)"),Nke=l(),Df=a("li"),hV=a("strong"),Dke=o("distilbert"),qke=o(" \u2014 "),k0=a("a"),Gke=o("DistilBertConfig"),Oke=o(" (DistilBERT model)"),Xke=l(),qf=a("li"),pV=a("strong"),zke=o("dpr"),Vke=o(" \u2014 "),R0=a("a"),Wke=o("DPRConfig"),Qke=o(" (DPR model)"),Hke=l(),Gf=a("li"),_V=a("strong"),Uke=o("electra"),Jke=o(" \u2014 "),S0=a("a"),Yke=o("ElectraConfig"),Kke=o(" (ELECTRA model)"),Zke=l(),Of=a("li"),uV=a("strong"),eRe=o("encoder-decoder"),oRe=o(" \u2014 "),P0=a("a"),rRe=o("EncoderDecoderConfig"),tRe=o(" (Encoder decoder model)"),aRe=l(),Xf=a("li"),bV=a("strong"),nRe=o("flaubert"),sRe=o(" \u2014 "),$0=a("a"),lRe=o("FlaubertConfig"),iRe=o(" (FlauBERT model)"),dRe=l(),zf=a("li"),vV=a("strong"),cRe=o("fnet"),fRe=o(" \u2014 "),I0=a("a"),mRe=o("FNetConfig"),gRe=o(" (FNet model)"),hRe=l(),Vf=a("li"),TV=a("strong"),pRe=o("fsmt"),_Re=o(" \u2014 "),j0=a("a"),uRe=o("FSMTConfig"),bRe=o(" (FairSeq Machine-Translation model)"),vRe=l(),Wf=a("li"),FV=a("strong"),TRe=o("funnel"),FRe=o(" \u2014 "),N0=a("a"),CRe=o("FunnelConfig"),MRe=o(" (Funnel Transformer model)"),ERe=l(),Qf=a("li"),CV=a("strong"),yRe=o("gpt2"),wRe=o(" \u2014 "),D0=a("a"),ARe=o("GPT2Config"),LRe=o(" (OpenAI GPT-2 model)"),BRe=l(),Hf=a("li"),MV=a("strong"),xRe=o("gpt_neo"),kRe=o(" \u2014 "),q0=a("a"),RRe=o("GPTNeoConfig"),SRe=o(" (GPT Neo model)"),PRe=l(),Uf=a("li"),EV=a("strong"),$Re=o("gptj"),IRe=o(" \u2014 "),G0=a("a"),jRe=o("GPTJConfig"),NRe=o(" (GPT-J model)"),DRe=l(),Jf=a("li"),yV=a("strong"),qRe=o("hubert"),GRe=o(" \u2014 "),O0=a("a"),ORe=o("HubertConfig"),XRe=o(" (Hubert model)"),zRe=l(),Yf=a("li"),wV=a("strong"),VRe=o("ibert"),WRe=o(" \u2014 "),X0=a("a"),QRe=o("IBertConfig"),HRe=o(" (I-BERT model)"),URe=l(),Kf=a("li"),AV=a("strong"),JRe=o("imagegpt"),YRe=o(" \u2014 "),z0=a("a"),KRe=o("ImageGPTConfig"),ZRe=o(" (ImageGPT model)"),eSe=l(),Zf=a("li"),LV=a("strong"),oSe=o("layoutlm"),rSe=o(" \u2014 "),V0=a("a"),tSe=o("LayoutLMConfig"),aSe=o(" (LayoutLM model)"),nSe=l(),em=a("li"),BV=a("strong"),sSe=o("layoutlmv2"),lSe=o(" \u2014 "),W0=a("a"),iSe=o("LayoutLMv2Config"),dSe=o(" (LayoutLMv2 model)"),cSe=l(),om=a("li"),xV=a("strong"),fSe=o("led"),mSe=o(" \u2014 "),Q0=a("a"),gSe=o("LEDConfig"),hSe=o(" (LED model)"),pSe=l(),rm=a("li"),kV=a("strong"),_Se=o("longformer"),uSe=o(" \u2014 "),H0=a("a"),bSe=o("LongformerConfig"),vSe=o(" (Longformer model)"),TSe=l(),tm=a("li"),RV=a("strong"),FSe=o("luke"),CSe=o(" \u2014 "),U0=a("a"),MSe=o("LukeConfig"),ESe=o(" (LUKE model)"),ySe=l(),am=a("li"),SV=a("strong"),wSe=o("lxmert"),ASe=o(" \u2014 "),J0=a("a"),LSe=o("LxmertConfig"),BSe=o(" (LXMERT model)"),xSe=l(),nm=a("li"),PV=a("strong"),kSe=o("m2m_100"),RSe=o(" \u2014 "),Y0=a("a"),SSe=o("M2M100Config"),PSe=o(" (M2M100 model)"),$Se=l(),sm=a("li"),$V=a("strong"),ISe=o("marian"),jSe=o(" \u2014 "),K0=a("a"),NSe=o("MarianConfig"),DSe=o(" (Marian model)"),qSe=l(),lm=a("li"),IV=a("strong"),GSe=o("mbart"),OSe=o(" \u2014 "),Z0=a("a"),XSe=o("MBartConfig"),zSe=o(" (mBART model)"),VSe=l(),im=a("li"),jV=a("strong"),WSe=o("megatron-bert"),QSe=o(" \u2014 "),eL=a("a"),HSe=o("MegatronBertConfig"),USe=o(" (MegatronBert model)"),JSe=l(),dm=a("li"),NV=a("strong"),YSe=o("mobilebert"),KSe=o(" \u2014 "),oL=a("a"),ZSe=o("MobileBertConfig"),ePe=o(" (MobileBERT model)"),oPe=l(),cm=a("li"),DV=a("strong"),rPe=o("mpnet"),tPe=o(" \u2014 "),rL=a("a"),aPe=o("MPNetConfig"),nPe=o(" (MPNet model)"),sPe=l(),fm=a("li"),qV=a("strong"),lPe=o("mt5"),iPe=o(" \u2014 "),tL=a("a"),dPe=o("MT5Config"),cPe=o(" (mT5 model)"),fPe=l(),mm=a("li"),GV=a("strong"),mPe=o("nystromformer"),gPe=o(" \u2014 "),aL=a("a"),hPe=o("NystromformerConfig"),pPe=o(" (Nystromformer model)"),_Pe=l(),gm=a("li"),OV=a("strong"),uPe=o("openai-gpt"),bPe=o(" \u2014 "),nL=a("a"),vPe=o("OpenAIGPTConfig"),TPe=o(" (OpenAI GPT model)"),FPe=l(),hm=a("li"),XV=a("strong"),CPe=o("pegasus"),MPe=o(" \u2014 "),sL=a("a"),EPe=o("PegasusConfig"),yPe=o(" (Pegasus model)"),wPe=l(),pm=a("li"),zV=a("strong"),APe=o("perceiver"),LPe=o(" \u2014 "),lL=a("a"),BPe=o("PerceiverConfig"),xPe=o(" (Perceiver model)"),kPe=l(),_m=a("li"),VV=a("strong"),RPe=o("prophetnet"),SPe=o(" \u2014 "),iL=a("a"),PPe=o("ProphetNetConfig"),$Pe=o(" (ProphetNet model)"),IPe=l(),um=a("li"),WV=a("strong"),jPe=o("qdqbert"),NPe=o(" \u2014 "),dL=a("a"),DPe=o("QDQBertConfig"),qPe=o(" (QDQBert model)"),GPe=l(),bm=a("li"),QV=a("strong"),OPe=o("rag"),XPe=o(" \u2014 "),cL=a("a"),zPe=o("RagConfig"),VPe=o(" (RAG model)"),WPe=l(),vm=a("li"),HV=a("strong"),QPe=o("realm"),HPe=o(" \u2014 "),fL=a("a"),UPe=o("RealmConfig"),JPe=o(" (Realm model)"),YPe=l(),Tm=a("li"),UV=a("strong"),KPe=o("reformer"),ZPe=o(" \u2014 "),mL=a("a"),e$e=o("ReformerConfig"),o$e=o(" (Reformer model)"),r$e=l(),Fm=a("li"),JV=a("strong"),t$e=o("rembert"),a$e=o(" \u2014 "),gL=a("a"),n$e=o("RemBertConfig"),s$e=o(" (RemBERT model)"),l$e=l(),Cm=a("li"),YV=a("strong"),i$e=o("retribert"),d$e=o(" \u2014 "),hL=a("a"),c$e=o("RetriBertConfig"),f$e=o(" (RetriBERT model)"),m$e=l(),Mm=a("li"),KV=a("strong"),g$e=o("roberta"),h$e=o(" \u2014 "),pL=a("a"),p$e=o("RobertaConfig"),_$e=o(" (RoBERTa model)"),u$e=l(),Em=a("li"),ZV=a("strong"),b$e=o("roformer"),v$e=o(" \u2014 "),_L=a("a"),T$e=o("RoFormerConfig"),F$e=o(" (RoFormer model)"),C$e=l(),ym=a("li"),eW=a("strong"),M$e=o("segformer"),E$e=o(" \u2014 "),uL=a("a"),y$e=o("SegformerConfig"),w$e=o(" (SegFormer model)"),A$e=l(),wm=a("li"),oW=a("strong"),L$e=o("sew"),B$e=o(" \u2014 "),bL=a("a"),x$e=o("SEWConfig"),k$e=o(" (SEW model)"),R$e=l(),Am=a("li"),rW=a("strong"),S$e=o("sew-d"),P$e=o(" \u2014 "),vL=a("a"),$$e=o("SEWDConfig"),I$e=o(" (SEW-D model)"),j$e=l(),Lm=a("li"),tW=a("strong"),N$e=o("speech-encoder-decoder"),D$e=o(" \u2014 "),TL=a("a"),q$e=o("SpeechEncoderDecoderConfig"),G$e=o(" (Speech Encoder decoder model)"),O$e=l(),Bm=a("li"),aW=a("strong"),X$e=o("speech_to_text"),z$e=o(" \u2014 "),FL=a("a"),V$e=o("Speech2TextConfig"),W$e=o(" (Speech2Text model)"),Q$e=l(),xm=a("li"),nW=a("strong"),H$e=o("speech_to_text_2"),U$e=o(" \u2014 "),CL=a("a"),J$e=o("Speech2Text2Config"),Y$e=o(" (Speech2Text2 model)"),K$e=l(),km=a("li"),sW=a("strong"),Z$e=o("splinter"),eIe=o(" \u2014 "),ML=a("a"),oIe=o("SplinterConfig"),rIe=o(" (Splinter model)"),tIe=l(),Rm=a("li"),lW=a("strong"),aIe=o("squeezebert"),nIe=o(" \u2014 "),EL=a("a"),sIe=o("SqueezeBertConfig"),lIe=o(" (SqueezeBERT model)"),iIe=l(),Sm=a("li"),iW=a("strong"),dIe=o("swin"),cIe=o(" \u2014 "),yL=a("a"),fIe=o("SwinConfig"),mIe=o(" (Swin model)"),gIe=l(),Pm=a("li"),dW=a("strong"),hIe=o("t5"),pIe=o(" \u2014 "),wL=a("a"),_Ie=o("T5Config"),uIe=o(" (T5 model)"),bIe=l(),$m=a("li"),cW=a("strong"),vIe=o("tapas"),TIe=o(" \u2014 "),AL=a("a"),FIe=o("TapasConfig"),CIe=o(" (TAPAS model)"),MIe=l(),Im=a("li"),fW=a("strong"),EIe=o("transfo-xl"),yIe=o(" \u2014 "),LL=a("a"),wIe=o("TransfoXLConfig"),AIe=o(" (Transformer-XL model)"),LIe=l(),jm=a("li"),mW=a("strong"),BIe=o("trocr"),xIe=o(" \u2014 "),BL=a("a"),kIe=o("TrOCRConfig"),RIe=o(" (TrOCR model)"),SIe=l(),Nm=a("li"),gW=a("strong"),PIe=o("unispeech"),$Ie=o(" \u2014 "),xL=a("a"),IIe=o("UniSpeechConfig"),jIe=o(" (UniSpeech model)"),NIe=l(),Dm=a("li"),hW=a("strong"),DIe=o("unispeech-sat"),qIe=o(" \u2014 "),kL=a("a"),GIe=o("UniSpeechSatConfig"),OIe=o(" (UniSpeechSat model)"),XIe=l(),qm=a("li"),pW=a("strong"),zIe=o("vilt"),VIe=o(" \u2014 "),RL=a("a"),WIe=o("ViltConfig"),QIe=o(" (ViLT model)"),HIe=l(),Gm=a("li"),_W=a("strong"),UIe=o("vision-encoder-decoder"),JIe=o(" \u2014 "),SL=a("a"),YIe=o("VisionEncoderDecoderConfig"),KIe=o(" (Vision Encoder decoder model)"),ZIe=l(),Om=a("li"),uW=a("strong"),eje=o("vision-text-dual-encoder"),oje=o(" \u2014 "),PL=a("a"),rje=o("VisionTextDualEncoderConfig"),tje=o(" (VisionTextDualEncoder model)"),aje=l(),Xm=a("li"),bW=a("strong"),nje=o("visual_bert"),sje=o(" \u2014 "),$L=a("a"),lje=o("VisualBertConfig"),ije=o(" (VisualBert model)"),dje=l(),zm=a("li"),vW=a("strong"),cje=o("vit"),fje=o(" \u2014 "),IL=a("a"),mje=o("ViTConfig"),gje=o(" (ViT model)"),hje=l(),Vm=a("li"),TW=a("strong"),pje=o("vit_mae"),_je=o(" \u2014 "),jL=a("a"),uje=o("ViTMAEConfig"),bje=o(" (ViTMAE model)"),vje=l(),Wm=a("li"),FW=a("strong"),Tje=o("wav2vec2"),Fje=o(" \u2014 "),NL=a("a"),Cje=o("Wav2Vec2Config"),Mje=o(" (Wav2Vec2 model)"),Eje=l(),Qm=a("li"),CW=a("strong"),yje=o("wavlm"),wje=o(" \u2014 "),DL=a("a"),Aje=o("WavLMConfig"),Lje=o(" (WavLM model)"),Bje=l(),Hm=a("li"),MW=a("strong"),xje=o("xglm"),kje=o(" \u2014 "),qL=a("a"),Rje=o("XGLMConfig"),Sje=o(" (XGLM model)"),Pje=l(),Um=a("li"),EW=a("strong"),$je=o("xlm"),Ije=o(" \u2014 "),GL=a("a"),jje=o("XLMConfig"),Nje=o(" (XLM model)"),Dje=l(),Jm=a("li"),yW=a("strong"),qje=o("xlm-prophetnet"),Gje=o(" \u2014 "),OL=a("a"),Oje=o("XLMProphetNetConfig"),Xje=o(" (XLMProphetNet model)"),zje=l(),Ym=a("li"),wW=a("strong"),Vje=o("xlm-roberta"),Wje=o(" \u2014 "),XL=a("a"),Qje=o("XLMRobertaConfig"),Hje=o(" (XLM-RoBERTa model)"),Uje=l(),Km=a("li"),AW=a("strong"),Jje=o("xlm-roberta-xl"),Yje=o(" \u2014 "),zL=a("a"),Kje=o("XLMRobertaXLConfig"),Zje=o(" (XLM-RoBERTa-XL model)"),eNe=l(),Zm=a("li"),LW=a("strong"),oNe=o("xlnet"),rNe=o(" \u2014 "),VL=a("a"),tNe=o("XLNetConfig"),aNe=o(" (XLNet model)"),nNe=l(),eg=a("li"),BW=a("strong"),sNe=o("yoso"),lNe=o(" \u2014 "),WL=a("a"),iNe=o("YosoConfig"),dNe=o(" (YOSO model)"),cNe=l(),xW=a("p"),fNe=o("Examples:"),mNe=l(),f(DC.$$.fragment),gNe=l(),og=a("div"),f(qC.$$.fragment),hNe=l(),kW=a("p"),pNe=o("Register a new configuration for this class."),yAe=l(),Bi=a("h2"),rg=a("a"),RW=a("span"),f(GC.$$.fragment),_Ne=l(),SW=a("span"),uNe=o("AutoTokenizer"),wAe=l(),Go=a("div"),f(OC.$$.fragment),bNe=l(),XC=a("p"),vNe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),QL=a("a"),TNe=o("AutoTokenizer.from_pretrained()"),FNe=o(" class method."),CNe=l(),zC=a("p"),MNe=o("This class cannot be instantiated directly using "),PW=a("code"),ENe=o("__init__()"),yNe=o(" (throws an error)."),wNe=l(),co=a("div"),f(VC.$$.fragment),ANe=l(),$W=a("p"),LNe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),BNe=l(),Sa=a("p"),xNe=o("The tokenizer class to instantiate is selected based on the "),IW=a("code"),kNe=o("model_type"),RNe=o(` property of the config object (either
passed as an argument or loaded from `),jW=a("code"),SNe=o("pretrained_model_name_or_path"),PNe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),NW=a("code"),$Ne=o("pretrained_model_name_or_path"),INe=o(":"),jNe=l(),M=a("ul"),Pn=a("li"),DW=a("strong"),NNe=o("albert"),DNe=o(" \u2014 "),HL=a("a"),qNe=o("AlbertTokenizer"),GNe=o(" or "),UL=a("a"),ONe=o("AlbertTokenizerFast"),XNe=o(" (ALBERT model)"),zNe=l(),$n=a("li"),qW=a("strong"),VNe=o("bart"),WNe=o(" \u2014 "),JL=a("a"),QNe=o("BartTokenizer"),HNe=o(" or "),YL=a("a"),UNe=o("BartTokenizerFast"),JNe=o(" (BART model)"),YNe=l(),In=a("li"),GW=a("strong"),KNe=o("barthez"),ZNe=o(" \u2014 "),KL=a("a"),eDe=o("BarthezTokenizer"),oDe=o(" or "),ZL=a("a"),rDe=o("BarthezTokenizerFast"),tDe=o(" (BARThez model)"),aDe=l(),tg=a("li"),OW=a("strong"),nDe=o("bartpho"),sDe=o(" \u2014 "),e9=a("a"),lDe=o("BartphoTokenizer"),iDe=o(" (BARTpho model)"),dDe=l(),jn=a("li"),XW=a("strong"),cDe=o("bert"),fDe=o(" \u2014 "),o9=a("a"),mDe=o("BertTokenizer"),gDe=o(" or "),r9=a("a"),hDe=o("BertTokenizerFast"),pDe=o(" (BERT model)"),_De=l(),ag=a("li"),zW=a("strong"),uDe=o("bert-generation"),bDe=o(" \u2014 "),t9=a("a"),vDe=o("BertGenerationTokenizer"),TDe=o(" (Bert Generation model)"),FDe=l(),ng=a("li"),VW=a("strong"),CDe=o("bert-japanese"),MDe=o(" \u2014 "),a9=a("a"),EDe=o("BertJapaneseTokenizer"),yDe=o(" (BertJapanese model)"),wDe=l(),sg=a("li"),WW=a("strong"),ADe=o("bertweet"),LDe=o(" \u2014 "),n9=a("a"),BDe=o("BertweetTokenizer"),xDe=o(" (Bertweet model)"),kDe=l(),Nn=a("li"),QW=a("strong"),RDe=o("big_bird"),SDe=o(" \u2014 "),s9=a("a"),PDe=o("BigBirdTokenizer"),$De=o(" or "),l9=a("a"),IDe=o("BigBirdTokenizerFast"),jDe=o(" (BigBird model)"),NDe=l(),Dn=a("li"),HW=a("strong"),DDe=o("bigbird_pegasus"),qDe=o(" \u2014 "),i9=a("a"),GDe=o("PegasusTokenizer"),ODe=o(" or "),d9=a("a"),XDe=o("PegasusTokenizerFast"),zDe=o(" (BigBirdPegasus model)"),VDe=l(),qn=a("li"),UW=a("strong"),WDe=o("blenderbot"),QDe=o(" \u2014 "),c9=a("a"),HDe=o("BlenderbotTokenizer"),UDe=o(" or "),f9=a("a"),JDe=o("BlenderbotTokenizerFast"),YDe=o(" (Blenderbot model)"),KDe=l(),lg=a("li"),JW=a("strong"),ZDe=o("blenderbot-small"),eqe=o(" \u2014 "),m9=a("a"),oqe=o("BlenderbotSmallTokenizer"),rqe=o(" (BlenderbotSmall model)"),tqe=l(),ig=a("li"),YW=a("strong"),aqe=o("byt5"),nqe=o(" \u2014 "),g9=a("a"),sqe=o("ByT5Tokenizer"),lqe=o(" (ByT5 model)"),iqe=l(),Gn=a("li"),KW=a("strong"),dqe=o("camembert"),cqe=o(" \u2014 "),h9=a("a"),fqe=o("CamembertTokenizer"),mqe=o(" or "),p9=a("a"),gqe=o("CamembertTokenizerFast"),hqe=o(" (CamemBERT model)"),pqe=l(),dg=a("li"),ZW=a("strong"),_qe=o("canine"),uqe=o(" \u2014 "),_9=a("a"),bqe=o("CanineTokenizer"),vqe=o(" (Canine model)"),Tqe=l(),On=a("li"),eQ=a("strong"),Fqe=o("clip"),Cqe=o(" \u2014 "),u9=a("a"),Mqe=o("CLIPTokenizer"),Eqe=o(" or "),b9=a("a"),yqe=o("CLIPTokenizerFast"),wqe=o(" (CLIP model)"),Aqe=l(),Xn=a("li"),oQ=a("strong"),Lqe=o("convbert"),Bqe=o(" \u2014 "),v9=a("a"),xqe=o("ConvBertTokenizer"),kqe=o(" or "),T9=a("a"),Rqe=o("ConvBertTokenizerFast"),Sqe=o(" (ConvBERT model)"),Pqe=l(),zn=a("li"),rQ=a("strong"),$qe=o("cpm"),Iqe=o(" \u2014 "),F9=a("a"),jqe=o("CpmTokenizer"),Nqe=o(" or "),tQ=a("code"),Dqe=o("CpmTokenizerFast"),qqe=o(" (CPM model)"),Gqe=l(),cg=a("li"),aQ=a("strong"),Oqe=o("ctrl"),Xqe=o(" \u2014 "),C9=a("a"),zqe=o("CTRLTokenizer"),Vqe=o(" (CTRL model)"),Wqe=l(),Vn=a("li"),nQ=a("strong"),Qqe=o("deberta"),Hqe=o(" \u2014 "),M9=a("a"),Uqe=o("DebertaTokenizer"),Jqe=o(" or "),E9=a("a"),Yqe=o("DebertaTokenizerFast"),Kqe=o(" (DeBERTa model)"),Zqe=l(),fg=a("li"),sQ=a("strong"),eGe=o("deberta-v2"),oGe=o(" \u2014 "),y9=a("a"),rGe=o("DebertaV2Tokenizer"),tGe=o(" (DeBERTa-v2 model)"),aGe=l(),Wn=a("li"),lQ=a("strong"),nGe=o("distilbert"),sGe=o(" \u2014 "),w9=a("a"),lGe=o("DistilBertTokenizer"),iGe=o(" or "),A9=a("a"),dGe=o("DistilBertTokenizerFast"),cGe=o(" (DistilBERT model)"),fGe=l(),Qn=a("li"),iQ=a("strong"),mGe=o("dpr"),gGe=o(" \u2014 "),L9=a("a"),hGe=o("DPRQuestionEncoderTokenizer"),pGe=o(" or "),B9=a("a"),_Ge=o("DPRQuestionEncoderTokenizerFast"),uGe=o(" (DPR model)"),bGe=l(),Hn=a("li"),dQ=a("strong"),vGe=o("electra"),TGe=o(" \u2014 "),x9=a("a"),FGe=o("ElectraTokenizer"),CGe=o(" or "),k9=a("a"),MGe=o("ElectraTokenizerFast"),EGe=o(" (ELECTRA model)"),yGe=l(),mg=a("li"),cQ=a("strong"),wGe=o("flaubert"),AGe=o(" \u2014 "),R9=a("a"),LGe=o("FlaubertTokenizer"),BGe=o(" (FlauBERT model)"),xGe=l(),Un=a("li"),fQ=a("strong"),kGe=o("fnet"),RGe=o(" \u2014 "),S9=a("a"),SGe=o("FNetTokenizer"),PGe=o(" or "),P9=a("a"),$Ge=o("FNetTokenizerFast"),IGe=o(" (FNet model)"),jGe=l(),gg=a("li"),mQ=a("strong"),NGe=o("fsmt"),DGe=o(" \u2014 "),$9=a("a"),qGe=o("FSMTTokenizer"),GGe=o(" (FairSeq Machine-Translation model)"),OGe=l(),Jn=a("li"),gQ=a("strong"),XGe=o("funnel"),zGe=o(" \u2014 "),I9=a("a"),VGe=o("FunnelTokenizer"),WGe=o(" or "),j9=a("a"),QGe=o("FunnelTokenizerFast"),HGe=o(" (Funnel Transformer model)"),UGe=l(),Yn=a("li"),hQ=a("strong"),JGe=o("gpt2"),YGe=o(" \u2014 "),N9=a("a"),KGe=o("GPT2Tokenizer"),ZGe=o(" or "),D9=a("a"),eOe=o("GPT2TokenizerFast"),oOe=o(" (OpenAI GPT-2 model)"),rOe=l(),Kn=a("li"),pQ=a("strong"),tOe=o("gpt_neo"),aOe=o(" \u2014 "),q9=a("a"),nOe=o("GPT2Tokenizer"),sOe=o(" or "),G9=a("a"),lOe=o("GPT2TokenizerFast"),iOe=o(" (GPT Neo model)"),dOe=l(),Zn=a("li"),_Q=a("strong"),cOe=o("herbert"),fOe=o(" \u2014 "),O9=a("a"),mOe=o("HerbertTokenizer"),gOe=o(" or "),X9=a("a"),hOe=o("HerbertTokenizerFast"),pOe=o(" (HerBERT model)"),_Oe=l(),hg=a("li"),uQ=a("strong"),uOe=o("hubert"),bOe=o(" \u2014 "),z9=a("a"),vOe=o("Wav2Vec2CTCTokenizer"),TOe=o(" (Hubert model)"),FOe=l(),es=a("li"),bQ=a("strong"),COe=o("ibert"),MOe=o(" \u2014 "),V9=a("a"),EOe=o("RobertaTokenizer"),yOe=o(" or "),W9=a("a"),wOe=o("RobertaTokenizerFast"),AOe=o(" (I-BERT model)"),LOe=l(),os=a("li"),vQ=a("strong"),BOe=o("layoutlm"),xOe=o(" \u2014 "),Q9=a("a"),kOe=o("LayoutLMTokenizer"),ROe=o(" or "),H9=a("a"),SOe=o("LayoutLMTokenizerFast"),POe=o(" (LayoutLM model)"),$Oe=l(),rs=a("li"),TQ=a("strong"),IOe=o("layoutlmv2"),jOe=o(" \u2014 "),U9=a("a"),NOe=o("LayoutLMv2Tokenizer"),DOe=o(" or "),J9=a("a"),qOe=o("LayoutLMv2TokenizerFast"),GOe=o(" (LayoutLMv2 model)"),OOe=l(),ts=a("li"),FQ=a("strong"),XOe=o("layoutxlm"),zOe=o(" \u2014 "),Y9=a("a"),VOe=o("LayoutXLMTokenizer"),WOe=o(" or "),K9=a("a"),QOe=o("LayoutXLMTokenizerFast"),HOe=o(" (LayoutXLM model)"),UOe=l(),as=a("li"),CQ=a("strong"),JOe=o("led"),YOe=o(" \u2014 "),Z9=a("a"),KOe=o("LEDTokenizer"),ZOe=o(" or "),eB=a("a"),eXe=o("LEDTokenizerFast"),oXe=o(" (LED model)"),rXe=l(),ns=a("li"),MQ=a("strong"),tXe=o("longformer"),aXe=o(" \u2014 "),oB=a("a"),nXe=o("LongformerTokenizer"),sXe=o(" or "),rB=a("a"),lXe=o("LongformerTokenizerFast"),iXe=o(" (Longformer model)"),dXe=l(),pg=a("li"),EQ=a("strong"),cXe=o("luke"),fXe=o(" \u2014 "),tB=a("a"),mXe=o("LukeTokenizer"),gXe=o(" (LUKE model)"),hXe=l(),ss=a("li"),yQ=a("strong"),pXe=o("lxmert"),_Xe=o(" \u2014 "),aB=a("a"),uXe=o("LxmertTokenizer"),bXe=o(" or "),nB=a("a"),vXe=o("LxmertTokenizerFast"),TXe=o(" (LXMERT model)"),FXe=l(),_g=a("li"),wQ=a("strong"),CXe=o("m2m_100"),MXe=o(" \u2014 "),sB=a("a"),EXe=o("M2M100Tokenizer"),yXe=o(" (M2M100 model)"),wXe=l(),ug=a("li"),AQ=a("strong"),AXe=o("marian"),LXe=o(" \u2014 "),lB=a("a"),BXe=o("MarianTokenizer"),xXe=o(" (Marian model)"),kXe=l(),ls=a("li"),LQ=a("strong"),RXe=o("mbart"),SXe=o(" \u2014 "),iB=a("a"),PXe=o("MBartTokenizer"),$Xe=o(" or "),dB=a("a"),IXe=o("MBartTokenizerFast"),jXe=o(" (mBART model)"),NXe=l(),is=a("li"),BQ=a("strong"),DXe=o("mbart50"),qXe=o(" \u2014 "),cB=a("a"),GXe=o("MBart50Tokenizer"),OXe=o(" or "),fB=a("a"),XXe=o("MBart50TokenizerFast"),zXe=o(" (mBART-50 model)"),VXe=l(),bg=a("li"),xQ=a("strong"),WXe=o("mluke"),QXe=o(" \u2014 "),mB=a("a"),HXe=o("MLukeTokenizer"),UXe=o(" (mLUKE model)"),JXe=l(),ds=a("li"),kQ=a("strong"),YXe=o("mobilebert"),KXe=o(" \u2014 "),gB=a("a"),ZXe=o("MobileBertTokenizer"),eze=o(" or "),hB=a("a"),oze=o("MobileBertTokenizerFast"),rze=o(" (MobileBERT model)"),tze=l(),cs=a("li"),RQ=a("strong"),aze=o("mpnet"),nze=o(" \u2014 "),pB=a("a"),sze=o("MPNetTokenizer"),lze=o(" or "),_B=a("a"),ize=o("MPNetTokenizerFast"),dze=o(" (MPNet model)"),cze=l(),fs=a("li"),SQ=a("strong"),fze=o("mt5"),mze=o(" \u2014 "),uB=a("a"),gze=o("MT5Tokenizer"),hze=o(" or "),bB=a("a"),pze=o("MT5TokenizerFast"),_ze=o(" (mT5 model)"),uze=l(),ms=a("li"),PQ=a("strong"),bze=o("openai-gpt"),vze=o(" \u2014 "),vB=a("a"),Tze=o("OpenAIGPTTokenizer"),Fze=o(" or "),TB=a("a"),Cze=o("OpenAIGPTTokenizerFast"),Mze=o(" (OpenAI GPT model)"),Eze=l(),gs=a("li"),$Q=a("strong"),yze=o("pegasus"),wze=o(" \u2014 "),FB=a("a"),Aze=o("PegasusTokenizer"),Lze=o(" or "),CB=a("a"),Bze=o("PegasusTokenizerFast"),xze=o(" (Pegasus model)"),kze=l(),vg=a("li"),IQ=a("strong"),Rze=o("perceiver"),Sze=o(" \u2014 "),MB=a("a"),Pze=o("PerceiverTokenizer"),$ze=o(" (Perceiver model)"),Ize=l(),Tg=a("li"),jQ=a("strong"),jze=o("phobert"),Nze=o(" \u2014 "),EB=a("a"),Dze=o("PhobertTokenizer"),qze=o(" (PhoBERT model)"),Gze=l(),Fg=a("li"),NQ=a("strong"),Oze=o("prophetnet"),Xze=o(" \u2014 "),yB=a("a"),zze=o("ProphetNetTokenizer"),Vze=o(" (ProphetNet model)"),Wze=l(),hs=a("li"),DQ=a("strong"),Qze=o("qdqbert"),Hze=o(" \u2014 "),wB=a("a"),Uze=o("BertTokenizer"),Jze=o(" or "),AB=a("a"),Yze=o("BertTokenizerFast"),Kze=o(" (QDQBert model)"),Zze=l(),Cg=a("li"),qQ=a("strong"),eVe=o("rag"),oVe=o(" \u2014 "),LB=a("a"),rVe=o("RagTokenizer"),tVe=o(" (RAG model)"),aVe=l(),ps=a("li"),GQ=a("strong"),nVe=o("reformer"),sVe=o(" \u2014 "),BB=a("a"),lVe=o("ReformerTokenizer"),iVe=o(" or "),xB=a("a"),dVe=o("ReformerTokenizerFast"),cVe=o(" (Reformer model)"),fVe=l(),_s=a("li"),OQ=a("strong"),mVe=o("rembert"),gVe=o(" \u2014 "),kB=a("a"),hVe=o("RemBertTokenizer"),pVe=o(" or "),RB=a("a"),_Ve=o("RemBertTokenizerFast"),uVe=o(" (RemBERT model)"),bVe=l(),us=a("li"),XQ=a("strong"),vVe=o("retribert"),TVe=o(" \u2014 "),SB=a("a"),FVe=o("RetriBertTokenizer"),CVe=o(" or "),PB=a("a"),MVe=o("RetriBertTokenizerFast"),EVe=o(" (RetriBERT model)"),yVe=l(),bs=a("li"),zQ=a("strong"),wVe=o("roberta"),AVe=o(" \u2014 "),$B=a("a"),LVe=o("RobertaTokenizer"),BVe=o(" or "),IB=a("a"),xVe=o("RobertaTokenizerFast"),kVe=o(" (RoBERTa model)"),RVe=l(),vs=a("li"),VQ=a("strong"),SVe=o("roformer"),PVe=o(" \u2014 "),jB=a("a"),$Ve=o("RoFormerTokenizer"),IVe=o(" or "),NB=a("a"),jVe=o("RoFormerTokenizerFast"),NVe=o(" (RoFormer model)"),DVe=l(),Mg=a("li"),WQ=a("strong"),qVe=o("speech_to_text"),GVe=o(" \u2014 "),DB=a("a"),OVe=o("Speech2TextTokenizer"),XVe=o(" (Speech2Text model)"),zVe=l(),Eg=a("li"),QQ=a("strong"),VVe=o("speech_to_text_2"),WVe=o(" \u2014 "),qB=a("a"),QVe=o("Speech2Text2Tokenizer"),HVe=o(" (Speech2Text2 model)"),UVe=l(),Ts=a("li"),HQ=a("strong"),JVe=o("splinter"),YVe=o(" \u2014 "),GB=a("a"),KVe=o("SplinterTokenizer"),ZVe=o(" or "),OB=a("a"),eWe=o("SplinterTokenizerFast"),oWe=o(" (Splinter model)"),rWe=l(),Fs=a("li"),UQ=a("strong"),tWe=o("squeezebert"),aWe=o(" \u2014 "),XB=a("a"),nWe=o("SqueezeBertTokenizer"),sWe=o(" or "),zB=a("a"),lWe=o("SqueezeBertTokenizerFast"),iWe=o(" (SqueezeBERT model)"),dWe=l(),Cs=a("li"),JQ=a("strong"),cWe=o("t5"),fWe=o(" \u2014 "),VB=a("a"),mWe=o("T5Tokenizer"),gWe=o(" or "),WB=a("a"),hWe=o("T5TokenizerFast"),pWe=o(" (T5 model)"),_We=l(),yg=a("li"),YQ=a("strong"),uWe=o("tapas"),bWe=o(" \u2014 "),QB=a("a"),vWe=o("TapasTokenizer"),TWe=o(" (TAPAS model)"),FWe=l(),wg=a("li"),KQ=a("strong"),CWe=o("transfo-xl"),MWe=o(" \u2014 "),HB=a("a"),EWe=o("TransfoXLTokenizer"),yWe=o(" (Transformer-XL model)"),wWe=l(),Ag=a("li"),ZQ=a("strong"),AWe=o("wav2vec2"),LWe=o(" \u2014 "),UB=a("a"),BWe=o("Wav2Vec2CTCTokenizer"),xWe=o(" (Wav2Vec2 model)"),kWe=l(),Lg=a("li"),eH=a("strong"),RWe=o("wav2vec2_phoneme"),SWe=o(" \u2014 "),JB=a("a"),PWe=o("Wav2Vec2PhonemeCTCTokenizer"),$We=o(" (Wav2Vec2Phoneme model)"),IWe=l(),Ms=a("li"),oH=a("strong"),jWe=o("xglm"),NWe=o(" \u2014 "),YB=a("a"),DWe=o("XGLMTokenizer"),qWe=o(" or "),KB=a("a"),GWe=o("XGLMTokenizerFast"),OWe=o(" (XGLM model)"),XWe=l(),Bg=a("li"),rH=a("strong"),zWe=o("xlm"),VWe=o(" \u2014 "),ZB=a("a"),WWe=o("XLMTokenizer"),QWe=o(" (XLM model)"),HWe=l(),xg=a("li"),tH=a("strong"),UWe=o("xlm-prophetnet"),JWe=o(" \u2014 "),ex=a("a"),YWe=o("XLMProphetNetTokenizer"),KWe=o(" (XLMProphetNet model)"),ZWe=l(),Es=a("li"),aH=a("strong"),eQe=o("xlm-roberta"),oQe=o(" \u2014 "),ox=a("a"),rQe=o("XLMRobertaTokenizer"),tQe=o(" or "),rx=a("a"),aQe=o("XLMRobertaTokenizerFast"),nQe=o(" (XLM-RoBERTa model)"),sQe=l(),ys=a("li"),nH=a("strong"),lQe=o("xlnet"),iQe=o(" \u2014 "),tx=a("a"),dQe=o("XLNetTokenizer"),cQe=o(" or "),ax=a("a"),fQe=o("XLNetTokenizerFast"),mQe=o(" (XLNet model)"),gQe=l(),sH=a("p"),hQe=o("Examples:"),pQe=l(),f(WC.$$.fragment),_Qe=l(),kg=a("div"),f(QC.$$.fragment),uQe=l(),lH=a("p"),bQe=o("Register a new tokenizer in this mapping."),AAe=l(),xi=a("h2"),Rg=a("a"),iH=a("span"),f(HC.$$.fragment),vQe=l(),dH=a("span"),TQe=o("AutoFeatureExtractor"),LAe=l(),Oo=a("div"),f(UC.$$.fragment),FQe=l(),JC=a("p"),CQe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),nx=a("a"),MQe=o("AutoFeatureExtractor.from_pretrained()"),EQe=o(" class method."),yQe=l(),YC=a("p"),wQe=o("This class cannot be instantiated directly using "),cH=a("code"),AQe=o("__init__()"),LQe=o(" (throws an error)."),BQe=l(),Le=a("div"),f(KC.$$.fragment),xQe=l(),fH=a("p"),kQe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),RQe=l(),Pa=a("p"),SQe=o("The feature extractor class to instantiate is selected based on the "),mH=a("code"),PQe=o("model_type"),$Qe=o(` property of the config object
(either passed as an argument or loaded from `),gH=a("code"),IQe=o("pretrained_model_name_or_path"),jQe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),hH=a("code"),NQe=o("pretrained_model_name_or_path"),DQe=o(":"),qQe=l(),se=a("ul"),Sg=a("li"),pH=a("strong"),GQe=o("beit"),OQe=o(" \u2014 "),sx=a("a"),XQe=o("BeitFeatureExtractor"),zQe=o(" (BEiT model)"),VQe=l(),Pg=a("li"),_H=a("strong"),WQe=o("clip"),QQe=o(" \u2014 "),lx=a("a"),HQe=o("CLIPFeatureExtractor"),UQe=o(" (CLIP model)"),JQe=l(),$g=a("li"),uH=a("strong"),YQe=o("convnext"),KQe=o(" \u2014 "),ix=a("a"),ZQe=o("ConvNextFeatureExtractor"),eHe=o(" (ConvNext model)"),oHe=l(),Ig=a("li"),bH=a("strong"),rHe=o("deit"),tHe=o(" \u2014 "),dx=a("a"),aHe=o("DeiTFeatureExtractor"),nHe=o(" (DeiT model)"),sHe=l(),jg=a("li"),vH=a("strong"),lHe=o("detr"),iHe=o(" \u2014 "),cx=a("a"),dHe=o("DetrFeatureExtractor"),cHe=o(" (DETR model)"),fHe=l(),Ng=a("li"),TH=a("strong"),mHe=o("hubert"),gHe=o(" \u2014 "),fx=a("a"),hHe=o("Wav2Vec2FeatureExtractor"),pHe=o(" (Hubert model)"),_He=l(),Dg=a("li"),FH=a("strong"),uHe=o("layoutlmv2"),bHe=o(" \u2014 "),mx=a("a"),vHe=o("LayoutLMv2FeatureExtractor"),THe=o(" (LayoutLMv2 model)"),FHe=l(),qg=a("li"),CH=a("strong"),CHe=o("perceiver"),MHe=o(" \u2014 "),gx=a("a"),EHe=o("PerceiverFeatureExtractor"),yHe=o(" (Perceiver model)"),wHe=l(),Gg=a("li"),MH=a("strong"),AHe=o("segformer"),LHe=o(" \u2014 "),hx=a("a"),BHe=o("SegformerFeatureExtractor"),xHe=o(" (SegFormer model)"),kHe=l(),Og=a("li"),EH=a("strong"),RHe=o("speech_to_text"),SHe=o(" \u2014 "),px=a("a"),PHe=o("Speech2TextFeatureExtractor"),$He=o(" (Speech2Text model)"),IHe=l(),Xg=a("li"),yH=a("strong"),jHe=o("swin"),NHe=o(" \u2014 "),_x=a("a"),DHe=o("ViTFeatureExtractor"),qHe=o(" (Swin model)"),GHe=l(),zg=a("li"),wH=a("strong"),OHe=o("vit"),XHe=o(" \u2014 "),ux=a("a"),zHe=o("ViTFeatureExtractor"),VHe=o(" (ViT model)"),WHe=l(),Vg=a("li"),AH=a("strong"),QHe=o("vit_mae"),HHe=o(" \u2014 "),bx=a("a"),UHe=o("ViTFeatureExtractor"),JHe=o(" (ViTMAE model)"),YHe=l(),Wg=a("li"),LH=a("strong"),KHe=o("wav2vec2"),ZHe=o(" \u2014 "),vx=a("a"),eUe=o("Wav2Vec2FeatureExtractor"),oUe=o(" (Wav2Vec2 model)"),rUe=l(),f(Qg.$$.fragment),tUe=l(),BH=a("p"),aUe=o("Examples:"),nUe=l(),f(ZC.$$.fragment),sUe=l(),Hg=a("div"),f(e4.$$.fragment),lUe=l(),xH=a("p"),iUe=o("Register a new feature extractor for this class."),BAe=l(),ki=a("h2"),Ug=a("a"),kH=a("span"),f(o4.$$.fragment),dUe=l(),RH=a("span"),cUe=o("AutoProcessor"),xAe=l(),Xo=a("div"),f(r4.$$.fragment),fUe=l(),t4=a("p"),mUe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),Tx=a("a"),gUe=o("AutoProcessor.from_pretrained()"),hUe=o(" class method."),pUe=l(),a4=a("p"),_Ue=o("This class cannot be instantiated directly using "),SH=a("code"),uUe=o("__init__()"),bUe=o(" (throws an error)."),vUe=l(),Be=a("div"),f(n4.$$.fragment),TUe=l(),PH=a("p"),FUe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),CUe=l(),Ri=a("p"),MUe=o("The processor class to instantiate is selected based on the "),$H=a("code"),EUe=o("model_type"),yUe=o(` property of the config object (either
passed as an argument or loaded from `),IH=a("code"),wUe=o("pretrained_model_name_or_path"),AUe=o(" if possible):"),LUe=l(),ye=a("ul"),Jg=a("li"),jH=a("strong"),BUe=o("clip"),xUe=o(" \u2014 "),Fx=a("a"),kUe=o("CLIPProcessor"),RUe=o(" (CLIP model)"),SUe=l(),Yg=a("li"),NH=a("strong"),PUe=o("layoutlmv2"),$Ue=o(" \u2014 "),Cx=a("a"),IUe=o("LayoutLMv2Processor"),jUe=o(" (LayoutLMv2 model)"),NUe=l(),Kg=a("li"),DH=a("strong"),DUe=o("layoutxlm"),qUe=o(" \u2014 "),Mx=a("a"),GUe=o("LayoutXLMProcessor"),OUe=o(" (LayoutXLM model)"),XUe=l(),Zg=a("li"),qH=a("strong"),zUe=o("speech_to_text"),VUe=o(" \u2014 "),Ex=a("a"),WUe=o("Speech2TextProcessor"),QUe=o(" (Speech2Text model)"),HUe=l(),eh=a("li"),GH=a("strong"),UUe=o("speech_to_text_2"),JUe=o(" \u2014 "),yx=a("a"),YUe=o("Speech2Text2Processor"),KUe=o(" (Speech2Text2 model)"),ZUe=l(),oh=a("li"),OH=a("strong"),eJe=o("trocr"),oJe=o(" \u2014 "),wx=a("a"),rJe=o("TrOCRProcessor"),tJe=o(" (TrOCR model)"),aJe=l(),rh=a("li"),XH=a("strong"),nJe=o("vision-text-dual-encoder"),sJe=o(" \u2014 "),Ax=a("a"),lJe=o("VisionTextDualEncoderProcessor"),iJe=o(" (VisionTextDualEncoder model)"),dJe=l(),th=a("li"),zH=a("strong"),cJe=o("wav2vec2"),fJe=o(" \u2014 "),Lx=a("a"),mJe=o("Wav2Vec2Processor"),gJe=o(" (Wav2Vec2 model)"),hJe=l(),f(ah.$$.fragment),pJe=l(),VH=a("p"),_Je=o("Examples:"),uJe=l(),f(s4.$$.fragment),bJe=l(),nh=a("div"),f(l4.$$.fragment),vJe=l(),WH=a("p"),TJe=o("Register a new processor for this class."),kAe=l(),Si=a("h2"),sh=a("a"),QH=a("span"),f(i4.$$.fragment),FJe=l(),HH=a("span"),CJe=o("AutoModel"),RAe=l(),zo=a("div"),f(d4.$$.fragment),MJe=l(),Pi=a("p"),EJe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UH=a("code"),yJe=o("from_pretrained()"),wJe=o("class method or the "),JH=a("code"),AJe=o("from_config()"),LJe=o(`class
method.`),BJe=l(),c4=a("p"),xJe=o("This class cannot be instantiated directly using "),YH=a("code"),kJe=o("__init__()"),RJe=o(" (throws an error)."),SJe=l(),Ir=a("div"),f(f4.$$.fragment),PJe=l(),KH=a("p"),$Je=o("Instantiates one of the base model classes of the library from a configuration."),IJe=l(),$i=a("p"),jJe=o(`Note:
Loading a model from its configuration file does `),ZH=a("strong"),NJe=o("not"),DJe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),eU=a("code"),qJe=o("from_pretrained()"),GJe=o("to load the model weights."),OJe=l(),oU=a("p"),XJe=o("Examples:"),zJe=l(),f(m4.$$.fragment),VJe=l(),xe=a("div"),f(g4.$$.fragment),WJe=l(),rU=a("p"),QJe=o("Instantiate one of the base model classes of the library from a pretrained model."),HJe=l(),$a=a("p"),UJe=o("The model class to instantiate is selected based on the "),tU=a("code"),JJe=o("model_type"),YJe=o(` property of the config object (either
passed as an argument or loaded from `),aU=a("code"),KJe=o("pretrained_model_name_or_path"),ZJe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nU=a("code"),eYe=o("pretrained_model_name_or_path"),oYe=o(":"),rYe=l(),F=a("ul"),lh=a("li"),sU=a("strong"),tYe=o("albert"),aYe=o(" \u2014 "),Bx=a("a"),nYe=o("AlbertModel"),sYe=o(" (ALBERT model)"),lYe=l(),ih=a("li"),lU=a("strong"),iYe=o("bart"),dYe=o(" \u2014 "),xx=a("a"),cYe=o("BartModel"),fYe=o(" (BART model)"),mYe=l(),dh=a("li"),iU=a("strong"),gYe=o("beit"),hYe=o(" \u2014 "),kx=a("a"),pYe=o("BeitModel"),_Ye=o(" (BEiT model)"),uYe=l(),ch=a("li"),dU=a("strong"),bYe=o("bert"),vYe=o(" \u2014 "),Rx=a("a"),TYe=o("BertModel"),FYe=o(" (BERT model)"),CYe=l(),fh=a("li"),cU=a("strong"),MYe=o("bert-generation"),EYe=o(" \u2014 "),Sx=a("a"),yYe=o("BertGenerationEncoder"),wYe=o(" (Bert Generation model)"),AYe=l(),mh=a("li"),fU=a("strong"),LYe=o("big_bird"),BYe=o(" \u2014 "),Px=a("a"),xYe=o("BigBirdModel"),kYe=o(" (BigBird model)"),RYe=l(),gh=a("li"),mU=a("strong"),SYe=o("bigbird_pegasus"),PYe=o(" \u2014 "),$x=a("a"),$Ye=o("BigBirdPegasusModel"),IYe=o(" (BigBirdPegasus model)"),jYe=l(),hh=a("li"),gU=a("strong"),NYe=o("blenderbot"),DYe=o(" \u2014 "),Ix=a("a"),qYe=o("BlenderbotModel"),GYe=o(" (Blenderbot model)"),OYe=l(),ph=a("li"),hU=a("strong"),XYe=o("blenderbot-small"),zYe=o(" \u2014 "),jx=a("a"),VYe=o("BlenderbotSmallModel"),WYe=o(" (BlenderbotSmall model)"),QYe=l(),_h=a("li"),pU=a("strong"),HYe=o("camembert"),UYe=o(" \u2014 "),Nx=a("a"),JYe=o("CamembertModel"),YYe=o(" (CamemBERT model)"),KYe=l(),uh=a("li"),_U=a("strong"),ZYe=o("canine"),eKe=o(" \u2014 "),Dx=a("a"),oKe=o("CanineModel"),rKe=o(" (Canine model)"),tKe=l(),bh=a("li"),uU=a("strong"),aKe=o("clip"),nKe=o(" \u2014 "),qx=a("a"),sKe=o("CLIPModel"),lKe=o(" (CLIP model)"),iKe=l(),vh=a("li"),bU=a("strong"),dKe=o("convbert"),cKe=o(" \u2014 "),Gx=a("a"),fKe=o("ConvBertModel"),mKe=o(" (ConvBERT model)"),gKe=l(),Th=a("li"),vU=a("strong"),hKe=o("convnext"),pKe=o(" \u2014 "),Ox=a("a"),_Ke=o("ConvNextModel"),uKe=o(" (ConvNext model)"),bKe=l(),Fh=a("li"),TU=a("strong"),vKe=o("ctrl"),TKe=o(" \u2014 "),Xx=a("a"),FKe=o("CTRLModel"),CKe=o(" (CTRL model)"),MKe=l(),Ch=a("li"),FU=a("strong"),EKe=o("deberta"),yKe=o(" \u2014 "),zx=a("a"),wKe=o("DebertaModel"),AKe=o(" (DeBERTa model)"),LKe=l(),Mh=a("li"),CU=a("strong"),BKe=o("deberta-v2"),xKe=o(" \u2014 "),Vx=a("a"),kKe=o("DebertaV2Model"),RKe=o(" (DeBERTa-v2 model)"),SKe=l(),Eh=a("li"),MU=a("strong"),PKe=o("deit"),$Ke=o(" \u2014 "),Wx=a("a"),IKe=o("DeiTModel"),jKe=o(" (DeiT model)"),NKe=l(),yh=a("li"),EU=a("strong"),DKe=o("detr"),qKe=o(" \u2014 "),Qx=a("a"),GKe=o("DetrModel"),OKe=o(" (DETR model)"),XKe=l(),wh=a("li"),yU=a("strong"),zKe=o("distilbert"),VKe=o(" \u2014 "),Hx=a("a"),WKe=o("DistilBertModel"),QKe=o(" (DistilBERT model)"),HKe=l(),Ah=a("li"),wU=a("strong"),UKe=o("dpr"),JKe=o(" \u2014 "),Ux=a("a"),YKe=o("DPRQuestionEncoder"),KKe=o(" (DPR model)"),ZKe=l(),Lh=a("li"),AU=a("strong"),eZe=o("electra"),oZe=o(" \u2014 "),Jx=a("a"),rZe=o("ElectraModel"),tZe=o(" (ELECTRA model)"),aZe=l(),Bh=a("li"),LU=a("strong"),nZe=o("flaubert"),sZe=o(" \u2014 "),Yx=a("a"),lZe=o("FlaubertModel"),iZe=o(" (FlauBERT model)"),dZe=l(),xh=a("li"),BU=a("strong"),cZe=o("fnet"),fZe=o(" \u2014 "),Kx=a("a"),mZe=o("FNetModel"),gZe=o(" (FNet model)"),hZe=l(),kh=a("li"),xU=a("strong"),pZe=o("fsmt"),_Ze=o(" \u2014 "),Zx=a("a"),uZe=o("FSMTModel"),bZe=o(" (FairSeq Machine-Translation model)"),vZe=l(),ws=a("li"),kU=a("strong"),TZe=o("funnel"),FZe=o(" \u2014 "),ek=a("a"),CZe=o("FunnelModel"),MZe=o(" or "),ok=a("a"),EZe=o("FunnelBaseModel"),yZe=o(" (Funnel Transformer model)"),wZe=l(),Rh=a("li"),RU=a("strong"),AZe=o("gpt2"),LZe=o(" \u2014 "),rk=a("a"),BZe=o("GPT2Model"),xZe=o(" (OpenAI GPT-2 model)"),kZe=l(),Sh=a("li"),SU=a("strong"),RZe=o("gpt_neo"),SZe=o(" \u2014 "),tk=a("a"),PZe=o("GPTNeoModel"),$Ze=o(" (GPT Neo model)"),IZe=l(),Ph=a("li"),PU=a("strong"),jZe=o("gptj"),NZe=o(" \u2014 "),ak=a("a"),DZe=o("GPTJModel"),qZe=o(" (GPT-J model)"),GZe=l(),$h=a("li"),$U=a("strong"),OZe=o("hubert"),XZe=o(" \u2014 "),nk=a("a"),zZe=o("HubertModel"),VZe=o(" (Hubert model)"),WZe=l(),Ih=a("li"),IU=a("strong"),QZe=o("ibert"),HZe=o(" \u2014 "),sk=a("a"),UZe=o("IBertModel"),JZe=o(" (I-BERT model)"),YZe=l(),jh=a("li"),jU=a("strong"),KZe=o("imagegpt"),ZZe=o(" \u2014 "),lk=a("a"),eeo=o("ImageGPTModel"),oeo=o(" (ImageGPT model)"),reo=l(),Nh=a("li"),NU=a("strong"),teo=o("layoutlm"),aeo=o(" \u2014 "),ik=a("a"),neo=o("LayoutLMModel"),seo=o(" (LayoutLM model)"),leo=l(),Dh=a("li"),DU=a("strong"),ieo=o("layoutlmv2"),deo=o(" \u2014 "),dk=a("a"),ceo=o("LayoutLMv2Model"),feo=o(" (LayoutLMv2 model)"),meo=l(),qh=a("li"),qU=a("strong"),geo=o("led"),heo=o(" \u2014 "),ck=a("a"),peo=o("LEDModel"),_eo=o(" (LED model)"),ueo=l(),Gh=a("li"),GU=a("strong"),beo=o("longformer"),veo=o(" \u2014 "),fk=a("a"),Teo=o("LongformerModel"),Feo=o(" (Longformer model)"),Ceo=l(),Oh=a("li"),OU=a("strong"),Meo=o("luke"),Eeo=o(" \u2014 "),mk=a("a"),yeo=o("LukeModel"),weo=o(" (LUKE model)"),Aeo=l(),Xh=a("li"),XU=a("strong"),Leo=o("lxmert"),Beo=o(" \u2014 "),gk=a("a"),xeo=o("LxmertModel"),keo=o(" (LXMERT model)"),Reo=l(),zh=a("li"),zU=a("strong"),Seo=o("m2m_100"),Peo=o(" \u2014 "),hk=a("a"),$eo=o("M2M100Model"),Ieo=o(" (M2M100 model)"),jeo=l(),Vh=a("li"),VU=a("strong"),Neo=o("marian"),Deo=o(" \u2014 "),pk=a("a"),qeo=o("MarianModel"),Geo=o(" (Marian model)"),Oeo=l(),Wh=a("li"),WU=a("strong"),Xeo=o("mbart"),zeo=o(" \u2014 "),_k=a("a"),Veo=o("MBartModel"),Weo=o(" (mBART model)"),Qeo=l(),Qh=a("li"),QU=a("strong"),Heo=o("megatron-bert"),Ueo=o(" \u2014 "),uk=a("a"),Jeo=o("MegatronBertModel"),Yeo=o(" (MegatronBert model)"),Keo=l(),Hh=a("li"),HU=a("strong"),Zeo=o("mobilebert"),eoo=o(" \u2014 "),bk=a("a"),ooo=o("MobileBertModel"),roo=o(" (MobileBERT model)"),too=l(),Uh=a("li"),UU=a("strong"),aoo=o("mpnet"),noo=o(" \u2014 "),vk=a("a"),soo=o("MPNetModel"),loo=o(" (MPNet model)"),ioo=l(),Jh=a("li"),JU=a("strong"),doo=o("mt5"),coo=o(" \u2014 "),Tk=a("a"),foo=o("MT5Model"),moo=o(" (mT5 model)"),goo=l(),Yh=a("li"),YU=a("strong"),hoo=o("nystromformer"),poo=o(" \u2014 "),Fk=a("a"),_oo=o("NystromformerModel"),uoo=o(" (Nystromformer model)"),boo=l(),Kh=a("li"),KU=a("strong"),voo=o("openai-gpt"),Too=o(" \u2014 "),Ck=a("a"),Foo=o("OpenAIGPTModel"),Coo=o(" (OpenAI GPT model)"),Moo=l(),Zh=a("li"),ZU=a("strong"),Eoo=o("pegasus"),yoo=o(" \u2014 "),Mk=a("a"),woo=o("PegasusModel"),Aoo=o(" (Pegasus model)"),Loo=l(),ep=a("li"),eJ=a("strong"),Boo=o("perceiver"),xoo=o(" \u2014 "),Ek=a("a"),koo=o("PerceiverModel"),Roo=o(" (Perceiver model)"),Soo=l(),op=a("li"),oJ=a("strong"),Poo=o("prophetnet"),$oo=o(" \u2014 "),yk=a("a"),Ioo=o("ProphetNetModel"),joo=o(" (ProphetNet model)"),Noo=l(),rp=a("li"),rJ=a("strong"),Doo=o("qdqbert"),qoo=o(" \u2014 "),wk=a("a"),Goo=o("QDQBertModel"),Ooo=o(" (QDQBert model)"),Xoo=l(),tp=a("li"),tJ=a("strong"),zoo=o("reformer"),Voo=o(" \u2014 "),Ak=a("a"),Woo=o("ReformerModel"),Qoo=o(" (Reformer model)"),Hoo=l(),ap=a("li"),aJ=a("strong"),Uoo=o("rembert"),Joo=o(" \u2014 "),Lk=a("a"),Yoo=o("RemBertModel"),Koo=o(" (RemBERT model)"),Zoo=l(),np=a("li"),nJ=a("strong"),ero=o("retribert"),oro=o(" \u2014 "),Bk=a("a"),rro=o("RetriBertModel"),tro=o(" (RetriBERT model)"),aro=l(),sp=a("li"),sJ=a("strong"),nro=o("roberta"),sro=o(" \u2014 "),xk=a("a"),lro=o("RobertaModel"),iro=o(" (RoBERTa model)"),dro=l(),lp=a("li"),lJ=a("strong"),cro=o("roformer"),fro=o(" \u2014 "),kk=a("a"),mro=o("RoFormerModel"),gro=o(" (RoFormer model)"),hro=l(),ip=a("li"),iJ=a("strong"),pro=o("segformer"),_ro=o(" \u2014 "),Rk=a("a"),uro=o("SegformerModel"),bro=o(" (SegFormer model)"),vro=l(),dp=a("li"),dJ=a("strong"),Tro=o("sew"),Fro=o(" \u2014 "),Sk=a("a"),Cro=o("SEWModel"),Mro=o(" (SEW model)"),Ero=l(),cp=a("li"),cJ=a("strong"),yro=o("sew-d"),wro=o(" \u2014 "),Pk=a("a"),Aro=o("SEWDModel"),Lro=o(" (SEW-D model)"),Bro=l(),fp=a("li"),fJ=a("strong"),xro=o("speech_to_text"),kro=o(" \u2014 "),$k=a("a"),Rro=o("Speech2TextModel"),Sro=o(" (Speech2Text model)"),Pro=l(),mp=a("li"),mJ=a("strong"),$ro=o("splinter"),Iro=o(" \u2014 "),Ik=a("a"),jro=o("SplinterModel"),Nro=o(" (Splinter model)"),Dro=l(),gp=a("li"),gJ=a("strong"),qro=o("squeezebert"),Gro=o(" \u2014 "),jk=a("a"),Oro=o("SqueezeBertModel"),Xro=o(" (SqueezeBERT model)"),zro=l(),hp=a("li"),hJ=a("strong"),Vro=o("swin"),Wro=o(" \u2014 "),Nk=a("a"),Qro=o("SwinModel"),Hro=o(" (Swin model)"),Uro=l(),pp=a("li"),pJ=a("strong"),Jro=o("t5"),Yro=o(" \u2014 "),Dk=a("a"),Kro=o("T5Model"),Zro=o(" (T5 model)"),eto=l(),_p=a("li"),_J=a("strong"),oto=o("tapas"),rto=o(" \u2014 "),qk=a("a"),tto=o("TapasModel"),ato=o(" (TAPAS model)"),nto=l(),up=a("li"),uJ=a("strong"),sto=o("transfo-xl"),lto=o(" \u2014 "),Gk=a("a"),ito=o("TransfoXLModel"),dto=o(" (Transformer-XL model)"),cto=l(),bp=a("li"),bJ=a("strong"),fto=o("unispeech"),mto=o(" \u2014 "),Ok=a("a"),gto=o("UniSpeechModel"),hto=o(" (UniSpeech model)"),pto=l(),vp=a("li"),vJ=a("strong"),_to=o("unispeech-sat"),uto=o(" \u2014 "),Xk=a("a"),bto=o("UniSpeechSatModel"),vto=o(" (UniSpeechSat model)"),Tto=l(),Tp=a("li"),TJ=a("strong"),Fto=o("vilt"),Cto=o(" \u2014 "),zk=a("a"),Mto=o("ViltModel"),Eto=o(" (ViLT model)"),yto=l(),Fp=a("li"),FJ=a("strong"),wto=o("vision-text-dual-encoder"),Ato=o(" \u2014 "),Vk=a("a"),Lto=o("VisionTextDualEncoderModel"),Bto=o(" (VisionTextDualEncoder model)"),xto=l(),Cp=a("li"),CJ=a("strong"),kto=o("visual_bert"),Rto=o(" \u2014 "),Wk=a("a"),Sto=o("VisualBertModel"),Pto=o(" (VisualBert model)"),$to=l(),Mp=a("li"),MJ=a("strong"),Ito=o("vit"),jto=o(" \u2014 "),Qk=a("a"),Nto=o("ViTModel"),Dto=o(" (ViT model)"),qto=l(),Ep=a("li"),EJ=a("strong"),Gto=o("vit_mae"),Oto=o(" \u2014 "),Hk=a("a"),Xto=o("ViTMAEModel"),zto=o(" (ViTMAE model)"),Vto=l(),yp=a("li"),yJ=a("strong"),Wto=o("wav2vec2"),Qto=o(" \u2014 "),Uk=a("a"),Hto=o("Wav2Vec2Model"),Uto=o(" (Wav2Vec2 model)"),Jto=l(),wp=a("li"),wJ=a("strong"),Yto=o("wavlm"),Kto=o(" \u2014 "),Jk=a("a"),Zto=o("WavLMModel"),eao=o(" (WavLM model)"),oao=l(),Ap=a("li"),AJ=a("strong"),rao=o("xglm"),tao=o(" \u2014 "),Yk=a("a"),aao=o("XGLMModel"),nao=o(" (XGLM model)"),sao=l(),Lp=a("li"),LJ=a("strong"),lao=o("xlm"),iao=o(" \u2014 "),Kk=a("a"),dao=o("XLMModel"),cao=o(" (XLM model)"),fao=l(),Bp=a("li"),BJ=a("strong"),mao=o("xlm-prophetnet"),gao=o(" \u2014 "),Zk=a("a"),hao=o("XLMProphetNetModel"),pao=o(" (XLMProphetNet model)"),_ao=l(),xp=a("li"),xJ=a("strong"),uao=o("xlm-roberta"),bao=o(" \u2014 "),eR=a("a"),vao=o("XLMRobertaModel"),Tao=o(" (XLM-RoBERTa model)"),Fao=l(),kp=a("li"),kJ=a("strong"),Cao=o("xlm-roberta-xl"),Mao=o(" \u2014 "),oR=a("a"),Eao=o("XLMRobertaXLModel"),yao=o(" (XLM-RoBERTa-XL model)"),wao=l(),Rp=a("li"),RJ=a("strong"),Aao=o("xlnet"),Lao=o(" \u2014 "),rR=a("a"),Bao=o("XLNetModel"),xao=o(" (XLNet model)"),kao=l(),Sp=a("li"),SJ=a("strong"),Rao=o("yoso"),Sao=o(" \u2014 "),tR=a("a"),Pao=o("YosoModel"),$ao=o(" (YOSO model)"),Iao=l(),Pp=a("p"),jao=o("The model is set in evaluation mode by default using "),PJ=a("code"),Nao=o("model.eval()"),Dao=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$J=a("code"),qao=o("model.train()"),Gao=l(),IJ=a("p"),Oao=o("Examples:"),Xao=l(),f(h4.$$.fragment),SAe=l(),Ii=a("h2"),$p=a("a"),jJ=a("span"),f(p4.$$.fragment),zao=l(),NJ=a("span"),Vao=o("AutoModelForPreTraining"),PAe=l(),Vo=a("div"),f(_4.$$.fragment),Wao=l(),ji=a("p"),Qao=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),DJ=a("code"),Hao=o("from_pretrained()"),Uao=o("class method or the "),qJ=a("code"),Jao=o("from_config()"),Yao=o(`class
method.`),Kao=l(),u4=a("p"),Zao=o("This class cannot be instantiated directly using "),GJ=a("code"),eno=o("__init__()"),ono=o(" (throws an error)."),rno=l(),jr=a("div"),f(b4.$$.fragment),tno=l(),OJ=a("p"),ano=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),nno=l(),Ni=a("p"),sno=o(`Note:
Loading a model from its configuration file does `),XJ=a("strong"),lno=o("not"),ino=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zJ=a("code"),dno=o("from_pretrained()"),cno=o("to load the model weights."),fno=l(),VJ=a("p"),mno=o("Examples:"),gno=l(),f(v4.$$.fragment),hno=l(),ke=a("div"),f(T4.$$.fragment),pno=l(),WJ=a("p"),_no=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),uno=l(),Ia=a("p"),bno=o("The model class to instantiate is selected based on the "),QJ=a("code"),vno=o("model_type"),Tno=o(` property of the config object (either
passed as an argument or loaded from `),HJ=a("code"),Fno=o("pretrained_model_name_or_path"),Cno=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),UJ=a("code"),Mno=o("pretrained_model_name_or_path"),Eno=o(":"),yno=l(),k=a("ul"),Ip=a("li"),JJ=a("strong"),wno=o("albert"),Ano=o(" \u2014 "),aR=a("a"),Lno=o("AlbertForPreTraining"),Bno=o(" (ALBERT model)"),xno=l(),jp=a("li"),YJ=a("strong"),kno=o("bart"),Rno=o(" \u2014 "),nR=a("a"),Sno=o("BartForConditionalGeneration"),Pno=o(" (BART model)"),$no=l(),Np=a("li"),KJ=a("strong"),Ino=o("bert"),jno=o(" \u2014 "),sR=a("a"),Nno=o("BertForPreTraining"),Dno=o(" (BERT model)"),qno=l(),Dp=a("li"),ZJ=a("strong"),Gno=o("big_bird"),Ono=o(" \u2014 "),lR=a("a"),Xno=o("BigBirdForPreTraining"),zno=o(" (BigBird model)"),Vno=l(),qp=a("li"),eY=a("strong"),Wno=o("camembert"),Qno=o(" \u2014 "),iR=a("a"),Hno=o("CamembertForMaskedLM"),Uno=o(" (CamemBERT model)"),Jno=l(),Gp=a("li"),oY=a("strong"),Yno=o("ctrl"),Kno=o(" \u2014 "),dR=a("a"),Zno=o("CTRLLMHeadModel"),eso=o(" (CTRL model)"),oso=l(),Op=a("li"),rY=a("strong"),rso=o("deberta"),tso=o(" \u2014 "),cR=a("a"),aso=o("DebertaForMaskedLM"),nso=o(" (DeBERTa model)"),sso=l(),Xp=a("li"),tY=a("strong"),lso=o("deberta-v2"),iso=o(" \u2014 "),fR=a("a"),dso=o("DebertaV2ForMaskedLM"),cso=o(" (DeBERTa-v2 model)"),fso=l(),zp=a("li"),aY=a("strong"),mso=o("distilbert"),gso=o(" \u2014 "),mR=a("a"),hso=o("DistilBertForMaskedLM"),pso=o(" (DistilBERT model)"),_so=l(),Vp=a("li"),nY=a("strong"),uso=o("electra"),bso=o(" \u2014 "),gR=a("a"),vso=o("ElectraForPreTraining"),Tso=o(" (ELECTRA model)"),Fso=l(),Wp=a("li"),sY=a("strong"),Cso=o("flaubert"),Mso=o(" \u2014 "),hR=a("a"),Eso=o("FlaubertWithLMHeadModel"),yso=o(" (FlauBERT model)"),wso=l(),Qp=a("li"),lY=a("strong"),Aso=o("fnet"),Lso=o(" \u2014 "),pR=a("a"),Bso=o("FNetForPreTraining"),xso=o(" (FNet model)"),kso=l(),Hp=a("li"),iY=a("strong"),Rso=o("fsmt"),Sso=o(" \u2014 "),_R=a("a"),Pso=o("FSMTForConditionalGeneration"),$so=o(" (FairSeq Machine-Translation model)"),Iso=l(),Up=a("li"),dY=a("strong"),jso=o("funnel"),Nso=o(" \u2014 "),uR=a("a"),Dso=o("FunnelForPreTraining"),qso=o(" (Funnel Transformer model)"),Gso=l(),Jp=a("li"),cY=a("strong"),Oso=o("gpt2"),Xso=o(" \u2014 "),bR=a("a"),zso=o("GPT2LMHeadModel"),Vso=o(" (OpenAI GPT-2 model)"),Wso=l(),Yp=a("li"),fY=a("strong"),Qso=o("ibert"),Hso=o(" \u2014 "),vR=a("a"),Uso=o("IBertForMaskedLM"),Jso=o(" (I-BERT model)"),Yso=l(),Kp=a("li"),mY=a("strong"),Kso=o("layoutlm"),Zso=o(" \u2014 "),TR=a("a"),elo=o("LayoutLMForMaskedLM"),olo=o(" (LayoutLM model)"),rlo=l(),Zp=a("li"),gY=a("strong"),tlo=o("longformer"),alo=o(" \u2014 "),FR=a("a"),nlo=o("LongformerForMaskedLM"),slo=o(" (Longformer model)"),llo=l(),e_=a("li"),hY=a("strong"),ilo=o("lxmert"),dlo=o(" \u2014 "),CR=a("a"),clo=o("LxmertForPreTraining"),flo=o(" (LXMERT model)"),mlo=l(),o_=a("li"),pY=a("strong"),glo=o("megatron-bert"),hlo=o(" \u2014 "),MR=a("a"),plo=o("MegatronBertForPreTraining"),_lo=o(" (MegatronBert model)"),ulo=l(),r_=a("li"),_Y=a("strong"),blo=o("mobilebert"),vlo=o(" \u2014 "),ER=a("a"),Tlo=o("MobileBertForPreTraining"),Flo=o(" (MobileBERT model)"),Clo=l(),t_=a("li"),uY=a("strong"),Mlo=o("mpnet"),Elo=o(" \u2014 "),yR=a("a"),ylo=o("MPNetForMaskedLM"),wlo=o(" (MPNet model)"),Alo=l(),a_=a("li"),bY=a("strong"),Llo=o("openai-gpt"),Blo=o(" \u2014 "),wR=a("a"),xlo=o("OpenAIGPTLMHeadModel"),klo=o(" (OpenAI GPT model)"),Rlo=l(),n_=a("li"),vY=a("strong"),Slo=o("retribert"),Plo=o(" \u2014 "),AR=a("a"),$lo=o("RetriBertModel"),Ilo=o(" (RetriBERT model)"),jlo=l(),s_=a("li"),TY=a("strong"),Nlo=o("roberta"),Dlo=o(" \u2014 "),LR=a("a"),qlo=o("RobertaForMaskedLM"),Glo=o(" (RoBERTa model)"),Olo=l(),l_=a("li"),FY=a("strong"),Xlo=o("squeezebert"),zlo=o(" \u2014 "),BR=a("a"),Vlo=o("SqueezeBertForMaskedLM"),Wlo=o(" (SqueezeBERT model)"),Qlo=l(),i_=a("li"),CY=a("strong"),Hlo=o("t5"),Ulo=o(" \u2014 "),xR=a("a"),Jlo=o("T5ForConditionalGeneration"),Ylo=o(" (T5 model)"),Klo=l(),d_=a("li"),MY=a("strong"),Zlo=o("tapas"),eio=o(" \u2014 "),kR=a("a"),oio=o("TapasForMaskedLM"),rio=o(" (TAPAS model)"),tio=l(),c_=a("li"),EY=a("strong"),aio=o("transfo-xl"),nio=o(" \u2014 "),RR=a("a"),sio=o("TransfoXLLMHeadModel"),lio=o(" (Transformer-XL model)"),iio=l(),f_=a("li"),yY=a("strong"),dio=o("unispeech"),cio=o(" \u2014 "),SR=a("a"),fio=o("UniSpeechForPreTraining"),mio=o(" (UniSpeech model)"),gio=l(),m_=a("li"),wY=a("strong"),hio=o("unispeech-sat"),pio=o(" \u2014 "),PR=a("a"),_io=o("UniSpeechSatForPreTraining"),uio=o(" (UniSpeechSat model)"),bio=l(),g_=a("li"),AY=a("strong"),vio=o("visual_bert"),Tio=o(" \u2014 "),$R=a("a"),Fio=o("VisualBertForPreTraining"),Cio=o(" (VisualBert model)"),Mio=l(),h_=a("li"),LY=a("strong"),Eio=o("vit_mae"),yio=o(" \u2014 "),IR=a("a"),wio=o("ViTMAEForPreTraining"),Aio=o(" (ViTMAE model)"),Lio=l(),p_=a("li"),BY=a("strong"),Bio=o("wav2vec2"),xio=o(" \u2014 "),jR=a("a"),kio=o("Wav2Vec2ForPreTraining"),Rio=o(" (Wav2Vec2 model)"),Sio=l(),__=a("li"),xY=a("strong"),Pio=o("xlm"),$io=o(" \u2014 "),NR=a("a"),Iio=o("XLMWithLMHeadModel"),jio=o(" (XLM model)"),Nio=l(),u_=a("li"),kY=a("strong"),Dio=o("xlm-roberta"),qio=o(" \u2014 "),DR=a("a"),Gio=o("XLMRobertaForMaskedLM"),Oio=o(" (XLM-RoBERTa model)"),Xio=l(),b_=a("li"),RY=a("strong"),zio=o("xlm-roberta-xl"),Vio=o(" \u2014 "),qR=a("a"),Wio=o("XLMRobertaXLForMaskedLM"),Qio=o(" (XLM-RoBERTa-XL model)"),Hio=l(),v_=a("li"),SY=a("strong"),Uio=o("xlnet"),Jio=o(" \u2014 "),GR=a("a"),Yio=o("XLNetLMHeadModel"),Kio=o(" (XLNet model)"),Zio=l(),T_=a("p"),edo=o("The model is set in evaluation mode by default using "),PY=a("code"),odo=o("model.eval()"),rdo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$Y=a("code"),tdo=o("model.train()"),ado=l(),IY=a("p"),ndo=o("Examples:"),sdo=l(),f(F4.$$.fragment),$Ae=l(),Di=a("h2"),F_=a("a"),jY=a("span"),f(C4.$$.fragment),ldo=l(),NY=a("span"),ido=o("AutoModelForCausalLM"),IAe=l(),Wo=a("div"),f(M4.$$.fragment),ddo=l(),qi=a("p"),cdo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),DY=a("code"),fdo=o("from_pretrained()"),mdo=o("class method or the "),qY=a("code"),gdo=o("from_config()"),hdo=o(`class
method.`),pdo=l(),E4=a("p"),_do=o("This class cannot be instantiated directly using "),GY=a("code"),udo=o("__init__()"),bdo=o(" (throws an error)."),vdo=l(),Nr=a("div"),f(y4.$$.fragment),Tdo=l(),OY=a("p"),Fdo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Cdo=l(),Gi=a("p"),Mdo=o(`Note:
Loading a model from its configuration file does `),XY=a("strong"),Edo=o("not"),ydo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zY=a("code"),wdo=o("from_pretrained()"),Ado=o("to load the model weights."),Ldo=l(),VY=a("p"),Bdo=o("Examples:"),xdo=l(),f(w4.$$.fragment),kdo=l(),Re=a("div"),f(A4.$$.fragment),Rdo=l(),WY=a("p"),Sdo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Pdo=l(),ja=a("p"),$do=o("The model class to instantiate is selected based on the "),QY=a("code"),Ido=o("model_type"),jdo=o(` property of the config object (either
passed as an argument or loaded from `),HY=a("code"),Ndo=o("pretrained_model_name_or_path"),Ddo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),UY=a("code"),qdo=o("pretrained_model_name_or_path"),Gdo=o(":"),Odo=l(),I=a("ul"),C_=a("li"),JY=a("strong"),Xdo=o("bart"),zdo=o(" \u2014 "),OR=a("a"),Vdo=o("BartForCausalLM"),Wdo=o(" (BART model)"),Qdo=l(),M_=a("li"),YY=a("strong"),Hdo=o("bert"),Udo=o(" \u2014 "),XR=a("a"),Jdo=o("BertLMHeadModel"),Ydo=o(" (BERT model)"),Kdo=l(),E_=a("li"),KY=a("strong"),Zdo=o("bert-generation"),eco=o(" \u2014 "),zR=a("a"),oco=o("BertGenerationDecoder"),rco=o(" (Bert Generation model)"),tco=l(),y_=a("li"),ZY=a("strong"),aco=o("big_bird"),nco=o(" \u2014 "),VR=a("a"),sco=o("BigBirdForCausalLM"),lco=o(" (BigBird model)"),ico=l(),w_=a("li"),eK=a("strong"),dco=o("bigbird_pegasus"),cco=o(" \u2014 "),WR=a("a"),fco=o("BigBirdPegasusForCausalLM"),mco=o(" (BigBirdPegasus model)"),gco=l(),A_=a("li"),oK=a("strong"),hco=o("blenderbot"),pco=o(" \u2014 "),QR=a("a"),_co=o("BlenderbotForCausalLM"),uco=o(" (Blenderbot model)"),bco=l(),L_=a("li"),rK=a("strong"),vco=o("blenderbot-small"),Tco=o(" \u2014 "),HR=a("a"),Fco=o("BlenderbotSmallForCausalLM"),Cco=o(" (BlenderbotSmall model)"),Mco=l(),B_=a("li"),tK=a("strong"),Eco=o("camembert"),yco=o(" \u2014 "),UR=a("a"),wco=o("CamembertForCausalLM"),Aco=o(" (CamemBERT model)"),Lco=l(),x_=a("li"),aK=a("strong"),Bco=o("ctrl"),xco=o(" \u2014 "),JR=a("a"),kco=o("CTRLLMHeadModel"),Rco=o(" (CTRL model)"),Sco=l(),k_=a("li"),nK=a("strong"),Pco=o("electra"),$co=o(" \u2014 "),YR=a("a"),Ico=o("ElectraForCausalLM"),jco=o(" (ELECTRA model)"),Nco=l(),R_=a("li"),sK=a("strong"),Dco=o("gpt2"),qco=o(" \u2014 "),KR=a("a"),Gco=o("GPT2LMHeadModel"),Oco=o(" (OpenAI GPT-2 model)"),Xco=l(),S_=a("li"),lK=a("strong"),zco=o("gpt_neo"),Vco=o(" \u2014 "),ZR=a("a"),Wco=o("GPTNeoForCausalLM"),Qco=o(" (GPT Neo model)"),Hco=l(),P_=a("li"),iK=a("strong"),Uco=o("gptj"),Jco=o(" \u2014 "),eS=a("a"),Yco=o("GPTJForCausalLM"),Kco=o(" (GPT-J model)"),Zco=l(),$_=a("li"),dK=a("strong"),efo=o("marian"),ofo=o(" \u2014 "),oS=a("a"),rfo=o("MarianForCausalLM"),tfo=o(" (Marian model)"),afo=l(),I_=a("li"),cK=a("strong"),nfo=o("mbart"),sfo=o(" \u2014 "),rS=a("a"),lfo=o("MBartForCausalLM"),ifo=o(" (mBART model)"),dfo=l(),j_=a("li"),fK=a("strong"),cfo=o("megatron-bert"),ffo=o(" \u2014 "),tS=a("a"),mfo=o("MegatronBertForCausalLM"),gfo=o(" (MegatronBert model)"),hfo=l(),N_=a("li"),mK=a("strong"),pfo=o("openai-gpt"),_fo=o(" \u2014 "),aS=a("a"),ufo=o("OpenAIGPTLMHeadModel"),bfo=o(" (OpenAI GPT model)"),vfo=l(),D_=a("li"),gK=a("strong"),Tfo=o("pegasus"),Ffo=o(" \u2014 "),nS=a("a"),Cfo=o("PegasusForCausalLM"),Mfo=o(" (Pegasus model)"),Efo=l(),q_=a("li"),hK=a("strong"),yfo=o("prophetnet"),wfo=o(" \u2014 "),sS=a("a"),Afo=o("ProphetNetForCausalLM"),Lfo=o(" (ProphetNet model)"),Bfo=l(),G_=a("li"),pK=a("strong"),xfo=o("qdqbert"),kfo=o(" \u2014 "),lS=a("a"),Rfo=o("QDQBertLMHeadModel"),Sfo=o(" (QDQBert model)"),Pfo=l(),O_=a("li"),_K=a("strong"),$fo=o("reformer"),Ifo=o(" \u2014 "),iS=a("a"),jfo=o("ReformerModelWithLMHead"),Nfo=o(" (Reformer model)"),Dfo=l(),X_=a("li"),uK=a("strong"),qfo=o("rembert"),Gfo=o(" \u2014 "),dS=a("a"),Ofo=o("RemBertForCausalLM"),Xfo=o(" (RemBERT model)"),zfo=l(),z_=a("li"),bK=a("strong"),Vfo=o("roberta"),Wfo=o(" \u2014 "),cS=a("a"),Qfo=o("RobertaForCausalLM"),Hfo=o(" (RoBERTa model)"),Ufo=l(),V_=a("li"),vK=a("strong"),Jfo=o("roformer"),Yfo=o(" \u2014 "),fS=a("a"),Kfo=o("RoFormerForCausalLM"),Zfo=o(" (RoFormer model)"),emo=l(),W_=a("li"),TK=a("strong"),omo=o("speech_to_text_2"),rmo=o(" \u2014 "),mS=a("a"),tmo=o("Speech2Text2ForCausalLM"),amo=o(" (Speech2Text2 model)"),nmo=l(),Q_=a("li"),FK=a("strong"),smo=o("transfo-xl"),lmo=o(" \u2014 "),gS=a("a"),imo=o("TransfoXLLMHeadModel"),dmo=o(" (Transformer-XL model)"),cmo=l(),H_=a("li"),CK=a("strong"),fmo=o("trocr"),mmo=o(" \u2014 "),hS=a("a"),gmo=o("TrOCRForCausalLM"),hmo=o(" (TrOCR model)"),pmo=l(),U_=a("li"),MK=a("strong"),_mo=o("xglm"),umo=o(" \u2014 "),pS=a("a"),bmo=o("XGLMForCausalLM"),vmo=o(" (XGLM model)"),Tmo=l(),J_=a("li"),EK=a("strong"),Fmo=o("xlm"),Cmo=o(" \u2014 "),_S=a("a"),Mmo=o("XLMWithLMHeadModel"),Emo=o(" (XLM model)"),ymo=l(),Y_=a("li"),yK=a("strong"),wmo=o("xlm-prophetnet"),Amo=o(" \u2014 "),uS=a("a"),Lmo=o("XLMProphetNetForCausalLM"),Bmo=o(" (XLMProphetNet model)"),xmo=l(),K_=a("li"),wK=a("strong"),kmo=o("xlm-roberta"),Rmo=o(" \u2014 "),bS=a("a"),Smo=o("XLMRobertaForCausalLM"),Pmo=o(" (XLM-RoBERTa model)"),$mo=l(),Z_=a("li"),AK=a("strong"),Imo=o("xlm-roberta-xl"),jmo=o(" \u2014 "),vS=a("a"),Nmo=o("XLMRobertaXLForCausalLM"),Dmo=o(" (XLM-RoBERTa-XL model)"),qmo=l(),eu=a("li"),LK=a("strong"),Gmo=o("xlnet"),Omo=o(" \u2014 "),TS=a("a"),Xmo=o("XLNetLMHeadModel"),zmo=o(" (XLNet model)"),Vmo=l(),ou=a("p"),Wmo=o("The model is set in evaluation mode by default using "),BK=a("code"),Qmo=o("model.eval()"),Hmo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xK=a("code"),Umo=o("model.train()"),Jmo=l(),kK=a("p"),Ymo=o("Examples:"),Kmo=l(),f(L4.$$.fragment),jAe=l(),Oi=a("h2"),ru=a("a"),RK=a("span"),f(B4.$$.fragment),Zmo=l(),SK=a("span"),ego=o("AutoModelForMaskedLM"),NAe=l(),Qo=a("div"),f(x4.$$.fragment),ogo=l(),Xi=a("p"),rgo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),PK=a("code"),tgo=o("from_pretrained()"),ago=o("class method or the "),$K=a("code"),ngo=o("from_config()"),sgo=o(`class
method.`),lgo=l(),k4=a("p"),igo=o("This class cannot be instantiated directly using "),IK=a("code"),dgo=o("__init__()"),cgo=o(" (throws an error)."),fgo=l(),Dr=a("div"),f(R4.$$.fragment),mgo=l(),jK=a("p"),ggo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),hgo=l(),zi=a("p"),pgo=o(`Note:
Loading a model from its configuration file does `),NK=a("strong"),_go=o("not"),ugo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),DK=a("code"),bgo=o("from_pretrained()"),vgo=o("to load the model weights."),Tgo=l(),qK=a("p"),Fgo=o("Examples:"),Cgo=l(),f(S4.$$.fragment),Mgo=l(),Se=a("div"),f(P4.$$.fragment),Ego=l(),GK=a("p"),ygo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),wgo=l(),Na=a("p"),Ago=o("The model class to instantiate is selected based on the "),OK=a("code"),Lgo=o("model_type"),Bgo=o(` property of the config object (either
passed as an argument or loaded from `),XK=a("code"),xgo=o("pretrained_model_name_or_path"),kgo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zK=a("code"),Rgo=o("pretrained_model_name_or_path"),Sgo=o(":"),Pgo=l(),$=a("ul"),tu=a("li"),VK=a("strong"),$go=o("albert"),Igo=o(" \u2014 "),FS=a("a"),jgo=o("AlbertForMaskedLM"),Ngo=o(" (ALBERT model)"),Dgo=l(),au=a("li"),WK=a("strong"),qgo=o("bart"),Ggo=o(" \u2014 "),CS=a("a"),Ogo=o("BartForConditionalGeneration"),Xgo=o(" (BART model)"),zgo=l(),nu=a("li"),QK=a("strong"),Vgo=o("bert"),Wgo=o(" \u2014 "),MS=a("a"),Qgo=o("BertForMaskedLM"),Hgo=o(" (BERT model)"),Ugo=l(),su=a("li"),HK=a("strong"),Jgo=o("big_bird"),Ygo=o(" \u2014 "),ES=a("a"),Kgo=o("BigBirdForMaskedLM"),Zgo=o(" (BigBird model)"),eho=l(),lu=a("li"),UK=a("strong"),oho=o("camembert"),rho=o(" \u2014 "),yS=a("a"),tho=o("CamembertForMaskedLM"),aho=o(" (CamemBERT model)"),nho=l(),iu=a("li"),JK=a("strong"),sho=o("convbert"),lho=o(" \u2014 "),wS=a("a"),iho=o("ConvBertForMaskedLM"),dho=o(" (ConvBERT model)"),cho=l(),du=a("li"),YK=a("strong"),fho=o("deberta"),mho=o(" \u2014 "),AS=a("a"),gho=o("DebertaForMaskedLM"),hho=o(" (DeBERTa model)"),pho=l(),cu=a("li"),KK=a("strong"),_ho=o("deberta-v2"),uho=o(" \u2014 "),LS=a("a"),bho=o("DebertaV2ForMaskedLM"),vho=o(" (DeBERTa-v2 model)"),Tho=l(),fu=a("li"),ZK=a("strong"),Fho=o("distilbert"),Cho=o(" \u2014 "),BS=a("a"),Mho=o("DistilBertForMaskedLM"),Eho=o(" (DistilBERT model)"),yho=l(),mu=a("li"),eZ=a("strong"),who=o("electra"),Aho=o(" \u2014 "),xS=a("a"),Lho=o("ElectraForMaskedLM"),Bho=o(" (ELECTRA model)"),xho=l(),gu=a("li"),oZ=a("strong"),kho=o("flaubert"),Rho=o(" \u2014 "),kS=a("a"),Sho=o("FlaubertWithLMHeadModel"),Pho=o(" (FlauBERT model)"),$ho=l(),hu=a("li"),rZ=a("strong"),Iho=o("fnet"),jho=o(" \u2014 "),RS=a("a"),Nho=o("FNetForMaskedLM"),Dho=o(" (FNet model)"),qho=l(),pu=a("li"),tZ=a("strong"),Gho=o("funnel"),Oho=o(" \u2014 "),SS=a("a"),Xho=o("FunnelForMaskedLM"),zho=o(" (Funnel Transformer model)"),Vho=l(),_u=a("li"),aZ=a("strong"),Who=o("ibert"),Qho=o(" \u2014 "),PS=a("a"),Hho=o("IBertForMaskedLM"),Uho=o(" (I-BERT model)"),Jho=l(),uu=a("li"),nZ=a("strong"),Yho=o("layoutlm"),Kho=o(" \u2014 "),$S=a("a"),Zho=o("LayoutLMForMaskedLM"),epo=o(" (LayoutLM model)"),opo=l(),bu=a("li"),sZ=a("strong"),rpo=o("longformer"),tpo=o(" \u2014 "),IS=a("a"),apo=o("LongformerForMaskedLM"),npo=o(" (Longformer model)"),spo=l(),vu=a("li"),lZ=a("strong"),lpo=o("mbart"),ipo=o(" \u2014 "),jS=a("a"),dpo=o("MBartForConditionalGeneration"),cpo=o(" (mBART model)"),fpo=l(),Tu=a("li"),iZ=a("strong"),mpo=o("megatron-bert"),gpo=o(" \u2014 "),NS=a("a"),hpo=o("MegatronBertForMaskedLM"),ppo=o(" (MegatronBert model)"),_po=l(),Fu=a("li"),dZ=a("strong"),upo=o("mobilebert"),bpo=o(" \u2014 "),DS=a("a"),vpo=o("MobileBertForMaskedLM"),Tpo=o(" (MobileBERT model)"),Fpo=l(),Cu=a("li"),cZ=a("strong"),Cpo=o("mpnet"),Mpo=o(" \u2014 "),qS=a("a"),Epo=o("MPNetForMaskedLM"),ypo=o(" (MPNet model)"),wpo=l(),Mu=a("li"),fZ=a("strong"),Apo=o("nystromformer"),Lpo=o(" \u2014 "),GS=a("a"),Bpo=o("NystromformerForMaskedLM"),xpo=o(" (Nystromformer model)"),kpo=l(),Eu=a("li"),mZ=a("strong"),Rpo=o("perceiver"),Spo=o(" \u2014 "),OS=a("a"),Ppo=o("PerceiverForMaskedLM"),$po=o(" (Perceiver model)"),Ipo=l(),yu=a("li"),gZ=a("strong"),jpo=o("qdqbert"),Npo=o(" \u2014 "),XS=a("a"),Dpo=o("QDQBertForMaskedLM"),qpo=o(" (QDQBert model)"),Gpo=l(),wu=a("li"),hZ=a("strong"),Opo=o("reformer"),Xpo=o(" \u2014 "),zS=a("a"),zpo=o("ReformerForMaskedLM"),Vpo=o(" (Reformer model)"),Wpo=l(),Au=a("li"),pZ=a("strong"),Qpo=o("rembert"),Hpo=o(" \u2014 "),VS=a("a"),Upo=o("RemBertForMaskedLM"),Jpo=o(" (RemBERT model)"),Ypo=l(),Lu=a("li"),_Z=a("strong"),Kpo=o("roberta"),Zpo=o(" \u2014 "),WS=a("a"),e_o=o("RobertaForMaskedLM"),o_o=o(" (RoBERTa model)"),r_o=l(),Bu=a("li"),uZ=a("strong"),t_o=o("roformer"),a_o=o(" \u2014 "),QS=a("a"),n_o=o("RoFormerForMaskedLM"),s_o=o(" (RoFormer model)"),l_o=l(),xu=a("li"),bZ=a("strong"),i_o=o("squeezebert"),d_o=o(" \u2014 "),HS=a("a"),c_o=o("SqueezeBertForMaskedLM"),f_o=o(" (SqueezeBERT model)"),m_o=l(),ku=a("li"),vZ=a("strong"),g_o=o("tapas"),h_o=o(" \u2014 "),US=a("a"),p_o=o("TapasForMaskedLM"),__o=o(" (TAPAS model)"),u_o=l(),Ru=a("li"),TZ=a("strong"),b_o=o("wav2vec2"),v_o=o(" \u2014 "),FZ=a("code"),T_o=o("Wav2Vec2ForMaskedLM"),F_o=o("(Wav2Vec2 model)"),C_o=l(),Su=a("li"),CZ=a("strong"),M_o=o("xlm"),E_o=o(" \u2014 "),JS=a("a"),y_o=o("XLMWithLMHeadModel"),w_o=o(" (XLM model)"),A_o=l(),Pu=a("li"),MZ=a("strong"),L_o=o("xlm-roberta"),B_o=o(" \u2014 "),YS=a("a"),x_o=o("XLMRobertaForMaskedLM"),k_o=o(" (XLM-RoBERTa model)"),R_o=l(),$u=a("li"),EZ=a("strong"),S_o=o("xlm-roberta-xl"),P_o=o(" \u2014 "),KS=a("a"),$_o=o("XLMRobertaXLForMaskedLM"),I_o=o(" (XLM-RoBERTa-XL model)"),j_o=l(),Iu=a("li"),yZ=a("strong"),N_o=o("yoso"),D_o=o(" \u2014 "),ZS=a("a"),q_o=o("YosoForMaskedLM"),G_o=o(" (YOSO model)"),O_o=l(),ju=a("p"),X_o=o("The model is set in evaluation mode by default using "),wZ=a("code"),z_o=o("model.eval()"),V_o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),AZ=a("code"),W_o=o("model.train()"),Q_o=l(),LZ=a("p"),H_o=o("Examples:"),U_o=l(),f($4.$$.fragment),DAe=l(),Vi=a("h2"),Nu=a("a"),BZ=a("span"),f(I4.$$.fragment),J_o=l(),xZ=a("span"),Y_o=o("AutoModelForSeq2SeqLM"),qAe=l(),Ho=a("div"),f(j4.$$.fragment),K_o=l(),Wi=a("p"),Z_o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),kZ=a("code"),euo=o("from_pretrained()"),ouo=o("class method or the "),RZ=a("code"),ruo=o("from_config()"),tuo=o(`class
method.`),auo=l(),N4=a("p"),nuo=o("This class cannot be instantiated directly using "),SZ=a("code"),suo=o("__init__()"),luo=o(" (throws an error)."),iuo=l(),qr=a("div"),f(D4.$$.fragment),duo=l(),PZ=a("p"),cuo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),fuo=l(),Qi=a("p"),muo=o(`Note:
Loading a model from its configuration file does `),$Z=a("strong"),guo=o("not"),huo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),IZ=a("code"),puo=o("from_pretrained()"),_uo=o("to load the model weights."),uuo=l(),jZ=a("p"),buo=o("Examples:"),vuo=l(),f(q4.$$.fragment),Tuo=l(),Pe=a("div"),f(G4.$$.fragment),Fuo=l(),NZ=a("p"),Cuo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Muo=l(),Da=a("p"),Euo=o("The model class to instantiate is selected based on the "),DZ=a("code"),yuo=o("model_type"),wuo=o(` property of the config object (either
passed as an argument or loaded from `),qZ=a("code"),Auo=o("pretrained_model_name_or_path"),Luo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),GZ=a("code"),Buo=o("pretrained_model_name_or_path"),xuo=o(":"),kuo=l(),ne=a("ul"),Du=a("li"),OZ=a("strong"),Ruo=o("bart"),Suo=o(" \u2014 "),eP=a("a"),Puo=o("BartForConditionalGeneration"),$uo=o(" (BART model)"),Iuo=l(),qu=a("li"),XZ=a("strong"),juo=o("bigbird_pegasus"),Nuo=o(" \u2014 "),oP=a("a"),Duo=o("BigBirdPegasusForConditionalGeneration"),quo=o(" (BigBirdPegasus model)"),Guo=l(),Gu=a("li"),zZ=a("strong"),Ouo=o("blenderbot"),Xuo=o(" \u2014 "),rP=a("a"),zuo=o("BlenderbotForConditionalGeneration"),Vuo=o(" (Blenderbot model)"),Wuo=l(),Ou=a("li"),VZ=a("strong"),Quo=o("blenderbot-small"),Huo=o(" \u2014 "),tP=a("a"),Uuo=o("BlenderbotSmallForConditionalGeneration"),Juo=o(" (BlenderbotSmall model)"),Yuo=l(),Xu=a("li"),WZ=a("strong"),Kuo=o("encoder-decoder"),Zuo=o(" \u2014 "),aP=a("a"),e1o=o("EncoderDecoderModel"),o1o=o(" (Encoder decoder model)"),r1o=l(),zu=a("li"),QZ=a("strong"),t1o=o("fsmt"),a1o=o(" \u2014 "),nP=a("a"),n1o=o("FSMTForConditionalGeneration"),s1o=o(" (FairSeq Machine-Translation model)"),l1o=l(),Vu=a("li"),HZ=a("strong"),i1o=o("led"),d1o=o(" \u2014 "),sP=a("a"),c1o=o("LEDForConditionalGeneration"),f1o=o(" (LED model)"),m1o=l(),Wu=a("li"),UZ=a("strong"),g1o=o("m2m_100"),h1o=o(" \u2014 "),lP=a("a"),p1o=o("M2M100ForConditionalGeneration"),_1o=o(" (M2M100 model)"),u1o=l(),Qu=a("li"),JZ=a("strong"),b1o=o("marian"),v1o=o(" \u2014 "),iP=a("a"),T1o=o("MarianMTModel"),F1o=o(" (Marian model)"),C1o=l(),Hu=a("li"),YZ=a("strong"),M1o=o("mbart"),E1o=o(" \u2014 "),dP=a("a"),y1o=o("MBartForConditionalGeneration"),w1o=o(" (mBART model)"),A1o=l(),Uu=a("li"),KZ=a("strong"),L1o=o("mt5"),B1o=o(" \u2014 "),cP=a("a"),x1o=o("MT5ForConditionalGeneration"),k1o=o(" (mT5 model)"),R1o=l(),Ju=a("li"),ZZ=a("strong"),S1o=o("pegasus"),P1o=o(" \u2014 "),fP=a("a"),$1o=o("PegasusForConditionalGeneration"),I1o=o(" (Pegasus model)"),j1o=l(),Yu=a("li"),eee=a("strong"),N1o=o("prophetnet"),D1o=o(" \u2014 "),mP=a("a"),q1o=o("ProphetNetForConditionalGeneration"),G1o=o(" (ProphetNet model)"),O1o=l(),Ku=a("li"),oee=a("strong"),X1o=o("t5"),z1o=o(" \u2014 "),gP=a("a"),V1o=o("T5ForConditionalGeneration"),W1o=o(" (T5 model)"),Q1o=l(),Zu=a("li"),ree=a("strong"),H1o=o("xlm-prophetnet"),U1o=o(" \u2014 "),hP=a("a"),J1o=o("XLMProphetNetForConditionalGeneration"),Y1o=o(" (XLMProphetNet model)"),K1o=l(),e1=a("p"),Z1o=o("The model is set in evaluation mode by default using "),tee=a("code"),ebo=o("model.eval()"),obo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),aee=a("code"),rbo=o("model.train()"),tbo=l(),nee=a("p"),abo=o("Examples:"),nbo=l(),f(O4.$$.fragment),GAe=l(),Hi=a("h2"),o1=a("a"),see=a("span"),f(X4.$$.fragment),sbo=l(),lee=a("span"),lbo=o("AutoModelForSequenceClassification"),OAe=l(),Uo=a("div"),f(z4.$$.fragment),ibo=l(),Ui=a("p"),dbo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),iee=a("code"),cbo=o("from_pretrained()"),fbo=o("class method or the "),dee=a("code"),mbo=o("from_config()"),gbo=o(`class
method.`),hbo=l(),V4=a("p"),pbo=o("This class cannot be instantiated directly using "),cee=a("code"),_bo=o("__init__()"),ubo=o(" (throws an error)."),bbo=l(),Gr=a("div"),f(W4.$$.fragment),vbo=l(),fee=a("p"),Tbo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Fbo=l(),Ji=a("p"),Cbo=o(`Note:
Loading a model from its configuration file does `),mee=a("strong"),Mbo=o("not"),Ebo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gee=a("code"),ybo=o("from_pretrained()"),wbo=o("to load the model weights."),Abo=l(),hee=a("p"),Lbo=o("Examples:"),Bbo=l(),f(Q4.$$.fragment),xbo=l(),$e=a("div"),f(H4.$$.fragment),kbo=l(),pee=a("p"),Rbo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Sbo=l(),qa=a("p"),Pbo=o("The model class to instantiate is selected based on the "),_ee=a("code"),$bo=o("model_type"),Ibo=o(` property of the config object (either
passed as an argument or loaded from `),uee=a("code"),jbo=o("pretrained_model_name_or_path"),Nbo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bee=a("code"),Dbo=o("pretrained_model_name_or_path"),qbo=o(":"),Gbo=l(),A=a("ul"),r1=a("li"),vee=a("strong"),Obo=o("albert"),Xbo=o(" \u2014 "),pP=a("a"),zbo=o("AlbertForSequenceClassification"),Vbo=o(" (ALBERT model)"),Wbo=l(),t1=a("li"),Tee=a("strong"),Qbo=o("bart"),Hbo=o(" \u2014 "),_P=a("a"),Ubo=o("BartForSequenceClassification"),Jbo=o(" (BART model)"),Ybo=l(),a1=a("li"),Fee=a("strong"),Kbo=o("bert"),Zbo=o(" \u2014 "),uP=a("a"),e5o=o("BertForSequenceClassification"),o5o=o(" (BERT model)"),r5o=l(),n1=a("li"),Cee=a("strong"),t5o=o("big_bird"),a5o=o(" \u2014 "),bP=a("a"),n5o=o("BigBirdForSequenceClassification"),s5o=o(" (BigBird model)"),l5o=l(),s1=a("li"),Mee=a("strong"),i5o=o("bigbird_pegasus"),d5o=o(" \u2014 "),vP=a("a"),c5o=o("BigBirdPegasusForSequenceClassification"),f5o=o(" (BigBirdPegasus model)"),m5o=l(),l1=a("li"),Eee=a("strong"),g5o=o("camembert"),h5o=o(" \u2014 "),TP=a("a"),p5o=o("CamembertForSequenceClassification"),_5o=o(" (CamemBERT model)"),u5o=l(),i1=a("li"),yee=a("strong"),b5o=o("canine"),v5o=o(" \u2014 "),FP=a("a"),T5o=o("CanineForSequenceClassification"),F5o=o(" (Canine model)"),C5o=l(),d1=a("li"),wee=a("strong"),M5o=o("convbert"),E5o=o(" \u2014 "),CP=a("a"),y5o=o("ConvBertForSequenceClassification"),w5o=o(" (ConvBERT model)"),A5o=l(),c1=a("li"),Aee=a("strong"),L5o=o("ctrl"),B5o=o(" \u2014 "),MP=a("a"),x5o=o("CTRLForSequenceClassification"),k5o=o(" (CTRL model)"),R5o=l(),f1=a("li"),Lee=a("strong"),S5o=o("deberta"),P5o=o(" \u2014 "),EP=a("a"),$5o=o("DebertaForSequenceClassification"),I5o=o(" (DeBERTa model)"),j5o=l(),m1=a("li"),Bee=a("strong"),N5o=o("deberta-v2"),D5o=o(" \u2014 "),yP=a("a"),q5o=o("DebertaV2ForSequenceClassification"),G5o=o(" (DeBERTa-v2 model)"),O5o=l(),g1=a("li"),xee=a("strong"),X5o=o("distilbert"),z5o=o(" \u2014 "),wP=a("a"),V5o=o("DistilBertForSequenceClassification"),W5o=o(" (DistilBERT model)"),Q5o=l(),h1=a("li"),kee=a("strong"),H5o=o("electra"),U5o=o(" \u2014 "),AP=a("a"),J5o=o("ElectraForSequenceClassification"),Y5o=o(" (ELECTRA model)"),K5o=l(),p1=a("li"),Ree=a("strong"),Z5o=o("flaubert"),e2o=o(" \u2014 "),LP=a("a"),o2o=o("FlaubertForSequenceClassification"),r2o=o(" (FlauBERT model)"),t2o=l(),_1=a("li"),See=a("strong"),a2o=o("fnet"),n2o=o(" \u2014 "),BP=a("a"),s2o=o("FNetForSequenceClassification"),l2o=o(" (FNet model)"),i2o=l(),u1=a("li"),Pee=a("strong"),d2o=o("funnel"),c2o=o(" \u2014 "),xP=a("a"),f2o=o("FunnelForSequenceClassification"),m2o=o(" (Funnel Transformer model)"),g2o=l(),b1=a("li"),$ee=a("strong"),h2o=o("gpt2"),p2o=o(" \u2014 "),kP=a("a"),_2o=o("GPT2ForSequenceClassification"),u2o=o(" (OpenAI GPT-2 model)"),b2o=l(),v1=a("li"),Iee=a("strong"),v2o=o("gpt_neo"),T2o=o(" \u2014 "),RP=a("a"),F2o=o("GPTNeoForSequenceClassification"),C2o=o(" (GPT Neo model)"),M2o=l(),T1=a("li"),jee=a("strong"),E2o=o("gptj"),y2o=o(" \u2014 "),SP=a("a"),w2o=o("GPTJForSequenceClassification"),A2o=o(" (GPT-J model)"),L2o=l(),F1=a("li"),Nee=a("strong"),B2o=o("ibert"),x2o=o(" \u2014 "),PP=a("a"),k2o=o("IBertForSequenceClassification"),R2o=o(" (I-BERT model)"),S2o=l(),C1=a("li"),Dee=a("strong"),P2o=o("layoutlm"),$2o=o(" \u2014 "),$P=a("a"),I2o=o("LayoutLMForSequenceClassification"),j2o=o(" (LayoutLM model)"),N2o=l(),M1=a("li"),qee=a("strong"),D2o=o("layoutlmv2"),q2o=o(" \u2014 "),IP=a("a"),G2o=o("LayoutLMv2ForSequenceClassification"),O2o=o(" (LayoutLMv2 model)"),X2o=l(),E1=a("li"),Gee=a("strong"),z2o=o("led"),V2o=o(" \u2014 "),jP=a("a"),W2o=o("LEDForSequenceClassification"),Q2o=o(" (LED model)"),H2o=l(),y1=a("li"),Oee=a("strong"),U2o=o("longformer"),J2o=o(" \u2014 "),NP=a("a"),Y2o=o("LongformerForSequenceClassification"),K2o=o(" (Longformer model)"),Z2o=l(),w1=a("li"),Xee=a("strong"),evo=o("mbart"),ovo=o(" \u2014 "),DP=a("a"),rvo=o("MBartForSequenceClassification"),tvo=o(" (mBART model)"),avo=l(),A1=a("li"),zee=a("strong"),nvo=o("megatron-bert"),svo=o(" \u2014 "),qP=a("a"),lvo=o("MegatronBertForSequenceClassification"),ivo=o(" (MegatronBert model)"),dvo=l(),L1=a("li"),Vee=a("strong"),cvo=o("mobilebert"),fvo=o(" \u2014 "),GP=a("a"),mvo=o("MobileBertForSequenceClassification"),gvo=o(" (MobileBERT model)"),hvo=l(),B1=a("li"),Wee=a("strong"),pvo=o("mpnet"),_vo=o(" \u2014 "),OP=a("a"),uvo=o("MPNetForSequenceClassification"),bvo=o(" (MPNet model)"),vvo=l(),x1=a("li"),Qee=a("strong"),Tvo=o("nystromformer"),Fvo=o(" \u2014 "),XP=a("a"),Cvo=o("NystromformerForSequenceClassification"),Mvo=o(" (Nystromformer model)"),Evo=l(),k1=a("li"),Hee=a("strong"),yvo=o("openai-gpt"),wvo=o(" \u2014 "),zP=a("a"),Avo=o("OpenAIGPTForSequenceClassification"),Lvo=o(" (OpenAI GPT model)"),Bvo=l(),R1=a("li"),Uee=a("strong"),xvo=o("perceiver"),kvo=o(" \u2014 "),VP=a("a"),Rvo=o("PerceiverForSequenceClassification"),Svo=o(" (Perceiver model)"),Pvo=l(),S1=a("li"),Jee=a("strong"),$vo=o("qdqbert"),Ivo=o(" \u2014 "),WP=a("a"),jvo=o("QDQBertForSequenceClassification"),Nvo=o(" (QDQBert model)"),Dvo=l(),P1=a("li"),Yee=a("strong"),qvo=o("reformer"),Gvo=o(" \u2014 "),QP=a("a"),Ovo=o("ReformerForSequenceClassification"),Xvo=o(" (Reformer model)"),zvo=l(),$1=a("li"),Kee=a("strong"),Vvo=o("rembert"),Wvo=o(" \u2014 "),HP=a("a"),Qvo=o("RemBertForSequenceClassification"),Hvo=o(" (RemBERT model)"),Uvo=l(),I1=a("li"),Zee=a("strong"),Jvo=o("roberta"),Yvo=o(" \u2014 "),UP=a("a"),Kvo=o("RobertaForSequenceClassification"),Zvo=o(" (RoBERTa model)"),e6o=l(),j1=a("li"),eoe=a("strong"),o6o=o("roformer"),r6o=o(" \u2014 "),JP=a("a"),t6o=o("RoFormerForSequenceClassification"),a6o=o(" (RoFormer model)"),n6o=l(),N1=a("li"),ooe=a("strong"),s6o=o("squeezebert"),l6o=o(" \u2014 "),YP=a("a"),i6o=o("SqueezeBertForSequenceClassification"),d6o=o(" (SqueezeBERT model)"),c6o=l(),D1=a("li"),roe=a("strong"),f6o=o("tapas"),m6o=o(" \u2014 "),KP=a("a"),g6o=o("TapasForSequenceClassification"),h6o=o(" (TAPAS model)"),p6o=l(),q1=a("li"),toe=a("strong"),_6o=o("transfo-xl"),u6o=o(" \u2014 "),ZP=a("a"),b6o=o("TransfoXLForSequenceClassification"),v6o=o(" (Transformer-XL model)"),T6o=l(),G1=a("li"),aoe=a("strong"),F6o=o("xlm"),C6o=o(" \u2014 "),e$=a("a"),M6o=o("XLMForSequenceClassification"),E6o=o(" (XLM model)"),y6o=l(),O1=a("li"),noe=a("strong"),w6o=o("xlm-roberta"),A6o=o(" \u2014 "),o$=a("a"),L6o=o("XLMRobertaForSequenceClassification"),B6o=o(" (XLM-RoBERTa model)"),x6o=l(),X1=a("li"),soe=a("strong"),k6o=o("xlm-roberta-xl"),R6o=o(" \u2014 "),r$=a("a"),S6o=o("XLMRobertaXLForSequenceClassification"),P6o=o(" (XLM-RoBERTa-XL model)"),$6o=l(),z1=a("li"),loe=a("strong"),I6o=o("xlnet"),j6o=o(" \u2014 "),t$=a("a"),N6o=o("XLNetForSequenceClassification"),D6o=o(" (XLNet model)"),q6o=l(),V1=a("li"),ioe=a("strong"),G6o=o("yoso"),O6o=o(" \u2014 "),a$=a("a"),X6o=o("YosoForSequenceClassification"),z6o=o(" (YOSO model)"),V6o=l(),W1=a("p"),W6o=o("The model is set in evaluation mode by default using "),doe=a("code"),Q6o=o("model.eval()"),H6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),coe=a("code"),U6o=o("model.train()"),J6o=l(),foe=a("p"),Y6o=o("Examples:"),K6o=l(),f(U4.$$.fragment),XAe=l(),Yi=a("h2"),Q1=a("a"),moe=a("span"),f(J4.$$.fragment),Z6o=l(),goe=a("span"),eTo=o("AutoModelForMultipleChoice"),zAe=l(),Jo=a("div"),f(Y4.$$.fragment),oTo=l(),Ki=a("p"),rTo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),hoe=a("code"),tTo=o("from_pretrained()"),aTo=o("class method or the "),poe=a("code"),nTo=o("from_config()"),sTo=o(`class
method.`),lTo=l(),K4=a("p"),iTo=o("This class cannot be instantiated directly using "),_oe=a("code"),dTo=o("__init__()"),cTo=o(" (throws an error)."),fTo=l(),Or=a("div"),f(Z4.$$.fragment),mTo=l(),uoe=a("p"),gTo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),hTo=l(),Zi=a("p"),pTo=o(`Note:
Loading a model from its configuration file does `),boe=a("strong"),_To=o("not"),uTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),voe=a("code"),bTo=o("from_pretrained()"),vTo=o("to load the model weights."),TTo=l(),Toe=a("p"),FTo=o("Examples:"),CTo=l(),f(eM.$$.fragment),MTo=l(),Ie=a("div"),f(oM.$$.fragment),ETo=l(),Foe=a("p"),yTo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),wTo=l(),Ga=a("p"),ATo=o("The model class to instantiate is selected based on the "),Coe=a("code"),LTo=o("model_type"),BTo=o(` property of the config object (either
passed as an argument or loaded from `),Moe=a("code"),xTo=o("pretrained_model_name_or_path"),kTo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Eoe=a("code"),RTo=o("pretrained_model_name_or_path"),STo=o(":"),PTo=l(),G=a("ul"),H1=a("li"),yoe=a("strong"),$To=o("albert"),ITo=o(" \u2014 "),n$=a("a"),jTo=o("AlbertForMultipleChoice"),NTo=o(" (ALBERT model)"),DTo=l(),U1=a("li"),woe=a("strong"),qTo=o("bert"),GTo=o(" \u2014 "),s$=a("a"),OTo=o("BertForMultipleChoice"),XTo=o(" (BERT model)"),zTo=l(),J1=a("li"),Aoe=a("strong"),VTo=o("big_bird"),WTo=o(" \u2014 "),l$=a("a"),QTo=o("BigBirdForMultipleChoice"),HTo=o(" (BigBird model)"),UTo=l(),Y1=a("li"),Loe=a("strong"),JTo=o("camembert"),YTo=o(" \u2014 "),i$=a("a"),KTo=o("CamembertForMultipleChoice"),ZTo=o(" (CamemBERT model)"),e7o=l(),K1=a("li"),Boe=a("strong"),o7o=o("canine"),r7o=o(" \u2014 "),d$=a("a"),t7o=o("CanineForMultipleChoice"),a7o=o(" (Canine model)"),n7o=l(),Z1=a("li"),xoe=a("strong"),s7o=o("convbert"),l7o=o(" \u2014 "),c$=a("a"),i7o=o("ConvBertForMultipleChoice"),d7o=o(" (ConvBERT model)"),c7o=l(),eb=a("li"),koe=a("strong"),f7o=o("distilbert"),m7o=o(" \u2014 "),f$=a("a"),g7o=o("DistilBertForMultipleChoice"),h7o=o(" (DistilBERT model)"),p7o=l(),ob=a("li"),Roe=a("strong"),_7o=o("electra"),u7o=o(" \u2014 "),m$=a("a"),b7o=o("ElectraForMultipleChoice"),v7o=o(" (ELECTRA model)"),T7o=l(),rb=a("li"),Soe=a("strong"),F7o=o("flaubert"),C7o=o(" \u2014 "),g$=a("a"),M7o=o("FlaubertForMultipleChoice"),E7o=o(" (FlauBERT model)"),y7o=l(),tb=a("li"),Poe=a("strong"),w7o=o("fnet"),A7o=o(" \u2014 "),h$=a("a"),L7o=o("FNetForMultipleChoice"),B7o=o(" (FNet model)"),x7o=l(),ab=a("li"),$oe=a("strong"),k7o=o("funnel"),R7o=o(" \u2014 "),p$=a("a"),S7o=o("FunnelForMultipleChoice"),P7o=o(" (Funnel Transformer model)"),$7o=l(),nb=a("li"),Ioe=a("strong"),I7o=o("ibert"),j7o=o(" \u2014 "),_$=a("a"),N7o=o("IBertForMultipleChoice"),D7o=o(" (I-BERT model)"),q7o=l(),sb=a("li"),joe=a("strong"),G7o=o("longformer"),O7o=o(" \u2014 "),u$=a("a"),X7o=o("LongformerForMultipleChoice"),z7o=o(" (Longformer model)"),V7o=l(),lb=a("li"),Noe=a("strong"),W7o=o("megatron-bert"),Q7o=o(" \u2014 "),b$=a("a"),H7o=o("MegatronBertForMultipleChoice"),U7o=o(" (MegatronBert model)"),J7o=l(),ib=a("li"),Doe=a("strong"),Y7o=o("mobilebert"),K7o=o(" \u2014 "),v$=a("a"),Z7o=o("MobileBertForMultipleChoice"),e8o=o(" (MobileBERT model)"),o8o=l(),db=a("li"),qoe=a("strong"),r8o=o("mpnet"),t8o=o(" \u2014 "),T$=a("a"),a8o=o("MPNetForMultipleChoice"),n8o=o(" (MPNet model)"),s8o=l(),cb=a("li"),Goe=a("strong"),l8o=o("nystromformer"),i8o=o(" \u2014 "),F$=a("a"),d8o=o("NystromformerForMultipleChoice"),c8o=o(" (Nystromformer model)"),f8o=l(),fb=a("li"),Ooe=a("strong"),m8o=o("qdqbert"),g8o=o(" \u2014 "),C$=a("a"),h8o=o("QDQBertForMultipleChoice"),p8o=o(" (QDQBert model)"),_8o=l(),mb=a("li"),Xoe=a("strong"),u8o=o("rembert"),b8o=o(" \u2014 "),M$=a("a"),v8o=o("RemBertForMultipleChoice"),T8o=o(" (RemBERT model)"),F8o=l(),gb=a("li"),zoe=a("strong"),C8o=o("roberta"),M8o=o(" \u2014 "),E$=a("a"),E8o=o("RobertaForMultipleChoice"),y8o=o(" (RoBERTa model)"),w8o=l(),hb=a("li"),Voe=a("strong"),A8o=o("roformer"),L8o=o(" \u2014 "),y$=a("a"),B8o=o("RoFormerForMultipleChoice"),x8o=o(" (RoFormer model)"),k8o=l(),pb=a("li"),Woe=a("strong"),R8o=o("squeezebert"),S8o=o(" \u2014 "),w$=a("a"),P8o=o("SqueezeBertForMultipleChoice"),$8o=o(" (SqueezeBERT model)"),I8o=l(),_b=a("li"),Qoe=a("strong"),j8o=o("xlm"),N8o=o(" \u2014 "),A$=a("a"),D8o=o("XLMForMultipleChoice"),q8o=o(" (XLM model)"),G8o=l(),ub=a("li"),Hoe=a("strong"),O8o=o("xlm-roberta"),X8o=o(" \u2014 "),L$=a("a"),z8o=o("XLMRobertaForMultipleChoice"),V8o=o(" (XLM-RoBERTa model)"),W8o=l(),bb=a("li"),Uoe=a("strong"),Q8o=o("xlm-roberta-xl"),H8o=o(" \u2014 "),B$=a("a"),U8o=o("XLMRobertaXLForMultipleChoice"),J8o=o(" (XLM-RoBERTa-XL model)"),Y8o=l(),vb=a("li"),Joe=a("strong"),K8o=o("xlnet"),Z8o=o(" \u2014 "),x$=a("a"),eFo=o("XLNetForMultipleChoice"),oFo=o(" (XLNet model)"),rFo=l(),Tb=a("li"),Yoe=a("strong"),tFo=o("yoso"),aFo=o(" \u2014 "),k$=a("a"),nFo=o("YosoForMultipleChoice"),sFo=o(" (YOSO model)"),lFo=l(),Fb=a("p"),iFo=o("The model is set in evaluation mode by default using "),Koe=a("code"),dFo=o("model.eval()"),cFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zoe=a("code"),fFo=o("model.train()"),mFo=l(),ere=a("p"),gFo=o("Examples:"),hFo=l(),f(rM.$$.fragment),VAe=l(),ed=a("h2"),Cb=a("a"),ore=a("span"),f(tM.$$.fragment),pFo=l(),rre=a("span"),_Fo=o("AutoModelForNextSentencePrediction"),WAe=l(),Yo=a("div"),f(aM.$$.fragment),uFo=l(),od=a("p"),bFo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),tre=a("code"),vFo=o("from_pretrained()"),TFo=o("class method or the "),are=a("code"),FFo=o("from_config()"),CFo=o(`class
method.`),MFo=l(),nM=a("p"),EFo=o("This class cannot be instantiated directly using "),nre=a("code"),yFo=o("__init__()"),wFo=o(" (throws an error)."),AFo=l(),Xr=a("div"),f(sM.$$.fragment),LFo=l(),sre=a("p"),BFo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),xFo=l(),rd=a("p"),kFo=o(`Note:
Loading a model from its configuration file does `),lre=a("strong"),RFo=o("not"),SFo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ire=a("code"),PFo=o("from_pretrained()"),$Fo=o("to load the model weights."),IFo=l(),dre=a("p"),jFo=o("Examples:"),NFo=l(),f(lM.$$.fragment),DFo=l(),je=a("div"),f(iM.$$.fragment),qFo=l(),cre=a("p"),GFo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),OFo=l(),Oa=a("p"),XFo=o("The model class to instantiate is selected based on the "),fre=a("code"),zFo=o("model_type"),VFo=o(` property of the config object (either
passed as an argument or loaded from `),mre=a("code"),WFo=o("pretrained_model_name_or_path"),QFo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gre=a("code"),HFo=o("pretrained_model_name_or_path"),UFo=o(":"),JFo=l(),oa=a("ul"),Mb=a("li"),hre=a("strong"),YFo=o("bert"),KFo=o(" \u2014 "),R$=a("a"),ZFo=o("BertForNextSentencePrediction"),eCo=o(" (BERT model)"),oCo=l(),Eb=a("li"),pre=a("strong"),rCo=o("fnet"),tCo=o(" \u2014 "),S$=a("a"),aCo=o("FNetForNextSentencePrediction"),nCo=o(" (FNet model)"),sCo=l(),yb=a("li"),_re=a("strong"),lCo=o("megatron-bert"),iCo=o(" \u2014 "),P$=a("a"),dCo=o("MegatronBertForNextSentencePrediction"),cCo=o(" (MegatronBert model)"),fCo=l(),wb=a("li"),ure=a("strong"),mCo=o("mobilebert"),gCo=o(" \u2014 "),$$=a("a"),hCo=o("MobileBertForNextSentencePrediction"),pCo=o(" (MobileBERT model)"),_Co=l(),Ab=a("li"),bre=a("strong"),uCo=o("qdqbert"),bCo=o(" \u2014 "),I$=a("a"),vCo=o("QDQBertForNextSentencePrediction"),TCo=o(" (QDQBert model)"),FCo=l(),Lb=a("p"),CCo=o("The model is set in evaluation mode by default using "),vre=a("code"),MCo=o("model.eval()"),ECo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tre=a("code"),yCo=o("model.train()"),wCo=l(),Fre=a("p"),ACo=o("Examples:"),LCo=l(),f(dM.$$.fragment),QAe=l(),td=a("h2"),Bb=a("a"),Cre=a("span"),f(cM.$$.fragment),BCo=l(),Mre=a("span"),xCo=o("AutoModelForTokenClassification"),HAe=l(),Ko=a("div"),f(fM.$$.fragment),kCo=l(),ad=a("p"),RCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Ere=a("code"),SCo=o("from_pretrained()"),PCo=o("class method or the "),yre=a("code"),$Co=o("from_config()"),ICo=o(`class
method.`),jCo=l(),mM=a("p"),NCo=o("This class cannot be instantiated directly using "),wre=a("code"),DCo=o("__init__()"),qCo=o(" (throws an error)."),GCo=l(),zr=a("div"),f(gM.$$.fragment),OCo=l(),Are=a("p"),XCo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),zCo=l(),nd=a("p"),VCo=o(`Note:
Loading a model from its configuration file does `),Lre=a("strong"),WCo=o("not"),QCo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bre=a("code"),HCo=o("from_pretrained()"),UCo=o("to load the model weights."),JCo=l(),xre=a("p"),YCo=o("Examples:"),KCo=l(),f(hM.$$.fragment),ZCo=l(),Ne=a("div"),f(pM.$$.fragment),e4o=l(),kre=a("p"),o4o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),r4o=l(),Xa=a("p"),t4o=o("The model class to instantiate is selected based on the "),Rre=a("code"),a4o=o("model_type"),n4o=o(` property of the config object (either
passed as an argument or loaded from `),Sre=a("code"),s4o=o("pretrained_model_name_or_path"),l4o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pre=a("code"),i4o=o("pretrained_model_name_or_path"),d4o=o(":"),c4o=l(),N=a("ul"),xb=a("li"),$re=a("strong"),f4o=o("albert"),m4o=o(" \u2014 "),j$=a("a"),g4o=o("AlbertForTokenClassification"),h4o=o(" (ALBERT model)"),p4o=l(),kb=a("li"),Ire=a("strong"),_4o=o("bert"),u4o=o(" \u2014 "),N$=a("a"),b4o=o("BertForTokenClassification"),v4o=o(" (BERT model)"),T4o=l(),Rb=a("li"),jre=a("strong"),F4o=o("big_bird"),C4o=o(" \u2014 "),D$=a("a"),M4o=o("BigBirdForTokenClassification"),E4o=o(" (BigBird model)"),y4o=l(),Sb=a("li"),Nre=a("strong"),w4o=o("camembert"),A4o=o(" \u2014 "),q$=a("a"),L4o=o("CamembertForTokenClassification"),B4o=o(" (CamemBERT model)"),x4o=l(),Pb=a("li"),Dre=a("strong"),k4o=o("canine"),R4o=o(" \u2014 "),G$=a("a"),S4o=o("CanineForTokenClassification"),P4o=o(" (Canine model)"),$4o=l(),$b=a("li"),qre=a("strong"),I4o=o("convbert"),j4o=o(" \u2014 "),O$=a("a"),N4o=o("ConvBertForTokenClassification"),D4o=o(" (ConvBERT model)"),q4o=l(),Ib=a("li"),Gre=a("strong"),G4o=o("deberta"),O4o=o(" \u2014 "),X$=a("a"),X4o=o("DebertaForTokenClassification"),z4o=o(" (DeBERTa model)"),V4o=l(),jb=a("li"),Ore=a("strong"),W4o=o("deberta-v2"),Q4o=o(" \u2014 "),z$=a("a"),H4o=o("DebertaV2ForTokenClassification"),U4o=o(" (DeBERTa-v2 model)"),J4o=l(),Nb=a("li"),Xre=a("strong"),Y4o=o("distilbert"),K4o=o(" \u2014 "),V$=a("a"),Z4o=o("DistilBertForTokenClassification"),eMo=o(" (DistilBERT model)"),oMo=l(),Db=a("li"),zre=a("strong"),rMo=o("electra"),tMo=o(" \u2014 "),W$=a("a"),aMo=o("ElectraForTokenClassification"),nMo=o(" (ELECTRA model)"),sMo=l(),qb=a("li"),Vre=a("strong"),lMo=o("flaubert"),iMo=o(" \u2014 "),Q$=a("a"),dMo=o("FlaubertForTokenClassification"),cMo=o(" (FlauBERT model)"),fMo=l(),Gb=a("li"),Wre=a("strong"),mMo=o("fnet"),gMo=o(" \u2014 "),H$=a("a"),hMo=o("FNetForTokenClassification"),pMo=o(" (FNet model)"),_Mo=l(),Ob=a("li"),Qre=a("strong"),uMo=o("funnel"),bMo=o(" \u2014 "),U$=a("a"),vMo=o("FunnelForTokenClassification"),TMo=o(" (Funnel Transformer model)"),FMo=l(),Xb=a("li"),Hre=a("strong"),CMo=o("gpt2"),MMo=o(" \u2014 "),J$=a("a"),EMo=o("GPT2ForTokenClassification"),yMo=o(" (OpenAI GPT-2 model)"),wMo=l(),zb=a("li"),Ure=a("strong"),AMo=o("ibert"),LMo=o(" \u2014 "),Y$=a("a"),BMo=o("IBertForTokenClassification"),xMo=o(" (I-BERT model)"),kMo=l(),Vb=a("li"),Jre=a("strong"),RMo=o("layoutlm"),SMo=o(" \u2014 "),K$=a("a"),PMo=o("LayoutLMForTokenClassification"),$Mo=o(" (LayoutLM model)"),IMo=l(),Wb=a("li"),Yre=a("strong"),jMo=o("layoutlmv2"),NMo=o(" \u2014 "),Z$=a("a"),DMo=o("LayoutLMv2ForTokenClassification"),qMo=o(" (LayoutLMv2 model)"),GMo=l(),Qb=a("li"),Kre=a("strong"),OMo=o("longformer"),XMo=o(" \u2014 "),eI=a("a"),zMo=o("LongformerForTokenClassification"),VMo=o(" (Longformer model)"),WMo=l(),Hb=a("li"),Zre=a("strong"),QMo=o("megatron-bert"),HMo=o(" \u2014 "),oI=a("a"),UMo=o("MegatronBertForTokenClassification"),JMo=o(" (MegatronBert model)"),YMo=l(),Ub=a("li"),ete=a("strong"),KMo=o("mobilebert"),ZMo=o(" \u2014 "),rI=a("a"),eEo=o("MobileBertForTokenClassification"),oEo=o(" (MobileBERT model)"),rEo=l(),Jb=a("li"),ote=a("strong"),tEo=o("mpnet"),aEo=o(" \u2014 "),tI=a("a"),nEo=o("MPNetForTokenClassification"),sEo=o(" (MPNet model)"),lEo=l(),Yb=a("li"),rte=a("strong"),iEo=o("nystromformer"),dEo=o(" \u2014 "),aI=a("a"),cEo=o("NystromformerForTokenClassification"),fEo=o(" (Nystromformer model)"),mEo=l(),Kb=a("li"),tte=a("strong"),gEo=o("qdqbert"),hEo=o(" \u2014 "),nI=a("a"),pEo=o("QDQBertForTokenClassification"),_Eo=o(" (QDQBert model)"),uEo=l(),Zb=a("li"),ate=a("strong"),bEo=o("rembert"),vEo=o(" \u2014 "),sI=a("a"),TEo=o("RemBertForTokenClassification"),FEo=o(" (RemBERT model)"),CEo=l(),e5=a("li"),nte=a("strong"),MEo=o("roberta"),EEo=o(" \u2014 "),lI=a("a"),yEo=o("RobertaForTokenClassification"),wEo=o(" (RoBERTa model)"),AEo=l(),o5=a("li"),ste=a("strong"),LEo=o("roformer"),BEo=o(" \u2014 "),iI=a("a"),xEo=o("RoFormerForTokenClassification"),kEo=o(" (RoFormer model)"),REo=l(),r5=a("li"),lte=a("strong"),SEo=o("squeezebert"),PEo=o(" \u2014 "),dI=a("a"),$Eo=o("SqueezeBertForTokenClassification"),IEo=o(" (SqueezeBERT model)"),jEo=l(),t5=a("li"),ite=a("strong"),NEo=o("xlm"),DEo=o(" \u2014 "),cI=a("a"),qEo=o("XLMForTokenClassification"),GEo=o(" (XLM model)"),OEo=l(),a5=a("li"),dte=a("strong"),XEo=o("xlm-roberta"),zEo=o(" \u2014 "),fI=a("a"),VEo=o("XLMRobertaForTokenClassification"),WEo=o(" (XLM-RoBERTa model)"),QEo=l(),n5=a("li"),cte=a("strong"),HEo=o("xlm-roberta-xl"),UEo=o(" \u2014 "),mI=a("a"),JEo=o("XLMRobertaXLForTokenClassification"),YEo=o(" (XLM-RoBERTa-XL model)"),KEo=l(),s5=a("li"),fte=a("strong"),ZEo=o("xlnet"),e3o=o(" \u2014 "),gI=a("a"),o3o=o("XLNetForTokenClassification"),r3o=o(" (XLNet model)"),t3o=l(),l5=a("li"),mte=a("strong"),a3o=o("yoso"),n3o=o(" \u2014 "),hI=a("a"),s3o=o("YosoForTokenClassification"),l3o=o(" (YOSO model)"),i3o=l(),i5=a("p"),d3o=o("The model is set in evaluation mode by default using "),gte=a("code"),c3o=o("model.eval()"),f3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hte=a("code"),m3o=o("model.train()"),g3o=l(),pte=a("p"),h3o=o("Examples:"),p3o=l(),f(_M.$$.fragment),UAe=l(),sd=a("h2"),d5=a("a"),_te=a("span"),f(uM.$$.fragment),_3o=l(),ute=a("span"),u3o=o("AutoModelForQuestionAnswering"),JAe=l(),Zo=a("div"),f(bM.$$.fragment),b3o=l(),ld=a("p"),v3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),bte=a("code"),T3o=o("from_pretrained()"),F3o=o("class method or the "),vte=a("code"),C3o=o("from_config()"),M3o=o(`class
method.`),E3o=l(),vM=a("p"),y3o=o("This class cannot be instantiated directly using "),Tte=a("code"),w3o=o("__init__()"),A3o=o(" (throws an error)."),L3o=l(),Vr=a("div"),f(TM.$$.fragment),B3o=l(),Fte=a("p"),x3o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),k3o=l(),id=a("p"),R3o=o(`Note:
Loading a model from its configuration file does `),Cte=a("strong"),S3o=o("not"),P3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mte=a("code"),$3o=o("from_pretrained()"),I3o=o("to load the model weights."),j3o=l(),Ete=a("p"),N3o=o("Examples:"),D3o=l(),f(FM.$$.fragment),q3o=l(),De=a("div"),f(CM.$$.fragment),G3o=l(),yte=a("p"),O3o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),X3o=l(),za=a("p"),z3o=o("The model class to instantiate is selected based on the "),wte=a("code"),V3o=o("model_type"),W3o=o(` property of the config object (either
passed as an argument or loaded from `),Ate=a("code"),Q3o=o("pretrained_model_name_or_path"),H3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lte=a("code"),U3o=o("pretrained_model_name_or_path"),J3o=o(":"),Y3o=l(),R=a("ul"),c5=a("li"),Bte=a("strong"),K3o=o("albert"),Z3o=o(" \u2014 "),pI=a("a"),eyo=o("AlbertForQuestionAnswering"),oyo=o(" (ALBERT model)"),ryo=l(),f5=a("li"),xte=a("strong"),tyo=o("bart"),ayo=o(" \u2014 "),_I=a("a"),nyo=o("BartForQuestionAnswering"),syo=o(" (BART model)"),lyo=l(),m5=a("li"),kte=a("strong"),iyo=o("bert"),dyo=o(" \u2014 "),uI=a("a"),cyo=o("BertForQuestionAnswering"),fyo=o(" (BERT model)"),myo=l(),g5=a("li"),Rte=a("strong"),gyo=o("big_bird"),hyo=o(" \u2014 "),bI=a("a"),pyo=o("BigBirdForQuestionAnswering"),_yo=o(" (BigBird model)"),uyo=l(),h5=a("li"),Ste=a("strong"),byo=o("bigbird_pegasus"),vyo=o(" \u2014 "),vI=a("a"),Tyo=o("BigBirdPegasusForQuestionAnswering"),Fyo=o(" (BigBirdPegasus model)"),Cyo=l(),p5=a("li"),Pte=a("strong"),Myo=o("camembert"),Eyo=o(" \u2014 "),TI=a("a"),yyo=o("CamembertForQuestionAnswering"),wyo=o(" (CamemBERT model)"),Ayo=l(),_5=a("li"),$te=a("strong"),Lyo=o("canine"),Byo=o(" \u2014 "),FI=a("a"),xyo=o("CanineForQuestionAnswering"),kyo=o(" (Canine model)"),Ryo=l(),u5=a("li"),Ite=a("strong"),Syo=o("convbert"),Pyo=o(" \u2014 "),CI=a("a"),$yo=o("ConvBertForQuestionAnswering"),Iyo=o(" (ConvBERT model)"),jyo=l(),b5=a("li"),jte=a("strong"),Nyo=o("deberta"),Dyo=o(" \u2014 "),MI=a("a"),qyo=o("DebertaForQuestionAnswering"),Gyo=o(" (DeBERTa model)"),Oyo=l(),v5=a("li"),Nte=a("strong"),Xyo=o("deberta-v2"),zyo=o(" \u2014 "),EI=a("a"),Vyo=o("DebertaV2ForQuestionAnswering"),Wyo=o(" (DeBERTa-v2 model)"),Qyo=l(),T5=a("li"),Dte=a("strong"),Hyo=o("distilbert"),Uyo=o(" \u2014 "),yI=a("a"),Jyo=o("DistilBertForQuestionAnswering"),Yyo=o(" (DistilBERT model)"),Kyo=l(),F5=a("li"),qte=a("strong"),Zyo=o("electra"),ewo=o(" \u2014 "),wI=a("a"),owo=o("ElectraForQuestionAnswering"),rwo=o(" (ELECTRA model)"),two=l(),C5=a("li"),Gte=a("strong"),awo=o("flaubert"),nwo=o(" \u2014 "),AI=a("a"),swo=o("FlaubertForQuestionAnsweringSimple"),lwo=o(" (FlauBERT model)"),iwo=l(),M5=a("li"),Ote=a("strong"),dwo=o("fnet"),cwo=o(" \u2014 "),LI=a("a"),fwo=o("FNetForQuestionAnswering"),mwo=o(" (FNet model)"),gwo=l(),E5=a("li"),Xte=a("strong"),hwo=o("funnel"),pwo=o(" \u2014 "),BI=a("a"),_wo=o("FunnelForQuestionAnswering"),uwo=o(" (Funnel Transformer model)"),bwo=l(),y5=a("li"),zte=a("strong"),vwo=o("gptj"),Two=o(" \u2014 "),xI=a("a"),Fwo=o("GPTJForQuestionAnswering"),Cwo=o(" (GPT-J model)"),Mwo=l(),w5=a("li"),Vte=a("strong"),Ewo=o("ibert"),ywo=o(" \u2014 "),kI=a("a"),wwo=o("IBertForQuestionAnswering"),Awo=o(" (I-BERT model)"),Lwo=l(),A5=a("li"),Wte=a("strong"),Bwo=o("layoutlmv2"),xwo=o(" \u2014 "),RI=a("a"),kwo=o("LayoutLMv2ForQuestionAnswering"),Rwo=o(" (LayoutLMv2 model)"),Swo=l(),L5=a("li"),Qte=a("strong"),Pwo=o("led"),$wo=o(" \u2014 "),SI=a("a"),Iwo=o("LEDForQuestionAnswering"),jwo=o(" (LED model)"),Nwo=l(),B5=a("li"),Hte=a("strong"),Dwo=o("longformer"),qwo=o(" \u2014 "),PI=a("a"),Gwo=o("LongformerForQuestionAnswering"),Owo=o(" (Longformer model)"),Xwo=l(),x5=a("li"),Ute=a("strong"),zwo=o("lxmert"),Vwo=o(" \u2014 "),$I=a("a"),Wwo=o("LxmertForQuestionAnswering"),Qwo=o(" (LXMERT model)"),Hwo=l(),k5=a("li"),Jte=a("strong"),Uwo=o("mbart"),Jwo=o(" \u2014 "),II=a("a"),Ywo=o("MBartForQuestionAnswering"),Kwo=o(" (mBART model)"),Zwo=l(),R5=a("li"),Yte=a("strong"),eAo=o("megatron-bert"),oAo=o(" \u2014 "),jI=a("a"),rAo=o("MegatronBertForQuestionAnswering"),tAo=o(" (MegatronBert model)"),aAo=l(),S5=a("li"),Kte=a("strong"),nAo=o("mobilebert"),sAo=o(" \u2014 "),NI=a("a"),lAo=o("MobileBertForQuestionAnswering"),iAo=o(" (MobileBERT model)"),dAo=l(),P5=a("li"),Zte=a("strong"),cAo=o("mpnet"),fAo=o(" \u2014 "),DI=a("a"),mAo=o("MPNetForQuestionAnswering"),gAo=o(" (MPNet model)"),hAo=l(),$5=a("li"),eae=a("strong"),pAo=o("nystromformer"),_Ao=o(" \u2014 "),qI=a("a"),uAo=o("NystromformerForQuestionAnswering"),bAo=o(" (Nystromformer model)"),vAo=l(),I5=a("li"),oae=a("strong"),TAo=o("qdqbert"),FAo=o(" \u2014 "),GI=a("a"),CAo=o("QDQBertForQuestionAnswering"),MAo=o(" (QDQBert model)"),EAo=l(),j5=a("li"),rae=a("strong"),yAo=o("reformer"),wAo=o(" \u2014 "),OI=a("a"),AAo=o("ReformerForQuestionAnswering"),LAo=o(" (Reformer model)"),BAo=l(),N5=a("li"),tae=a("strong"),xAo=o("rembert"),kAo=o(" \u2014 "),XI=a("a"),RAo=o("RemBertForQuestionAnswering"),SAo=o(" (RemBERT model)"),PAo=l(),D5=a("li"),aae=a("strong"),$Ao=o("roberta"),IAo=o(" \u2014 "),zI=a("a"),jAo=o("RobertaForQuestionAnswering"),NAo=o(" (RoBERTa model)"),DAo=l(),q5=a("li"),nae=a("strong"),qAo=o("roformer"),GAo=o(" \u2014 "),VI=a("a"),OAo=o("RoFormerForQuestionAnswering"),XAo=o(" (RoFormer model)"),zAo=l(),G5=a("li"),sae=a("strong"),VAo=o("splinter"),WAo=o(" \u2014 "),WI=a("a"),QAo=o("SplinterForQuestionAnswering"),HAo=o(" (Splinter model)"),UAo=l(),O5=a("li"),lae=a("strong"),JAo=o("squeezebert"),YAo=o(" \u2014 "),QI=a("a"),KAo=o("SqueezeBertForQuestionAnswering"),ZAo=o(" (SqueezeBERT model)"),e0o=l(),X5=a("li"),iae=a("strong"),o0o=o("xlm"),r0o=o(" \u2014 "),HI=a("a"),t0o=o("XLMForQuestionAnsweringSimple"),a0o=o(" (XLM model)"),n0o=l(),z5=a("li"),dae=a("strong"),s0o=o("xlm-roberta"),l0o=o(" \u2014 "),UI=a("a"),i0o=o("XLMRobertaForQuestionAnswering"),d0o=o(" (XLM-RoBERTa model)"),c0o=l(),V5=a("li"),cae=a("strong"),f0o=o("xlm-roberta-xl"),m0o=o(" \u2014 "),JI=a("a"),g0o=o("XLMRobertaXLForQuestionAnswering"),h0o=o(" (XLM-RoBERTa-XL model)"),p0o=l(),W5=a("li"),fae=a("strong"),_0o=o("xlnet"),u0o=o(" \u2014 "),YI=a("a"),b0o=o("XLNetForQuestionAnsweringSimple"),v0o=o(" (XLNet model)"),T0o=l(),Q5=a("li"),mae=a("strong"),F0o=o("yoso"),C0o=o(" \u2014 "),KI=a("a"),M0o=o("YosoForQuestionAnswering"),E0o=o(" (YOSO model)"),y0o=l(),H5=a("p"),w0o=o("The model is set in evaluation mode by default using "),gae=a("code"),A0o=o("model.eval()"),L0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hae=a("code"),B0o=o("model.train()"),x0o=l(),pae=a("p"),k0o=o("Examples:"),R0o=l(),f(MM.$$.fragment),YAe=l(),dd=a("h2"),U5=a("a"),_ae=a("span"),f(EM.$$.fragment),S0o=l(),uae=a("span"),P0o=o("AutoModelForTableQuestionAnswering"),KAe=l(),er=a("div"),f(yM.$$.fragment),$0o=l(),cd=a("p"),I0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),bae=a("code"),j0o=o("from_pretrained()"),N0o=o("class method or the "),vae=a("code"),D0o=o("from_config()"),q0o=o(`class
method.`),G0o=l(),wM=a("p"),O0o=o("This class cannot be instantiated directly using "),Tae=a("code"),X0o=o("__init__()"),z0o=o(" (throws an error)."),V0o=l(),Wr=a("div"),f(AM.$$.fragment),W0o=l(),Fae=a("p"),Q0o=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),H0o=l(),fd=a("p"),U0o=o(`Note:
Loading a model from its configuration file does `),Cae=a("strong"),J0o=o("not"),Y0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mae=a("code"),K0o=o("from_pretrained()"),Z0o=o("to load the model weights."),eLo=l(),Eae=a("p"),oLo=o("Examples:"),rLo=l(),f(LM.$$.fragment),tLo=l(),qe=a("div"),f(BM.$$.fragment),aLo=l(),yae=a("p"),nLo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),sLo=l(),Va=a("p"),lLo=o("The model class to instantiate is selected based on the "),wae=a("code"),iLo=o("model_type"),dLo=o(` property of the config object (either
passed as an argument or loaded from `),Aae=a("code"),cLo=o("pretrained_model_name_or_path"),fLo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lae=a("code"),mLo=o("pretrained_model_name_or_path"),gLo=o(":"),hLo=l(),Bae=a("ul"),J5=a("li"),xae=a("strong"),pLo=o("tapas"),_Lo=o(" \u2014 "),ZI=a("a"),uLo=o("TapasForQuestionAnswering"),bLo=o(" (TAPAS model)"),vLo=l(),Y5=a("p"),TLo=o("The model is set in evaluation mode by default using "),kae=a("code"),FLo=o("model.eval()"),CLo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rae=a("code"),MLo=o("model.train()"),ELo=l(),Sae=a("p"),yLo=o("Examples:"),wLo=l(),f(xM.$$.fragment),ZAe=l(),md=a("h2"),K5=a("a"),Pae=a("span"),f(kM.$$.fragment),ALo=l(),$ae=a("span"),LLo=o("AutoModelForImageClassification"),e0e=l(),or=a("div"),f(RM.$$.fragment),BLo=l(),gd=a("p"),xLo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Iae=a("code"),kLo=o("from_pretrained()"),RLo=o("class method or the "),jae=a("code"),SLo=o("from_config()"),PLo=o(`class
method.`),$Lo=l(),SM=a("p"),ILo=o("This class cannot be instantiated directly using "),Nae=a("code"),jLo=o("__init__()"),NLo=o(" (throws an error)."),DLo=l(),Qr=a("div"),f(PM.$$.fragment),qLo=l(),Dae=a("p"),GLo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),OLo=l(),hd=a("p"),XLo=o(`Note:
Loading a model from its configuration file does `),qae=a("strong"),zLo=o("not"),VLo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gae=a("code"),WLo=o("from_pretrained()"),QLo=o("to load the model weights."),HLo=l(),Oae=a("p"),ULo=o("Examples:"),JLo=l(),f($M.$$.fragment),YLo=l(),Ge=a("div"),f(IM.$$.fragment),KLo=l(),Xae=a("p"),ZLo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),e9o=l(),Wa=a("p"),o9o=o("The model class to instantiate is selected based on the "),zae=a("code"),r9o=o("model_type"),t9o=o(` property of the config object (either
passed as an argument or loaded from `),Vae=a("code"),a9o=o("pretrained_model_name_or_path"),n9o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wae=a("code"),s9o=o("pretrained_model_name_or_path"),l9o=o(":"),i9o=l(),we=a("ul"),Z5=a("li"),Qae=a("strong"),d9o=o("beit"),c9o=o(" \u2014 "),ej=a("a"),f9o=o("BeitForImageClassification"),m9o=o(" (BEiT model)"),g9o=l(),e2=a("li"),Hae=a("strong"),h9o=o("convnext"),p9o=o(" \u2014 "),oj=a("a"),_9o=o("ConvNextForImageClassification"),u9o=o(" (ConvNext model)"),b9o=l(),As=a("li"),Uae=a("strong"),v9o=o("deit"),T9o=o(" \u2014 "),rj=a("a"),F9o=o("DeiTForImageClassification"),C9o=o(" or "),tj=a("a"),M9o=o("DeiTForImageClassificationWithTeacher"),E9o=o(" (DeiT model)"),y9o=l(),o2=a("li"),Jae=a("strong"),w9o=o("imagegpt"),A9o=o(" \u2014 "),aj=a("a"),L9o=o("ImageGPTForImageClassification"),B9o=o(" (ImageGPT model)"),x9o=l(),ta=a("li"),Yae=a("strong"),k9o=o("perceiver"),R9o=o(" \u2014 "),nj=a("a"),S9o=o("PerceiverForImageClassificationLearned"),P9o=o(" or "),sj=a("a"),$9o=o("PerceiverForImageClassificationFourier"),I9o=o(" or "),lj=a("a"),j9o=o("PerceiverForImageClassificationConvProcessing"),N9o=o(" (Perceiver model)"),D9o=l(),r2=a("li"),Kae=a("strong"),q9o=o("segformer"),G9o=o(" \u2014 "),ij=a("a"),O9o=o("SegformerForImageClassification"),X9o=o(" (SegFormer model)"),z9o=l(),t2=a("li"),Zae=a("strong"),V9o=o("swin"),W9o=o(" \u2014 "),dj=a("a"),Q9o=o("SwinForImageClassification"),H9o=o(" (Swin model)"),U9o=l(),a2=a("li"),ene=a("strong"),J9o=o("vit"),Y9o=o(" \u2014 "),cj=a("a"),K9o=o("ViTForImageClassification"),Z9o=o(" (ViT model)"),eBo=l(),n2=a("p"),oBo=o("The model is set in evaluation mode by default using "),one=a("code"),rBo=o("model.eval()"),tBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),rne=a("code"),aBo=o("model.train()"),nBo=l(),tne=a("p"),sBo=o("Examples:"),lBo=l(),f(jM.$$.fragment),o0e=l(),pd=a("h2"),s2=a("a"),ane=a("span"),f(NM.$$.fragment),iBo=l(),nne=a("span"),dBo=o("AutoModelForVision2Seq"),r0e=l(),rr=a("div"),f(DM.$$.fragment),cBo=l(),_d=a("p"),fBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),sne=a("code"),mBo=o("from_pretrained()"),gBo=o("class method or the "),lne=a("code"),hBo=o("from_config()"),pBo=o(`class
method.`),_Bo=l(),qM=a("p"),uBo=o("This class cannot be instantiated directly using "),ine=a("code"),bBo=o("__init__()"),vBo=o(" (throws an error)."),TBo=l(),Hr=a("div"),f(GM.$$.fragment),FBo=l(),dne=a("p"),CBo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),MBo=l(),ud=a("p"),EBo=o(`Note:
Loading a model from its configuration file does `),cne=a("strong"),yBo=o("not"),wBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fne=a("code"),ABo=o("from_pretrained()"),LBo=o("to load the model weights."),BBo=l(),mne=a("p"),xBo=o("Examples:"),kBo=l(),f(OM.$$.fragment),RBo=l(),Oe=a("div"),f(XM.$$.fragment),SBo=l(),gne=a("p"),PBo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),$Bo=l(),Qa=a("p"),IBo=o("The model class to instantiate is selected based on the "),hne=a("code"),jBo=o("model_type"),NBo=o(` property of the config object (either
passed as an argument or loaded from `),pne=a("code"),DBo=o("pretrained_model_name_or_path"),qBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_ne=a("code"),GBo=o("pretrained_model_name_or_path"),OBo=o(":"),XBo=l(),une=a("ul"),l2=a("li"),bne=a("strong"),zBo=o("vision-encoder-decoder"),VBo=o(" \u2014 "),fj=a("a"),WBo=o("VisionEncoderDecoderModel"),QBo=o(" (Vision Encoder decoder model)"),HBo=l(),i2=a("p"),UBo=o("The model is set in evaluation mode by default using "),vne=a("code"),JBo=o("model.eval()"),YBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tne=a("code"),KBo=o("model.train()"),ZBo=l(),Fne=a("p"),exo=o("Examples:"),oxo=l(),f(zM.$$.fragment),t0e=l(),bd=a("h2"),d2=a("a"),Cne=a("span"),f(VM.$$.fragment),rxo=l(),Mne=a("span"),txo=o("AutoModelForAudioClassification"),a0e=l(),tr=a("div"),f(WM.$$.fragment),axo=l(),vd=a("p"),nxo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Ene=a("code"),sxo=o("from_pretrained()"),lxo=o("class method or the "),yne=a("code"),ixo=o("from_config()"),dxo=o(`class
method.`),cxo=l(),QM=a("p"),fxo=o("This class cannot be instantiated directly using "),wne=a("code"),mxo=o("__init__()"),gxo=o(" (throws an error)."),hxo=l(),Ur=a("div"),f(HM.$$.fragment),pxo=l(),Ane=a("p"),_xo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),uxo=l(),Td=a("p"),bxo=o(`Note:
Loading a model from its configuration file does `),Lne=a("strong"),vxo=o("not"),Txo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bne=a("code"),Fxo=o("from_pretrained()"),Cxo=o("to load the model weights."),Mxo=l(),xne=a("p"),Exo=o("Examples:"),yxo=l(),f(UM.$$.fragment),wxo=l(),Xe=a("div"),f(JM.$$.fragment),Axo=l(),kne=a("p"),Lxo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),Bxo=l(),Ha=a("p"),xxo=o("The model class to instantiate is selected based on the "),Rne=a("code"),kxo=o("model_type"),Rxo=o(` property of the config object (either
passed as an argument or loaded from `),Sne=a("code"),Sxo=o("pretrained_model_name_or_path"),Pxo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pne=a("code"),$xo=o("pretrained_model_name_or_path"),Ixo=o(":"),jxo=l(),ro=a("ul"),c2=a("li"),$ne=a("strong"),Nxo=o("hubert"),Dxo=o(" \u2014 "),mj=a("a"),qxo=o("HubertForSequenceClassification"),Gxo=o(" (Hubert model)"),Oxo=l(),f2=a("li"),Ine=a("strong"),Xxo=o("sew"),zxo=o(" \u2014 "),gj=a("a"),Vxo=o("SEWForSequenceClassification"),Wxo=o(" (SEW model)"),Qxo=l(),m2=a("li"),jne=a("strong"),Hxo=o("sew-d"),Uxo=o(" \u2014 "),hj=a("a"),Jxo=o("SEWDForSequenceClassification"),Yxo=o(" (SEW-D model)"),Kxo=l(),g2=a("li"),Nne=a("strong"),Zxo=o("unispeech"),eko=o(" \u2014 "),pj=a("a"),oko=o("UniSpeechForSequenceClassification"),rko=o(" (UniSpeech model)"),tko=l(),h2=a("li"),Dne=a("strong"),ako=o("unispeech-sat"),nko=o(" \u2014 "),_j=a("a"),sko=o("UniSpeechSatForSequenceClassification"),lko=o(" (UniSpeechSat model)"),iko=l(),p2=a("li"),qne=a("strong"),dko=o("wav2vec2"),cko=o(" \u2014 "),uj=a("a"),fko=o("Wav2Vec2ForSequenceClassification"),mko=o(" (Wav2Vec2 model)"),gko=l(),_2=a("li"),Gne=a("strong"),hko=o("wavlm"),pko=o(" \u2014 "),bj=a("a"),_ko=o("WavLMForSequenceClassification"),uko=o(" (WavLM model)"),bko=l(),u2=a("p"),vko=o("The model is set in evaluation mode by default using "),One=a("code"),Tko=o("model.eval()"),Fko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xne=a("code"),Cko=o("model.train()"),Mko=l(),zne=a("p"),Eko=o("Examples:"),yko=l(),f(YM.$$.fragment),n0e=l(),Fd=a("h2"),b2=a("a"),Vne=a("span"),f(KM.$$.fragment),wko=l(),Wne=a("span"),Ako=o("AutoModelForAudioFrameClassification"),s0e=l(),ar=a("div"),f(ZM.$$.fragment),Lko=l(),Cd=a("p"),Bko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Qne=a("code"),xko=o("from_pretrained()"),kko=o("class method or the "),Hne=a("code"),Rko=o("from_config()"),Sko=o(`class
method.`),Pko=l(),eE=a("p"),$ko=o("This class cannot be instantiated directly using "),Une=a("code"),Iko=o("__init__()"),jko=o(" (throws an error)."),Nko=l(),Jr=a("div"),f(oE.$$.fragment),Dko=l(),Jne=a("p"),qko=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),Gko=l(),Md=a("p"),Oko=o(`Note:
Loading a model from its configuration file does `),Yne=a("strong"),Xko=o("not"),zko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kne=a("code"),Vko=o("from_pretrained()"),Wko=o("to load the model weights."),Qko=l(),Zne=a("p"),Hko=o("Examples:"),Uko=l(),f(rE.$$.fragment),Jko=l(),ze=a("div"),f(tE.$$.fragment),Yko=l(),ese=a("p"),Kko=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),Zko=l(),Ua=a("p"),eRo=o("The model class to instantiate is selected based on the "),ose=a("code"),oRo=o("model_type"),rRo=o(` property of the config object (either
passed as an argument or loaded from `),rse=a("code"),tRo=o("pretrained_model_name_or_path"),aRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tse=a("code"),nRo=o("pretrained_model_name_or_path"),sRo=o(":"),lRo=l(),Ed=a("ul"),v2=a("li"),ase=a("strong"),iRo=o("unispeech-sat"),dRo=o(" \u2014 "),vj=a("a"),cRo=o("UniSpeechSatForAudioFrameClassification"),fRo=o(" (UniSpeechSat model)"),mRo=l(),T2=a("li"),nse=a("strong"),gRo=o("wav2vec2"),hRo=o(" \u2014 "),Tj=a("a"),pRo=o("Wav2Vec2ForAudioFrameClassification"),_Ro=o(" (Wav2Vec2 model)"),uRo=l(),F2=a("li"),sse=a("strong"),bRo=o("wavlm"),vRo=o(" \u2014 "),Fj=a("a"),TRo=o("WavLMForAudioFrameClassification"),FRo=o(" (WavLM model)"),CRo=l(),C2=a("p"),MRo=o("The model is set in evaluation mode by default using "),lse=a("code"),ERo=o("model.eval()"),yRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=a("code"),wRo=o("model.train()"),ARo=l(),dse=a("p"),LRo=o("Examples:"),BRo=l(),f(aE.$$.fragment),l0e=l(),yd=a("h2"),M2=a("a"),cse=a("span"),f(nE.$$.fragment),xRo=l(),fse=a("span"),kRo=o("AutoModelForCTC"),i0e=l(),nr=a("div"),f(sE.$$.fragment),RRo=l(),wd=a("p"),SRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),mse=a("code"),PRo=o("from_pretrained()"),$Ro=o("class method or the "),gse=a("code"),IRo=o("from_config()"),jRo=o(`class
method.`),NRo=l(),lE=a("p"),DRo=o("This class cannot be instantiated directly using "),hse=a("code"),qRo=o("__init__()"),GRo=o(" (throws an error)."),ORo=l(),Yr=a("div"),f(iE.$$.fragment),XRo=l(),pse=a("p"),zRo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),VRo=l(),Ad=a("p"),WRo=o(`Note:
Loading a model from its configuration file does `),_se=a("strong"),QRo=o("not"),HRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),use=a("code"),URo=o("from_pretrained()"),JRo=o("to load the model weights."),YRo=l(),bse=a("p"),KRo=o("Examples:"),ZRo=l(),f(dE.$$.fragment),eSo=l(),Ve=a("div"),f(cE.$$.fragment),oSo=l(),vse=a("p"),rSo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),tSo=l(),Ja=a("p"),aSo=o("The model class to instantiate is selected based on the "),Tse=a("code"),nSo=o("model_type"),sSo=o(` property of the config object (either
passed as an argument or loaded from `),Fse=a("code"),lSo=o("pretrained_model_name_or_path"),iSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=a("code"),dSo=o("pretrained_model_name_or_path"),cSo=o(":"),fSo=l(),to=a("ul"),E2=a("li"),Mse=a("strong"),mSo=o("hubert"),gSo=o(" \u2014 "),Cj=a("a"),hSo=o("HubertForCTC"),pSo=o(" (Hubert model)"),_So=l(),y2=a("li"),Ese=a("strong"),uSo=o("sew"),bSo=o(" \u2014 "),Mj=a("a"),vSo=o("SEWForCTC"),TSo=o(" (SEW model)"),FSo=l(),w2=a("li"),yse=a("strong"),CSo=o("sew-d"),MSo=o(" \u2014 "),Ej=a("a"),ESo=o("SEWDForCTC"),ySo=o(" (SEW-D model)"),wSo=l(),A2=a("li"),wse=a("strong"),ASo=o("unispeech"),LSo=o(" \u2014 "),yj=a("a"),BSo=o("UniSpeechForCTC"),xSo=o(" (UniSpeech model)"),kSo=l(),L2=a("li"),Ase=a("strong"),RSo=o("unispeech-sat"),SSo=o(" \u2014 "),wj=a("a"),PSo=o("UniSpeechSatForCTC"),$So=o(" (UniSpeechSat model)"),ISo=l(),B2=a("li"),Lse=a("strong"),jSo=o("wav2vec2"),NSo=o(" \u2014 "),Aj=a("a"),DSo=o("Wav2Vec2ForCTC"),qSo=o(" (Wav2Vec2 model)"),GSo=l(),x2=a("li"),Bse=a("strong"),OSo=o("wavlm"),XSo=o(" \u2014 "),Lj=a("a"),zSo=o("WavLMForCTC"),VSo=o(" (WavLM model)"),WSo=l(),k2=a("p"),QSo=o("The model is set in evaluation mode by default using "),xse=a("code"),HSo=o("model.eval()"),USo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kse=a("code"),JSo=o("model.train()"),YSo=l(),Rse=a("p"),KSo=o("Examples:"),ZSo=l(),f(fE.$$.fragment),d0e=l(),Ld=a("h2"),R2=a("a"),Sse=a("span"),f(mE.$$.fragment),ePo=l(),Pse=a("span"),oPo=o("AutoModelForSpeechSeq2Seq"),c0e=l(),sr=a("div"),f(gE.$$.fragment),rPo=l(),Bd=a("p"),tPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),$se=a("code"),aPo=o("from_pretrained()"),nPo=o("class method or the "),Ise=a("code"),sPo=o("from_config()"),lPo=o(`class
method.`),iPo=l(),hE=a("p"),dPo=o("This class cannot be instantiated directly using "),jse=a("code"),cPo=o("__init__()"),fPo=o(" (throws an error)."),mPo=l(),Kr=a("div"),f(pE.$$.fragment),gPo=l(),Nse=a("p"),hPo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),pPo=l(),xd=a("p"),_Po=o(`Note:
Loading a model from its configuration file does `),Dse=a("strong"),uPo=o("not"),bPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qse=a("code"),vPo=o("from_pretrained()"),TPo=o("to load the model weights."),FPo=l(),Gse=a("p"),CPo=o("Examples:"),MPo=l(),f(_E.$$.fragment),EPo=l(),We=a("div"),f(uE.$$.fragment),yPo=l(),Ose=a("p"),wPo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),APo=l(),Ya=a("p"),LPo=o("The model class to instantiate is selected based on the "),Xse=a("code"),BPo=o("model_type"),xPo=o(` property of the config object (either
passed as an argument or loaded from `),zse=a("code"),kPo=o("pretrained_model_name_or_path"),RPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vse=a("code"),SPo=o("pretrained_model_name_or_path"),PPo=o(":"),$Po=l(),bE=a("ul"),S2=a("li"),Wse=a("strong"),IPo=o("speech-encoder-decoder"),jPo=o(" \u2014 "),Bj=a("a"),NPo=o("SpeechEncoderDecoderModel"),DPo=o(" (Speech Encoder decoder model)"),qPo=l(),P2=a("li"),Qse=a("strong"),GPo=o("speech_to_text"),OPo=o(" \u2014 "),xj=a("a"),XPo=o("Speech2TextForConditionalGeneration"),zPo=o(" (Speech2Text model)"),VPo=l(),$2=a("p"),WPo=o("The model is set in evaluation mode by default using "),Hse=a("code"),QPo=o("model.eval()"),HPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=a("code"),UPo=o("model.train()"),JPo=l(),Jse=a("p"),YPo=o("Examples:"),KPo=l(),f(vE.$$.fragment),f0e=l(),kd=a("h2"),I2=a("a"),Yse=a("span"),f(TE.$$.fragment),ZPo=l(),Kse=a("span"),e$o=o("AutoModelForAudioXVector"),m0e=l(),lr=a("div"),f(FE.$$.fragment),o$o=l(),Rd=a("p"),r$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),Zse=a("code"),t$o=o("from_pretrained()"),a$o=o("class method or the "),ele=a("code"),n$o=o("from_config()"),s$o=o(`class
method.`),l$o=l(),CE=a("p"),i$o=o("This class cannot be instantiated directly using "),ole=a("code"),d$o=o("__init__()"),c$o=o(" (throws an error)."),f$o=l(),Zr=a("div"),f(ME.$$.fragment),m$o=l(),rle=a("p"),g$o=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),h$o=l(),Sd=a("p"),p$o=o(`Note:
Loading a model from its configuration file does `),tle=a("strong"),_$o=o("not"),u$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=a("code"),b$o=o("from_pretrained()"),v$o=o("to load the model weights."),T$o=l(),nle=a("p"),F$o=o("Examples:"),C$o=l(),f(EE.$$.fragment),M$o=l(),Qe=a("div"),f(yE.$$.fragment),E$o=l(),sle=a("p"),y$o=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),w$o=l(),Ka=a("p"),A$o=o("The model class to instantiate is selected based on the "),lle=a("code"),L$o=o("model_type"),B$o=o(` property of the config object (either
passed as an argument or loaded from `),ile=a("code"),x$o=o("pretrained_model_name_or_path"),k$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=a("code"),R$o=o("pretrained_model_name_or_path"),S$o=o(":"),P$o=l(),Pd=a("ul"),j2=a("li"),cle=a("strong"),$$o=o("unispeech-sat"),I$o=o(" \u2014 "),kj=a("a"),j$o=o("UniSpeechSatForXVector"),N$o=o(" (UniSpeechSat model)"),D$o=l(),N2=a("li"),fle=a("strong"),q$o=o("wav2vec2"),G$o=o(" \u2014 "),Rj=a("a"),O$o=o("Wav2Vec2ForXVector"),X$o=o(" (Wav2Vec2 model)"),z$o=l(),D2=a("li"),mle=a("strong"),V$o=o("wavlm"),W$o=o(" \u2014 "),Sj=a("a"),Q$o=o("WavLMForXVector"),H$o=o(" (WavLM model)"),U$o=l(),q2=a("p"),J$o=o("The model is set in evaluation mode by default using "),gle=a("code"),Y$o=o("model.eval()"),K$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=a("code"),Z$o=o("model.train()"),eIo=l(),ple=a("p"),oIo=o("Examples:"),rIo=l(),f(wE.$$.fragment),g0e=l(),$d=a("h2"),G2=a("a"),_le=a("span"),f(AE.$$.fragment),tIo=l(),ule=a("span"),aIo=o("AutoModelForObjectDetection"),h0e=l(),ir=a("div"),f(LE.$$.fragment),nIo=l(),Id=a("p"),sIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),ble=a("code"),lIo=o("from_pretrained()"),iIo=o("class method or the "),vle=a("code"),dIo=o("from_config()"),cIo=o(`class
method.`),fIo=l(),BE=a("p"),mIo=o("This class cannot be instantiated directly using "),Tle=a("code"),gIo=o("__init__()"),hIo=o(" (throws an error)."),pIo=l(),et=a("div"),f(xE.$$.fragment),_Io=l(),Fle=a("p"),uIo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),bIo=l(),jd=a("p"),vIo=o(`Note:
Loading a model from its configuration file does `),Cle=a("strong"),TIo=o("not"),FIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=a("code"),CIo=o("from_pretrained()"),MIo=o("to load the model weights."),EIo=l(),Ele=a("p"),yIo=o("Examples:"),wIo=l(),f(kE.$$.fragment),AIo=l(),He=a("div"),f(RE.$$.fragment),LIo=l(),yle=a("p"),BIo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),xIo=l(),Za=a("p"),kIo=o("The model class to instantiate is selected based on the "),wle=a("code"),RIo=o("model_type"),SIo=o(` property of the config object (either
passed as an argument or loaded from `),Ale=a("code"),PIo=o("pretrained_model_name_or_path"),$Io=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=a("code"),IIo=o("pretrained_model_name_or_path"),jIo=o(":"),NIo=l(),Ble=a("ul"),O2=a("li"),xle=a("strong"),DIo=o("detr"),qIo=o(" \u2014 "),Pj=a("a"),GIo=o("DetrForObjectDetection"),OIo=o(" (DETR model)"),XIo=l(),X2=a("p"),zIo=o("The model is set in evaluation mode by default using "),kle=a("code"),VIo=o("model.eval()"),WIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rle=a("code"),QIo=o("model.train()"),HIo=l(),Sle=a("p"),UIo=o("Examples:"),JIo=l(),f(SE.$$.fragment),p0e=l(),Nd=a("h2"),z2=a("a"),Ple=a("span"),f(PE.$$.fragment),YIo=l(),$le=a("span"),KIo=o("AutoModelForImageSegmentation"),_0e=l(),dr=a("div"),f($E.$$.fragment),ZIo=l(),Dd=a("p"),ejo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Ile=a("code"),ojo=o("from_pretrained()"),rjo=o("class method or the "),jle=a("code"),tjo=o("from_config()"),ajo=o(`class
method.`),njo=l(),IE=a("p"),sjo=o("This class cannot be instantiated directly using "),Nle=a("code"),ljo=o("__init__()"),ijo=o(" (throws an error)."),djo=l(),ot=a("div"),f(jE.$$.fragment),cjo=l(),Dle=a("p"),fjo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),mjo=l(),qd=a("p"),gjo=o(`Note:
Loading a model from its configuration file does `),qle=a("strong"),hjo=o("not"),pjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gle=a("code"),_jo=o("from_pretrained()"),ujo=o("to load the model weights."),bjo=l(),Ole=a("p"),vjo=o("Examples:"),Tjo=l(),f(NE.$$.fragment),Fjo=l(),Ue=a("div"),f(DE.$$.fragment),Cjo=l(),Xle=a("p"),Mjo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),Ejo=l(),en=a("p"),yjo=o("The model class to instantiate is selected based on the "),zle=a("code"),wjo=o("model_type"),Ajo=o(` property of the config object (either
passed as an argument or loaded from `),Vle=a("code"),Ljo=o("pretrained_model_name_or_path"),Bjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wle=a("code"),xjo=o("pretrained_model_name_or_path"),kjo=o(":"),Rjo=l(),Qle=a("ul"),V2=a("li"),Hle=a("strong"),Sjo=o("detr"),Pjo=o(" \u2014 "),$j=a("a"),$jo=o("DetrForSegmentation"),Ijo=o(" (DETR model)"),jjo=l(),W2=a("p"),Njo=o("The model is set in evaluation mode by default using "),Ule=a("code"),Djo=o("model.eval()"),qjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jle=a("code"),Gjo=o("model.train()"),Ojo=l(),Yle=a("p"),Xjo=o("Examples:"),zjo=l(),f(qE.$$.fragment),u0e=l(),Gd=a("h2"),Q2=a("a"),Kle=a("span"),f(GE.$$.fragment),Vjo=l(),Zle=a("span"),Wjo=o("AutoModelForSemanticSegmentation"),b0e=l(),cr=a("div"),f(OE.$$.fragment),Qjo=l(),Od=a("p"),Hjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),eie=a("code"),Ujo=o("from_pretrained()"),Jjo=o("class method or the "),oie=a("code"),Yjo=o("from_config()"),Kjo=o(`class
method.`),Zjo=l(),XE=a("p"),eNo=o("This class cannot be instantiated directly using "),rie=a("code"),oNo=o("__init__()"),rNo=o(" (throws an error)."),tNo=l(),rt=a("div"),f(zE.$$.fragment),aNo=l(),tie=a("p"),nNo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),sNo=l(),Xd=a("p"),lNo=o(`Note:
Loading a model from its configuration file does `),aie=a("strong"),iNo=o("not"),dNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nie=a("code"),cNo=o("from_pretrained()"),fNo=o("to load the model weights."),mNo=l(),sie=a("p"),gNo=o("Examples:"),hNo=l(),f(VE.$$.fragment),pNo=l(),Je=a("div"),f(WE.$$.fragment),_No=l(),lie=a("p"),uNo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),bNo=l(),on=a("p"),vNo=o("The model class to instantiate is selected based on the "),iie=a("code"),TNo=o("model_type"),FNo=o(` property of the config object (either
passed as an argument or loaded from `),die=a("code"),CNo=o("pretrained_model_name_or_path"),MNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cie=a("code"),ENo=o("pretrained_model_name_or_path"),yNo=o(":"),wNo=l(),QE=a("ul"),H2=a("li"),fie=a("strong"),ANo=o("beit"),LNo=o(" \u2014 "),Ij=a("a"),BNo=o("BeitForSemanticSegmentation"),xNo=o(" (BEiT model)"),kNo=l(),U2=a("li"),mie=a("strong"),RNo=o("segformer"),SNo=o(" \u2014 "),jj=a("a"),PNo=o("SegformerForSemanticSegmentation"),$No=o(" (SegFormer model)"),INo=l(),J2=a("p"),jNo=o("The model is set in evaluation mode by default using "),gie=a("code"),NNo=o("model.eval()"),DNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hie=a("code"),qNo=o("model.train()"),GNo=l(),pie=a("p"),ONo=o("Examples:"),XNo=l(),f(HE.$$.fragment),v0e=l(),zd=a("h2"),Y2=a("a"),_ie=a("span"),f(UE.$$.fragment),zNo=l(),uie=a("span"),VNo=o("TFAutoModel"),T0e=l(),fr=a("div"),f(JE.$$.fragment),WNo=l(),Vd=a("p"),QNo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),bie=a("code"),HNo=o("from_pretrained()"),UNo=o("class method or the "),vie=a("code"),JNo=o("from_config()"),YNo=o(`class
method.`),KNo=l(),YE=a("p"),ZNo=o("This class cannot be instantiated directly using "),Tie=a("code"),eDo=o("__init__()"),oDo=o(" (throws an error)."),rDo=l(),tt=a("div"),f(KE.$$.fragment),tDo=l(),Fie=a("p"),aDo=o("Instantiates one of the base model classes of the library from a configuration."),nDo=l(),Wd=a("p"),sDo=o(`Note:
Loading a model from its configuration file does `),Cie=a("strong"),lDo=o("not"),iDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mie=a("code"),dDo=o("from_pretrained()"),cDo=o("to load the model weights."),fDo=l(),Eie=a("p"),mDo=o("Examples:"),gDo=l(),f(ZE.$$.fragment),hDo=l(),fo=a("div"),f(e3.$$.fragment),pDo=l(),yie=a("p"),_Do=o("Instantiate one of the base model classes of the library from a pretrained model."),uDo=l(),rn=a("p"),bDo=o("The model class to instantiate is selected based on the "),wie=a("code"),vDo=o("model_type"),TDo=o(` property of the config object (either
passed as an argument or loaded from `),Aie=a("code"),FDo=o("pretrained_model_name_or_path"),CDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lie=a("code"),MDo=o("pretrained_model_name_or_path"),EDo=o(":"),yDo=l(),B=a("ul"),K2=a("li"),Bie=a("strong"),wDo=o("albert"),ADo=o(" \u2014 "),Nj=a("a"),LDo=o("TFAlbertModel"),BDo=o(" (ALBERT model)"),xDo=l(),Z2=a("li"),xie=a("strong"),kDo=o("bart"),RDo=o(" \u2014 "),Dj=a("a"),SDo=o("TFBartModel"),PDo=o(" (BART model)"),$Do=l(),ev=a("li"),kie=a("strong"),IDo=o("bert"),jDo=o(" \u2014 "),qj=a("a"),NDo=o("TFBertModel"),DDo=o(" (BERT model)"),qDo=l(),ov=a("li"),Rie=a("strong"),GDo=o("blenderbot"),ODo=o(" \u2014 "),Gj=a("a"),XDo=o("TFBlenderbotModel"),zDo=o(" (Blenderbot model)"),VDo=l(),rv=a("li"),Sie=a("strong"),WDo=o("blenderbot-small"),QDo=o(" \u2014 "),Oj=a("a"),HDo=o("TFBlenderbotSmallModel"),UDo=o(" (BlenderbotSmall model)"),JDo=l(),tv=a("li"),Pie=a("strong"),YDo=o("camembert"),KDo=o(" \u2014 "),Xj=a("a"),ZDo=o("TFCamembertModel"),eqo=o(" (CamemBERT model)"),oqo=l(),av=a("li"),$ie=a("strong"),rqo=o("clip"),tqo=o(" \u2014 "),zj=a("a"),aqo=o("TFCLIPModel"),nqo=o(" (CLIP model)"),sqo=l(),nv=a("li"),Iie=a("strong"),lqo=o("convbert"),iqo=o(" \u2014 "),Vj=a("a"),dqo=o("TFConvBertModel"),cqo=o(" (ConvBERT model)"),fqo=l(),sv=a("li"),jie=a("strong"),mqo=o("ctrl"),gqo=o(" \u2014 "),Wj=a("a"),hqo=o("TFCTRLModel"),pqo=o(" (CTRL model)"),_qo=l(),lv=a("li"),Nie=a("strong"),uqo=o("deberta"),bqo=o(" \u2014 "),Qj=a("a"),vqo=o("TFDebertaModel"),Tqo=o(" (DeBERTa model)"),Fqo=l(),iv=a("li"),Die=a("strong"),Cqo=o("deberta-v2"),Mqo=o(" \u2014 "),Hj=a("a"),Eqo=o("TFDebertaV2Model"),yqo=o(" (DeBERTa-v2 model)"),wqo=l(),dv=a("li"),qie=a("strong"),Aqo=o("distilbert"),Lqo=o(" \u2014 "),Uj=a("a"),Bqo=o("TFDistilBertModel"),xqo=o(" (DistilBERT model)"),kqo=l(),cv=a("li"),Gie=a("strong"),Rqo=o("dpr"),Sqo=o(" \u2014 "),Jj=a("a"),Pqo=o("TFDPRQuestionEncoder"),$qo=o(" (DPR model)"),Iqo=l(),fv=a("li"),Oie=a("strong"),jqo=o("electra"),Nqo=o(" \u2014 "),Yj=a("a"),Dqo=o("TFElectraModel"),qqo=o(" (ELECTRA model)"),Gqo=l(),mv=a("li"),Xie=a("strong"),Oqo=o("flaubert"),Xqo=o(" \u2014 "),Kj=a("a"),zqo=o("TFFlaubertModel"),Vqo=o(" (FlauBERT model)"),Wqo=l(),Ls=a("li"),zie=a("strong"),Qqo=o("funnel"),Hqo=o(" \u2014 "),Zj=a("a"),Uqo=o("TFFunnelModel"),Jqo=o(" or "),eN=a("a"),Yqo=o("TFFunnelBaseModel"),Kqo=o(" (Funnel Transformer model)"),Zqo=l(),gv=a("li"),Vie=a("strong"),eGo=o("gpt2"),oGo=o(" \u2014 "),oN=a("a"),rGo=o("TFGPT2Model"),tGo=o(" (OpenAI GPT-2 model)"),aGo=l(),hv=a("li"),Wie=a("strong"),nGo=o("hubert"),sGo=o(" \u2014 "),rN=a("a"),lGo=o("TFHubertModel"),iGo=o(" (Hubert model)"),dGo=l(),pv=a("li"),Qie=a("strong"),cGo=o("layoutlm"),fGo=o(" \u2014 "),tN=a("a"),mGo=o("TFLayoutLMModel"),gGo=o(" (LayoutLM model)"),hGo=l(),_v=a("li"),Hie=a("strong"),pGo=o("led"),_Go=o(" \u2014 "),aN=a("a"),uGo=o("TFLEDModel"),bGo=o(" (LED model)"),vGo=l(),uv=a("li"),Uie=a("strong"),TGo=o("longformer"),FGo=o(" \u2014 "),nN=a("a"),CGo=o("TFLongformerModel"),MGo=o(" (Longformer model)"),EGo=l(),bv=a("li"),Jie=a("strong"),yGo=o("lxmert"),wGo=o(" \u2014 "),sN=a("a"),AGo=o("TFLxmertModel"),LGo=o(" (LXMERT model)"),BGo=l(),vv=a("li"),Yie=a("strong"),xGo=o("marian"),kGo=o(" \u2014 "),lN=a("a"),RGo=o("TFMarianModel"),SGo=o(" (Marian model)"),PGo=l(),Tv=a("li"),Kie=a("strong"),$Go=o("mbart"),IGo=o(" \u2014 "),iN=a("a"),jGo=o("TFMBartModel"),NGo=o(" (mBART model)"),DGo=l(),Fv=a("li"),Zie=a("strong"),qGo=o("mobilebert"),GGo=o(" \u2014 "),dN=a("a"),OGo=o("TFMobileBertModel"),XGo=o(" (MobileBERT model)"),zGo=l(),Cv=a("li"),ede=a("strong"),VGo=o("mpnet"),WGo=o(" \u2014 "),cN=a("a"),QGo=o("TFMPNetModel"),HGo=o(" (MPNet model)"),UGo=l(),Mv=a("li"),ode=a("strong"),JGo=o("mt5"),YGo=o(" \u2014 "),fN=a("a"),KGo=o("TFMT5Model"),ZGo=o(" (mT5 model)"),eOo=l(),Ev=a("li"),rde=a("strong"),oOo=o("openai-gpt"),rOo=o(" \u2014 "),mN=a("a"),tOo=o("TFOpenAIGPTModel"),aOo=o(" (OpenAI GPT model)"),nOo=l(),yv=a("li"),tde=a("strong"),sOo=o("pegasus"),lOo=o(" \u2014 "),gN=a("a"),iOo=o("TFPegasusModel"),dOo=o(" (Pegasus model)"),cOo=l(),wv=a("li"),ade=a("strong"),fOo=o("rembert"),mOo=o(" \u2014 "),hN=a("a"),gOo=o("TFRemBertModel"),hOo=o(" (RemBERT model)"),pOo=l(),Av=a("li"),nde=a("strong"),_Oo=o("roberta"),uOo=o(" \u2014 "),pN=a("a"),bOo=o("TFRobertaModel"),vOo=o(" (RoBERTa model)"),TOo=l(),Lv=a("li"),sde=a("strong"),FOo=o("roformer"),COo=o(" \u2014 "),_N=a("a"),MOo=o("TFRoFormerModel"),EOo=o(" (RoFormer model)"),yOo=l(),Bv=a("li"),lde=a("strong"),wOo=o("speech_to_text"),AOo=o(" \u2014 "),uN=a("a"),LOo=o("TFSpeech2TextModel"),BOo=o(" (Speech2Text model)"),xOo=l(),xv=a("li"),ide=a("strong"),kOo=o("t5"),ROo=o(" \u2014 "),bN=a("a"),SOo=o("TFT5Model"),POo=o(" (T5 model)"),$Oo=l(),kv=a("li"),dde=a("strong"),IOo=o("tapas"),jOo=o(" \u2014 "),vN=a("a"),NOo=o("TFTapasModel"),DOo=o(" (TAPAS model)"),qOo=l(),Rv=a("li"),cde=a("strong"),GOo=o("transfo-xl"),OOo=o(" \u2014 "),TN=a("a"),XOo=o("TFTransfoXLModel"),zOo=o(" (Transformer-XL model)"),VOo=l(),Sv=a("li"),fde=a("strong"),WOo=o("vit"),QOo=o(" \u2014 "),FN=a("a"),HOo=o("TFViTModel"),UOo=o(" (ViT model)"),JOo=l(),Pv=a("li"),mde=a("strong"),YOo=o("wav2vec2"),KOo=o(" \u2014 "),CN=a("a"),ZOo=o("TFWav2Vec2Model"),eXo=o(" (Wav2Vec2 model)"),oXo=l(),$v=a("li"),gde=a("strong"),rXo=o("xlm"),tXo=o(" \u2014 "),MN=a("a"),aXo=o("TFXLMModel"),nXo=o(" (XLM model)"),sXo=l(),Iv=a("li"),hde=a("strong"),lXo=o("xlm-roberta"),iXo=o(" \u2014 "),EN=a("a"),dXo=o("TFXLMRobertaModel"),cXo=o(" (XLM-RoBERTa model)"),fXo=l(),jv=a("li"),pde=a("strong"),mXo=o("xlnet"),gXo=o(" \u2014 "),yN=a("a"),hXo=o("TFXLNetModel"),pXo=o(" (XLNet model)"),_Xo=l(),_de=a("p"),uXo=o("Examples:"),bXo=l(),f(o3.$$.fragment),F0e=l(),Qd=a("h2"),Nv=a("a"),ude=a("span"),f(r3.$$.fragment),vXo=l(),bde=a("span"),TXo=o("TFAutoModelForPreTraining"),C0e=l(),mr=a("div"),f(t3.$$.fragment),FXo=l(),Hd=a("p"),CXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),vde=a("code"),MXo=o("from_pretrained()"),EXo=o("class method or the "),Tde=a("code"),yXo=o("from_config()"),wXo=o(`class
method.`),AXo=l(),a3=a("p"),LXo=o("This class cannot be instantiated directly using "),Fde=a("code"),BXo=o("__init__()"),xXo=o(" (throws an error)."),kXo=l(),at=a("div"),f(n3.$$.fragment),RXo=l(),Cde=a("p"),SXo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),PXo=l(),Ud=a("p"),$Xo=o(`Note:
Loading a model from its configuration file does `),Mde=a("strong"),IXo=o("not"),jXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ede=a("code"),NXo=o("from_pretrained()"),DXo=o("to load the model weights."),qXo=l(),yde=a("p"),GXo=o("Examples:"),OXo=l(),f(s3.$$.fragment),XXo=l(),mo=a("div"),f(l3.$$.fragment),zXo=l(),wde=a("p"),VXo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),WXo=l(),tn=a("p"),QXo=o("The model class to instantiate is selected based on the "),Ade=a("code"),HXo=o("model_type"),UXo=o(` property of the config object (either
passed as an argument or loaded from `),Lde=a("code"),JXo=o("pretrained_model_name_or_path"),YXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bde=a("code"),KXo=o("pretrained_model_name_or_path"),ZXo=o(":"),ezo=l(),H=a("ul"),Dv=a("li"),xde=a("strong"),ozo=o("albert"),rzo=o(" \u2014 "),wN=a("a"),tzo=o("TFAlbertForPreTraining"),azo=o(" (ALBERT model)"),nzo=l(),qv=a("li"),kde=a("strong"),szo=o("bart"),lzo=o(" \u2014 "),AN=a("a"),izo=o("TFBartForConditionalGeneration"),dzo=o(" (BART model)"),czo=l(),Gv=a("li"),Rde=a("strong"),fzo=o("bert"),mzo=o(" \u2014 "),LN=a("a"),gzo=o("TFBertForPreTraining"),hzo=o(" (BERT model)"),pzo=l(),Ov=a("li"),Sde=a("strong"),_zo=o("camembert"),uzo=o(" \u2014 "),BN=a("a"),bzo=o("TFCamembertForMaskedLM"),vzo=o(" (CamemBERT model)"),Tzo=l(),Xv=a("li"),Pde=a("strong"),Fzo=o("ctrl"),Czo=o(" \u2014 "),xN=a("a"),Mzo=o("TFCTRLLMHeadModel"),Ezo=o(" (CTRL model)"),yzo=l(),zv=a("li"),$de=a("strong"),wzo=o("distilbert"),Azo=o(" \u2014 "),kN=a("a"),Lzo=o("TFDistilBertForMaskedLM"),Bzo=o(" (DistilBERT model)"),xzo=l(),Vv=a("li"),Ide=a("strong"),kzo=o("electra"),Rzo=o(" \u2014 "),RN=a("a"),Szo=o("TFElectraForPreTraining"),Pzo=o(" (ELECTRA model)"),$zo=l(),Wv=a("li"),jde=a("strong"),Izo=o("flaubert"),jzo=o(" \u2014 "),SN=a("a"),Nzo=o("TFFlaubertWithLMHeadModel"),Dzo=o(" (FlauBERT model)"),qzo=l(),Qv=a("li"),Nde=a("strong"),Gzo=o("funnel"),Ozo=o(" \u2014 "),PN=a("a"),Xzo=o("TFFunnelForPreTraining"),zzo=o(" (Funnel Transformer model)"),Vzo=l(),Hv=a("li"),Dde=a("strong"),Wzo=o("gpt2"),Qzo=o(" \u2014 "),$N=a("a"),Hzo=o("TFGPT2LMHeadModel"),Uzo=o(" (OpenAI GPT-2 model)"),Jzo=l(),Uv=a("li"),qde=a("strong"),Yzo=o("layoutlm"),Kzo=o(" \u2014 "),IN=a("a"),Zzo=o("TFLayoutLMForMaskedLM"),eVo=o(" (LayoutLM model)"),oVo=l(),Jv=a("li"),Gde=a("strong"),rVo=o("lxmert"),tVo=o(" \u2014 "),jN=a("a"),aVo=o("TFLxmertForPreTraining"),nVo=o(" (LXMERT model)"),sVo=l(),Yv=a("li"),Ode=a("strong"),lVo=o("mobilebert"),iVo=o(" \u2014 "),NN=a("a"),dVo=o("TFMobileBertForPreTraining"),cVo=o(" (MobileBERT model)"),fVo=l(),Kv=a("li"),Xde=a("strong"),mVo=o("mpnet"),gVo=o(" \u2014 "),DN=a("a"),hVo=o("TFMPNetForMaskedLM"),pVo=o(" (MPNet model)"),_Vo=l(),Zv=a("li"),zde=a("strong"),uVo=o("openai-gpt"),bVo=o(" \u2014 "),qN=a("a"),vVo=o("TFOpenAIGPTLMHeadModel"),TVo=o(" (OpenAI GPT model)"),FVo=l(),e6=a("li"),Vde=a("strong"),CVo=o("roberta"),MVo=o(" \u2014 "),GN=a("a"),EVo=o("TFRobertaForMaskedLM"),yVo=o(" (RoBERTa model)"),wVo=l(),o6=a("li"),Wde=a("strong"),AVo=o("t5"),LVo=o(" \u2014 "),ON=a("a"),BVo=o("TFT5ForConditionalGeneration"),xVo=o(" (T5 model)"),kVo=l(),r6=a("li"),Qde=a("strong"),RVo=o("tapas"),SVo=o(" \u2014 "),XN=a("a"),PVo=o("TFTapasForMaskedLM"),$Vo=o(" (TAPAS model)"),IVo=l(),t6=a("li"),Hde=a("strong"),jVo=o("transfo-xl"),NVo=o(" \u2014 "),zN=a("a"),DVo=o("TFTransfoXLLMHeadModel"),qVo=o(" (Transformer-XL model)"),GVo=l(),a6=a("li"),Ude=a("strong"),OVo=o("xlm"),XVo=o(" \u2014 "),VN=a("a"),zVo=o("TFXLMWithLMHeadModel"),VVo=o(" (XLM model)"),WVo=l(),n6=a("li"),Jde=a("strong"),QVo=o("xlm-roberta"),HVo=o(" \u2014 "),WN=a("a"),UVo=o("TFXLMRobertaForMaskedLM"),JVo=o(" (XLM-RoBERTa model)"),YVo=l(),s6=a("li"),Yde=a("strong"),KVo=o("xlnet"),ZVo=o(" \u2014 "),QN=a("a"),eWo=o("TFXLNetLMHeadModel"),oWo=o(" (XLNet model)"),rWo=l(),Kde=a("p"),tWo=o("Examples:"),aWo=l(),f(i3.$$.fragment),M0e=l(),Jd=a("h2"),l6=a("a"),Zde=a("span"),f(d3.$$.fragment),nWo=l(),ece=a("span"),sWo=o("TFAutoModelForCausalLM"),E0e=l(),gr=a("div"),f(c3.$$.fragment),lWo=l(),Yd=a("p"),iWo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),oce=a("code"),dWo=o("from_pretrained()"),cWo=o("class method or the "),rce=a("code"),fWo=o("from_config()"),mWo=o(`class
method.`),gWo=l(),f3=a("p"),hWo=o("This class cannot be instantiated directly using "),tce=a("code"),pWo=o("__init__()"),_Wo=o(" (throws an error)."),uWo=l(),nt=a("div"),f(m3.$$.fragment),bWo=l(),ace=a("p"),vWo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),TWo=l(),Kd=a("p"),FWo=o(`Note:
Loading a model from its configuration file does `),nce=a("strong"),CWo=o("not"),MWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sce=a("code"),EWo=o("from_pretrained()"),yWo=o("to load the model weights."),wWo=l(),lce=a("p"),AWo=o("Examples:"),LWo=l(),f(g3.$$.fragment),BWo=l(),go=a("div"),f(h3.$$.fragment),xWo=l(),ice=a("p"),kWo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),RWo=l(),an=a("p"),SWo=o("The model class to instantiate is selected based on the "),dce=a("code"),PWo=o("model_type"),$Wo=o(` property of the config object (either
passed as an argument or loaded from `),cce=a("code"),IWo=o("pretrained_model_name_or_path"),jWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fce=a("code"),NWo=o("pretrained_model_name_or_path"),DWo=o(":"),qWo=l(),he=a("ul"),i6=a("li"),mce=a("strong"),GWo=o("bert"),OWo=o(" \u2014 "),HN=a("a"),XWo=o("TFBertLMHeadModel"),zWo=o(" (BERT model)"),VWo=l(),d6=a("li"),gce=a("strong"),WWo=o("ctrl"),QWo=o(" \u2014 "),UN=a("a"),HWo=o("TFCTRLLMHeadModel"),UWo=o(" (CTRL model)"),JWo=l(),c6=a("li"),hce=a("strong"),YWo=o("gpt2"),KWo=o(" \u2014 "),JN=a("a"),ZWo=o("TFGPT2LMHeadModel"),eQo=o(" (OpenAI GPT-2 model)"),oQo=l(),f6=a("li"),pce=a("strong"),rQo=o("openai-gpt"),tQo=o(" \u2014 "),YN=a("a"),aQo=o("TFOpenAIGPTLMHeadModel"),nQo=o(" (OpenAI GPT model)"),sQo=l(),m6=a("li"),_ce=a("strong"),lQo=o("rembert"),iQo=o(" \u2014 "),KN=a("a"),dQo=o("TFRemBertForCausalLM"),cQo=o(" (RemBERT model)"),fQo=l(),g6=a("li"),uce=a("strong"),mQo=o("roberta"),gQo=o(" \u2014 "),ZN=a("a"),hQo=o("TFRobertaForCausalLM"),pQo=o(" (RoBERTa model)"),_Qo=l(),h6=a("li"),bce=a("strong"),uQo=o("roformer"),bQo=o(" \u2014 "),eD=a("a"),vQo=o("TFRoFormerForCausalLM"),TQo=o(" (RoFormer model)"),FQo=l(),p6=a("li"),vce=a("strong"),CQo=o("transfo-xl"),MQo=o(" \u2014 "),oD=a("a"),EQo=o("TFTransfoXLLMHeadModel"),yQo=o(" (Transformer-XL model)"),wQo=l(),_6=a("li"),Tce=a("strong"),AQo=o("xlm"),LQo=o(" \u2014 "),rD=a("a"),BQo=o("TFXLMWithLMHeadModel"),xQo=o(" (XLM model)"),kQo=l(),u6=a("li"),Fce=a("strong"),RQo=o("xlnet"),SQo=o(" \u2014 "),tD=a("a"),PQo=o("TFXLNetLMHeadModel"),$Qo=o(" (XLNet model)"),IQo=l(),Cce=a("p"),jQo=o("Examples:"),NQo=l(),f(p3.$$.fragment),y0e=l(),Zd=a("h2"),b6=a("a"),Mce=a("span"),f(_3.$$.fragment),DQo=l(),Ece=a("span"),qQo=o("TFAutoModelForImageClassification"),w0e=l(),hr=a("div"),f(u3.$$.fragment),GQo=l(),ec=a("p"),OQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),yce=a("code"),XQo=o("from_pretrained()"),zQo=o("class method or the "),wce=a("code"),VQo=o("from_config()"),WQo=o(`class
method.`),QQo=l(),b3=a("p"),HQo=o("This class cannot be instantiated directly using "),Ace=a("code"),UQo=o("__init__()"),JQo=o(" (throws an error)."),YQo=l(),st=a("div"),f(v3.$$.fragment),KQo=l(),Lce=a("p"),ZQo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),eHo=l(),oc=a("p"),oHo=o(`Note:
Loading a model from its configuration file does `),Bce=a("strong"),rHo=o("not"),tHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xce=a("code"),aHo=o("from_pretrained()"),nHo=o("to load the model weights."),sHo=l(),kce=a("p"),lHo=o("Examples:"),iHo=l(),f(T3.$$.fragment),dHo=l(),ho=a("div"),f(F3.$$.fragment),cHo=l(),Rce=a("p"),fHo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),mHo=l(),nn=a("p"),gHo=o("The model class to instantiate is selected based on the "),Sce=a("code"),hHo=o("model_type"),pHo=o(` property of the config object (either
passed as an argument or loaded from `),Pce=a("code"),_Ho=o("pretrained_model_name_or_path"),uHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$ce=a("code"),bHo=o("pretrained_model_name_or_path"),vHo=o(":"),THo=l(),Ice=a("ul"),v6=a("li"),jce=a("strong"),FHo=o("vit"),CHo=o(" \u2014 "),aD=a("a"),MHo=o("TFViTForImageClassification"),EHo=o(" (ViT model)"),yHo=l(),Nce=a("p"),wHo=o("Examples:"),AHo=l(),f(C3.$$.fragment),A0e=l(),rc=a("h2"),T6=a("a"),Dce=a("span"),f(M3.$$.fragment),LHo=l(),qce=a("span"),BHo=o("TFAutoModelForMaskedLM"),L0e=l(),pr=a("div"),f(E3.$$.fragment),xHo=l(),tc=a("p"),kHo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Gce=a("code"),RHo=o("from_pretrained()"),SHo=o("class method or the "),Oce=a("code"),PHo=o("from_config()"),$Ho=o(`class
method.`),IHo=l(),y3=a("p"),jHo=o("This class cannot be instantiated directly using "),Xce=a("code"),NHo=o("__init__()"),DHo=o(" (throws an error)."),qHo=l(),lt=a("div"),f(w3.$$.fragment),GHo=l(),zce=a("p"),OHo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),XHo=l(),ac=a("p"),zHo=o(`Note:
Loading a model from its configuration file does `),Vce=a("strong"),VHo=o("not"),WHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wce=a("code"),QHo=o("from_pretrained()"),HHo=o("to load the model weights."),UHo=l(),Qce=a("p"),JHo=o("Examples:"),YHo=l(),f(A3.$$.fragment),KHo=l(),po=a("div"),f(L3.$$.fragment),ZHo=l(),Hce=a("p"),eUo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),oUo=l(),sn=a("p"),rUo=o("The model class to instantiate is selected based on the "),Uce=a("code"),tUo=o("model_type"),aUo=o(` property of the config object (either
passed as an argument or loaded from `),Jce=a("code"),nUo=o("pretrained_model_name_or_path"),sUo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yce=a("code"),lUo=o("pretrained_model_name_or_path"),iUo=o(":"),dUo=l(),Y=a("ul"),F6=a("li"),Kce=a("strong"),cUo=o("albert"),fUo=o(" \u2014 "),nD=a("a"),mUo=o("TFAlbertForMaskedLM"),gUo=o(" (ALBERT model)"),hUo=l(),C6=a("li"),Zce=a("strong"),pUo=o("bert"),_Uo=o(" \u2014 "),sD=a("a"),uUo=o("TFBertForMaskedLM"),bUo=o(" (BERT model)"),vUo=l(),M6=a("li"),efe=a("strong"),TUo=o("camembert"),FUo=o(" \u2014 "),lD=a("a"),CUo=o("TFCamembertForMaskedLM"),MUo=o(" (CamemBERT model)"),EUo=l(),E6=a("li"),ofe=a("strong"),yUo=o("convbert"),wUo=o(" \u2014 "),iD=a("a"),AUo=o("TFConvBertForMaskedLM"),LUo=o(" (ConvBERT model)"),BUo=l(),y6=a("li"),rfe=a("strong"),xUo=o("deberta"),kUo=o(" \u2014 "),dD=a("a"),RUo=o("TFDebertaForMaskedLM"),SUo=o(" (DeBERTa model)"),PUo=l(),w6=a("li"),tfe=a("strong"),$Uo=o("deberta-v2"),IUo=o(" \u2014 "),cD=a("a"),jUo=o("TFDebertaV2ForMaskedLM"),NUo=o(" (DeBERTa-v2 model)"),DUo=l(),A6=a("li"),afe=a("strong"),qUo=o("distilbert"),GUo=o(" \u2014 "),fD=a("a"),OUo=o("TFDistilBertForMaskedLM"),XUo=o(" (DistilBERT model)"),zUo=l(),L6=a("li"),nfe=a("strong"),VUo=o("electra"),WUo=o(" \u2014 "),mD=a("a"),QUo=o("TFElectraForMaskedLM"),HUo=o(" (ELECTRA model)"),UUo=l(),B6=a("li"),sfe=a("strong"),JUo=o("flaubert"),YUo=o(" \u2014 "),gD=a("a"),KUo=o("TFFlaubertWithLMHeadModel"),ZUo=o(" (FlauBERT model)"),eJo=l(),x6=a("li"),lfe=a("strong"),oJo=o("funnel"),rJo=o(" \u2014 "),hD=a("a"),tJo=o("TFFunnelForMaskedLM"),aJo=o(" (Funnel Transformer model)"),nJo=l(),k6=a("li"),ife=a("strong"),sJo=o("layoutlm"),lJo=o(" \u2014 "),pD=a("a"),iJo=o("TFLayoutLMForMaskedLM"),dJo=o(" (LayoutLM model)"),cJo=l(),R6=a("li"),dfe=a("strong"),fJo=o("longformer"),mJo=o(" \u2014 "),_D=a("a"),gJo=o("TFLongformerForMaskedLM"),hJo=o(" (Longformer model)"),pJo=l(),S6=a("li"),cfe=a("strong"),_Jo=o("mobilebert"),uJo=o(" \u2014 "),uD=a("a"),bJo=o("TFMobileBertForMaskedLM"),vJo=o(" (MobileBERT model)"),TJo=l(),P6=a("li"),ffe=a("strong"),FJo=o("mpnet"),CJo=o(" \u2014 "),bD=a("a"),MJo=o("TFMPNetForMaskedLM"),EJo=o(" (MPNet model)"),yJo=l(),$6=a("li"),mfe=a("strong"),wJo=o("rembert"),AJo=o(" \u2014 "),vD=a("a"),LJo=o("TFRemBertForMaskedLM"),BJo=o(" (RemBERT model)"),xJo=l(),I6=a("li"),gfe=a("strong"),kJo=o("roberta"),RJo=o(" \u2014 "),TD=a("a"),SJo=o("TFRobertaForMaskedLM"),PJo=o(" (RoBERTa model)"),$Jo=l(),j6=a("li"),hfe=a("strong"),IJo=o("roformer"),jJo=o(" \u2014 "),FD=a("a"),NJo=o("TFRoFormerForMaskedLM"),DJo=o(" (RoFormer model)"),qJo=l(),N6=a("li"),pfe=a("strong"),GJo=o("tapas"),OJo=o(" \u2014 "),CD=a("a"),XJo=o("TFTapasForMaskedLM"),zJo=o(" (TAPAS model)"),VJo=l(),D6=a("li"),_fe=a("strong"),WJo=o("xlm"),QJo=o(" \u2014 "),MD=a("a"),HJo=o("TFXLMWithLMHeadModel"),UJo=o(" (XLM model)"),JJo=l(),q6=a("li"),ufe=a("strong"),YJo=o("xlm-roberta"),KJo=o(" \u2014 "),ED=a("a"),ZJo=o("TFXLMRobertaForMaskedLM"),eYo=o(" (XLM-RoBERTa model)"),oYo=l(),bfe=a("p"),rYo=o("Examples:"),tYo=l(),f(B3.$$.fragment),B0e=l(),nc=a("h2"),G6=a("a"),vfe=a("span"),f(x3.$$.fragment),aYo=l(),Tfe=a("span"),nYo=o("TFAutoModelForSeq2SeqLM"),x0e=l(),_r=a("div"),f(k3.$$.fragment),sYo=l(),sc=a("p"),lYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Ffe=a("code"),iYo=o("from_pretrained()"),dYo=o("class method or the "),Cfe=a("code"),cYo=o("from_config()"),fYo=o(`class
method.`),mYo=l(),R3=a("p"),gYo=o("This class cannot be instantiated directly using "),Mfe=a("code"),hYo=o("__init__()"),pYo=o(" (throws an error)."),_Yo=l(),it=a("div"),f(S3.$$.fragment),uYo=l(),Efe=a("p"),bYo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),vYo=l(),lc=a("p"),TYo=o(`Note:
Loading a model from its configuration file does `),yfe=a("strong"),FYo=o("not"),CYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wfe=a("code"),MYo=o("from_pretrained()"),EYo=o("to load the model weights."),yYo=l(),Afe=a("p"),wYo=o("Examples:"),AYo=l(),f(P3.$$.fragment),LYo=l(),_o=a("div"),f($3.$$.fragment),BYo=l(),Lfe=a("p"),xYo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),kYo=l(),ln=a("p"),RYo=o("The model class to instantiate is selected based on the "),Bfe=a("code"),SYo=o("model_type"),PYo=o(` property of the config object (either
passed as an argument or loaded from `),xfe=a("code"),$Yo=o("pretrained_model_name_or_path"),IYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kfe=a("code"),jYo=o("pretrained_model_name_or_path"),NYo=o(":"),DYo=l(),pe=a("ul"),O6=a("li"),Rfe=a("strong"),qYo=o("bart"),GYo=o(" \u2014 "),yD=a("a"),OYo=o("TFBartForConditionalGeneration"),XYo=o(" (BART model)"),zYo=l(),X6=a("li"),Sfe=a("strong"),VYo=o("blenderbot"),WYo=o(" \u2014 "),wD=a("a"),QYo=o("TFBlenderbotForConditionalGeneration"),HYo=o(" (Blenderbot model)"),UYo=l(),z6=a("li"),Pfe=a("strong"),JYo=o("blenderbot-small"),YYo=o(" \u2014 "),AD=a("a"),KYo=o("TFBlenderbotSmallForConditionalGeneration"),ZYo=o(" (BlenderbotSmall model)"),eKo=l(),V6=a("li"),$fe=a("strong"),oKo=o("encoder-decoder"),rKo=o(" \u2014 "),LD=a("a"),tKo=o("TFEncoderDecoderModel"),aKo=o(" (Encoder decoder model)"),nKo=l(),W6=a("li"),Ife=a("strong"),sKo=o("led"),lKo=o(" \u2014 "),BD=a("a"),iKo=o("TFLEDForConditionalGeneration"),dKo=o(" (LED model)"),cKo=l(),Q6=a("li"),jfe=a("strong"),fKo=o("marian"),mKo=o(" \u2014 "),xD=a("a"),gKo=o("TFMarianMTModel"),hKo=o(" (Marian model)"),pKo=l(),H6=a("li"),Nfe=a("strong"),_Ko=o("mbart"),uKo=o(" \u2014 "),kD=a("a"),bKo=o("TFMBartForConditionalGeneration"),vKo=o(" (mBART model)"),TKo=l(),U6=a("li"),Dfe=a("strong"),FKo=o("mt5"),CKo=o(" \u2014 "),RD=a("a"),MKo=o("TFMT5ForConditionalGeneration"),EKo=o(" (mT5 model)"),yKo=l(),J6=a("li"),qfe=a("strong"),wKo=o("pegasus"),AKo=o(" \u2014 "),SD=a("a"),LKo=o("TFPegasusForConditionalGeneration"),BKo=o(" (Pegasus model)"),xKo=l(),Y6=a("li"),Gfe=a("strong"),kKo=o("t5"),RKo=o(" \u2014 "),PD=a("a"),SKo=o("TFT5ForConditionalGeneration"),PKo=o(" (T5 model)"),$Ko=l(),Ofe=a("p"),IKo=o("Examples:"),jKo=l(),f(I3.$$.fragment),k0e=l(),ic=a("h2"),K6=a("a"),Xfe=a("span"),f(j3.$$.fragment),NKo=l(),zfe=a("span"),DKo=o("TFAutoModelForSequenceClassification"),R0e=l(),ur=a("div"),f(N3.$$.fragment),qKo=l(),dc=a("p"),GKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Vfe=a("code"),OKo=o("from_pretrained()"),XKo=o("class method or the "),Wfe=a("code"),zKo=o("from_config()"),VKo=o(`class
method.`),WKo=l(),D3=a("p"),QKo=o("This class cannot be instantiated directly using "),Qfe=a("code"),HKo=o("__init__()"),UKo=o(" (throws an error)."),JKo=l(),dt=a("div"),f(q3.$$.fragment),YKo=l(),Hfe=a("p"),KKo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),ZKo=l(),cc=a("p"),eZo=o(`Note:
Loading a model from its configuration file does `),Ufe=a("strong"),oZo=o("not"),rZo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jfe=a("code"),tZo=o("from_pretrained()"),aZo=o("to load the model weights."),nZo=l(),Yfe=a("p"),sZo=o("Examples:"),lZo=l(),f(G3.$$.fragment),iZo=l(),uo=a("div"),f(O3.$$.fragment),dZo=l(),Kfe=a("p"),cZo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),fZo=l(),dn=a("p"),mZo=o("The model class to instantiate is selected based on the "),Zfe=a("code"),gZo=o("model_type"),hZo=o(` property of the config object (either
passed as an argument or loaded from `),eme=a("code"),pZo=o("pretrained_model_name_or_path"),_Zo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ome=a("code"),uZo=o("pretrained_model_name_or_path"),bZo=o(":"),vZo=l(),X=a("ul"),Z6=a("li"),rme=a("strong"),TZo=o("albert"),FZo=o(" \u2014 "),$D=a("a"),CZo=o("TFAlbertForSequenceClassification"),MZo=o(" (ALBERT model)"),EZo=l(),eT=a("li"),tme=a("strong"),yZo=o("bert"),wZo=o(" \u2014 "),ID=a("a"),AZo=o("TFBertForSequenceClassification"),LZo=o(" (BERT model)"),BZo=l(),oT=a("li"),ame=a("strong"),xZo=o("camembert"),kZo=o(" \u2014 "),jD=a("a"),RZo=o("TFCamembertForSequenceClassification"),SZo=o(" (CamemBERT model)"),PZo=l(),rT=a("li"),nme=a("strong"),$Zo=o("convbert"),IZo=o(" \u2014 "),ND=a("a"),jZo=o("TFConvBertForSequenceClassification"),NZo=o(" (ConvBERT model)"),DZo=l(),tT=a("li"),sme=a("strong"),qZo=o("ctrl"),GZo=o(" \u2014 "),DD=a("a"),OZo=o("TFCTRLForSequenceClassification"),XZo=o(" (CTRL model)"),zZo=l(),aT=a("li"),lme=a("strong"),VZo=o("deberta"),WZo=o(" \u2014 "),qD=a("a"),QZo=o("TFDebertaForSequenceClassification"),HZo=o(" (DeBERTa model)"),UZo=l(),nT=a("li"),ime=a("strong"),JZo=o("deberta-v2"),YZo=o(" \u2014 "),GD=a("a"),KZo=o("TFDebertaV2ForSequenceClassification"),ZZo=o(" (DeBERTa-v2 model)"),eer=l(),sT=a("li"),dme=a("strong"),oer=o("distilbert"),rer=o(" \u2014 "),OD=a("a"),ter=o("TFDistilBertForSequenceClassification"),aer=o(" (DistilBERT model)"),ner=l(),lT=a("li"),cme=a("strong"),ser=o("electra"),ler=o(" \u2014 "),XD=a("a"),ier=o("TFElectraForSequenceClassification"),der=o(" (ELECTRA model)"),cer=l(),iT=a("li"),fme=a("strong"),fer=o("flaubert"),mer=o(" \u2014 "),zD=a("a"),ger=o("TFFlaubertForSequenceClassification"),her=o(" (FlauBERT model)"),per=l(),dT=a("li"),mme=a("strong"),_er=o("funnel"),uer=o(" \u2014 "),VD=a("a"),ber=o("TFFunnelForSequenceClassification"),ver=o(" (Funnel Transformer model)"),Ter=l(),cT=a("li"),gme=a("strong"),Fer=o("gpt2"),Cer=o(" \u2014 "),WD=a("a"),Mer=o("TFGPT2ForSequenceClassification"),Eer=o(" (OpenAI GPT-2 model)"),yer=l(),fT=a("li"),hme=a("strong"),wer=o("layoutlm"),Aer=o(" \u2014 "),QD=a("a"),Ler=o("TFLayoutLMForSequenceClassification"),Ber=o(" (LayoutLM model)"),xer=l(),mT=a("li"),pme=a("strong"),ker=o("longformer"),Rer=o(" \u2014 "),HD=a("a"),Ser=o("TFLongformerForSequenceClassification"),Per=o(" (Longformer model)"),$er=l(),gT=a("li"),_me=a("strong"),Ier=o("mobilebert"),jer=o(" \u2014 "),UD=a("a"),Ner=o("TFMobileBertForSequenceClassification"),Der=o(" (MobileBERT model)"),qer=l(),hT=a("li"),ume=a("strong"),Ger=o("mpnet"),Oer=o(" \u2014 "),JD=a("a"),Xer=o("TFMPNetForSequenceClassification"),zer=o(" (MPNet model)"),Ver=l(),pT=a("li"),bme=a("strong"),Wer=o("openai-gpt"),Qer=o(" \u2014 "),YD=a("a"),Her=o("TFOpenAIGPTForSequenceClassification"),Uer=o(" (OpenAI GPT model)"),Jer=l(),_T=a("li"),vme=a("strong"),Yer=o("rembert"),Ker=o(" \u2014 "),KD=a("a"),Zer=o("TFRemBertForSequenceClassification"),eor=o(" (RemBERT model)"),oor=l(),uT=a("li"),Tme=a("strong"),ror=o("roberta"),tor=o(" \u2014 "),ZD=a("a"),aor=o("TFRobertaForSequenceClassification"),nor=o(" (RoBERTa model)"),sor=l(),bT=a("li"),Fme=a("strong"),lor=o("roformer"),ior=o(" \u2014 "),eq=a("a"),dor=o("TFRoFormerForSequenceClassification"),cor=o(" (RoFormer model)"),mor=l(),vT=a("li"),Cme=a("strong"),gor=o("tapas"),hor=o(" \u2014 "),oq=a("a"),por=o("TFTapasForSequenceClassification"),_or=o(" (TAPAS model)"),uor=l(),TT=a("li"),Mme=a("strong"),bor=o("transfo-xl"),vor=o(" \u2014 "),rq=a("a"),Tor=o("TFTransfoXLForSequenceClassification"),For=o(" (Transformer-XL model)"),Cor=l(),FT=a("li"),Eme=a("strong"),Mor=o("xlm"),Eor=o(" \u2014 "),tq=a("a"),yor=o("TFXLMForSequenceClassification"),wor=o(" (XLM model)"),Aor=l(),CT=a("li"),yme=a("strong"),Lor=o("xlm-roberta"),Bor=o(" \u2014 "),aq=a("a"),xor=o("TFXLMRobertaForSequenceClassification"),kor=o(" (XLM-RoBERTa model)"),Ror=l(),MT=a("li"),wme=a("strong"),Sor=o("xlnet"),Por=o(" \u2014 "),nq=a("a"),$or=o("TFXLNetForSequenceClassification"),Ior=o(" (XLNet model)"),jor=l(),Ame=a("p"),Nor=o("Examples:"),Dor=l(),f(X3.$$.fragment),S0e=l(),fc=a("h2"),ET=a("a"),Lme=a("span"),f(z3.$$.fragment),qor=l(),Bme=a("span"),Gor=o("TFAutoModelForMultipleChoice"),P0e=l(),br=a("div"),f(V3.$$.fragment),Oor=l(),mc=a("p"),Xor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),xme=a("code"),zor=o("from_pretrained()"),Vor=o("class method or the "),kme=a("code"),Wor=o("from_config()"),Qor=o(`class
method.`),Hor=l(),W3=a("p"),Uor=o("This class cannot be instantiated directly using "),Rme=a("code"),Jor=o("__init__()"),Yor=o(" (throws an error)."),Kor=l(),ct=a("div"),f(Q3.$$.fragment),Zor=l(),Sme=a("p"),err=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),orr=l(),gc=a("p"),rrr=o(`Note:
Loading a model from its configuration file does `),Pme=a("strong"),trr=o("not"),arr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$me=a("code"),nrr=o("from_pretrained()"),srr=o("to load the model weights."),lrr=l(),Ime=a("p"),irr=o("Examples:"),drr=l(),f(H3.$$.fragment),crr=l(),bo=a("div"),f(U3.$$.fragment),frr=l(),jme=a("p"),mrr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),grr=l(),cn=a("p"),hrr=o("The model class to instantiate is selected based on the "),Nme=a("code"),prr=o("model_type"),_rr=o(` property of the config object (either
passed as an argument or loaded from `),Dme=a("code"),urr=o("pretrained_model_name_or_path"),brr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qme=a("code"),vrr=o("pretrained_model_name_or_path"),Trr=o(":"),Frr=l(),te=a("ul"),yT=a("li"),Gme=a("strong"),Crr=o("albert"),Mrr=o(" \u2014 "),sq=a("a"),Err=o("TFAlbertForMultipleChoice"),yrr=o(" (ALBERT model)"),wrr=l(),wT=a("li"),Ome=a("strong"),Arr=o("bert"),Lrr=o(" \u2014 "),lq=a("a"),Brr=o("TFBertForMultipleChoice"),xrr=o(" (BERT model)"),krr=l(),AT=a("li"),Xme=a("strong"),Rrr=o("camembert"),Srr=o(" \u2014 "),iq=a("a"),Prr=o("TFCamembertForMultipleChoice"),$rr=o(" (CamemBERT model)"),Irr=l(),LT=a("li"),zme=a("strong"),jrr=o("convbert"),Nrr=o(" \u2014 "),dq=a("a"),Drr=o("TFConvBertForMultipleChoice"),qrr=o(" (ConvBERT model)"),Grr=l(),BT=a("li"),Vme=a("strong"),Orr=o("distilbert"),Xrr=o(" \u2014 "),cq=a("a"),zrr=o("TFDistilBertForMultipleChoice"),Vrr=o(" (DistilBERT model)"),Wrr=l(),xT=a("li"),Wme=a("strong"),Qrr=o("electra"),Hrr=o(" \u2014 "),fq=a("a"),Urr=o("TFElectraForMultipleChoice"),Jrr=o(" (ELECTRA model)"),Yrr=l(),kT=a("li"),Qme=a("strong"),Krr=o("flaubert"),Zrr=o(" \u2014 "),mq=a("a"),etr=o("TFFlaubertForMultipleChoice"),otr=o(" (FlauBERT model)"),rtr=l(),RT=a("li"),Hme=a("strong"),ttr=o("funnel"),atr=o(" \u2014 "),gq=a("a"),ntr=o("TFFunnelForMultipleChoice"),str=o(" (Funnel Transformer model)"),ltr=l(),ST=a("li"),Ume=a("strong"),itr=o("longformer"),dtr=o(" \u2014 "),hq=a("a"),ctr=o("TFLongformerForMultipleChoice"),ftr=o(" (Longformer model)"),mtr=l(),PT=a("li"),Jme=a("strong"),gtr=o("mobilebert"),htr=o(" \u2014 "),pq=a("a"),ptr=o("TFMobileBertForMultipleChoice"),_tr=o(" (MobileBERT model)"),utr=l(),$T=a("li"),Yme=a("strong"),btr=o("mpnet"),vtr=o(" \u2014 "),_q=a("a"),Ttr=o("TFMPNetForMultipleChoice"),Ftr=o(" (MPNet model)"),Ctr=l(),IT=a("li"),Kme=a("strong"),Mtr=o("rembert"),Etr=o(" \u2014 "),uq=a("a"),ytr=o("TFRemBertForMultipleChoice"),wtr=o(" (RemBERT model)"),Atr=l(),jT=a("li"),Zme=a("strong"),Ltr=o("roberta"),Btr=o(" \u2014 "),bq=a("a"),xtr=o("TFRobertaForMultipleChoice"),ktr=o(" (RoBERTa model)"),Rtr=l(),NT=a("li"),ege=a("strong"),Str=o("roformer"),Ptr=o(" \u2014 "),vq=a("a"),$tr=o("TFRoFormerForMultipleChoice"),Itr=o(" (RoFormer model)"),jtr=l(),DT=a("li"),oge=a("strong"),Ntr=o("xlm"),Dtr=o(" \u2014 "),Tq=a("a"),qtr=o("TFXLMForMultipleChoice"),Gtr=o(" (XLM model)"),Otr=l(),qT=a("li"),rge=a("strong"),Xtr=o("xlm-roberta"),ztr=o(" \u2014 "),Fq=a("a"),Vtr=o("TFXLMRobertaForMultipleChoice"),Wtr=o(" (XLM-RoBERTa model)"),Qtr=l(),GT=a("li"),tge=a("strong"),Htr=o("xlnet"),Utr=o(" \u2014 "),Cq=a("a"),Jtr=o("TFXLNetForMultipleChoice"),Ytr=o(" (XLNet model)"),Ktr=l(),age=a("p"),Ztr=o("Examples:"),ear=l(),f(J3.$$.fragment),$0e=l(),hc=a("h2"),OT=a("a"),nge=a("span"),f(Y3.$$.fragment),oar=l(),sge=a("span"),rar=o("TFAutoModelForTableQuestionAnswering"),I0e=l(),vr=a("div"),f(K3.$$.fragment),tar=l(),pc=a("p"),aar=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),lge=a("code"),nar=o("from_pretrained()"),sar=o("class method or the "),ige=a("code"),lar=o("from_config()"),iar=o(`class
method.`),dar=l(),Z3=a("p"),car=o("This class cannot be instantiated directly using "),dge=a("code"),far=o("__init__()"),mar=o(" (throws an error)."),gar=l(),ft=a("div"),f(ey.$$.fragment),har=l(),cge=a("p"),par=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),_ar=l(),_c=a("p"),uar=o(`Note:
Loading a model from its configuration file does `),fge=a("strong"),bar=o("not"),Tar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mge=a("code"),Far=o("from_pretrained()"),Car=o("to load the model weights."),Mar=l(),gge=a("p"),Ear=o("Examples:"),yar=l(),f(oy.$$.fragment),war=l(),vo=a("div"),f(ry.$$.fragment),Aar=l(),hge=a("p"),Lar=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Bar=l(),fn=a("p"),xar=o("The model class to instantiate is selected based on the "),pge=a("code"),kar=o("model_type"),Rar=o(` property of the config object (either
passed as an argument or loaded from `),_ge=a("code"),Sar=o("pretrained_model_name_or_path"),Par=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),uge=a("code"),$ar=o("pretrained_model_name_or_path"),Iar=o(":"),jar=l(),bge=a("ul"),XT=a("li"),vge=a("strong"),Nar=o("tapas"),Dar=o(" \u2014 "),Mq=a("a"),qar=o("TFTapasForQuestionAnswering"),Gar=o(" (TAPAS model)"),Oar=l(),Tge=a("p"),Xar=o("Examples:"),zar=l(),f(ty.$$.fragment),j0e=l(),uc=a("h2"),zT=a("a"),Fge=a("span"),f(ay.$$.fragment),Var=l(),Cge=a("span"),War=o("TFAutoModelForTokenClassification"),N0e=l(),Tr=a("div"),f(ny.$$.fragment),Qar=l(),bc=a("p"),Har=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Mge=a("code"),Uar=o("from_pretrained()"),Jar=o("class method or the "),Ege=a("code"),Yar=o("from_config()"),Kar=o(`class
method.`),Zar=l(),sy=a("p"),enr=o("This class cannot be instantiated directly using "),yge=a("code"),onr=o("__init__()"),rnr=o(" (throws an error)."),tnr=l(),mt=a("div"),f(ly.$$.fragment),anr=l(),wge=a("p"),nnr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),snr=l(),vc=a("p"),lnr=o(`Note:
Loading a model from its configuration file does `),Age=a("strong"),inr=o("not"),dnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lge=a("code"),cnr=o("from_pretrained()"),fnr=o("to load the model weights."),mnr=l(),Bge=a("p"),gnr=o("Examples:"),hnr=l(),f(iy.$$.fragment),pnr=l(),To=a("div"),f(dy.$$.fragment),_nr=l(),xge=a("p"),unr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),bnr=l(),mn=a("p"),vnr=o("The model class to instantiate is selected based on the "),kge=a("code"),Tnr=o("model_type"),Fnr=o(` property of the config object (either
passed as an argument or loaded from `),Rge=a("code"),Cnr=o("pretrained_model_name_or_path"),Mnr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sge=a("code"),Enr=o("pretrained_model_name_or_path"),ynr=o(":"),wnr=l(),K=a("ul"),VT=a("li"),Pge=a("strong"),Anr=o("albert"),Lnr=o(" \u2014 "),Eq=a("a"),Bnr=o("TFAlbertForTokenClassification"),xnr=o(" (ALBERT model)"),knr=l(),WT=a("li"),$ge=a("strong"),Rnr=o("bert"),Snr=o(" \u2014 "),yq=a("a"),Pnr=o("TFBertForTokenClassification"),$nr=o(" (BERT model)"),Inr=l(),QT=a("li"),Ige=a("strong"),jnr=o("camembert"),Nnr=o(" \u2014 "),wq=a("a"),Dnr=o("TFCamembertForTokenClassification"),qnr=o(" (CamemBERT model)"),Gnr=l(),HT=a("li"),jge=a("strong"),Onr=o("convbert"),Xnr=o(" \u2014 "),Aq=a("a"),znr=o("TFConvBertForTokenClassification"),Vnr=o(" (ConvBERT model)"),Wnr=l(),UT=a("li"),Nge=a("strong"),Qnr=o("deberta"),Hnr=o(" \u2014 "),Lq=a("a"),Unr=o("TFDebertaForTokenClassification"),Jnr=o(" (DeBERTa model)"),Ynr=l(),JT=a("li"),Dge=a("strong"),Knr=o("deberta-v2"),Znr=o(" \u2014 "),Bq=a("a"),esr=o("TFDebertaV2ForTokenClassification"),osr=o(" (DeBERTa-v2 model)"),rsr=l(),YT=a("li"),qge=a("strong"),tsr=o("distilbert"),asr=o(" \u2014 "),xq=a("a"),nsr=o("TFDistilBertForTokenClassification"),ssr=o(" (DistilBERT model)"),lsr=l(),KT=a("li"),Gge=a("strong"),isr=o("electra"),dsr=o(" \u2014 "),kq=a("a"),csr=o("TFElectraForTokenClassification"),fsr=o(" (ELECTRA model)"),msr=l(),ZT=a("li"),Oge=a("strong"),gsr=o("flaubert"),hsr=o(" \u2014 "),Rq=a("a"),psr=o("TFFlaubertForTokenClassification"),_sr=o(" (FlauBERT model)"),usr=l(),e7=a("li"),Xge=a("strong"),bsr=o("funnel"),vsr=o(" \u2014 "),Sq=a("a"),Tsr=o("TFFunnelForTokenClassification"),Fsr=o(" (Funnel Transformer model)"),Csr=l(),o7=a("li"),zge=a("strong"),Msr=o("layoutlm"),Esr=o(" \u2014 "),Pq=a("a"),ysr=o("TFLayoutLMForTokenClassification"),wsr=o(" (LayoutLM model)"),Asr=l(),r7=a("li"),Vge=a("strong"),Lsr=o("longformer"),Bsr=o(" \u2014 "),$q=a("a"),xsr=o("TFLongformerForTokenClassification"),ksr=o(" (Longformer model)"),Rsr=l(),t7=a("li"),Wge=a("strong"),Ssr=o("mobilebert"),Psr=o(" \u2014 "),Iq=a("a"),$sr=o("TFMobileBertForTokenClassification"),Isr=o(" (MobileBERT model)"),jsr=l(),a7=a("li"),Qge=a("strong"),Nsr=o("mpnet"),Dsr=o(" \u2014 "),jq=a("a"),qsr=o("TFMPNetForTokenClassification"),Gsr=o(" (MPNet model)"),Osr=l(),n7=a("li"),Hge=a("strong"),Xsr=o("rembert"),zsr=o(" \u2014 "),Nq=a("a"),Vsr=o("TFRemBertForTokenClassification"),Wsr=o(" (RemBERT model)"),Qsr=l(),s7=a("li"),Uge=a("strong"),Hsr=o("roberta"),Usr=o(" \u2014 "),Dq=a("a"),Jsr=o("TFRobertaForTokenClassification"),Ysr=o(" (RoBERTa model)"),Ksr=l(),l7=a("li"),Jge=a("strong"),Zsr=o("roformer"),elr=o(" \u2014 "),qq=a("a"),olr=o("TFRoFormerForTokenClassification"),rlr=o(" (RoFormer model)"),tlr=l(),i7=a("li"),Yge=a("strong"),alr=o("xlm"),nlr=o(" \u2014 "),Gq=a("a"),slr=o("TFXLMForTokenClassification"),llr=o(" (XLM model)"),ilr=l(),d7=a("li"),Kge=a("strong"),dlr=o("xlm-roberta"),clr=o(" \u2014 "),Oq=a("a"),flr=o("TFXLMRobertaForTokenClassification"),mlr=o(" (XLM-RoBERTa model)"),glr=l(),c7=a("li"),Zge=a("strong"),hlr=o("xlnet"),plr=o(" \u2014 "),Xq=a("a"),_lr=o("TFXLNetForTokenClassification"),ulr=o(" (XLNet model)"),blr=l(),ehe=a("p"),vlr=o("Examples:"),Tlr=l(),f(cy.$$.fragment),D0e=l(),Tc=a("h2"),f7=a("a"),ohe=a("span"),f(fy.$$.fragment),Flr=l(),rhe=a("span"),Clr=o("TFAutoModelForQuestionAnswering"),q0e=l(),Fr=a("div"),f(my.$$.fragment),Mlr=l(),Fc=a("p"),Elr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),the=a("code"),ylr=o("from_pretrained()"),wlr=o("class method or the "),ahe=a("code"),Alr=o("from_config()"),Llr=o(`class
method.`),Blr=l(),gy=a("p"),xlr=o("This class cannot be instantiated directly using "),nhe=a("code"),klr=o("__init__()"),Rlr=o(" (throws an error)."),Slr=l(),gt=a("div"),f(hy.$$.fragment),Plr=l(),she=a("p"),$lr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Ilr=l(),Cc=a("p"),jlr=o(`Note:
Loading a model from its configuration file does `),lhe=a("strong"),Nlr=o("not"),Dlr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ihe=a("code"),qlr=o("from_pretrained()"),Glr=o("to load the model weights."),Olr=l(),dhe=a("p"),Xlr=o("Examples:"),zlr=l(),f(py.$$.fragment),Vlr=l(),Fo=a("div"),f(_y.$$.fragment),Wlr=l(),che=a("p"),Qlr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Hlr=l(),gn=a("p"),Ulr=o("The model class to instantiate is selected based on the "),fhe=a("code"),Jlr=o("model_type"),Ylr=o(` property of the config object (either
passed as an argument or loaded from `),mhe=a("code"),Klr=o("pretrained_model_name_or_path"),Zlr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ghe=a("code"),eir=o("pretrained_model_name_or_path"),oir=o(":"),rir=l(),Z=a("ul"),m7=a("li"),hhe=a("strong"),tir=o("albert"),air=o(" \u2014 "),zq=a("a"),nir=o("TFAlbertForQuestionAnswering"),sir=o(" (ALBERT model)"),lir=l(),g7=a("li"),phe=a("strong"),iir=o("bert"),dir=o(" \u2014 "),Vq=a("a"),cir=o("TFBertForQuestionAnswering"),fir=o(" (BERT model)"),mir=l(),h7=a("li"),_he=a("strong"),gir=o("camembert"),hir=o(" \u2014 "),Wq=a("a"),pir=o("TFCamembertForQuestionAnswering"),_ir=o(" (CamemBERT model)"),uir=l(),p7=a("li"),uhe=a("strong"),bir=o("convbert"),vir=o(" \u2014 "),Qq=a("a"),Tir=o("TFConvBertForQuestionAnswering"),Fir=o(" (ConvBERT model)"),Cir=l(),_7=a("li"),bhe=a("strong"),Mir=o("deberta"),Eir=o(" \u2014 "),Hq=a("a"),yir=o("TFDebertaForQuestionAnswering"),wir=o(" (DeBERTa model)"),Air=l(),u7=a("li"),vhe=a("strong"),Lir=o("deberta-v2"),Bir=o(" \u2014 "),Uq=a("a"),xir=o("TFDebertaV2ForQuestionAnswering"),kir=o(" (DeBERTa-v2 model)"),Rir=l(),b7=a("li"),The=a("strong"),Sir=o("distilbert"),Pir=o(" \u2014 "),Jq=a("a"),$ir=o("TFDistilBertForQuestionAnswering"),Iir=o(" (DistilBERT model)"),jir=l(),v7=a("li"),Fhe=a("strong"),Nir=o("electra"),Dir=o(" \u2014 "),Yq=a("a"),qir=o("TFElectraForQuestionAnswering"),Gir=o(" (ELECTRA model)"),Oir=l(),T7=a("li"),Che=a("strong"),Xir=o("flaubert"),zir=o(" \u2014 "),Kq=a("a"),Vir=o("TFFlaubertForQuestionAnsweringSimple"),Wir=o(" (FlauBERT model)"),Qir=l(),F7=a("li"),Mhe=a("strong"),Hir=o("funnel"),Uir=o(" \u2014 "),Zq=a("a"),Jir=o("TFFunnelForQuestionAnswering"),Yir=o(" (Funnel Transformer model)"),Kir=l(),C7=a("li"),Ehe=a("strong"),Zir=o("longformer"),edr=o(" \u2014 "),eG=a("a"),odr=o("TFLongformerForQuestionAnswering"),rdr=o(" (Longformer model)"),tdr=l(),M7=a("li"),yhe=a("strong"),adr=o("mobilebert"),ndr=o(" \u2014 "),oG=a("a"),sdr=o("TFMobileBertForQuestionAnswering"),ldr=o(" (MobileBERT model)"),idr=l(),E7=a("li"),whe=a("strong"),ddr=o("mpnet"),cdr=o(" \u2014 "),rG=a("a"),fdr=o("TFMPNetForQuestionAnswering"),mdr=o(" (MPNet model)"),gdr=l(),y7=a("li"),Ahe=a("strong"),hdr=o("rembert"),pdr=o(" \u2014 "),tG=a("a"),_dr=o("TFRemBertForQuestionAnswering"),udr=o(" (RemBERT model)"),bdr=l(),w7=a("li"),Lhe=a("strong"),vdr=o("roberta"),Tdr=o(" \u2014 "),aG=a("a"),Fdr=o("TFRobertaForQuestionAnswering"),Cdr=o(" (RoBERTa model)"),Mdr=l(),A7=a("li"),Bhe=a("strong"),Edr=o("roformer"),ydr=o(" \u2014 "),nG=a("a"),wdr=o("TFRoFormerForQuestionAnswering"),Adr=o(" (RoFormer model)"),Ldr=l(),L7=a("li"),xhe=a("strong"),Bdr=o("xlm"),xdr=o(" \u2014 "),sG=a("a"),kdr=o("TFXLMForQuestionAnsweringSimple"),Rdr=o(" (XLM model)"),Sdr=l(),B7=a("li"),khe=a("strong"),Pdr=o("xlm-roberta"),$dr=o(" \u2014 "),lG=a("a"),Idr=o("TFXLMRobertaForQuestionAnswering"),jdr=o(" (XLM-RoBERTa model)"),Ndr=l(),x7=a("li"),Rhe=a("strong"),Ddr=o("xlnet"),qdr=o(" \u2014 "),iG=a("a"),Gdr=o("TFXLNetForQuestionAnsweringSimple"),Odr=o(" (XLNet model)"),Xdr=l(),She=a("p"),zdr=o("Examples:"),Vdr=l(),f(uy.$$.fragment),G0e=l(),Mc=a("h2"),k7=a("a"),Phe=a("span"),f(by.$$.fragment),Wdr=l(),$he=a("span"),Qdr=o("TFAutoModelForVision2Seq"),O0e=l(),Cr=a("div"),f(vy.$$.fragment),Hdr=l(),Ec=a("p"),Udr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ihe=a("code"),Jdr=o("from_pretrained()"),Ydr=o("class method or the "),jhe=a("code"),Kdr=o("from_config()"),Zdr=o(`class
method.`),ecr=l(),Ty=a("p"),ocr=o("This class cannot be instantiated directly using "),Nhe=a("code"),rcr=o("__init__()"),tcr=o(" (throws an error)."),acr=l(),ht=a("div"),f(Fy.$$.fragment),ncr=l(),Dhe=a("p"),scr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),lcr=l(),yc=a("p"),icr=o(`Note:
Loading a model from its configuration file does `),qhe=a("strong"),dcr=o("not"),ccr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ghe=a("code"),fcr=o("from_pretrained()"),mcr=o("to load the model weights."),gcr=l(),Ohe=a("p"),hcr=o("Examples:"),pcr=l(),f(Cy.$$.fragment),_cr=l(),Co=a("div"),f(My.$$.fragment),ucr=l(),Xhe=a("p"),bcr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),vcr=l(),hn=a("p"),Tcr=o("The model class to instantiate is selected based on the "),zhe=a("code"),Fcr=o("model_type"),Ccr=o(` property of the config object (either
passed as an argument or loaded from `),Vhe=a("code"),Mcr=o("pretrained_model_name_or_path"),Ecr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Whe=a("code"),ycr=o("pretrained_model_name_or_path"),wcr=o(":"),Acr=l(),Qhe=a("ul"),R7=a("li"),Hhe=a("strong"),Lcr=o("vision-encoder-decoder"),Bcr=o(" \u2014 "),dG=a("a"),xcr=o("TFVisionEncoderDecoderModel"),kcr=o(" (Vision Encoder decoder model)"),Rcr=l(),Uhe=a("p"),Scr=o("Examples:"),Pcr=l(),f(Ey.$$.fragment),X0e=l(),wc=a("h2"),S7=a("a"),Jhe=a("span"),f(yy.$$.fragment),$cr=l(),Yhe=a("span"),Icr=o("TFAutoModelForSpeechSeq2Seq"),z0e=l(),Mr=a("div"),f(wy.$$.fragment),jcr=l(),Ac=a("p"),Ncr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Khe=a("code"),Dcr=o("from_pretrained()"),qcr=o("class method or the "),Zhe=a("code"),Gcr=o("from_config()"),Ocr=o(`class
method.`),Xcr=l(),Ay=a("p"),zcr=o("This class cannot be instantiated directly using "),epe=a("code"),Vcr=o("__init__()"),Wcr=o(" (throws an error)."),Qcr=l(),pt=a("div"),f(Ly.$$.fragment),Hcr=l(),ope=a("p"),Ucr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Jcr=l(),Lc=a("p"),Ycr=o(`Note:
Loading a model from its configuration file does `),rpe=a("strong"),Kcr=o("not"),Zcr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tpe=a("code"),efr=o("from_pretrained()"),ofr=o("to load the model weights."),rfr=l(),ape=a("p"),tfr=o("Examples:"),afr=l(),f(By.$$.fragment),nfr=l(),Mo=a("div"),f(xy.$$.fragment),sfr=l(),npe=a("p"),lfr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),ifr=l(),pn=a("p"),dfr=o("The model class to instantiate is selected based on the "),spe=a("code"),cfr=o("model_type"),ffr=o(` property of the config object (either
passed as an argument or loaded from `),lpe=a("code"),mfr=o("pretrained_model_name_or_path"),gfr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ipe=a("code"),hfr=o("pretrained_model_name_or_path"),pfr=o(":"),_fr=l(),dpe=a("ul"),P7=a("li"),cpe=a("strong"),ufr=o("speech_to_text"),bfr=o(" \u2014 "),cG=a("a"),vfr=o("TFSpeech2TextForConditionalGeneration"),Tfr=o(" (Speech2Text model)"),Ffr=l(),fpe=a("p"),Cfr=o("Examples:"),Mfr=l(),f(ky.$$.fragment),V0e=l(),Bc=a("h2"),$7=a("a"),mpe=a("span"),f(Ry.$$.fragment),Efr=l(),gpe=a("span"),yfr=o("FlaxAutoModel"),W0e=l(),Er=a("div"),f(Sy.$$.fragment),wfr=l(),xc=a("p"),Afr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),hpe=a("code"),Lfr=o("from_pretrained()"),Bfr=o("class method or the "),ppe=a("code"),xfr=o("from_config()"),kfr=o(`class
method.`),Rfr=l(),Py=a("p"),Sfr=o("This class cannot be instantiated directly using "),_pe=a("code"),Pfr=o("__init__()"),$fr=o(" (throws an error)."),Ifr=l(),_t=a("div"),f($y.$$.fragment),jfr=l(),upe=a("p"),Nfr=o("Instantiates one of the base model classes of the library from a configuration."),Dfr=l(),kc=a("p"),qfr=o(`Note:
Loading a model from its configuration file does `),bpe=a("strong"),Gfr=o("not"),Ofr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vpe=a("code"),Xfr=o("from_pretrained()"),zfr=o("to load the model weights."),Vfr=l(),Tpe=a("p"),Wfr=o("Examples:"),Qfr=l(),f(Iy.$$.fragment),Hfr=l(),Eo=a("div"),f(jy.$$.fragment),Ufr=l(),Fpe=a("p"),Jfr=o("Instantiate one of the base model classes of the library from a pretrained model."),Yfr=l(),_n=a("p"),Kfr=o("The model class to instantiate is selected based on the "),Cpe=a("code"),Zfr=o("model_type"),emr=o(` property of the config object (either
passed as an argument or loaded from `),Mpe=a("code"),omr=o("pretrained_model_name_or_path"),rmr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Epe=a("code"),tmr=o("pretrained_model_name_or_path"),amr=o(":"),nmr=l(),V=a("ul"),I7=a("li"),ype=a("strong"),smr=o("albert"),lmr=o(" \u2014 "),fG=a("a"),imr=o("FlaxAlbertModel"),dmr=o(" (ALBERT model)"),cmr=l(),j7=a("li"),wpe=a("strong"),fmr=o("bart"),mmr=o(" \u2014 "),mG=a("a"),gmr=o("FlaxBartModel"),hmr=o(" (BART model)"),pmr=l(),N7=a("li"),Ape=a("strong"),_mr=o("beit"),umr=o(" \u2014 "),gG=a("a"),bmr=o("FlaxBeitModel"),vmr=o(" (BEiT model)"),Tmr=l(),D7=a("li"),Lpe=a("strong"),Fmr=o("bert"),Cmr=o(" \u2014 "),hG=a("a"),Mmr=o("FlaxBertModel"),Emr=o(" (BERT model)"),ymr=l(),q7=a("li"),Bpe=a("strong"),wmr=o("big_bird"),Amr=o(" \u2014 "),pG=a("a"),Lmr=o("FlaxBigBirdModel"),Bmr=o(" (BigBird model)"),xmr=l(),G7=a("li"),xpe=a("strong"),kmr=o("blenderbot"),Rmr=o(" \u2014 "),_G=a("a"),Smr=o("FlaxBlenderbotModel"),Pmr=o(" (Blenderbot model)"),$mr=l(),O7=a("li"),kpe=a("strong"),Imr=o("blenderbot-small"),jmr=o(" \u2014 "),uG=a("a"),Nmr=o("FlaxBlenderbotSmallModel"),Dmr=o(" (BlenderbotSmall model)"),qmr=l(),X7=a("li"),Rpe=a("strong"),Gmr=o("clip"),Omr=o(" \u2014 "),bG=a("a"),Xmr=o("FlaxCLIPModel"),zmr=o(" (CLIP model)"),Vmr=l(),z7=a("li"),Spe=a("strong"),Wmr=o("distilbert"),Qmr=o(" \u2014 "),vG=a("a"),Hmr=o("FlaxDistilBertModel"),Umr=o(" (DistilBERT model)"),Jmr=l(),V7=a("li"),Ppe=a("strong"),Ymr=o("electra"),Kmr=o(" \u2014 "),TG=a("a"),Zmr=o("FlaxElectraModel"),egr=o(" (ELECTRA model)"),ogr=l(),W7=a("li"),$pe=a("strong"),rgr=o("gpt2"),tgr=o(" \u2014 "),FG=a("a"),agr=o("FlaxGPT2Model"),ngr=o(" (OpenAI GPT-2 model)"),sgr=l(),Q7=a("li"),Ipe=a("strong"),lgr=o("gpt_neo"),igr=o(" \u2014 "),CG=a("a"),dgr=o("FlaxGPTNeoModel"),cgr=o(" (GPT Neo model)"),fgr=l(),H7=a("li"),jpe=a("strong"),mgr=o("gptj"),ggr=o(" \u2014 "),MG=a("a"),hgr=o("FlaxGPTJModel"),pgr=o(" (GPT-J model)"),_gr=l(),U7=a("li"),Npe=a("strong"),ugr=o("marian"),bgr=o(" \u2014 "),EG=a("a"),vgr=o("FlaxMarianModel"),Tgr=o(" (Marian model)"),Fgr=l(),J7=a("li"),Dpe=a("strong"),Cgr=o("mbart"),Mgr=o(" \u2014 "),yG=a("a"),Egr=o("FlaxMBartModel"),ygr=o(" (mBART model)"),wgr=l(),Y7=a("li"),qpe=a("strong"),Agr=o("mt5"),Lgr=o(" \u2014 "),wG=a("a"),Bgr=o("FlaxMT5Model"),xgr=o(" (mT5 model)"),kgr=l(),K7=a("li"),Gpe=a("strong"),Rgr=o("pegasus"),Sgr=o(" \u2014 "),AG=a("a"),Pgr=o("FlaxPegasusModel"),$gr=o(" (Pegasus model)"),Igr=l(),Z7=a("li"),Ope=a("strong"),jgr=o("roberta"),Ngr=o(" \u2014 "),LG=a("a"),Dgr=o("FlaxRobertaModel"),qgr=o(" (RoBERTa model)"),Ggr=l(),e8=a("li"),Xpe=a("strong"),Ogr=o("roformer"),Xgr=o(" \u2014 "),BG=a("a"),zgr=o("FlaxRoFormerModel"),Vgr=o(" (RoFormer model)"),Wgr=l(),o8=a("li"),zpe=a("strong"),Qgr=o("t5"),Hgr=o(" \u2014 "),xG=a("a"),Ugr=o("FlaxT5Model"),Jgr=o(" (T5 model)"),Ygr=l(),r8=a("li"),Vpe=a("strong"),Kgr=o("vision-text-dual-encoder"),Zgr=o(" \u2014 "),kG=a("a"),ehr=o("FlaxVisionTextDualEncoderModel"),ohr=o(" (VisionTextDualEncoder model)"),rhr=l(),t8=a("li"),Wpe=a("strong"),thr=o("vit"),ahr=o(" \u2014 "),RG=a("a"),nhr=o("FlaxViTModel"),shr=o(" (ViT model)"),lhr=l(),a8=a("li"),Qpe=a("strong"),ihr=o("wav2vec2"),dhr=o(" \u2014 "),SG=a("a"),chr=o("FlaxWav2Vec2Model"),fhr=o(" (Wav2Vec2 model)"),mhr=l(),n8=a("li"),Hpe=a("strong"),ghr=o("xglm"),hhr=o(" \u2014 "),PG=a("a"),phr=o("FlaxXGLMModel"),_hr=o(" (XGLM model)"),uhr=l(),Upe=a("p"),bhr=o("Examples:"),vhr=l(),f(Ny.$$.fragment),Q0e=l(),Rc=a("h2"),s8=a("a"),Jpe=a("span"),f(Dy.$$.fragment),Thr=l(),Ype=a("span"),Fhr=o("FlaxAutoModelForCausalLM"),H0e=l(),yr=a("div"),f(qy.$$.fragment),Chr=l(),Sc=a("p"),Mhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Kpe=a("code"),Ehr=o("from_pretrained()"),yhr=o("class method or the "),Zpe=a("code"),whr=o("from_config()"),Ahr=o(`class
method.`),Lhr=l(),Gy=a("p"),Bhr=o("This class cannot be instantiated directly using "),e_e=a("code"),xhr=o("__init__()"),khr=o(" (throws an error)."),Rhr=l(),ut=a("div"),f(Oy.$$.fragment),Shr=l(),o_e=a("p"),Phr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),$hr=l(),Pc=a("p"),Ihr=o(`Note:
Loading a model from its configuration file does `),r_e=a("strong"),jhr=o("not"),Nhr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),t_e=a("code"),Dhr=o("from_pretrained()"),qhr=o("to load the model weights."),Ghr=l(),a_e=a("p"),Ohr=o("Examples:"),Xhr=l(),f(Xy.$$.fragment),zhr=l(),yo=a("div"),f(zy.$$.fragment),Vhr=l(),n_e=a("p"),Whr=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Qhr=l(),un=a("p"),Hhr=o("The model class to instantiate is selected based on the "),s_e=a("code"),Uhr=o("model_type"),Jhr=o(` property of the config object (either
passed as an argument or loaded from `),l_e=a("code"),Yhr=o("pretrained_model_name_or_path"),Khr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),i_e=a("code"),Zhr=o("pretrained_model_name_or_path"),epr=o(":"),opr=l(),bn=a("ul"),l8=a("li"),d_e=a("strong"),rpr=o("gpt2"),tpr=o(" \u2014 "),$G=a("a"),apr=o("FlaxGPT2LMHeadModel"),npr=o(" (OpenAI GPT-2 model)"),spr=l(),i8=a("li"),c_e=a("strong"),lpr=o("gpt_neo"),ipr=o(" \u2014 "),IG=a("a"),dpr=o("FlaxGPTNeoForCausalLM"),cpr=o(" (GPT Neo model)"),fpr=l(),d8=a("li"),f_e=a("strong"),mpr=o("gptj"),gpr=o(" \u2014 "),jG=a("a"),hpr=o("FlaxGPTJForCausalLM"),ppr=o(" (GPT-J model)"),_pr=l(),c8=a("li"),m_e=a("strong"),upr=o("xglm"),bpr=o(" \u2014 "),NG=a("a"),vpr=o("FlaxXGLMForCausalLM"),Tpr=o(" (XGLM model)"),Fpr=l(),g_e=a("p"),Cpr=o("Examples:"),Mpr=l(),f(Vy.$$.fragment),U0e=l(),$c=a("h2"),f8=a("a"),h_e=a("span"),f(Wy.$$.fragment),Epr=l(),p_e=a("span"),ypr=o("FlaxAutoModelForPreTraining"),J0e=l(),wr=a("div"),f(Qy.$$.fragment),wpr=l(),Ic=a("p"),Apr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),__e=a("code"),Lpr=o("from_pretrained()"),Bpr=o("class method or the "),u_e=a("code"),xpr=o("from_config()"),kpr=o(`class
method.`),Rpr=l(),Hy=a("p"),Spr=o("This class cannot be instantiated directly using "),b_e=a("code"),Ppr=o("__init__()"),$pr=o(" (throws an error)."),Ipr=l(),bt=a("div"),f(Uy.$$.fragment),jpr=l(),v_e=a("p"),Npr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Dpr=l(),jc=a("p"),qpr=o(`Note:
Loading a model from its configuration file does `),T_e=a("strong"),Gpr=o("not"),Opr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F_e=a("code"),Xpr=o("from_pretrained()"),zpr=o("to load the model weights."),Vpr=l(),C_e=a("p"),Wpr=o("Examples:"),Qpr=l(),f(Jy.$$.fragment),Hpr=l(),wo=a("div"),f(Yy.$$.fragment),Upr=l(),M_e=a("p"),Jpr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ypr=l(),vn=a("p"),Kpr=o("The model class to instantiate is selected based on the "),E_e=a("code"),Zpr=o("model_type"),e_r=o(` property of the config object (either
passed as an argument or loaded from `),y_e=a("code"),o_r=o("pretrained_model_name_or_path"),r_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w_e=a("code"),t_r=o("pretrained_model_name_or_path"),a_r=o(":"),n_r=l(),fe=a("ul"),m8=a("li"),A_e=a("strong"),s_r=o("albert"),l_r=o(" \u2014 "),DG=a("a"),i_r=o("FlaxAlbertForPreTraining"),d_r=o(" (ALBERT model)"),c_r=l(),g8=a("li"),L_e=a("strong"),f_r=o("bart"),m_r=o(" \u2014 "),qG=a("a"),g_r=o("FlaxBartForConditionalGeneration"),h_r=o(" (BART model)"),p_r=l(),h8=a("li"),B_e=a("strong"),__r=o("bert"),u_r=o(" \u2014 "),GG=a("a"),b_r=o("FlaxBertForPreTraining"),v_r=o(" (BERT model)"),T_r=l(),p8=a("li"),x_e=a("strong"),F_r=o("big_bird"),C_r=o(" \u2014 "),OG=a("a"),M_r=o("FlaxBigBirdForPreTraining"),E_r=o(" (BigBird model)"),y_r=l(),_8=a("li"),k_e=a("strong"),w_r=o("electra"),A_r=o(" \u2014 "),XG=a("a"),L_r=o("FlaxElectraForPreTraining"),B_r=o(" (ELECTRA model)"),x_r=l(),u8=a("li"),R_e=a("strong"),k_r=o("mbart"),R_r=o(" \u2014 "),zG=a("a"),S_r=o("FlaxMBartForConditionalGeneration"),P_r=o(" (mBART model)"),$_r=l(),b8=a("li"),S_e=a("strong"),I_r=o("mt5"),j_r=o(" \u2014 "),VG=a("a"),N_r=o("FlaxMT5ForConditionalGeneration"),D_r=o(" (mT5 model)"),q_r=l(),v8=a("li"),P_e=a("strong"),G_r=o("roberta"),O_r=o(" \u2014 "),WG=a("a"),X_r=o("FlaxRobertaForMaskedLM"),z_r=o(" (RoBERTa model)"),V_r=l(),T8=a("li"),$_e=a("strong"),W_r=o("roformer"),Q_r=o(" \u2014 "),QG=a("a"),H_r=o("FlaxRoFormerForMaskedLM"),U_r=o(" (RoFormer model)"),J_r=l(),F8=a("li"),I_e=a("strong"),Y_r=o("t5"),K_r=o(" \u2014 "),HG=a("a"),Z_r=o("FlaxT5ForConditionalGeneration"),eur=o(" (T5 model)"),our=l(),C8=a("li"),j_e=a("strong"),rur=o("wav2vec2"),tur=o(" \u2014 "),UG=a("a"),aur=o("FlaxWav2Vec2ForPreTraining"),nur=o(" (Wav2Vec2 model)"),sur=l(),N_e=a("p"),lur=o("Examples:"),iur=l(),f(Ky.$$.fragment),Y0e=l(),Nc=a("h2"),M8=a("a"),D_e=a("span"),f(Zy.$$.fragment),dur=l(),q_e=a("span"),cur=o("FlaxAutoModelForMaskedLM"),K0e=l(),Ar=a("div"),f(ew.$$.fragment),fur=l(),Dc=a("p"),mur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),G_e=a("code"),gur=o("from_pretrained()"),hur=o("class method or the "),O_e=a("code"),pur=o("from_config()"),_ur=o(`class
method.`),uur=l(),ow=a("p"),bur=o("This class cannot be instantiated directly using "),X_e=a("code"),vur=o("__init__()"),Tur=o(" (throws an error)."),Fur=l(),vt=a("div"),f(rw.$$.fragment),Cur=l(),z_e=a("p"),Mur=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Eur=l(),qc=a("p"),yur=o(`Note:
Loading a model from its configuration file does `),V_e=a("strong"),wur=o("not"),Aur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),W_e=a("code"),Lur=o("from_pretrained()"),Bur=o("to load the model weights."),xur=l(),Q_e=a("p"),kur=o("Examples:"),Rur=l(),f(tw.$$.fragment),Sur=l(),Ao=a("div"),f(aw.$$.fragment),Pur=l(),H_e=a("p"),$ur=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Iur=l(),Tn=a("p"),jur=o("The model class to instantiate is selected based on the "),U_e=a("code"),Nur=o("model_type"),Dur=o(` property of the config object (either
passed as an argument or loaded from `),J_e=a("code"),qur=o("pretrained_model_name_or_path"),Gur=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Y_e=a("code"),Our=o("pretrained_model_name_or_path"),Xur=o(":"),zur=l(),be=a("ul"),E8=a("li"),K_e=a("strong"),Vur=o("albert"),Wur=o(" \u2014 "),JG=a("a"),Qur=o("FlaxAlbertForMaskedLM"),Hur=o(" (ALBERT model)"),Uur=l(),y8=a("li"),Z_e=a("strong"),Jur=o("bart"),Yur=o(" \u2014 "),YG=a("a"),Kur=o("FlaxBartForConditionalGeneration"),Zur=o(" (BART model)"),e1r=l(),w8=a("li"),eue=a("strong"),o1r=o("bert"),r1r=o(" \u2014 "),KG=a("a"),t1r=o("FlaxBertForMaskedLM"),a1r=o(" (BERT model)"),n1r=l(),A8=a("li"),oue=a("strong"),s1r=o("big_bird"),l1r=o(" \u2014 "),ZG=a("a"),i1r=o("FlaxBigBirdForMaskedLM"),d1r=o(" (BigBird model)"),c1r=l(),L8=a("li"),rue=a("strong"),f1r=o("distilbert"),m1r=o(" \u2014 "),eO=a("a"),g1r=o("FlaxDistilBertForMaskedLM"),h1r=o(" (DistilBERT model)"),p1r=l(),B8=a("li"),tue=a("strong"),_1r=o("electra"),u1r=o(" \u2014 "),oO=a("a"),b1r=o("FlaxElectraForMaskedLM"),v1r=o(" (ELECTRA model)"),T1r=l(),x8=a("li"),aue=a("strong"),F1r=o("mbart"),C1r=o(" \u2014 "),rO=a("a"),M1r=o("FlaxMBartForConditionalGeneration"),E1r=o(" (mBART model)"),y1r=l(),k8=a("li"),nue=a("strong"),w1r=o("roberta"),A1r=o(" \u2014 "),tO=a("a"),L1r=o("FlaxRobertaForMaskedLM"),B1r=o(" (RoBERTa model)"),x1r=l(),R8=a("li"),sue=a("strong"),k1r=o("roformer"),R1r=o(" \u2014 "),aO=a("a"),S1r=o("FlaxRoFormerForMaskedLM"),P1r=o(" (RoFormer model)"),$1r=l(),lue=a("p"),I1r=o("Examples:"),j1r=l(),f(nw.$$.fragment),Z0e=l(),Gc=a("h2"),S8=a("a"),iue=a("span"),f(sw.$$.fragment),N1r=l(),due=a("span"),D1r=o("FlaxAutoModelForSeq2SeqLM"),eLe=l(),Lr=a("div"),f(lw.$$.fragment),q1r=l(),Oc=a("p"),G1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),cue=a("code"),O1r=o("from_pretrained()"),X1r=o("class method or the "),fue=a("code"),z1r=o("from_config()"),V1r=o(`class
method.`),W1r=l(),iw=a("p"),Q1r=o("This class cannot be instantiated directly using "),mue=a("code"),H1r=o("__init__()"),U1r=o(" (throws an error)."),J1r=l(),Tt=a("div"),f(dw.$$.fragment),Y1r=l(),gue=a("p"),K1r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Z1r=l(),Xc=a("p"),ebr=o(`Note:
Loading a model from its configuration file does `),hue=a("strong"),obr=o("not"),rbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pue=a("code"),tbr=o("from_pretrained()"),abr=o("to load the model weights."),nbr=l(),_ue=a("p"),sbr=o("Examples:"),lbr=l(),f(cw.$$.fragment),ibr=l(),Lo=a("div"),f(fw.$$.fragment),dbr=l(),uue=a("p"),cbr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),fbr=l(),Fn=a("p"),mbr=o("The model class to instantiate is selected based on the "),bue=a("code"),gbr=o("model_type"),hbr=o(` property of the config object (either
passed as an argument or loaded from `),vue=a("code"),pbr=o("pretrained_model_name_or_path"),_br=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tue=a("code"),ubr=o("pretrained_model_name_or_path"),bbr=o(":"),vbr=l(),ve=a("ul"),P8=a("li"),Fue=a("strong"),Tbr=o("bart"),Fbr=o(" \u2014 "),nO=a("a"),Cbr=o("FlaxBartForConditionalGeneration"),Mbr=o(" (BART model)"),Ebr=l(),$8=a("li"),Cue=a("strong"),ybr=o("blenderbot"),wbr=o(" \u2014 "),sO=a("a"),Abr=o("FlaxBlenderbotForConditionalGeneration"),Lbr=o(" (Blenderbot model)"),Bbr=l(),I8=a("li"),Mue=a("strong"),xbr=o("blenderbot-small"),kbr=o(" \u2014 "),lO=a("a"),Rbr=o("FlaxBlenderbotSmallForConditionalGeneration"),Sbr=o(" (BlenderbotSmall model)"),Pbr=l(),j8=a("li"),Eue=a("strong"),$br=o("encoder-decoder"),Ibr=o(" \u2014 "),iO=a("a"),jbr=o("FlaxEncoderDecoderModel"),Nbr=o(" (Encoder decoder model)"),Dbr=l(),N8=a("li"),yue=a("strong"),qbr=o("marian"),Gbr=o(" \u2014 "),dO=a("a"),Obr=o("FlaxMarianMTModel"),Xbr=o(" (Marian model)"),zbr=l(),D8=a("li"),wue=a("strong"),Vbr=o("mbart"),Wbr=o(" \u2014 "),cO=a("a"),Qbr=o("FlaxMBartForConditionalGeneration"),Hbr=o(" (mBART model)"),Ubr=l(),q8=a("li"),Aue=a("strong"),Jbr=o("mt5"),Ybr=o(" \u2014 "),fO=a("a"),Kbr=o("FlaxMT5ForConditionalGeneration"),Zbr=o(" (mT5 model)"),e5r=l(),G8=a("li"),Lue=a("strong"),o5r=o("pegasus"),r5r=o(" \u2014 "),mO=a("a"),t5r=o("FlaxPegasusForConditionalGeneration"),a5r=o(" (Pegasus model)"),n5r=l(),O8=a("li"),Bue=a("strong"),s5r=o("t5"),l5r=o(" \u2014 "),gO=a("a"),i5r=o("FlaxT5ForConditionalGeneration"),d5r=o(" (T5 model)"),c5r=l(),xue=a("p"),f5r=o("Examples:"),m5r=l(),f(mw.$$.fragment),oLe=l(),zc=a("h2"),X8=a("a"),kue=a("span"),f(gw.$$.fragment),g5r=l(),Rue=a("span"),h5r=o("FlaxAutoModelForSequenceClassification"),rLe=l(),Br=a("div"),f(hw.$$.fragment),p5r=l(),Vc=a("p"),_5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Sue=a("code"),u5r=o("from_pretrained()"),b5r=o("class method or the "),Pue=a("code"),v5r=o("from_config()"),T5r=o(`class
method.`),F5r=l(),pw=a("p"),C5r=o("This class cannot be instantiated directly using "),$ue=a("code"),M5r=o("__init__()"),E5r=o(" (throws an error)."),y5r=l(),Ft=a("div"),f(_w.$$.fragment),w5r=l(),Iue=a("p"),A5r=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),L5r=l(),Wc=a("p"),B5r=o(`Note:
Loading a model from its configuration file does `),jue=a("strong"),x5r=o("not"),k5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Nue=a("code"),R5r=o("from_pretrained()"),S5r=o("to load the model weights."),P5r=l(),Due=a("p"),$5r=o("Examples:"),I5r=l(),f(uw.$$.fragment),j5r=l(),Bo=a("div"),f(bw.$$.fragment),N5r=l(),que=a("p"),D5r=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),q5r=l(),Cn=a("p"),G5r=o("The model class to instantiate is selected based on the "),Gue=a("code"),O5r=o("model_type"),X5r=o(` property of the config object (either
passed as an argument or loaded from `),Oue=a("code"),z5r=o("pretrained_model_name_or_path"),V5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xue=a("code"),W5r=o("pretrained_model_name_or_path"),Q5r=o(":"),H5r=l(),Te=a("ul"),z8=a("li"),zue=a("strong"),U5r=o("albert"),J5r=o(" \u2014 "),hO=a("a"),Y5r=o("FlaxAlbertForSequenceClassification"),K5r=o(" (ALBERT model)"),Z5r=l(),V8=a("li"),Vue=a("strong"),e2r=o("bart"),o2r=o(" \u2014 "),pO=a("a"),r2r=o("FlaxBartForSequenceClassification"),t2r=o(" (BART model)"),a2r=l(),W8=a("li"),Wue=a("strong"),n2r=o("bert"),s2r=o(" \u2014 "),_O=a("a"),l2r=o("FlaxBertForSequenceClassification"),i2r=o(" (BERT model)"),d2r=l(),Q8=a("li"),Que=a("strong"),c2r=o("big_bird"),f2r=o(" \u2014 "),uO=a("a"),m2r=o("FlaxBigBirdForSequenceClassification"),g2r=o(" (BigBird model)"),h2r=l(),H8=a("li"),Hue=a("strong"),p2r=o("distilbert"),_2r=o(" \u2014 "),bO=a("a"),u2r=o("FlaxDistilBertForSequenceClassification"),b2r=o(" (DistilBERT model)"),v2r=l(),U8=a("li"),Uue=a("strong"),T2r=o("electra"),F2r=o(" \u2014 "),vO=a("a"),C2r=o("FlaxElectraForSequenceClassification"),M2r=o(" (ELECTRA model)"),E2r=l(),J8=a("li"),Jue=a("strong"),y2r=o("mbart"),w2r=o(" \u2014 "),TO=a("a"),A2r=o("FlaxMBartForSequenceClassification"),L2r=o(" (mBART model)"),B2r=l(),Y8=a("li"),Yue=a("strong"),x2r=o("roberta"),k2r=o(" \u2014 "),FO=a("a"),R2r=o("FlaxRobertaForSequenceClassification"),S2r=o(" (RoBERTa model)"),P2r=l(),K8=a("li"),Kue=a("strong"),$2r=o("roformer"),I2r=o(" \u2014 "),CO=a("a"),j2r=o("FlaxRoFormerForSequenceClassification"),N2r=o(" (RoFormer model)"),D2r=l(),Zue=a("p"),q2r=o("Examples:"),G2r=l(),f(vw.$$.fragment),tLe=l(),Qc=a("h2"),Z8=a("a"),e1e=a("span"),f(Tw.$$.fragment),O2r=l(),o1e=a("span"),X2r=o("FlaxAutoModelForQuestionAnswering"),aLe=l(),xr=a("div"),f(Fw.$$.fragment),z2r=l(),Hc=a("p"),V2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),r1e=a("code"),W2r=o("from_pretrained()"),Q2r=o("class method or the "),t1e=a("code"),H2r=o("from_config()"),U2r=o(`class
method.`),J2r=l(),Cw=a("p"),Y2r=o("This class cannot be instantiated directly using "),a1e=a("code"),K2r=o("__init__()"),Z2r=o(" (throws an error)."),evr=l(),Ct=a("div"),f(Mw.$$.fragment),ovr=l(),n1e=a("p"),rvr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),tvr=l(),Uc=a("p"),avr=o(`Note:
Loading a model from its configuration file does `),s1e=a("strong"),nvr=o("not"),svr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),l1e=a("code"),lvr=o("from_pretrained()"),ivr=o("to load the model weights."),dvr=l(),i1e=a("p"),cvr=o("Examples:"),fvr=l(),f(Ew.$$.fragment),mvr=l(),xo=a("div"),f(yw.$$.fragment),gvr=l(),d1e=a("p"),hvr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),pvr=l(),Mn=a("p"),_vr=o("The model class to instantiate is selected based on the "),c1e=a("code"),uvr=o("model_type"),bvr=o(` property of the config object (either
passed as an argument or loaded from `),f1e=a("code"),vvr=o("pretrained_model_name_or_path"),Tvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),m1e=a("code"),Fvr=o("pretrained_model_name_or_path"),Cvr=o(":"),Mvr=l(),Fe=a("ul"),eF=a("li"),g1e=a("strong"),Evr=o("albert"),yvr=o(" \u2014 "),MO=a("a"),wvr=o("FlaxAlbertForQuestionAnswering"),Avr=o(" (ALBERT model)"),Lvr=l(),oF=a("li"),h1e=a("strong"),Bvr=o("bart"),xvr=o(" \u2014 "),EO=a("a"),kvr=o("FlaxBartForQuestionAnswering"),Rvr=o(" (BART model)"),Svr=l(),rF=a("li"),p1e=a("strong"),Pvr=o("bert"),$vr=o(" \u2014 "),yO=a("a"),Ivr=o("FlaxBertForQuestionAnswering"),jvr=o(" (BERT model)"),Nvr=l(),tF=a("li"),_1e=a("strong"),Dvr=o("big_bird"),qvr=o(" \u2014 "),wO=a("a"),Gvr=o("FlaxBigBirdForQuestionAnswering"),Ovr=o(" (BigBird model)"),Xvr=l(),aF=a("li"),u1e=a("strong"),zvr=o("distilbert"),Vvr=o(" \u2014 "),AO=a("a"),Wvr=o("FlaxDistilBertForQuestionAnswering"),Qvr=o(" (DistilBERT model)"),Hvr=l(),nF=a("li"),b1e=a("strong"),Uvr=o("electra"),Jvr=o(" \u2014 "),LO=a("a"),Yvr=o("FlaxElectraForQuestionAnswering"),Kvr=o(" (ELECTRA model)"),Zvr=l(),sF=a("li"),v1e=a("strong"),e6r=o("mbart"),o6r=o(" \u2014 "),BO=a("a"),r6r=o("FlaxMBartForQuestionAnswering"),t6r=o(" (mBART model)"),a6r=l(),lF=a("li"),T1e=a("strong"),n6r=o("roberta"),s6r=o(" \u2014 "),xO=a("a"),l6r=o("FlaxRobertaForQuestionAnswering"),i6r=o(" (RoBERTa model)"),d6r=l(),iF=a("li"),F1e=a("strong"),c6r=o("roformer"),f6r=o(" \u2014 "),kO=a("a"),m6r=o("FlaxRoFormerForQuestionAnswering"),g6r=o(" (RoFormer model)"),h6r=l(),C1e=a("p"),p6r=o("Examples:"),_6r=l(),f(ww.$$.fragment),nLe=l(),Jc=a("h2"),dF=a("a"),M1e=a("span"),f(Aw.$$.fragment),u6r=l(),E1e=a("span"),b6r=o("FlaxAutoModelForTokenClassification"),sLe=l(),kr=a("div"),f(Lw.$$.fragment),v6r=l(),Yc=a("p"),T6r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),y1e=a("code"),F6r=o("from_pretrained()"),C6r=o("class method or the "),w1e=a("code"),M6r=o("from_config()"),E6r=o(`class
method.`),y6r=l(),Bw=a("p"),w6r=o("This class cannot be instantiated directly using "),A1e=a("code"),A6r=o("__init__()"),L6r=o(" (throws an error)."),B6r=l(),Mt=a("div"),f(xw.$$.fragment),x6r=l(),L1e=a("p"),k6r=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),R6r=l(),Kc=a("p"),S6r=o(`Note:
Loading a model from its configuration file does `),B1e=a("strong"),P6r=o("not"),$6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),x1e=a("code"),I6r=o("from_pretrained()"),j6r=o("to load the model weights."),N6r=l(),k1e=a("p"),D6r=o("Examples:"),q6r=l(),f(kw.$$.fragment),G6r=l(),ko=a("div"),f(Rw.$$.fragment),O6r=l(),R1e=a("p"),X6r=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),z6r=l(),En=a("p"),V6r=o("The model class to instantiate is selected based on the "),S1e=a("code"),W6r=o("model_type"),Q6r=o(` property of the config object (either
passed as an argument or loaded from `),P1e=a("code"),H6r=o("pretrained_model_name_or_path"),U6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$1e=a("code"),J6r=o("pretrained_model_name_or_path"),Y6r=o(":"),K6r=l(),ao=a("ul"),cF=a("li"),I1e=a("strong"),Z6r=o("albert"),eTr=o(" \u2014 "),RO=a("a"),oTr=o("FlaxAlbertForTokenClassification"),rTr=o(" (ALBERT model)"),tTr=l(),fF=a("li"),j1e=a("strong"),aTr=o("bert"),nTr=o(" \u2014 "),SO=a("a"),sTr=o("FlaxBertForTokenClassification"),lTr=o(" (BERT model)"),iTr=l(),mF=a("li"),N1e=a("strong"),dTr=o("big_bird"),cTr=o(" \u2014 "),PO=a("a"),fTr=o("FlaxBigBirdForTokenClassification"),mTr=o(" (BigBird model)"),gTr=l(),gF=a("li"),D1e=a("strong"),hTr=o("distilbert"),pTr=o(" \u2014 "),$O=a("a"),_Tr=o("FlaxDistilBertForTokenClassification"),uTr=o(" (DistilBERT model)"),bTr=l(),hF=a("li"),q1e=a("strong"),vTr=o("electra"),TTr=o(" \u2014 "),IO=a("a"),FTr=o("FlaxElectraForTokenClassification"),CTr=o(" (ELECTRA model)"),MTr=l(),pF=a("li"),G1e=a("strong"),ETr=o("roberta"),yTr=o(" \u2014 "),jO=a("a"),wTr=o("FlaxRobertaForTokenClassification"),ATr=o(" (RoBERTa model)"),LTr=l(),_F=a("li"),O1e=a("strong"),BTr=o("roformer"),xTr=o(" \u2014 "),NO=a("a"),kTr=o("FlaxRoFormerForTokenClassification"),RTr=o(" (RoFormer model)"),STr=l(),X1e=a("p"),PTr=o("Examples:"),$Tr=l(),f(Sw.$$.fragment),lLe=l(),Zc=a("h2"),uF=a("a"),z1e=a("span"),f(Pw.$$.fragment),ITr=l(),V1e=a("span"),jTr=o("FlaxAutoModelForMultipleChoice"),iLe=l(),Rr=a("div"),f($w.$$.fragment),NTr=l(),ef=a("p"),DTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),W1e=a("code"),qTr=o("from_pretrained()"),GTr=o("class method or the "),Q1e=a("code"),OTr=o("from_config()"),XTr=o(`class
method.`),zTr=l(),Iw=a("p"),VTr=o("This class cannot be instantiated directly using "),H1e=a("code"),WTr=o("__init__()"),QTr=o(" (throws an error)."),HTr=l(),Et=a("div"),f(jw.$$.fragment),UTr=l(),U1e=a("p"),JTr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),YTr=l(),of=a("p"),KTr=o(`Note:
Loading a model from its configuration file does `),J1e=a("strong"),ZTr=o("not"),e7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Y1e=a("code"),o7r=o("from_pretrained()"),r7r=o("to load the model weights."),t7r=l(),K1e=a("p"),a7r=o("Examples:"),n7r=l(),f(Nw.$$.fragment),s7r=l(),Ro=a("div"),f(Dw.$$.fragment),l7r=l(),Z1e=a("p"),i7r=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),d7r=l(),yn=a("p"),c7r=o("The model class to instantiate is selected based on the "),ebe=a("code"),f7r=o("model_type"),m7r=o(` property of the config object (either
passed as an argument or loaded from `),obe=a("code"),g7r=o("pretrained_model_name_or_path"),h7r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rbe=a("code"),p7r=o("pretrained_model_name_or_path"),_7r=o(":"),u7r=l(),no=a("ul"),bF=a("li"),tbe=a("strong"),b7r=o("albert"),v7r=o(" \u2014 "),DO=a("a"),T7r=o("FlaxAlbertForMultipleChoice"),F7r=o(" (ALBERT model)"),C7r=l(),vF=a("li"),abe=a("strong"),M7r=o("bert"),E7r=o(" \u2014 "),qO=a("a"),y7r=o("FlaxBertForMultipleChoice"),w7r=o(" (BERT model)"),A7r=l(),TF=a("li"),nbe=a("strong"),L7r=o("big_bird"),B7r=o(" \u2014 "),GO=a("a"),x7r=o("FlaxBigBirdForMultipleChoice"),k7r=o(" (BigBird model)"),R7r=l(),FF=a("li"),sbe=a("strong"),S7r=o("distilbert"),P7r=o(" \u2014 "),OO=a("a"),$7r=o("FlaxDistilBertForMultipleChoice"),I7r=o(" (DistilBERT model)"),j7r=l(),CF=a("li"),lbe=a("strong"),N7r=o("electra"),D7r=o(" \u2014 "),XO=a("a"),q7r=o("FlaxElectraForMultipleChoice"),G7r=o(" (ELECTRA model)"),O7r=l(),MF=a("li"),ibe=a("strong"),X7r=o("roberta"),z7r=o(" \u2014 "),zO=a("a"),V7r=o("FlaxRobertaForMultipleChoice"),W7r=o(" (RoBERTa model)"),Q7r=l(),EF=a("li"),dbe=a("strong"),H7r=o("roformer"),U7r=o(" \u2014 "),VO=a("a"),J7r=o("FlaxRoFormerForMultipleChoice"),Y7r=o(" (RoFormer model)"),K7r=l(),cbe=a("p"),Z7r=o("Examples:"),e8r=l(),f(qw.$$.fragment),dLe=l(),rf=a("h2"),yF=a("a"),fbe=a("span"),f(Gw.$$.fragment),o8r=l(),mbe=a("span"),r8r=o("FlaxAutoModelForNextSentencePrediction"),cLe=l(),Sr=a("div"),f(Ow.$$.fragment),t8r=l(),tf=a("p"),a8r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),gbe=a("code"),n8r=o("from_pretrained()"),s8r=o("class method or the "),hbe=a("code"),l8r=o("from_config()"),i8r=o(`class
method.`),d8r=l(),Xw=a("p"),c8r=o("This class cannot be instantiated directly using "),pbe=a("code"),f8r=o("__init__()"),m8r=o(" (throws an error)."),g8r=l(),yt=a("div"),f(zw.$$.fragment),h8r=l(),_be=a("p"),p8r=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),_8r=l(),af=a("p"),u8r=o(`Note:
Loading a model from its configuration file does `),ube=a("strong"),b8r=o("not"),v8r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bbe=a("code"),T8r=o("from_pretrained()"),F8r=o("to load the model weights."),C8r=l(),vbe=a("p"),M8r=o("Examples:"),E8r=l(),f(Vw.$$.fragment),y8r=l(),So=a("div"),f(Ww.$$.fragment),w8r=l(),Tbe=a("p"),A8r=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),L8r=l(),wn=a("p"),B8r=o("The model class to instantiate is selected based on the "),Fbe=a("code"),x8r=o("model_type"),k8r=o(` property of the config object (either
passed as an argument or loaded from `),Cbe=a("code"),R8r=o("pretrained_model_name_or_path"),S8r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mbe=a("code"),P8r=o("pretrained_model_name_or_path"),$8r=o(":"),I8r=l(),Ebe=a("ul"),wF=a("li"),ybe=a("strong"),j8r=o("bert"),N8r=o(" \u2014 "),WO=a("a"),D8r=o("FlaxBertForNextSentencePrediction"),q8r=o(" (BERT model)"),G8r=l(),wbe=a("p"),O8r=o("Examples:"),X8r=l(),f(Qw.$$.fragment),fLe=l(),nf=a("h2"),AF=a("a"),Abe=a("span"),f(Hw.$$.fragment),z8r=l(),Lbe=a("span"),V8r=o("FlaxAutoModelForImageClassification"),mLe=l(),Pr=a("div"),f(Uw.$$.fragment),W8r=l(),sf=a("p"),Q8r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Bbe=a("code"),H8r=o("from_pretrained()"),U8r=o("class method or the "),xbe=a("code"),J8r=o("from_config()"),Y8r=o(`class
method.`),K8r=l(),Jw=a("p"),Z8r=o("This class cannot be instantiated directly using "),kbe=a("code"),eFr=o("__init__()"),oFr=o(" (throws an error)."),rFr=l(),wt=a("div"),f(Yw.$$.fragment),tFr=l(),Rbe=a("p"),aFr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),nFr=l(),lf=a("p"),sFr=o(`Note:
Loading a model from its configuration file does `),Sbe=a("strong"),lFr=o("not"),iFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=a("code"),dFr=o("from_pretrained()"),cFr=o("to load the model weights."),fFr=l(),$be=a("p"),mFr=o("Examples:"),gFr=l(),f(Kw.$$.fragment),hFr=l(),Po=a("div"),f(Zw.$$.fragment),pFr=l(),Ibe=a("p"),_Fr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),uFr=l(),An=a("p"),bFr=o("The model class to instantiate is selected based on the "),jbe=a("code"),vFr=o("model_type"),TFr=o(` property of the config object (either
passed as an argument or loaded from `),Nbe=a("code"),FFr=o("pretrained_model_name_or_path"),CFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dbe=a("code"),MFr=o("pretrained_model_name_or_path"),EFr=o(":"),yFr=l(),eA=a("ul"),LF=a("li"),qbe=a("strong"),wFr=o("beit"),AFr=o(" \u2014 "),QO=a("a"),LFr=o("FlaxBeitForImageClassification"),BFr=o(" (BEiT model)"),xFr=l(),BF=a("li"),Gbe=a("strong"),kFr=o("vit"),RFr=o(" \u2014 "),HO=a("a"),SFr=o("FlaxViTForImageClassification"),PFr=o(" (ViT model)"),$Fr=l(),Obe=a("p"),IFr=o("Examples:"),jFr=l(),f(oA.$$.fragment),gLe=l(),df=a("h2"),xF=a("a"),Xbe=a("span"),f(rA.$$.fragment),NFr=l(),zbe=a("span"),DFr=o("FlaxAutoModelForVision2Seq"),hLe=l(),$r=a("div"),f(tA.$$.fragment),qFr=l(),cf=a("p"),GFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Vbe=a("code"),OFr=o("from_pretrained()"),XFr=o("class method or the "),Wbe=a("code"),zFr=o("from_config()"),VFr=o(`class
method.`),WFr=l(),aA=a("p"),QFr=o("This class cannot be instantiated directly using "),Qbe=a("code"),HFr=o("__init__()"),UFr=o(" (throws an error)."),JFr=l(),At=a("div"),f(nA.$$.fragment),YFr=l(),Hbe=a("p"),KFr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),ZFr=l(),ff=a("p"),eCr=o(`Note:
Loading a model from its configuration file does `),Ube=a("strong"),oCr=o("not"),rCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jbe=a("code"),tCr=o("from_pretrained()"),aCr=o("to load the model weights."),nCr=l(),Ybe=a("p"),sCr=o("Examples:"),lCr=l(),f(sA.$$.fragment),iCr=l(),$o=a("div"),f(lA.$$.fragment),dCr=l(),Kbe=a("p"),cCr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),fCr=l(),Ln=a("p"),mCr=o("The model class to instantiate is selected based on the "),Zbe=a("code"),gCr=o("model_type"),hCr=o(` property of the config object (either
passed as an argument or loaded from `),e5e=a("code"),pCr=o("pretrained_model_name_or_path"),_Cr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o5e=a("code"),uCr=o("pretrained_model_name_or_path"),bCr=o(":"),vCr=l(),r5e=a("ul"),kF=a("li"),t5e=a("strong"),TCr=o("vision-encoder-decoder"),FCr=o(" \u2014 "),UO=a("a"),CCr=o("FlaxVisionEncoderDecoderModel"),MCr=o(" (Vision Encoder decoder model)"),ECr=l(),a5e=a("p"),yCr=o("Examples:"),wCr=l(),f(iA.$$.fragment),this.h()},l(d){const u=Oft('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Ae=i(d),le=n(d,"H1",{class:!0});var dA=s(le);me=n(dA,"A",{id:!0,class:!0,href:!0});var n5e=s(me);oo=n(n5e,"SPAN",{});var s5e=s(oo);m(ce.$$.fragment,s5e),s5e.forEach(t),n5e.forEach(t),ue=i(dA),No=n(dA,"SPAN",{});var LCr=s(No);vi=r(LCr,"Auto Classes"),LCr.forEach(t),dA.forEach(t),gf=i(d),ra=n(d,"P",{});var _Le=s(ra);Ti=r(_Le,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Fi=n(_Le,"CODE",{});var BCr=s(Fi);LC=r(BCr,"from_pretrained()"),BCr.forEach(t),hf=r(_Le,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),_Le.forEach(t),Ee=i(d),so=n(d,"P",{});var RF=s(so);Ci=r(RF,"Instantiating one of "),Bn=n(RF,"A",{href:!0});var xCr=s(Bn);BC=r(xCr,"AutoConfig"),xCr.forEach(t),xn=r(RF,", "),kn=n(RF,"A",{href:!0});var kCr=s(kn);xC=r(kCr,"AutoModel"),kCr.forEach(t),Mi=r(RF,`, and
`),Rn=n(RF,"A",{href:!0});var RCr=s(Rn);kC=r(RCr,"AutoTokenizer"),RCr.forEach(t),Ei=r(RF," will directly create a class of the relevant architecture. For instance"),RF.forEach(t),pf=i(d),m(ka.$$.fragment,d),lo=i(d),ge=n(d,"P",{});var uLe=s(ge);n0=r(uLe,"will create a model that is an instance of "),yi=n(uLe,"A",{href:!0});var SCr=s(yi);s0=r(SCr,"BertModel"),SCr.forEach(t),l0=r(uLe,"."),uLe.forEach(t),Do=i(d),Ra=n(d,"P",{});var bLe=s(Ra);i0=r(bLe,"There is one class of "),_f=n(bLe,"CODE",{});var PCr=s(_f);d0=r(PCr,"AutoModel"),PCr.forEach(t),MBe=r(bLe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),bLe.forEach(t),bAe=i(d),wi=n(d,"H2",{class:!0});var vLe=s(wi);uf=n(vLe,"A",{id:!0,class:!0,href:!0});var $Cr=s(uf);Dz=n($Cr,"SPAN",{});var ICr=s(Dz);m(RC.$$.fragment,ICr),ICr.forEach(t),$Cr.forEach(t),EBe=i(vLe),qz=n(vLe,"SPAN",{});var jCr=s(qz);yBe=r(jCr,"Extending the Auto Classes"),jCr.forEach(t),vLe.forEach(t),vAe=i(d),Sn=n(d,"P",{});var JO=s(Sn);wBe=r(JO,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),Gz=n(JO,"CODE",{});var NCr=s(Gz);ABe=r(NCr,"NewModel"),NCr.forEach(t),LBe=r(JO,", make sure you have a "),Oz=n(JO,"CODE",{});var DCr=s(Oz);BBe=r(DCr,"NewModelConfig"),DCr.forEach(t),xBe=r(JO,` then you can add those to the auto
classes like this:`),JO.forEach(t),TAe=i(d),m(SC.$$.fragment,d),FAe=i(d),c0=n(d,"P",{});var qCr=s(c0);kBe=r(qCr,"You will then be able to use the auto classes like you would usually do!"),qCr.forEach(t),CAe=i(d),m(bf.$$.fragment,d),MAe=i(d),Ai=n(d,"H2",{class:!0});var TLe=s(Ai);vf=n(TLe,"A",{id:!0,class:!0,href:!0});var GCr=s(vf);Xz=n(GCr,"SPAN",{});var OCr=s(Xz);m(PC.$$.fragment,OCr),OCr.forEach(t),GCr.forEach(t),RBe=i(TLe),zz=n(TLe,"SPAN",{});var XCr=s(zz);SBe=r(XCr,"AutoConfig"),XCr.forEach(t),TLe.forEach(t),EAe=i(d),qo=n(d,"DIV",{class:!0});var Bs=s(qo);m($C.$$.fragment,Bs),PBe=i(Bs),IC=n(Bs,"P",{});var FLe=s(IC);$Be=r(FLe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),f0=n(FLe,"A",{href:!0});var zCr=s(f0);IBe=r(zCr,"from_pretrained()"),zCr.forEach(t),jBe=r(FLe," class method."),FLe.forEach(t),NBe=i(Bs),jC=n(Bs,"P",{});var CLe=s(jC);DBe=r(CLe,"This class cannot be instantiated directly using "),Vz=n(CLe,"CODE",{});var VCr=s(Vz);qBe=r(VCr,"__init__()"),VCr.forEach(t),GBe=r(CLe," (throws an error)."),CLe.forEach(t),OBe=i(Bs),io=n(Bs,"DIV",{class:!0});var aa=s(io);m(NC.$$.fragment,aa),XBe=i(aa),Wz=n(aa,"P",{});var WCr=s(Wz);zBe=r(WCr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),WCr.forEach(t),VBe=i(aa),Li=n(aa,"P",{});var YO=s(Li);WBe=r(YO,"The configuration class to instantiate is selected based on the "),Qz=n(YO,"CODE",{});var QCr=s(Qz);QBe=r(QCr,"model_type"),QCr.forEach(t),HBe=r(YO,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),Hz=n(YO,"CODE",{});var HCr=s(Hz);UBe=r(HCr,"pretrained_model_name_or_path"),HCr.forEach(t),JBe=r(YO,":"),YO.forEach(t),YBe=i(aa),v=n(aa,"UL",{});var T=s(v);Tf=n(T,"LI",{});var l5e=s(Tf);Uz=n(l5e,"STRONG",{});var UCr=s(Uz);KBe=r(UCr,"albert"),UCr.forEach(t),ZBe=r(l5e," \u2014 "),m0=n(l5e,"A",{href:!0});var JCr=s(m0);exe=r(JCr,"AlbertConfig"),JCr.forEach(t),oxe=r(l5e," (ALBERT model)"),l5e.forEach(t),rxe=i(T),Ff=n(T,"LI",{});var i5e=s(Ff);Jz=n(i5e,"STRONG",{});var YCr=s(Jz);txe=r(YCr,"bart"),YCr.forEach(t),axe=r(i5e," \u2014 "),g0=n(i5e,"A",{href:!0});var KCr=s(g0);nxe=r(KCr,"BartConfig"),KCr.forEach(t),sxe=r(i5e," (BART model)"),i5e.forEach(t),lxe=i(T),Cf=n(T,"LI",{});var d5e=s(Cf);Yz=n(d5e,"STRONG",{});var ZCr=s(Yz);ixe=r(ZCr,"beit"),ZCr.forEach(t),dxe=r(d5e," \u2014 "),h0=n(d5e,"A",{href:!0});var e4r=s(h0);cxe=r(e4r,"BeitConfig"),e4r.forEach(t),fxe=r(d5e," (BEiT model)"),d5e.forEach(t),mxe=i(T),Mf=n(T,"LI",{});var c5e=s(Mf);Kz=n(c5e,"STRONG",{});var o4r=s(Kz);gxe=r(o4r,"bert"),o4r.forEach(t),hxe=r(c5e," \u2014 "),p0=n(c5e,"A",{href:!0});var r4r=s(p0);pxe=r(r4r,"BertConfig"),r4r.forEach(t),_xe=r(c5e," (BERT model)"),c5e.forEach(t),uxe=i(T),Ef=n(T,"LI",{});var f5e=s(Ef);Zz=n(f5e,"STRONG",{});var t4r=s(Zz);bxe=r(t4r,"bert-generation"),t4r.forEach(t),vxe=r(f5e," \u2014 "),_0=n(f5e,"A",{href:!0});var a4r=s(_0);Txe=r(a4r,"BertGenerationConfig"),a4r.forEach(t),Fxe=r(f5e," (Bert Generation model)"),f5e.forEach(t),Cxe=i(T),yf=n(T,"LI",{});var m5e=s(yf);eV=n(m5e,"STRONG",{});var n4r=s(eV);Mxe=r(n4r,"big_bird"),n4r.forEach(t),Exe=r(m5e," \u2014 "),u0=n(m5e,"A",{href:!0});var s4r=s(u0);yxe=r(s4r,"BigBirdConfig"),s4r.forEach(t),wxe=r(m5e," (BigBird model)"),m5e.forEach(t),Axe=i(T),wf=n(T,"LI",{});var g5e=s(wf);oV=n(g5e,"STRONG",{});var l4r=s(oV);Lxe=r(l4r,"bigbird_pegasus"),l4r.forEach(t),Bxe=r(g5e," \u2014 "),b0=n(g5e,"A",{href:!0});var i4r=s(b0);xxe=r(i4r,"BigBirdPegasusConfig"),i4r.forEach(t),kxe=r(g5e," (BigBirdPegasus model)"),g5e.forEach(t),Rxe=i(T),Af=n(T,"LI",{});var h5e=s(Af);rV=n(h5e,"STRONG",{});var d4r=s(rV);Sxe=r(d4r,"blenderbot"),d4r.forEach(t),Pxe=r(h5e," \u2014 "),v0=n(h5e,"A",{href:!0});var c4r=s(v0);$xe=r(c4r,"BlenderbotConfig"),c4r.forEach(t),Ixe=r(h5e," (Blenderbot model)"),h5e.forEach(t),jxe=i(T),Lf=n(T,"LI",{});var p5e=s(Lf);tV=n(p5e,"STRONG",{});var f4r=s(tV);Nxe=r(f4r,"blenderbot-small"),f4r.forEach(t),Dxe=r(p5e," \u2014 "),T0=n(p5e,"A",{href:!0});var m4r=s(T0);qxe=r(m4r,"BlenderbotSmallConfig"),m4r.forEach(t),Gxe=r(p5e," (BlenderbotSmall model)"),p5e.forEach(t),Oxe=i(T),Bf=n(T,"LI",{});var _5e=s(Bf);aV=n(_5e,"STRONG",{});var g4r=s(aV);Xxe=r(g4r,"camembert"),g4r.forEach(t),zxe=r(_5e," \u2014 "),F0=n(_5e,"A",{href:!0});var h4r=s(F0);Vxe=r(h4r,"CamembertConfig"),h4r.forEach(t),Wxe=r(_5e," (CamemBERT model)"),_5e.forEach(t),Qxe=i(T),xf=n(T,"LI",{});var u5e=s(xf);nV=n(u5e,"STRONG",{});var p4r=s(nV);Hxe=r(p4r,"canine"),p4r.forEach(t),Uxe=r(u5e," \u2014 "),C0=n(u5e,"A",{href:!0});var _4r=s(C0);Jxe=r(_4r,"CanineConfig"),_4r.forEach(t),Yxe=r(u5e," (Canine model)"),u5e.forEach(t),Kxe=i(T),kf=n(T,"LI",{});var b5e=s(kf);sV=n(b5e,"STRONG",{});var u4r=s(sV);Zxe=r(u4r,"clip"),u4r.forEach(t),eke=r(b5e," \u2014 "),M0=n(b5e,"A",{href:!0});var b4r=s(M0);oke=r(b4r,"CLIPConfig"),b4r.forEach(t),rke=r(b5e," (CLIP model)"),b5e.forEach(t),tke=i(T),Rf=n(T,"LI",{});var v5e=s(Rf);lV=n(v5e,"STRONG",{});var v4r=s(lV);ake=r(v4r,"convbert"),v4r.forEach(t),nke=r(v5e," \u2014 "),E0=n(v5e,"A",{href:!0});var T4r=s(E0);ske=r(T4r,"ConvBertConfig"),T4r.forEach(t),lke=r(v5e," (ConvBERT model)"),v5e.forEach(t),ike=i(T),Sf=n(T,"LI",{});var T5e=s(Sf);iV=n(T5e,"STRONG",{});var F4r=s(iV);dke=r(F4r,"convnext"),F4r.forEach(t),cke=r(T5e," \u2014 "),y0=n(T5e,"A",{href:!0});var C4r=s(y0);fke=r(C4r,"ConvNextConfig"),C4r.forEach(t),mke=r(T5e," (ConvNext model)"),T5e.forEach(t),gke=i(T),Pf=n(T,"LI",{});var F5e=s(Pf);dV=n(F5e,"STRONG",{});var M4r=s(dV);hke=r(M4r,"ctrl"),M4r.forEach(t),pke=r(F5e," \u2014 "),w0=n(F5e,"A",{href:!0});var E4r=s(w0);_ke=r(E4r,"CTRLConfig"),E4r.forEach(t),uke=r(F5e," (CTRL model)"),F5e.forEach(t),bke=i(T),$f=n(T,"LI",{});var C5e=s($f);cV=n(C5e,"STRONG",{});var y4r=s(cV);vke=r(y4r,"deberta"),y4r.forEach(t),Tke=r(C5e," \u2014 "),A0=n(C5e,"A",{href:!0});var w4r=s(A0);Fke=r(w4r,"DebertaConfig"),w4r.forEach(t),Cke=r(C5e," (DeBERTa model)"),C5e.forEach(t),Mke=i(T),If=n(T,"LI",{});var M5e=s(If);fV=n(M5e,"STRONG",{});var A4r=s(fV);Eke=r(A4r,"deberta-v2"),A4r.forEach(t),yke=r(M5e," \u2014 "),L0=n(M5e,"A",{href:!0});var L4r=s(L0);wke=r(L4r,"DebertaV2Config"),L4r.forEach(t),Ake=r(M5e," (DeBERTa-v2 model)"),M5e.forEach(t),Lke=i(T),jf=n(T,"LI",{});var E5e=s(jf);mV=n(E5e,"STRONG",{});var B4r=s(mV);Bke=r(B4r,"deit"),B4r.forEach(t),xke=r(E5e," \u2014 "),B0=n(E5e,"A",{href:!0});var x4r=s(B0);kke=r(x4r,"DeiTConfig"),x4r.forEach(t),Rke=r(E5e," (DeiT model)"),E5e.forEach(t),Ske=i(T),Nf=n(T,"LI",{});var y5e=s(Nf);gV=n(y5e,"STRONG",{});var k4r=s(gV);Pke=r(k4r,"detr"),k4r.forEach(t),$ke=r(y5e," \u2014 "),x0=n(y5e,"A",{href:!0});var R4r=s(x0);Ike=r(R4r,"DetrConfig"),R4r.forEach(t),jke=r(y5e," (DETR model)"),y5e.forEach(t),Nke=i(T),Df=n(T,"LI",{});var w5e=s(Df);hV=n(w5e,"STRONG",{});var S4r=s(hV);Dke=r(S4r,"distilbert"),S4r.forEach(t),qke=r(w5e," \u2014 "),k0=n(w5e,"A",{href:!0});var P4r=s(k0);Gke=r(P4r,"DistilBertConfig"),P4r.forEach(t),Oke=r(w5e," (DistilBERT model)"),w5e.forEach(t),Xke=i(T),qf=n(T,"LI",{});var A5e=s(qf);pV=n(A5e,"STRONG",{});var $4r=s(pV);zke=r($4r,"dpr"),$4r.forEach(t),Vke=r(A5e," \u2014 "),R0=n(A5e,"A",{href:!0});var I4r=s(R0);Wke=r(I4r,"DPRConfig"),I4r.forEach(t),Qke=r(A5e," (DPR model)"),A5e.forEach(t),Hke=i(T),Gf=n(T,"LI",{});var L5e=s(Gf);_V=n(L5e,"STRONG",{});var j4r=s(_V);Uke=r(j4r,"electra"),j4r.forEach(t),Jke=r(L5e," \u2014 "),S0=n(L5e,"A",{href:!0});var N4r=s(S0);Yke=r(N4r,"ElectraConfig"),N4r.forEach(t),Kke=r(L5e," (ELECTRA model)"),L5e.forEach(t),Zke=i(T),Of=n(T,"LI",{});var B5e=s(Of);uV=n(B5e,"STRONG",{});var D4r=s(uV);eRe=r(D4r,"encoder-decoder"),D4r.forEach(t),oRe=r(B5e," \u2014 "),P0=n(B5e,"A",{href:!0});var q4r=s(P0);rRe=r(q4r,"EncoderDecoderConfig"),q4r.forEach(t),tRe=r(B5e," (Encoder decoder model)"),B5e.forEach(t),aRe=i(T),Xf=n(T,"LI",{});var x5e=s(Xf);bV=n(x5e,"STRONG",{});var G4r=s(bV);nRe=r(G4r,"flaubert"),G4r.forEach(t),sRe=r(x5e," \u2014 "),$0=n(x5e,"A",{href:!0});var O4r=s($0);lRe=r(O4r,"FlaubertConfig"),O4r.forEach(t),iRe=r(x5e," (FlauBERT model)"),x5e.forEach(t),dRe=i(T),zf=n(T,"LI",{});var k5e=s(zf);vV=n(k5e,"STRONG",{});var X4r=s(vV);cRe=r(X4r,"fnet"),X4r.forEach(t),fRe=r(k5e," \u2014 "),I0=n(k5e,"A",{href:!0});var z4r=s(I0);mRe=r(z4r,"FNetConfig"),z4r.forEach(t),gRe=r(k5e," (FNet model)"),k5e.forEach(t),hRe=i(T),Vf=n(T,"LI",{});var R5e=s(Vf);TV=n(R5e,"STRONG",{});var V4r=s(TV);pRe=r(V4r,"fsmt"),V4r.forEach(t),_Re=r(R5e," \u2014 "),j0=n(R5e,"A",{href:!0});var W4r=s(j0);uRe=r(W4r,"FSMTConfig"),W4r.forEach(t),bRe=r(R5e," (FairSeq Machine-Translation model)"),R5e.forEach(t),vRe=i(T),Wf=n(T,"LI",{});var S5e=s(Wf);FV=n(S5e,"STRONG",{});var Q4r=s(FV);TRe=r(Q4r,"funnel"),Q4r.forEach(t),FRe=r(S5e," \u2014 "),N0=n(S5e,"A",{href:!0});var H4r=s(N0);CRe=r(H4r,"FunnelConfig"),H4r.forEach(t),MRe=r(S5e," (Funnel Transformer model)"),S5e.forEach(t),ERe=i(T),Qf=n(T,"LI",{});var P5e=s(Qf);CV=n(P5e,"STRONG",{});var U4r=s(CV);yRe=r(U4r,"gpt2"),U4r.forEach(t),wRe=r(P5e," \u2014 "),D0=n(P5e,"A",{href:!0});var J4r=s(D0);ARe=r(J4r,"GPT2Config"),J4r.forEach(t),LRe=r(P5e," (OpenAI GPT-2 model)"),P5e.forEach(t),BRe=i(T),Hf=n(T,"LI",{});var $5e=s(Hf);MV=n($5e,"STRONG",{});var Y4r=s(MV);xRe=r(Y4r,"gpt_neo"),Y4r.forEach(t),kRe=r($5e," \u2014 "),q0=n($5e,"A",{href:!0});var K4r=s(q0);RRe=r(K4r,"GPTNeoConfig"),K4r.forEach(t),SRe=r($5e," (GPT Neo model)"),$5e.forEach(t),PRe=i(T),Uf=n(T,"LI",{});var I5e=s(Uf);EV=n(I5e,"STRONG",{});var Z4r=s(EV);$Re=r(Z4r,"gptj"),Z4r.forEach(t),IRe=r(I5e," \u2014 "),G0=n(I5e,"A",{href:!0});var eMr=s(G0);jRe=r(eMr,"GPTJConfig"),eMr.forEach(t),NRe=r(I5e," (GPT-J model)"),I5e.forEach(t),DRe=i(T),Jf=n(T,"LI",{});var j5e=s(Jf);yV=n(j5e,"STRONG",{});var oMr=s(yV);qRe=r(oMr,"hubert"),oMr.forEach(t),GRe=r(j5e," \u2014 "),O0=n(j5e,"A",{href:!0});var rMr=s(O0);ORe=r(rMr,"HubertConfig"),rMr.forEach(t),XRe=r(j5e," (Hubert model)"),j5e.forEach(t),zRe=i(T),Yf=n(T,"LI",{});var N5e=s(Yf);wV=n(N5e,"STRONG",{});var tMr=s(wV);VRe=r(tMr,"ibert"),tMr.forEach(t),WRe=r(N5e," \u2014 "),X0=n(N5e,"A",{href:!0});var aMr=s(X0);QRe=r(aMr,"IBertConfig"),aMr.forEach(t),HRe=r(N5e," (I-BERT model)"),N5e.forEach(t),URe=i(T),Kf=n(T,"LI",{});var D5e=s(Kf);AV=n(D5e,"STRONG",{});var nMr=s(AV);JRe=r(nMr,"imagegpt"),nMr.forEach(t),YRe=r(D5e," \u2014 "),z0=n(D5e,"A",{href:!0});var sMr=s(z0);KRe=r(sMr,"ImageGPTConfig"),sMr.forEach(t),ZRe=r(D5e," (ImageGPT model)"),D5e.forEach(t),eSe=i(T),Zf=n(T,"LI",{});var q5e=s(Zf);LV=n(q5e,"STRONG",{});var lMr=s(LV);oSe=r(lMr,"layoutlm"),lMr.forEach(t),rSe=r(q5e," \u2014 "),V0=n(q5e,"A",{href:!0});var iMr=s(V0);tSe=r(iMr,"LayoutLMConfig"),iMr.forEach(t),aSe=r(q5e," (LayoutLM model)"),q5e.forEach(t),nSe=i(T),em=n(T,"LI",{});var G5e=s(em);BV=n(G5e,"STRONG",{});var dMr=s(BV);sSe=r(dMr,"layoutlmv2"),dMr.forEach(t),lSe=r(G5e," \u2014 "),W0=n(G5e,"A",{href:!0});var cMr=s(W0);iSe=r(cMr,"LayoutLMv2Config"),cMr.forEach(t),dSe=r(G5e," (LayoutLMv2 model)"),G5e.forEach(t),cSe=i(T),om=n(T,"LI",{});var O5e=s(om);xV=n(O5e,"STRONG",{});var fMr=s(xV);fSe=r(fMr,"led"),fMr.forEach(t),mSe=r(O5e," \u2014 "),Q0=n(O5e,"A",{href:!0});var mMr=s(Q0);gSe=r(mMr,"LEDConfig"),mMr.forEach(t),hSe=r(O5e," (LED model)"),O5e.forEach(t),pSe=i(T),rm=n(T,"LI",{});var X5e=s(rm);kV=n(X5e,"STRONG",{});var gMr=s(kV);_Se=r(gMr,"longformer"),gMr.forEach(t),uSe=r(X5e," \u2014 "),H0=n(X5e,"A",{href:!0});var hMr=s(H0);bSe=r(hMr,"LongformerConfig"),hMr.forEach(t),vSe=r(X5e," (Longformer model)"),X5e.forEach(t),TSe=i(T),tm=n(T,"LI",{});var z5e=s(tm);RV=n(z5e,"STRONG",{});var pMr=s(RV);FSe=r(pMr,"luke"),pMr.forEach(t),CSe=r(z5e," \u2014 "),U0=n(z5e,"A",{href:!0});var _Mr=s(U0);MSe=r(_Mr,"LukeConfig"),_Mr.forEach(t),ESe=r(z5e," (LUKE model)"),z5e.forEach(t),ySe=i(T),am=n(T,"LI",{});var V5e=s(am);SV=n(V5e,"STRONG",{});var uMr=s(SV);wSe=r(uMr,"lxmert"),uMr.forEach(t),ASe=r(V5e," \u2014 "),J0=n(V5e,"A",{href:!0});var bMr=s(J0);LSe=r(bMr,"LxmertConfig"),bMr.forEach(t),BSe=r(V5e," (LXMERT model)"),V5e.forEach(t),xSe=i(T),nm=n(T,"LI",{});var W5e=s(nm);PV=n(W5e,"STRONG",{});var vMr=s(PV);kSe=r(vMr,"m2m_100"),vMr.forEach(t),RSe=r(W5e," \u2014 "),Y0=n(W5e,"A",{href:!0});var TMr=s(Y0);SSe=r(TMr,"M2M100Config"),TMr.forEach(t),PSe=r(W5e," (M2M100 model)"),W5e.forEach(t),$Se=i(T),sm=n(T,"LI",{});var Q5e=s(sm);$V=n(Q5e,"STRONG",{});var FMr=s($V);ISe=r(FMr,"marian"),FMr.forEach(t),jSe=r(Q5e," \u2014 "),K0=n(Q5e,"A",{href:!0});var CMr=s(K0);NSe=r(CMr,"MarianConfig"),CMr.forEach(t),DSe=r(Q5e," (Marian model)"),Q5e.forEach(t),qSe=i(T),lm=n(T,"LI",{});var H5e=s(lm);IV=n(H5e,"STRONG",{});var MMr=s(IV);GSe=r(MMr,"mbart"),MMr.forEach(t),OSe=r(H5e," \u2014 "),Z0=n(H5e,"A",{href:!0});var EMr=s(Z0);XSe=r(EMr,"MBartConfig"),EMr.forEach(t),zSe=r(H5e," (mBART model)"),H5e.forEach(t),VSe=i(T),im=n(T,"LI",{});var U5e=s(im);jV=n(U5e,"STRONG",{});var yMr=s(jV);WSe=r(yMr,"megatron-bert"),yMr.forEach(t),QSe=r(U5e," \u2014 "),eL=n(U5e,"A",{href:!0});var wMr=s(eL);HSe=r(wMr,"MegatronBertConfig"),wMr.forEach(t),USe=r(U5e," (MegatronBert model)"),U5e.forEach(t),JSe=i(T),dm=n(T,"LI",{});var J5e=s(dm);NV=n(J5e,"STRONG",{});var AMr=s(NV);YSe=r(AMr,"mobilebert"),AMr.forEach(t),KSe=r(J5e," \u2014 "),oL=n(J5e,"A",{href:!0});var LMr=s(oL);ZSe=r(LMr,"MobileBertConfig"),LMr.forEach(t),ePe=r(J5e," (MobileBERT model)"),J5e.forEach(t),oPe=i(T),cm=n(T,"LI",{});var Y5e=s(cm);DV=n(Y5e,"STRONG",{});var BMr=s(DV);rPe=r(BMr,"mpnet"),BMr.forEach(t),tPe=r(Y5e," \u2014 "),rL=n(Y5e,"A",{href:!0});var xMr=s(rL);aPe=r(xMr,"MPNetConfig"),xMr.forEach(t),nPe=r(Y5e," (MPNet model)"),Y5e.forEach(t),sPe=i(T),fm=n(T,"LI",{});var K5e=s(fm);qV=n(K5e,"STRONG",{});var kMr=s(qV);lPe=r(kMr,"mt5"),kMr.forEach(t),iPe=r(K5e," \u2014 "),tL=n(K5e,"A",{href:!0});var RMr=s(tL);dPe=r(RMr,"MT5Config"),RMr.forEach(t),cPe=r(K5e," (mT5 model)"),K5e.forEach(t),fPe=i(T),mm=n(T,"LI",{});var Z5e=s(mm);GV=n(Z5e,"STRONG",{});var SMr=s(GV);mPe=r(SMr,"nystromformer"),SMr.forEach(t),gPe=r(Z5e," \u2014 "),aL=n(Z5e,"A",{href:!0});var PMr=s(aL);hPe=r(PMr,"NystromformerConfig"),PMr.forEach(t),pPe=r(Z5e," (Nystromformer model)"),Z5e.forEach(t),_Pe=i(T),gm=n(T,"LI",{});var e2e=s(gm);OV=n(e2e,"STRONG",{});var $Mr=s(OV);uPe=r($Mr,"openai-gpt"),$Mr.forEach(t),bPe=r(e2e," \u2014 "),nL=n(e2e,"A",{href:!0});var IMr=s(nL);vPe=r(IMr,"OpenAIGPTConfig"),IMr.forEach(t),TPe=r(e2e," (OpenAI GPT model)"),e2e.forEach(t),FPe=i(T),hm=n(T,"LI",{});var o2e=s(hm);XV=n(o2e,"STRONG",{});var jMr=s(XV);CPe=r(jMr,"pegasus"),jMr.forEach(t),MPe=r(o2e," \u2014 "),sL=n(o2e,"A",{href:!0});var NMr=s(sL);EPe=r(NMr,"PegasusConfig"),NMr.forEach(t),yPe=r(o2e," (Pegasus model)"),o2e.forEach(t),wPe=i(T),pm=n(T,"LI",{});var r2e=s(pm);zV=n(r2e,"STRONG",{});var DMr=s(zV);APe=r(DMr,"perceiver"),DMr.forEach(t),LPe=r(r2e," \u2014 "),lL=n(r2e,"A",{href:!0});var qMr=s(lL);BPe=r(qMr,"PerceiverConfig"),qMr.forEach(t),xPe=r(r2e," (Perceiver model)"),r2e.forEach(t),kPe=i(T),_m=n(T,"LI",{});var t2e=s(_m);VV=n(t2e,"STRONG",{});var GMr=s(VV);RPe=r(GMr,"prophetnet"),GMr.forEach(t),SPe=r(t2e," \u2014 "),iL=n(t2e,"A",{href:!0});var OMr=s(iL);PPe=r(OMr,"ProphetNetConfig"),OMr.forEach(t),$Pe=r(t2e," (ProphetNet model)"),t2e.forEach(t),IPe=i(T),um=n(T,"LI",{});var a2e=s(um);WV=n(a2e,"STRONG",{});var XMr=s(WV);jPe=r(XMr,"qdqbert"),XMr.forEach(t),NPe=r(a2e," \u2014 "),dL=n(a2e,"A",{href:!0});var zMr=s(dL);DPe=r(zMr,"QDQBertConfig"),zMr.forEach(t),qPe=r(a2e," (QDQBert model)"),a2e.forEach(t),GPe=i(T),bm=n(T,"LI",{});var n2e=s(bm);QV=n(n2e,"STRONG",{});var VMr=s(QV);OPe=r(VMr,"rag"),VMr.forEach(t),XPe=r(n2e," \u2014 "),cL=n(n2e,"A",{href:!0});var WMr=s(cL);zPe=r(WMr,"RagConfig"),WMr.forEach(t),VPe=r(n2e," (RAG model)"),n2e.forEach(t),WPe=i(T),vm=n(T,"LI",{});var s2e=s(vm);HV=n(s2e,"STRONG",{});var QMr=s(HV);QPe=r(QMr,"realm"),QMr.forEach(t),HPe=r(s2e," \u2014 "),fL=n(s2e,"A",{href:!0});var HMr=s(fL);UPe=r(HMr,"RealmConfig"),HMr.forEach(t),JPe=r(s2e," (Realm model)"),s2e.forEach(t),YPe=i(T),Tm=n(T,"LI",{});var l2e=s(Tm);UV=n(l2e,"STRONG",{});var UMr=s(UV);KPe=r(UMr,"reformer"),UMr.forEach(t),ZPe=r(l2e," \u2014 "),mL=n(l2e,"A",{href:!0});var JMr=s(mL);e$e=r(JMr,"ReformerConfig"),JMr.forEach(t),o$e=r(l2e," (Reformer model)"),l2e.forEach(t),r$e=i(T),Fm=n(T,"LI",{});var i2e=s(Fm);JV=n(i2e,"STRONG",{});var YMr=s(JV);t$e=r(YMr,"rembert"),YMr.forEach(t),a$e=r(i2e," \u2014 "),gL=n(i2e,"A",{href:!0});var KMr=s(gL);n$e=r(KMr,"RemBertConfig"),KMr.forEach(t),s$e=r(i2e," (RemBERT model)"),i2e.forEach(t),l$e=i(T),Cm=n(T,"LI",{});var d2e=s(Cm);YV=n(d2e,"STRONG",{});var ZMr=s(YV);i$e=r(ZMr,"retribert"),ZMr.forEach(t),d$e=r(d2e," \u2014 "),hL=n(d2e,"A",{href:!0});var eEr=s(hL);c$e=r(eEr,"RetriBertConfig"),eEr.forEach(t),f$e=r(d2e," (RetriBERT model)"),d2e.forEach(t),m$e=i(T),Mm=n(T,"LI",{});var c2e=s(Mm);KV=n(c2e,"STRONG",{});var oEr=s(KV);g$e=r(oEr,"roberta"),oEr.forEach(t),h$e=r(c2e," \u2014 "),pL=n(c2e,"A",{href:!0});var rEr=s(pL);p$e=r(rEr,"RobertaConfig"),rEr.forEach(t),_$e=r(c2e," (RoBERTa model)"),c2e.forEach(t),u$e=i(T),Em=n(T,"LI",{});var f2e=s(Em);ZV=n(f2e,"STRONG",{});var tEr=s(ZV);b$e=r(tEr,"roformer"),tEr.forEach(t),v$e=r(f2e," \u2014 "),_L=n(f2e,"A",{href:!0});var aEr=s(_L);T$e=r(aEr,"RoFormerConfig"),aEr.forEach(t),F$e=r(f2e," (RoFormer model)"),f2e.forEach(t),C$e=i(T),ym=n(T,"LI",{});var m2e=s(ym);eW=n(m2e,"STRONG",{});var nEr=s(eW);M$e=r(nEr,"segformer"),nEr.forEach(t),E$e=r(m2e," \u2014 "),uL=n(m2e,"A",{href:!0});var sEr=s(uL);y$e=r(sEr,"SegformerConfig"),sEr.forEach(t),w$e=r(m2e," (SegFormer model)"),m2e.forEach(t),A$e=i(T),wm=n(T,"LI",{});var g2e=s(wm);oW=n(g2e,"STRONG",{});var lEr=s(oW);L$e=r(lEr,"sew"),lEr.forEach(t),B$e=r(g2e," \u2014 "),bL=n(g2e,"A",{href:!0});var iEr=s(bL);x$e=r(iEr,"SEWConfig"),iEr.forEach(t),k$e=r(g2e," (SEW model)"),g2e.forEach(t),R$e=i(T),Am=n(T,"LI",{});var h2e=s(Am);rW=n(h2e,"STRONG",{});var dEr=s(rW);S$e=r(dEr,"sew-d"),dEr.forEach(t),P$e=r(h2e," \u2014 "),vL=n(h2e,"A",{href:!0});var cEr=s(vL);$$e=r(cEr,"SEWDConfig"),cEr.forEach(t),I$e=r(h2e," (SEW-D model)"),h2e.forEach(t),j$e=i(T),Lm=n(T,"LI",{});var p2e=s(Lm);tW=n(p2e,"STRONG",{});var fEr=s(tW);N$e=r(fEr,"speech-encoder-decoder"),fEr.forEach(t),D$e=r(p2e," \u2014 "),TL=n(p2e,"A",{href:!0});var mEr=s(TL);q$e=r(mEr,"SpeechEncoderDecoderConfig"),mEr.forEach(t),G$e=r(p2e," (Speech Encoder decoder model)"),p2e.forEach(t),O$e=i(T),Bm=n(T,"LI",{});var _2e=s(Bm);aW=n(_2e,"STRONG",{});var gEr=s(aW);X$e=r(gEr,"speech_to_text"),gEr.forEach(t),z$e=r(_2e," \u2014 "),FL=n(_2e,"A",{href:!0});var hEr=s(FL);V$e=r(hEr,"Speech2TextConfig"),hEr.forEach(t),W$e=r(_2e," (Speech2Text model)"),_2e.forEach(t),Q$e=i(T),xm=n(T,"LI",{});var u2e=s(xm);nW=n(u2e,"STRONG",{});var pEr=s(nW);H$e=r(pEr,"speech_to_text_2"),pEr.forEach(t),U$e=r(u2e," \u2014 "),CL=n(u2e,"A",{href:!0});var _Er=s(CL);J$e=r(_Er,"Speech2Text2Config"),_Er.forEach(t),Y$e=r(u2e," (Speech2Text2 model)"),u2e.forEach(t),K$e=i(T),km=n(T,"LI",{});var b2e=s(km);sW=n(b2e,"STRONG",{});var uEr=s(sW);Z$e=r(uEr,"splinter"),uEr.forEach(t),eIe=r(b2e," \u2014 "),ML=n(b2e,"A",{href:!0});var bEr=s(ML);oIe=r(bEr,"SplinterConfig"),bEr.forEach(t),rIe=r(b2e," (Splinter model)"),b2e.forEach(t),tIe=i(T),Rm=n(T,"LI",{});var v2e=s(Rm);lW=n(v2e,"STRONG",{});var vEr=s(lW);aIe=r(vEr,"squeezebert"),vEr.forEach(t),nIe=r(v2e," \u2014 "),EL=n(v2e,"A",{href:!0});var TEr=s(EL);sIe=r(TEr,"SqueezeBertConfig"),TEr.forEach(t),lIe=r(v2e," (SqueezeBERT model)"),v2e.forEach(t),iIe=i(T),Sm=n(T,"LI",{});var T2e=s(Sm);iW=n(T2e,"STRONG",{});var FEr=s(iW);dIe=r(FEr,"swin"),FEr.forEach(t),cIe=r(T2e," \u2014 "),yL=n(T2e,"A",{href:!0});var CEr=s(yL);fIe=r(CEr,"SwinConfig"),CEr.forEach(t),mIe=r(T2e," (Swin model)"),T2e.forEach(t),gIe=i(T),Pm=n(T,"LI",{});var F2e=s(Pm);dW=n(F2e,"STRONG",{});var MEr=s(dW);hIe=r(MEr,"t5"),MEr.forEach(t),pIe=r(F2e," \u2014 "),wL=n(F2e,"A",{href:!0});var EEr=s(wL);_Ie=r(EEr,"T5Config"),EEr.forEach(t),uIe=r(F2e," (T5 model)"),F2e.forEach(t),bIe=i(T),$m=n(T,"LI",{});var C2e=s($m);cW=n(C2e,"STRONG",{});var yEr=s(cW);vIe=r(yEr,"tapas"),yEr.forEach(t),TIe=r(C2e," \u2014 "),AL=n(C2e,"A",{href:!0});var wEr=s(AL);FIe=r(wEr,"TapasConfig"),wEr.forEach(t),CIe=r(C2e," (TAPAS model)"),C2e.forEach(t),MIe=i(T),Im=n(T,"LI",{});var M2e=s(Im);fW=n(M2e,"STRONG",{});var AEr=s(fW);EIe=r(AEr,"transfo-xl"),AEr.forEach(t),yIe=r(M2e," \u2014 "),LL=n(M2e,"A",{href:!0});var LEr=s(LL);wIe=r(LEr,"TransfoXLConfig"),LEr.forEach(t),AIe=r(M2e," (Transformer-XL model)"),M2e.forEach(t),LIe=i(T),jm=n(T,"LI",{});var E2e=s(jm);mW=n(E2e,"STRONG",{});var BEr=s(mW);BIe=r(BEr,"trocr"),BEr.forEach(t),xIe=r(E2e," \u2014 "),BL=n(E2e,"A",{href:!0});var xEr=s(BL);kIe=r(xEr,"TrOCRConfig"),xEr.forEach(t),RIe=r(E2e," (TrOCR model)"),E2e.forEach(t),SIe=i(T),Nm=n(T,"LI",{});var y2e=s(Nm);gW=n(y2e,"STRONG",{});var kEr=s(gW);PIe=r(kEr,"unispeech"),kEr.forEach(t),$Ie=r(y2e," \u2014 "),xL=n(y2e,"A",{href:!0});var REr=s(xL);IIe=r(REr,"UniSpeechConfig"),REr.forEach(t),jIe=r(y2e," (UniSpeech model)"),y2e.forEach(t),NIe=i(T),Dm=n(T,"LI",{});var w2e=s(Dm);hW=n(w2e,"STRONG",{});var SEr=s(hW);DIe=r(SEr,"unispeech-sat"),SEr.forEach(t),qIe=r(w2e," \u2014 "),kL=n(w2e,"A",{href:!0});var PEr=s(kL);GIe=r(PEr,"UniSpeechSatConfig"),PEr.forEach(t),OIe=r(w2e," (UniSpeechSat model)"),w2e.forEach(t),XIe=i(T),qm=n(T,"LI",{});var A2e=s(qm);pW=n(A2e,"STRONG",{});var $Er=s(pW);zIe=r($Er,"vilt"),$Er.forEach(t),VIe=r(A2e," \u2014 "),RL=n(A2e,"A",{href:!0});var IEr=s(RL);WIe=r(IEr,"ViltConfig"),IEr.forEach(t),QIe=r(A2e," (ViLT model)"),A2e.forEach(t),HIe=i(T),Gm=n(T,"LI",{});var L2e=s(Gm);_W=n(L2e,"STRONG",{});var jEr=s(_W);UIe=r(jEr,"vision-encoder-decoder"),jEr.forEach(t),JIe=r(L2e," \u2014 "),SL=n(L2e,"A",{href:!0});var NEr=s(SL);YIe=r(NEr,"VisionEncoderDecoderConfig"),NEr.forEach(t),KIe=r(L2e," (Vision Encoder decoder model)"),L2e.forEach(t),ZIe=i(T),Om=n(T,"LI",{});var B2e=s(Om);uW=n(B2e,"STRONG",{});var DEr=s(uW);eje=r(DEr,"vision-text-dual-encoder"),DEr.forEach(t),oje=r(B2e," \u2014 "),PL=n(B2e,"A",{href:!0});var qEr=s(PL);rje=r(qEr,"VisionTextDualEncoderConfig"),qEr.forEach(t),tje=r(B2e," (VisionTextDualEncoder model)"),B2e.forEach(t),aje=i(T),Xm=n(T,"LI",{});var x2e=s(Xm);bW=n(x2e,"STRONG",{});var GEr=s(bW);nje=r(GEr,"visual_bert"),GEr.forEach(t),sje=r(x2e," \u2014 "),$L=n(x2e,"A",{href:!0});var OEr=s($L);lje=r(OEr,"VisualBertConfig"),OEr.forEach(t),ije=r(x2e," (VisualBert model)"),x2e.forEach(t),dje=i(T),zm=n(T,"LI",{});var k2e=s(zm);vW=n(k2e,"STRONG",{});var XEr=s(vW);cje=r(XEr,"vit"),XEr.forEach(t),fje=r(k2e," \u2014 "),IL=n(k2e,"A",{href:!0});var zEr=s(IL);mje=r(zEr,"ViTConfig"),zEr.forEach(t),gje=r(k2e," (ViT model)"),k2e.forEach(t),hje=i(T),Vm=n(T,"LI",{});var R2e=s(Vm);TW=n(R2e,"STRONG",{});var VEr=s(TW);pje=r(VEr,"vit_mae"),VEr.forEach(t),_je=r(R2e," \u2014 "),jL=n(R2e,"A",{href:!0});var WEr=s(jL);uje=r(WEr,"ViTMAEConfig"),WEr.forEach(t),bje=r(R2e," (ViTMAE model)"),R2e.forEach(t),vje=i(T),Wm=n(T,"LI",{});var S2e=s(Wm);FW=n(S2e,"STRONG",{});var QEr=s(FW);Tje=r(QEr,"wav2vec2"),QEr.forEach(t),Fje=r(S2e," \u2014 "),NL=n(S2e,"A",{href:!0});var HEr=s(NL);Cje=r(HEr,"Wav2Vec2Config"),HEr.forEach(t),Mje=r(S2e," (Wav2Vec2 model)"),S2e.forEach(t),Eje=i(T),Qm=n(T,"LI",{});var P2e=s(Qm);CW=n(P2e,"STRONG",{});var UEr=s(CW);yje=r(UEr,"wavlm"),UEr.forEach(t),wje=r(P2e," \u2014 "),DL=n(P2e,"A",{href:!0});var JEr=s(DL);Aje=r(JEr,"WavLMConfig"),JEr.forEach(t),Lje=r(P2e," (WavLM model)"),P2e.forEach(t),Bje=i(T),Hm=n(T,"LI",{});var $2e=s(Hm);MW=n($2e,"STRONG",{});var YEr=s(MW);xje=r(YEr,"xglm"),YEr.forEach(t),kje=r($2e," \u2014 "),qL=n($2e,"A",{href:!0});var KEr=s(qL);Rje=r(KEr,"XGLMConfig"),KEr.forEach(t),Sje=r($2e," (XGLM model)"),$2e.forEach(t),Pje=i(T),Um=n(T,"LI",{});var I2e=s(Um);EW=n(I2e,"STRONG",{});var ZEr=s(EW);$je=r(ZEr,"xlm"),ZEr.forEach(t),Ije=r(I2e," \u2014 "),GL=n(I2e,"A",{href:!0});var e3r=s(GL);jje=r(e3r,"XLMConfig"),e3r.forEach(t),Nje=r(I2e," (XLM model)"),I2e.forEach(t),Dje=i(T),Jm=n(T,"LI",{});var j2e=s(Jm);yW=n(j2e,"STRONG",{});var o3r=s(yW);qje=r(o3r,"xlm-prophetnet"),o3r.forEach(t),Gje=r(j2e," \u2014 "),OL=n(j2e,"A",{href:!0});var r3r=s(OL);Oje=r(r3r,"XLMProphetNetConfig"),r3r.forEach(t),Xje=r(j2e," (XLMProphetNet model)"),j2e.forEach(t),zje=i(T),Ym=n(T,"LI",{});var N2e=s(Ym);wW=n(N2e,"STRONG",{});var t3r=s(wW);Vje=r(t3r,"xlm-roberta"),t3r.forEach(t),Wje=r(N2e," \u2014 "),XL=n(N2e,"A",{href:!0});var a3r=s(XL);Qje=r(a3r,"XLMRobertaConfig"),a3r.forEach(t),Hje=r(N2e," (XLM-RoBERTa model)"),N2e.forEach(t),Uje=i(T),Km=n(T,"LI",{});var D2e=s(Km);AW=n(D2e,"STRONG",{});var n3r=s(AW);Jje=r(n3r,"xlm-roberta-xl"),n3r.forEach(t),Yje=r(D2e," \u2014 "),zL=n(D2e,"A",{href:!0});var s3r=s(zL);Kje=r(s3r,"XLMRobertaXLConfig"),s3r.forEach(t),Zje=r(D2e," (XLM-RoBERTa-XL model)"),D2e.forEach(t),eNe=i(T),Zm=n(T,"LI",{});var q2e=s(Zm);LW=n(q2e,"STRONG",{});var l3r=s(LW);oNe=r(l3r,"xlnet"),l3r.forEach(t),rNe=r(q2e," \u2014 "),VL=n(q2e,"A",{href:!0});var i3r=s(VL);tNe=r(i3r,"XLNetConfig"),i3r.forEach(t),aNe=r(q2e," (XLNet model)"),q2e.forEach(t),nNe=i(T),eg=n(T,"LI",{});var G2e=s(eg);BW=n(G2e,"STRONG",{});var d3r=s(BW);sNe=r(d3r,"yoso"),d3r.forEach(t),lNe=r(G2e," \u2014 "),WL=n(G2e,"A",{href:!0});var c3r=s(WL);iNe=r(c3r,"YosoConfig"),c3r.forEach(t),dNe=r(G2e," (YOSO model)"),G2e.forEach(t),T.forEach(t),cNe=i(aa),xW=n(aa,"P",{});var f3r=s(xW);fNe=r(f3r,"Examples:"),f3r.forEach(t),mNe=i(aa),m(DC.$$.fragment,aa),aa.forEach(t),gNe=i(Bs),og=n(Bs,"DIV",{class:!0});var MLe=s(og);m(qC.$$.fragment,MLe),hNe=i(MLe),kW=n(MLe,"P",{});var m3r=s(kW);pNe=r(m3r,"Register a new configuration for this class."),m3r.forEach(t),MLe.forEach(t),Bs.forEach(t),yAe=i(d),Bi=n(d,"H2",{class:!0});var ELe=s(Bi);rg=n(ELe,"A",{id:!0,class:!0,href:!0});var g3r=s(rg);RW=n(g3r,"SPAN",{});var h3r=s(RW);m(GC.$$.fragment,h3r),h3r.forEach(t),g3r.forEach(t),_Ne=i(ELe),SW=n(ELe,"SPAN",{});var p3r=s(SW);uNe=r(p3r,"AutoTokenizer"),p3r.forEach(t),ELe.forEach(t),wAe=i(d),Go=n(d,"DIV",{class:!0});var xs=s(Go);m(OC.$$.fragment,xs),bNe=i(xs),XC=n(xs,"P",{});var yLe=s(XC);vNe=r(yLe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),QL=n(yLe,"A",{href:!0});var _3r=s(QL);TNe=r(_3r,"AutoTokenizer.from_pretrained()"),_3r.forEach(t),FNe=r(yLe," class method."),yLe.forEach(t),CNe=i(xs),zC=n(xs,"P",{});var wLe=s(zC);MNe=r(wLe,"This class cannot be instantiated directly using "),PW=n(wLe,"CODE",{});var u3r=s(PW);ENe=r(u3r,"__init__()"),u3r.forEach(t),yNe=r(wLe," (throws an error)."),wLe.forEach(t),wNe=i(xs),co=n(xs,"DIV",{class:!0});var na=s(co);m(VC.$$.fragment,na),ANe=i(na),$W=n(na,"P",{});var b3r=s($W);LNe=r(b3r,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),b3r.forEach(t),BNe=i(na),Sa=n(na,"P",{});var SF=s(Sa);xNe=r(SF,"The tokenizer class to instantiate is selected based on the "),IW=n(SF,"CODE",{});var v3r=s(IW);kNe=r(v3r,"model_type"),v3r.forEach(t),RNe=r(SF,` property of the config object (either
passed as an argument or loaded from `),jW=n(SF,"CODE",{});var T3r=s(jW);SNe=r(T3r,"pretrained_model_name_or_path"),T3r.forEach(t),PNe=r(SF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),NW=n(SF,"CODE",{});var F3r=s(NW);$Ne=r(F3r,"pretrained_model_name_or_path"),F3r.forEach(t),INe=r(SF,":"),SF.forEach(t),jNe=i(na),M=n(na,"UL",{});var y=s(M);Pn=n(y,"LI",{});var cA=s(Pn);DW=n(cA,"STRONG",{});var C3r=s(DW);NNe=r(C3r,"albert"),C3r.forEach(t),DNe=r(cA," \u2014 "),HL=n(cA,"A",{href:!0});var M3r=s(HL);qNe=r(M3r,"AlbertTokenizer"),M3r.forEach(t),GNe=r(cA," or "),UL=n(cA,"A",{href:!0});var E3r=s(UL);ONe=r(E3r,"AlbertTokenizerFast"),E3r.forEach(t),XNe=r(cA," (ALBERT model)"),cA.forEach(t),zNe=i(y),$n=n(y,"LI",{});var fA=s($n);qW=n(fA,"STRONG",{});var y3r=s(qW);VNe=r(y3r,"bart"),y3r.forEach(t),WNe=r(fA," \u2014 "),JL=n(fA,"A",{href:!0});var w3r=s(JL);QNe=r(w3r,"BartTokenizer"),w3r.forEach(t),HNe=r(fA," or "),YL=n(fA,"A",{href:!0});var A3r=s(YL);UNe=r(A3r,"BartTokenizerFast"),A3r.forEach(t),JNe=r(fA," (BART model)"),fA.forEach(t),YNe=i(y),In=n(y,"LI",{});var mA=s(In);GW=n(mA,"STRONG",{});var L3r=s(GW);KNe=r(L3r,"barthez"),L3r.forEach(t),ZNe=r(mA," \u2014 "),KL=n(mA,"A",{href:!0});var B3r=s(KL);eDe=r(B3r,"BarthezTokenizer"),B3r.forEach(t),oDe=r(mA," or "),ZL=n(mA,"A",{href:!0});var x3r=s(ZL);rDe=r(x3r,"BarthezTokenizerFast"),x3r.forEach(t),tDe=r(mA," (BARThez model)"),mA.forEach(t),aDe=i(y),tg=n(y,"LI",{});var O2e=s(tg);OW=n(O2e,"STRONG",{});var k3r=s(OW);nDe=r(k3r,"bartpho"),k3r.forEach(t),sDe=r(O2e," \u2014 "),e9=n(O2e,"A",{href:!0});var R3r=s(e9);lDe=r(R3r,"BartphoTokenizer"),R3r.forEach(t),iDe=r(O2e," (BARTpho model)"),O2e.forEach(t),dDe=i(y),jn=n(y,"LI",{});var gA=s(jn);XW=n(gA,"STRONG",{});var S3r=s(XW);cDe=r(S3r,"bert"),S3r.forEach(t),fDe=r(gA," \u2014 "),o9=n(gA,"A",{href:!0});var P3r=s(o9);mDe=r(P3r,"BertTokenizer"),P3r.forEach(t),gDe=r(gA," or "),r9=n(gA,"A",{href:!0});var $3r=s(r9);hDe=r($3r,"BertTokenizerFast"),$3r.forEach(t),pDe=r(gA," (BERT model)"),gA.forEach(t),_De=i(y),ag=n(y,"LI",{});var X2e=s(ag);zW=n(X2e,"STRONG",{});var I3r=s(zW);uDe=r(I3r,"bert-generation"),I3r.forEach(t),bDe=r(X2e," \u2014 "),t9=n(X2e,"A",{href:!0});var j3r=s(t9);vDe=r(j3r,"BertGenerationTokenizer"),j3r.forEach(t),TDe=r(X2e," (Bert Generation model)"),X2e.forEach(t),FDe=i(y),ng=n(y,"LI",{});var z2e=s(ng);VW=n(z2e,"STRONG",{});var N3r=s(VW);CDe=r(N3r,"bert-japanese"),N3r.forEach(t),MDe=r(z2e," \u2014 "),a9=n(z2e,"A",{href:!0});var D3r=s(a9);EDe=r(D3r,"BertJapaneseTokenizer"),D3r.forEach(t),yDe=r(z2e," (BertJapanese model)"),z2e.forEach(t),wDe=i(y),sg=n(y,"LI",{});var V2e=s(sg);WW=n(V2e,"STRONG",{});var q3r=s(WW);ADe=r(q3r,"bertweet"),q3r.forEach(t),LDe=r(V2e," \u2014 "),n9=n(V2e,"A",{href:!0});var G3r=s(n9);BDe=r(G3r,"BertweetTokenizer"),G3r.forEach(t),xDe=r(V2e," (Bertweet model)"),V2e.forEach(t),kDe=i(y),Nn=n(y,"LI",{});var hA=s(Nn);QW=n(hA,"STRONG",{});var O3r=s(QW);RDe=r(O3r,"big_bird"),O3r.forEach(t),SDe=r(hA," \u2014 "),s9=n(hA,"A",{href:!0});var X3r=s(s9);PDe=r(X3r,"BigBirdTokenizer"),X3r.forEach(t),$De=r(hA," or "),l9=n(hA,"A",{href:!0});var z3r=s(l9);IDe=r(z3r,"BigBirdTokenizerFast"),z3r.forEach(t),jDe=r(hA," (BigBird model)"),hA.forEach(t),NDe=i(y),Dn=n(y,"LI",{});var pA=s(Dn);HW=n(pA,"STRONG",{});var V3r=s(HW);DDe=r(V3r,"bigbird_pegasus"),V3r.forEach(t),qDe=r(pA," \u2014 "),i9=n(pA,"A",{href:!0});var W3r=s(i9);GDe=r(W3r,"PegasusTokenizer"),W3r.forEach(t),ODe=r(pA," or "),d9=n(pA,"A",{href:!0});var Q3r=s(d9);XDe=r(Q3r,"PegasusTokenizerFast"),Q3r.forEach(t),zDe=r(pA," (BigBirdPegasus model)"),pA.forEach(t),VDe=i(y),qn=n(y,"LI",{});var _A=s(qn);UW=n(_A,"STRONG",{});var H3r=s(UW);WDe=r(H3r,"blenderbot"),H3r.forEach(t),QDe=r(_A," \u2014 "),c9=n(_A,"A",{href:!0});var U3r=s(c9);HDe=r(U3r,"BlenderbotTokenizer"),U3r.forEach(t),UDe=r(_A," or "),f9=n(_A,"A",{href:!0});var J3r=s(f9);JDe=r(J3r,"BlenderbotTokenizerFast"),J3r.forEach(t),YDe=r(_A," (Blenderbot model)"),_A.forEach(t),KDe=i(y),lg=n(y,"LI",{});var W2e=s(lg);JW=n(W2e,"STRONG",{});var Y3r=s(JW);ZDe=r(Y3r,"blenderbot-small"),Y3r.forEach(t),eqe=r(W2e," \u2014 "),m9=n(W2e,"A",{href:!0});var K3r=s(m9);oqe=r(K3r,"BlenderbotSmallTokenizer"),K3r.forEach(t),rqe=r(W2e," (BlenderbotSmall model)"),W2e.forEach(t),tqe=i(y),ig=n(y,"LI",{});var Q2e=s(ig);YW=n(Q2e,"STRONG",{});var Z3r=s(YW);aqe=r(Z3r,"byt5"),Z3r.forEach(t),nqe=r(Q2e," \u2014 "),g9=n(Q2e,"A",{href:!0});var eyr=s(g9);sqe=r(eyr,"ByT5Tokenizer"),eyr.forEach(t),lqe=r(Q2e," (ByT5 model)"),Q2e.forEach(t),iqe=i(y),Gn=n(y,"LI",{});var uA=s(Gn);KW=n(uA,"STRONG",{});var oyr=s(KW);dqe=r(oyr,"camembert"),oyr.forEach(t),cqe=r(uA," \u2014 "),h9=n(uA,"A",{href:!0});var ryr=s(h9);fqe=r(ryr,"CamembertTokenizer"),ryr.forEach(t),mqe=r(uA," or "),p9=n(uA,"A",{href:!0});var tyr=s(p9);gqe=r(tyr,"CamembertTokenizerFast"),tyr.forEach(t),hqe=r(uA," (CamemBERT model)"),uA.forEach(t),pqe=i(y),dg=n(y,"LI",{});var H2e=s(dg);ZW=n(H2e,"STRONG",{});var ayr=s(ZW);_qe=r(ayr,"canine"),ayr.forEach(t),uqe=r(H2e," \u2014 "),_9=n(H2e,"A",{href:!0});var nyr=s(_9);bqe=r(nyr,"CanineTokenizer"),nyr.forEach(t),vqe=r(H2e," (Canine model)"),H2e.forEach(t),Tqe=i(y),On=n(y,"LI",{});var bA=s(On);eQ=n(bA,"STRONG",{});var syr=s(eQ);Fqe=r(syr,"clip"),syr.forEach(t),Cqe=r(bA," \u2014 "),u9=n(bA,"A",{href:!0});var lyr=s(u9);Mqe=r(lyr,"CLIPTokenizer"),lyr.forEach(t),Eqe=r(bA," or "),b9=n(bA,"A",{href:!0});var iyr=s(b9);yqe=r(iyr,"CLIPTokenizerFast"),iyr.forEach(t),wqe=r(bA," (CLIP model)"),bA.forEach(t),Aqe=i(y),Xn=n(y,"LI",{});var vA=s(Xn);oQ=n(vA,"STRONG",{});var dyr=s(oQ);Lqe=r(dyr,"convbert"),dyr.forEach(t),Bqe=r(vA," \u2014 "),v9=n(vA,"A",{href:!0});var cyr=s(v9);xqe=r(cyr,"ConvBertTokenizer"),cyr.forEach(t),kqe=r(vA," or "),T9=n(vA,"A",{href:!0});var fyr=s(T9);Rqe=r(fyr,"ConvBertTokenizerFast"),fyr.forEach(t),Sqe=r(vA," (ConvBERT model)"),vA.forEach(t),Pqe=i(y),zn=n(y,"LI",{});var TA=s(zn);rQ=n(TA,"STRONG",{});var myr=s(rQ);$qe=r(myr,"cpm"),myr.forEach(t),Iqe=r(TA," \u2014 "),F9=n(TA,"A",{href:!0});var gyr=s(F9);jqe=r(gyr,"CpmTokenizer"),gyr.forEach(t),Nqe=r(TA," or "),tQ=n(TA,"CODE",{});var hyr=s(tQ);Dqe=r(hyr,"CpmTokenizerFast"),hyr.forEach(t),qqe=r(TA," (CPM model)"),TA.forEach(t),Gqe=i(y),cg=n(y,"LI",{});var U2e=s(cg);aQ=n(U2e,"STRONG",{});var pyr=s(aQ);Oqe=r(pyr,"ctrl"),pyr.forEach(t),Xqe=r(U2e," \u2014 "),C9=n(U2e,"A",{href:!0});var _yr=s(C9);zqe=r(_yr,"CTRLTokenizer"),_yr.forEach(t),Vqe=r(U2e," (CTRL model)"),U2e.forEach(t),Wqe=i(y),Vn=n(y,"LI",{});var FA=s(Vn);nQ=n(FA,"STRONG",{});var uyr=s(nQ);Qqe=r(uyr,"deberta"),uyr.forEach(t),Hqe=r(FA," \u2014 "),M9=n(FA,"A",{href:!0});var byr=s(M9);Uqe=r(byr,"DebertaTokenizer"),byr.forEach(t),Jqe=r(FA," or "),E9=n(FA,"A",{href:!0});var vyr=s(E9);Yqe=r(vyr,"DebertaTokenizerFast"),vyr.forEach(t),Kqe=r(FA," (DeBERTa model)"),FA.forEach(t),Zqe=i(y),fg=n(y,"LI",{});var J2e=s(fg);sQ=n(J2e,"STRONG",{});var Tyr=s(sQ);eGe=r(Tyr,"deberta-v2"),Tyr.forEach(t),oGe=r(J2e," \u2014 "),y9=n(J2e,"A",{href:!0});var Fyr=s(y9);rGe=r(Fyr,"DebertaV2Tokenizer"),Fyr.forEach(t),tGe=r(J2e," (DeBERTa-v2 model)"),J2e.forEach(t),aGe=i(y),Wn=n(y,"LI",{});var CA=s(Wn);lQ=n(CA,"STRONG",{});var Cyr=s(lQ);nGe=r(Cyr,"distilbert"),Cyr.forEach(t),sGe=r(CA," \u2014 "),w9=n(CA,"A",{href:!0});var Myr=s(w9);lGe=r(Myr,"DistilBertTokenizer"),Myr.forEach(t),iGe=r(CA," or "),A9=n(CA,"A",{href:!0});var Eyr=s(A9);dGe=r(Eyr,"DistilBertTokenizerFast"),Eyr.forEach(t),cGe=r(CA," (DistilBERT model)"),CA.forEach(t),fGe=i(y),Qn=n(y,"LI",{});var MA=s(Qn);iQ=n(MA,"STRONG",{});var yyr=s(iQ);mGe=r(yyr,"dpr"),yyr.forEach(t),gGe=r(MA," \u2014 "),L9=n(MA,"A",{href:!0});var wyr=s(L9);hGe=r(wyr,"DPRQuestionEncoderTokenizer"),wyr.forEach(t),pGe=r(MA," or "),B9=n(MA,"A",{href:!0});var Ayr=s(B9);_Ge=r(Ayr,"DPRQuestionEncoderTokenizerFast"),Ayr.forEach(t),uGe=r(MA," (DPR model)"),MA.forEach(t),bGe=i(y),Hn=n(y,"LI",{});var EA=s(Hn);dQ=n(EA,"STRONG",{});var Lyr=s(dQ);vGe=r(Lyr,"electra"),Lyr.forEach(t),TGe=r(EA," \u2014 "),x9=n(EA,"A",{href:!0});var Byr=s(x9);FGe=r(Byr,"ElectraTokenizer"),Byr.forEach(t),CGe=r(EA," or "),k9=n(EA,"A",{href:!0});var xyr=s(k9);MGe=r(xyr,"ElectraTokenizerFast"),xyr.forEach(t),EGe=r(EA," (ELECTRA model)"),EA.forEach(t),yGe=i(y),mg=n(y,"LI",{});var Y2e=s(mg);cQ=n(Y2e,"STRONG",{});var kyr=s(cQ);wGe=r(kyr,"flaubert"),kyr.forEach(t),AGe=r(Y2e," \u2014 "),R9=n(Y2e,"A",{href:!0});var Ryr=s(R9);LGe=r(Ryr,"FlaubertTokenizer"),Ryr.forEach(t),BGe=r(Y2e," (FlauBERT model)"),Y2e.forEach(t),xGe=i(y),Un=n(y,"LI",{});var yA=s(Un);fQ=n(yA,"STRONG",{});var Syr=s(fQ);kGe=r(Syr,"fnet"),Syr.forEach(t),RGe=r(yA," \u2014 "),S9=n(yA,"A",{href:!0});var Pyr=s(S9);SGe=r(Pyr,"FNetTokenizer"),Pyr.forEach(t),PGe=r(yA," or "),P9=n(yA,"A",{href:!0});var $yr=s(P9);$Ge=r($yr,"FNetTokenizerFast"),$yr.forEach(t),IGe=r(yA," (FNet model)"),yA.forEach(t),jGe=i(y),gg=n(y,"LI",{});var K2e=s(gg);mQ=n(K2e,"STRONG",{});var Iyr=s(mQ);NGe=r(Iyr,"fsmt"),Iyr.forEach(t),DGe=r(K2e," \u2014 "),$9=n(K2e,"A",{href:!0});var jyr=s($9);qGe=r(jyr,"FSMTTokenizer"),jyr.forEach(t),GGe=r(K2e," (FairSeq Machine-Translation model)"),K2e.forEach(t),OGe=i(y),Jn=n(y,"LI",{});var wA=s(Jn);gQ=n(wA,"STRONG",{});var Nyr=s(gQ);XGe=r(Nyr,"funnel"),Nyr.forEach(t),zGe=r(wA," \u2014 "),I9=n(wA,"A",{href:!0});var Dyr=s(I9);VGe=r(Dyr,"FunnelTokenizer"),Dyr.forEach(t),WGe=r(wA," or "),j9=n(wA,"A",{href:!0});var qyr=s(j9);QGe=r(qyr,"FunnelTokenizerFast"),qyr.forEach(t),HGe=r(wA," (Funnel Transformer model)"),wA.forEach(t),UGe=i(y),Yn=n(y,"LI",{});var AA=s(Yn);hQ=n(AA,"STRONG",{});var Gyr=s(hQ);JGe=r(Gyr,"gpt2"),Gyr.forEach(t),YGe=r(AA," \u2014 "),N9=n(AA,"A",{href:!0});var Oyr=s(N9);KGe=r(Oyr,"GPT2Tokenizer"),Oyr.forEach(t),ZGe=r(AA," or "),D9=n(AA,"A",{href:!0});var Xyr=s(D9);eOe=r(Xyr,"GPT2TokenizerFast"),Xyr.forEach(t),oOe=r(AA," (OpenAI GPT-2 model)"),AA.forEach(t),rOe=i(y),Kn=n(y,"LI",{});var LA=s(Kn);pQ=n(LA,"STRONG",{});var zyr=s(pQ);tOe=r(zyr,"gpt_neo"),zyr.forEach(t),aOe=r(LA," \u2014 "),q9=n(LA,"A",{href:!0});var Vyr=s(q9);nOe=r(Vyr,"GPT2Tokenizer"),Vyr.forEach(t),sOe=r(LA," or "),G9=n(LA,"A",{href:!0});var Wyr=s(G9);lOe=r(Wyr,"GPT2TokenizerFast"),Wyr.forEach(t),iOe=r(LA," (GPT Neo model)"),LA.forEach(t),dOe=i(y),Zn=n(y,"LI",{});var BA=s(Zn);_Q=n(BA,"STRONG",{});var Qyr=s(_Q);cOe=r(Qyr,"herbert"),Qyr.forEach(t),fOe=r(BA," \u2014 "),O9=n(BA,"A",{href:!0});var Hyr=s(O9);mOe=r(Hyr,"HerbertTokenizer"),Hyr.forEach(t),gOe=r(BA," or "),X9=n(BA,"A",{href:!0});var Uyr=s(X9);hOe=r(Uyr,"HerbertTokenizerFast"),Uyr.forEach(t),pOe=r(BA," (HerBERT model)"),BA.forEach(t),_Oe=i(y),hg=n(y,"LI",{});var Z2e=s(hg);uQ=n(Z2e,"STRONG",{});var Jyr=s(uQ);uOe=r(Jyr,"hubert"),Jyr.forEach(t),bOe=r(Z2e," \u2014 "),z9=n(Z2e,"A",{href:!0});var Yyr=s(z9);vOe=r(Yyr,"Wav2Vec2CTCTokenizer"),Yyr.forEach(t),TOe=r(Z2e," (Hubert model)"),Z2e.forEach(t),FOe=i(y),es=n(y,"LI",{});var xA=s(es);bQ=n(xA,"STRONG",{});var Kyr=s(bQ);COe=r(Kyr,"ibert"),Kyr.forEach(t),MOe=r(xA," \u2014 "),V9=n(xA,"A",{href:!0});var Zyr=s(V9);EOe=r(Zyr,"RobertaTokenizer"),Zyr.forEach(t),yOe=r(xA," or "),W9=n(xA,"A",{href:!0});var ewr=s(W9);wOe=r(ewr,"RobertaTokenizerFast"),ewr.forEach(t),AOe=r(xA," (I-BERT model)"),xA.forEach(t),LOe=i(y),os=n(y,"LI",{});var kA=s(os);vQ=n(kA,"STRONG",{});var owr=s(vQ);BOe=r(owr,"layoutlm"),owr.forEach(t),xOe=r(kA," \u2014 "),Q9=n(kA,"A",{href:!0});var rwr=s(Q9);kOe=r(rwr,"LayoutLMTokenizer"),rwr.forEach(t),ROe=r(kA," or "),H9=n(kA,"A",{href:!0});var twr=s(H9);SOe=r(twr,"LayoutLMTokenizerFast"),twr.forEach(t),POe=r(kA," (LayoutLM model)"),kA.forEach(t),$Oe=i(y),rs=n(y,"LI",{});var RA=s(rs);TQ=n(RA,"STRONG",{});var awr=s(TQ);IOe=r(awr,"layoutlmv2"),awr.forEach(t),jOe=r(RA," \u2014 "),U9=n(RA,"A",{href:!0});var nwr=s(U9);NOe=r(nwr,"LayoutLMv2Tokenizer"),nwr.forEach(t),DOe=r(RA," or "),J9=n(RA,"A",{href:!0});var swr=s(J9);qOe=r(swr,"LayoutLMv2TokenizerFast"),swr.forEach(t),GOe=r(RA," (LayoutLMv2 model)"),RA.forEach(t),OOe=i(y),ts=n(y,"LI",{});var SA=s(ts);FQ=n(SA,"STRONG",{});var lwr=s(FQ);XOe=r(lwr,"layoutxlm"),lwr.forEach(t),zOe=r(SA," \u2014 "),Y9=n(SA,"A",{href:!0});var iwr=s(Y9);VOe=r(iwr,"LayoutXLMTokenizer"),iwr.forEach(t),WOe=r(SA," or "),K9=n(SA,"A",{href:!0});var dwr=s(K9);QOe=r(dwr,"LayoutXLMTokenizerFast"),dwr.forEach(t),HOe=r(SA," (LayoutXLM model)"),SA.forEach(t),UOe=i(y),as=n(y,"LI",{});var PA=s(as);CQ=n(PA,"STRONG",{});var cwr=s(CQ);JOe=r(cwr,"led"),cwr.forEach(t),YOe=r(PA," \u2014 "),Z9=n(PA,"A",{href:!0});var fwr=s(Z9);KOe=r(fwr,"LEDTokenizer"),fwr.forEach(t),ZOe=r(PA," or "),eB=n(PA,"A",{href:!0});var mwr=s(eB);eXe=r(mwr,"LEDTokenizerFast"),mwr.forEach(t),oXe=r(PA," (LED model)"),PA.forEach(t),rXe=i(y),ns=n(y,"LI",{});var $A=s(ns);MQ=n($A,"STRONG",{});var gwr=s(MQ);tXe=r(gwr,"longformer"),gwr.forEach(t),aXe=r($A," \u2014 "),oB=n($A,"A",{href:!0});var hwr=s(oB);nXe=r(hwr,"LongformerTokenizer"),hwr.forEach(t),sXe=r($A," or "),rB=n($A,"A",{href:!0});var pwr=s(rB);lXe=r(pwr,"LongformerTokenizerFast"),pwr.forEach(t),iXe=r($A," (Longformer model)"),$A.forEach(t),dXe=i(y),pg=n(y,"LI",{});var eve=s(pg);EQ=n(eve,"STRONG",{});var _wr=s(EQ);cXe=r(_wr,"luke"),_wr.forEach(t),fXe=r(eve," \u2014 "),tB=n(eve,"A",{href:!0});var uwr=s(tB);mXe=r(uwr,"LukeTokenizer"),uwr.forEach(t),gXe=r(eve," (LUKE model)"),eve.forEach(t),hXe=i(y),ss=n(y,"LI",{});var IA=s(ss);yQ=n(IA,"STRONG",{});var bwr=s(yQ);pXe=r(bwr,"lxmert"),bwr.forEach(t),_Xe=r(IA," \u2014 "),aB=n(IA,"A",{href:!0});var vwr=s(aB);uXe=r(vwr,"LxmertTokenizer"),vwr.forEach(t),bXe=r(IA," or "),nB=n(IA,"A",{href:!0});var Twr=s(nB);vXe=r(Twr,"LxmertTokenizerFast"),Twr.forEach(t),TXe=r(IA," (LXMERT model)"),IA.forEach(t),FXe=i(y),_g=n(y,"LI",{});var ove=s(_g);wQ=n(ove,"STRONG",{});var Fwr=s(wQ);CXe=r(Fwr,"m2m_100"),Fwr.forEach(t),MXe=r(ove," \u2014 "),sB=n(ove,"A",{href:!0});var Cwr=s(sB);EXe=r(Cwr,"M2M100Tokenizer"),Cwr.forEach(t),yXe=r(ove," (M2M100 model)"),ove.forEach(t),wXe=i(y),ug=n(y,"LI",{});var rve=s(ug);AQ=n(rve,"STRONG",{});var Mwr=s(AQ);AXe=r(Mwr,"marian"),Mwr.forEach(t),LXe=r(rve," \u2014 "),lB=n(rve,"A",{href:!0});var Ewr=s(lB);BXe=r(Ewr,"MarianTokenizer"),Ewr.forEach(t),xXe=r(rve," (Marian model)"),rve.forEach(t),kXe=i(y),ls=n(y,"LI",{});var jA=s(ls);LQ=n(jA,"STRONG",{});var ywr=s(LQ);RXe=r(ywr,"mbart"),ywr.forEach(t),SXe=r(jA," \u2014 "),iB=n(jA,"A",{href:!0});var wwr=s(iB);PXe=r(wwr,"MBartTokenizer"),wwr.forEach(t),$Xe=r(jA," or "),dB=n(jA,"A",{href:!0});var Awr=s(dB);IXe=r(Awr,"MBartTokenizerFast"),Awr.forEach(t),jXe=r(jA," (mBART model)"),jA.forEach(t),NXe=i(y),is=n(y,"LI",{});var NA=s(is);BQ=n(NA,"STRONG",{});var Lwr=s(BQ);DXe=r(Lwr,"mbart50"),Lwr.forEach(t),qXe=r(NA," \u2014 "),cB=n(NA,"A",{href:!0});var Bwr=s(cB);GXe=r(Bwr,"MBart50Tokenizer"),Bwr.forEach(t),OXe=r(NA," or "),fB=n(NA,"A",{href:!0});var xwr=s(fB);XXe=r(xwr,"MBart50TokenizerFast"),xwr.forEach(t),zXe=r(NA," (mBART-50 model)"),NA.forEach(t),VXe=i(y),bg=n(y,"LI",{});var tve=s(bg);xQ=n(tve,"STRONG",{});var kwr=s(xQ);WXe=r(kwr,"mluke"),kwr.forEach(t),QXe=r(tve," \u2014 "),mB=n(tve,"A",{href:!0});var Rwr=s(mB);HXe=r(Rwr,"MLukeTokenizer"),Rwr.forEach(t),UXe=r(tve," (mLUKE model)"),tve.forEach(t),JXe=i(y),ds=n(y,"LI",{});var DA=s(ds);kQ=n(DA,"STRONG",{});var Swr=s(kQ);YXe=r(Swr,"mobilebert"),Swr.forEach(t),KXe=r(DA," \u2014 "),gB=n(DA,"A",{href:!0});var Pwr=s(gB);ZXe=r(Pwr,"MobileBertTokenizer"),Pwr.forEach(t),eze=r(DA," or "),hB=n(DA,"A",{href:!0});var $wr=s(hB);oze=r($wr,"MobileBertTokenizerFast"),$wr.forEach(t),rze=r(DA," (MobileBERT model)"),DA.forEach(t),tze=i(y),cs=n(y,"LI",{});var qA=s(cs);RQ=n(qA,"STRONG",{});var Iwr=s(RQ);aze=r(Iwr,"mpnet"),Iwr.forEach(t),nze=r(qA," \u2014 "),pB=n(qA,"A",{href:!0});var jwr=s(pB);sze=r(jwr,"MPNetTokenizer"),jwr.forEach(t),lze=r(qA," or "),_B=n(qA,"A",{href:!0});var Nwr=s(_B);ize=r(Nwr,"MPNetTokenizerFast"),Nwr.forEach(t),dze=r(qA," (MPNet model)"),qA.forEach(t),cze=i(y),fs=n(y,"LI",{});var GA=s(fs);SQ=n(GA,"STRONG",{});var Dwr=s(SQ);fze=r(Dwr,"mt5"),Dwr.forEach(t),mze=r(GA," \u2014 "),uB=n(GA,"A",{href:!0});var qwr=s(uB);gze=r(qwr,"MT5Tokenizer"),qwr.forEach(t),hze=r(GA," or "),bB=n(GA,"A",{href:!0});var Gwr=s(bB);pze=r(Gwr,"MT5TokenizerFast"),Gwr.forEach(t),_ze=r(GA," (mT5 model)"),GA.forEach(t),uze=i(y),ms=n(y,"LI",{});var OA=s(ms);PQ=n(OA,"STRONG",{});var Owr=s(PQ);bze=r(Owr,"openai-gpt"),Owr.forEach(t),vze=r(OA," \u2014 "),vB=n(OA,"A",{href:!0});var Xwr=s(vB);Tze=r(Xwr,"OpenAIGPTTokenizer"),Xwr.forEach(t),Fze=r(OA," or "),TB=n(OA,"A",{href:!0});var zwr=s(TB);Cze=r(zwr,"OpenAIGPTTokenizerFast"),zwr.forEach(t),Mze=r(OA," (OpenAI GPT model)"),OA.forEach(t),Eze=i(y),gs=n(y,"LI",{});var XA=s(gs);$Q=n(XA,"STRONG",{});var Vwr=s($Q);yze=r(Vwr,"pegasus"),Vwr.forEach(t),wze=r(XA," \u2014 "),FB=n(XA,"A",{href:!0});var Wwr=s(FB);Aze=r(Wwr,"PegasusTokenizer"),Wwr.forEach(t),Lze=r(XA," or "),CB=n(XA,"A",{href:!0});var Qwr=s(CB);Bze=r(Qwr,"PegasusTokenizerFast"),Qwr.forEach(t),xze=r(XA," (Pegasus model)"),XA.forEach(t),kze=i(y),vg=n(y,"LI",{});var ave=s(vg);IQ=n(ave,"STRONG",{});var Hwr=s(IQ);Rze=r(Hwr,"perceiver"),Hwr.forEach(t),Sze=r(ave," \u2014 "),MB=n(ave,"A",{href:!0});var Uwr=s(MB);Pze=r(Uwr,"PerceiverTokenizer"),Uwr.forEach(t),$ze=r(ave," (Perceiver model)"),ave.forEach(t),Ize=i(y),Tg=n(y,"LI",{});var nve=s(Tg);jQ=n(nve,"STRONG",{});var Jwr=s(jQ);jze=r(Jwr,"phobert"),Jwr.forEach(t),Nze=r(nve," \u2014 "),EB=n(nve,"A",{href:!0});var Ywr=s(EB);Dze=r(Ywr,"PhobertTokenizer"),Ywr.forEach(t),qze=r(nve," (PhoBERT model)"),nve.forEach(t),Gze=i(y),Fg=n(y,"LI",{});var sve=s(Fg);NQ=n(sve,"STRONG",{});var Kwr=s(NQ);Oze=r(Kwr,"prophetnet"),Kwr.forEach(t),Xze=r(sve," \u2014 "),yB=n(sve,"A",{href:!0});var Zwr=s(yB);zze=r(Zwr,"ProphetNetTokenizer"),Zwr.forEach(t),Vze=r(sve," (ProphetNet model)"),sve.forEach(t),Wze=i(y),hs=n(y,"LI",{});var zA=s(hs);DQ=n(zA,"STRONG",{});var eAr=s(DQ);Qze=r(eAr,"qdqbert"),eAr.forEach(t),Hze=r(zA," \u2014 "),wB=n(zA,"A",{href:!0});var oAr=s(wB);Uze=r(oAr,"BertTokenizer"),oAr.forEach(t),Jze=r(zA," or "),AB=n(zA,"A",{href:!0});var rAr=s(AB);Yze=r(rAr,"BertTokenizerFast"),rAr.forEach(t),Kze=r(zA," (QDQBert model)"),zA.forEach(t),Zze=i(y),Cg=n(y,"LI",{});var lve=s(Cg);qQ=n(lve,"STRONG",{});var tAr=s(qQ);eVe=r(tAr,"rag"),tAr.forEach(t),oVe=r(lve," \u2014 "),LB=n(lve,"A",{href:!0});var aAr=s(LB);rVe=r(aAr,"RagTokenizer"),aAr.forEach(t),tVe=r(lve," (RAG model)"),lve.forEach(t),aVe=i(y),ps=n(y,"LI",{});var VA=s(ps);GQ=n(VA,"STRONG",{});var nAr=s(GQ);nVe=r(nAr,"reformer"),nAr.forEach(t),sVe=r(VA," \u2014 "),BB=n(VA,"A",{href:!0});var sAr=s(BB);lVe=r(sAr,"ReformerTokenizer"),sAr.forEach(t),iVe=r(VA," or "),xB=n(VA,"A",{href:!0});var lAr=s(xB);dVe=r(lAr,"ReformerTokenizerFast"),lAr.forEach(t),cVe=r(VA," (Reformer model)"),VA.forEach(t),fVe=i(y),_s=n(y,"LI",{});var WA=s(_s);OQ=n(WA,"STRONG",{});var iAr=s(OQ);mVe=r(iAr,"rembert"),iAr.forEach(t),gVe=r(WA," \u2014 "),kB=n(WA,"A",{href:!0});var dAr=s(kB);hVe=r(dAr,"RemBertTokenizer"),dAr.forEach(t),pVe=r(WA," or "),RB=n(WA,"A",{href:!0});var cAr=s(RB);_Ve=r(cAr,"RemBertTokenizerFast"),cAr.forEach(t),uVe=r(WA," (RemBERT model)"),WA.forEach(t),bVe=i(y),us=n(y,"LI",{});var QA=s(us);XQ=n(QA,"STRONG",{});var fAr=s(XQ);vVe=r(fAr,"retribert"),fAr.forEach(t),TVe=r(QA," \u2014 "),SB=n(QA,"A",{href:!0});var mAr=s(SB);FVe=r(mAr,"RetriBertTokenizer"),mAr.forEach(t),CVe=r(QA," or "),PB=n(QA,"A",{href:!0});var gAr=s(PB);MVe=r(gAr,"RetriBertTokenizerFast"),gAr.forEach(t),EVe=r(QA," (RetriBERT model)"),QA.forEach(t),yVe=i(y),bs=n(y,"LI",{});var HA=s(bs);zQ=n(HA,"STRONG",{});var hAr=s(zQ);wVe=r(hAr,"roberta"),hAr.forEach(t),AVe=r(HA," \u2014 "),$B=n(HA,"A",{href:!0});var pAr=s($B);LVe=r(pAr,"RobertaTokenizer"),pAr.forEach(t),BVe=r(HA," or "),IB=n(HA,"A",{href:!0});var _Ar=s(IB);xVe=r(_Ar,"RobertaTokenizerFast"),_Ar.forEach(t),kVe=r(HA," (RoBERTa model)"),HA.forEach(t),RVe=i(y),vs=n(y,"LI",{});var UA=s(vs);VQ=n(UA,"STRONG",{});var uAr=s(VQ);SVe=r(uAr,"roformer"),uAr.forEach(t),PVe=r(UA," \u2014 "),jB=n(UA,"A",{href:!0});var bAr=s(jB);$Ve=r(bAr,"RoFormerTokenizer"),bAr.forEach(t),IVe=r(UA," or "),NB=n(UA,"A",{href:!0});var vAr=s(NB);jVe=r(vAr,"RoFormerTokenizerFast"),vAr.forEach(t),NVe=r(UA," (RoFormer model)"),UA.forEach(t),DVe=i(y),Mg=n(y,"LI",{});var ive=s(Mg);WQ=n(ive,"STRONG",{});var TAr=s(WQ);qVe=r(TAr,"speech_to_text"),TAr.forEach(t),GVe=r(ive," \u2014 "),DB=n(ive,"A",{href:!0});var FAr=s(DB);OVe=r(FAr,"Speech2TextTokenizer"),FAr.forEach(t),XVe=r(ive," (Speech2Text model)"),ive.forEach(t),zVe=i(y),Eg=n(y,"LI",{});var dve=s(Eg);QQ=n(dve,"STRONG",{});var CAr=s(QQ);VVe=r(CAr,"speech_to_text_2"),CAr.forEach(t),WVe=r(dve," \u2014 "),qB=n(dve,"A",{href:!0});var MAr=s(qB);QVe=r(MAr,"Speech2Text2Tokenizer"),MAr.forEach(t),HVe=r(dve," (Speech2Text2 model)"),dve.forEach(t),UVe=i(y),Ts=n(y,"LI",{});var JA=s(Ts);HQ=n(JA,"STRONG",{});var EAr=s(HQ);JVe=r(EAr,"splinter"),EAr.forEach(t),YVe=r(JA," \u2014 "),GB=n(JA,"A",{href:!0});var yAr=s(GB);KVe=r(yAr,"SplinterTokenizer"),yAr.forEach(t),ZVe=r(JA," or "),OB=n(JA,"A",{href:!0});var wAr=s(OB);eWe=r(wAr,"SplinterTokenizerFast"),wAr.forEach(t),oWe=r(JA," (Splinter model)"),JA.forEach(t),rWe=i(y),Fs=n(y,"LI",{});var YA=s(Fs);UQ=n(YA,"STRONG",{});var AAr=s(UQ);tWe=r(AAr,"squeezebert"),AAr.forEach(t),aWe=r(YA," \u2014 "),XB=n(YA,"A",{href:!0});var LAr=s(XB);nWe=r(LAr,"SqueezeBertTokenizer"),LAr.forEach(t),sWe=r(YA," or "),zB=n(YA,"A",{href:!0});var BAr=s(zB);lWe=r(BAr,"SqueezeBertTokenizerFast"),BAr.forEach(t),iWe=r(YA," (SqueezeBERT model)"),YA.forEach(t),dWe=i(y),Cs=n(y,"LI",{});var KA=s(Cs);JQ=n(KA,"STRONG",{});var xAr=s(JQ);cWe=r(xAr,"t5"),xAr.forEach(t),fWe=r(KA," \u2014 "),VB=n(KA,"A",{href:!0});var kAr=s(VB);mWe=r(kAr,"T5Tokenizer"),kAr.forEach(t),gWe=r(KA," or "),WB=n(KA,"A",{href:!0});var RAr=s(WB);hWe=r(RAr,"T5TokenizerFast"),RAr.forEach(t),pWe=r(KA," (T5 model)"),KA.forEach(t),_We=i(y),yg=n(y,"LI",{});var cve=s(yg);YQ=n(cve,"STRONG",{});var SAr=s(YQ);uWe=r(SAr,"tapas"),SAr.forEach(t),bWe=r(cve," \u2014 "),QB=n(cve,"A",{href:!0});var PAr=s(QB);vWe=r(PAr,"TapasTokenizer"),PAr.forEach(t),TWe=r(cve," (TAPAS model)"),cve.forEach(t),FWe=i(y),wg=n(y,"LI",{});var fve=s(wg);KQ=n(fve,"STRONG",{});var $Ar=s(KQ);CWe=r($Ar,"transfo-xl"),$Ar.forEach(t),MWe=r(fve," \u2014 "),HB=n(fve,"A",{href:!0});var IAr=s(HB);EWe=r(IAr,"TransfoXLTokenizer"),IAr.forEach(t),yWe=r(fve," (Transformer-XL model)"),fve.forEach(t),wWe=i(y),Ag=n(y,"LI",{});var mve=s(Ag);ZQ=n(mve,"STRONG",{});var jAr=s(ZQ);AWe=r(jAr,"wav2vec2"),jAr.forEach(t),LWe=r(mve," \u2014 "),UB=n(mve,"A",{href:!0});var NAr=s(UB);BWe=r(NAr,"Wav2Vec2CTCTokenizer"),NAr.forEach(t),xWe=r(mve," (Wav2Vec2 model)"),mve.forEach(t),kWe=i(y),Lg=n(y,"LI",{});var gve=s(Lg);eH=n(gve,"STRONG",{});var DAr=s(eH);RWe=r(DAr,"wav2vec2_phoneme"),DAr.forEach(t),SWe=r(gve," \u2014 "),JB=n(gve,"A",{href:!0});var qAr=s(JB);PWe=r(qAr,"Wav2Vec2PhonemeCTCTokenizer"),qAr.forEach(t),$We=r(gve," (Wav2Vec2Phoneme model)"),gve.forEach(t),IWe=i(y),Ms=n(y,"LI",{});var ZA=s(Ms);oH=n(ZA,"STRONG",{});var GAr=s(oH);jWe=r(GAr,"xglm"),GAr.forEach(t),NWe=r(ZA," \u2014 "),YB=n(ZA,"A",{href:!0});var OAr=s(YB);DWe=r(OAr,"XGLMTokenizer"),OAr.forEach(t),qWe=r(ZA," or "),KB=n(ZA,"A",{href:!0});var XAr=s(KB);GWe=r(XAr,"XGLMTokenizerFast"),XAr.forEach(t),OWe=r(ZA," (XGLM model)"),ZA.forEach(t),XWe=i(y),Bg=n(y,"LI",{});var hve=s(Bg);rH=n(hve,"STRONG",{});var zAr=s(rH);zWe=r(zAr,"xlm"),zAr.forEach(t),VWe=r(hve," \u2014 "),ZB=n(hve,"A",{href:!0});var VAr=s(ZB);WWe=r(VAr,"XLMTokenizer"),VAr.forEach(t),QWe=r(hve," (XLM model)"),hve.forEach(t),HWe=i(y),xg=n(y,"LI",{});var pve=s(xg);tH=n(pve,"STRONG",{});var WAr=s(tH);UWe=r(WAr,"xlm-prophetnet"),WAr.forEach(t),JWe=r(pve," \u2014 "),ex=n(pve,"A",{href:!0});var QAr=s(ex);YWe=r(QAr,"XLMProphetNetTokenizer"),QAr.forEach(t),KWe=r(pve," (XLMProphetNet model)"),pve.forEach(t),ZWe=i(y),Es=n(y,"LI",{});var e0=s(Es);aH=n(e0,"STRONG",{});var HAr=s(aH);eQe=r(HAr,"xlm-roberta"),HAr.forEach(t),oQe=r(e0," \u2014 "),ox=n(e0,"A",{href:!0});var UAr=s(ox);rQe=r(UAr,"XLMRobertaTokenizer"),UAr.forEach(t),tQe=r(e0," or "),rx=n(e0,"A",{href:!0});var JAr=s(rx);aQe=r(JAr,"XLMRobertaTokenizerFast"),JAr.forEach(t),nQe=r(e0," (XLM-RoBERTa model)"),e0.forEach(t),sQe=i(y),ys=n(y,"LI",{});var o0=s(ys);nH=n(o0,"STRONG",{});var YAr=s(nH);lQe=r(YAr,"xlnet"),YAr.forEach(t),iQe=r(o0," \u2014 "),tx=n(o0,"A",{href:!0});var KAr=s(tx);dQe=r(KAr,"XLNetTokenizer"),KAr.forEach(t),cQe=r(o0," or "),ax=n(o0,"A",{href:!0});var ZAr=s(ax);fQe=r(ZAr,"XLNetTokenizerFast"),ZAr.forEach(t),mQe=r(o0," (XLNet model)"),o0.forEach(t),y.forEach(t),gQe=i(na),sH=n(na,"P",{});var e0r=s(sH);hQe=r(e0r,"Examples:"),e0r.forEach(t),pQe=i(na),m(WC.$$.fragment,na),na.forEach(t),_Qe=i(xs),kg=n(xs,"DIV",{class:!0});var ALe=s(kg);m(QC.$$.fragment,ALe),uQe=i(ALe),lH=n(ALe,"P",{});var o0r=s(lH);bQe=r(o0r,"Register a new tokenizer in this mapping."),o0r.forEach(t),ALe.forEach(t),xs.forEach(t),AAe=i(d),xi=n(d,"H2",{class:!0});var LLe=s(xi);Rg=n(LLe,"A",{id:!0,class:!0,href:!0});var r0r=s(Rg);iH=n(r0r,"SPAN",{});var t0r=s(iH);m(HC.$$.fragment,t0r),t0r.forEach(t),r0r.forEach(t),vQe=i(LLe),dH=n(LLe,"SPAN",{});var a0r=s(dH);TQe=r(a0r,"AutoFeatureExtractor"),a0r.forEach(t),LLe.forEach(t),LAe=i(d),Oo=n(d,"DIV",{class:!0});var ks=s(Oo);m(UC.$$.fragment,ks),FQe=i(ks),JC=n(ks,"P",{});var BLe=s(JC);CQe=r(BLe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),nx=n(BLe,"A",{href:!0});var n0r=s(nx);MQe=r(n0r,"AutoFeatureExtractor.from_pretrained()"),n0r.forEach(t),EQe=r(BLe," class method."),BLe.forEach(t),yQe=i(ks),YC=n(ks,"P",{});var xLe=s(YC);wQe=r(xLe,"This class cannot be instantiated directly using "),cH=n(xLe,"CODE",{});var s0r=s(cH);AQe=r(s0r,"__init__()"),s0r.forEach(t),LQe=r(xLe," (throws an error)."),xLe.forEach(t),BQe=i(ks),Le=n(ks,"DIV",{class:!0});var Lt=s(Le);m(KC.$$.fragment,Lt),xQe=i(Lt),fH=n(Lt,"P",{});var l0r=s(fH);kQe=r(l0r,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),l0r.forEach(t),RQe=i(Lt),Pa=n(Lt,"P",{});var PF=s(Pa);SQe=r(PF,"The feature extractor class to instantiate is selected based on the "),mH=n(PF,"CODE",{});var i0r=s(mH);PQe=r(i0r,"model_type"),i0r.forEach(t),$Qe=r(PF,` property of the config object
(either passed as an argument or loaded from `),gH=n(PF,"CODE",{});var d0r=s(gH);IQe=r(d0r,"pretrained_model_name_or_path"),d0r.forEach(t),jQe=r(PF,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),hH=n(PF,"CODE",{});var c0r=s(hH);NQe=r(c0r,"pretrained_model_name_or_path"),c0r.forEach(t),DQe=r(PF,":"),PF.forEach(t),qQe=i(Lt),se=n(Lt,"UL",{});var de=s(se);Sg=n(de,"LI",{});var _ve=s(Sg);pH=n(_ve,"STRONG",{});var f0r=s(pH);GQe=r(f0r,"beit"),f0r.forEach(t),OQe=r(_ve," \u2014 "),sx=n(_ve,"A",{href:!0});var m0r=s(sx);XQe=r(m0r,"BeitFeatureExtractor"),m0r.forEach(t),zQe=r(_ve," (BEiT model)"),_ve.forEach(t),VQe=i(de),Pg=n(de,"LI",{});var uve=s(Pg);_H=n(uve,"STRONG",{});var g0r=s(_H);WQe=r(g0r,"clip"),g0r.forEach(t),QQe=r(uve," \u2014 "),lx=n(uve,"A",{href:!0});var h0r=s(lx);HQe=r(h0r,"CLIPFeatureExtractor"),h0r.forEach(t),UQe=r(uve," (CLIP model)"),uve.forEach(t),JQe=i(de),$g=n(de,"LI",{});var bve=s($g);uH=n(bve,"STRONG",{});var p0r=s(uH);YQe=r(p0r,"convnext"),p0r.forEach(t),KQe=r(bve," \u2014 "),ix=n(bve,"A",{href:!0});var _0r=s(ix);ZQe=r(_0r,"ConvNextFeatureExtractor"),_0r.forEach(t),eHe=r(bve," (ConvNext model)"),bve.forEach(t),oHe=i(de),Ig=n(de,"LI",{});var vve=s(Ig);bH=n(vve,"STRONG",{});var u0r=s(bH);rHe=r(u0r,"deit"),u0r.forEach(t),tHe=r(vve," \u2014 "),dx=n(vve,"A",{href:!0});var b0r=s(dx);aHe=r(b0r,"DeiTFeatureExtractor"),b0r.forEach(t),nHe=r(vve," (DeiT model)"),vve.forEach(t),sHe=i(de),jg=n(de,"LI",{});var Tve=s(jg);vH=n(Tve,"STRONG",{});var v0r=s(vH);lHe=r(v0r,"detr"),v0r.forEach(t),iHe=r(Tve," \u2014 "),cx=n(Tve,"A",{href:!0});var T0r=s(cx);dHe=r(T0r,"DetrFeatureExtractor"),T0r.forEach(t),cHe=r(Tve," (DETR model)"),Tve.forEach(t),fHe=i(de),Ng=n(de,"LI",{});var Fve=s(Ng);TH=n(Fve,"STRONG",{});var F0r=s(TH);mHe=r(F0r,"hubert"),F0r.forEach(t),gHe=r(Fve," \u2014 "),fx=n(Fve,"A",{href:!0});var C0r=s(fx);hHe=r(C0r,"Wav2Vec2FeatureExtractor"),C0r.forEach(t),pHe=r(Fve," (Hubert model)"),Fve.forEach(t),_He=i(de),Dg=n(de,"LI",{});var Cve=s(Dg);FH=n(Cve,"STRONG",{});var M0r=s(FH);uHe=r(M0r,"layoutlmv2"),M0r.forEach(t),bHe=r(Cve," \u2014 "),mx=n(Cve,"A",{href:!0});var E0r=s(mx);vHe=r(E0r,"LayoutLMv2FeatureExtractor"),E0r.forEach(t),THe=r(Cve," (LayoutLMv2 model)"),Cve.forEach(t),FHe=i(de),qg=n(de,"LI",{});var Mve=s(qg);CH=n(Mve,"STRONG",{});var y0r=s(CH);CHe=r(y0r,"perceiver"),y0r.forEach(t),MHe=r(Mve," \u2014 "),gx=n(Mve,"A",{href:!0});var w0r=s(gx);EHe=r(w0r,"PerceiverFeatureExtractor"),w0r.forEach(t),yHe=r(Mve," (Perceiver model)"),Mve.forEach(t),wHe=i(de),Gg=n(de,"LI",{});var Eve=s(Gg);MH=n(Eve,"STRONG",{});var A0r=s(MH);AHe=r(A0r,"segformer"),A0r.forEach(t),LHe=r(Eve," \u2014 "),hx=n(Eve,"A",{href:!0});var L0r=s(hx);BHe=r(L0r,"SegformerFeatureExtractor"),L0r.forEach(t),xHe=r(Eve," (SegFormer model)"),Eve.forEach(t),kHe=i(de),Og=n(de,"LI",{});var yve=s(Og);EH=n(yve,"STRONG",{});var B0r=s(EH);RHe=r(B0r,"speech_to_text"),B0r.forEach(t),SHe=r(yve," \u2014 "),px=n(yve,"A",{href:!0});var x0r=s(px);PHe=r(x0r,"Speech2TextFeatureExtractor"),x0r.forEach(t),$He=r(yve," (Speech2Text model)"),yve.forEach(t),IHe=i(de),Xg=n(de,"LI",{});var wve=s(Xg);yH=n(wve,"STRONG",{});var k0r=s(yH);jHe=r(k0r,"swin"),k0r.forEach(t),NHe=r(wve," \u2014 "),_x=n(wve,"A",{href:!0});var R0r=s(_x);DHe=r(R0r,"ViTFeatureExtractor"),R0r.forEach(t),qHe=r(wve," (Swin model)"),wve.forEach(t),GHe=i(de),zg=n(de,"LI",{});var Ave=s(zg);wH=n(Ave,"STRONG",{});var S0r=s(wH);OHe=r(S0r,"vit"),S0r.forEach(t),XHe=r(Ave," \u2014 "),ux=n(Ave,"A",{href:!0});var P0r=s(ux);zHe=r(P0r,"ViTFeatureExtractor"),P0r.forEach(t),VHe=r(Ave," (ViT model)"),Ave.forEach(t),WHe=i(de),Vg=n(de,"LI",{});var Lve=s(Vg);AH=n(Lve,"STRONG",{});var $0r=s(AH);QHe=r($0r,"vit_mae"),$0r.forEach(t),HHe=r(Lve," \u2014 "),bx=n(Lve,"A",{href:!0});var I0r=s(bx);UHe=r(I0r,"ViTFeatureExtractor"),I0r.forEach(t),JHe=r(Lve," (ViTMAE model)"),Lve.forEach(t),YHe=i(de),Wg=n(de,"LI",{});var Bve=s(Wg);LH=n(Bve,"STRONG",{});var j0r=s(LH);KHe=r(j0r,"wav2vec2"),j0r.forEach(t),ZHe=r(Bve," \u2014 "),vx=n(Bve,"A",{href:!0});var N0r=s(vx);eUe=r(N0r,"Wav2Vec2FeatureExtractor"),N0r.forEach(t),oUe=r(Bve," (Wav2Vec2 model)"),Bve.forEach(t),de.forEach(t),rUe=i(Lt),m(Qg.$$.fragment,Lt),tUe=i(Lt),BH=n(Lt,"P",{});var D0r=s(BH);aUe=r(D0r,"Examples:"),D0r.forEach(t),nUe=i(Lt),m(ZC.$$.fragment,Lt),Lt.forEach(t),sUe=i(ks),Hg=n(ks,"DIV",{class:!0});var kLe=s(Hg);m(e4.$$.fragment,kLe),lUe=i(kLe),xH=n(kLe,"P",{});var q0r=s(xH);iUe=r(q0r,"Register a new feature extractor for this class."),q0r.forEach(t),kLe.forEach(t),ks.forEach(t),BAe=i(d),ki=n(d,"H2",{class:!0});var RLe=s(ki);Ug=n(RLe,"A",{id:!0,class:!0,href:!0});var G0r=s(Ug);kH=n(G0r,"SPAN",{});var O0r=s(kH);m(o4.$$.fragment,O0r),O0r.forEach(t),G0r.forEach(t),dUe=i(RLe),RH=n(RLe,"SPAN",{});var X0r=s(RH);cUe=r(X0r,"AutoProcessor"),X0r.forEach(t),RLe.forEach(t),xAe=i(d),Xo=n(d,"DIV",{class:!0});var Rs=s(Xo);m(r4.$$.fragment,Rs),fUe=i(Rs),t4=n(Rs,"P",{});var SLe=s(t4);mUe=r(SLe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),Tx=n(SLe,"A",{href:!0});var z0r=s(Tx);gUe=r(z0r,"AutoProcessor.from_pretrained()"),z0r.forEach(t),hUe=r(SLe," class method."),SLe.forEach(t),pUe=i(Rs),a4=n(Rs,"P",{});var PLe=s(a4);_Ue=r(PLe,"This class cannot be instantiated directly using "),SH=n(PLe,"CODE",{});var V0r=s(SH);uUe=r(V0r,"__init__()"),V0r.forEach(t),bUe=r(PLe," (throws an error)."),PLe.forEach(t),vUe=i(Rs),Be=n(Rs,"DIV",{class:!0});var Bt=s(Be);m(n4.$$.fragment,Bt),TUe=i(Bt),PH=n(Bt,"P",{});var W0r=s(PH);FUe=r(W0r,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),W0r.forEach(t),CUe=i(Bt),Ri=n(Bt,"P",{});var KO=s(Ri);MUe=r(KO,"The processor class to instantiate is selected based on the "),$H=n(KO,"CODE",{});var Q0r=s($H);EUe=r(Q0r,"model_type"),Q0r.forEach(t),yUe=r(KO,` property of the config object (either
passed as an argument or loaded from `),IH=n(KO,"CODE",{});var H0r=s(IH);wUe=r(H0r,"pretrained_model_name_or_path"),H0r.forEach(t),AUe=r(KO," if possible):"),KO.forEach(t),LUe=i(Bt),ye=n(Bt,"UL",{});var Io=s(ye);Jg=n(Io,"LI",{});var xve=s(Jg);jH=n(xve,"STRONG",{});var U0r=s(jH);BUe=r(U0r,"clip"),U0r.forEach(t),xUe=r(xve," \u2014 "),Fx=n(xve,"A",{href:!0});var J0r=s(Fx);kUe=r(J0r,"CLIPProcessor"),J0r.forEach(t),RUe=r(xve," (CLIP model)"),xve.forEach(t),SUe=i(Io),Yg=n(Io,"LI",{});var kve=s(Yg);NH=n(kve,"STRONG",{});var Y0r=s(NH);PUe=r(Y0r,"layoutlmv2"),Y0r.forEach(t),$Ue=r(kve," \u2014 "),Cx=n(kve,"A",{href:!0});var K0r=s(Cx);IUe=r(K0r,"LayoutLMv2Processor"),K0r.forEach(t),jUe=r(kve," (LayoutLMv2 model)"),kve.forEach(t),NUe=i(Io),Kg=n(Io,"LI",{});var Rve=s(Kg);DH=n(Rve,"STRONG",{});var Z0r=s(DH);DUe=r(Z0r,"layoutxlm"),Z0r.forEach(t),qUe=r(Rve," \u2014 "),Mx=n(Rve,"A",{href:!0});var eLr=s(Mx);GUe=r(eLr,"LayoutXLMProcessor"),eLr.forEach(t),OUe=r(Rve," (LayoutXLM model)"),Rve.forEach(t),XUe=i(Io),Zg=n(Io,"LI",{});var Sve=s(Zg);qH=n(Sve,"STRONG",{});var oLr=s(qH);zUe=r(oLr,"speech_to_text"),oLr.forEach(t),VUe=r(Sve," \u2014 "),Ex=n(Sve,"A",{href:!0});var rLr=s(Ex);WUe=r(rLr,"Speech2TextProcessor"),rLr.forEach(t),QUe=r(Sve," (Speech2Text model)"),Sve.forEach(t),HUe=i(Io),eh=n(Io,"LI",{});var Pve=s(eh);GH=n(Pve,"STRONG",{});var tLr=s(GH);UUe=r(tLr,"speech_to_text_2"),tLr.forEach(t),JUe=r(Pve," \u2014 "),yx=n(Pve,"A",{href:!0});var aLr=s(yx);YUe=r(aLr,"Speech2Text2Processor"),aLr.forEach(t),KUe=r(Pve," (Speech2Text2 model)"),Pve.forEach(t),ZUe=i(Io),oh=n(Io,"LI",{});var $ve=s(oh);OH=n($ve,"STRONG",{});var nLr=s(OH);eJe=r(nLr,"trocr"),nLr.forEach(t),oJe=r($ve," \u2014 "),wx=n($ve,"A",{href:!0});var sLr=s(wx);rJe=r(sLr,"TrOCRProcessor"),sLr.forEach(t),tJe=r($ve," (TrOCR model)"),$ve.forEach(t),aJe=i(Io),rh=n(Io,"LI",{});var Ive=s(rh);XH=n(Ive,"STRONG",{});var lLr=s(XH);nJe=r(lLr,"vision-text-dual-encoder"),lLr.forEach(t),sJe=r(Ive," \u2014 "),Ax=n(Ive,"A",{href:!0});var iLr=s(Ax);lJe=r(iLr,"VisionTextDualEncoderProcessor"),iLr.forEach(t),iJe=r(Ive," (VisionTextDualEncoder model)"),Ive.forEach(t),dJe=i(Io),th=n(Io,"LI",{});var jve=s(th);zH=n(jve,"STRONG",{});var dLr=s(zH);cJe=r(dLr,"wav2vec2"),dLr.forEach(t),fJe=r(jve," \u2014 "),Lx=n(jve,"A",{href:!0});var cLr=s(Lx);mJe=r(cLr,"Wav2Vec2Processor"),cLr.forEach(t),gJe=r(jve," (Wav2Vec2 model)"),jve.forEach(t),Io.forEach(t),hJe=i(Bt),m(ah.$$.fragment,Bt),pJe=i(Bt),VH=n(Bt,"P",{});var fLr=s(VH);_Je=r(fLr,"Examples:"),fLr.forEach(t),uJe=i(Bt),m(s4.$$.fragment,Bt),Bt.forEach(t),bJe=i(Rs),nh=n(Rs,"DIV",{class:!0});var $Le=s(nh);m(l4.$$.fragment,$Le),vJe=i($Le),WH=n($Le,"P",{});var mLr=s(WH);TJe=r(mLr,"Register a new processor for this class."),mLr.forEach(t),$Le.forEach(t),Rs.forEach(t),kAe=i(d),Si=n(d,"H2",{class:!0});var ILe=s(Si);sh=n(ILe,"A",{id:!0,class:!0,href:!0});var gLr=s(sh);QH=n(gLr,"SPAN",{});var hLr=s(QH);m(i4.$$.fragment,hLr),hLr.forEach(t),gLr.forEach(t),FJe=i(ILe),HH=n(ILe,"SPAN",{});var pLr=s(HH);CJe=r(pLr,"AutoModel"),pLr.forEach(t),ILe.forEach(t),RAe=i(d),zo=n(d,"DIV",{class:!0});var Ss=s(zo);m(d4.$$.fragment,Ss),MJe=i(Ss),Pi=n(Ss,"P",{});var ZO=s(Pi);EJe=r(ZO,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UH=n(ZO,"CODE",{});var _Lr=s(UH);yJe=r(_Lr,"from_pretrained()"),_Lr.forEach(t),wJe=r(ZO,"class method or the "),JH=n(ZO,"CODE",{});var uLr=s(JH);AJe=r(uLr,"from_config()"),uLr.forEach(t),LJe=r(ZO,`class
method.`),ZO.forEach(t),BJe=i(Ss),c4=n(Ss,"P",{});var jLe=s(c4);xJe=r(jLe,"This class cannot be instantiated directly using "),YH=n(jLe,"CODE",{});var bLr=s(YH);kJe=r(bLr,"__init__()"),bLr.forEach(t),RJe=r(jLe," (throws an error)."),jLe.forEach(t),SJe=i(Ss),Ir=n(Ss,"DIV",{class:!0});var Ps=s(Ir);m(f4.$$.fragment,Ps),PJe=i(Ps),KH=n(Ps,"P",{});var vLr=s(KH);$Je=r(vLr,"Instantiates one of the base model classes of the library from a configuration."),vLr.forEach(t),IJe=i(Ps),$i=n(Ps,"P",{});var eX=s($i);jJe=r(eX,`Note:
Loading a model from its configuration file does `),ZH=n(eX,"STRONG",{});var TLr=s(ZH);NJe=r(TLr,"not"),TLr.forEach(t),DJe=r(eX,` load the model weights. It only affects the
model\u2019s configuration. Use `),eU=n(eX,"CODE",{});var FLr=s(eU);qJe=r(FLr,"from_pretrained()"),FLr.forEach(t),GJe=r(eX,"to load the model weights."),eX.forEach(t),OJe=i(Ps),oU=n(Ps,"P",{});var CLr=s(oU);XJe=r(CLr,"Examples:"),CLr.forEach(t),zJe=i(Ps),m(m4.$$.fragment,Ps),Ps.forEach(t),VJe=i(Ss),xe=n(Ss,"DIV",{class:!0});var xt=s(xe);m(g4.$$.fragment,xt),WJe=i(xt),rU=n(xt,"P",{});var MLr=s(rU);QJe=r(MLr,"Instantiate one of the base model classes of the library from a pretrained model."),MLr.forEach(t),HJe=i(xt),$a=n(xt,"P",{});var $F=s($a);UJe=r($F,"The model class to instantiate is selected based on the "),tU=n($F,"CODE",{});var ELr=s(tU);JJe=r(ELr,"model_type"),ELr.forEach(t),YJe=r($F,` property of the config object (either
passed as an argument or loaded from `),aU=n($F,"CODE",{});var yLr=s(aU);KJe=r(yLr,"pretrained_model_name_or_path"),yLr.forEach(t),ZJe=r($F,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nU=n($F,"CODE",{});var wLr=s(nU);eYe=r(wLr,"pretrained_model_name_or_path"),wLr.forEach(t),oYe=r($F,":"),$F.forEach(t),rYe=i(xt),F=n(xt,"UL",{});var C=s(F);lh=n(C,"LI",{});var Nve=s(lh);sU=n(Nve,"STRONG",{});var ALr=s(sU);tYe=r(ALr,"albert"),ALr.forEach(t),aYe=r(Nve," \u2014 "),Bx=n(Nve,"A",{href:!0});var LLr=s(Bx);nYe=r(LLr,"AlbertModel"),LLr.forEach(t),sYe=r(Nve," (ALBERT model)"),Nve.forEach(t),lYe=i(C),ih=n(C,"LI",{});var Dve=s(ih);lU=n(Dve,"STRONG",{});var BLr=s(lU);iYe=r(BLr,"bart"),BLr.forEach(t),dYe=r(Dve," \u2014 "),xx=n(Dve,"A",{href:!0});var xLr=s(xx);cYe=r(xLr,"BartModel"),xLr.forEach(t),fYe=r(Dve," (BART model)"),Dve.forEach(t),mYe=i(C),dh=n(C,"LI",{});var qve=s(dh);iU=n(qve,"STRONG",{});var kLr=s(iU);gYe=r(kLr,"beit"),kLr.forEach(t),hYe=r(qve," \u2014 "),kx=n(qve,"A",{href:!0});var RLr=s(kx);pYe=r(RLr,"BeitModel"),RLr.forEach(t),_Ye=r(qve," (BEiT model)"),qve.forEach(t),uYe=i(C),ch=n(C,"LI",{});var Gve=s(ch);dU=n(Gve,"STRONG",{});var SLr=s(dU);bYe=r(SLr,"bert"),SLr.forEach(t),vYe=r(Gve," \u2014 "),Rx=n(Gve,"A",{href:!0});var PLr=s(Rx);TYe=r(PLr,"BertModel"),PLr.forEach(t),FYe=r(Gve," (BERT model)"),Gve.forEach(t),CYe=i(C),fh=n(C,"LI",{});var Ove=s(fh);cU=n(Ove,"STRONG",{});var $Lr=s(cU);MYe=r($Lr,"bert-generation"),$Lr.forEach(t),EYe=r(Ove," \u2014 "),Sx=n(Ove,"A",{href:!0});var ILr=s(Sx);yYe=r(ILr,"BertGenerationEncoder"),ILr.forEach(t),wYe=r(Ove," (Bert Generation model)"),Ove.forEach(t),AYe=i(C),mh=n(C,"LI",{});var Xve=s(mh);fU=n(Xve,"STRONG",{});var jLr=s(fU);LYe=r(jLr,"big_bird"),jLr.forEach(t),BYe=r(Xve," \u2014 "),Px=n(Xve,"A",{href:!0});var NLr=s(Px);xYe=r(NLr,"BigBirdModel"),NLr.forEach(t),kYe=r(Xve," (BigBird model)"),Xve.forEach(t),RYe=i(C),gh=n(C,"LI",{});var zve=s(gh);mU=n(zve,"STRONG",{});var DLr=s(mU);SYe=r(DLr,"bigbird_pegasus"),DLr.forEach(t),PYe=r(zve," \u2014 "),$x=n(zve,"A",{href:!0});var qLr=s($x);$Ye=r(qLr,"BigBirdPegasusModel"),qLr.forEach(t),IYe=r(zve," (BigBirdPegasus model)"),zve.forEach(t),jYe=i(C),hh=n(C,"LI",{});var Vve=s(hh);gU=n(Vve,"STRONG",{});var GLr=s(gU);NYe=r(GLr,"blenderbot"),GLr.forEach(t),DYe=r(Vve," \u2014 "),Ix=n(Vve,"A",{href:!0});var OLr=s(Ix);qYe=r(OLr,"BlenderbotModel"),OLr.forEach(t),GYe=r(Vve," (Blenderbot model)"),Vve.forEach(t),OYe=i(C),ph=n(C,"LI",{});var Wve=s(ph);hU=n(Wve,"STRONG",{});var XLr=s(hU);XYe=r(XLr,"blenderbot-small"),XLr.forEach(t),zYe=r(Wve," \u2014 "),jx=n(Wve,"A",{href:!0});var zLr=s(jx);VYe=r(zLr,"BlenderbotSmallModel"),zLr.forEach(t),WYe=r(Wve," (BlenderbotSmall model)"),Wve.forEach(t),QYe=i(C),_h=n(C,"LI",{});var Qve=s(_h);pU=n(Qve,"STRONG",{});var VLr=s(pU);HYe=r(VLr,"camembert"),VLr.forEach(t),UYe=r(Qve," \u2014 "),Nx=n(Qve,"A",{href:!0});var WLr=s(Nx);JYe=r(WLr,"CamembertModel"),WLr.forEach(t),YYe=r(Qve," (CamemBERT model)"),Qve.forEach(t),KYe=i(C),uh=n(C,"LI",{});var Hve=s(uh);_U=n(Hve,"STRONG",{});var QLr=s(_U);ZYe=r(QLr,"canine"),QLr.forEach(t),eKe=r(Hve," \u2014 "),Dx=n(Hve,"A",{href:!0});var HLr=s(Dx);oKe=r(HLr,"CanineModel"),HLr.forEach(t),rKe=r(Hve," (Canine model)"),Hve.forEach(t),tKe=i(C),bh=n(C,"LI",{});var Uve=s(bh);uU=n(Uve,"STRONG",{});var ULr=s(uU);aKe=r(ULr,"clip"),ULr.forEach(t),nKe=r(Uve," \u2014 "),qx=n(Uve,"A",{href:!0});var JLr=s(qx);sKe=r(JLr,"CLIPModel"),JLr.forEach(t),lKe=r(Uve," (CLIP model)"),Uve.forEach(t),iKe=i(C),vh=n(C,"LI",{});var Jve=s(vh);bU=n(Jve,"STRONG",{});var YLr=s(bU);dKe=r(YLr,"convbert"),YLr.forEach(t),cKe=r(Jve," \u2014 "),Gx=n(Jve,"A",{href:!0});var KLr=s(Gx);fKe=r(KLr,"ConvBertModel"),KLr.forEach(t),mKe=r(Jve," (ConvBERT model)"),Jve.forEach(t),gKe=i(C),Th=n(C,"LI",{});var Yve=s(Th);vU=n(Yve,"STRONG",{});var ZLr=s(vU);hKe=r(ZLr,"convnext"),ZLr.forEach(t),pKe=r(Yve," \u2014 "),Ox=n(Yve,"A",{href:!0});var e9r=s(Ox);_Ke=r(e9r,"ConvNextModel"),e9r.forEach(t),uKe=r(Yve," (ConvNext model)"),Yve.forEach(t),bKe=i(C),Fh=n(C,"LI",{});var Kve=s(Fh);TU=n(Kve,"STRONG",{});var o9r=s(TU);vKe=r(o9r,"ctrl"),o9r.forEach(t),TKe=r(Kve," \u2014 "),Xx=n(Kve,"A",{href:!0});var r9r=s(Xx);FKe=r(r9r,"CTRLModel"),r9r.forEach(t),CKe=r(Kve," (CTRL model)"),Kve.forEach(t),MKe=i(C),Ch=n(C,"LI",{});var Zve=s(Ch);FU=n(Zve,"STRONG",{});var t9r=s(FU);EKe=r(t9r,"deberta"),t9r.forEach(t),yKe=r(Zve," \u2014 "),zx=n(Zve,"A",{href:!0});var a9r=s(zx);wKe=r(a9r,"DebertaModel"),a9r.forEach(t),AKe=r(Zve," (DeBERTa model)"),Zve.forEach(t),LKe=i(C),Mh=n(C,"LI",{});var e6e=s(Mh);CU=n(e6e,"STRONG",{});var n9r=s(CU);BKe=r(n9r,"deberta-v2"),n9r.forEach(t),xKe=r(e6e," \u2014 "),Vx=n(e6e,"A",{href:!0});var s9r=s(Vx);kKe=r(s9r,"DebertaV2Model"),s9r.forEach(t),RKe=r(e6e," (DeBERTa-v2 model)"),e6e.forEach(t),SKe=i(C),Eh=n(C,"LI",{});var o6e=s(Eh);MU=n(o6e,"STRONG",{});var l9r=s(MU);PKe=r(l9r,"deit"),l9r.forEach(t),$Ke=r(o6e," \u2014 "),Wx=n(o6e,"A",{href:!0});var i9r=s(Wx);IKe=r(i9r,"DeiTModel"),i9r.forEach(t),jKe=r(o6e," (DeiT model)"),o6e.forEach(t),NKe=i(C),yh=n(C,"LI",{});var r6e=s(yh);EU=n(r6e,"STRONG",{});var d9r=s(EU);DKe=r(d9r,"detr"),d9r.forEach(t),qKe=r(r6e," \u2014 "),Qx=n(r6e,"A",{href:!0});var c9r=s(Qx);GKe=r(c9r,"DetrModel"),c9r.forEach(t),OKe=r(r6e," (DETR model)"),r6e.forEach(t),XKe=i(C),wh=n(C,"LI",{});var t6e=s(wh);yU=n(t6e,"STRONG",{});var f9r=s(yU);zKe=r(f9r,"distilbert"),f9r.forEach(t),VKe=r(t6e," \u2014 "),Hx=n(t6e,"A",{href:!0});var m9r=s(Hx);WKe=r(m9r,"DistilBertModel"),m9r.forEach(t),QKe=r(t6e," (DistilBERT model)"),t6e.forEach(t),HKe=i(C),Ah=n(C,"LI",{});var a6e=s(Ah);wU=n(a6e,"STRONG",{});var g9r=s(wU);UKe=r(g9r,"dpr"),g9r.forEach(t),JKe=r(a6e," \u2014 "),Ux=n(a6e,"A",{href:!0});var h9r=s(Ux);YKe=r(h9r,"DPRQuestionEncoder"),h9r.forEach(t),KKe=r(a6e," (DPR model)"),a6e.forEach(t),ZKe=i(C),Lh=n(C,"LI",{});var n6e=s(Lh);AU=n(n6e,"STRONG",{});var p9r=s(AU);eZe=r(p9r,"electra"),p9r.forEach(t),oZe=r(n6e," \u2014 "),Jx=n(n6e,"A",{href:!0});var _9r=s(Jx);rZe=r(_9r,"ElectraModel"),_9r.forEach(t),tZe=r(n6e," (ELECTRA model)"),n6e.forEach(t),aZe=i(C),Bh=n(C,"LI",{});var s6e=s(Bh);LU=n(s6e,"STRONG",{});var u9r=s(LU);nZe=r(u9r,"flaubert"),u9r.forEach(t),sZe=r(s6e," \u2014 "),Yx=n(s6e,"A",{href:!0});var b9r=s(Yx);lZe=r(b9r,"FlaubertModel"),b9r.forEach(t),iZe=r(s6e," (FlauBERT model)"),s6e.forEach(t),dZe=i(C),xh=n(C,"LI",{});var l6e=s(xh);BU=n(l6e,"STRONG",{});var v9r=s(BU);cZe=r(v9r,"fnet"),v9r.forEach(t),fZe=r(l6e," \u2014 "),Kx=n(l6e,"A",{href:!0});var T9r=s(Kx);mZe=r(T9r,"FNetModel"),T9r.forEach(t),gZe=r(l6e," (FNet model)"),l6e.forEach(t),hZe=i(C),kh=n(C,"LI",{});var i6e=s(kh);xU=n(i6e,"STRONG",{});var F9r=s(xU);pZe=r(F9r,"fsmt"),F9r.forEach(t),_Ze=r(i6e," \u2014 "),Zx=n(i6e,"A",{href:!0});var C9r=s(Zx);uZe=r(C9r,"FSMTModel"),C9r.forEach(t),bZe=r(i6e," (FairSeq Machine-Translation model)"),i6e.forEach(t),vZe=i(C),ws=n(C,"LI",{});var r0=s(ws);kU=n(r0,"STRONG",{});var M9r=s(kU);TZe=r(M9r,"funnel"),M9r.forEach(t),FZe=r(r0," \u2014 "),ek=n(r0,"A",{href:!0});var E9r=s(ek);CZe=r(E9r,"FunnelModel"),E9r.forEach(t),MZe=r(r0," or "),ok=n(r0,"A",{href:!0});var y9r=s(ok);EZe=r(y9r,"FunnelBaseModel"),y9r.forEach(t),yZe=r(r0," (Funnel Transformer model)"),r0.forEach(t),wZe=i(C),Rh=n(C,"LI",{});var d6e=s(Rh);RU=n(d6e,"STRONG",{});var w9r=s(RU);AZe=r(w9r,"gpt2"),w9r.forEach(t),LZe=r(d6e," \u2014 "),rk=n(d6e,"A",{href:!0});var A9r=s(rk);BZe=r(A9r,"GPT2Model"),A9r.forEach(t),xZe=r(d6e," (OpenAI GPT-2 model)"),d6e.forEach(t),kZe=i(C),Sh=n(C,"LI",{});var c6e=s(Sh);SU=n(c6e,"STRONG",{});var L9r=s(SU);RZe=r(L9r,"gpt_neo"),L9r.forEach(t),SZe=r(c6e," \u2014 "),tk=n(c6e,"A",{href:!0});var B9r=s(tk);PZe=r(B9r,"GPTNeoModel"),B9r.forEach(t),$Ze=r(c6e," (GPT Neo model)"),c6e.forEach(t),IZe=i(C),Ph=n(C,"LI",{});var f6e=s(Ph);PU=n(f6e,"STRONG",{});var x9r=s(PU);jZe=r(x9r,"gptj"),x9r.forEach(t),NZe=r(f6e," \u2014 "),ak=n(f6e,"A",{href:!0});var k9r=s(ak);DZe=r(k9r,"GPTJModel"),k9r.forEach(t),qZe=r(f6e," (GPT-J model)"),f6e.forEach(t),GZe=i(C),$h=n(C,"LI",{});var m6e=s($h);$U=n(m6e,"STRONG",{});var R9r=s($U);OZe=r(R9r,"hubert"),R9r.forEach(t),XZe=r(m6e," \u2014 "),nk=n(m6e,"A",{href:!0});var S9r=s(nk);zZe=r(S9r,"HubertModel"),S9r.forEach(t),VZe=r(m6e," (Hubert model)"),m6e.forEach(t),WZe=i(C),Ih=n(C,"LI",{});var g6e=s(Ih);IU=n(g6e,"STRONG",{});var P9r=s(IU);QZe=r(P9r,"ibert"),P9r.forEach(t),HZe=r(g6e," \u2014 "),sk=n(g6e,"A",{href:!0});var $9r=s(sk);UZe=r($9r,"IBertModel"),$9r.forEach(t),JZe=r(g6e," (I-BERT model)"),g6e.forEach(t),YZe=i(C),jh=n(C,"LI",{});var h6e=s(jh);jU=n(h6e,"STRONG",{});var I9r=s(jU);KZe=r(I9r,"imagegpt"),I9r.forEach(t),ZZe=r(h6e," \u2014 "),lk=n(h6e,"A",{href:!0});var j9r=s(lk);eeo=r(j9r,"ImageGPTModel"),j9r.forEach(t),oeo=r(h6e," (ImageGPT model)"),h6e.forEach(t),reo=i(C),Nh=n(C,"LI",{});var p6e=s(Nh);NU=n(p6e,"STRONG",{});var N9r=s(NU);teo=r(N9r,"layoutlm"),N9r.forEach(t),aeo=r(p6e," \u2014 "),ik=n(p6e,"A",{href:!0});var D9r=s(ik);neo=r(D9r,"LayoutLMModel"),D9r.forEach(t),seo=r(p6e," (LayoutLM model)"),p6e.forEach(t),leo=i(C),Dh=n(C,"LI",{});var _6e=s(Dh);DU=n(_6e,"STRONG",{});var q9r=s(DU);ieo=r(q9r,"layoutlmv2"),q9r.forEach(t),deo=r(_6e," \u2014 "),dk=n(_6e,"A",{href:!0});var G9r=s(dk);ceo=r(G9r,"LayoutLMv2Model"),G9r.forEach(t),feo=r(_6e," (LayoutLMv2 model)"),_6e.forEach(t),meo=i(C),qh=n(C,"LI",{});var u6e=s(qh);qU=n(u6e,"STRONG",{});var O9r=s(qU);geo=r(O9r,"led"),O9r.forEach(t),heo=r(u6e," \u2014 "),ck=n(u6e,"A",{href:!0});var X9r=s(ck);peo=r(X9r,"LEDModel"),X9r.forEach(t),_eo=r(u6e," (LED model)"),u6e.forEach(t),ueo=i(C),Gh=n(C,"LI",{});var b6e=s(Gh);GU=n(b6e,"STRONG",{});var z9r=s(GU);beo=r(z9r,"longformer"),z9r.forEach(t),veo=r(b6e," \u2014 "),fk=n(b6e,"A",{href:!0});var V9r=s(fk);Teo=r(V9r,"LongformerModel"),V9r.forEach(t),Feo=r(b6e," (Longformer model)"),b6e.forEach(t),Ceo=i(C),Oh=n(C,"LI",{});var v6e=s(Oh);OU=n(v6e,"STRONG",{});var W9r=s(OU);Meo=r(W9r,"luke"),W9r.forEach(t),Eeo=r(v6e," \u2014 "),mk=n(v6e,"A",{href:!0});var Q9r=s(mk);yeo=r(Q9r,"LukeModel"),Q9r.forEach(t),weo=r(v6e," (LUKE model)"),v6e.forEach(t),Aeo=i(C),Xh=n(C,"LI",{});var T6e=s(Xh);XU=n(T6e,"STRONG",{});var H9r=s(XU);Leo=r(H9r,"lxmert"),H9r.forEach(t),Beo=r(T6e," \u2014 "),gk=n(T6e,"A",{href:!0});var U9r=s(gk);xeo=r(U9r,"LxmertModel"),U9r.forEach(t),keo=r(T6e," (LXMERT model)"),T6e.forEach(t),Reo=i(C),zh=n(C,"LI",{});var F6e=s(zh);zU=n(F6e,"STRONG",{});var J9r=s(zU);Seo=r(J9r,"m2m_100"),J9r.forEach(t),Peo=r(F6e," \u2014 "),hk=n(F6e,"A",{href:!0});var Y9r=s(hk);$eo=r(Y9r,"M2M100Model"),Y9r.forEach(t),Ieo=r(F6e," (M2M100 model)"),F6e.forEach(t),jeo=i(C),Vh=n(C,"LI",{});var C6e=s(Vh);VU=n(C6e,"STRONG",{});var K9r=s(VU);Neo=r(K9r,"marian"),K9r.forEach(t),Deo=r(C6e," \u2014 "),pk=n(C6e,"A",{href:!0});var Z9r=s(pk);qeo=r(Z9r,"MarianModel"),Z9r.forEach(t),Geo=r(C6e," (Marian model)"),C6e.forEach(t),Oeo=i(C),Wh=n(C,"LI",{});var M6e=s(Wh);WU=n(M6e,"STRONG",{});var eBr=s(WU);Xeo=r(eBr,"mbart"),eBr.forEach(t),zeo=r(M6e," \u2014 "),_k=n(M6e,"A",{href:!0});var oBr=s(_k);Veo=r(oBr,"MBartModel"),oBr.forEach(t),Weo=r(M6e," (mBART model)"),M6e.forEach(t),Qeo=i(C),Qh=n(C,"LI",{});var E6e=s(Qh);QU=n(E6e,"STRONG",{});var rBr=s(QU);Heo=r(rBr,"megatron-bert"),rBr.forEach(t),Ueo=r(E6e," \u2014 "),uk=n(E6e,"A",{href:!0});var tBr=s(uk);Jeo=r(tBr,"MegatronBertModel"),tBr.forEach(t),Yeo=r(E6e," (MegatronBert model)"),E6e.forEach(t),Keo=i(C),Hh=n(C,"LI",{});var y6e=s(Hh);HU=n(y6e,"STRONG",{});var aBr=s(HU);Zeo=r(aBr,"mobilebert"),aBr.forEach(t),eoo=r(y6e," \u2014 "),bk=n(y6e,"A",{href:!0});var nBr=s(bk);ooo=r(nBr,"MobileBertModel"),nBr.forEach(t),roo=r(y6e," (MobileBERT model)"),y6e.forEach(t),too=i(C),Uh=n(C,"LI",{});var w6e=s(Uh);UU=n(w6e,"STRONG",{});var sBr=s(UU);aoo=r(sBr,"mpnet"),sBr.forEach(t),noo=r(w6e," \u2014 "),vk=n(w6e,"A",{href:!0});var lBr=s(vk);soo=r(lBr,"MPNetModel"),lBr.forEach(t),loo=r(w6e," (MPNet model)"),w6e.forEach(t),ioo=i(C),Jh=n(C,"LI",{});var A6e=s(Jh);JU=n(A6e,"STRONG",{});var iBr=s(JU);doo=r(iBr,"mt5"),iBr.forEach(t),coo=r(A6e," \u2014 "),Tk=n(A6e,"A",{href:!0});var dBr=s(Tk);foo=r(dBr,"MT5Model"),dBr.forEach(t),moo=r(A6e," (mT5 model)"),A6e.forEach(t),goo=i(C),Yh=n(C,"LI",{});var L6e=s(Yh);YU=n(L6e,"STRONG",{});var cBr=s(YU);hoo=r(cBr,"nystromformer"),cBr.forEach(t),poo=r(L6e," \u2014 "),Fk=n(L6e,"A",{href:!0});var fBr=s(Fk);_oo=r(fBr,"NystromformerModel"),fBr.forEach(t),uoo=r(L6e," (Nystromformer model)"),L6e.forEach(t),boo=i(C),Kh=n(C,"LI",{});var B6e=s(Kh);KU=n(B6e,"STRONG",{});var mBr=s(KU);voo=r(mBr,"openai-gpt"),mBr.forEach(t),Too=r(B6e," \u2014 "),Ck=n(B6e,"A",{href:!0});var gBr=s(Ck);Foo=r(gBr,"OpenAIGPTModel"),gBr.forEach(t),Coo=r(B6e," (OpenAI GPT model)"),B6e.forEach(t),Moo=i(C),Zh=n(C,"LI",{});var x6e=s(Zh);ZU=n(x6e,"STRONG",{});var hBr=s(ZU);Eoo=r(hBr,"pegasus"),hBr.forEach(t),yoo=r(x6e," \u2014 "),Mk=n(x6e,"A",{href:!0});var pBr=s(Mk);woo=r(pBr,"PegasusModel"),pBr.forEach(t),Aoo=r(x6e," (Pegasus model)"),x6e.forEach(t),Loo=i(C),ep=n(C,"LI",{});var k6e=s(ep);eJ=n(k6e,"STRONG",{});var _Br=s(eJ);Boo=r(_Br,"perceiver"),_Br.forEach(t),xoo=r(k6e," \u2014 "),Ek=n(k6e,"A",{href:!0});var uBr=s(Ek);koo=r(uBr,"PerceiverModel"),uBr.forEach(t),Roo=r(k6e," (Perceiver model)"),k6e.forEach(t),Soo=i(C),op=n(C,"LI",{});var R6e=s(op);oJ=n(R6e,"STRONG",{});var bBr=s(oJ);Poo=r(bBr,"prophetnet"),bBr.forEach(t),$oo=r(R6e," \u2014 "),yk=n(R6e,"A",{href:!0});var vBr=s(yk);Ioo=r(vBr,"ProphetNetModel"),vBr.forEach(t),joo=r(R6e," (ProphetNet model)"),R6e.forEach(t),Noo=i(C),rp=n(C,"LI",{});var S6e=s(rp);rJ=n(S6e,"STRONG",{});var TBr=s(rJ);Doo=r(TBr,"qdqbert"),TBr.forEach(t),qoo=r(S6e," \u2014 "),wk=n(S6e,"A",{href:!0});var FBr=s(wk);Goo=r(FBr,"QDQBertModel"),FBr.forEach(t),Ooo=r(S6e," (QDQBert model)"),S6e.forEach(t),Xoo=i(C),tp=n(C,"LI",{});var P6e=s(tp);tJ=n(P6e,"STRONG",{});var CBr=s(tJ);zoo=r(CBr,"reformer"),CBr.forEach(t),Voo=r(P6e," \u2014 "),Ak=n(P6e,"A",{href:!0});var MBr=s(Ak);Woo=r(MBr,"ReformerModel"),MBr.forEach(t),Qoo=r(P6e," (Reformer model)"),P6e.forEach(t),Hoo=i(C),ap=n(C,"LI",{});var $6e=s(ap);aJ=n($6e,"STRONG",{});var EBr=s(aJ);Uoo=r(EBr,"rembert"),EBr.forEach(t),Joo=r($6e," \u2014 "),Lk=n($6e,"A",{href:!0});var yBr=s(Lk);Yoo=r(yBr,"RemBertModel"),yBr.forEach(t),Koo=r($6e," (RemBERT model)"),$6e.forEach(t),Zoo=i(C),np=n(C,"LI",{});var I6e=s(np);nJ=n(I6e,"STRONG",{});var wBr=s(nJ);ero=r(wBr,"retribert"),wBr.forEach(t),oro=r(I6e," \u2014 "),Bk=n(I6e,"A",{href:!0});var ABr=s(Bk);rro=r(ABr,"RetriBertModel"),ABr.forEach(t),tro=r(I6e," (RetriBERT model)"),I6e.forEach(t),aro=i(C),sp=n(C,"LI",{});var j6e=s(sp);sJ=n(j6e,"STRONG",{});var LBr=s(sJ);nro=r(LBr,"roberta"),LBr.forEach(t),sro=r(j6e," \u2014 "),xk=n(j6e,"A",{href:!0});var BBr=s(xk);lro=r(BBr,"RobertaModel"),BBr.forEach(t),iro=r(j6e," (RoBERTa model)"),j6e.forEach(t),dro=i(C),lp=n(C,"LI",{});var N6e=s(lp);lJ=n(N6e,"STRONG",{});var xBr=s(lJ);cro=r(xBr,"roformer"),xBr.forEach(t),fro=r(N6e," \u2014 "),kk=n(N6e,"A",{href:!0});var kBr=s(kk);mro=r(kBr,"RoFormerModel"),kBr.forEach(t),gro=r(N6e," (RoFormer model)"),N6e.forEach(t),hro=i(C),ip=n(C,"LI",{});var D6e=s(ip);iJ=n(D6e,"STRONG",{});var RBr=s(iJ);pro=r(RBr,"segformer"),RBr.forEach(t),_ro=r(D6e," \u2014 "),Rk=n(D6e,"A",{href:!0});var SBr=s(Rk);uro=r(SBr,"SegformerModel"),SBr.forEach(t),bro=r(D6e," (SegFormer model)"),D6e.forEach(t),vro=i(C),dp=n(C,"LI",{});var q6e=s(dp);dJ=n(q6e,"STRONG",{});var PBr=s(dJ);Tro=r(PBr,"sew"),PBr.forEach(t),Fro=r(q6e," \u2014 "),Sk=n(q6e,"A",{href:!0});var $Br=s(Sk);Cro=r($Br,"SEWModel"),$Br.forEach(t),Mro=r(q6e," (SEW model)"),q6e.forEach(t),Ero=i(C),cp=n(C,"LI",{});var G6e=s(cp);cJ=n(G6e,"STRONG",{});var IBr=s(cJ);yro=r(IBr,"sew-d"),IBr.forEach(t),wro=r(G6e," \u2014 "),Pk=n(G6e,"A",{href:!0});var jBr=s(Pk);Aro=r(jBr,"SEWDModel"),jBr.forEach(t),Lro=r(G6e," (SEW-D model)"),G6e.forEach(t),Bro=i(C),fp=n(C,"LI",{});var O6e=s(fp);fJ=n(O6e,"STRONG",{});var NBr=s(fJ);xro=r(NBr,"speech_to_text"),NBr.forEach(t),kro=r(O6e," \u2014 "),$k=n(O6e,"A",{href:!0});var DBr=s($k);Rro=r(DBr,"Speech2TextModel"),DBr.forEach(t),Sro=r(O6e," (Speech2Text model)"),O6e.forEach(t),Pro=i(C),mp=n(C,"LI",{});var X6e=s(mp);mJ=n(X6e,"STRONG",{});var qBr=s(mJ);$ro=r(qBr,"splinter"),qBr.forEach(t),Iro=r(X6e," \u2014 "),Ik=n(X6e,"A",{href:!0});var GBr=s(Ik);jro=r(GBr,"SplinterModel"),GBr.forEach(t),Nro=r(X6e," (Splinter model)"),X6e.forEach(t),Dro=i(C),gp=n(C,"LI",{});var z6e=s(gp);gJ=n(z6e,"STRONG",{});var OBr=s(gJ);qro=r(OBr,"squeezebert"),OBr.forEach(t),Gro=r(z6e," \u2014 "),jk=n(z6e,"A",{href:!0});var XBr=s(jk);Oro=r(XBr,"SqueezeBertModel"),XBr.forEach(t),Xro=r(z6e," (SqueezeBERT model)"),z6e.forEach(t),zro=i(C),hp=n(C,"LI",{});var V6e=s(hp);hJ=n(V6e,"STRONG",{});var zBr=s(hJ);Vro=r(zBr,"swin"),zBr.forEach(t),Wro=r(V6e," \u2014 "),Nk=n(V6e,"A",{href:!0});var VBr=s(Nk);Qro=r(VBr,"SwinModel"),VBr.forEach(t),Hro=r(V6e," (Swin model)"),V6e.forEach(t),Uro=i(C),pp=n(C,"LI",{});var W6e=s(pp);pJ=n(W6e,"STRONG",{});var WBr=s(pJ);Jro=r(WBr,"t5"),WBr.forEach(t),Yro=r(W6e," \u2014 "),Dk=n(W6e,"A",{href:!0});var QBr=s(Dk);Kro=r(QBr,"T5Model"),QBr.forEach(t),Zro=r(W6e," (T5 model)"),W6e.forEach(t),eto=i(C),_p=n(C,"LI",{});var Q6e=s(_p);_J=n(Q6e,"STRONG",{});var HBr=s(_J);oto=r(HBr,"tapas"),HBr.forEach(t),rto=r(Q6e," \u2014 "),qk=n(Q6e,"A",{href:!0});var UBr=s(qk);tto=r(UBr,"TapasModel"),UBr.forEach(t),ato=r(Q6e," (TAPAS model)"),Q6e.forEach(t),nto=i(C),up=n(C,"LI",{});var H6e=s(up);uJ=n(H6e,"STRONG",{});var JBr=s(uJ);sto=r(JBr,"transfo-xl"),JBr.forEach(t),lto=r(H6e," \u2014 "),Gk=n(H6e,"A",{href:!0});var YBr=s(Gk);ito=r(YBr,"TransfoXLModel"),YBr.forEach(t),dto=r(H6e," (Transformer-XL model)"),H6e.forEach(t),cto=i(C),bp=n(C,"LI",{});var U6e=s(bp);bJ=n(U6e,"STRONG",{});var KBr=s(bJ);fto=r(KBr,"unispeech"),KBr.forEach(t),mto=r(U6e," \u2014 "),Ok=n(U6e,"A",{href:!0});var ZBr=s(Ok);gto=r(ZBr,"UniSpeechModel"),ZBr.forEach(t),hto=r(U6e," (UniSpeech model)"),U6e.forEach(t),pto=i(C),vp=n(C,"LI",{});var J6e=s(vp);vJ=n(J6e,"STRONG",{});var exr=s(vJ);_to=r(exr,"unispeech-sat"),exr.forEach(t),uto=r(J6e," \u2014 "),Xk=n(J6e,"A",{href:!0});var oxr=s(Xk);bto=r(oxr,"UniSpeechSatModel"),oxr.forEach(t),vto=r(J6e," (UniSpeechSat model)"),J6e.forEach(t),Tto=i(C),Tp=n(C,"LI",{});var Y6e=s(Tp);TJ=n(Y6e,"STRONG",{});var rxr=s(TJ);Fto=r(rxr,"vilt"),rxr.forEach(t),Cto=r(Y6e," \u2014 "),zk=n(Y6e,"A",{href:!0});var txr=s(zk);Mto=r(txr,"ViltModel"),txr.forEach(t),Eto=r(Y6e," (ViLT model)"),Y6e.forEach(t),yto=i(C),Fp=n(C,"LI",{});var K6e=s(Fp);FJ=n(K6e,"STRONG",{});var axr=s(FJ);wto=r(axr,"vision-text-dual-encoder"),axr.forEach(t),Ato=r(K6e," \u2014 "),Vk=n(K6e,"A",{href:!0});var nxr=s(Vk);Lto=r(nxr,"VisionTextDualEncoderModel"),nxr.forEach(t),Bto=r(K6e," (VisionTextDualEncoder model)"),K6e.forEach(t),xto=i(C),Cp=n(C,"LI",{});var Z6e=s(Cp);CJ=n(Z6e,"STRONG",{});var sxr=s(CJ);kto=r(sxr,"visual_bert"),sxr.forEach(t),Rto=r(Z6e," \u2014 "),Wk=n(Z6e,"A",{href:!0});var lxr=s(Wk);Sto=r(lxr,"VisualBertModel"),lxr.forEach(t),Pto=r(Z6e," (VisualBert model)"),Z6e.forEach(t),$to=i(C),Mp=n(C,"LI",{});var eTe=s(Mp);MJ=n(eTe,"STRONG",{});var ixr=s(MJ);Ito=r(ixr,"vit"),ixr.forEach(t),jto=r(eTe," \u2014 "),Qk=n(eTe,"A",{href:!0});var dxr=s(Qk);Nto=r(dxr,"ViTModel"),dxr.forEach(t),Dto=r(eTe," (ViT model)"),eTe.forEach(t),qto=i(C),Ep=n(C,"LI",{});var oTe=s(Ep);EJ=n(oTe,"STRONG",{});var cxr=s(EJ);Gto=r(cxr,"vit_mae"),cxr.forEach(t),Oto=r(oTe," \u2014 "),Hk=n(oTe,"A",{href:!0});var fxr=s(Hk);Xto=r(fxr,"ViTMAEModel"),fxr.forEach(t),zto=r(oTe," (ViTMAE model)"),oTe.forEach(t),Vto=i(C),yp=n(C,"LI",{});var rTe=s(yp);yJ=n(rTe,"STRONG",{});var mxr=s(yJ);Wto=r(mxr,"wav2vec2"),mxr.forEach(t),Qto=r(rTe," \u2014 "),Uk=n(rTe,"A",{href:!0});var gxr=s(Uk);Hto=r(gxr,"Wav2Vec2Model"),gxr.forEach(t),Uto=r(rTe," (Wav2Vec2 model)"),rTe.forEach(t),Jto=i(C),wp=n(C,"LI",{});var tTe=s(wp);wJ=n(tTe,"STRONG",{});var hxr=s(wJ);Yto=r(hxr,"wavlm"),hxr.forEach(t),Kto=r(tTe," \u2014 "),Jk=n(tTe,"A",{href:!0});var pxr=s(Jk);Zto=r(pxr,"WavLMModel"),pxr.forEach(t),eao=r(tTe," (WavLM model)"),tTe.forEach(t),oao=i(C),Ap=n(C,"LI",{});var aTe=s(Ap);AJ=n(aTe,"STRONG",{});var _xr=s(AJ);rao=r(_xr,"xglm"),_xr.forEach(t),tao=r(aTe," \u2014 "),Yk=n(aTe,"A",{href:!0});var uxr=s(Yk);aao=r(uxr,"XGLMModel"),uxr.forEach(t),nao=r(aTe," (XGLM model)"),aTe.forEach(t),sao=i(C),Lp=n(C,"LI",{});var nTe=s(Lp);LJ=n(nTe,"STRONG",{});var bxr=s(LJ);lao=r(bxr,"xlm"),bxr.forEach(t),iao=r(nTe," \u2014 "),Kk=n(nTe,"A",{href:!0});var vxr=s(Kk);dao=r(vxr,"XLMModel"),vxr.forEach(t),cao=r(nTe," (XLM model)"),nTe.forEach(t),fao=i(C),Bp=n(C,"LI",{});var sTe=s(Bp);BJ=n(sTe,"STRONG",{});var Txr=s(BJ);mao=r(Txr,"xlm-prophetnet"),Txr.forEach(t),gao=r(sTe," \u2014 "),Zk=n(sTe,"A",{href:!0});var Fxr=s(Zk);hao=r(Fxr,"XLMProphetNetModel"),Fxr.forEach(t),pao=r(sTe," (XLMProphetNet model)"),sTe.forEach(t),_ao=i(C),xp=n(C,"LI",{});var lTe=s(xp);xJ=n(lTe,"STRONG",{});var Cxr=s(xJ);uao=r(Cxr,"xlm-roberta"),Cxr.forEach(t),bao=r(lTe," \u2014 "),eR=n(lTe,"A",{href:!0});var Mxr=s(eR);vao=r(Mxr,"XLMRobertaModel"),Mxr.forEach(t),Tao=r(lTe," (XLM-RoBERTa model)"),lTe.forEach(t),Fao=i(C),kp=n(C,"LI",{});var iTe=s(kp);kJ=n(iTe,"STRONG",{});var Exr=s(kJ);Cao=r(Exr,"xlm-roberta-xl"),Exr.forEach(t),Mao=r(iTe," \u2014 "),oR=n(iTe,"A",{href:!0});var yxr=s(oR);Eao=r(yxr,"XLMRobertaXLModel"),yxr.forEach(t),yao=r(iTe," (XLM-RoBERTa-XL model)"),iTe.forEach(t),wao=i(C),Rp=n(C,"LI",{});var dTe=s(Rp);RJ=n(dTe,"STRONG",{});var wxr=s(RJ);Aao=r(wxr,"xlnet"),wxr.forEach(t),Lao=r(dTe," \u2014 "),rR=n(dTe,"A",{href:!0});var Axr=s(rR);Bao=r(Axr,"XLNetModel"),Axr.forEach(t),xao=r(dTe," (XLNet model)"),dTe.forEach(t),kao=i(C),Sp=n(C,"LI",{});var cTe=s(Sp);SJ=n(cTe,"STRONG",{});var Lxr=s(SJ);Rao=r(Lxr,"yoso"),Lxr.forEach(t),Sao=r(cTe," \u2014 "),tR=n(cTe,"A",{href:!0});var Bxr=s(tR);Pao=r(Bxr,"YosoModel"),Bxr.forEach(t),$ao=r(cTe," (YOSO model)"),cTe.forEach(t),C.forEach(t),Iao=i(xt),Pp=n(xt,"P",{});var fTe=s(Pp);jao=r(fTe,"The model is set in evaluation mode by default using "),PJ=n(fTe,"CODE",{});var xxr=s(PJ);Nao=r(xxr,"model.eval()"),xxr.forEach(t),Dao=r(fTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$J=n(fTe,"CODE",{});var kxr=s($J);qao=r(kxr,"model.train()"),kxr.forEach(t),fTe.forEach(t),Gao=i(xt),IJ=n(xt,"P",{});var Rxr=s(IJ);Oao=r(Rxr,"Examples:"),Rxr.forEach(t),Xao=i(xt),m(h4.$$.fragment,xt),xt.forEach(t),Ss.forEach(t),SAe=i(d),Ii=n(d,"H2",{class:!0});var NLe=s(Ii);$p=n(NLe,"A",{id:!0,class:!0,href:!0});var Sxr=s($p);jJ=n(Sxr,"SPAN",{});var Pxr=s(jJ);m(p4.$$.fragment,Pxr),Pxr.forEach(t),Sxr.forEach(t),zao=i(NLe),NJ=n(NLe,"SPAN",{});var $xr=s(NJ);Vao=r($xr,"AutoModelForPreTraining"),$xr.forEach(t),NLe.forEach(t),PAe=i(d),Vo=n(d,"DIV",{class:!0});var $s=s(Vo);m(_4.$$.fragment,$s),Wao=i($s),ji=n($s,"P",{});var oX=s(ji);Qao=r(oX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),DJ=n(oX,"CODE",{});var Ixr=s(DJ);Hao=r(Ixr,"from_pretrained()"),Ixr.forEach(t),Uao=r(oX,"class method or the "),qJ=n(oX,"CODE",{});var jxr=s(qJ);Jao=r(jxr,"from_config()"),jxr.forEach(t),Yao=r(oX,`class
method.`),oX.forEach(t),Kao=i($s),u4=n($s,"P",{});var DLe=s(u4);Zao=r(DLe,"This class cannot be instantiated directly using "),GJ=n(DLe,"CODE",{});var Nxr=s(GJ);eno=r(Nxr,"__init__()"),Nxr.forEach(t),ono=r(DLe," (throws an error)."),DLe.forEach(t),rno=i($s),jr=n($s,"DIV",{class:!0});var Is=s(jr);m(b4.$$.fragment,Is),tno=i(Is),OJ=n(Is,"P",{});var Dxr=s(OJ);ano=r(Dxr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Dxr.forEach(t),nno=i(Is),Ni=n(Is,"P",{});var rX=s(Ni);sno=r(rX,`Note:
Loading a model from its configuration file does `),XJ=n(rX,"STRONG",{});var qxr=s(XJ);lno=r(qxr,"not"),qxr.forEach(t),ino=r(rX,` load the model weights. It only affects the
model\u2019s configuration. Use `),zJ=n(rX,"CODE",{});var Gxr=s(zJ);dno=r(Gxr,"from_pretrained()"),Gxr.forEach(t),cno=r(rX,"to load the model weights."),rX.forEach(t),fno=i(Is),VJ=n(Is,"P",{});var Oxr=s(VJ);mno=r(Oxr,"Examples:"),Oxr.forEach(t),gno=i(Is),m(v4.$$.fragment,Is),Is.forEach(t),hno=i($s),ke=n($s,"DIV",{class:!0});var kt=s(ke);m(T4.$$.fragment,kt),pno=i(kt),WJ=n(kt,"P",{});var Xxr=s(WJ);_no=r(Xxr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Xxr.forEach(t),uno=i(kt),Ia=n(kt,"P",{});var IF=s(Ia);bno=r(IF,"The model class to instantiate is selected based on the "),QJ=n(IF,"CODE",{});var zxr=s(QJ);vno=r(zxr,"model_type"),zxr.forEach(t),Tno=r(IF,` property of the config object (either
passed as an argument or loaded from `),HJ=n(IF,"CODE",{});var Vxr=s(HJ);Fno=r(Vxr,"pretrained_model_name_or_path"),Vxr.forEach(t),Cno=r(IF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),UJ=n(IF,"CODE",{});var Wxr=s(UJ);Mno=r(Wxr,"pretrained_model_name_or_path"),Wxr.forEach(t),Eno=r(IF,":"),IF.forEach(t),yno=i(kt),k=n(kt,"UL",{});var S=s(k);Ip=n(S,"LI",{});var mTe=s(Ip);JJ=n(mTe,"STRONG",{});var Qxr=s(JJ);wno=r(Qxr,"albert"),Qxr.forEach(t),Ano=r(mTe," \u2014 "),aR=n(mTe,"A",{href:!0});var Hxr=s(aR);Lno=r(Hxr,"AlbertForPreTraining"),Hxr.forEach(t),Bno=r(mTe," (ALBERT model)"),mTe.forEach(t),xno=i(S),jp=n(S,"LI",{});var gTe=s(jp);YJ=n(gTe,"STRONG",{});var Uxr=s(YJ);kno=r(Uxr,"bart"),Uxr.forEach(t),Rno=r(gTe," \u2014 "),nR=n(gTe,"A",{href:!0});var Jxr=s(nR);Sno=r(Jxr,"BartForConditionalGeneration"),Jxr.forEach(t),Pno=r(gTe," (BART model)"),gTe.forEach(t),$no=i(S),Np=n(S,"LI",{});var hTe=s(Np);KJ=n(hTe,"STRONG",{});var Yxr=s(KJ);Ino=r(Yxr,"bert"),Yxr.forEach(t),jno=r(hTe," \u2014 "),sR=n(hTe,"A",{href:!0});var Kxr=s(sR);Nno=r(Kxr,"BertForPreTraining"),Kxr.forEach(t),Dno=r(hTe," (BERT model)"),hTe.forEach(t),qno=i(S),Dp=n(S,"LI",{});var pTe=s(Dp);ZJ=n(pTe,"STRONG",{});var Zxr=s(ZJ);Gno=r(Zxr,"big_bird"),Zxr.forEach(t),Ono=r(pTe," \u2014 "),lR=n(pTe,"A",{href:!0});var ekr=s(lR);Xno=r(ekr,"BigBirdForPreTraining"),ekr.forEach(t),zno=r(pTe," (BigBird model)"),pTe.forEach(t),Vno=i(S),qp=n(S,"LI",{});var _Te=s(qp);eY=n(_Te,"STRONG",{});var okr=s(eY);Wno=r(okr,"camembert"),okr.forEach(t),Qno=r(_Te," \u2014 "),iR=n(_Te,"A",{href:!0});var rkr=s(iR);Hno=r(rkr,"CamembertForMaskedLM"),rkr.forEach(t),Uno=r(_Te," (CamemBERT model)"),_Te.forEach(t),Jno=i(S),Gp=n(S,"LI",{});var uTe=s(Gp);oY=n(uTe,"STRONG",{});var tkr=s(oY);Yno=r(tkr,"ctrl"),tkr.forEach(t),Kno=r(uTe," \u2014 "),dR=n(uTe,"A",{href:!0});var akr=s(dR);Zno=r(akr,"CTRLLMHeadModel"),akr.forEach(t),eso=r(uTe," (CTRL model)"),uTe.forEach(t),oso=i(S),Op=n(S,"LI",{});var bTe=s(Op);rY=n(bTe,"STRONG",{});var nkr=s(rY);rso=r(nkr,"deberta"),nkr.forEach(t),tso=r(bTe," \u2014 "),cR=n(bTe,"A",{href:!0});var skr=s(cR);aso=r(skr,"DebertaForMaskedLM"),skr.forEach(t),nso=r(bTe," (DeBERTa model)"),bTe.forEach(t),sso=i(S),Xp=n(S,"LI",{});var vTe=s(Xp);tY=n(vTe,"STRONG",{});var lkr=s(tY);lso=r(lkr,"deberta-v2"),lkr.forEach(t),iso=r(vTe," \u2014 "),fR=n(vTe,"A",{href:!0});var ikr=s(fR);dso=r(ikr,"DebertaV2ForMaskedLM"),ikr.forEach(t),cso=r(vTe," (DeBERTa-v2 model)"),vTe.forEach(t),fso=i(S),zp=n(S,"LI",{});var TTe=s(zp);aY=n(TTe,"STRONG",{});var dkr=s(aY);mso=r(dkr,"distilbert"),dkr.forEach(t),gso=r(TTe," \u2014 "),mR=n(TTe,"A",{href:!0});var ckr=s(mR);hso=r(ckr,"DistilBertForMaskedLM"),ckr.forEach(t),pso=r(TTe," (DistilBERT model)"),TTe.forEach(t),_so=i(S),Vp=n(S,"LI",{});var FTe=s(Vp);nY=n(FTe,"STRONG",{});var fkr=s(nY);uso=r(fkr,"electra"),fkr.forEach(t),bso=r(FTe," \u2014 "),gR=n(FTe,"A",{href:!0});var mkr=s(gR);vso=r(mkr,"ElectraForPreTraining"),mkr.forEach(t),Tso=r(FTe," (ELECTRA model)"),FTe.forEach(t),Fso=i(S),Wp=n(S,"LI",{});var CTe=s(Wp);sY=n(CTe,"STRONG",{});var gkr=s(sY);Cso=r(gkr,"flaubert"),gkr.forEach(t),Mso=r(CTe," \u2014 "),hR=n(CTe,"A",{href:!0});var hkr=s(hR);Eso=r(hkr,"FlaubertWithLMHeadModel"),hkr.forEach(t),yso=r(CTe," (FlauBERT model)"),CTe.forEach(t),wso=i(S),Qp=n(S,"LI",{});var MTe=s(Qp);lY=n(MTe,"STRONG",{});var pkr=s(lY);Aso=r(pkr,"fnet"),pkr.forEach(t),Lso=r(MTe," \u2014 "),pR=n(MTe,"A",{href:!0});var _kr=s(pR);Bso=r(_kr,"FNetForPreTraining"),_kr.forEach(t),xso=r(MTe," (FNet model)"),MTe.forEach(t),kso=i(S),Hp=n(S,"LI",{});var ETe=s(Hp);iY=n(ETe,"STRONG",{});var ukr=s(iY);Rso=r(ukr,"fsmt"),ukr.forEach(t),Sso=r(ETe," \u2014 "),_R=n(ETe,"A",{href:!0});var bkr=s(_R);Pso=r(bkr,"FSMTForConditionalGeneration"),bkr.forEach(t),$so=r(ETe," (FairSeq Machine-Translation model)"),ETe.forEach(t),Iso=i(S),Up=n(S,"LI",{});var yTe=s(Up);dY=n(yTe,"STRONG",{});var vkr=s(dY);jso=r(vkr,"funnel"),vkr.forEach(t),Nso=r(yTe," \u2014 "),uR=n(yTe,"A",{href:!0});var Tkr=s(uR);Dso=r(Tkr,"FunnelForPreTraining"),Tkr.forEach(t),qso=r(yTe," (Funnel Transformer model)"),yTe.forEach(t),Gso=i(S),Jp=n(S,"LI",{});var wTe=s(Jp);cY=n(wTe,"STRONG",{});var Fkr=s(cY);Oso=r(Fkr,"gpt2"),Fkr.forEach(t),Xso=r(wTe," \u2014 "),bR=n(wTe,"A",{href:!0});var Ckr=s(bR);zso=r(Ckr,"GPT2LMHeadModel"),Ckr.forEach(t),Vso=r(wTe," (OpenAI GPT-2 model)"),wTe.forEach(t),Wso=i(S),Yp=n(S,"LI",{});var ATe=s(Yp);fY=n(ATe,"STRONG",{});var Mkr=s(fY);Qso=r(Mkr,"ibert"),Mkr.forEach(t),Hso=r(ATe," \u2014 "),vR=n(ATe,"A",{href:!0});var Ekr=s(vR);Uso=r(Ekr,"IBertForMaskedLM"),Ekr.forEach(t),Jso=r(ATe," (I-BERT model)"),ATe.forEach(t),Yso=i(S),Kp=n(S,"LI",{});var LTe=s(Kp);mY=n(LTe,"STRONG",{});var ykr=s(mY);Kso=r(ykr,"layoutlm"),ykr.forEach(t),Zso=r(LTe," \u2014 "),TR=n(LTe,"A",{href:!0});var wkr=s(TR);elo=r(wkr,"LayoutLMForMaskedLM"),wkr.forEach(t),olo=r(LTe," (LayoutLM model)"),LTe.forEach(t),rlo=i(S),Zp=n(S,"LI",{});var BTe=s(Zp);gY=n(BTe,"STRONG",{});var Akr=s(gY);tlo=r(Akr,"longformer"),Akr.forEach(t),alo=r(BTe," \u2014 "),FR=n(BTe,"A",{href:!0});var Lkr=s(FR);nlo=r(Lkr,"LongformerForMaskedLM"),Lkr.forEach(t),slo=r(BTe," (Longformer model)"),BTe.forEach(t),llo=i(S),e_=n(S,"LI",{});var xTe=s(e_);hY=n(xTe,"STRONG",{});var Bkr=s(hY);ilo=r(Bkr,"lxmert"),Bkr.forEach(t),dlo=r(xTe," \u2014 "),CR=n(xTe,"A",{href:!0});var xkr=s(CR);clo=r(xkr,"LxmertForPreTraining"),xkr.forEach(t),flo=r(xTe," (LXMERT model)"),xTe.forEach(t),mlo=i(S),o_=n(S,"LI",{});var kTe=s(o_);pY=n(kTe,"STRONG",{});var kkr=s(pY);glo=r(kkr,"megatron-bert"),kkr.forEach(t),hlo=r(kTe," \u2014 "),MR=n(kTe,"A",{href:!0});var Rkr=s(MR);plo=r(Rkr,"MegatronBertForPreTraining"),Rkr.forEach(t),_lo=r(kTe," (MegatronBert model)"),kTe.forEach(t),ulo=i(S),r_=n(S,"LI",{});var RTe=s(r_);_Y=n(RTe,"STRONG",{});var Skr=s(_Y);blo=r(Skr,"mobilebert"),Skr.forEach(t),vlo=r(RTe," \u2014 "),ER=n(RTe,"A",{href:!0});var Pkr=s(ER);Tlo=r(Pkr,"MobileBertForPreTraining"),Pkr.forEach(t),Flo=r(RTe," (MobileBERT model)"),RTe.forEach(t),Clo=i(S),t_=n(S,"LI",{});var STe=s(t_);uY=n(STe,"STRONG",{});var $kr=s(uY);Mlo=r($kr,"mpnet"),$kr.forEach(t),Elo=r(STe," \u2014 "),yR=n(STe,"A",{href:!0});var Ikr=s(yR);ylo=r(Ikr,"MPNetForMaskedLM"),Ikr.forEach(t),wlo=r(STe," (MPNet model)"),STe.forEach(t),Alo=i(S),a_=n(S,"LI",{});var PTe=s(a_);bY=n(PTe,"STRONG",{});var jkr=s(bY);Llo=r(jkr,"openai-gpt"),jkr.forEach(t),Blo=r(PTe," \u2014 "),wR=n(PTe,"A",{href:!0});var Nkr=s(wR);xlo=r(Nkr,"OpenAIGPTLMHeadModel"),Nkr.forEach(t),klo=r(PTe," (OpenAI GPT model)"),PTe.forEach(t),Rlo=i(S),n_=n(S,"LI",{});var $Te=s(n_);vY=n($Te,"STRONG",{});var Dkr=s(vY);Slo=r(Dkr,"retribert"),Dkr.forEach(t),Plo=r($Te," \u2014 "),AR=n($Te,"A",{href:!0});var qkr=s(AR);$lo=r(qkr,"RetriBertModel"),qkr.forEach(t),Ilo=r($Te," (RetriBERT model)"),$Te.forEach(t),jlo=i(S),s_=n(S,"LI",{});var ITe=s(s_);TY=n(ITe,"STRONG",{});var Gkr=s(TY);Nlo=r(Gkr,"roberta"),Gkr.forEach(t),Dlo=r(ITe," \u2014 "),LR=n(ITe,"A",{href:!0});var Okr=s(LR);qlo=r(Okr,"RobertaForMaskedLM"),Okr.forEach(t),Glo=r(ITe," (RoBERTa model)"),ITe.forEach(t),Olo=i(S),l_=n(S,"LI",{});var jTe=s(l_);FY=n(jTe,"STRONG",{});var Xkr=s(FY);Xlo=r(Xkr,"squeezebert"),Xkr.forEach(t),zlo=r(jTe," \u2014 "),BR=n(jTe,"A",{href:!0});var zkr=s(BR);Vlo=r(zkr,"SqueezeBertForMaskedLM"),zkr.forEach(t),Wlo=r(jTe," (SqueezeBERT model)"),jTe.forEach(t),Qlo=i(S),i_=n(S,"LI",{});var NTe=s(i_);CY=n(NTe,"STRONG",{});var Vkr=s(CY);Hlo=r(Vkr,"t5"),Vkr.forEach(t),Ulo=r(NTe," \u2014 "),xR=n(NTe,"A",{href:!0});var Wkr=s(xR);Jlo=r(Wkr,"T5ForConditionalGeneration"),Wkr.forEach(t),Ylo=r(NTe," (T5 model)"),NTe.forEach(t),Klo=i(S),d_=n(S,"LI",{});var DTe=s(d_);MY=n(DTe,"STRONG",{});var Qkr=s(MY);Zlo=r(Qkr,"tapas"),Qkr.forEach(t),eio=r(DTe," \u2014 "),kR=n(DTe,"A",{href:!0});var Hkr=s(kR);oio=r(Hkr,"TapasForMaskedLM"),Hkr.forEach(t),rio=r(DTe," (TAPAS model)"),DTe.forEach(t),tio=i(S),c_=n(S,"LI",{});var qTe=s(c_);EY=n(qTe,"STRONG",{});var Ukr=s(EY);aio=r(Ukr,"transfo-xl"),Ukr.forEach(t),nio=r(qTe," \u2014 "),RR=n(qTe,"A",{href:!0});var Jkr=s(RR);sio=r(Jkr,"TransfoXLLMHeadModel"),Jkr.forEach(t),lio=r(qTe," (Transformer-XL model)"),qTe.forEach(t),iio=i(S),f_=n(S,"LI",{});var GTe=s(f_);yY=n(GTe,"STRONG",{});var Ykr=s(yY);dio=r(Ykr,"unispeech"),Ykr.forEach(t),cio=r(GTe," \u2014 "),SR=n(GTe,"A",{href:!0});var Kkr=s(SR);fio=r(Kkr,"UniSpeechForPreTraining"),Kkr.forEach(t),mio=r(GTe," (UniSpeech model)"),GTe.forEach(t),gio=i(S),m_=n(S,"LI",{});var OTe=s(m_);wY=n(OTe,"STRONG",{});var Zkr=s(wY);hio=r(Zkr,"unispeech-sat"),Zkr.forEach(t),pio=r(OTe," \u2014 "),PR=n(OTe,"A",{href:!0});var eRr=s(PR);_io=r(eRr,"UniSpeechSatForPreTraining"),eRr.forEach(t),uio=r(OTe," (UniSpeechSat model)"),OTe.forEach(t),bio=i(S),g_=n(S,"LI",{});var XTe=s(g_);AY=n(XTe,"STRONG",{});var oRr=s(AY);vio=r(oRr,"visual_bert"),oRr.forEach(t),Tio=r(XTe," \u2014 "),$R=n(XTe,"A",{href:!0});var rRr=s($R);Fio=r(rRr,"VisualBertForPreTraining"),rRr.forEach(t),Cio=r(XTe," (VisualBert model)"),XTe.forEach(t),Mio=i(S),h_=n(S,"LI",{});var zTe=s(h_);LY=n(zTe,"STRONG",{});var tRr=s(LY);Eio=r(tRr,"vit_mae"),tRr.forEach(t),yio=r(zTe," \u2014 "),IR=n(zTe,"A",{href:!0});var aRr=s(IR);wio=r(aRr,"ViTMAEForPreTraining"),aRr.forEach(t),Aio=r(zTe," (ViTMAE model)"),zTe.forEach(t),Lio=i(S),p_=n(S,"LI",{});var VTe=s(p_);BY=n(VTe,"STRONG",{});var nRr=s(BY);Bio=r(nRr,"wav2vec2"),nRr.forEach(t),xio=r(VTe," \u2014 "),jR=n(VTe,"A",{href:!0});var sRr=s(jR);kio=r(sRr,"Wav2Vec2ForPreTraining"),sRr.forEach(t),Rio=r(VTe," (Wav2Vec2 model)"),VTe.forEach(t),Sio=i(S),__=n(S,"LI",{});var WTe=s(__);xY=n(WTe,"STRONG",{});var lRr=s(xY);Pio=r(lRr,"xlm"),lRr.forEach(t),$io=r(WTe," \u2014 "),NR=n(WTe,"A",{href:!0});var iRr=s(NR);Iio=r(iRr,"XLMWithLMHeadModel"),iRr.forEach(t),jio=r(WTe," (XLM model)"),WTe.forEach(t),Nio=i(S),u_=n(S,"LI",{});var QTe=s(u_);kY=n(QTe,"STRONG",{});var dRr=s(kY);Dio=r(dRr,"xlm-roberta"),dRr.forEach(t),qio=r(QTe," \u2014 "),DR=n(QTe,"A",{href:!0});var cRr=s(DR);Gio=r(cRr,"XLMRobertaForMaskedLM"),cRr.forEach(t),Oio=r(QTe," (XLM-RoBERTa model)"),QTe.forEach(t),Xio=i(S),b_=n(S,"LI",{});var HTe=s(b_);RY=n(HTe,"STRONG",{});var fRr=s(RY);zio=r(fRr,"xlm-roberta-xl"),fRr.forEach(t),Vio=r(HTe," \u2014 "),qR=n(HTe,"A",{href:!0});var mRr=s(qR);Wio=r(mRr,"XLMRobertaXLForMaskedLM"),mRr.forEach(t),Qio=r(HTe," (XLM-RoBERTa-XL model)"),HTe.forEach(t),Hio=i(S),v_=n(S,"LI",{});var UTe=s(v_);SY=n(UTe,"STRONG",{});var gRr=s(SY);Uio=r(gRr,"xlnet"),gRr.forEach(t),Jio=r(UTe," \u2014 "),GR=n(UTe,"A",{href:!0});var hRr=s(GR);Yio=r(hRr,"XLNetLMHeadModel"),hRr.forEach(t),Kio=r(UTe," (XLNet model)"),UTe.forEach(t),S.forEach(t),Zio=i(kt),T_=n(kt,"P",{});var JTe=s(T_);edo=r(JTe,"The model is set in evaluation mode by default using "),PY=n(JTe,"CODE",{});var pRr=s(PY);odo=r(pRr,"model.eval()"),pRr.forEach(t),rdo=r(JTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$Y=n(JTe,"CODE",{});var _Rr=s($Y);tdo=r(_Rr,"model.train()"),_Rr.forEach(t),JTe.forEach(t),ado=i(kt),IY=n(kt,"P",{});var uRr=s(IY);ndo=r(uRr,"Examples:"),uRr.forEach(t),sdo=i(kt),m(F4.$$.fragment,kt),kt.forEach(t),$s.forEach(t),$Ae=i(d),Di=n(d,"H2",{class:!0});var qLe=s(Di);F_=n(qLe,"A",{id:!0,class:!0,href:!0});var bRr=s(F_);jY=n(bRr,"SPAN",{});var vRr=s(jY);m(C4.$$.fragment,vRr),vRr.forEach(t),bRr.forEach(t),ldo=i(qLe),NY=n(qLe,"SPAN",{});var TRr=s(NY);ido=r(TRr,"AutoModelForCausalLM"),TRr.forEach(t),qLe.forEach(t),IAe=i(d),Wo=n(d,"DIV",{class:!0});var js=s(Wo);m(M4.$$.fragment,js),ddo=i(js),qi=n(js,"P",{});var tX=s(qi);cdo=r(tX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),DY=n(tX,"CODE",{});var FRr=s(DY);fdo=r(FRr,"from_pretrained()"),FRr.forEach(t),mdo=r(tX,"class method or the "),qY=n(tX,"CODE",{});var CRr=s(qY);gdo=r(CRr,"from_config()"),CRr.forEach(t),hdo=r(tX,`class
method.`),tX.forEach(t),pdo=i(js),E4=n(js,"P",{});var GLe=s(E4);_do=r(GLe,"This class cannot be instantiated directly using "),GY=n(GLe,"CODE",{});var MRr=s(GY);udo=r(MRr,"__init__()"),MRr.forEach(t),bdo=r(GLe," (throws an error)."),GLe.forEach(t),vdo=i(js),Nr=n(js,"DIV",{class:!0});var Ns=s(Nr);m(y4.$$.fragment,Ns),Tdo=i(Ns),OY=n(Ns,"P",{});var ERr=s(OY);Fdo=r(ERr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ERr.forEach(t),Cdo=i(Ns),Gi=n(Ns,"P",{});var aX=s(Gi);Mdo=r(aX,`Note:
Loading a model from its configuration file does `),XY=n(aX,"STRONG",{});var yRr=s(XY);Edo=r(yRr,"not"),yRr.forEach(t),ydo=r(aX,` load the model weights. It only affects the
model\u2019s configuration. Use `),zY=n(aX,"CODE",{});var wRr=s(zY);wdo=r(wRr,"from_pretrained()"),wRr.forEach(t),Ado=r(aX,"to load the model weights."),aX.forEach(t),Ldo=i(Ns),VY=n(Ns,"P",{});var ARr=s(VY);Bdo=r(ARr,"Examples:"),ARr.forEach(t),xdo=i(Ns),m(w4.$$.fragment,Ns),Ns.forEach(t),kdo=i(js),Re=n(js,"DIV",{class:!0});var Rt=s(Re);m(A4.$$.fragment,Rt),Rdo=i(Rt),WY=n(Rt,"P",{});var LRr=s(WY);Sdo=r(LRr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),LRr.forEach(t),Pdo=i(Rt),ja=n(Rt,"P",{});var jF=s(ja);$do=r(jF,"The model class to instantiate is selected based on the "),QY=n(jF,"CODE",{});var BRr=s(QY);Ido=r(BRr,"model_type"),BRr.forEach(t),jdo=r(jF,` property of the config object (either
passed as an argument or loaded from `),HY=n(jF,"CODE",{});var xRr=s(HY);Ndo=r(xRr,"pretrained_model_name_or_path"),xRr.forEach(t),Ddo=r(jF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),UY=n(jF,"CODE",{});var kRr=s(UY);qdo=r(kRr,"pretrained_model_name_or_path"),kRr.forEach(t),Gdo=r(jF,":"),jF.forEach(t),Odo=i(Rt),I=n(Rt,"UL",{});var D=s(I);C_=n(D,"LI",{});var YTe=s(C_);JY=n(YTe,"STRONG",{});var RRr=s(JY);Xdo=r(RRr,"bart"),RRr.forEach(t),zdo=r(YTe," \u2014 "),OR=n(YTe,"A",{href:!0});var SRr=s(OR);Vdo=r(SRr,"BartForCausalLM"),SRr.forEach(t),Wdo=r(YTe," (BART model)"),YTe.forEach(t),Qdo=i(D),M_=n(D,"LI",{});var KTe=s(M_);YY=n(KTe,"STRONG",{});var PRr=s(YY);Hdo=r(PRr,"bert"),PRr.forEach(t),Udo=r(KTe," \u2014 "),XR=n(KTe,"A",{href:!0});var $Rr=s(XR);Jdo=r($Rr,"BertLMHeadModel"),$Rr.forEach(t),Ydo=r(KTe," (BERT model)"),KTe.forEach(t),Kdo=i(D),E_=n(D,"LI",{});var ZTe=s(E_);KY=n(ZTe,"STRONG",{});var IRr=s(KY);Zdo=r(IRr,"bert-generation"),IRr.forEach(t),eco=r(ZTe," \u2014 "),zR=n(ZTe,"A",{href:!0});var jRr=s(zR);oco=r(jRr,"BertGenerationDecoder"),jRr.forEach(t),rco=r(ZTe," (Bert Generation model)"),ZTe.forEach(t),tco=i(D),y_=n(D,"LI",{});var e7e=s(y_);ZY=n(e7e,"STRONG",{});var NRr=s(ZY);aco=r(NRr,"big_bird"),NRr.forEach(t),nco=r(e7e," \u2014 "),VR=n(e7e,"A",{href:!0});var DRr=s(VR);sco=r(DRr,"BigBirdForCausalLM"),DRr.forEach(t),lco=r(e7e," (BigBird model)"),e7e.forEach(t),ico=i(D),w_=n(D,"LI",{});var o7e=s(w_);eK=n(o7e,"STRONG",{});var qRr=s(eK);dco=r(qRr,"bigbird_pegasus"),qRr.forEach(t),cco=r(o7e," \u2014 "),WR=n(o7e,"A",{href:!0});var GRr=s(WR);fco=r(GRr,"BigBirdPegasusForCausalLM"),GRr.forEach(t),mco=r(o7e," (BigBirdPegasus model)"),o7e.forEach(t),gco=i(D),A_=n(D,"LI",{});var r7e=s(A_);oK=n(r7e,"STRONG",{});var ORr=s(oK);hco=r(ORr,"blenderbot"),ORr.forEach(t),pco=r(r7e," \u2014 "),QR=n(r7e,"A",{href:!0});var XRr=s(QR);_co=r(XRr,"BlenderbotForCausalLM"),XRr.forEach(t),uco=r(r7e," (Blenderbot model)"),r7e.forEach(t),bco=i(D),L_=n(D,"LI",{});var t7e=s(L_);rK=n(t7e,"STRONG",{});var zRr=s(rK);vco=r(zRr,"blenderbot-small"),zRr.forEach(t),Tco=r(t7e," \u2014 "),HR=n(t7e,"A",{href:!0});var VRr=s(HR);Fco=r(VRr,"BlenderbotSmallForCausalLM"),VRr.forEach(t),Cco=r(t7e," (BlenderbotSmall model)"),t7e.forEach(t),Mco=i(D),B_=n(D,"LI",{});var a7e=s(B_);tK=n(a7e,"STRONG",{});var WRr=s(tK);Eco=r(WRr,"camembert"),WRr.forEach(t),yco=r(a7e," \u2014 "),UR=n(a7e,"A",{href:!0});var QRr=s(UR);wco=r(QRr,"CamembertForCausalLM"),QRr.forEach(t),Aco=r(a7e," (CamemBERT model)"),a7e.forEach(t),Lco=i(D),x_=n(D,"LI",{});var n7e=s(x_);aK=n(n7e,"STRONG",{});var HRr=s(aK);Bco=r(HRr,"ctrl"),HRr.forEach(t),xco=r(n7e," \u2014 "),JR=n(n7e,"A",{href:!0});var URr=s(JR);kco=r(URr,"CTRLLMHeadModel"),URr.forEach(t),Rco=r(n7e," (CTRL model)"),n7e.forEach(t),Sco=i(D),k_=n(D,"LI",{});var s7e=s(k_);nK=n(s7e,"STRONG",{});var JRr=s(nK);Pco=r(JRr,"electra"),JRr.forEach(t),$co=r(s7e," \u2014 "),YR=n(s7e,"A",{href:!0});var YRr=s(YR);Ico=r(YRr,"ElectraForCausalLM"),YRr.forEach(t),jco=r(s7e," (ELECTRA model)"),s7e.forEach(t),Nco=i(D),R_=n(D,"LI",{});var l7e=s(R_);sK=n(l7e,"STRONG",{});var KRr=s(sK);Dco=r(KRr,"gpt2"),KRr.forEach(t),qco=r(l7e," \u2014 "),KR=n(l7e,"A",{href:!0});var ZRr=s(KR);Gco=r(ZRr,"GPT2LMHeadModel"),ZRr.forEach(t),Oco=r(l7e," (OpenAI GPT-2 model)"),l7e.forEach(t),Xco=i(D),S_=n(D,"LI",{});var i7e=s(S_);lK=n(i7e,"STRONG",{});var eSr=s(lK);zco=r(eSr,"gpt_neo"),eSr.forEach(t),Vco=r(i7e," \u2014 "),ZR=n(i7e,"A",{href:!0});var oSr=s(ZR);Wco=r(oSr,"GPTNeoForCausalLM"),oSr.forEach(t),Qco=r(i7e," (GPT Neo model)"),i7e.forEach(t),Hco=i(D),P_=n(D,"LI",{});var d7e=s(P_);iK=n(d7e,"STRONG",{});var rSr=s(iK);Uco=r(rSr,"gptj"),rSr.forEach(t),Jco=r(d7e," \u2014 "),eS=n(d7e,"A",{href:!0});var tSr=s(eS);Yco=r(tSr,"GPTJForCausalLM"),tSr.forEach(t),Kco=r(d7e," (GPT-J model)"),d7e.forEach(t),Zco=i(D),$_=n(D,"LI",{});var c7e=s($_);dK=n(c7e,"STRONG",{});var aSr=s(dK);efo=r(aSr,"marian"),aSr.forEach(t),ofo=r(c7e," \u2014 "),oS=n(c7e,"A",{href:!0});var nSr=s(oS);rfo=r(nSr,"MarianForCausalLM"),nSr.forEach(t),tfo=r(c7e," (Marian model)"),c7e.forEach(t),afo=i(D),I_=n(D,"LI",{});var f7e=s(I_);cK=n(f7e,"STRONG",{});var sSr=s(cK);nfo=r(sSr,"mbart"),sSr.forEach(t),sfo=r(f7e," \u2014 "),rS=n(f7e,"A",{href:!0});var lSr=s(rS);lfo=r(lSr,"MBartForCausalLM"),lSr.forEach(t),ifo=r(f7e," (mBART model)"),f7e.forEach(t),dfo=i(D),j_=n(D,"LI",{});var m7e=s(j_);fK=n(m7e,"STRONG",{});var iSr=s(fK);cfo=r(iSr,"megatron-bert"),iSr.forEach(t),ffo=r(m7e," \u2014 "),tS=n(m7e,"A",{href:!0});var dSr=s(tS);mfo=r(dSr,"MegatronBertForCausalLM"),dSr.forEach(t),gfo=r(m7e," (MegatronBert model)"),m7e.forEach(t),hfo=i(D),N_=n(D,"LI",{});var g7e=s(N_);mK=n(g7e,"STRONG",{});var cSr=s(mK);pfo=r(cSr,"openai-gpt"),cSr.forEach(t),_fo=r(g7e," \u2014 "),aS=n(g7e,"A",{href:!0});var fSr=s(aS);ufo=r(fSr,"OpenAIGPTLMHeadModel"),fSr.forEach(t),bfo=r(g7e," (OpenAI GPT model)"),g7e.forEach(t),vfo=i(D),D_=n(D,"LI",{});var h7e=s(D_);gK=n(h7e,"STRONG",{});var mSr=s(gK);Tfo=r(mSr,"pegasus"),mSr.forEach(t),Ffo=r(h7e," \u2014 "),nS=n(h7e,"A",{href:!0});var gSr=s(nS);Cfo=r(gSr,"PegasusForCausalLM"),gSr.forEach(t),Mfo=r(h7e," (Pegasus model)"),h7e.forEach(t),Efo=i(D),q_=n(D,"LI",{});var p7e=s(q_);hK=n(p7e,"STRONG",{});var hSr=s(hK);yfo=r(hSr,"prophetnet"),hSr.forEach(t),wfo=r(p7e," \u2014 "),sS=n(p7e,"A",{href:!0});var pSr=s(sS);Afo=r(pSr,"ProphetNetForCausalLM"),pSr.forEach(t),Lfo=r(p7e," (ProphetNet model)"),p7e.forEach(t),Bfo=i(D),G_=n(D,"LI",{});var _7e=s(G_);pK=n(_7e,"STRONG",{});var _Sr=s(pK);xfo=r(_Sr,"qdqbert"),_Sr.forEach(t),kfo=r(_7e," \u2014 "),lS=n(_7e,"A",{href:!0});var uSr=s(lS);Rfo=r(uSr,"QDQBertLMHeadModel"),uSr.forEach(t),Sfo=r(_7e," (QDQBert model)"),_7e.forEach(t),Pfo=i(D),O_=n(D,"LI",{});var u7e=s(O_);_K=n(u7e,"STRONG",{});var bSr=s(_K);$fo=r(bSr,"reformer"),bSr.forEach(t),Ifo=r(u7e," \u2014 "),iS=n(u7e,"A",{href:!0});var vSr=s(iS);jfo=r(vSr,"ReformerModelWithLMHead"),vSr.forEach(t),Nfo=r(u7e," (Reformer model)"),u7e.forEach(t),Dfo=i(D),X_=n(D,"LI",{});var b7e=s(X_);uK=n(b7e,"STRONG",{});var TSr=s(uK);qfo=r(TSr,"rembert"),TSr.forEach(t),Gfo=r(b7e," \u2014 "),dS=n(b7e,"A",{href:!0});var FSr=s(dS);Ofo=r(FSr,"RemBertForCausalLM"),FSr.forEach(t),Xfo=r(b7e," (RemBERT model)"),b7e.forEach(t),zfo=i(D),z_=n(D,"LI",{});var v7e=s(z_);bK=n(v7e,"STRONG",{});var CSr=s(bK);Vfo=r(CSr,"roberta"),CSr.forEach(t),Wfo=r(v7e," \u2014 "),cS=n(v7e,"A",{href:!0});var MSr=s(cS);Qfo=r(MSr,"RobertaForCausalLM"),MSr.forEach(t),Hfo=r(v7e," (RoBERTa model)"),v7e.forEach(t),Ufo=i(D),V_=n(D,"LI",{});var T7e=s(V_);vK=n(T7e,"STRONG",{});var ESr=s(vK);Jfo=r(ESr,"roformer"),ESr.forEach(t),Yfo=r(T7e," \u2014 "),fS=n(T7e,"A",{href:!0});var ySr=s(fS);Kfo=r(ySr,"RoFormerForCausalLM"),ySr.forEach(t),Zfo=r(T7e," (RoFormer model)"),T7e.forEach(t),emo=i(D),W_=n(D,"LI",{});var F7e=s(W_);TK=n(F7e,"STRONG",{});var wSr=s(TK);omo=r(wSr,"speech_to_text_2"),wSr.forEach(t),rmo=r(F7e," \u2014 "),mS=n(F7e,"A",{href:!0});var ASr=s(mS);tmo=r(ASr,"Speech2Text2ForCausalLM"),ASr.forEach(t),amo=r(F7e," (Speech2Text2 model)"),F7e.forEach(t),nmo=i(D),Q_=n(D,"LI",{});var C7e=s(Q_);FK=n(C7e,"STRONG",{});var LSr=s(FK);smo=r(LSr,"transfo-xl"),LSr.forEach(t),lmo=r(C7e," \u2014 "),gS=n(C7e,"A",{href:!0});var BSr=s(gS);imo=r(BSr,"TransfoXLLMHeadModel"),BSr.forEach(t),dmo=r(C7e," (Transformer-XL model)"),C7e.forEach(t),cmo=i(D),H_=n(D,"LI",{});var M7e=s(H_);CK=n(M7e,"STRONG",{});var xSr=s(CK);fmo=r(xSr,"trocr"),xSr.forEach(t),mmo=r(M7e," \u2014 "),hS=n(M7e,"A",{href:!0});var kSr=s(hS);gmo=r(kSr,"TrOCRForCausalLM"),kSr.forEach(t),hmo=r(M7e," (TrOCR model)"),M7e.forEach(t),pmo=i(D),U_=n(D,"LI",{});var E7e=s(U_);MK=n(E7e,"STRONG",{});var RSr=s(MK);_mo=r(RSr,"xglm"),RSr.forEach(t),umo=r(E7e," \u2014 "),pS=n(E7e,"A",{href:!0});var SSr=s(pS);bmo=r(SSr,"XGLMForCausalLM"),SSr.forEach(t),vmo=r(E7e," (XGLM model)"),E7e.forEach(t),Tmo=i(D),J_=n(D,"LI",{});var y7e=s(J_);EK=n(y7e,"STRONG",{});var PSr=s(EK);Fmo=r(PSr,"xlm"),PSr.forEach(t),Cmo=r(y7e," \u2014 "),_S=n(y7e,"A",{href:!0});var $Sr=s(_S);Mmo=r($Sr,"XLMWithLMHeadModel"),$Sr.forEach(t),Emo=r(y7e," (XLM model)"),y7e.forEach(t),ymo=i(D),Y_=n(D,"LI",{});var w7e=s(Y_);yK=n(w7e,"STRONG",{});var ISr=s(yK);wmo=r(ISr,"xlm-prophetnet"),ISr.forEach(t),Amo=r(w7e," \u2014 "),uS=n(w7e,"A",{href:!0});var jSr=s(uS);Lmo=r(jSr,"XLMProphetNetForCausalLM"),jSr.forEach(t),Bmo=r(w7e," (XLMProphetNet model)"),w7e.forEach(t),xmo=i(D),K_=n(D,"LI",{});var A7e=s(K_);wK=n(A7e,"STRONG",{});var NSr=s(wK);kmo=r(NSr,"xlm-roberta"),NSr.forEach(t),Rmo=r(A7e," \u2014 "),bS=n(A7e,"A",{href:!0});var DSr=s(bS);Smo=r(DSr,"XLMRobertaForCausalLM"),DSr.forEach(t),Pmo=r(A7e," (XLM-RoBERTa model)"),A7e.forEach(t),$mo=i(D),Z_=n(D,"LI",{});var L7e=s(Z_);AK=n(L7e,"STRONG",{});var qSr=s(AK);Imo=r(qSr,"xlm-roberta-xl"),qSr.forEach(t),jmo=r(L7e," \u2014 "),vS=n(L7e,"A",{href:!0});var GSr=s(vS);Nmo=r(GSr,"XLMRobertaXLForCausalLM"),GSr.forEach(t),Dmo=r(L7e," (XLM-RoBERTa-XL model)"),L7e.forEach(t),qmo=i(D),eu=n(D,"LI",{});var B7e=s(eu);LK=n(B7e,"STRONG",{});var OSr=s(LK);Gmo=r(OSr,"xlnet"),OSr.forEach(t),Omo=r(B7e," \u2014 "),TS=n(B7e,"A",{href:!0});var XSr=s(TS);Xmo=r(XSr,"XLNetLMHeadModel"),XSr.forEach(t),zmo=r(B7e," (XLNet model)"),B7e.forEach(t),D.forEach(t),Vmo=i(Rt),ou=n(Rt,"P",{});var x7e=s(ou);Wmo=r(x7e,"The model is set in evaluation mode by default using "),BK=n(x7e,"CODE",{});var zSr=s(BK);Qmo=r(zSr,"model.eval()"),zSr.forEach(t),Hmo=r(x7e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xK=n(x7e,"CODE",{});var VSr=s(xK);Umo=r(VSr,"model.train()"),VSr.forEach(t),x7e.forEach(t),Jmo=i(Rt),kK=n(Rt,"P",{});var WSr=s(kK);Ymo=r(WSr,"Examples:"),WSr.forEach(t),Kmo=i(Rt),m(L4.$$.fragment,Rt),Rt.forEach(t),js.forEach(t),jAe=i(d),Oi=n(d,"H2",{class:!0});var OLe=s(Oi);ru=n(OLe,"A",{id:!0,class:!0,href:!0});var QSr=s(ru);RK=n(QSr,"SPAN",{});var HSr=s(RK);m(B4.$$.fragment,HSr),HSr.forEach(t),QSr.forEach(t),Zmo=i(OLe),SK=n(OLe,"SPAN",{});var USr=s(SK);ego=r(USr,"AutoModelForMaskedLM"),USr.forEach(t),OLe.forEach(t),NAe=i(d),Qo=n(d,"DIV",{class:!0});var Ds=s(Qo);m(x4.$$.fragment,Ds),ogo=i(Ds),Xi=n(Ds,"P",{});var nX=s(Xi);rgo=r(nX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),PK=n(nX,"CODE",{});var JSr=s(PK);tgo=r(JSr,"from_pretrained()"),JSr.forEach(t),ago=r(nX,"class method or the "),$K=n(nX,"CODE",{});var YSr=s($K);ngo=r(YSr,"from_config()"),YSr.forEach(t),sgo=r(nX,`class
method.`),nX.forEach(t),lgo=i(Ds),k4=n(Ds,"P",{});var XLe=s(k4);igo=r(XLe,"This class cannot be instantiated directly using "),IK=n(XLe,"CODE",{});var KSr=s(IK);dgo=r(KSr,"__init__()"),KSr.forEach(t),cgo=r(XLe," (throws an error)."),XLe.forEach(t),fgo=i(Ds),Dr=n(Ds,"DIV",{class:!0});var qs=s(Dr);m(R4.$$.fragment,qs),mgo=i(qs),jK=n(qs,"P",{});var ZSr=s(jK);ggo=r(ZSr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),ZSr.forEach(t),hgo=i(qs),zi=n(qs,"P",{});var sX=s(zi);pgo=r(sX,`Note:
Loading a model from its configuration file does `),NK=n(sX,"STRONG",{});var ePr=s(NK);_go=r(ePr,"not"),ePr.forEach(t),ugo=r(sX,` load the model weights. It only affects the
model\u2019s configuration. Use `),DK=n(sX,"CODE",{});var oPr=s(DK);bgo=r(oPr,"from_pretrained()"),oPr.forEach(t),vgo=r(sX,"to load the model weights."),sX.forEach(t),Tgo=i(qs),qK=n(qs,"P",{});var rPr=s(qK);Fgo=r(rPr,"Examples:"),rPr.forEach(t),Cgo=i(qs),m(S4.$$.fragment,qs),qs.forEach(t),Mgo=i(Ds),Se=n(Ds,"DIV",{class:!0});var St=s(Se);m(P4.$$.fragment,St),Ego=i(St),GK=n(St,"P",{});var tPr=s(GK);ygo=r(tPr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),tPr.forEach(t),wgo=i(St),Na=n(St,"P",{});var NF=s(Na);Ago=r(NF,"The model class to instantiate is selected based on the "),OK=n(NF,"CODE",{});var aPr=s(OK);Lgo=r(aPr,"model_type"),aPr.forEach(t),Bgo=r(NF,` property of the config object (either
passed as an argument or loaded from `),XK=n(NF,"CODE",{});var nPr=s(XK);xgo=r(nPr,"pretrained_model_name_or_path"),nPr.forEach(t),kgo=r(NF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zK=n(NF,"CODE",{});var sPr=s(zK);Rgo=r(sPr,"pretrained_model_name_or_path"),sPr.forEach(t),Sgo=r(NF,":"),NF.forEach(t),Pgo=i(St),$=n(St,"UL",{});var j=s($);tu=n(j,"LI",{});var k7e=s(tu);VK=n(k7e,"STRONG",{});var lPr=s(VK);$go=r(lPr,"albert"),lPr.forEach(t),Igo=r(k7e," \u2014 "),FS=n(k7e,"A",{href:!0});var iPr=s(FS);jgo=r(iPr,"AlbertForMaskedLM"),iPr.forEach(t),Ngo=r(k7e," (ALBERT model)"),k7e.forEach(t),Dgo=i(j),au=n(j,"LI",{});var R7e=s(au);WK=n(R7e,"STRONG",{});var dPr=s(WK);qgo=r(dPr,"bart"),dPr.forEach(t),Ggo=r(R7e," \u2014 "),CS=n(R7e,"A",{href:!0});var cPr=s(CS);Ogo=r(cPr,"BartForConditionalGeneration"),cPr.forEach(t),Xgo=r(R7e," (BART model)"),R7e.forEach(t),zgo=i(j),nu=n(j,"LI",{});var S7e=s(nu);QK=n(S7e,"STRONG",{});var fPr=s(QK);Vgo=r(fPr,"bert"),fPr.forEach(t),Wgo=r(S7e," \u2014 "),MS=n(S7e,"A",{href:!0});var mPr=s(MS);Qgo=r(mPr,"BertForMaskedLM"),mPr.forEach(t),Hgo=r(S7e," (BERT model)"),S7e.forEach(t),Ugo=i(j),su=n(j,"LI",{});var P7e=s(su);HK=n(P7e,"STRONG",{});var gPr=s(HK);Jgo=r(gPr,"big_bird"),gPr.forEach(t),Ygo=r(P7e," \u2014 "),ES=n(P7e,"A",{href:!0});var hPr=s(ES);Kgo=r(hPr,"BigBirdForMaskedLM"),hPr.forEach(t),Zgo=r(P7e," (BigBird model)"),P7e.forEach(t),eho=i(j),lu=n(j,"LI",{});var $7e=s(lu);UK=n($7e,"STRONG",{});var pPr=s(UK);oho=r(pPr,"camembert"),pPr.forEach(t),rho=r($7e," \u2014 "),yS=n($7e,"A",{href:!0});var _Pr=s(yS);tho=r(_Pr,"CamembertForMaskedLM"),_Pr.forEach(t),aho=r($7e," (CamemBERT model)"),$7e.forEach(t),nho=i(j),iu=n(j,"LI",{});var I7e=s(iu);JK=n(I7e,"STRONG",{});var uPr=s(JK);sho=r(uPr,"convbert"),uPr.forEach(t),lho=r(I7e," \u2014 "),wS=n(I7e,"A",{href:!0});var bPr=s(wS);iho=r(bPr,"ConvBertForMaskedLM"),bPr.forEach(t),dho=r(I7e," (ConvBERT model)"),I7e.forEach(t),cho=i(j),du=n(j,"LI",{});var j7e=s(du);YK=n(j7e,"STRONG",{});var vPr=s(YK);fho=r(vPr,"deberta"),vPr.forEach(t),mho=r(j7e," \u2014 "),AS=n(j7e,"A",{href:!0});var TPr=s(AS);gho=r(TPr,"DebertaForMaskedLM"),TPr.forEach(t),hho=r(j7e," (DeBERTa model)"),j7e.forEach(t),pho=i(j),cu=n(j,"LI",{});var N7e=s(cu);KK=n(N7e,"STRONG",{});var FPr=s(KK);_ho=r(FPr,"deberta-v2"),FPr.forEach(t),uho=r(N7e," \u2014 "),LS=n(N7e,"A",{href:!0});var CPr=s(LS);bho=r(CPr,"DebertaV2ForMaskedLM"),CPr.forEach(t),vho=r(N7e," (DeBERTa-v2 model)"),N7e.forEach(t),Tho=i(j),fu=n(j,"LI",{});var D7e=s(fu);ZK=n(D7e,"STRONG",{});var MPr=s(ZK);Fho=r(MPr,"distilbert"),MPr.forEach(t),Cho=r(D7e," \u2014 "),BS=n(D7e,"A",{href:!0});var EPr=s(BS);Mho=r(EPr,"DistilBertForMaskedLM"),EPr.forEach(t),Eho=r(D7e," (DistilBERT model)"),D7e.forEach(t),yho=i(j),mu=n(j,"LI",{});var q7e=s(mu);eZ=n(q7e,"STRONG",{});var yPr=s(eZ);who=r(yPr,"electra"),yPr.forEach(t),Aho=r(q7e," \u2014 "),xS=n(q7e,"A",{href:!0});var wPr=s(xS);Lho=r(wPr,"ElectraForMaskedLM"),wPr.forEach(t),Bho=r(q7e," (ELECTRA model)"),q7e.forEach(t),xho=i(j),gu=n(j,"LI",{});var G7e=s(gu);oZ=n(G7e,"STRONG",{});var APr=s(oZ);kho=r(APr,"flaubert"),APr.forEach(t),Rho=r(G7e," \u2014 "),kS=n(G7e,"A",{href:!0});var LPr=s(kS);Sho=r(LPr,"FlaubertWithLMHeadModel"),LPr.forEach(t),Pho=r(G7e," (FlauBERT model)"),G7e.forEach(t),$ho=i(j),hu=n(j,"LI",{});var O7e=s(hu);rZ=n(O7e,"STRONG",{});var BPr=s(rZ);Iho=r(BPr,"fnet"),BPr.forEach(t),jho=r(O7e," \u2014 "),RS=n(O7e,"A",{href:!0});var xPr=s(RS);Nho=r(xPr,"FNetForMaskedLM"),xPr.forEach(t),Dho=r(O7e," (FNet model)"),O7e.forEach(t),qho=i(j),pu=n(j,"LI",{});var X7e=s(pu);tZ=n(X7e,"STRONG",{});var kPr=s(tZ);Gho=r(kPr,"funnel"),kPr.forEach(t),Oho=r(X7e," \u2014 "),SS=n(X7e,"A",{href:!0});var RPr=s(SS);Xho=r(RPr,"FunnelForMaskedLM"),RPr.forEach(t),zho=r(X7e," (Funnel Transformer model)"),X7e.forEach(t),Vho=i(j),_u=n(j,"LI",{});var z7e=s(_u);aZ=n(z7e,"STRONG",{});var SPr=s(aZ);Who=r(SPr,"ibert"),SPr.forEach(t),Qho=r(z7e," \u2014 "),PS=n(z7e,"A",{href:!0});var PPr=s(PS);Hho=r(PPr,"IBertForMaskedLM"),PPr.forEach(t),Uho=r(z7e," (I-BERT model)"),z7e.forEach(t),Jho=i(j),uu=n(j,"LI",{});var V7e=s(uu);nZ=n(V7e,"STRONG",{});var $Pr=s(nZ);Yho=r($Pr,"layoutlm"),$Pr.forEach(t),Kho=r(V7e," \u2014 "),$S=n(V7e,"A",{href:!0});var IPr=s($S);Zho=r(IPr,"LayoutLMForMaskedLM"),IPr.forEach(t),epo=r(V7e," (LayoutLM model)"),V7e.forEach(t),opo=i(j),bu=n(j,"LI",{});var W7e=s(bu);sZ=n(W7e,"STRONG",{});var jPr=s(sZ);rpo=r(jPr,"longformer"),jPr.forEach(t),tpo=r(W7e," \u2014 "),IS=n(W7e,"A",{href:!0});var NPr=s(IS);apo=r(NPr,"LongformerForMaskedLM"),NPr.forEach(t),npo=r(W7e," (Longformer model)"),W7e.forEach(t),spo=i(j),vu=n(j,"LI",{});var Q7e=s(vu);lZ=n(Q7e,"STRONG",{});var DPr=s(lZ);lpo=r(DPr,"mbart"),DPr.forEach(t),ipo=r(Q7e," \u2014 "),jS=n(Q7e,"A",{href:!0});var qPr=s(jS);dpo=r(qPr,"MBartForConditionalGeneration"),qPr.forEach(t),cpo=r(Q7e," (mBART model)"),Q7e.forEach(t),fpo=i(j),Tu=n(j,"LI",{});var H7e=s(Tu);iZ=n(H7e,"STRONG",{});var GPr=s(iZ);mpo=r(GPr,"megatron-bert"),GPr.forEach(t),gpo=r(H7e," \u2014 "),NS=n(H7e,"A",{href:!0});var OPr=s(NS);hpo=r(OPr,"MegatronBertForMaskedLM"),OPr.forEach(t),ppo=r(H7e," (MegatronBert model)"),H7e.forEach(t),_po=i(j),Fu=n(j,"LI",{});var U7e=s(Fu);dZ=n(U7e,"STRONG",{});var XPr=s(dZ);upo=r(XPr,"mobilebert"),XPr.forEach(t),bpo=r(U7e," \u2014 "),DS=n(U7e,"A",{href:!0});var zPr=s(DS);vpo=r(zPr,"MobileBertForMaskedLM"),zPr.forEach(t),Tpo=r(U7e," (MobileBERT model)"),U7e.forEach(t),Fpo=i(j),Cu=n(j,"LI",{});var J7e=s(Cu);cZ=n(J7e,"STRONG",{});var VPr=s(cZ);Cpo=r(VPr,"mpnet"),VPr.forEach(t),Mpo=r(J7e," \u2014 "),qS=n(J7e,"A",{href:!0});var WPr=s(qS);Epo=r(WPr,"MPNetForMaskedLM"),WPr.forEach(t),ypo=r(J7e," (MPNet model)"),J7e.forEach(t),wpo=i(j),Mu=n(j,"LI",{});var Y7e=s(Mu);fZ=n(Y7e,"STRONG",{});var QPr=s(fZ);Apo=r(QPr,"nystromformer"),QPr.forEach(t),Lpo=r(Y7e," \u2014 "),GS=n(Y7e,"A",{href:!0});var HPr=s(GS);Bpo=r(HPr,"NystromformerForMaskedLM"),HPr.forEach(t),xpo=r(Y7e," (Nystromformer model)"),Y7e.forEach(t),kpo=i(j),Eu=n(j,"LI",{});var K7e=s(Eu);mZ=n(K7e,"STRONG",{});var UPr=s(mZ);Rpo=r(UPr,"perceiver"),UPr.forEach(t),Spo=r(K7e," \u2014 "),OS=n(K7e,"A",{href:!0});var JPr=s(OS);Ppo=r(JPr,"PerceiverForMaskedLM"),JPr.forEach(t),$po=r(K7e," (Perceiver model)"),K7e.forEach(t),Ipo=i(j),yu=n(j,"LI",{});var Z7e=s(yu);gZ=n(Z7e,"STRONG",{});var YPr=s(gZ);jpo=r(YPr,"qdqbert"),YPr.forEach(t),Npo=r(Z7e," \u2014 "),XS=n(Z7e,"A",{href:!0});var KPr=s(XS);Dpo=r(KPr,"QDQBertForMaskedLM"),KPr.forEach(t),qpo=r(Z7e," (QDQBert model)"),Z7e.forEach(t),Gpo=i(j),wu=n(j,"LI",{});var e8e=s(wu);hZ=n(e8e,"STRONG",{});var ZPr=s(hZ);Opo=r(ZPr,"reformer"),ZPr.forEach(t),Xpo=r(e8e," \u2014 "),zS=n(e8e,"A",{href:!0});var e$r=s(zS);zpo=r(e$r,"ReformerForMaskedLM"),e$r.forEach(t),Vpo=r(e8e," (Reformer model)"),e8e.forEach(t),Wpo=i(j),Au=n(j,"LI",{});var o8e=s(Au);pZ=n(o8e,"STRONG",{});var o$r=s(pZ);Qpo=r(o$r,"rembert"),o$r.forEach(t),Hpo=r(o8e," \u2014 "),VS=n(o8e,"A",{href:!0});var r$r=s(VS);Upo=r(r$r,"RemBertForMaskedLM"),r$r.forEach(t),Jpo=r(o8e," (RemBERT model)"),o8e.forEach(t),Ypo=i(j),Lu=n(j,"LI",{});var r8e=s(Lu);_Z=n(r8e,"STRONG",{});var t$r=s(_Z);Kpo=r(t$r,"roberta"),t$r.forEach(t),Zpo=r(r8e," \u2014 "),WS=n(r8e,"A",{href:!0});var a$r=s(WS);e_o=r(a$r,"RobertaForMaskedLM"),a$r.forEach(t),o_o=r(r8e," (RoBERTa model)"),r8e.forEach(t),r_o=i(j),Bu=n(j,"LI",{});var t8e=s(Bu);uZ=n(t8e,"STRONG",{});var n$r=s(uZ);t_o=r(n$r,"roformer"),n$r.forEach(t),a_o=r(t8e," \u2014 "),QS=n(t8e,"A",{href:!0});var s$r=s(QS);n_o=r(s$r,"RoFormerForMaskedLM"),s$r.forEach(t),s_o=r(t8e," (RoFormer model)"),t8e.forEach(t),l_o=i(j),xu=n(j,"LI",{});var a8e=s(xu);bZ=n(a8e,"STRONG",{});var l$r=s(bZ);i_o=r(l$r,"squeezebert"),l$r.forEach(t),d_o=r(a8e," \u2014 "),HS=n(a8e,"A",{href:!0});var i$r=s(HS);c_o=r(i$r,"SqueezeBertForMaskedLM"),i$r.forEach(t),f_o=r(a8e," (SqueezeBERT model)"),a8e.forEach(t),m_o=i(j),ku=n(j,"LI",{});var n8e=s(ku);vZ=n(n8e,"STRONG",{});var d$r=s(vZ);g_o=r(d$r,"tapas"),d$r.forEach(t),h_o=r(n8e," \u2014 "),US=n(n8e,"A",{href:!0});var c$r=s(US);p_o=r(c$r,"TapasForMaskedLM"),c$r.forEach(t),__o=r(n8e," (TAPAS model)"),n8e.forEach(t),u_o=i(j),Ru=n(j,"LI",{});var s8e=s(Ru);TZ=n(s8e,"STRONG",{});var f$r=s(TZ);b_o=r(f$r,"wav2vec2"),f$r.forEach(t),v_o=r(s8e," \u2014 "),FZ=n(s8e,"CODE",{});var m$r=s(FZ);T_o=r(m$r,"Wav2Vec2ForMaskedLM"),m$r.forEach(t),F_o=r(s8e,"(Wav2Vec2 model)"),s8e.forEach(t),C_o=i(j),Su=n(j,"LI",{});var l8e=s(Su);CZ=n(l8e,"STRONG",{});var g$r=s(CZ);M_o=r(g$r,"xlm"),g$r.forEach(t),E_o=r(l8e," \u2014 "),JS=n(l8e,"A",{href:!0});var h$r=s(JS);y_o=r(h$r,"XLMWithLMHeadModel"),h$r.forEach(t),w_o=r(l8e," (XLM model)"),l8e.forEach(t),A_o=i(j),Pu=n(j,"LI",{});var i8e=s(Pu);MZ=n(i8e,"STRONG",{});var p$r=s(MZ);L_o=r(p$r,"xlm-roberta"),p$r.forEach(t),B_o=r(i8e," \u2014 "),YS=n(i8e,"A",{href:!0});var _$r=s(YS);x_o=r(_$r,"XLMRobertaForMaskedLM"),_$r.forEach(t),k_o=r(i8e," (XLM-RoBERTa model)"),i8e.forEach(t),R_o=i(j),$u=n(j,"LI",{});var d8e=s($u);EZ=n(d8e,"STRONG",{});var u$r=s(EZ);S_o=r(u$r,"xlm-roberta-xl"),u$r.forEach(t),P_o=r(d8e," \u2014 "),KS=n(d8e,"A",{href:!0});var b$r=s(KS);$_o=r(b$r,"XLMRobertaXLForMaskedLM"),b$r.forEach(t),I_o=r(d8e," (XLM-RoBERTa-XL model)"),d8e.forEach(t),j_o=i(j),Iu=n(j,"LI",{});var c8e=s(Iu);yZ=n(c8e,"STRONG",{});var v$r=s(yZ);N_o=r(v$r,"yoso"),v$r.forEach(t),D_o=r(c8e," \u2014 "),ZS=n(c8e,"A",{href:!0});var T$r=s(ZS);q_o=r(T$r,"YosoForMaskedLM"),T$r.forEach(t),G_o=r(c8e," (YOSO model)"),c8e.forEach(t),j.forEach(t),O_o=i(St),ju=n(St,"P",{});var f8e=s(ju);X_o=r(f8e,"The model is set in evaluation mode by default using "),wZ=n(f8e,"CODE",{});var F$r=s(wZ);z_o=r(F$r,"model.eval()"),F$r.forEach(t),V_o=r(f8e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),AZ=n(f8e,"CODE",{});var C$r=s(AZ);W_o=r(C$r,"model.train()"),C$r.forEach(t),f8e.forEach(t),Q_o=i(St),LZ=n(St,"P",{});var M$r=s(LZ);H_o=r(M$r,"Examples:"),M$r.forEach(t),U_o=i(St),m($4.$$.fragment,St),St.forEach(t),Ds.forEach(t),DAe=i(d),Vi=n(d,"H2",{class:!0});var zLe=s(Vi);Nu=n(zLe,"A",{id:!0,class:!0,href:!0});var E$r=s(Nu);BZ=n(E$r,"SPAN",{});var y$r=s(BZ);m(I4.$$.fragment,y$r),y$r.forEach(t),E$r.forEach(t),J_o=i(zLe),xZ=n(zLe,"SPAN",{});var w$r=s(xZ);Y_o=r(w$r,"AutoModelForSeq2SeqLM"),w$r.forEach(t),zLe.forEach(t),qAe=i(d),Ho=n(d,"DIV",{class:!0});var Gs=s(Ho);m(j4.$$.fragment,Gs),K_o=i(Gs),Wi=n(Gs,"P",{});var lX=s(Wi);Z_o=r(lX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),kZ=n(lX,"CODE",{});var A$r=s(kZ);euo=r(A$r,"from_pretrained()"),A$r.forEach(t),ouo=r(lX,"class method or the "),RZ=n(lX,"CODE",{});var L$r=s(RZ);ruo=r(L$r,"from_config()"),L$r.forEach(t),tuo=r(lX,`class
method.`),lX.forEach(t),auo=i(Gs),N4=n(Gs,"P",{});var VLe=s(N4);nuo=r(VLe,"This class cannot be instantiated directly using "),SZ=n(VLe,"CODE",{});var B$r=s(SZ);suo=r(B$r,"__init__()"),B$r.forEach(t),luo=r(VLe," (throws an error)."),VLe.forEach(t),iuo=i(Gs),qr=n(Gs,"DIV",{class:!0});var Os=s(qr);m(D4.$$.fragment,Os),duo=i(Os),PZ=n(Os,"P",{});var x$r=s(PZ);cuo=r(x$r,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),x$r.forEach(t),fuo=i(Os),Qi=n(Os,"P",{});var iX=s(Qi);muo=r(iX,`Note:
Loading a model from its configuration file does `),$Z=n(iX,"STRONG",{});var k$r=s($Z);guo=r(k$r,"not"),k$r.forEach(t),huo=r(iX,` load the model weights. It only affects the
model\u2019s configuration. Use `),IZ=n(iX,"CODE",{});var R$r=s(IZ);puo=r(R$r,"from_pretrained()"),R$r.forEach(t),_uo=r(iX,"to load the model weights."),iX.forEach(t),uuo=i(Os),jZ=n(Os,"P",{});var S$r=s(jZ);buo=r(S$r,"Examples:"),S$r.forEach(t),vuo=i(Os),m(q4.$$.fragment,Os),Os.forEach(t),Tuo=i(Gs),Pe=n(Gs,"DIV",{class:!0});var Pt=s(Pe);m(G4.$$.fragment,Pt),Fuo=i(Pt),NZ=n(Pt,"P",{});var P$r=s(NZ);Cuo=r(P$r,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),P$r.forEach(t),Muo=i(Pt),Da=n(Pt,"P",{});var DF=s(Da);Euo=r(DF,"The model class to instantiate is selected based on the "),DZ=n(DF,"CODE",{});var $$r=s(DZ);yuo=r($$r,"model_type"),$$r.forEach(t),wuo=r(DF,` property of the config object (either
passed as an argument or loaded from `),qZ=n(DF,"CODE",{});var I$r=s(qZ);Auo=r(I$r,"pretrained_model_name_or_path"),I$r.forEach(t),Luo=r(DF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),GZ=n(DF,"CODE",{});var j$r=s(GZ);Buo=r(j$r,"pretrained_model_name_or_path"),j$r.forEach(t),xuo=r(DF,":"),DF.forEach(t),kuo=i(Pt),ne=n(Pt,"UL",{});var ie=s(ne);Du=n(ie,"LI",{});var m8e=s(Du);OZ=n(m8e,"STRONG",{});var N$r=s(OZ);Ruo=r(N$r,"bart"),N$r.forEach(t),Suo=r(m8e," \u2014 "),eP=n(m8e,"A",{href:!0});var D$r=s(eP);Puo=r(D$r,"BartForConditionalGeneration"),D$r.forEach(t),$uo=r(m8e," (BART model)"),m8e.forEach(t),Iuo=i(ie),qu=n(ie,"LI",{});var g8e=s(qu);XZ=n(g8e,"STRONG",{});var q$r=s(XZ);juo=r(q$r,"bigbird_pegasus"),q$r.forEach(t),Nuo=r(g8e," \u2014 "),oP=n(g8e,"A",{href:!0});var G$r=s(oP);Duo=r(G$r,"BigBirdPegasusForConditionalGeneration"),G$r.forEach(t),quo=r(g8e," (BigBirdPegasus model)"),g8e.forEach(t),Guo=i(ie),Gu=n(ie,"LI",{});var h8e=s(Gu);zZ=n(h8e,"STRONG",{});var O$r=s(zZ);Ouo=r(O$r,"blenderbot"),O$r.forEach(t),Xuo=r(h8e," \u2014 "),rP=n(h8e,"A",{href:!0});var X$r=s(rP);zuo=r(X$r,"BlenderbotForConditionalGeneration"),X$r.forEach(t),Vuo=r(h8e," (Blenderbot model)"),h8e.forEach(t),Wuo=i(ie),Ou=n(ie,"LI",{});var p8e=s(Ou);VZ=n(p8e,"STRONG",{});var z$r=s(VZ);Quo=r(z$r,"blenderbot-small"),z$r.forEach(t),Huo=r(p8e," \u2014 "),tP=n(p8e,"A",{href:!0});var V$r=s(tP);Uuo=r(V$r,"BlenderbotSmallForConditionalGeneration"),V$r.forEach(t),Juo=r(p8e," (BlenderbotSmall model)"),p8e.forEach(t),Yuo=i(ie),Xu=n(ie,"LI",{});var _8e=s(Xu);WZ=n(_8e,"STRONG",{});var W$r=s(WZ);Kuo=r(W$r,"encoder-decoder"),W$r.forEach(t),Zuo=r(_8e," \u2014 "),aP=n(_8e,"A",{href:!0});var Q$r=s(aP);e1o=r(Q$r,"EncoderDecoderModel"),Q$r.forEach(t),o1o=r(_8e," (Encoder decoder model)"),_8e.forEach(t),r1o=i(ie),zu=n(ie,"LI",{});var u8e=s(zu);QZ=n(u8e,"STRONG",{});var H$r=s(QZ);t1o=r(H$r,"fsmt"),H$r.forEach(t),a1o=r(u8e," \u2014 "),nP=n(u8e,"A",{href:!0});var U$r=s(nP);n1o=r(U$r,"FSMTForConditionalGeneration"),U$r.forEach(t),s1o=r(u8e," (FairSeq Machine-Translation model)"),u8e.forEach(t),l1o=i(ie),Vu=n(ie,"LI",{});var b8e=s(Vu);HZ=n(b8e,"STRONG",{});var J$r=s(HZ);i1o=r(J$r,"led"),J$r.forEach(t),d1o=r(b8e," \u2014 "),sP=n(b8e,"A",{href:!0});var Y$r=s(sP);c1o=r(Y$r,"LEDForConditionalGeneration"),Y$r.forEach(t),f1o=r(b8e," (LED model)"),b8e.forEach(t),m1o=i(ie),Wu=n(ie,"LI",{});var v8e=s(Wu);UZ=n(v8e,"STRONG",{});var K$r=s(UZ);g1o=r(K$r,"m2m_100"),K$r.forEach(t),h1o=r(v8e," \u2014 "),lP=n(v8e,"A",{href:!0});var Z$r=s(lP);p1o=r(Z$r,"M2M100ForConditionalGeneration"),Z$r.forEach(t),_1o=r(v8e," (M2M100 model)"),v8e.forEach(t),u1o=i(ie),Qu=n(ie,"LI",{});var T8e=s(Qu);JZ=n(T8e,"STRONG",{});var eIr=s(JZ);b1o=r(eIr,"marian"),eIr.forEach(t),v1o=r(T8e," \u2014 "),iP=n(T8e,"A",{href:!0});var oIr=s(iP);T1o=r(oIr,"MarianMTModel"),oIr.forEach(t),F1o=r(T8e," (Marian model)"),T8e.forEach(t),C1o=i(ie),Hu=n(ie,"LI",{});var F8e=s(Hu);YZ=n(F8e,"STRONG",{});var rIr=s(YZ);M1o=r(rIr,"mbart"),rIr.forEach(t),E1o=r(F8e," \u2014 "),dP=n(F8e,"A",{href:!0});var tIr=s(dP);y1o=r(tIr,"MBartForConditionalGeneration"),tIr.forEach(t),w1o=r(F8e," (mBART model)"),F8e.forEach(t),A1o=i(ie),Uu=n(ie,"LI",{});var C8e=s(Uu);KZ=n(C8e,"STRONG",{});var aIr=s(KZ);L1o=r(aIr,"mt5"),aIr.forEach(t),B1o=r(C8e," \u2014 "),cP=n(C8e,"A",{href:!0});var nIr=s(cP);x1o=r(nIr,"MT5ForConditionalGeneration"),nIr.forEach(t),k1o=r(C8e," (mT5 model)"),C8e.forEach(t),R1o=i(ie),Ju=n(ie,"LI",{});var M8e=s(Ju);ZZ=n(M8e,"STRONG",{});var sIr=s(ZZ);S1o=r(sIr,"pegasus"),sIr.forEach(t),P1o=r(M8e," \u2014 "),fP=n(M8e,"A",{href:!0});var lIr=s(fP);$1o=r(lIr,"PegasusForConditionalGeneration"),lIr.forEach(t),I1o=r(M8e," (Pegasus model)"),M8e.forEach(t),j1o=i(ie),Yu=n(ie,"LI",{});var E8e=s(Yu);eee=n(E8e,"STRONG",{});var iIr=s(eee);N1o=r(iIr,"prophetnet"),iIr.forEach(t),D1o=r(E8e," \u2014 "),mP=n(E8e,"A",{href:!0});var dIr=s(mP);q1o=r(dIr,"ProphetNetForConditionalGeneration"),dIr.forEach(t),G1o=r(E8e," (ProphetNet model)"),E8e.forEach(t),O1o=i(ie),Ku=n(ie,"LI",{});var y8e=s(Ku);oee=n(y8e,"STRONG",{});var cIr=s(oee);X1o=r(cIr,"t5"),cIr.forEach(t),z1o=r(y8e," \u2014 "),gP=n(y8e,"A",{href:!0});var fIr=s(gP);V1o=r(fIr,"T5ForConditionalGeneration"),fIr.forEach(t),W1o=r(y8e," (T5 model)"),y8e.forEach(t),Q1o=i(ie),Zu=n(ie,"LI",{});var w8e=s(Zu);ree=n(w8e,"STRONG",{});var mIr=s(ree);H1o=r(mIr,"xlm-prophetnet"),mIr.forEach(t),U1o=r(w8e," \u2014 "),hP=n(w8e,"A",{href:!0});var gIr=s(hP);J1o=r(gIr,"XLMProphetNetForConditionalGeneration"),gIr.forEach(t),Y1o=r(w8e," (XLMProphetNet model)"),w8e.forEach(t),ie.forEach(t),K1o=i(Pt),e1=n(Pt,"P",{});var A8e=s(e1);Z1o=r(A8e,"The model is set in evaluation mode by default using "),tee=n(A8e,"CODE",{});var hIr=s(tee);ebo=r(hIr,"model.eval()"),hIr.forEach(t),obo=r(A8e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),aee=n(A8e,"CODE",{});var pIr=s(aee);rbo=r(pIr,"model.train()"),pIr.forEach(t),A8e.forEach(t),tbo=i(Pt),nee=n(Pt,"P",{});var _Ir=s(nee);abo=r(_Ir,"Examples:"),_Ir.forEach(t),nbo=i(Pt),m(O4.$$.fragment,Pt),Pt.forEach(t),Gs.forEach(t),GAe=i(d),Hi=n(d,"H2",{class:!0});var WLe=s(Hi);o1=n(WLe,"A",{id:!0,class:!0,href:!0});var uIr=s(o1);see=n(uIr,"SPAN",{});var bIr=s(see);m(X4.$$.fragment,bIr),bIr.forEach(t),uIr.forEach(t),sbo=i(WLe),lee=n(WLe,"SPAN",{});var vIr=s(lee);lbo=r(vIr,"AutoModelForSequenceClassification"),vIr.forEach(t),WLe.forEach(t),OAe=i(d),Uo=n(d,"DIV",{class:!0});var Xs=s(Uo);m(z4.$$.fragment,Xs),ibo=i(Xs),Ui=n(Xs,"P",{});var dX=s(Ui);dbo=r(dX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),iee=n(dX,"CODE",{});var TIr=s(iee);cbo=r(TIr,"from_pretrained()"),TIr.forEach(t),fbo=r(dX,"class method or the "),dee=n(dX,"CODE",{});var FIr=s(dee);mbo=r(FIr,"from_config()"),FIr.forEach(t),gbo=r(dX,`class
method.`),dX.forEach(t),hbo=i(Xs),V4=n(Xs,"P",{});var QLe=s(V4);pbo=r(QLe,"This class cannot be instantiated directly using "),cee=n(QLe,"CODE",{});var CIr=s(cee);_bo=r(CIr,"__init__()"),CIr.forEach(t),ubo=r(QLe," (throws an error)."),QLe.forEach(t),bbo=i(Xs),Gr=n(Xs,"DIV",{class:!0});var zs=s(Gr);m(W4.$$.fragment,zs),vbo=i(zs),fee=n(zs,"P",{});var MIr=s(fee);Tbo=r(MIr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),MIr.forEach(t),Fbo=i(zs),Ji=n(zs,"P",{});var cX=s(Ji);Cbo=r(cX,`Note:
Loading a model from its configuration file does `),mee=n(cX,"STRONG",{});var EIr=s(mee);Mbo=r(EIr,"not"),EIr.forEach(t),Ebo=r(cX,` load the model weights. It only affects the
model\u2019s configuration. Use `),gee=n(cX,"CODE",{});var yIr=s(gee);ybo=r(yIr,"from_pretrained()"),yIr.forEach(t),wbo=r(cX,"to load the model weights."),cX.forEach(t),Abo=i(zs),hee=n(zs,"P",{});var wIr=s(hee);Lbo=r(wIr,"Examples:"),wIr.forEach(t),Bbo=i(zs),m(Q4.$$.fragment,zs),zs.forEach(t),xbo=i(Xs),$e=n(Xs,"DIV",{class:!0});var $t=s($e);m(H4.$$.fragment,$t),kbo=i($t),pee=n($t,"P",{});var AIr=s(pee);Rbo=r(AIr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),AIr.forEach(t),Sbo=i($t),qa=n($t,"P",{});var qF=s(qa);Pbo=r(qF,"The model class to instantiate is selected based on the "),_ee=n(qF,"CODE",{});var LIr=s(_ee);$bo=r(LIr,"model_type"),LIr.forEach(t),Ibo=r(qF,` property of the config object (either
passed as an argument or loaded from `),uee=n(qF,"CODE",{});var BIr=s(uee);jbo=r(BIr,"pretrained_model_name_or_path"),BIr.forEach(t),Nbo=r(qF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bee=n(qF,"CODE",{});var xIr=s(bee);Dbo=r(xIr,"pretrained_model_name_or_path"),xIr.forEach(t),qbo=r(qF,":"),qF.forEach(t),Gbo=i($t),A=n($t,"UL",{});var L=s(A);r1=n(L,"LI",{});var L8e=s(r1);vee=n(L8e,"STRONG",{});var kIr=s(vee);Obo=r(kIr,"albert"),kIr.forEach(t),Xbo=r(L8e," \u2014 "),pP=n(L8e,"A",{href:!0});var RIr=s(pP);zbo=r(RIr,"AlbertForSequenceClassification"),RIr.forEach(t),Vbo=r(L8e," (ALBERT model)"),L8e.forEach(t),Wbo=i(L),t1=n(L,"LI",{});var B8e=s(t1);Tee=n(B8e,"STRONG",{});var SIr=s(Tee);Qbo=r(SIr,"bart"),SIr.forEach(t),Hbo=r(B8e," \u2014 "),_P=n(B8e,"A",{href:!0});var PIr=s(_P);Ubo=r(PIr,"BartForSequenceClassification"),PIr.forEach(t),Jbo=r(B8e," (BART model)"),B8e.forEach(t),Ybo=i(L),a1=n(L,"LI",{});var x8e=s(a1);Fee=n(x8e,"STRONG",{});var $Ir=s(Fee);Kbo=r($Ir,"bert"),$Ir.forEach(t),Zbo=r(x8e," \u2014 "),uP=n(x8e,"A",{href:!0});var IIr=s(uP);e5o=r(IIr,"BertForSequenceClassification"),IIr.forEach(t),o5o=r(x8e," (BERT model)"),x8e.forEach(t),r5o=i(L),n1=n(L,"LI",{});var k8e=s(n1);Cee=n(k8e,"STRONG",{});var jIr=s(Cee);t5o=r(jIr,"big_bird"),jIr.forEach(t),a5o=r(k8e," \u2014 "),bP=n(k8e,"A",{href:!0});var NIr=s(bP);n5o=r(NIr,"BigBirdForSequenceClassification"),NIr.forEach(t),s5o=r(k8e," (BigBird model)"),k8e.forEach(t),l5o=i(L),s1=n(L,"LI",{});var R8e=s(s1);Mee=n(R8e,"STRONG",{});var DIr=s(Mee);i5o=r(DIr,"bigbird_pegasus"),DIr.forEach(t),d5o=r(R8e," \u2014 "),vP=n(R8e,"A",{href:!0});var qIr=s(vP);c5o=r(qIr,"BigBirdPegasusForSequenceClassification"),qIr.forEach(t),f5o=r(R8e," (BigBirdPegasus model)"),R8e.forEach(t),m5o=i(L),l1=n(L,"LI",{});var S8e=s(l1);Eee=n(S8e,"STRONG",{});var GIr=s(Eee);g5o=r(GIr,"camembert"),GIr.forEach(t),h5o=r(S8e," \u2014 "),TP=n(S8e,"A",{href:!0});var OIr=s(TP);p5o=r(OIr,"CamembertForSequenceClassification"),OIr.forEach(t),_5o=r(S8e," (CamemBERT model)"),S8e.forEach(t),u5o=i(L),i1=n(L,"LI",{});var P8e=s(i1);yee=n(P8e,"STRONG",{});var XIr=s(yee);b5o=r(XIr,"canine"),XIr.forEach(t),v5o=r(P8e," \u2014 "),FP=n(P8e,"A",{href:!0});var zIr=s(FP);T5o=r(zIr,"CanineForSequenceClassification"),zIr.forEach(t),F5o=r(P8e," (Canine model)"),P8e.forEach(t),C5o=i(L),d1=n(L,"LI",{});var $8e=s(d1);wee=n($8e,"STRONG",{});var VIr=s(wee);M5o=r(VIr,"convbert"),VIr.forEach(t),E5o=r($8e," \u2014 "),CP=n($8e,"A",{href:!0});var WIr=s(CP);y5o=r(WIr,"ConvBertForSequenceClassification"),WIr.forEach(t),w5o=r($8e," (ConvBERT model)"),$8e.forEach(t),A5o=i(L),c1=n(L,"LI",{});var I8e=s(c1);Aee=n(I8e,"STRONG",{});var QIr=s(Aee);L5o=r(QIr,"ctrl"),QIr.forEach(t),B5o=r(I8e," \u2014 "),MP=n(I8e,"A",{href:!0});var HIr=s(MP);x5o=r(HIr,"CTRLForSequenceClassification"),HIr.forEach(t),k5o=r(I8e," (CTRL model)"),I8e.forEach(t),R5o=i(L),f1=n(L,"LI",{});var j8e=s(f1);Lee=n(j8e,"STRONG",{});var UIr=s(Lee);S5o=r(UIr,"deberta"),UIr.forEach(t),P5o=r(j8e," \u2014 "),EP=n(j8e,"A",{href:!0});var JIr=s(EP);$5o=r(JIr,"DebertaForSequenceClassification"),JIr.forEach(t),I5o=r(j8e," (DeBERTa model)"),j8e.forEach(t),j5o=i(L),m1=n(L,"LI",{});var N8e=s(m1);Bee=n(N8e,"STRONG",{});var YIr=s(Bee);N5o=r(YIr,"deberta-v2"),YIr.forEach(t),D5o=r(N8e," \u2014 "),yP=n(N8e,"A",{href:!0});var KIr=s(yP);q5o=r(KIr,"DebertaV2ForSequenceClassification"),KIr.forEach(t),G5o=r(N8e," (DeBERTa-v2 model)"),N8e.forEach(t),O5o=i(L),g1=n(L,"LI",{});var D8e=s(g1);xee=n(D8e,"STRONG",{});var ZIr=s(xee);X5o=r(ZIr,"distilbert"),ZIr.forEach(t),z5o=r(D8e," \u2014 "),wP=n(D8e,"A",{href:!0});var ejr=s(wP);V5o=r(ejr,"DistilBertForSequenceClassification"),ejr.forEach(t),W5o=r(D8e," (DistilBERT model)"),D8e.forEach(t),Q5o=i(L),h1=n(L,"LI",{});var q8e=s(h1);kee=n(q8e,"STRONG",{});var ojr=s(kee);H5o=r(ojr,"electra"),ojr.forEach(t),U5o=r(q8e," \u2014 "),AP=n(q8e,"A",{href:!0});var rjr=s(AP);J5o=r(rjr,"ElectraForSequenceClassification"),rjr.forEach(t),Y5o=r(q8e," (ELECTRA model)"),q8e.forEach(t),K5o=i(L),p1=n(L,"LI",{});var G8e=s(p1);Ree=n(G8e,"STRONG",{});var tjr=s(Ree);Z5o=r(tjr,"flaubert"),tjr.forEach(t),e2o=r(G8e," \u2014 "),LP=n(G8e,"A",{href:!0});var ajr=s(LP);o2o=r(ajr,"FlaubertForSequenceClassification"),ajr.forEach(t),r2o=r(G8e," (FlauBERT model)"),G8e.forEach(t),t2o=i(L),_1=n(L,"LI",{});var O8e=s(_1);See=n(O8e,"STRONG",{});var njr=s(See);a2o=r(njr,"fnet"),njr.forEach(t),n2o=r(O8e," \u2014 "),BP=n(O8e,"A",{href:!0});var sjr=s(BP);s2o=r(sjr,"FNetForSequenceClassification"),sjr.forEach(t),l2o=r(O8e," (FNet model)"),O8e.forEach(t),i2o=i(L),u1=n(L,"LI",{});var X8e=s(u1);Pee=n(X8e,"STRONG",{});var ljr=s(Pee);d2o=r(ljr,"funnel"),ljr.forEach(t),c2o=r(X8e," \u2014 "),xP=n(X8e,"A",{href:!0});var ijr=s(xP);f2o=r(ijr,"FunnelForSequenceClassification"),ijr.forEach(t),m2o=r(X8e," (Funnel Transformer model)"),X8e.forEach(t),g2o=i(L),b1=n(L,"LI",{});var z8e=s(b1);$ee=n(z8e,"STRONG",{});var djr=s($ee);h2o=r(djr,"gpt2"),djr.forEach(t),p2o=r(z8e," \u2014 "),kP=n(z8e,"A",{href:!0});var cjr=s(kP);_2o=r(cjr,"GPT2ForSequenceClassification"),cjr.forEach(t),u2o=r(z8e," (OpenAI GPT-2 model)"),z8e.forEach(t),b2o=i(L),v1=n(L,"LI",{});var V8e=s(v1);Iee=n(V8e,"STRONG",{});var fjr=s(Iee);v2o=r(fjr,"gpt_neo"),fjr.forEach(t),T2o=r(V8e," \u2014 "),RP=n(V8e,"A",{href:!0});var mjr=s(RP);F2o=r(mjr,"GPTNeoForSequenceClassification"),mjr.forEach(t),C2o=r(V8e," (GPT Neo model)"),V8e.forEach(t),M2o=i(L),T1=n(L,"LI",{});var W8e=s(T1);jee=n(W8e,"STRONG",{});var gjr=s(jee);E2o=r(gjr,"gptj"),gjr.forEach(t),y2o=r(W8e," \u2014 "),SP=n(W8e,"A",{href:!0});var hjr=s(SP);w2o=r(hjr,"GPTJForSequenceClassification"),hjr.forEach(t),A2o=r(W8e," (GPT-J model)"),W8e.forEach(t),L2o=i(L),F1=n(L,"LI",{});var Q8e=s(F1);Nee=n(Q8e,"STRONG",{});var pjr=s(Nee);B2o=r(pjr,"ibert"),pjr.forEach(t),x2o=r(Q8e," \u2014 "),PP=n(Q8e,"A",{href:!0});var _jr=s(PP);k2o=r(_jr,"IBertForSequenceClassification"),_jr.forEach(t),R2o=r(Q8e," (I-BERT model)"),Q8e.forEach(t),S2o=i(L),C1=n(L,"LI",{});var H8e=s(C1);Dee=n(H8e,"STRONG",{});var ujr=s(Dee);P2o=r(ujr,"layoutlm"),ujr.forEach(t),$2o=r(H8e," \u2014 "),$P=n(H8e,"A",{href:!0});var bjr=s($P);I2o=r(bjr,"LayoutLMForSequenceClassification"),bjr.forEach(t),j2o=r(H8e," (LayoutLM model)"),H8e.forEach(t),N2o=i(L),M1=n(L,"LI",{});var U8e=s(M1);qee=n(U8e,"STRONG",{});var vjr=s(qee);D2o=r(vjr,"layoutlmv2"),vjr.forEach(t),q2o=r(U8e," \u2014 "),IP=n(U8e,"A",{href:!0});var Tjr=s(IP);G2o=r(Tjr,"LayoutLMv2ForSequenceClassification"),Tjr.forEach(t),O2o=r(U8e," (LayoutLMv2 model)"),U8e.forEach(t),X2o=i(L),E1=n(L,"LI",{});var J8e=s(E1);Gee=n(J8e,"STRONG",{});var Fjr=s(Gee);z2o=r(Fjr,"led"),Fjr.forEach(t),V2o=r(J8e," \u2014 "),jP=n(J8e,"A",{href:!0});var Cjr=s(jP);W2o=r(Cjr,"LEDForSequenceClassification"),Cjr.forEach(t),Q2o=r(J8e," (LED model)"),J8e.forEach(t),H2o=i(L),y1=n(L,"LI",{});var Y8e=s(y1);Oee=n(Y8e,"STRONG",{});var Mjr=s(Oee);U2o=r(Mjr,"longformer"),Mjr.forEach(t),J2o=r(Y8e," \u2014 "),NP=n(Y8e,"A",{href:!0});var Ejr=s(NP);Y2o=r(Ejr,"LongformerForSequenceClassification"),Ejr.forEach(t),K2o=r(Y8e," (Longformer model)"),Y8e.forEach(t),Z2o=i(L),w1=n(L,"LI",{});var K8e=s(w1);Xee=n(K8e,"STRONG",{});var yjr=s(Xee);evo=r(yjr,"mbart"),yjr.forEach(t),ovo=r(K8e," \u2014 "),DP=n(K8e,"A",{href:!0});var wjr=s(DP);rvo=r(wjr,"MBartForSequenceClassification"),wjr.forEach(t),tvo=r(K8e," (mBART model)"),K8e.forEach(t),avo=i(L),A1=n(L,"LI",{});var Z8e=s(A1);zee=n(Z8e,"STRONG",{});var Ajr=s(zee);nvo=r(Ajr,"megatron-bert"),Ajr.forEach(t),svo=r(Z8e," \u2014 "),qP=n(Z8e,"A",{href:!0});var Ljr=s(qP);lvo=r(Ljr,"MegatronBertForSequenceClassification"),Ljr.forEach(t),ivo=r(Z8e," (MegatronBert model)"),Z8e.forEach(t),dvo=i(L),L1=n(L,"LI",{});var eFe=s(L1);Vee=n(eFe,"STRONG",{});var Bjr=s(Vee);cvo=r(Bjr,"mobilebert"),Bjr.forEach(t),fvo=r(eFe," \u2014 "),GP=n(eFe,"A",{href:!0});var xjr=s(GP);mvo=r(xjr,"MobileBertForSequenceClassification"),xjr.forEach(t),gvo=r(eFe," (MobileBERT model)"),eFe.forEach(t),hvo=i(L),B1=n(L,"LI",{});var oFe=s(B1);Wee=n(oFe,"STRONG",{});var kjr=s(Wee);pvo=r(kjr,"mpnet"),kjr.forEach(t),_vo=r(oFe," \u2014 "),OP=n(oFe,"A",{href:!0});var Rjr=s(OP);uvo=r(Rjr,"MPNetForSequenceClassification"),Rjr.forEach(t),bvo=r(oFe," (MPNet model)"),oFe.forEach(t),vvo=i(L),x1=n(L,"LI",{});var rFe=s(x1);Qee=n(rFe,"STRONG",{});var Sjr=s(Qee);Tvo=r(Sjr,"nystromformer"),Sjr.forEach(t),Fvo=r(rFe," \u2014 "),XP=n(rFe,"A",{href:!0});var Pjr=s(XP);Cvo=r(Pjr,"NystromformerForSequenceClassification"),Pjr.forEach(t),Mvo=r(rFe," (Nystromformer model)"),rFe.forEach(t),Evo=i(L),k1=n(L,"LI",{});var tFe=s(k1);Hee=n(tFe,"STRONG",{});var $jr=s(Hee);yvo=r($jr,"openai-gpt"),$jr.forEach(t),wvo=r(tFe," \u2014 "),zP=n(tFe,"A",{href:!0});var Ijr=s(zP);Avo=r(Ijr,"OpenAIGPTForSequenceClassification"),Ijr.forEach(t),Lvo=r(tFe," (OpenAI GPT model)"),tFe.forEach(t),Bvo=i(L),R1=n(L,"LI",{});var aFe=s(R1);Uee=n(aFe,"STRONG",{});var jjr=s(Uee);xvo=r(jjr,"perceiver"),jjr.forEach(t),kvo=r(aFe," \u2014 "),VP=n(aFe,"A",{href:!0});var Njr=s(VP);Rvo=r(Njr,"PerceiverForSequenceClassification"),Njr.forEach(t),Svo=r(aFe," (Perceiver model)"),aFe.forEach(t),Pvo=i(L),S1=n(L,"LI",{});var nFe=s(S1);Jee=n(nFe,"STRONG",{});var Djr=s(Jee);$vo=r(Djr,"qdqbert"),Djr.forEach(t),Ivo=r(nFe," \u2014 "),WP=n(nFe,"A",{href:!0});var qjr=s(WP);jvo=r(qjr,"QDQBertForSequenceClassification"),qjr.forEach(t),Nvo=r(nFe," (QDQBert model)"),nFe.forEach(t),Dvo=i(L),P1=n(L,"LI",{});var sFe=s(P1);Yee=n(sFe,"STRONG",{});var Gjr=s(Yee);qvo=r(Gjr,"reformer"),Gjr.forEach(t),Gvo=r(sFe," \u2014 "),QP=n(sFe,"A",{href:!0});var Ojr=s(QP);Ovo=r(Ojr,"ReformerForSequenceClassification"),Ojr.forEach(t),Xvo=r(sFe," (Reformer model)"),sFe.forEach(t),zvo=i(L),$1=n(L,"LI",{});var lFe=s($1);Kee=n(lFe,"STRONG",{});var Xjr=s(Kee);Vvo=r(Xjr,"rembert"),Xjr.forEach(t),Wvo=r(lFe," \u2014 "),HP=n(lFe,"A",{href:!0});var zjr=s(HP);Qvo=r(zjr,"RemBertForSequenceClassification"),zjr.forEach(t),Hvo=r(lFe," (RemBERT model)"),lFe.forEach(t),Uvo=i(L),I1=n(L,"LI",{});var iFe=s(I1);Zee=n(iFe,"STRONG",{});var Vjr=s(Zee);Jvo=r(Vjr,"roberta"),Vjr.forEach(t),Yvo=r(iFe," \u2014 "),UP=n(iFe,"A",{href:!0});var Wjr=s(UP);Kvo=r(Wjr,"RobertaForSequenceClassification"),Wjr.forEach(t),Zvo=r(iFe," (RoBERTa model)"),iFe.forEach(t),e6o=i(L),j1=n(L,"LI",{});var dFe=s(j1);eoe=n(dFe,"STRONG",{});var Qjr=s(eoe);o6o=r(Qjr,"roformer"),Qjr.forEach(t),r6o=r(dFe," \u2014 "),JP=n(dFe,"A",{href:!0});var Hjr=s(JP);t6o=r(Hjr,"RoFormerForSequenceClassification"),Hjr.forEach(t),a6o=r(dFe," (RoFormer model)"),dFe.forEach(t),n6o=i(L),N1=n(L,"LI",{});var cFe=s(N1);ooe=n(cFe,"STRONG",{});var Ujr=s(ooe);s6o=r(Ujr,"squeezebert"),Ujr.forEach(t),l6o=r(cFe," \u2014 "),YP=n(cFe,"A",{href:!0});var Jjr=s(YP);i6o=r(Jjr,"SqueezeBertForSequenceClassification"),Jjr.forEach(t),d6o=r(cFe," (SqueezeBERT model)"),cFe.forEach(t),c6o=i(L),D1=n(L,"LI",{});var fFe=s(D1);roe=n(fFe,"STRONG",{});var Yjr=s(roe);f6o=r(Yjr,"tapas"),Yjr.forEach(t),m6o=r(fFe," \u2014 "),KP=n(fFe,"A",{href:!0});var Kjr=s(KP);g6o=r(Kjr,"TapasForSequenceClassification"),Kjr.forEach(t),h6o=r(fFe," (TAPAS model)"),fFe.forEach(t),p6o=i(L),q1=n(L,"LI",{});var mFe=s(q1);toe=n(mFe,"STRONG",{});var Zjr=s(toe);_6o=r(Zjr,"transfo-xl"),Zjr.forEach(t),u6o=r(mFe," \u2014 "),ZP=n(mFe,"A",{href:!0});var eNr=s(ZP);b6o=r(eNr,"TransfoXLForSequenceClassification"),eNr.forEach(t),v6o=r(mFe," (Transformer-XL model)"),mFe.forEach(t),T6o=i(L),G1=n(L,"LI",{});var gFe=s(G1);aoe=n(gFe,"STRONG",{});var oNr=s(aoe);F6o=r(oNr,"xlm"),oNr.forEach(t),C6o=r(gFe," \u2014 "),e$=n(gFe,"A",{href:!0});var rNr=s(e$);M6o=r(rNr,"XLMForSequenceClassification"),rNr.forEach(t),E6o=r(gFe," (XLM model)"),gFe.forEach(t),y6o=i(L),O1=n(L,"LI",{});var hFe=s(O1);noe=n(hFe,"STRONG",{});var tNr=s(noe);w6o=r(tNr,"xlm-roberta"),tNr.forEach(t),A6o=r(hFe," \u2014 "),o$=n(hFe,"A",{href:!0});var aNr=s(o$);L6o=r(aNr,"XLMRobertaForSequenceClassification"),aNr.forEach(t),B6o=r(hFe," (XLM-RoBERTa model)"),hFe.forEach(t),x6o=i(L),X1=n(L,"LI",{});var pFe=s(X1);soe=n(pFe,"STRONG",{});var nNr=s(soe);k6o=r(nNr,"xlm-roberta-xl"),nNr.forEach(t),R6o=r(pFe," \u2014 "),r$=n(pFe,"A",{href:!0});var sNr=s(r$);S6o=r(sNr,"XLMRobertaXLForSequenceClassification"),sNr.forEach(t),P6o=r(pFe," (XLM-RoBERTa-XL model)"),pFe.forEach(t),$6o=i(L),z1=n(L,"LI",{});var _Fe=s(z1);loe=n(_Fe,"STRONG",{});var lNr=s(loe);I6o=r(lNr,"xlnet"),lNr.forEach(t),j6o=r(_Fe," \u2014 "),t$=n(_Fe,"A",{href:!0});var iNr=s(t$);N6o=r(iNr,"XLNetForSequenceClassification"),iNr.forEach(t),D6o=r(_Fe," (XLNet model)"),_Fe.forEach(t),q6o=i(L),V1=n(L,"LI",{});var uFe=s(V1);ioe=n(uFe,"STRONG",{});var dNr=s(ioe);G6o=r(dNr,"yoso"),dNr.forEach(t),O6o=r(uFe," \u2014 "),a$=n(uFe,"A",{href:!0});var cNr=s(a$);X6o=r(cNr,"YosoForSequenceClassification"),cNr.forEach(t),z6o=r(uFe," (YOSO model)"),uFe.forEach(t),L.forEach(t),V6o=i($t),W1=n($t,"P",{});var bFe=s(W1);W6o=r(bFe,"The model is set in evaluation mode by default using "),doe=n(bFe,"CODE",{});var fNr=s(doe);Q6o=r(fNr,"model.eval()"),fNr.forEach(t),H6o=r(bFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),coe=n(bFe,"CODE",{});var mNr=s(coe);U6o=r(mNr,"model.train()"),mNr.forEach(t),bFe.forEach(t),J6o=i($t),foe=n($t,"P",{});var gNr=s(foe);Y6o=r(gNr,"Examples:"),gNr.forEach(t),K6o=i($t),m(U4.$$.fragment,$t),$t.forEach(t),Xs.forEach(t),XAe=i(d),Yi=n(d,"H2",{class:!0});var HLe=s(Yi);Q1=n(HLe,"A",{id:!0,class:!0,href:!0});var hNr=s(Q1);moe=n(hNr,"SPAN",{});var pNr=s(moe);m(J4.$$.fragment,pNr),pNr.forEach(t),hNr.forEach(t),Z6o=i(HLe),goe=n(HLe,"SPAN",{});var _Nr=s(goe);eTo=r(_Nr,"AutoModelForMultipleChoice"),_Nr.forEach(t),HLe.forEach(t),zAe=i(d),Jo=n(d,"DIV",{class:!0});var Vs=s(Jo);m(Y4.$$.fragment,Vs),oTo=i(Vs),Ki=n(Vs,"P",{});var fX=s(Ki);rTo=r(fX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),hoe=n(fX,"CODE",{});var uNr=s(hoe);tTo=r(uNr,"from_pretrained()"),uNr.forEach(t),aTo=r(fX,"class method or the "),poe=n(fX,"CODE",{});var bNr=s(poe);nTo=r(bNr,"from_config()"),bNr.forEach(t),sTo=r(fX,`class
method.`),fX.forEach(t),lTo=i(Vs),K4=n(Vs,"P",{});var ULe=s(K4);iTo=r(ULe,"This class cannot be instantiated directly using "),_oe=n(ULe,"CODE",{});var vNr=s(_oe);dTo=r(vNr,"__init__()"),vNr.forEach(t),cTo=r(ULe," (throws an error)."),ULe.forEach(t),fTo=i(Vs),Or=n(Vs,"DIV",{class:!0});var Ws=s(Or);m(Z4.$$.fragment,Ws),mTo=i(Ws),uoe=n(Ws,"P",{});var TNr=s(uoe);gTo=r(TNr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),TNr.forEach(t),hTo=i(Ws),Zi=n(Ws,"P",{});var mX=s(Zi);pTo=r(mX,`Note:
Loading a model from its configuration file does `),boe=n(mX,"STRONG",{});var FNr=s(boe);_To=r(FNr,"not"),FNr.forEach(t),uTo=r(mX,` load the model weights. It only affects the
model\u2019s configuration. Use `),voe=n(mX,"CODE",{});var CNr=s(voe);bTo=r(CNr,"from_pretrained()"),CNr.forEach(t),vTo=r(mX,"to load the model weights."),mX.forEach(t),TTo=i(Ws),Toe=n(Ws,"P",{});var MNr=s(Toe);FTo=r(MNr,"Examples:"),MNr.forEach(t),CTo=i(Ws),m(eM.$$.fragment,Ws),Ws.forEach(t),MTo=i(Vs),Ie=n(Vs,"DIV",{class:!0});var It=s(Ie);m(oM.$$.fragment,It),ETo=i(It),Foe=n(It,"P",{});var ENr=s(Foe);yTo=r(ENr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),ENr.forEach(t),wTo=i(It),Ga=n(It,"P",{});var GF=s(Ga);ATo=r(GF,"The model class to instantiate is selected based on the "),Coe=n(GF,"CODE",{});var yNr=s(Coe);LTo=r(yNr,"model_type"),yNr.forEach(t),BTo=r(GF,` property of the config object (either
passed as an argument or loaded from `),Moe=n(GF,"CODE",{});var wNr=s(Moe);xTo=r(wNr,"pretrained_model_name_or_path"),wNr.forEach(t),kTo=r(GF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Eoe=n(GF,"CODE",{});var ANr=s(Eoe);RTo=r(ANr,"pretrained_model_name_or_path"),ANr.forEach(t),STo=r(GF,":"),GF.forEach(t),PTo=i(It),G=n(It,"UL",{});var O=s(G);H1=n(O,"LI",{});var vFe=s(H1);yoe=n(vFe,"STRONG",{});var LNr=s(yoe);$To=r(LNr,"albert"),LNr.forEach(t),ITo=r(vFe," \u2014 "),n$=n(vFe,"A",{href:!0});var BNr=s(n$);jTo=r(BNr,"AlbertForMultipleChoice"),BNr.forEach(t),NTo=r(vFe," (ALBERT model)"),vFe.forEach(t),DTo=i(O),U1=n(O,"LI",{});var TFe=s(U1);woe=n(TFe,"STRONG",{});var xNr=s(woe);qTo=r(xNr,"bert"),xNr.forEach(t),GTo=r(TFe," \u2014 "),s$=n(TFe,"A",{href:!0});var kNr=s(s$);OTo=r(kNr,"BertForMultipleChoice"),kNr.forEach(t),XTo=r(TFe," (BERT model)"),TFe.forEach(t),zTo=i(O),J1=n(O,"LI",{});var FFe=s(J1);Aoe=n(FFe,"STRONG",{});var RNr=s(Aoe);VTo=r(RNr,"big_bird"),RNr.forEach(t),WTo=r(FFe," \u2014 "),l$=n(FFe,"A",{href:!0});var SNr=s(l$);QTo=r(SNr,"BigBirdForMultipleChoice"),SNr.forEach(t),HTo=r(FFe," (BigBird model)"),FFe.forEach(t),UTo=i(O),Y1=n(O,"LI",{});var CFe=s(Y1);Loe=n(CFe,"STRONG",{});var PNr=s(Loe);JTo=r(PNr,"camembert"),PNr.forEach(t),YTo=r(CFe," \u2014 "),i$=n(CFe,"A",{href:!0});var $Nr=s(i$);KTo=r($Nr,"CamembertForMultipleChoice"),$Nr.forEach(t),ZTo=r(CFe," (CamemBERT model)"),CFe.forEach(t),e7o=i(O),K1=n(O,"LI",{});var MFe=s(K1);Boe=n(MFe,"STRONG",{});var INr=s(Boe);o7o=r(INr,"canine"),INr.forEach(t),r7o=r(MFe," \u2014 "),d$=n(MFe,"A",{href:!0});var jNr=s(d$);t7o=r(jNr,"CanineForMultipleChoice"),jNr.forEach(t),a7o=r(MFe," (Canine model)"),MFe.forEach(t),n7o=i(O),Z1=n(O,"LI",{});var EFe=s(Z1);xoe=n(EFe,"STRONG",{});var NNr=s(xoe);s7o=r(NNr,"convbert"),NNr.forEach(t),l7o=r(EFe," \u2014 "),c$=n(EFe,"A",{href:!0});var DNr=s(c$);i7o=r(DNr,"ConvBertForMultipleChoice"),DNr.forEach(t),d7o=r(EFe," (ConvBERT model)"),EFe.forEach(t),c7o=i(O),eb=n(O,"LI",{});var yFe=s(eb);koe=n(yFe,"STRONG",{});var qNr=s(koe);f7o=r(qNr,"distilbert"),qNr.forEach(t),m7o=r(yFe," \u2014 "),f$=n(yFe,"A",{href:!0});var GNr=s(f$);g7o=r(GNr,"DistilBertForMultipleChoice"),GNr.forEach(t),h7o=r(yFe," (DistilBERT model)"),yFe.forEach(t),p7o=i(O),ob=n(O,"LI",{});var wFe=s(ob);Roe=n(wFe,"STRONG",{});var ONr=s(Roe);_7o=r(ONr,"electra"),ONr.forEach(t),u7o=r(wFe," \u2014 "),m$=n(wFe,"A",{href:!0});var XNr=s(m$);b7o=r(XNr,"ElectraForMultipleChoice"),XNr.forEach(t),v7o=r(wFe," (ELECTRA model)"),wFe.forEach(t),T7o=i(O),rb=n(O,"LI",{});var AFe=s(rb);Soe=n(AFe,"STRONG",{});var zNr=s(Soe);F7o=r(zNr,"flaubert"),zNr.forEach(t),C7o=r(AFe," \u2014 "),g$=n(AFe,"A",{href:!0});var VNr=s(g$);M7o=r(VNr,"FlaubertForMultipleChoice"),VNr.forEach(t),E7o=r(AFe," (FlauBERT model)"),AFe.forEach(t),y7o=i(O),tb=n(O,"LI",{});var LFe=s(tb);Poe=n(LFe,"STRONG",{});var WNr=s(Poe);w7o=r(WNr,"fnet"),WNr.forEach(t),A7o=r(LFe," \u2014 "),h$=n(LFe,"A",{href:!0});var QNr=s(h$);L7o=r(QNr,"FNetForMultipleChoice"),QNr.forEach(t),B7o=r(LFe," (FNet model)"),LFe.forEach(t),x7o=i(O),ab=n(O,"LI",{});var BFe=s(ab);$oe=n(BFe,"STRONG",{});var HNr=s($oe);k7o=r(HNr,"funnel"),HNr.forEach(t),R7o=r(BFe," \u2014 "),p$=n(BFe,"A",{href:!0});var UNr=s(p$);S7o=r(UNr,"FunnelForMultipleChoice"),UNr.forEach(t),P7o=r(BFe," (Funnel Transformer model)"),BFe.forEach(t),$7o=i(O),nb=n(O,"LI",{});var xFe=s(nb);Ioe=n(xFe,"STRONG",{});var JNr=s(Ioe);I7o=r(JNr,"ibert"),JNr.forEach(t),j7o=r(xFe," \u2014 "),_$=n(xFe,"A",{href:!0});var YNr=s(_$);N7o=r(YNr,"IBertForMultipleChoice"),YNr.forEach(t),D7o=r(xFe," (I-BERT model)"),xFe.forEach(t),q7o=i(O),sb=n(O,"LI",{});var kFe=s(sb);joe=n(kFe,"STRONG",{});var KNr=s(joe);G7o=r(KNr,"longformer"),KNr.forEach(t),O7o=r(kFe," \u2014 "),u$=n(kFe,"A",{href:!0});var ZNr=s(u$);X7o=r(ZNr,"LongformerForMultipleChoice"),ZNr.forEach(t),z7o=r(kFe," (Longformer model)"),kFe.forEach(t),V7o=i(O),lb=n(O,"LI",{});var RFe=s(lb);Noe=n(RFe,"STRONG",{});var eDr=s(Noe);W7o=r(eDr,"megatron-bert"),eDr.forEach(t),Q7o=r(RFe," \u2014 "),b$=n(RFe,"A",{href:!0});var oDr=s(b$);H7o=r(oDr,"MegatronBertForMultipleChoice"),oDr.forEach(t),U7o=r(RFe," (MegatronBert model)"),RFe.forEach(t),J7o=i(O),ib=n(O,"LI",{});var SFe=s(ib);Doe=n(SFe,"STRONG",{});var rDr=s(Doe);Y7o=r(rDr,"mobilebert"),rDr.forEach(t),K7o=r(SFe," \u2014 "),v$=n(SFe,"A",{href:!0});var tDr=s(v$);Z7o=r(tDr,"MobileBertForMultipleChoice"),tDr.forEach(t),e8o=r(SFe," (MobileBERT model)"),SFe.forEach(t),o8o=i(O),db=n(O,"LI",{});var PFe=s(db);qoe=n(PFe,"STRONG",{});var aDr=s(qoe);r8o=r(aDr,"mpnet"),aDr.forEach(t),t8o=r(PFe," \u2014 "),T$=n(PFe,"A",{href:!0});var nDr=s(T$);a8o=r(nDr,"MPNetForMultipleChoice"),nDr.forEach(t),n8o=r(PFe," (MPNet model)"),PFe.forEach(t),s8o=i(O),cb=n(O,"LI",{});var $Fe=s(cb);Goe=n($Fe,"STRONG",{});var sDr=s(Goe);l8o=r(sDr,"nystromformer"),sDr.forEach(t),i8o=r($Fe," \u2014 "),F$=n($Fe,"A",{href:!0});var lDr=s(F$);d8o=r(lDr,"NystromformerForMultipleChoice"),lDr.forEach(t),c8o=r($Fe," (Nystromformer model)"),$Fe.forEach(t),f8o=i(O),fb=n(O,"LI",{});var IFe=s(fb);Ooe=n(IFe,"STRONG",{});var iDr=s(Ooe);m8o=r(iDr,"qdqbert"),iDr.forEach(t),g8o=r(IFe," \u2014 "),C$=n(IFe,"A",{href:!0});var dDr=s(C$);h8o=r(dDr,"QDQBertForMultipleChoice"),dDr.forEach(t),p8o=r(IFe," (QDQBert model)"),IFe.forEach(t),_8o=i(O),mb=n(O,"LI",{});var jFe=s(mb);Xoe=n(jFe,"STRONG",{});var cDr=s(Xoe);u8o=r(cDr,"rembert"),cDr.forEach(t),b8o=r(jFe," \u2014 "),M$=n(jFe,"A",{href:!0});var fDr=s(M$);v8o=r(fDr,"RemBertForMultipleChoice"),fDr.forEach(t),T8o=r(jFe," (RemBERT model)"),jFe.forEach(t),F8o=i(O),gb=n(O,"LI",{});var NFe=s(gb);zoe=n(NFe,"STRONG",{});var mDr=s(zoe);C8o=r(mDr,"roberta"),mDr.forEach(t),M8o=r(NFe," \u2014 "),E$=n(NFe,"A",{href:!0});var gDr=s(E$);E8o=r(gDr,"RobertaForMultipleChoice"),gDr.forEach(t),y8o=r(NFe," (RoBERTa model)"),NFe.forEach(t),w8o=i(O),hb=n(O,"LI",{});var DFe=s(hb);Voe=n(DFe,"STRONG",{});var hDr=s(Voe);A8o=r(hDr,"roformer"),hDr.forEach(t),L8o=r(DFe," \u2014 "),y$=n(DFe,"A",{href:!0});var pDr=s(y$);B8o=r(pDr,"RoFormerForMultipleChoice"),pDr.forEach(t),x8o=r(DFe," (RoFormer model)"),DFe.forEach(t),k8o=i(O),pb=n(O,"LI",{});var qFe=s(pb);Woe=n(qFe,"STRONG",{});var _Dr=s(Woe);R8o=r(_Dr,"squeezebert"),_Dr.forEach(t),S8o=r(qFe," \u2014 "),w$=n(qFe,"A",{href:!0});var uDr=s(w$);P8o=r(uDr,"SqueezeBertForMultipleChoice"),uDr.forEach(t),$8o=r(qFe," (SqueezeBERT model)"),qFe.forEach(t),I8o=i(O),_b=n(O,"LI",{});var GFe=s(_b);Qoe=n(GFe,"STRONG",{});var bDr=s(Qoe);j8o=r(bDr,"xlm"),bDr.forEach(t),N8o=r(GFe," \u2014 "),A$=n(GFe,"A",{href:!0});var vDr=s(A$);D8o=r(vDr,"XLMForMultipleChoice"),vDr.forEach(t),q8o=r(GFe," (XLM model)"),GFe.forEach(t),G8o=i(O),ub=n(O,"LI",{});var OFe=s(ub);Hoe=n(OFe,"STRONG",{});var TDr=s(Hoe);O8o=r(TDr,"xlm-roberta"),TDr.forEach(t),X8o=r(OFe," \u2014 "),L$=n(OFe,"A",{href:!0});var FDr=s(L$);z8o=r(FDr,"XLMRobertaForMultipleChoice"),FDr.forEach(t),V8o=r(OFe," (XLM-RoBERTa model)"),OFe.forEach(t),W8o=i(O),bb=n(O,"LI",{});var XFe=s(bb);Uoe=n(XFe,"STRONG",{});var CDr=s(Uoe);Q8o=r(CDr,"xlm-roberta-xl"),CDr.forEach(t),H8o=r(XFe," \u2014 "),B$=n(XFe,"A",{href:!0});var MDr=s(B$);U8o=r(MDr,"XLMRobertaXLForMultipleChoice"),MDr.forEach(t),J8o=r(XFe," (XLM-RoBERTa-XL model)"),XFe.forEach(t),Y8o=i(O),vb=n(O,"LI",{});var zFe=s(vb);Joe=n(zFe,"STRONG",{});var EDr=s(Joe);K8o=r(EDr,"xlnet"),EDr.forEach(t),Z8o=r(zFe," \u2014 "),x$=n(zFe,"A",{href:!0});var yDr=s(x$);eFo=r(yDr,"XLNetForMultipleChoice"),yDr.forEach(t),oFo=r(zFe," (XLNet model)"),zFe.forEach(t),rFo=i(O),Tb=n(O,"LI",{});var VFe=s(Tb);Yoe=n(VFe,"STRONG",{});var wDr=s(Yoe);tFo=r(wDr,"yoso"),wDr.forEach(t),aFo=r(VFe," \u2014 "),k$=n(VFe,"A",{href:!0});var ADr=s(k$);nFo=r(ADr,"YosoForMultipleChoice"),ADr.forEach(t),sFo=r(VFe," (YOSO model)"),VFe.forEach(t),O.forEach(t),lFo=i(It),Fb=n(It,"P",{});var WFe=s(Fb);iFo=r(WFe,"The model is set in evaluation mode by default using "),Koe=n(WFe,"CODE",{});var LDr=s(Koe);dFo=r(LDr,"model.eval()"),LDr.forEach(t),cFo=r(WFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zoe=n(WFe,"CODE",{});var BDr=s(Zoe);fFo=r(BDr,"model.train()"),BDr.forEach(t),WFe.forEach(t),mFo=i(It),ere=n(It,"P",{});var xDr=s(ere);gFo=r(xDr,"Examples:"),xDr.forEach(t),hFo=i(It),m(rM.$$.fragment,It),It.forEach(t),Vs.forEach(t),VAe=i(d),ed=n(d,"H2",{class:!0});var JLe=s(ed);Cb=n(JLe,"A",{id:!0,class:!0,href:!0});var kDr=s(Cb);ore=n(kDr,"SPAN",{});var RDr=s(ore);m(tM.$$.fragment,RDr),RDr.forEach(t),kDr.forEach(t),pFo=i(JLe),rre=n(JLe,"SPAN",{});var SDr=s(rre);_Fo=r(SDr,"AutoModelForNextSentencePrediction"),SDr.forEach(t),JLe.forEach(t),WAe=i(d),Yo=n(d,"DIV",{class:!0});var Qs=s(Yo);m(aM.$$.fragment,Qs),uFo=i(Qs),od=n(Qs,"P",{});var gX=s(od);bFo=r(gX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),tre=n(gX,"CODE",{});var PDr=s(tre);vFo=r(PDr,"from_pretrained()"),PDr.forEach(t),TFo=r(gX,"class method or the "),are=n(gX,"CODE",{});var $Dr=s(are);FFo=r($Dr,"from_config()"),$Dr.forEach(t),CFo=r(gX,`class
method.`),gX.forEach(t),MFo=i(Qs),nM=n(Qs,"P",{});var YLe=s(nM);EFo=r(YLe,"This class cannot be instantiated directly using "),nre=n(YLe,"CODE",{});var IDr=s(nre);yFo=r(IDr,"__init__()"),IDr.forEach(t),wFo=r(YLe," (throws an error)."),YLe.forEach(t),AFo=i(Qs),Xr=n(Qs,"DIV",{class:!0});var Hs=s(Xr);m(sM.$$.fragment,Hs),LFo=i(Hs),sre=n(Hs,"P",{});var jDr=s(sre);BFo=r(jDr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),jDr.forEach(t),xFo=i(Hs),rd=n(Hs,"P",{});var hX=s(rd);kFo=r(hX,`Note:
Loading a model from its configuration file does `),lre=n(hX,"STRONG",{});var NDr=s(lre);RFo=r(NDr,"not"),NDr.forEach(t),SFo=r(hX,` load the model weights. It only affects the
model\u2019s configuration. Use `),ire=n(hX,"CODE",{});var DDr=s(ire);PFo=r(DDr,"from_pretrained()"),DDr.forEach(t),$Fo=r(hX,"to load the model weights."),hX.forEach(t),IFo=i(Hs),dre=n(Hs,"P",{});var qDr=s(dre);jFo=r(qDr,"Examples:"),qDr.forEach(t),NFo=i(Hs),m(lM.$$.fragment,Hs),Hs.forEach(t),DFo=i(Qs),je=n(Qs,"DIV",{class:!0});var jt=s(je);m(iM.$$.fragment,jt),qFo=i(jt),cre=n(jt,"P",{});var GDr=s(cre);GFo=r(GDr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),GDr.forEach(t),OFo=i(jt),Oa=n(jt,"P",{});var OF=s(Oa);XFo=r(OF,"The model class to instantiate is selected based on the "),fre=n(OF,"CODE",{});var ODr=s(fre);zFo=r(ODr,"model_type"),ODr.forEach(t),VFo=r(OF,` property of the config object (either
passed as an argument or loaded from `),mre=n(OF,"CODE",{});var XDr=s(mre);WFo=r(XDr,"pretrained_model_name_or_path"),XDr.forEach(t),QFo=r(OF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gre=n(OF,"CODE",{});var zDr=s(gre);HFo=r(zDr,"pretrained_model_name_or_path"),zDr.forEach(t),UFo=r(OF,":"),OF.forEach(t),JFo=i(jt),oa=n(jt,"UL",{});var Us=s(oa);Mb=n(Us,"LI",{});var QFe=s(Mb);hre=n(QFe,"STRONG",{});var VDr=s(hre);YFo=r(VDr,"bert"),VDr.forEach(t),KFo=r(QFe," \u2014 "),R$=n(QFe,"A",{href:!0});var WDr=s(R$);ZFo=r(WDr,"BertForNextSentencePrediction"),WDr.forEach(t),eCo=r(QFe," (BERT model)"),QFe.forEach(t),oCo=i(Us),Eb=n(Us,"LI",{});var HFe=s(Eb);pre=n(HFe,"STRONG",{});var QDr=s(pre);rCo=r(QDr,"fnet"),QDr.forEach(t),tCo=r(HFe," \u2014 "),S$=n(HFe,"A",{href:!0});var HDr=s(S$);aCo=r(HDr,"FNetForNextSentencePrediction"),HDr.forEach(t),nCo=r(HFe," (FNet model)"),HFe.forEach(t),sCo=i(Us),yb=n(Us,"LI",{});var UFe=s(yb);_re=n(UFe,"STRONG",{});var UDr=s(_re);lCo=r(UDr,"megatron-bert"),UDr.forEach(t),iCo=r(UFe," \u2014 "),P$=n(UFe,"A",{href:!0});var JDr=s(P$);dCo=r(JDr,"MegatronBertForNextSentencePrediction"),JDr.forEach(t),cCo=r(UFe," (MegatronBert model)"),UFe.forEach(t),fCo=i(Us),wb=n(Us,"LI",{});var JFe=s(wb);ure=n(JFe,"STRONG",{});var YDr=s(ure);mCo=r(YDr,"mobilebert"),YDr.forEach(t),gCo=r(JFe," \u2014 "),$$=n(JFe,"A",{href:!0});var KDr=s($$);hCo=r(KDr,"MobileBertForNextSentencePrediction"),KDr.forEach(t),pCo=r(JFe," (MobileBERT model)"),JFe.forEach(t),_Co=i(Us),Ab=n(Us,"LI",{});var YFe=s(Ab);bre=n(YFe,"STRONG",{});var ZDr=s(bre);uCo=r(ZDr,"qdqbert"),ZDr.forEach(t),bCo=r(YFe," \u2014 "),I$=n(YFe,"A",{href:!0});var eqr=s(I$);vCo=r(eqr,"QDQBertForNextSentencePrediction"),eqr.forEach(t),TCo=r(YFe," (QDQBert model)"),YFe.forEach(t),Us.forEach(t),FCo=i(jt),Lb=n(jt,"P",{});var KFe=s(Lb);CCo=r(KFe,"The model is set in evaluation mode by default using "),vre=n(KFe,"CODE",{});var oqr=s(vre);MCo=r(oqr,"model.eval()"),oqr.forEach(t),ECo=r(KFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tre=n(KFe,"CODE",{});var rqr=s(Tre);yCo=r(rqr,"model.train()"),rqr.forEach(t),KFe.forEach(t),wCo=i(jt),Fre=n(jt,"P",{});var tqr=s(Fre);ACo=r(tqr,"Examples:"),tqr.forEach(t),LCo=i(jt),m(dM.$$.fragment,jt),jt.forEach(t),Qs.forEach(t),QAe=i(d),td=n(d,"H2",{class:!0});var KLe=s(td);Bb=n(KLe,"A",{id:!0,class:!0,href:!0});var aqr=s(Bb);Cre=n(aqr,"SPAN",{});var nqr=s(Cre);m(cM.$$.fragment,nqr),nqr.forEach(t),aqr.forEach(t),BCo=i(KLe),Mre=n(KLe,"SPAN",{});var sqr=s(Mre);xCo=r(sqr,"AutoModelForTokenClassification"),sqr.forEach(t),KLe.forEach(t),HAe=i(d),Ko=n(d,"DIV",{class:!0});var Js=s(Ko);m(fM.$$.fragment,Js),kCo=i(Js),ad=n(Js,"P",{});var pX=s(ad);RCo=r(pX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Ere=n(pX,"CODE",{});var lqr=s(Ere);SCo=r(lqr,"from_pretrained()"),lqr.forEach(t),PCo=r(pX,"class method or the "),yre=n(pX,"CODE",{});var iqr=s(yre);$Co=r(iqr,"from_config()"),iqr.forEach(t),ICo=r(pX,`class
method.`),pX.forEach(t),jCo=i(Js),mM=n(Js,"P",{});var ZLe=s(mM);NCo=r(ZLe,"This class cannot be instantiated directly using "),wre=n(ZLe,"CODE",{});var dqr=s(wre);DCo=r(dqr,"__init__()"),dqr.forEach(t),qCo=r(ZLe," (throws an error)."),ZLe.forEach(t),GCo=i(Js),zr=n(Js,"DIV",{class:!0});var Ys=s(zr);m(gM.$$.fragment,Ys),OCo=i(Ys),Are=n(Ys,"P",{});var cqr=s(Are);XCo=r(cqr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),cqr.forEach(t),zCo=i(Ys),nd=n(Ys,"P",{});var _X=s(nd);VCo=r(_X,`Note:
Loading a model from its configuration file does `),Lre=n(_X,"STRONG",{});var fqr=s(Lre);WCo=r(fqr,"not"),fqr.forEach(t),QCo=r(_X,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bre=n(_X,"CODE",{});var mqr=s(Bre);HCo=r(mqr,"from_pretrained()"),mqr.forEach(t),UCo=r(_X,"to load the model weights."),_X.forEach(t),JCo=i(Ys),xre=n(Ys,"P",{});var gqr=s(xre);YCo=r(gqr,"Examples:"),gqr.forEach(t),KCo=i(Ys),m(hM.$$.fragment,Ys),Ys.forEach(t),ZCo=i(Js),Ne=n(Js,"DIV",{class:!0});var Nt=s(Ne);m(pM.$$.fragment,Nt),e4o=i(Nt),kre=n(Nt,"P",{});var hqr=s(kre);o4o=r(hqr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),hqr.forEach(t),r4o=i(Nt),Xa=n(Nt,"P",{});var XF=s(Xa);t4o=r(XF,"The model class to instantiate is selected based on the "),Rre=n(XF,"CODE",{});var pqr=s(Rre);a4o=r(pqr,"model_type"),pqr.forEach(t),n4o=r(XF,` property of the config object (either
passed as an argument or loaded from `),Sre=n(XF,"CODE",{});var _qr=s(Sre);s4o=r(_qr,"pretrained_model_name_or_path"),_qr.forEach(t),l4o=r(XF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pre=n(XF,"CODE",{});var uqr=s(Pre);i4o=r(uqr,"pretrained_model_name_or_path"),uqr.forEach(t),d4o=r(XF,":"),XF.forEach(t),c4o=i(Nt),N=n(Nt,"UL",{});var q=s(N);xb=n(q,"LI",{});var ZFe=s(xb);$re=n(ZFe,"STRONG",{});var bqr=s($re);f4o=r(bqr,"albert"),bqr.forEach(t),m4o=r(ZFe," \u2014 "),j$=n(ZFe,"A",{href:!0});var vqr=s(j$);g4o=r(vqr,"AlbertForTokenClassification"),vqr.forEach(t),h4o=r(ZFe," (ALBERT model)"),ZFe.forEach(t),p4o=i(q),kb=n(q,"LI",{});var eCe=s(kb);Ire=n(eCe,"STRONG",{});var Tqr=s(Ire);_4o=r(Tqr,"bert"),Tqr.forEach(t),u4o=r(eCe," \u2014 "),N$=n(eCe,"A",{href:!0});var Fqr=s(N$);b4o=r(Fqr,"BertForTokenClassification"),Fqr.forEach(t),v4o=r(eCe," (BERT model)"),eCe.forEach(t),T4o=i(q),Rb=n(q,"LI",{});var oCe=s(Rb);jre=n(oCe,"STRONG",{});var Cqr=s(jre);F4o=r(Cqr,"big_bird"),Cqr.forEach(t),C4o=r(oCe," \u2014 "),D$=n(oCe,"A",{href:!0});var Mqr=s(D$);M4o=r(Mqr,"BigBirdForTokenClassification"),Mqr.forEach(t),E4o=r(oCe," (BigBird model)"),oCe.forEach(t),y4o=i(q),Sb=n(q,"LI",{});var rCe=s(Sb);Nre=n(rCe,"STRONG",{});var Eqr=s(Nre);w4o=r(Eqr,"camembert"),Eqr.forEach(t),A4o=r(rCe," \u2014 "),q$=n(rCe,"A",{href:!0});var yqr=s(q$);L4o=r(yqr,"CamembertForTokenClassification"),yqr.forEach(t),B4o=r(rCe," (CamemBERT model)"),rCe.forEach(t),x4o=i(q),Pb=n(q,"LI",{});var tCe=s(Pb);Dre=n(tCe,"STRONG",{});var wqr=s(Dre);k4o=r(wqr,"canine"),wqr.forEach(t),R4o=r(tCe," \u2014 "),G$=n(tCe,"A",{href:!0});var Aqr=s(G$);S4o=r(Aqr,"CanineForTokenClassification"),Aqr.forEach(t),P4o=r(tCe," (Canine model)"),tCe.forEach(t),$4o=i(q),$b=n(q,"LI",{});var aCe=s($b);qre=n(aCe,"STRONG",{});var Lqr=s(qre);I4o=r(Lqr,"convbert"),Lqr.forEach(t),j4o=r(aCe," \u2014 "),O$=n(aCe,"A",{href:!0});var Bqr=s(O$);N4o=r(Bqr,"ConvBertForTokenClassification"),Bqr.forEach(t),D4o=r(aCe," (ConvBERT model)"),aCe.forEach(t),q4o=i(q),Ib=n(q,"LI",{});var nCe=s(Ib);Gre=n(nCe,"STRONG",{});var xqr=s(Gre);G4o=r(xqr,"deberta"),xqr.forEach(t),O4o=r(nCe," \u2014 "),X$=n(nCe,"A",{href:!0});var kqr=s(X$);X4o=r(kqr,"DebertaForTokenClassification"),kqr.forEach(t),z4o=r(nCe," (DeBERTa model)"),nCe.forEach(t),V4o=i(q),jb=n(q,"LI",{});var sCe=s(jb);Ore=n(sCe,"STRONG",{});var Rqr=s(Ore);W4o=r(Rqr,"deberta-v2"),Rqr.forEach(t),Q4o=r(sCe," \u2014 "),z$=n(sCe,"A",{href:!0});var Sqr=s(z$);H4o=r(Sqr,"DebertaV2ForTokenClassification"),Sqr.forEach(t),U4o=r(sCe," (DeBERTa-v2 model)"),sCe.forEach(t),J4o=i(q),Nb=n(q,"LI",{});var lCe=s(Nb);Xre=n(lCe,"STRONG",{});var Pqr=s(Xre);Y4o=r(Pqr,"distilbert"),Pqr.forEach(t),K4o=r(lCe," \u2014 "),V$=n(lCe,"A",{href:!0});var $qr=s(V$);Z4o=r($qr,"DistilBertForTokenClassification"),$qr.forEach(t),eMo=r(lCe," (DistilBERT model)"),lCe.forEach(t),oMo=i(q),Db=n(q,"LI",{});var iCe=s(Db);zre=n(iCe,"STRONG",{});var Iqr=s(zre);rMo=r(Iqr,"electra"),Iqr.forEach(t),tMo=r(iCe," \u2014 "),W$=n(iCe,"A",{href:!0});var jqr=s(W$);aMo=r(jqr,"ElectraForTokenClassification"),jqr.forEach(t),nMo=r(iCe," (ELECTRA model)"),iCe.forEach(t),sMo=i(q),qb=n(q,"LI",{});var dCe=s(qb);Vre=n(dCe,"STRONG",{});var Nqr=s(Vre);lMo=r(Nqr,"flaubert"),Nqr.forEach(t),iMo=r(dCe," \u2014 "),Q$=n(dCe,"A",{href:!0});var Dqr=s(Q$);dMo=r(Dqr,"FlaubertForTokenClassification"),Dqr.forEach(t),cMo=r(dCe," (FlauBERT model)"),dCe.forEach(t),fMo=i(q),Gb=n(q,"LI",{});var cCe=s(Gb);Wre=n(cCe,"STRONG",{});var qqr=s(Wre);mMo=r(qqr,"fnet"),qqr.forEach(t),gMo=r(cCe," \u2014 "),H$=n(cCe,"A",{href:!0});var Gqr=s(H$);hMo=r(Gqr,"FNetForTokenClassification"),Gqr.forEach(t),pMo=r(cCe," (FNet model)"),cCe.forEach(t),_Mo=i(q),Ob=n(q,"LI",{});var fCe=s(Ob);Qre=n(fCe,"STRONG",{});var Oqr=s(Qre);uMo=r(Oqr,"funnel"),Oqr.forEach(t),bMo=r(fCe," \u2014 "),U$=n(fCe,"A",{href:!0});var Xqr=s(U$);vMo=r(Xqr,"FunnelForTokenClassification"),Xqr.forEach(t),TMo=r(fCe," (Funnel Transformer model)"),fCe.forEach(t),FMo=i(q),Xb=n(q,"LI",{});var mCe=s(Xb);Hre=n(mCe,"STRONG",{});var zqr=s(Hre);CMo=r(zqr,"gpt2"),zqr.forEach(t),MMo=r(mCe," \u2014 "),J$=n(mCe,"A",{href:!0});var Vqr=s(J$);EMo=r(Vqr,"GPT2ForTokenClassification"),Vqr.forEach(t),yMo=r(mCe," (OpenAI GPT-2 model)"),mCe.forEach(t),wMo=i(q),zb=n(q,"LI",{});var gCe=s(zb);Ure=n(gCe,"STRONG",{});var Wqr=s(Ure);AMo=r(Wqr,"ibert"),Wqr.forEach(t),LMo=r(gCe," \u2014 "),Y$=n(gCe,"A",{href:!0});var Qqr=s(Y$);BMo=r(Qqr,"IBertForTokenClassification"),Qqr.forEach(t),xMo=r(gCe," (I-BERT model)"),gCe.forEach(t),kMo=i(q),Vb=n(q,"LI",{});var hCe=s(Vb);Jre=n(hCe,"STRONG",{});var Hqr=s(Jre);RMo=r(Hqr,"layoutlm"),Hqr.forEach(t),SMo=r(hCe," \u2014 "),K$=n(hCe,"A",{href:!0});var Uqr=s(K$);PMo=r(Uqr,"LayoutLMForTokenClassification"),Uqr.forEach(t),$Mo=r(hCe," (LayoutLM model)"),hCe.forEach(t),IMo=i(q),Wb=n(q,"LI",{});var pCe=s(Wb);Yre=n(pCe,"STRONG",{});var Jqr=s(Yre);jMo=r(Jqr,"layoutlmv2"),Jqr.forEach(t),NMo=r(pCe," \u2014 "),Z$=n(pCe,"A",{href:!0});var Yqr=s(Z$);DMo=r(Yqr,"LayoutLMv2ForTokenClassification"),Yqr.forEach(t),qMo=r(pCe," (LayoutLMv2 model)"),pCe.forEach(t),GMo=i(q),Qb=n(q,"LI",{});var _Ce=s(Qb);Kre=n(_Ce,"STRONG",{});var Kqr=s(Kre);OMo=r(Kqr,"longformer"),Kqr.forEach(t),XMo=r(_Ce," \u2014 "),eI=n(_Ce,"A",{href:!0});var Zqr=s(eI);zMo=r(Zqr,"LongformerForTokenClassification"),Zqr.forEach(t),VMo=r(_Ce," (Longformer model)"),_Ce.forEach(t),WMo=i(q),Hb=n(q,"LI",{});var uCe=s(Hb);Zre=n(uCe,"STRONG",{});var eGr=s(Zre);QMo=r(eGr,"megatron-bert"),eGr.forEach(t),HMo=r(uCe," \u2014 "),oI=n(uCe,"A",{href:!0});var oGr=s(oI);UMo=r(oGr,"MegatronBertForTokenClassification"),oGr.forEach(t),JMo=r(uCe," (MegatronBert model)"),uCe.forEach(t),YMo=i(q),Ub=n(q,"LI",{});var bCe=s(Ub);ete=n(bCe,"STRONG",{});var rGr=s(ete);KMo=r(rGr,"mobilebert"),rGr.forEach(t),ZMo=r(bCe," \u2014 "),rI=n(bCe,"A",{href:!0});var tGr=s(rI);eEo=r(tGr,"MobileBertForTokenClassification"),tGr.forEach(t),oEo=r(bCe," (MobileBERT model)"),bCe.forEach(t),rEo=i(q),Jb=n(q,"LI",{});var vCe=s(Jb);ote=n(vCe,"STRONG",{});var aGr=s(ote);tEo=r(aGr,"mpnet"),aGr.forEach(t),aEo=r(vCe," \u2014 "),tI=n(vCe,"A",{href:!0});var nGr=s(tI);nEo=r(nGr,"MPNetForTokenClassification"),nGr.forEach(t),sEo=r(vCe," (MPNet model)"),vCe.forEach(t),lEo=i(q),Yb=n(q,"LI",{});var TCe=s(Yb);rte=n(TCe,"STRONG",{});var sGr=s(rte);iEo=r(sGr,"nystromformer"),sGr.forEach(t),dEo=r(TCe," \u2014 "),aI=n(TCe,"A",{href:!0});var lGr=s(aI);cEo=r(lGr,"NystromformerForTokenClassification"),lGr.forEach(t),fEo=r(TCe," (Nystromformer model)"),TCe.forEach(t),mEo=i(q),Kb=n(q,"LI",{});var FCe=s(Kb);tte=n(FCe,"STRONG",{});var iGr=s(tte);gEo=r(iGr,"qdqbert"),iGr.forEach(t),hEo=r(FCe," \u2014 "),nI=n(FCe,"A",{href:!0});var dGr=s(nI);pEo=r(dGr,"QDQBertForTokenClassification"),dGr.forEach(t),_Eo=r(FCe," (QDQBert model)"),FCe.forEach(t),uEo=i(q),Zb=n(q,"LI",{});var CCe=s(Zb);ate=n(CCe,"STRONG",{});var cGr=s(ate);bEo=r(cGr,"rembert"),cGr.forEach(t),vEo=r(CCe," \u2014 "),sI=n(CCe,"A",{href:!0});var fGr=s(sI);TEo=r(fGr,"RemBertForTokenClassification"),fGr.forEach(t),FEo=r(CCe," (RemBERT model)"),CCe.forEach(t),CEo=i(q),e5=n(q,"LI",{});var MCe=s(e5);nte=n(MCe,"STRONG",{});var mGr=s(nte);MEo=r(mGr,"roberta"),mGr.forEach(t),EEo=r(MCe," \u2014 "),lI=n(MCe,"A",{href:!0});var gGr=s(lI);yEo=r(gGr,"RobertaForTokenClassification"),gGr.forEach(t),wEo=r(MCe," (RoBERTa model)"),MCe.forEach(t),AEo=i(q),o5=n(q,"LI",{});var ECe=s(o5);ste=n(ECe,"STRONG",{});var hGr=s(ste);LEo=r(hGr,"roformer"),hGr.forEach(t),BEo=r(ECe," \u2014 "),iI=n(ECe,"A",{href:!0});var pGr=s(iI);xEo=r(pGr,"RoFormerForTokenClassification"),pGr.forEach(t),kEo=r(ECe," (RoFormer model)"),ECe.forEach(t),REo=i(q),r5=n(q,"LI",{});var yCe=s(r5);lte=n(yCe,"STRONG",{});var _Gr=s(lte);SEo=r(_Gr,"squeezebert"),_Gr.forEach(t),PEo=r(yCe," \u2014 "),dI=n(yCe,"A",{href:!0});var uGr=s(dI);$Eo=r(uGr,"SqueezeBertForTokenClassification"),uGr.forEach(t),IEo=r(yCe," (SqueezeBERT model)"),yCe.forEach(t),jEo=i(q),t5=n(q,"LI",{});var wCe=s(t5);ite=n(wCe,"STRONG",{});var bGr=s(ite);NEo=r(bGr,"xlm"),bGr.forEach(t),DEo=r(wCe," \u2014 "),cI=n(wCe,"A",{href:!0});var vGr=s(cI);qEo=r(vGr,"XLMForTokenClassification"),vGr.forEach(t),GEo=r(wCe," (XLM model)"),wCe.forEach(t),OEo=i(q),a5=n(q,"LI",{});var ACe=s(a5);dte=n(ACe,"STRONG",{});var TGr=s(dte);XEo=r(TGr,"xlm-roberta"),TGr.forEach(t),zEo=r(ACe," \u2014 "),fI=n(ACe,"A",{href:!0});var FGr=s(fI);VEo=r(FGr,"XLMRobertaForTokenClassification"),FGr.forEach(t),WEo=r(ACe," (XLM-RoBERTa model)"),ACe.forEach(t),QEo=i(q),n5=n(q,"LI",{});var LCe=s(n5);cte=n(LCe,"STRONG",{});var CGr=s(cte);HEo=r(CGr,"xlm-roberta-xl"),CGr.forEach(t),UEo=r(LCe," \u2014 "),mI=n(LCe,"A",{href:!0});var MGr=s(mI);JEo=r(MGr,"XLMRobertaXLForTokenClassification"),MGr.forEach(t),YEo=r(LCe," (XLM-RoBERTa-XL model)"),LCe.forEach(t),KEo=i(q),s5=n(q,"LI",{});var BCe=s(s5);fte=n(BCe,"STRONG",{});var EGr=s(fte);ZEo=r(EGr,"xlnet"),EGr.forEach(t),e3o=r(BCe," \u2014 "),gI=n(BCe,"A",{href:!0});var yGr=s(gI);o3o=r(yGr,"XLNetForTokenClassification"),yGr.forEach(t),r3o=r(BCe," (XLNet model)"),BCe.forEach(t),t3o=i(q),l5=n(q,"LI",{});var xCe=s(l5);mte=n(xCe,"STRONG",{});var wGr=s(mte);a3o=r(wGr,"yoso"),wGr.forEach(t),n3o=r(xCe," \u2014 "),hI=n(xCe,"A",{href:!0});var AGr=s(hI);s3o=r(AGr,"YosoForTokenClassification"),AGr.forEach(t),l3o=r(xCe," (YOSO model)"),xCe.forEach(t),q.forEach(t),i3o=i(Nt),i5=n(Nt,"P",{});var kCe=s(i5);d3o=r(kCe,"The model is set in evaluation mode by default using "),gte=n(kCe,"CODE",{});var LGr=s(gte);c3o=r(LGr,"model.eval()"),LGr.forEach(t),f3o=r(kCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hte=n(kCe,"CODE",{});var BGr=s(hte);m3o=r(BGr,"model.train()"),BGr.forEach(t),kCe.forEach(t),g3o=i(Nt),pte=n(Nt,"P",{});var xGr=s(pte);h3o=r(xGr,"Examples:"),xGr.forEach(t),p3o=i(Nt),m(_M.$$.fragment,Nt),Nt.forEach(t),Js.forEach(t),UAe=i(d),sd=n(d,"H2",{class:!0});var e9e=s(sd);d5=n(e9e,"A",{id:!0,class:!0,href:!0});var kGr=s(d5);_te=n(kGr,"SPAN",{});var RGr=s(_te);m(uM.$$.fragment,RGr),RGr.forEach(t),kGr.forEach(t),_3o=i(e9e),ute=n(e9e,"SPAN",{});var SGr=s(ute);u3o=r(SGr,"AutoModelForQuestionAnswering"),SGr.forEach(t),e9e.forEach(t),JAe=i(d),Zo=n(d,"DIV",{class:!0});var Ks=s(Zo);m(bM.$$.fragment,Ks),b3o=i(Ks),ld=n(Ks,"P",{});var uX=s(ld);v3o=r(uX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),bte=n(uX,"CODE",{});var PGr=s(bte);T3o=r(PGr,"from_pretrained()"),PGr.forEach(t),F3o=r(uX,"class method or the "),vte=n(uX,"CODE",{});var $Gr=s(vte);C3o=r($Gr,"from_config()"),$Gr.forEach(t),M3o=r(uX,`class
method.`),uX.forEach(t),E3o=i(Ks),vM=n(Ks,"P",{});var o9e=s(vM);y3o=r(o9e,"This class cannot be instantiated directly using "),Tte=n(o9e,"CODE",{});var IGr=s(Tte);w3o=r(IGr,"__init__()"),IGr.forEach(t),A3o=r(o9e," (throws an error)."),o9e.forEach(t),L3o=i(Ks),Vr=n(Ks,"DIV",{class:!0});var Zs=s(Vr);m(TM.$$.fragment,Zs),B3o=i(Zs),Fte=n(Zs,"P",{});var jGr=s(Fte);x3o=r(jGr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),jGr.forEach(t),k3o=i(Zs),id=n(Zs,"P",{});var bX=s(id);R3o=r(bX,`Note:
Loading a model from its configuration file does `),Cte=n(bX,"STRONG",{});var NGr=s(Cte);S3o=r(NGr,"not"),NGr.forEach(t),P3o=r(bX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mte=n(bX,"CODE",{});var DGr=s(Mte);$3o=r(DGr,"from_pretrained()"),DGr.forEach(t),I3o=r(bX,"to load the model weights."),bX.forEach(t),j3o=i(Zs),Ete=n(Zs,"P",{});var qGr=s(Ete);N3o=r(qGr,"Examples:"),qGr.forEach(t),D3o=i(Zs),m(FM.$$.fragment,Zs),Zs.forEach(t),q3o=i(Ks),De=n(Ks,"DIV",{class:!0});var Dt=s(De);m(CM.$$.fragment,Dt),G3o=i(Dt),yte=n(Dt,"P",{});var GGr=s(yte);O3o=r(GGr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),GGr.forEach(t),X3o=i(Dt),za=n(Dt,"P",{});var zF=s(za);z3o=r(zF,"The model class to instantiate is selected based on the "),wte=n(zF,"CODE",{});var OGr=s(wte);V3o=r(OGr,"model_type"),OGr.forEach(t),W3o=r(zF,` property of the config object (either
passed as an argument or loaded from `),Ate=n(zF,"CODE",{});var XGr=s(Ate);Q3o=r(XGr,"pretrained_model_name_or_path"),XGr.forEach(t),H3o=r(zF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lte=n(zF,"CODE",{});var zGr=s(Lte);U3o=r(zGr,"pretrained_model_name_or_path"),zGr.forEach(t),J3o=r(zF,":"),zF.forEach(t),Y3o=i(Dt),R=n(Dt,"UL",{});var P=s(R);c5=n(P,"LI",{});var RCe=s(c5);Bte=n(RCe,"STRONG",{});var VGr=s(Bte);K3o=r(VGr,"albert"),VGr.forEach(t),Z3o=r(RCe," \u2014 "),pI=n(RCe,"A",{href:!0});var WGr=s(pI);eyo=r(WGr,"AlbertForQuestionAnswering"),WGr.forEach(t),oyo=r(RCe," (ALBERT model)"),RCe.forEach(t),ryo=i(P),f5=n(P,"LI",{});var SCe=s(f5);xte=n(SCe,"STRONG",{});var QGr=s(xte);tyo=r(QGr,"bart"),QGr.forEach(t),ayo=r(SCe," \u2014 "),_I=n(SCe,"A",{href:!0});var HGr=s(_I);nyo=r(HGr,"BartForQuestionAnswering"),HGr.forEach(t),syo=r(SCe," (BART model)"),SCe.forEach(t),lyo=i(P),m5=n(P,"LI",{});var PCe=s(m5);kte=n(PCe,"STRONG",{});var UGr=s(kte);iyo=r(UGr,"bert"),UGr.forEach(t),dyo=r(PCe," \u2014 "),uI=n(PCe,"A",{href:!0});var JGr=s(uI);cyo=r(JGr,"BertForQuestionAnswering"),JGr.forEach(t),fyo=r(PCe," (BERT model)"),PCe.forEach(t),myo=i(P),g5=n(P,"LI",{});var $Ce=s(g5);Rte=n($Ce,"STRONG",{});var YGr=s(Rte);gyo=r(YGr,"big_bird"),YGr.forEach(t),hyo=r($Ce," \u2014 "),bI=n($Ce,"A",{href:!0});var KGr=s(bI);pyo=r(KGr,"BigBirdForQuestionAnswering"),KGr.forEach(t),_yo=r($Ce," (BigBird model)"),$Ce.forEach(t),uyo=i(P),h5=n(P,"LI",{});var ICe=s(h5);Ste=n(ICe,"STRONG",{});var ZGr=s(Ste);byo=r(ZGr,"bigbird_pegasus"),ZGr.forEach(t),vyo=r(ICe," \u2014 "),vI=n(ICe,"A",{href:!0});var eOr=s(vI);Tyo=r(eOr,"BigBirdPegasusForQuestionAnswering"),eOr.forEach(t),Fyo=r(ICe," (BigBirdPegasus model)"),ICe.forEach(t),Cyo=i(P),p5=n(P,"LI",{});var jCe=s(p5);Pte=n(jCe,"STRONG",{});var oOr=s(Pte);Myo=r(oOr,"camembert"),oOr.forEach(t),Eyo=r(jCe," \u2014 "),TI=n(jCe,"A",{href:!0});var rOr=s(TI);yyo=r(rOr,"CamembertForQuestionAnswering"),rOr.forEach(t),wyo=r(jCe," (CamemBERT model)"),jCe.forEach(t),Ayo=i(P),_5=n(P,"LI",{});var NCe=s(_5);$te=n(NCe,"STRONG",{});var tOr=s($te);Lyo=r(tOr,"canine"),tOr.forEach(t),Byo=r(NCe," \u2014 "),FI=n(NCe,"A",{href:!0});var aOr=s(FI);xyo=r(aOr,"CanineForQuestionAnswering"),aOr.forEach(t),kyo=r(NCe," (Canine model)"),NCe.forEach(t),Ryo=i(P),u5=n(P,"LI",{});var DCe=s(u5);Ite=n(DCe,"STRONG",{});var nOr=s(Ite);Syo=r(nOr,"convbert"),nOr.forEach(t),Pyo=r(DCe," \u2014 "),CI=n(DCe,"A",{href:!0});var sOr=s(CI);$yo=r(sOr,"ConvBertForQuestionAnswering"),sOr.forEach(t),Iyo=r(DCe," (ConvBERT model)"),DCe.forEach(t),jyo=i(P),b5=n(P,"LI",{});var qCe=s(b5);jte=n(qCe,"STRONG",{});var lOr=s(jte);Nyo=r(lOr,"deberta"),lOr.forEach(t),Dyo=r(qCe," \u2014 "),MI=n(qCe,"A",{href:!0});var iOr=s(MI);qyo=r(iOr,"DebertaForQuestionAnswering"),iOr.forEach(t),Gyo=r(qCe," (DeBERTa model)"),qCe.forEach(t),Oyo=i(P),v5=n(P,"LI",{});var GCe=s(v5);Nte=n(GCe,"STRONG",{});var dOr=s(Nte);Xyo=r(dOr,"deberta-v2"),dOr.forEach(t),zyo=r(GCe," \u2014 "),EI=n(GCe,"A",{href:!0});var cOr=s(EI);Vyo=r(cOr,"DebertaV2ForQuestionAnswering"),cOr.forEach(t),Wyo=r(GCe," (DeBERTa-v2 model)"),GCe.forEach(t),Qyo=i(P),T5=n(P,"LI",{});var OCe=s(T5);Dte=n(OCe,"STRONG",{});var fOr=s(Dte);Hyo=r(fOr,"distilbert"),fOr.forEach(t),Uyo=r(OCe," \u2014 "),yI=n(OCe,"A",{href:!0});var mOr=s(yI);Jyo=r(mOr,"DistilBertForQuestionAnswering"),mOr.forEach(t),Yyo=r(OCe," (DistilBERT model)"),OCe.forEach(t),Kyo=i(P),F5=n(P,"LI",{});var XCe=s(F5);qte=n(XCe,"STRONG",{});var gOr=s(qte);Zyo=r(gOr,"electra"),gOr.forEach(t),ewo=r(XCe," \u2014 "),wI=n(XCe,"A",{href:!0});var hOr=s(wI);owo=r(hOr,"ElectraForQuestionAnswering"),hOr.forEach(t),rwo=r(XCe," (ELECTRA model)"),XCe.forEach(t),two=i(P),C5=n(P,"LI",{});var zCe=s(C5);Gte=n(zCe,"STRONG",{});var pOr=s(Gte);awo=r(pOr,"flaubert"),pOr.forEach(t),nwo=r(zCe," \u2014 "),AI=n(zCe,"A",{href:!0});var _Or=s(AI);swo=r(_Or,"FlaubertForQuestionAnsweringSimple"),_Or.forEach(t),lwo=r(zCe," (FlauBERT model)"),zCe.forEach(t),iwo=i(P),M5=n(P,"LI",{});var VCe=s(M5);Ote=n(VCe,"STRONG",{});var uOr=s(Ote);dwo=r(uOr,"fnet"),uOr.forEach(t),cwo=r(VCe," \u2014 "),LI=n(VCe,"A",{href:!0});var bOr=s(LI);fwo=r(bOr,"FNetForQuestionAnswering"),bOr.forEach(t),mwo=r(VCe," (FNet model)"),VCe.forEach(t),gwo=i(P),E5=n(P,"LI",{});var WCe=s(E5);Xte=n(WCe,"STRONG",{});var vOr=s(Xte);hwo=r(vOr,"funnel"),vOr.forEach(t),pwo=r(WCe," \u2014 "),BI=n(WCe,"A",{href:!0});var TOr=s(BI);_wo=r(TOr,"FunnelForQuestionAnswering"),TOr.forEach(t),uwo=r(WCe," (Funnel Transformer model)"),WCe.forEach(t),bwo=i(P),y5=n(P,"LI",{});var QCe=s(y5);zte=n(QCe,"STRONG",{});var FOr=s(zte);vwo=r(FOr,"gptj"),FOr.forEach(t),Two=r(QCe," \u2014 "),xI=n(QCe,"A",{href:!0});var COr=s(xI);Fwo=r(COr,"GPTJForQuestionAnswering"),COr.forEach(t),Cwo=r(QCe," (GPT-J model)"),QCe.forEach(t),Mwo=i(P),w5=n(P,"LI",{});var HCe=s(w5);Vte=n(HCe,"STRONG",{});var MOr=s(Vte);Ewo=r(MOr,"ibert"),MOr.forEach(t),ywo=r(HCe," \u2014 "),kI=n(HCe,"A",{href:!0});var EOr=s(kI);wwo=r(EOr,"IBertForQuestionAnswering"),EOr.forEach(t),Awo=r(HCe," (I-BERT model)"),HCe.forEach(t),Lwo=i(P),A5=n(P,"LI",{});var UCe=s(A5);Wte=n(UCe,"STRONG",{});var yOr=s(Wte);Bwo=r(yOr,"layoutlmv2"),yOr.forEach(t),xwo=r(UCe," \u2014 "),RI=n(UCe,"A",{href:!0});var wOr=s(RI);kwo=r(wOr,"LayoutLMv2ForQuestionAnswering"),wOr.forEach(t),Rwo=r(UCe," (LayoutLMv2 model)"),UCe.forEach(t),Swo=i(P),L5=n(P,"LI",{});var JCe=s(L5);Qte=n(JCe,"STRONG",{});var AOr=s(Qte);Pwo=r(AOr,"led"),AOr.forEach(t),$wo=r(JCe," \u2014 "),SI=n(JCe,"A",{href:!0});var LOr=s(SI);Iwo=r(LOr,"LEDForQuestionAnswering"),LOr.forEach(t),jwo=r(JCe," (LED model)"),JCe.forEach(t),Nwo=i(P),B5=n(P,"LI",{});var YCe=s(B5);Hte=n(YCe,"STRONG",{});var BOr=s(Hte);Dwo=r(BOr,"longformer"),BOr.forEach(t),qwo=r(YCe," \u2014 "),PI=n(YCe,"A",{href:!0});var xOr=s(PI);Gwo=r(xOr,"LongformerForQuestionAnswering"),xOr.forEach(t),Owo=r(YCe," (Longformer model)"),YCe.forEach(t),Xwo=i(P),x5=n(P,"LI",{});var KCe=s(x5);Ute=n(KCe,"STRONG",{});var kOr=s(Ute);zwo=r(kOr,"lxmert"),kOr.forEach(t),Vwo=r(KCe," \u2014 "),$I=n(KCe,"A",{href:!0});var ROr=s($I);Wwo=r(ROr,"LxmertForQuestionAnswering"),ROr.forEach(t),Qwo=r(KCe," (LXMERT model)"),KCe.forEach(t),Hwo=i(P),k5=n(P,"LI",{});var ZCe=s(k5);Jte=n(ZCe,"STRONG",{});var SOr=s(Jte);Uwo=r(SOr,"mbart"),SOr.forEach(t),Jwo=r(ZCe," \u2014 "),II=n(ZCe,"A",{href:!0});var POr=s(II);Ywo=r(POr,"MBartForQuestionAnswering"),POr.forEach(t),Kwo=r(ZCe," (mBART model)"),ZCe.forEach(t),Zwo=i(P),R5=n(P,"LI",{});var e4e=s(R5);Yte=n(e4e,"STRONG",{});var $Or=s(Yte);eAo=r($Or,"megatron-bert"),$Or.forEach(t),oAo=r(e4e," \u2014 "),jI=n(e4e,"A",{href:!0});var IOr=s(jI);rAo=r(IOr,"MegatronBertForQuestionAnswering"),IOr.forEach(t),tAo=r(e4e," (MegatronBert model)"),e4e.forEach(t),aAo=i(P),S5=n(P,"LI",{});var o4e=s(S5);Kte=n(o4e,"STRONG",{});var jOr=s(Kte);nAo=r(jOr,"mobilebert"),jOr.forEach(t),sAo=r(o4e," \u2014 "),NI=n(o4e,"A",{href:!0});var NOr=s(NI);lAo=r(NOr,"MobileBertForQuestionAnswering"),NOr.forEach(t),iAo=r(o4e," (MobileBERT model)"),o4e.forEach(t),dAo=i(P),P5=n(P,"LI",{});var r4e=s(P5);Zte=n(r4e,"STRONG",{});var DOr=s(Zte);cAo=r(DOr,"mpnet"),DOr.forEach(t),fAo=r(r4e," \u2014 "),DI=n(r4e,"A",{href:!0});var qOr=s(DI);mAo=r(qOr,"MPNetForQuestionAnswering"),qOr.forEach(t),gAo=r(r4e," (MPNet model)"),r4e.forEach(t),hAo=i(P),$5=n(P,"LI",{});var t4e=s($5);eae=n(t4e,"STRONG",{});var GOr=s(eae);pAo=r(GOr,"nystromformer"),GOr.forEach(t),_Ao=r(t4e," \u2014 "),qI=n(t4e,"A",{href:!0});var OOr=s(qI);uAo=r(OOr,"NystromformerForQuestionAnswering"),OOr.forEach(t),bAo=r(t4e," (Nystromformer model)"),t4e.forEach(t),vAo=i(P),I5=n(P,"LI",{});var a4e=s(I5);oae=n(a4e,"STRONG",{});var XOr=s(oae);TAo=r(XOr,"qdqbert"),XOr.forEach(t),FAo=r(a4e," \u2014 "),GI=n(a4e,"A",{href:!0});var zOr=s(GI);CAo=r(zOr,"QDQBertForQuestionAnswering"),zOr.forEach(t),MAo=r(a4e," (QDQBert model)"),a4e.forEach(t),EAo=i(P),j5=n(P,"LI",{});var n4e=s(j5);rae=n(n4e,"STRONG",{});var VOr=s(rae);yAo=r(VOr,"reformer"),VOr.forEach(t),wAo=r(n4e," \u2014 "),OI=n(n4e,"A",{href:!0});var WOr=s(OI);AAo=r(WOr,"ReformerForQuestionAnswering"),WOr.forEach(t),LAo=r(n4e," (Reformer model)"),n4e.forEach(t),BAo=i(P),N5=n(P,"LI",{});var s4e=s(N5);tae=n(s4e,"STRONG",{});var QOr=s(tae);xAo=r(QOr,"rembert"),QOr.forEach(t),kAo=r(s4e," \u2014 "),XI=n(s4e,"A",{href:!0});var HOr=s(XI);RAo=r(HOr,"RemBertForQuestionAnswering"),HOr.forEach(t),SAo=r(s4e," (RemBERT model)"),s4e.forEach(t),PAo=i(P),D5=n(P,"LI",{});var l4e=s(D5);aae=n(l4e,"STRONG",{});var UOr=s(aae);$Ao=r(UOr,"roberta"),UOr.forEach(t),IAo=r(l4e," \u2014 "),zI=n(l4e,"A",{href:!0});var JOr=s(zI);jAo=r(JOr,"RobertaForQuestionAnswering"),JOr.forEach(t),NAo=r(l4e," (RoBERTa model)"),l4e.forEach(t),DAo=i(P),q5=n(P,"LI",{});var i4e=s(q5);nae=n(i4e,"STRONG",{});var YOr=s(nae);qAo=r(YOr,"roformer"),YOr.forEach(t),GAo=r(i4e," \u2014 "),VI=n(i4e,"A",{href:!0});var KOr=s(VI);OAo=r(KOr,"RoFormerForQuestionAnswering"),KOr.forEach(t),XAo=r(i4e," (RoFormer model)"),i4e.forEach(t),zAo=i(P),G5=n(P,"LI",{});var d4e=s(G5);sae=n(d4e,"STRONG",{});var ZOr=s(sae);VAo=r(ZOr,"splinter"),ZOr.forEach(t),WAo=r(d4e," \u2014 "),WI=n(d4e,"A",{href:!0});var eXr=s(WI);QAo=r(eXr,"SplinterForQuestionAnswering"),eXr.forEach(t),HAo=r(d4e," (Splinter model)"),d4e.forEach(t),UAo=i(P),O5=n(P,"LI",{});var c4e=s(O5);lae=n(c4e,"STRONG",{});var oXr=s(lae);JAo=r(oXr,"squeezebert"),oXr.forEach(t),YAo=r(c4e," \u2014 "),QI=n(c4e,"A",{href:!0});var rXr=s(QI);KAo=r(rXr,"SqueezeBertForQuestionAnswering"),rXr.forEach(t),ZAo=r(c4e," (SqueezeBERT model)"),c4e.forEach(t),e0o=i(P),X5=n(P,"LI",{});var f4e=s(X5);iae=n(f4e,"STRONG",{});var tXr=s(iae);o0o=r(tXr,"xlm"),tXr.forEach(t),r0o=r(f4e," \u2014 "),HI=n(f4e,"A",{href:!0});var aXr=s(HI);t0o=r(aXr,"XLMForQuestionAnsweringSimple"),aXr.forEach(t),a0o=r(f4e," (XLM model)"),f4e.forEach(t),n0o=i(P),z5=n(P,"LI",{});var m4e=s(z5);dae=n(m4e,"STRONG",{});var nXr=s(dae);s0o=r(nXr,"xlm-roberta"),nXr.forEach(t),l0o=r(m4e," \u2014 "),UI=n(m4e,"A",{href:!0});var sXr=s(UI);i0o=r(sXr,"XLMRobertaForQuestionAnswering"),sXr.forEach(t),d0o=r(m4e," (XLM-RoBERTa model)"),m4e.forEach(t),c0o=i(P),V5=n(P,"LI",{});var g4e=s(V5);cae=n(g4e,"STRONG",{});var lXr=s(cae);f0o=r(lXr,"xlm-roberta-xl"),lXr.forEach(t),m0o=r(g4e," \u2014 "),JI=n(g4e,"A",{href:!0});var iXr=s(JI);g0o=r(iXr,"XLMRobertaXLForQuestionAnswering"),iXr.forEach(t),h0o=r(g4e," (XLM-RoBERTa-XL model)"),g4e.forEach(t),p0o=i(P),W5=n(P,"LI",{});var h4e=s(W5);fae=n(h4e,"STRONG",{});var dXr=s(fae);_0o=r(dXr,"xlnet"),dXr.forEach(t),u0o=r(h4e," \u2014 "),YI=n(h4e,"A",{href:!0});var cXr=s(YI);b0o=r(cXr,"XLNetForQuestionAnsweringSimple"),cXr.forEach(t),v0o=r(h4e," (XLNet model)"),h4e.forEach(t),T0o=i(P),Q5=n(P,"LI",{});var p4e=s(Q5);mae=n(p4e,"STRONG",{});var fXr=s(mae);F0o=r(fXr,"yoso"),fXr.forEach(t),C0o=r(p4e," \u2014 "),KI=n(p4e,"A",{href:!0});var mXr=s(KI);M0o=r(mXr,"YosoForQuestionAnswering"),mXr.forEach(t),E0o=r(p4e," (YOSO model)"),p4e.forEach(t),P.forEach(t),y0o=i(Dt),H5=n(Dt,"P",{});var _4e=s(H5);w0o=r(_4e,"The model is set in evaluation mode by default using "),gae=n(_4e,"CODE",{});var gXr=s(gae);A0o=r(gXr,"model.eval()"),gXr.forEach(t),L0o=r(_4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hae=n(_4e,"CODE",{});var hXr=s(hae);B0o=r(hXr,"model.train()"),hXr.forEach(t),_4e.forEach(t),x0o=i(Dt),pae=n(Dt,"P",{});var pXr=s(pae);k0o=r(pXr,"Examples:"),pXr.forEach(t),R0o=i(Dt),m(MM.$$.fragment,Dt),Dt.forEach(t),Ks.forEach(t),YAe=i(d),dd=n(d,"H2",{class:!0});var r9e=s(dd);U5=n(r9e,"A",{id:!0,class:!0,href:!0});var _Xr=s(U5);_ae=n(_Xr,"SPAN",{});var uXr=s(_ae);m(EM.$$.fragment,uXr),uXr.forEach(t),_Xr.forEach(t),S0o=i(r9e),uae=n(r9e,"SPAN",{});var bXr=s(uae);P0o=r(bXr,"AutoModelForTableQuestionAnswering"),bXr.forEach(t),r9e.forEach(t),KAe=i(d),er=n(d,"DIV",{class:!0});var el=s(er);m(yM.$$.fragment,el),$0o=i(el),cd=n(el,"P",{});var vX=s(cd);I0o=r(vX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),bae=n(vX,"CODE",{});var vXr=s(bae);j0o=r(vXr,"from_pretrained()"),vXr.forEach(t),N0o=r(vX,"class method or the "),vae=n(vX,"CODE",{});var TXr=s(vae);D0o=r(TXr,"from_config()"),TXr.forEach(t),q0o=r(vX,`class
method.`),vX.forEach(t),G0o=i(el),wM=n(el,"P",{});var t9e=s(wM);O0o=r(t9e,"This class cannot be instantiated directly using "),Tae=n(t9e,"CODE",{});var FXr=s(Tae);X0o=r(FXr,"__init__()"),FXr.forEach(t),z0o=r(t9e," (throws an error)."),t9e.forEach(t),V0o=i(el),Wr=n(el,"DIV",{class:!0});var ol=s(Wr);m(AM.$$.fragment,ol),W0o=i(ol),Fae=n(ol,"P",{});var CXr=s(Fae);Q0o=r(CXr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),CXr.forEach(t),H0o=i(ol),fd=n(ol,"P",{});var TX=s(fd);U0o=r(TX,`Note:
Loading a model from its configuration file does `),Cae=n(TX,"STRONG",{});var MXr=s(Cae);J0o=r(MXr,"not"),MXr.forEach(t),Y0o=r(TX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mae=n(TX,"CODE",{});var EXr=s(Mae);K0o=r(EXr,"from_pretrained()"),EXr.forEach(t),Z0o=r(TX,"to load the model weights."),TX.forEach(t),eLo=i(ol),Eae=n(ol,"P",{});var yXr=s(Eae);oLo=r(yXr,"Examples:"),yXr.forEach(t),rLo=i(ol),m(LM.$$.fragment,ol),ol.forEach(t),tLo=i(el),qe=n(el,"DIV",{class:!0});var qt=s(qe);m(BM.$$.fragment,qt),aLo=i(qt),yae=n(qt,"P",{});var wXr=s(yae);nLo=r(wXr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),wXr.forEach(t),sLo=i(qt),Va=n(qt,"P",{});var VF=s(Va);lLo=r(VF,"The model class to instantiate is selected based on the "),wae=n(VF,"CODE",{});var AXr=s(wae);iLo=r(AXr,"model_type"),AXr.forEach(t),dLo=r(VF,` property of the config object (either
passed as an argument or loaded from `),Aae=n(VF,"CODE",{});var LXr=s(Aae);cLo=r(LXr,"pretrained_model_name_or_path"),LXr.forEach(t),fLo=r(VF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lae=n(VF,"CODE",{});var BXr=s(Lae);mLo=r(BXr,"pretrained_model_name_or_path"),BXr.forEach(t),gLo=r(VF,":"),VF.forEach(t),hLo=i(qt),Bae=n(qt,"UL",{});var xXr=s(Bae);J5=n(xXr,"LI",{});var u4e=s(J5);xae=n(u4e,"STRONG",{});var kXr=s(xae);pLo=r(kXr,"tapas"),kXr.forEach(t),_Lo=r(u4e," \u2014 "),ZI=n(u4e,"A",{href:!0});var RXr=s(ZI);uLo=r(RXr,"TapasForQuestionAnswering"),RXr.forEach(t),bLo=r(u4e," (TAPAS model)"),u4e.forEach(t),xXr.forEach(t),vLo=i(qt),Y5=n(qt,"P",{});var b4e=s(Y5);TLo=r(b4e,"The model is set in evaluation mode by default using "),kae=n(b4e,"CODE",{});var SXr=s(kae);FLo=r(SXr,"model.eval()"),SXr.forEach(t),CLo=r(b4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rae=n(b4e,"CODE",{});var PXr=s(Rae);MLo=r(PXr,"model.train()"),PXr.forEach(t),b4e.forEach(t),ELo=i(qt),Sae=n(qt,"P",{});var $Xr=s(Sae);yLo=r($Xr,"Examples:"),$Xr.forEach(t),wLo=i(qt),m(xM.$$.fragment,qt),qt.forEach(t),el.forEach(t),ZAe=i(d),md=n(d,"H2",{class:!0});var a9e=s(md);K5=n(a9e,"A",{id:!0,class:!0,href:!0});var IXr=s(K5);Pae=n(IXr,"SPAN",{});var jXr=s(Pae);m(kM.$$.fragment,jXr),jXr.forEach(t),IXr.forEach(t),ALo=i(a9e),$ae=n(a9e,"SPAN",{});var NXr=s($ae);LLo=r(NXr,"AutoModelForImageClassification"),NXr.forEach(t),a9e.forEach(t),e0e=i(d),or=n(d,"DIV",{class:!0});var rl=s(or);m(RM.$$.fragment,rl),BLo=i(rl),gd=n(rl,"P",{});var FX=s(gd);xLo=r(FX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Iae=n(FX,"CODE",{});var DXr=s(Iae);kLo=r(DXr,"from_pretrained()"),DXr.forEach(t),RLo=r(FX,"class method or the "),jae=n(FX,"CODE",{});var qXr=s(jae);SLo=r(qXr,"from_config()"),qXr.forEach(t),PLo=r(FX,`class
method.`),FX.forEach(t),$Lo=i(rl),SM=n(rl,"P",{});var n9e=s(SM);ILo=r(n9e,"This class cannot be instantiated directly using "),Nae=n(n9e,"CODE",{});var GXr=s(Nae);jLo=r(GXr,"__init__()"),GXr.forEach(t),NLo=r(n9e," (throws an error)."),n9e.forEach(t),DLo=i(rl),Qr=n(rl,"DIV",{class:!0});var tl=s(Qr);m(PM.$$.fragment,tl),qLo=i(tl),Dae=n(tl,"P",{});var OXr=s(Dae);GLo=r(OXr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),OXr.forEach(t),OLo=i(tl),hd=n(tl,"P",{});var CX=s(hd);XLo=r(CX,`Note:
Loading a model from its configuration file does `),qae=n(CX,"STRONG",{});var XXr=s(qae);zLo=r(XXr,"not"),XXr.forEach(t),VLo=r(CX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gae=n(CX,"CODE",{});var zXr=s(Gae);WLo=r(zXr,"from_pretrained()"),zXr.forEach(t),QLo=r(CX,"to load the model weights."),CX.forEach(t),HLo=i(tl),Oae=n(tl,"P",{});var VXr=s(Oae);ULo=r(VXr,"Examples:"),VXr.forEach(t),JLo=i(tl),m($M.$$.fragment,tl),tl.forEach(t),YLo=i(rl),Ge=n(rl,"DIV",{class:!0});var Gt=s(Ge);m(IM.$$.fragment,Gt),KLo=i(Gt),Xae=n(Gt,"P",{});var WXr=s(Xae);ZLo=r(WXr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),WXr.forEach(t),e9o=i(Gt),Wa=n(Gt,"P",{});var WF=s(Wa);o9o=r(WF,"The model class to instantiate is selected based on the "),zae=n(WF,"CODE",{});var QXr=s(zae);r9o=r(QXr,"model_type"),QXr.forEach(t),t9o=r(WF,` property of the config object (either
passed as an argument or loaded from `),Vae=n(WF,"CODE",{});var HXr=s(Vae);a9o=r(HXr,"pretrained_model_name_or_path"),HXr.forEach(t),n9o=r(WF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wae=n(WF,"CODE",{});var UXr=s(Wae);s9o=r(UXr,"pretrained_model_name_or_path"),UXr.forEach(t),l9o=r(WF,":"),WF.forEach(t),i9o=i(Gt),we=n(Gt,"UL",{});var jo=s(we);Z5=n(jo,"LI",{});var v4e=s(Z5);Qae=n(v4e,"STRONG",{});var JXr=s(Qae);d9o=r(JXr,"beit"),JXr.forEach(t),c9o=r(v4e," \u2014 "),ej=n(v4e,"A",{href:!0});var YXr=s(ej);f9o=r(YXr,"BeitForImageClassification"),YXr.forEach(t),m9o=r(v4e," (BEiT model)"),v4e.forEach(t),g9o=i(jo),e2=n(jo,"LI",{});var T4e=s(e2);Hae=n(T4e,"STRONG",{});var KXr=s(Hae);h9o=r(KXr,"convnext"),KXr.forEach(t),p9o=r(T4e," \u2014 "),oj=n(T4e,"A",{href:!0});var ZXr=s(oj);_9o=r(ZXr,"ConvNextForImageClassification"),ZXr.forEach(t),u9o=r(T4e," (ConvNext model)"),T4e.forEach(t),b9o=i(jo),As=n(jo,"LI",{});var t0=s(As);Uae=n(t0,"STRONG",{});var ezr=s(Uae);v9o=r(ezr,"deit"),ezr.forEach(t),T9o=r(t0," \u2014 "),rj=n(t0,"A",{href:!0});var ozr=s(rj);F9o=r(ozr,"DeiTForImageClassification"),ozr.forEach(t),C9o=r(t0," or "),tj=n(t0,"A",{href:!0});var rzr=s(tj);M9o=r(rzr,"DeiTForImageClassificationWithTeacher"),rzr.forEach(t),E9o=r(t0," (DeiT model)"),t0.forEach(t),y9o=i(jo),o2=n(jo,"LI",{});var F4e=s(o2);Jae=n(F4e,"STRONG",{});var tzr=s(Jae);w9o=r(tzr,"imagegpt"),tzr.forEach(t),A9o=r(F4e," \u2014 "),aj=n(F4e,"A",{href:!0});var azr=s(aj);L9o=r(azr,"ImageGPTForImageClassification"),azr.forEach(t),B9o=r(F4e," (ImageGPT model)"),F4e.forEach(t),x9o=i(jo),ta=n(jo,"LI",{});var mf=s(ta);Yae=n(mf,"STRONG",{});var nzr=s(Yae);k9o=r(nzr,"perceiver"),nzr.forEach(t),R9o=r(mf," \u2014 "),nj=n(mf,"A",{href:!0});var szr=s(nj);S9o=r(szr,"PerceiverForImageClassificationLearned"),szr.forEach(t),P9o=r(mf," or "),sj=n(mf,"A",{href:!0});var lzr=s(sj);$9o=r(lzr,"PerceiverForImageClassificationFourier"),lzr.forEach(t),I9o=r(mf," or "),lj=n(mf,"A",{href:!0});var izr=s(lj);j9o=r(izr,"PerceiverForImageClassificationConvProcessing"),izr.forEach(t),N9o=r(mf," (Perceiver model)"),mf.forEach(t),D9o=i(jo),r2=n(jo,"LI",{});var C4e=s(r2);Kae=n(C4e,"STRONG",{});var dzr=s(Kae);q9o=r(dzr,"segformer"),dzr.forEach(t),G9o=r(C4e," \u2014 "),ij=n(C4e,"A",{href:!0});var czr=s(ij);O9o=r(czr,"SegformerForImageClassification"),czr.forEach(t),X9o=r(C4e," (SegFormer model)"),C4e.forEach(t),z9o=i(jo),t2=n(jo,"LI",{});var M4e=s(t2);Zae=n(M4e,"STRONG",{});var fzr=s(Zae);V9o=r(fzr,"swin"),fzr.forEach(t),W9o=r(M4e," \u2014 "),dj=n(M4e,"A",{href:!0});var mzr=s(dj);Q9o=r(mzr,"SwinForImageClassification"),mzr.forEach(t),H9o=r(M4e," (Swin model)"),M4e.forEach(t),U9o=i(jo),a2=n(jo,"LI",{});var E4e=s(a2);ene=n(E4e,"STRONG",{});var gzr=s(ene);J9o=r(gzr,"vit"),gzr.forEach(t),Y9o=r(E4e," \u2014 "),cj=n(E4e,"A",{href:!0});var hzr=s(cj);K9o=r(hzr,"ViTForImageClassification"),hzr.forEach(t),Z9o=r(E4e," (ViT model)"),E4e.forEach(t),jo.forEach(t),eBo=i(Gt),n2=n(Gt,"P",{});var y4e=s(n2);oBo=r(y4e,"The model is set in evaluation mode by default using "),one=n(y4e,"CODE",{});var pzr=s(one);rBo=r(pzr,"model.eval()"),pzr.forEach(t),tBo=r(y4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),rne=n(y4e,"CODE",{});var _zr=s(rne);aBo=r(_zr,"model.train()"),_zr.forEach(t),y4e.forEach(t),nBo=i(Gt),tne=n(Gt,"P",{});var uzr=s(tne);sBo=r(uzr,"Examples:"),uzr.forEach(t),lBo=i(Gt),m(jM.$$.fragment,Gt),Gt.forEach(t),rl.forEach(t),o0e=i(d),pd=n(d,"H2",{class:!0});var s9e=s(pd);s2=n(s9e,"A",{id:!0,class:!0,href:!0});var bzr=s(s2);ane=n(bzr,"SPAN",{});var vzr=s(ane);m(NM.$$.fragment,vzr),vzr.forEach(t),bzr.forEach(t),iBo=i(s9e),nne=n(s9e,"SPAN",{});var Tzr=s(nne);dBo=r(Tzr,"AutoModelForVision2Seq"),Tzr.forEach(t),s9e.forEach(t),r0e=i(d),rr=n(d,"DIV",{class:!0});var al=s(rr);m(DM.$$.fragment,al),cBo=i(al),_d=n(al,"P",{});var MX=s(_d);fBo=r(MX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),sne=n(MX,"CODE",{});var Fzr=s(sne);mBo=r(Fzr,"from_pretrained()"),Fzr.forEach(t),gBo=r(MX,"class method or the "),lne=n(MX,"CODE",{});var Czr=s(lne);hBo=r(Czr,"from_config()"),Czr.forEach(t),pBo=r(MX,`class
method.`),MX.forEach(t),_Bo=i(al),qM=n(al,"P",{});var l9e=s(qM);uBo=r(l9e,"This class cannot be instantiated directly using "),ine=n(l9e,"CODE",{});var Mzr=s(ine);bBo=r(Mzr,"__init__()"),Mzr.forEach(t),vBo=r(l9e," (throws an error)."),l9e.forEach(t),TBo=i(al),Hr=n(al,"DIV",{class:!0});var nl=s(Hr);m(GM.$$.fragment,nl),FBo=i(nl),dne=n(nl,"P",{});var Ezr=s(dne);CBo=r(Ezr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Ezr.forEach(t),MBo=i(nl),ud=n(nl,"P",{});var EX=s(ud);EBo=r(EX,`Note:
Loading a model from its configuration file does `),cne=n(EX,"STRONG",{});var yzr=s(cne);yBo=r(yzr,"not"),yzr.forEach(t),wBo=r(EX,` load the model weights. It only affects the
model\u2019s configuration. Use `),fne=n(EX,"CODE",{});var wzr=s(fne);ABo=r(wzr,"from_pretrained()"),wzr.forEach(t),LBo=r(EX,"to load the model weights."),EX.forEach(t),BBo=i(nl),mne=n(nl,"P",{});var Azr=s(mne);xBo=r(Azr,"Examples:"),Azr.forEach(t),kBo=i(nl),m(OM.$$.fragment,nl),nl.forEach(t),RBo=i(al),Oe=n(al,"DIV",{class:!0});var Ot=s(Oe);m(XM.$$.fragment,Ot),SBo=i(Ot),gne=n(Ot,"P",{});var Lzr=s(gne);PBo=r(Lzr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Lzr.forEach(t),$Bo=i(Ot),Qa=n(Ot,"P",{});var QF=s(Qa);IBo=r(QF,"The model class to instantiate is selected based on the "),hne=n(QF,"CODE",{});var Bzr=s(hne);jBo=r(Bzr,"model_type"),Bzr.forEach(t),NBo=r(QF,` property of the config object (either
passed as an argument or loaded from `),pne=n(QF,"CODE",{});var xzr=s(pne);DBo=r(xzr,"pretrained_model_name_or_path"),xzr.forEach(t),qBo=r(QF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_ne=n(QF,"CODE",{});var kzr=s(_ne);GBo=r(kzr,"pretrained_model_name_or_path"),kzr.forEach(t),OBo=r(QF,":"),QF.forEach(t),XBo=i(Ot),une=n(Ot,"UL",{});var Rzr=s(une);l2=n(Rzr,"LI",{});var w4e=s(l2);bne=n(w4e,"STRONG",{});var Szr=s(bne);zBo=r(Szr,"vision-encoder-decoder"),Szr.forEach(t),VBo=r(w4e," \u2014 "),fj=n(w4e,"A",{href:!0});var Pzr=s(fj);WBo=r(Pzr,"VisionEncoderDecoderModel"),Pzr.forEach(t),QBo=r(w4e," (Vision Encoder decoder model)"),w4e.forEach(t),Rzr.forEach(t),HBo=i(Ot),i2=n(Ot,"P",{});var A4e=s(i2);UBo=r(A4e,"The model is set in evaluation mode by default using "),vne=n(A4e,"CODE",{});var $zr=s(vne);JBo=r($zr,"model.eval()"),$zr.forEach(t),YBo=r(A4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tne=n(A4e,"CODE",{});var Izr=s(Tne);KBo=r(Izr,"model.train()"),Izr.forEach(t),A4e.forEach(t),ZBo=i(Ot),Fne=n(Ot,"P",{});var jzr=s(Fne);exo=r(jzr,"Examples:"),jzr.forEach(t),oxo=i(Ot),m(zM.$$.fragment,Ot),Ot.forEach(t),al.forEach(t),t0e=i(d),bd=n(d,"H2",{class:!0});var i9e=s(bd);d2=n(i9e,"A",{id:!0,class:!0,href:!0});var Nzr=s(d2);Cne=n(Nzr,"SPAN",{});var Dzr=s(Cne);m(VM.$$.fragment,Dzr),Dzr.forEach(t),Nzr.forEach(t),rxo=i(i9e),Mne=n(i9e,"SPAN",{});var qzr=s(Mne);txo=r(qzr,"AutoModelForAudioClassification"),qzr.forEach(t),i9e.forEach(t),a0e=i(d),tr=n(d,"DIV",{class:!0});var sl=s(tr);m(WM.$$.fragment,sl),axo=i(sl),vd=n(sl,"P",{});var yX=s(vd);nxo=r(yX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Ene=n(yX,"CODE",{});var Gzr=s(Ene);sxo=r(Gzr,"from_pretrained()"),Gzr.forEach(t),lxo=r(yX,"class method or the "),yne=n(yX,"CODE",{});var Ozr=s(yne);ixo=r(Ozr,"from_config()"),Ozr.forEach(t),dxo=r(yX,`class
method.`),yX.forEach(t),cxo=i(sl),QM=n(sl,"P",{});var d9e=s(QM);fxo=r(d9e,"This class cannot be instantiated directly using "),wne=n(d9e,"CODE",{});var Xzr=s(wne);mxo=r(Xzr,"__init__()"),Xzr.forEach(t),gxo=r(d9e," (throws an error)."),d9e.forEach(t),hxo=i(sl),Ur=n(sl,"DIV",{class:!0});var ll=s(Ur);m(HM.$$.fragment,ll),pxo=i(ll),Ane=n(ll,"P",{});var zzr=s(Ane);_xo=r(zzr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),zzr.forEach(t),uxo=i(ll),Td=n(ll,"P",{});var wX=s(Td);bxo=r(wX,`Note:
Loading a model from its configuration file does `),Lne=n(wX,"STRONG",{});var Vzr=s(Lne);vxo=r(Vzr,"not"),Vzr.forEach(t),Txo=r(wX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bne=n(wX,"CODE",{});var Wzr=s(Bne);Fxo=r(Wzr,"from_pretrained()"),Wzr.forEach(t),Cxo=r(wX,"to load the model weights."),wX.forEach(t),Mxo=i(ll),xne=n(ll,"P",{});var Qzr=s(xne);Exo=r(Qzr,"Examples:"),Qzr.forEach(t),yxo=i(ll),m(UM.$$.fragment,ll),ll.forEach(t),wxo=i(sl),Xe=n(sl,"DIV",{class:!0});var Xt=s(Xe);m(JM.$$.fragment,Xt),Axo=i(Xt),kne=n(Xt,"P",{});var Hzr=s(kne);Lxo=r(Hzr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),Hzr.forEach(t),Bxo=i(Xt),Ha=n(Xt,"P",{});var HF=s(Ha);xxo=r(HF,"The model class to instantiate is selected based on the "),Rne=n(HF,"CODE",{});var Uzr=s(Rne);kxo=r(Uzr,"model_type"),Uzr.forEach(t),Rxo=r(HF,` property of the config object (either
passed as an argument or loaded from `),Sne=n(HF,"CODE",{});var Jzr=s(Sne);Sxo=r(Jzr,"pretrained_model_name_or_path"),Jzr.forEach(t),Pxo=r(HF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pne=n(HF,"CODE",{});var Yzr=s(Pne);$xo=r(Yzr,"pretrained_model_name_or_path"),Yzr.forEach(t),Ixo=r(HF,":"),HF.forEach(t),jxo=i(Xt),ro=n(Xt,"UL",{});var zt=s(ro);c2=n(zt,"LI",{});var L4e=s(c2);$ne=n(L4e,"STRONG",{});var Kzr=s($ne);Nxo=r(Kzr,"hubert"),Kzr.forEach(t),Dxo=r(L4e," \u2014 "),mj=n(L4e,"A",{href:!0});var Zzr=s(mj);qxo=r(Zzr,"HubertForSequenceClassification"),Zzr.forEach(t),Gxo=r(L4e," (Hubert model)"),L4e.forEach(t),Oxo=i(zt),f2=n(zt,"LI",{});var B4e=s(f2);Ine=n(B4e,"STRONG",{});var eVr=s(Ine);Xxo=r(eVr,"sew"),eVr.forEach(t),zxo=r(B4e," \u2014 "),gj=n(B4e,"A",{href:!0});var oVr=s(gj);Vxo=r(oVr,"SEWForSequenceClassification"),oVr.forEach(t),Wxo=r(B4e," (SEW model)"),B4e.forEach(t),Qxo=i(zt),m2=n(zt,"LI",{});var x4e=s(m2);jne=n(x4e,"STRONG",{});var rVr=s(jne);Hxo=r(rVr,"sew-d"),rVr.forEach(t),Uxo=r(x4e," \u2014 "),hj=n(x4e,"A",{href:!0});var tVr=s(hj);Jxo=r(tVr,"SEWDForSequenceClassification"),tVr.forEach(t),Yxo=r(x4e," (SEW-D model)"),x4e.forEach(t),Kxo=i(zt),g2=n(zt,"LI",{});var k4e=s(g2);Nne=n(k4e,"STRONG",{});var aVr=s(Nne);Zxo=r(aVr,"unispeech"),aVr.forEach(t),eko=r(k4e," \u2014 "),pj=n(k4e,"A",{href:!0});var nVr=s(pj);oko=r(nVr,"UniSpeechForSequenceClassification"),nVr.forEach(t),rko=r(k4e," (UniSpeech model)"),k4e.forEach(t),tko=i(zt),h2=n(zt,"LI",{});var R4e=s(h2);Dne=n(R4e,"STRONG",{});var sVr=s(Dne);ako=r(sVr,"unispeech-sat"),sVr.forEach(t),nko=r(R4e," \u2014 "),_j=n(R4e,"A",{href:!0});var lVr=s(_j);sko=r(lVr,"UniSpeechSatForSequenceClassification"),lVr.forEach(t),lko=r(R4e," (UniSpeechSat model)"),R4e.forEach(t),iko=i(zt),p2=n(zt,"LI",{});var S4e=s(p2);qne=n(S4e,"STRONG",{});var iVr=s(qne);dko=r(iVr,"wav2vec2"),iVr.forEach(t),cko=r(S4e," \u2014 "),uj=n(S4e,"A",{href:!0});var dVr=s(uj);fko=r(dVr,"Wav2Vec2ForSequenceClassification"),dVr.forEach(t),mko=r(S4e," (Wav2Vec2 model)"),S4e.forEach(t),gko=i(zt),_2=n(zt,"LI",{});var P4e=s(_2);Gne=n(P4e,"STRONG",{});var cVr=s(Gne);hko=r(cVr,"wavlm"),cVr.forEach(t),pko=r(P4e," \u2014 "),bj=n(P4e,"A",{href:!0});var fVr=s(bj);_ko=r(fVr,"WavLMForSequenceClassification"),fVr.forEach(t),uko=r(P4e," (WavLM model)"),P4e.forEach(t),zt.forEach(t),bko=i(Xt),u2=n(Xt,"P",{});var $4e=s(u2);vko=r($4e,"The model is set in evaluation mode by default using "),One=n($4e,"CODE",{});var mVr=s(One);Tko=r(mVr,"model.eval()"),mVr.forEach(t),Fko=r($4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xne=n($4e,"CODE",{});var gVr=s(Xne);Cko=r(gVr,"model.train()"),gVr.forEach(t),$4e.forEach(t),Mko=i(Xt),zne=n(Xt,"P",{});var hVr=s(zne);Eko=r(hVr,"Examples:"),hVr.forEach(t),yko=i(Xt),m(YM.$$.fragment,Xt),Xt.forEach(t),sl.forEach(t),n0e=i(d),Fd=n(d,"H2",{class:!0});var c9e=s(Fd);b2=n(c9e,"A",{id:!0,class:!0,href:!0});var pVr=s(b2);Vne=n(pVr,"SPAN",{});var _Vr=s(Vne);m(KM.$$.fragment,_Vr),_Vr.forEach(t),pVr.forEach(t),wko=i(c9e),Wne=n(c9e,"SPAN",{});var uVr=s(Wne);Ako=r(uVr,"AutoModelForAudioFrameClassification"),uVr.forEach(t),c9e.forEach(t),s0e=i(d),ar=n(d,"DIV",{class:!0});var il=s(ar);m(ZM.$$.fragment,il),Lko=i(il),Cd=n(il,"P",{});var AX=s(Cd);Bko=r(AX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Qne=n(AX,"CODE",{});var bVr=s(Qne);xko=r(bVr,"from_pretrained()"),bVr.forEach(t),kko=r(AX,"class method or the "),Hne=n(AX,"CODE",{});var vVr=s(Hne);Rko=r(vVr,"from_config()"),vVr.forEach(t),Sko=r(AX,`class
method.`),AX.forEach(t),Pko=i(il),eE=n(il,"P",{});var f9e=s(eE);$ko=r(f9e,"This class cannot be instantiated directly using "),Une=n(f9e,"CODE",{});var TVr=s(Une);Iko=r(TVr,"__init__()"),TVr.forEach(t),jko=r(f9e," (throws an error)."),f9e.forEach(t),Nko=i(il),Jr=n(il,"DIV",{class:!0});var dl=s(Jr);m(oE.$$.fragment,dl),Dko=i(dl),Jne=n(dl,"P",{});var FVr=s(Jne);qko=r(FVr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),FVr.forEach(t),Gko=i(dl),Md=n(dl,"P",{});var LX=s(Md);Oko=r(LX,`Note:
Loading a model from its configuration file does `),Yne=n(LX,"STRONG",{});var CVr=s(Yne);Xko=r(CVr,"not"),CVr.forEach(t),zko=r(LX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kne=n(LX,"CODE",{});var MVr=s(Kne);Vko=r(MVr,"from_pretrained()"),MVr.forEach(t),Wko=r(LX,"to load the model weights."),LX.forEach(t),Qko=i(dl),Zne=n(dl,"P",{});var EVr=s(Zne);Hko=r(EVr,"Examples:"),EVr.forEach(t),Uko=i(dl),m(rE.$$.fragment,dl),dl.forEach(t),Jko=i(il),ze=n(il,"DIV",{class:!0});var Vt=s(ze);m(tE.$$.fragment,Vt),Yko=i(Vt),ese=n(Vt,"P",{});var yVr=s(ese);Kko=r(yVr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),yVr.forEach(t),Zko=i(Vt),Ua=n(Vt,"P",{});var UF=s(Ua);eRo=r(UF,"The model class to instantiate is selected based on the "),ose=n(UF,"CODE",{});var wVr=s(ose);oRo=r(wVr,"model_type"),wVr.forEach(t),rRo=r(UF,` property of the config object (either
passed as an argument or loaded from `),rse=n(UF,"CODE",{});var AVr=s(rse);tRo=r(AVr,"pretrained_model_name_or_path"),AVr.forEach(t),aRo=r(UF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tse=n(UF,"CODE",{});var LVr=s(tse);nRo=r(LVr,"pretrained_model_name_or_path"),LVr.forEach(t),sRo=r(UF,":"),UF.forEach(t),lRo=i(Vt),Ed=n(Vt,"UL",{});var BX=s(Ed);v2=n(BX,"LI",{});var I4e=s(v2);ase=n(I4e,"STRONG",{});var BVr=s(ase);iRo=r(BVr,"unispeech-sat"),BVr.forEach(t),dRo=r(I4e," \u2014 "),vj=n(I4e,"A",{href:!0});var xVr=s(vj);cRo=r(xVr,"UniSpeechSatForAudioFrameClassification"),xVr.forEach(t),fRo=r(I4e," (UniSpeechSat model)"),I4e.forEach(t),mRo=i(BX),T2=n(BX,"LI",{});var j4e=s(T2);nse=n(j4e,"STRONG",{});var kVr=s(nse);gRo=r(kVr,"wav2vec2"),kVr.forEach(t),hRo=r(j4e," \u2014 "),Tj=n(j4e,"A",{href:!0});var RVr=s(Tj);pRo=r(RVr,"Wav2Vec2ForAudioFrameClassification"),RVr.forEach(t),_Ro=r(j4e," (Wav2Vec2 model)"),j4e.forEach(t),uRo=i(BX),F2=n(BX,"LI",{});var N4e=s(F2);sse=n(N4e,"STRONG",{});var SVr=s(sse);bRo=r(SVr,"wavlm"),SVr.forEach(t),vRo=r(N4e," \u2014 "),Fj=n(N4e,"A",{href:!0});var PVr=s(Fj);TRo=r(PVr,"WavLMForAudioFrameClassification"),PVr.forEach(t),FRo=r(N4e," (WavLM model)"),N4e.forEach(t),BX.forEach(t),CRo=i(Vt),C2=n(Vt,"P",{});var D4e=s(C2);MRo=r(D4e,"The model is set in evaluation mode by default using "),lse=n(D4e,"CODE",{});var $Vr=s(lse);ERo=r($Vr,"model.eval()"),$Vr.forEach(t),yRo=r(D4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ise=n(D4e,"CODE",{});var IVr=s(ise);wRo=r(IVr,"model.train()"),IVr.forEach(t),D4e.forEach(t),ARo=i(Vt),dse=n(Vt,"P",{});var jVr=s(dse);LRo=r(jVr,"Examples:"),jVr.forEach(t),BRo=i(Vt),m(aE.$$.fragment,Vt),Vt.forEach(t),il.forEach(t),l0e=i(d),yd=n(d,"H2",{class:!0});var m9e=s(yd);M2=n(m9e,"A",{id:!0,class:!0,href:!0});var NVr=s(M2);cse=n(NVr,"SPAN",{});var DVr=s(cse);m(nE.$$.fragment,DVr),DVr.forEach(t),NVr.forEach(t),xRo=i(m9e),fse=n(m9e,"SPAN",{});var qVr=s(fse);kRo=r(qVr,"AutoModelForCTC"),qVr.forEach(t),m9e.forEach(t),i0e=i(d),nr=n(d,"DIV",{class:!0});var cl=s(nr);m(sE.$$.fragment,cl),RRo=i(cl),wd=n(cl,"P",{});var xX=s(wd);SRo=r(xX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),mse=n(xX,"CODE",{});var GVr=s(mse);PRo=r(GVr,"from_pretrained()"),GVr.forEach(t),$Ro=r(xX,"class method or the "),gse=n(xX,"CODE",{});var OVr=s(gse);IRo=r(OVr,"from_config()"),OVr.forEach(t),jRo=r(xX,`class
method.`),xX.forEach(t),NRo=i(cl),lE=n(cl,"P",{});var g9e=s(lE);DRo=r(g9e,"This class cannot be instantiated directly using "),hse=n(g9e,"CODE",{});var XVr=s(hse);qRo=r(XVr,"__init__()"),XVr.forEach(t),GRo=r(g9e," (throws an error)."),g9e.forEach(t),ORo=i(cl),Yr=n(cl,"DIV",{class:!0});var fl=s(Yr);m(iE.$$.fragment,fl),XRo=i(fl),pse=n(fl,"P",{});var zVr=s(pse);zRo=r(zVr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),zVr.forEach(t),VRo=i(fl),Ad=n(fl,"P",{});var kX=s(Ad);WRo=r(kX,`Note:
Loading a model from its configuration file does `),_se=n(kX,"STRONG",{});var VVr=s(_se);QRo=r(VVr,"not"),VVr.forEach(t),HRo=r(kX,` load the model weights. It only affects the
model\u2019s configuration. Use `),use=n(kX,"CODE",{});var WVr=s(use);URo=r(WVr,"from_pretrained()"),WVr.forEach(t),JRo=r(kX,"to load the model weights."),kX.forEach(t),YRo=i(fl),bse=n(fl,"P",{});var QVr=s(bse);KRo=r(QVr,"Examples:"),QVr.forEach(t),ZRo=i(fl),m(dE.$$.fragment,fl),fl.forEach(t),eSo=i(cl),Ve=n(cl,"DIV",{class:!0});var Wt=s(Ve);m(cE.$$.fragment,Wt),oSo=i(Wt),vse=n(Wt,"P",{});var HVr=s(vse);rSo=r(HVr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),HVr.forEach(t),tSo=i(Wt),Ja=n(Wt,"P",{});var JF=s(Ja);aSo=r(JF,"The model class to instantiate is selected based on the "),Tse=n(JF,"CODE",{});var UVr=s(Tse);nSo=r(UVr,"model_type"),UVr.forEach(t),sSo=r(JF,` property of the config object (either
passed as an argument or loaded from `),Fse=n(JF,"CODE",{});var JVr=s(Fse);lSo=r(JVr,"pretrained_model_name_or_path"),JVr.forEach(t),iSo=r(JF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cse=n(JF,"CODE",{});var YVr=s(Cse);dSo=r(YVr,"pretrained_model_name_or_path"),YVr.forEach(t),cSo=r(JF,":"),JF.forEach(t),fSo=i(Wt),to=n(Wt,"UL",{});var Qt=s(to);E2=n(Qt,"LI",{});var q4e=s(E2);Mse=n(q4e,"STRONG",{});var KVr=s(Mse);mSo=r(KVr,"hubert"),KVr.forEach(t),gSo=r(q4e," \u2014 "),Cj=n(q4e,"A",{href:!0});var ZVr=s(Cj);hSo=r(ZVr,"HubertForCTC"),ZVr.forEach(t),pSo=r(q4e," (Hubert model)"),q4e.forEach(t),_So=i(Qt),y2=n(Qt,"LI",{});var G4e=s(y2);Ese=n(G4e,"STRONG",{});var eWr=s(Ese);uSo=r(eWr,"sew"),eWr.forEach(t),bSo=r(G4e," \u2014 "),Mj=n(G4e,"A",{href:!0});var oWr=s(Mj);vSo=r(oWr,"SEWForCTC"),oWr.forEach(t),TSo=r(G4e," (SEW model)"),G4e.forEach(t),FSo=i(Qt),w2=n(Qt,"LI",{});var O4e=s(w2);yse=n(O4e,"STRONG",{});var rWr=s(yse);CSo=r(rWr,"sew-d"),rWr.forEach(t),MSo=r(O4e," \u2014 "),Ej=n(O4e,"A",{href:!0});var tWr=s(Ej);ESo=r(tWr,"SEWDForCTC"),tWr.forEach(t),ySo=r(O4e," (SEW-D model)"),O4e.forEach(t),wSo=i(Qt),A2=n(Qt,"LI",{});var X4e=s(A2);wse=n(X4e,"STRONG",{});var aWr=s(wse);ASo=r(aWr,"unispeech"),aWr.forEach(t),LSo=r(X4e," \u2014 "),yj=n(X4e,"A",{href:!0});var nWr=s(yj);BSo=r(nWr,"UniSpeechForCTC"),nWr.forEach(t),xSo=r(X4e," (UniSpeech model)"),X4e.forEach(t),kSo=i(Qt),L2=n(Qt,"LI",{});var z4e=s(L2);Ase=n(z4e,"STRONG",{});var sWr=s(Ase);RSo=r(sWr,"unispeech-sat"),sWr.forEach(t),SSo=r(z4e," \u2014 "),wj=n(z4e,"A",{href:!0});var lWr=s(wj);PSo=r(lWr,"UniSpeechSatForCTC"),lWr.forEach(t),$So=r(z4e," (UniSpeechSat model)"),z4e.forEach(t),ISo=i(Qt),B2=n(Qt,"LI",{});var V4e=s(B2);Lse=n(V4e,"STRONG",{});var iWr=s(Lse);jSo=r(iWr,"wav2vec2"),iWr.forEach(t),NSo=r(V4e," \u2014 "),Aj=n(V4e,"A",{href:!0});var dWr=s(Aj);DSo=r(dWr,"Wav2Vec2ForCTC"),dWr.forEach(t),qSo=r(V4e," (Wav2Vec2 model)"),V4e.forEach(t),GSo=i(Qt),x2=n(Qt,"LI",{});var W4e=s(x2);Bse=n(W4e,"STRONG",{});var cWr=s(Bse);OSo=r(cWr,"wavlm"),cWr.forEach(t),XSo=r(W4e," \u2014 "),Lj=n(W4e,"A",{href:!0});var fWr=s(Lj);zSo=r(fWr,"WavLMForCTC"),fWr.forEach(t),VSo=r(W4e," (WavLM model)"),W4e.forEach(t),Qt.forEach(t),WSo=i(Wt),k2=n(Wt,"P",{});var Q4e=s(k2);QSo=r(Q4e,"The model is set in evaluation mode by default using "),xse=n(Q4e,"CODE",{});var mWr=s(xse);HSo=r(mWr,"model.eval()"),mWr.forEach(t),USo=r(Q4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kse=n(Q4e,"CODE",{});var gWr=s(kse);JSo=r(gWr,"model.train()"),gWr.forEach(t),Q4e.forEach(t),YSo=i(Wt),Rse=n(Wt,"P",{});var hWr=s(Rse);KSo=r(hWr,"Examples:"),hWr.forEach(t),ZSo=i(Wt),m(fE.$$.fragment,Wt),Wt.forEach(t),cl.forEach(t),d0e=i(d),Ld=n(d,"H2",{class:!0});var h9e=s(Ld);R2=n(h9e,"A",{id:!0,class:!0,href:!0});var pWr=s(R2);Sse=n(pWr,"SPAN",{});var _Wr=s(Sse);m(mE.$$.fragment,_Wr),_Wr.forEach(t),pWr.forEach(t),ePo=i(h9e),Pse=n(h9e,"SPAN",{});var uWr=s(Pse);oPo=r(uWr,"AutoModelForSpeechSeq2Seq"),uWr.forEach(t),h9e.forEach(t),c0e=i(d),sr=n(d,"DIV",{class:!0});var ml=s(sr);m(gE.$$.fragment,ml),rPo=i(ml),Bd=n(ml,"P",{});var RX=s(Bd);tPo=r(RX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),$se=n(RX,"CODE",{});var bWr=s($se);aPo=r(bWr,"from_pretrained()"),bWr.forEach(t),nPo=r(RX,"class method or the "),Ise=n(RX,"CODE",{});var vWr=s(Ise);sPo=r(vWr,"from_config()"),vWr.forEach(t),lPo=r(RX,`class
method.`),RX.forEach(t),iPo=i(ml),hE=n(ml,"P",{});var p9e=s(hE);dPo=r(p9e,"This class cannot be instantiated directly using "),jse=n(p9e,"CODE",{});var TWr=s(jse);cPo=r(TWr,"__init__()"),TWr.forEach(t),fPo=r(p9e," (throws an error)."),p9e.forEach(t),mPo=i(ml),Kr=n(ml,"DIV",{class:!0});var gl=s(Kr);m(pE.$$.fragment,gl),gPo=i(gl),Nse=n(gl,"P",{});var FWr=s(Nse);hPo=r(FWr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),FWr.forEach(t),pPo=i(gl),xd=n(gl,"P",{});var SX=s(xd);_Po=r(SX,`Note:
Loading a model from its configuration file does `),Dse=n(SX,"STRONG",{});var CWr=s(Dse);uPo=r(CWr,"not"),CWr.forEach(t),bPo=r(SX,` load the model weights. It only affects the
model\u2019s configuration. Use `),qse=n(SX,"CODE",{});var MWr=s(qse);vPo=r(MWr,"from_pretrained()"),MWr.forEach(t),TPo=r(SX,"to load the model weights."),SX.forEach(t),FPo=i(gl),Gse=n(gl,"P",{});var EWr=s(Gse);CPo=r(EWr,"Examples:"),EWr.forEach(t),MPo=i(gl),m(_E.$$.fragment,gl),gl.forEach(t),EPo=i(ml),We=n(ml,"DIV",{class:!0});var Ht=s(We);m(uE.$$.fragment,Ht),yPo=i(Ht),Ose=n(Ht,"P",{});var yWr=s(Ose);wPo=r(yWr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),yWr.forEach(t),APo=i(Ht),Ya=n(Ht,"P",{});var YF=s(Ya);LPo=r(YF,"The model class to instantiate is selected based on the "),Xse=n(YF,"CODE",{});var wWr=s(Xse);BPo=r(wWr,"model_type"),wWr.forEach(t),xPo=r(YF,` property of the config object (either
passed as an argument or loaded from `),zse=n(YF,"CODE",{});var AWr=s(zse);kPo=r(AWr,"pretrained_model_name_or_path"),AWr.forEach(t),RPo=r(YF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vse=n(YF,"CODE",{});var LWr=s(Vse);SPo=r(LWr,"pretrained_model_name_or_path"),LWr.forEach(t),PPo=r(YF,":"),YF.forEach(t),$Po=i(Ht),bE=n(Ht,"UL",{});var _9e=s(bE);S2=n(_9e,"LI",{});var H4e=s(S2);Wse=n(H4e,"STRONG",{});var BWr=s(Wse);IPo=r(BWr,"speech-encoder-decoder"),BWr.forEach(t),jPo=r(H4e," \u2014 "),Bj=n(H4e,"A",{href:!0});var xWr=s(Bj);NPo=r(xWr,"SpeechEncoderDecoderModel"),xWr.forEach(t),DPo=r(H4e," (Speech Encoder decoder model)"),H4e.forEach(t),qPo=i(_9e),P2=n(_9e,"LI",{});var U4e=s(P2);Qse=n(U4e,"STRONG",{});var kWr=s(Qse);GPo=r(kWr,"speech_to_text"),kWr.forEach(t),OPo=r(U4e," \u2014 "),xj=n(U4e,"A",{href:!0});var RWr=s(xj);XPo=r(RWr,"Speech2TextForConditionalGeneration"),RWr.forEach(t),zPo=r(U4e," (Speech2Text model)"),U4e.forEach(t),_9e.forEach(t),VPo=i(Ht),$2=n(Ht,"P",{});var J4e=s($2);WPo=r(J4e,"The model is set in evaluation mode by default using "),Hse=n(J4e,"CODE",{});var SWr=s(Hse);QPo=r(SWr,"model.eval()"),SWr.forEach(t),HPo=r(J4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Use=n(J4e,"CODE",{});var PWr=s(Use);UPo=r(PWr,"model.train()"),PWr.forEach(t),J4e.forEach(t),JPo=i(Ht),Jse=n(Ht,"P",{});var $Wr=s(Jse);YPo=r($Wr,"Examples:"),$Wr.forEach(t),KPo=i(Ht),m(vE.$$.fragment,Ht),Ht.forEach(t),ml.forEach(t),f0e=i(d),kd=n(d,"H2",{class:!0});var u9e=s(kd);I2=n(u9e,"A",{id:!0,class:!0,href:!0});var IWr=s(I2);Yse=n(IWr,"SPAN",{});var jWr=s(Yse);m(TE.$$.fragment,jWr),jWr.forEach(t),IWr.forEach(t),ZPo=i(u9e),Kse=n(u9e,"SPAN",{});var NWr=s(Kse);e$o=r(NWr,"AutoModelForAudioXVector"),NWr.forEach(t),u9e.forEach(t),m0e=i(d),lr=n(d,"DIV",{class:!0});var hl=s(lr);m(FE.$$.fragment,hl),o$o=i(hl),Rd=n(hl,"P",{});var PX=s(Rd);r$o=r(PX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),Zse=n(PX,"CODE",{});var DWr=s(Zse);t$o=r(DWr,"from_pretrained()"),DWr.forEach(t),a$o=r(PX,"class method or the "),ele=n(PX,"CODE",{});var qWr=s(ele);n$o=r(qWr,"from_config()"),qWr.forEach(t),s$o=r(PX,`class
method.`),PX.forEach(t),l$o=i(hl),CE=n(hl,"P",{});var b9e=s(CE);i$o=r(b9e,"This class cannot be instantiated directly using "),ole=n(b9e,"CODE",{});var GWr=s(ole);d$o=r(GWr,"__init__()"),GWr.forEach(t),c$o=r(b9e," (throws an error)."),b9e.forEach(t),f$o=i(hl),Zr=n(hl,"DIV",{class:!0});var pl=s(Zr);m(ME.$$.fragment,pl),m$o=i(pl),rle=n(pl,"P",{});var OWr=s(rle);g$o=r(OWr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),OWr.forEach(t),h$o=i(pl),Sd=n(pl,"P",{});var $X=s(Sd);p$o=r($X,`Note:
Loading a model from its configuration file does `),tle=n($X,"STRONG",{});var XWr=s(tle);_$o=r(XWr,"not"),XWr.forEach(t),u$o=r($X,` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=n($X,"CODE",{});var zWr=s(ale);b$o=r(zWr,"from_pretrained()"),zWr.forEach(t),v$o=r($X,"to load the model weights."),$X.forEach(t),T$o=i(pl),nle=n(pl,"P",{});var VWr=s(nle);F$o=r(VWr,"Examples:"),VWr.forEach(t),C$o=i(pl),m(EE.$$.fragment,pl),pl.forEach(t),M$o=i(hl),Qe=n(hl,"DIV",{class:!0});var Ut=s(Qe);m(yE.$$.fragment,Ut),E$o=i(Ut),sle=n(Ut,"P",{});var WWr=s(sle);y$o=r(WWr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),WWr.forEach(t),w$o=i(Ut),Ka=n(Ut,"P",{});var KF=s(Ka);A$o=r(KF,"The model class to instantiate is selected based on the "),lle=n(KF,"CODE",{});var QWr=s(lle);L$o=r(QWr,"model_type"),QWr.forEach(t),B$o=r(KF,` property of the config object (either
passed as an argument or loaded from `),ile=n(KF,"CODE",{});var HWr=s(ile);x$o=r(HWr,"pretrained_model_name_or_path"),HWr.forEach(t),k$o=r(KF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=n(KF,"CODE",{});var UWr=s(dle);R$o=r(UWr,"pretrained_model_name_or_path"),UWr.forEach(t),S$o=r(KF,":"),KF.forEach(t),P$o=i(Ut),Pd=n(Ut,"UL",{});var IX=s(Pd);j2=n(IX,"LI",{});var Y4e=s(j2);cle=n(Y4e,"STRONG",{});var JWr=s(cle);$$o=r(JWr,"unispeech-sat"),JWr.forEach(t),I$o=r(Y4e," \u2014 "),kj=n(Y4e,"A",{href:!0});var YWr=s(kj);j$o=r(YWr,"UniSpeechSatForXVector"),YWr.forEach(t),N$o=r(Y4e," (UniSpeechSat model)"),Y4e.forEach(t),D$o=i(IX),N2=n(IX,"LI",{});var K4e=s(N2);fle=n(K4e,"STRONG",{});var KWr=s(fle);q$o=r(KWr,"wav2vec2"),KWr.forEach(t),G$o=r(K4e," \u2014 "),Rj=n(K4e,"A",{href:!0});var ZWr=s(Rj);O$o=r(ZWr,"Wav2Vec2ForXVector"),ZWr.forEach(t),X$o=r(K4e," (Wav2Vec2 model)"),K4e.forEach(t),z$o=i(IX),D2=n(IX,"LI",{});var Z4e=s(D2);mle=n(Z4e,"STRONG",{});var eQr=s(mle);V$o=r(eQr,"wavlm"),eQr.forEach(t),W$o=r(Z4e," \u2014 "),Sj=n(Z4e,"A",{href:!0});var oQr=s(Sj);Q$o=r(oQr,"WavLMForXVector"),oQr.forEach(t),H$o=r(Z4e," (WavLM model)"),Z4e.forEach(t),IX.forEach(t),U$o=i(Ut),q2=n(Ut,"P",{});var eMe=s(q2);J$o=r(eMe,"The model is set in evaluation mode by default using "),gle=n(eMe,"CODE",{});var rQr=s(gle);Y$o=r(rQr,"model.eval()"),rQr.forEach(t),K$o=r(eMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=n(eMe,"CODE",{});var tQr=s(hle);Z$o=r(tQr,"model.train()"),tQr.forEach(t),eMe.forEach(t),eIo=i(Ut),ple=n(Ut,"P",{});var aQr=s(ple);oIo=r(aQr,"Examples:"),aQr.forEach(t),rIo=i(Ut),m(wE.$$.fragment,Ut),Ut.forEach(t),hl.forEach(t),g0e=i(d),$d=n(d,"H2",{class:!0});var v9e=s($d);G2=n(v9e,"A",{id:!0,class:!0,href:!0});var nQr=s(G2);_le=n(nQr,"SPAN",{});var sQr=s(_le);m(AE.$$.fragment,sQr),sQr.forEach(t),nQr.forEach(t),tIo=i(v9e),ule=n(v9e,"SPAN",{});var lQr=s(ule);aIo=r(lQr,"AutoModelForObjectDetection"),lQr.forEach(t),v9e.forEach(t),h0e=i(d),ir=n(d,"DIV",{class:!0});var _l=s(ir);m(LE.$$.fragment,_l),nIo=i(_l),Id=n(_l,"P",{});var jX=s(Id);sIo=r(jX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),ble=n(jX,"CODE",{});var iQr=s(ble);lIo=r(iQr,"from_pretrained()"),iQr.forEach(t),iIo=r(jX,"class method or the "),vle=n(jX,"CODE",{});var dQr=s(vle);dIo=r(dQr,"from_config()"),dQr.forEach(t),cIo=r(jX,`class
method.`),jX.forEach(t),fIo=i(_l),BE=n(_l,"P",{});var T9e=s(BE);mIo=r(T9e,"This class cannot be instantiated directly using "),Tle=n(T9e,"CODE",{});var cQr=s(Tle);gIo=r(cQr,"__init__()"),cQr.forEach(t),hIo=r(T9e," (throws an error)."),T9e.forEach(t),pIo=i(_l),et=n(_l,"DIV",{class:!0});var ul=s(et);m(xE.$$.fragment,ul),_Io=i(ul),Fle=n(ul,"P",{});var fQr=s(Fle);uIo=r(fQr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),fQr.forEach(t),bIo=i(ul),jd=n(ul,"P",{});var NX=s(jd);vIo=r(NX,`Note:
Loading a model from its configuration file does `),Cle=n(NX,"STRONG",{});var mQr=s(Cle);TIo=r(mQr,"not"),mQr.forEach(t),FIo=r(NX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=n(NX,"CODE",{});var gQr=s(Mle);CIo=r(gQr,"from_pretrained()"),gQr.forEach(t),MIo=r(NX,"to load the model weights."),NX.forEach(t),EIo=i(ul),Ele=n(ul,"P",{});var hQr=s(Ele);yIo=r(hQr,"Examples:"),hQr.forEach(t),wIo=i(ul),m(kE.$$.fragment,ul),ul.forEach(t),AIo=i(_l),He=n(_l,"DIV",{class:!0});var Jt=s(He);m(RE.$$.fragment,Jt),LIo=i(Jt),yle=n(Jt,"P",{});var pQr=s(yle);BIo=r(pQr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),pQr.forEach(t),xIo=i(Jt),Za=n(Jt,"P",{});var ZF=s(Za);kIo=r(ZF,"The model class to instantiate is selected based on the "),wle=n(ZF,"CODE",{});var _Qr=s(wle);RIo=r(_Qr,"model_type"),_Qr.forEach(t),SIo=r(ZF,` property of the config object (either
passed as an argument or loaded from `),Ale=n(ZF,"CODE",{});var uQr=s(Ale);PIo=r(uQr,"pretrained_model_name_or_path"),uQr.forEach(t),$Io=r(ZF,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=n(ZF,"CODE",{});var bQr=s(Lle);IIo=r(bQr,"pretrained_model_name_or_path"),bQr.forEach(t),jIo=r(ZF,":"),ZF.forEach(t),NIo=i(Jt),Ble=n(Jt,"UL",{});var vQr=s(Ble);O2=n(vQr,"LI",{});var oMe=s(O2);xle=n(oMe,"STRONG",{});var TQr=s(xle);DIo=r(TQr,"detr"),TQr.forEach(t),qIo=r(oMe," \u2014 "),Pj=n(oMe,"A",{href:!0});var FQr=s(Pj);GIo=r(FQr,"DetrForObjectDetection"),FQr.forEach(t),OIo=r(oMe," (DETR model)"),oMe.forEach(t),vQr.forEach(t),XIo=i(Jt),X2=n(Jt,"P",{});var rMe=s(X2);zIo=r(rMe,"The model is set in evaluation mode by default using "),kle=n(rMe,"CODE",{});var CQr=s(kle);VIo=r(CQr,"model.eval()"),CQr.forEach(t),WIo=r(rMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rle=n(rMe,"CODE",{});var MQr=s(Rle);QIo=r(MQr,"model.train()"),MQr.forEach(t),rMe.forEach(t),HIo=i(Jt),Sle=n(Jt,"P",{});var EQr=s(Sle);UIo=r(EQr,"Examples:"),EQr.forEach(t),JIo=i(Jt),m(SE.$$.fragment,Jt),Jt.forEach(t),_l.forEach(t),p0e=i(d),Nd=n(d,"H2",{class:!0});var F9e=s(Nd);z2=n(F9e,"A",{id:!0,class:!0,href:!0});var yQr=s(z2);Ple=n(yQr,"SPAN",{});var wQr=s(Ple);m(PE.$$.fragment,wQr),wQr.forEach(t),yQr.forEach(t),YIo=i(F9e),$le=n(F9e,"SPAN",{});var AQr=s($le);KIo=r(AQr,"AutoModelForImageSegmentation"),AQr.forEach(t),F9e.forEach(t),_0e=i(d),dr=n(d,"DIV",{class:!0});var bl=s(dr);m($E.$$.fragment,bl),ZIo=i(bl),Dd=n(bl,"P",{});var DX=s(Dd);ejo=r(DX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Ile=n(DX,"CODE",{});var LQr=s(Ile);ojo=r(LQr,"from_pretrained()"),LQr.forEach(t),rjo=r(DX,"class method or the "),jle=n(DX,"CODE",{});var BQr=s(jle);tjo=r(BQr,"from_config()"),BQr.forEach(t),ajo=r(DX,`class
method.`),DX.forEach(t),njo=i(bl),IE=n(bl,"P",{});var C9e=s(IE);sjo=r(C9e,"This class cannot be instantiated directly using "),Nle=n(C9e,"CODE",{});var xQr=s(Nle);ljo=r(xQr,"__init__()"),xQr.forEach(t),ijo=r(C9e," (throws an error)."),C9e.forEach(t),djo=i(bl),ot=n(bl,"DIV",{class:!0});var vl=s(ot);m(jE.$$.fragment,vl),cjo=i(vl),Dle=n(vl,"P",{});var kQr=s(Dle);fjo=r(kQr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),kQr.forEach(t),mjo=i(vl),qd=n(vl,"P",{});var qX=s(qd);gjo=r(qX,`Note:
Loading a model from its configuration file does `),qle=n(qX,"STRONG",{});var RQr=s(qle);hjo=r(RQr,"not"),RQr.forEach(t),pjo=r(qX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gle=n(qX,"CODE",{});var SQr=s(Gle);_jo=r(SQr,"from_pretrained()"),SQr.forEach(t),ujo=r(qX,"to load the model weights."),qX.forEach(t),bjo=i(vl),Ole=n(vl,"P",{});var PQr=s(Ole);vjo=r(PQr,"Examples:"),PQr.forEach(t),Tjo=i(vl),m(NE.$$.fragment,vl),vl.forEach(t),Fjo=i(bl),Ue=n(bl,"DIV",{class:!0});var Yt=s(Ue);m(DE.$$.fragment,Yt),Cjo=i(Yt),Xle=n(Yt,"P",{});var $Qr=s(Xle);Mjo=r($Qr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),$Qr.forEach(t),Ejo=i(Yt),en=n(Yt,"P",{});var eC=s(en);yjo=r(eC,"The model class to instantiate is selected based on the "),zle=n(eC,"CODE",{});var IQr=s(zle);wjo=r(IQr,"model_type"),IQr.forEach(t),Ajo=r(eC,` property of the config object (either
passed as an argument or loaded from `),Vle=n(eC,"CODE",{});var jQr=s(Vle);Ljo=r(jQr,"pretrained_model_name_or_path"),jQr.forEach(t),Bjo=r(eC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wle=n(eC,"CODE",{});var NQr=s(Wle);xjo=r(NQr,"pretrained_model_name_or_path"),NQr.forEach(t),kjo=r(eC,":"),eC.forEach(t),Rjo=i(Yt),Qle=n(Yt,"UL",{});var DQr=s(Qle);V2=n(DQr,"LI",{});var tMe=s(V2);Hle=n(tMe,"STRONG",{});var qQr=s(Hle);Sjo=r(qQr,"detr"),qQr.forEach(t),Pjo=r(tMe," \u2014 "),$j=n(tMe,"A",{href:!0});var GQr=s($j);$jo=r(GQr,"DetrForSegmentation"),GQr.forEach(t),Ijo=r(tMe," (DETR model)"),tMe.forEach(t),DQr.forEach(t),jjo=i(Yt),W2=n(Yt,"P",{});var aMe=s(W2);Njo=r(aMe,"The model is set in evaluation mode by default using "),Ule=n(aMe,"CODE",{});var OQr=s(Ule);Djo=r(OQr,"model.eval()"),OQr.forEach(t),qjo=r(aMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jle=n(aMe,"CODE",{});var XQr=s(Jle);Gjo=r(XQr,"model.train()"),XQr.forEach(t),aMe.forEach(t),Ojo=i(Yt),Yle=n(Yt,"P",{});var zQr=s(Yle);Xjo=r(zQr,"Examples:"),zQr.forEach(t),zjo=i(Yt),m(qE.$$.fragment,Yt),Yt.forEach(t),bl.forEach(t),u0e=i(d),Gd=n(d,"H2",{class:!0});var M9e=s(Gd);Q2=n(M9e,"A",{id:!0,class:!0,href:!0});var VQr=s(Q2);Kle=n(VQr,"SPAN",{});var WQr=s(Kle);m(GE.$$.fragment,WQr),WQr.forEach(t),VQr.forEach(t),Vjo=i(M9e),Zle=n(M9e,"SPAN",{});var QQr=s(Zle);Wjo=r(QQr,"AutoModelForSemanticSegmentation"),QQr.forEach(t),M9e.forEach(t),b0e=i(d),cr=n(d,"DIV",{class:!0});var Tl=s(cr);m(OE.$$.fragment,Tl),Qjo=i(Tl),Od=n(Tl,"P",{});var GX=s(Od);Hjo=r(GX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),eie=n(GX,"CODE",{});var HQr=s(eie);Ujo=r(HQr,"from_pretrained()"),HQr.forEach(t),Jjo=r(GX,"class method or the "),oie=n(GX,"CODE",{});var UQr=s(oie);Yjo=r(UQr,"from_config()"),UQr.forEach(t),Kjo=r(GX,`class
method.`),GX.forEach(t),Zjo=i(Tl),XE=n(Tl,"P",{});var E9e=s(XE);eNo=r(E9e,"This class cannot be instantiated directly using "),rie=n(E9e,"CODE",{});var JQr=s(rie);oNo=r(JQr,"__init__()"),JQr.forEach(t),rNo=r(E9e," (throws an error)."),E9e.forEach(t),tNo=i(Tl),rt=n(Tl,"DIV",{class:!0});var Fl=s(rt);m(zE.$$.fragment,Fl),aNo=i(Fl),tie=n(Fl,"P",{});var YQr=s(tie);nNo=r(YQr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),YQr.forEach(t),sNo=i(Fl),Xd=n(Fl,"P",{});var OX=s(Xd);lNo=r(OX,`Note:
Loading a model from its configuration file does `),aie=n(OX,"STRONG",{});var KQr=s(aie);iNo=r(KQr,"not"),KQr.forEach(t),dNo=r(OX,` load the model weights. It only affects the
model\u2019s configuration. Use `),nie=n(OX,"CODE",{});var ZQr=s(nie);cNo=r(ZQr,"from_pretrained()"),ZQr.forEach(t),fNo=r(OX,"to load the model weights."),OX.forEach(t),mNo=i(Fl),sie=n(Fl,"P",{});var eHr=s(sie);gNo=r(eHr,"Examples:"),eHr.forEach(t),hNo=i(Fl),m(VE.$$.fragment,Fl),Fl.forEach(t),pNo=i(Tl),Je=n(Tl,"DIV",{class:!0});var Kt=s(Je);m(WE.$$.fragment,Kt),_No=i(Kt),lie=n(Kt,"P",{});var oHr=s(lie);uNo=r(oHr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),oHr.forEach(t),bNo=i(Kt),on=n(Kt,"P",{});var oC=s(on);vNo=r(oC,"The model class to instantiate is selected based on the "),iie=n(oC,"CODE",{});var rHr=s(iie);TNo=r(rHr,"model_type"),rHr.forEach(t),FNo=r(oC,` property of the config object (either
passed as an argument or loaded from `),die=n(oC,"CODE",{});var tHr=s(die);CNo=r(tHr,"pretrained_model_name_or_path"),tHr.forEach(t),MNo=r(oC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cie=n(oC,"CODE",{});var aHr=s(cie);ENo=r(aHr,"pretrained_model_name_or_path"),aHr.forEach(t),yNo=r(oC,":"),oC.forEach(t),wNo=i(Kt),QE=n(Kt,"UL",{});var y9e=s(QE);H2=n(y9e,"LI",{});var nMe=s(H2);fie=n(nMe,"STRONG",{});var nHr=s(fie);ANo=r(nHr,"beit"),nHr.forEach(t),LNo=r(nMe," \u2014 "),Ij=n(nMe,"A",{href:!0});var sHr=s(Ij);BNo=r(sHr,"BeitForSemanticSegmentation"),sHr.forEach(t),xNo=r(nMe," (BEiT model)"),nMe.forEach(t),kNo=i(y9e),U2=n(y9e,"LI",{});var sMe=s(U2);mie=n(sMe,"STRONG",{});var lHr=s(mie);RNo=r(lHr,"segformer"),lHr.forEach(t),SNo=r(sMe," \u2014 "),jj=n(sMe,"A",{href:!0});var iHr=s(jj);PNo=r(iHr,"SegformerForSemanticSegmentation"),iHr.forEach(t),$No=r(sMe," (SegFormer model)"),sMe.forEach(t),y9e.forEach(t),INo=i(Kt),J2=n(Kt,"P",{});var lMe=s(J2);jNo=r(lMe,"The model is set in evaluation mode by default using "),gie=n(lMe,"CODE",{});var dHr=s(gie);NNo=r(dHr,"model.eval()"),dHr.forEach(t),DNo=r(lMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hie=n(lMe,"CODE",{});var cHr=s(hie);qNo=r(cHr,"model.train()"),cHr.forEach(t),lMe.forEach(t),GNo=i(Kt),pie=n(Kt,"P",{});var fHr=s(pie);ONo=r(fHr,"Examples:"),fHr.forEach(t),XNo=i(Kt),m(HE.$$.fragment,Kt),Kt.forEach(t),Tl.forEach(t),v0e=i(d),zd=n(d,"H2",{class:!0});var w9e=s(zd);Y2=n(w9e,"A",{id:!0,class:!0,href:!0});var mHr=s(Y2);_ie=n(mHr,"SPAN",{});var gHr=s(_ie);m(UE.$$.fragment,gHr),gHr.forEach(t),mHr.forEach(t),zNo=i(w9e),uie=n(w9e,"SPAN",{});var hHr=s(uie);VNo=r(hHr,"TFAutoModel"),hHr.forEach(t),w9e.forEach(t),T0e=i(d),fr=n(d,"DIV",{class:!0});var Cl=s(fr);m(JE.$$.fragment,Cl),WNo=i(Cl),Vd=n(Cl,"P",{});var XX=s(Vd);QNo=r(XX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),bie=n(XX,"CODE",{});var pHr=s(bie);HNo=r(pHr,"from_pretrained()"),pHr.forEach(t),UNo=r(XX,"class method or the "),vie=n(XX,"CODE",{});var _Hr=s(vie);JNo=r(_Hr,"from_config()"),_Hr.forEach(t),YNo=r(XX,`class
method.`),XX.forEach(t),KNo=i(Cl),YE=n(Cl,"P",{});var A9e=s(YE);ZNo=r(A9e,"This class cannot be instantiated directly using "),Tie=n(A9e,"CODE",{});var uHr=s(Tie);eDo=r(uHr,"__init__()"),uHr.forEach(t),oDo=r(A9e," (throws an error)."),A9e.forEach(t),rDo=i(Cl),tt=n(Cl,"DIV",{class:!0});var Ml=s(tt);m(KE.$$.fragment,Ml),tDo=i(Ml),Fie=n(Ml,"P",{});var bHr=s(Fie);aDo=r(bHr,"Instantiates one of the base model classes of the library from a configuration."),bHr.forEach(t),nDo=i(Ml),Wd=n(Ml,"P",{});var zX=s(Wd);sDo=r(zX,`Note:
Loading a model from its configuration file does `),Cie=n(zX,"STRONG",{});var vHr=s(Cie);lDo=r(vHr,"not"),vHr.forEach(t),iDo=r(zX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mie=n(zX,"CODE",{});var THr=s(Mie);dDo=r(THr,"from_pretrained()"),THr.forEach(t),cDo=r(zX,"to load the model weights."),zX.forEach(t),fDo=i(Ml),Eie=n(Ml,"P",{});var FHr=s(Eie);mDo=r(FHr,"Examples:"),FHr.forEach(t),gDo=i(Ml),m(ZE.$$.fragment,Ml),Ml.forEach(t),hDo=i(Cl),fo=n(Cl,"DIV",{class:!0});var sa=s(fo);m(e3.$$.fragment,sa),pDo=i(sa),yie=n(sa,"P",{});var CHr=s(yie);_Do=r(CHr,"Instantiate one of the base model classes of the library from a pretrained model."),CHr.forEach(t),uDo=i(sa),rn=n(sa,"P",{});var rC=s(rn);bDo=r(rC,"The model class to instantiate is selected based on the "),wie=n(rC,"CODE",{});var MHr=s(wie);vDo=r(MHr,"model_type"),MHr.forEach(t),TDo=r(rC,` property of the config object (either
passed as an argument or loaded from `),Aie=n(rC,"CODE",{});var EHr=s(Aie);FDo=r(EHr,"pretrained_model_name_or_path"),EHr.forEach(t),CDo=r(rC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lie=n(rC,"CODE",{});var yHr=s(Lie);MDo=r(yHr,"pretrained_model_name_or_path"),yHr.forEach(t),EDo=r(rC,":"),rC.forEach(t),yDo=i(sa),B=n(sa,"UL",{});var x=s(B);K2=n(x,"LI",{});var iMe=s(K2);Bie=n(iMe,"STRONG",{});var wHr=s(Bie);wDo=r(wHr,"albert"),wHr.forEach(t),ADo=r(iMe," \u2014 "),Nj=n(iMe,"A",{href:!0});var AHr=s(Nj);LDo=r(AHr,"TFAlbertModel"),AHr.forEach(t),BDo=r(iMe," (ALBERT model)"),iMe.forEach(t),xDo=i(x),Z2=n(x,"LI",{});var dMe=s(Z2);xie=n(dMe,"STRONG",{});var LHr=s(xie);kDo=r(LHr,"bart"),LHr.forEach(t),RDo=r(dMe," \u2014 "),Dj=n(dMe,"A",{href:!0});var BHr=s(Dj);SDo=r(BHr,"TFBartModel"),BHr.forEach(t),PDo=r(dMe," (BART model)"),dMe.forEach(t),$Do=i(x),ev=n(x,"LI",{});var cMe=s(ev);kie=n(cMe,"STRONG",{});var xHr=s(kie);IDo=r(xHr,"bert"),xHr.forEach(t),jDo=r(cMe," \u2014 "),qj=n(cMe,"A",{href:!0});var kHr=s(qj);NDo=r(kHr,"TFBertModel"),kHr.forEach(t),DDo=r(cMe," (BERT model)"),cMe.forEach(t),qDo=i(x),ov=n(x,"LI",{});var fMe=s(ov);Rie=n(fMe,"STRONG",{});var RHr=s(Rie);GDo=r(RHr,"blenderbot"),RHr.forEach(t),ODo=r(fMe," \u2014 "),Gj=n(fMe,"A",{href:!0});var SHr=s(Gj);XDo=r(SHr,"TFBlenderbotModel"),SHr.forEach(t),zDo=r(fMe," (Blenderbot model)"),fMe.forEach(t),VDo=i(x),rv=n(x,"LI",{});var mMe=s(rv);Sie=n(mMe,"STRONG",{});var PHr=s(Sie);WDo=r(PHr,"blenderbot-small"),PHr.forEach(t),QDo=r(mMe," \u2014 "),Oj=n(mMe,"A",{href:!0});var $Hr=s(Oj);HDo=r($Hr,"TFBlenderbotSmallModel"),$Hr.forEach(t),UDo=r(mMe," (BlenderbotSmall model)"),mMe.forEach(t),JDo=i(x),tv=n(x,"LI",{});var gMe=s(tv);Pie=n(gMe,"STRONG",{});var IHr=s(Pie);YDo=r(IHr,"camembert"),IHr.forEach(t),KDo=r(gMe," \u2014 "),Xj=n(gMe,"A",{href:!0});var jHr=s(Xj);ZDo=r(jHr,"TFCamembertModel"),jHr.forEach(t),eqo=r(gMe," (CamemBERT model)"),gMe.forEach(t),oqo=i(x),av=n(x,"LI",{});var hMe=s(av);$ie=n(hMe,"STRONG",{});var NHr=s($ie);rqo=r(NHr,"clip"),NHr.forEach(t),tqo=r(hMe," \u2014 "),zj=n(hMe,"A",{href:!0});var DHr=s(zj);aqo=r(DHr,"TFCLIPModel"),DHr.forEach(t),nqo=r(hMe," (CLIP model)"),hMe.forEach(t),sqo=i(x),nv=n(x,"LI",{});var pMe=s(nv);Iie=n(pMe,"STRONG",{});var qHr=s(Iie);lqo=r(qHr,"convbert"),qHr.forEach(t),iqo=r(pMe," \u2014 "),Vj=n(pMe,"A",{href:!0});var GHr=s(Vj);dqo=r(GHr,"TFConvBertModel"),GHr.forEach(t),cqo=r(pMe," (ConvBERT model)"),pMe.forEach(t),fqo=i(x),sv=n(x,"LI",{});var _Me=s(sv);jie=n(_Me,"STRONG",{});var OHr=s(jie);mqo=r(OHr,"ctrl"),OHr.forEach(t),gqo=r(_Me," \u2014 "),Wj=n(_Me,"A",{href:!0});var XHr=s(Wj);hqo=r(XHr,"TFCTRLModel"),XHr.forEach(t),pqo=r(_Me," (CTRL model)"),_Me.forEach(t),_qo=i(x),lv=n(x,"LI",{});var uMe=s(lv);Nie=n(uMe,"STRONG",{});var zHr=s(Nie);uqo=r(zHr,"deberta"),zHr.forEach(t),bqo=r(uMe," \u2014 "),Qj=n(uMe,"A",{href:!0});var VHr=s(Qj);vqo=r(VHr,"TFDebertaModel"),VHr.forEach(t),Tqo=r(uMe," (DeBERTa model)"),uMe.forEach(t),Fqo=i(x),iv=n(x,"LI",{});var bMe=s(iv);Die=n(bMe,"STRONG",{});var WHr=s(Die);Cqo=r(WHr,"deberta-v2"),WHr.forEach(t),Mqo=r(bMe," \u2014 "),Hj=n(bMe,"A",{href:!0});var QHr=s(Hj);Eqo=r(QHr,"TFDebertaV2Model"),QHr.forEach(t),yqo=r(bMe," (DeBERTa-v2 model)"),bMe.forEach(t),wqo=i(x),dv=n(x,"LI",{});var vMe=s(dv);qie=n(vMe,"STRONG",{});var HHr=s(qie);Aqo=r(HHr,"distilbert"),HHr.forEach(t),Lqo=r(vMe," \u2014 "),Uj=n(vMe,"A",{href:!0});var UHr=s(Uj);Bqo=r(UHr,"TFDistilBertModel"),UHr.forEach(t),xqo=r(vMe," (DistilBERT model)"),vMe.forEach(t),kqo=i(x),cv=n(x,"LI",{});var TMe=s(cv);Gie=n(TMe,"STRONG",{});var JHr=s(Gie);Rqo=r(JHr,"dpr"),JHr.forEach(t),Sqo=r(TMe," \u2014 "),Jj=n(TMe,"A",{href:!0});var YHr=s(Jj);Pqo=r(YHr,"TFDPRQuestionEncoder"),YHr.forEach(t),$qo=r(TMe," (DPR model)"),TMe.forEach(t),Iqo=i(x),fv=n(x,"LI",{});var FMe=s(fv);Oie=n(FMe,"STRONG",{});var KHr=s(Oie);jqo=r(KHr,"electra"),KHr.forEach(t),Nqo=r(FMe," \u2014 "),Yj=n(FMe,"A",{href:!0});var ZHr=s(Yj);Dqo=r(ZHr,"TFElectraModel"),ZHr.forEach(t),qqo=r(FMe," (ELECTRA model)"),FMe.forEach(t),Gqo=i(x),mv=n(x,"LI",{});var CMe=s(mv);Xie=n(CMe,"STRONG",{});var eUr=s(Xie);Oqo=r(eUr,"flaubert"),eUr.forEach(t),Xqo=r(CMe," \u2014 "),Kj=n(CMe,"A",{href:!0});var oUr=s(Kj);zqo=r(oUr,"TFFlaubertModel"),oUr.forEach(t),Vqo=r(CMe," (FlauBERT model)"),CMe.forEach(t),Wqo=i(x),Ls=n(x,"LI",{});var a0=s(Ls);zie=n(a0,"STRONG",{});var rUr=s(zie);Qqo=r(rUr,"funnel"),rUr.forEach(t),Hqo=r(a0," \u2014 "),Zj=n(a0,"A",{href:!0});var tUr=s(Zj);Uqo=r(tUr,"TFFunnelModel"),tUr.forEach(t),Jqo=r(a0," or "),eN=n(a0,"A",{href:!0});var aUr=s(eN);Yqo=r(aUr,"TFFunnelBaseModel"),aUr.forEach(t),Kqo=r(a0," (Funnel Transformer model)"),a0.forEach(t),Zqo=i(x),gv=n(x,"LI",{});var MMe=s(gv);Vie=n(MMe,"STRONG",{});var nUr=s(Vie);eGo=r(nUr,"gpt2"),nUr.forEach(t),oGo=r(MMe," \u2014 "),oN=n(MMe,"A",{href:!0});var sUr=s(oN);rGo=r(sUr,"TFGPT2Model"),sUr.forEach(t),tGo=r(MMe," (OpenAI GPT-2 model)"),MMe.forEach(t),aGo=i(x),hv=n(x,"LI",{});var EMe=s(hv);Wie=n(EMe,"STRONG",{});var lUr=s(Wie);nGo=r(lUr,"hubert"),lUr.forEach(t),sGo=r(EMe," \u2014 "),rN=n(EMe,"A",{href:!0});var iUr=s(rN);lGo=r(iUr,"TFHubertModel"),iUr.forEach(t),iGo=r(EMe," (Hubert model)"),EMe.forEach(t),dGo=i(x),pv=n(x,"LI",{});var yMe=s(pv);Qie=n(yMe,"STRONG",{});var dUr=s(Qie);cGo=r(dUr,"layoutlm"),dUr.forEach(t),fGo=r(yMe," \u2014 "),tN=n(yMe,"A",{href:!0});var cUr=s(tN);mGo=r(cUr,"TFLayoutLMModel"),cUr.forEach(t),gGo=r(yMe," (LayoutLM model)"),yMe.forEach(t),hGo=i(x),_v=n(x,"LI",{});var wMe=s(_v);Hie=n(wMe,"STRONG",{});var fUr=s(Hie);pGo=r(fUr,"led"),fUr.forEach(t),_Go=r(wMe," \u2014 "),aN=n(wMe,"A",{href:!0});var mUr=s(aN);uGo=r(mUr,"TFLEDModel"),mUr.forEach(t),bGo=r(wMe," (LED model)"),wMe.forEach(t),vGo=i(x),uv=n(x,"LI",{});var AMe=s(uv);Uie=n(AMe,"STRONG",{});var gUr=s(Uie);TGo=r(gUr,"longformer"),gUr.forEach(t),FGo=r(AMe," \u2014 "),nN=n(AMe,"A",{href:!0});var hUr=s(nN);CGo=r(hUr,"TFLongformerModel"),hUr.forEach(t),MGo=r(AMe," (Longformer model)"),AMe.forEach(t),EGo=i(x),bv=n(x,"LI",{});var LMe=s(bv);Jie=n(LMe,"STRONG",{});var pUr=s(Jie);yGo=r(pUr,"lxmert"),pUr.forEach(t),wGo=r(LMe," \u2014 "),sN=n(LMe,"A",{href:!0});var _Ur=s(sN);AGo=r(_Ur,"TFLxmertModel"),_Ur.forEach(t),LGo=r(LMe," (LXMERT model)"),LMe.forEach(t),BGo=i(x),vv=n(x,"LI",{});var BMe=s(vv);Yie=n(BMe,"STRONG",{});var uUr=s(Yie);xGo=r(uUr,"marian"),uUr.forEach(t),kGo=r(BMe," \u2014 "),lN=n(BMe,"A",{href:!0});var bUr=s(lN);RGo=r(bUr,"TFMarianModel"),bUr.forEach(t),SGo=r(BMe," (Marian model)"),BMe.forEach(t),PGo=i(x),Tv=n(x,"LI",{});var xMe=s(Tv);Kie=n(xMe,"STRONG",{});var vUr=s(Kie);$Go=r(vUr,"mbart"),vUr.forEach(t),IGo=r(xMe," \u2014 "),iN=n(xMe,"A",{href:!0});var TUr=s(iN);jGo=r(TUr,"TFMBartModel"),TUr.forEach(t),NGo=r(xMe," (mBART model)"),xMe.forEach(t),DGo=i(x),Fv=n(x,"LI",{});var kMe=s(Fv);Zie=n(kMe,"STRONG",{});var FUr=s(Zie);qGo=r(FUr,"mobilebert"),FUr.forEach(t),GGo=r(kMe," \u2014 "),dN=n(kMe,"A",{href:!0});var CUr=s(dN);OGo=r(CUr,"TFMobileBertModel"),CUr.forEach(t),XGo=r(kMe," (MobileBERT model)"),kMe.forEach(t),zGo=i(x),Cv=n(x,"LI",{});var RMe=s(Cv);ede=n(RMe,"STRONG",{});var MUr=s(ede);VGo=r(MUr,"mpnet"),MUr.forEach(t),WGo=r(RMe," \u2014 "),cN=n(RMe,"A",{href:!0});var EUr=s(cN);QGo=r(EUr,"TFMPNetModel"),EUr.forEach(t),HGo=r(RMe," (MPNet model)"),RMe.forEach(t),UGo=i(x),Mv=n(x,"LI",{});var SMe=s(Mv);ode=n(SMe,"STRONG",{});var yUr=s(ode);JGo=r(yUr,"mt5"),yUr.forEach(t),YGo=r(SMe," \u2014 "),fN=n(SMe,"A",{href:!0});var wUr=s(fN);KGo=r(wUr,"TFMT5Model"),wUr.forEach(t),ZGo=r(SMe," (mT5 model)"),SMe.forEach(t),eOo=i(x),Ev=n(x,"LI",{});var PMe=s(Ev);rde=n(PMe,"STRONG",{});var AUr=s(rde);oOo=r(AUr,"openai-gpt"),AUr.forEach(t),rOo=r(PMe," \u2014 "),mN=n(PMe,"A",{href:!0});var LUr=s(mN);tOo=r(LUr,"TFOpenAIGPTModel"),LUr.forEach(t),aOo=r(PMe," (OpenAI GPT model)"),PMe.forEach(t),nOo=i(x),yv=n(x,"LI",{});var $Me=s(yv);tde=n($Me,"STRONG",{});var BUr=s(tde);sOo=r(BUr,"pegasus"),BUr.forEach(t),lOo=r($Me," \u2014 "),gN=n($Me,"A",{href:!0});var xUr=s(gN);iOo=r(xUr,"TFPegasusModel"),xUr.forEach(t),dOo=r($Me," (Pegasus model)"),$Me.forEach(t),cOo=i(x),wv=n(x,"LI",{});var IMe=s(wv);ade=n(IMe,"STRONG",{});var kUr=s(ade);fOo=r(kUr,"rembert"),kUr.forEach(t),mOo=r(IMe," \u2014 "),hN=n(IMe,"A",{href:!0});var RUr=s(hN);gOo=r(RUr,"TFRemBertModel"),RUr.forEach(t),hOo=r(IMe," (RemBERT model)"),IMe.forEach(t),pOo=i(x),Av=n(x,"LI",{});var jMe=s(Av);nde=n(jMe,"STRONG",{});var SUr=s(nde);_Oo=r(SUr,"roberta"),SUr.forEach(t),uOo=r(jMe," \u2014 "),pN=n(jMe,"A",{href:!0});var PUr=s(pN);bOo=r(PUr,"TFRobertaModel"),PUr.forEach(t),vOo=r(jMe," (RoBERTa model)"),jMe.forEach(t),TOo=i(x),Lv=n(x,"LI",{});var NMe=s(Lv);sde=n(NMe,"STRONG",{});var $Ur=s(sde);FOo=r($Ur,"roformer"),$Ur.forEach(t),COo=r(NMe," \u2014 "),_N=n(NMe,"A",{href:!0});var IUr=s(_N);MOo=r(IUr,"TFRoFormerModel"),IUr.forEach(t),EOo=r(NMe," (RoFormer model)"),NMe.forEach(t),yOo=i(x),Bv=n(x,"LI",{});var DMe=s(Bv);lde=n(DMe,"STRONG",{});var jUr=s(lde);wOo=r(jUr,"speech_to_text"),jUr.forEach(t),AOo=r(DMe," \u2014 "),uN=n(DMe,"A",{href:!0});var NUr=s(uN);LOo=r(NUr,"TFSpeech2TextModel"),NUr.forEach(t),BOo=r(DMe," (Speech2Text model)"),DMe.forEach(t),xOo=i(x),xv=n(x,"LI",{});var qMe=s(xv);ide=n(qMe,"STRONG",{});var DUr=s(ide);kOo=r(DUr,"t5"),DUr.forEach(t),ROo=r(qMe," \u2014 "),bN=n(qMe,"A",{href:!0});var qUr=s(bN);SOo=r(qUr,"TFT5Model"),qUr.forEach(t),POo=r(qMe," (T5 model)"),qMe.forEach(t),$Oo=i(x),kv=n(x,"LI",{});var GMe=s(kv);dde=n(GMe,"STRONG",{});var GUr=s(dde);IOo=r(GUr,"tapas"),GUr.forEach(t),jOo=r(GMe," \u2014 "),vN=n(GMe,"A",{href:!0});var OUr=s(vN);NOo=r(OUr,"TFTapasModel"),OUr.forEach(t),DOo=r(GMe," (TAPAS model)"),GMe.forEach(t),qOo=i(x),Rv=n(x,"LI",{});var OMe=s(Rv);cde=n(OMe,"STRONG",{});var XUr=s(cde);GOo=r(XUr,"transfo-xl"),XUr.forEach(t),OOo=r(OMe," \u2014 "),TN=n(OMe,"A",{href:!0});var zUr=s(TN);XOo=r(zUr,"TFTransfoXLModel"),zUr.forEach(t),zOo=r(OMe," (Transformer-XL model)"),OMe.forEach(t),VOo=i(x),Sv=n(x,"LI",{});var XMe=s(Sv);fde=n(XMe,"STRONG",{});var VUr=s(fde);WOo=r(VUr,"vit"),VUr.forEach(t),QOo=r(XMe," \u2014 "),FN=n(XMe,"A",{href:!0});var WUr=s(FN);HOo=r(WUr,"TFViTModel"),WUr.forEach(t),UOo=r(XMe," (ViT model)"),XMe.forEach(t),JOo=i(x),Pv=n(x,"LI",{});var zMe=s(Pv);mde=n(zMe,"STRONG",{});var QUr=s(mde);YOo=r(QUr,"wav2vec2"),QUr.forEach(t),KOo=r(zMe," \u2014 "),CN=n(zMe,"A",{href:!0});var HUr=s(CN);ZOo=r(HUr,"TFWav2Vec2Model"),HUr.forEach(t),eXo=r(zMe," (Wav2Vec2 model)"),zMe.forEach(t),oXo=i(x),$v=n(x,"LI",{});var VMe=s($v);gde=n(VMe,"STRONG",{});var UUr=s(gde);rXo=r(UUr,"xlm"),UUr.forEach(t),tXo=r(VMe," \u2014 "),MN=n(VMe,"A",{href:!0});var JUr=s(MN);aXo=r(JUr,"TFXLMModel"),JUr.forEach(t),nXo=r(VMe," (XLM model)"),VMe.forEach(t),sXo=i(x),Iv=n(x,"LI",{});var WMe=s(Iv);hde=n(WMe,"STRONG",{});var YUr=s(hde);lXo=r(YUr,"xlm-roberta"),YUr.forEach(t),iXo=r(WMe," \u2014 "),EN=n(WMe,"A",{href:!0});var KUr=s(EN);dXo=r(KUr,"TFXLMRobertaModel"),KUr.forEach(t),cXo=r(WMe," (XLM-RoBERTa model)"),WMe.forEach(t),fXo=i(x),jv=n(x,"LI",{});var QMe=s(jv);pde=n(QMe,"STRONG",{});var ZUr=s(pde);mXo=r(ZUr,"xlnet"),ZUr.forEach(t),gXo=r(QMe," \u2014 "),yN=n(QMe,"A",{href:!0});var eJr=s(yN);hXo=r(eJr,"TFXLNetModel"),eJr.forEach(t),pXo=r(QMe," (XLNet model)"),QMe.forEach(t),x.forEach(t),_Xo=i(sa),_de=n(sa,"P",{});var oJr=s(_de);uXo=r(oJr,"Examples:"),oJr.forEach(t),bXo=i(sa),m(o3.$$.fragment,sa),sa.forEach(t),Cl.forEach(t),F0e=i(d),Qd=n(d,"H2",{class:!0});var L9e=s(Qd);Nv=n(L9e,"A",{id:!0,class:!0,href:!0});var rJr=s(Nv);ude=n(rJr,"SPAN",{});var tJr=s(ude);m(r3.$$.fragment,tJr),tJr.forEach(t),rJr.forEach(t),vXo=i(L9e),bde=n(L9e,"SPAN",{});var aJr=s(bde);TXo=r(aJr,"TFAutoModelForPreTraining"),aJr.forEach(t),L9e.forEach(t),C0e=i(d),mr=n(d,"DIV",{class:!0});var El=s(mr);m(t3.$$.fragment,El),FXo=i(El),Hd=n(El,"P",{});var VX=s(Hd);CXo=r(VX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),vde=n(VX,"CODE",{});var nJr=s(vde);MXo=r(nJr,"from_pretrained()"),nJr.forEach(t),EXo=r(VX,"class method or the "),Tde=n(VX,"CODE",{});var sJr=s(Tde);yXo=r(sJr,"from_config()"),sJr.forEach(t),wXo=r(VX,`class
method.`),VX.forEach(t),AXo=i(El),a3=n(El,"P",{});var B9e=s(a3);LXo=r(B9e,"This class cannot be instantiated directly using "),Fde=n(B9e,"CODE",{});var lJr=s(Fde);BXo=r(lJr,"__init__()"),lJr.forEach(t),xXo=r(B9e," (throws an error)."),B9e.forEach(t),kXo=i(El),at=n(El,"DIV",{class:!0});var yl=s(at);m(n3.$$.fragment,yl),RXo=i(yl),Cde=n(yl,"P",{});var iJr=s(Cde);SXo=r(iJr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),iJr.forEach(t),PXo=i(yl),Ud=n(yl,"P",{});var WX=s(Ud);$Xo=r(WX,`Note:
Loading a model from its configuration file does `),Mde=n(WX,"STRONG",{});var dJr=s(Mde);IXo=r(dJr,"not"),dJr.forEach(t),jXo=r(WX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ede=n(WX,"CODE",{});var cJr=s(Ede);NXo=r(cJr,"from_pretrained()"),cJr.forEach(t),DXo=r(WX,"to load the model weights."),WX.forEach(t),qXo=i(yl),yde=n(yl,"P",{});var fJr=s(yde);GXo=r(fJr,"Examples:"),fJr.forEach(t),OXo=i(yl),m(s3.$$.fragment,yl),yl.forEach(t),XXo=i(El),mo=n(El,"DIV",{class:!0});var la=s(mo);m(l3.$$.fragment,la),zXo=i(la),wde=n(la,"P",{});var mJr=s(wde);VXo=r(mJr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),mJr.forEach(t),WXo=i(la),tn=n(la,"P",{});var tC=s(tn);QXo=r(tC,"The model class to instantiate is selected based on the "),Ade=n(tC,"CODE",{});var gJr=s(Ade);HXo=r(gJr,"model_type"),gJr.forEach(t),UXo=r(tC,` property of the config object (either
passed as an argument or loaded from `),Lde=n(tC,"CODE",{});var hJr=s(Lde);JXo=r(hJr,"pretrained_model_name_or_path"),hJr.forEach(t),YXo=r(tC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bde=n(tC,"CODE",{});var pJr=s(Bde);KXo=r(pJr,"pretrained_model_name_or_path"),pJr.forEach(t),ZXo=r(tC,":"),tC.forEach(t),ezo=i(la),H=n(la,"UL",{});var U=s(H);Dv=n(U,"LI",{});var HMe=s(Dv);xde=n(HMe,"STRONG",{});var _Jr=s(xde);ozo=r(_Jr,"albert"),_Jr.forEach(t),rzo=r(HMe," \u2014 "),wN=n(HMe,"A",{href:!0});var uJr=s(wN);tzo=r(uJr,"TFAlbertForPreTraining"),uJr.forEach(t),azo=r(HMe," (ALBERT model)"),HMe.forEach(t),nzo=i(U),qv=n(U,"LI",{});var UMe=s(qv);kde=n(UMe,"STRONG",{});var bJr=s(kde);szo=r(bJr,"bart"),bJr.forEach(t),lzo=r(UMe," \u2014 "),AN=n(UMe,"A",{href:!0});var vJr=s(AN);izo=r(vJr,"TFBartForConditionalGeneration"),vJr.forEach(t),dzo=r(UMe," (BART model)"),UMe.forEach(t),czo=i(U),Gv=n(U,"LI",{});var JMe=s(Gv);Rde=n(JMe,"STRONG",{});var TJr=s(Rde);fzo=r(TJr,"bert"),TJr.forEach(t),mzo=r(JMe," \u2014 "),LN=n(JMe,"A",{href:!0});var FJr=s(LN);gzo=r(FJr,"TFBertForPreTraining"),FJr.forEach(t),hzo=r(JMe," (BERT model)"),JMe.forEach(t),pzo=i(U),Ov=n(U,"LI",{});var YMe=s(Ov);Sde=n(YMe,"STRONG",{});var CJr=s(Sde);_zo=r(CJr,"camembert"),CJr.forEach(t),uzo=r(YMe," \u2014 "),BN=n(YMe,"A",{href:!0});var MJr=s(BN);bzo=r(MJr,"TFCamembertForMaskedLM"),MJr.forEach(t),vzo=r(YMe," (CamemBERT model)"),YMe.forEach(t),Tzo=i(U),Xv=n(U,"LI",{});var KMe=s(Xv);Pde=n(KMe,"STRONG",{});var EJr=s(Pde);Fzo=r(EJr,"ctrl"),EJr.forEach(t),Czo=r(KMe," \u2014 "),xN=n(KMe,"A",{href:!0});var yJr=s(xN);Mzo=r(yJr,"TFCTRLLMHeadModel"),yJr.forEach(t),Ezo=r(KMe," (CTRL model)"),KMe.forEach(t),yzo=i(U),zv=n(U,"LI",{});var ZMe=s(zv);$de=n(ZMe,"STRONG",{});var wJr=s($de);wzo=r(wJr,"distilbert"),wJr.forEach(t),Azo=r(ZMe," \u2014 "),kN=n(ZMe,"A",{href:!0});var AJr=s(kN);Lzo=r(AJr,"TFDistilBertForMaskedLM"),AJr.forEach(t),Bzo=r(ZMe," (DistilBERT model)"),ZMe.forEach(t),xzo=i(U),Vv=n(U,"LI",{});var eEe=s(Vv);Ide=n(eEe,"STRONG",{});var LJr=s(Ide);kzo=r(LJr,"electra"),LJr.forEach(t),Rzo=r(eEe," \u2014 "),RN=n(eEe,"A",{href:!0});var BJr=s(RN);Szo=r(BJr,"TFElectraForPreTraining"),BJr.forEach(t),Pzo=r(eEe," (ELECTRA model)"),eEe.forEach(t),$zo=i(U),Wv=n(U,"LI",{});var oEe=s(Wv);jde=n(oEe,"STRONG",{});var xJr=s(jde);Izo=r(xJr,"flaubert"),xJr.forEach(t),jzo=r(oEe," \u2014 "),SN=n(oEe,"A",{href:!0});var kJr=s(SN);Nzo=r(kJr,"TFFlaubertWithLMHeadModel"),kJr.forEach(t),Dzo=r(oEe," (FlauBERT model)"),oEe.forEach(t),qzo=i(U),Qv=n(U,"LI",{});var rEe=s(Qv);Nde=n(rEe,"STRONG",{});var RJr=s(Nde);Gzo=r(RJr,"funnel"),RJr.forEach(t),Ozo=r(rEe," \u2014 "),PN=n(rEe,"A",{href:!0});var SJr=s(PN);Xzo=r(SJr,"TFFunnelForPreTraining"),SJr.forEach(t),zzo=r(rEe," (Funnel Transformer model)"),rEe.forEach(t),Vzo=i(U),Hv=n(U,"LI",{});var tEe=s(Hv);Dde=n(tEe,"STRONG",{});var PJr=s(Dde);Wzo=r(PJr,"gpt2"),PJr.forEach(t),Qzo=r(tEe," \u2014 "),$N=n(tEe,"A",{href:!0});var $Jr=s($N);Hzo=r($Jr,"TFGPT2LMHeadModel"),$Jr.forEach(t),Uzo=r(tEe," (OpenAI GPT-2 model)"),tEe.forEach(t),Jzo=i(U),Uv=n(U,"LI",{});var aEe=s(Uv);qde=n(aEe,"STRONG",{});var IJr=s(qde);Yzo=r(IJr,"layoutlm"),IJr.forEach(t),Kzo=r(aEe," \u2014 "),IN=n(aEe,"A",{href:!0});var jJr=s(IN);Zzo=r(jJr,"TFLayoutLMForMaskedLM"),jJr.forEach(t),eVo=r(aEe," (LayoutLM model)"),aEe.forEach(t),oVo=i(U),Jv=n(U,"LI",{});var nEe=s(Jv);Gde=n(nEe,"STRONG",{});var NJr=s(Gde);rVo=r(NJr,"lxmert"),NJr.forEach(t),tVo=r(nEe," \u2014 "),jN=n(nEe,"A",{href:!0});var DJr=s(jN);aVo=r(DJr,"TFLxmertForPreTraining"),DJr.forEach(t),nVo=r(nEe," (LXMERT model)"),nEe.forEach(t),sVo=i(U),Yv=n(U,"LI",{});var sEe=s(Yv);Ode=n(sEe,"STRONG",{});var qJr=s(Ode);lVo=r(qJr,"mobilebert"),qJr.forEach(t),iVo=r(sEe," \u2014 "),NN=n(sEe,"A",{href:!0});var GJr=s(NN);dVo=r(GJr,"TFMobileBertForPreTraining"),GJr.forEach(t),cVo=r(sEe," (MobileBERT model)"),sEe.forEach(t),fVo=i(U),Kv=n(U,"LI",{});var lEe=s(Kv);Xde=n(lEe,"STRONG",{});var OJr=s(Xde);mVo=r(OJr,"mpnet"),OJr.forEach(t),gVo=r(lEe," \u2014 "),DN=n(lEe,"A",{href:!0});var XJr=s(DN);hVo=r(XJr,"TFMPNetForMaskedLM"),XJr.forEach(t),pVo=r(lEe," (MPNet model)"),lEe.forEach(t),_Vo=i(U),Zv=n(U,"LI",{});var iEe=s(Zv);zde=n(iEe,"STRONG",{});var zJr=s(zde);uVo=r(zJr,"openai-gpt"),zJr.forEach(t),bVo=r(iEe," \u2014 "),qN=n(iEe,"A",{href:!0});var VJr=s(qN);vVo=r(VJr,"TFOpenAIGPTLMHeadModel"),VJr.forEach(t),TVo=r(iEe," (OpenAI GPT model)"),iEe.forEach(t),FVo=i(U),e6=n(U,"LI",{});var dEe=s(e6);Vde=n(dEe,"STRONG",{});var WJr=s(Vde);CVo=r(WJr,"roberta"),WJr.forEach(t),MVo=r(dEe," \u2014 "),GN=n(dEe,"A",{href:!0});var QJr=s(GN);EVo=r(QJr,"TFRobertaForMaskedLM"),QJr.forEach(t),yVo=r(dEe," (RoBERTa model)"),dEe.forEach(t),wVo=i(U),o6=n(U,"LI",{});var cEe=s(o6);Wde=n(cEe,"STRONG",{});var HJr=s(Wde);AVo=r(HJr,"t5"),HJr.forEach(t),LVo=r(cEe," \u2014 "),ON=n(cEe,"A",{href:!0});var UJr=s(ON);BVo=r(UJr,"TFT5ForConditionalGeneration"),UJr.forEach(t),xVo=r(cEe," (T5 model)"),cEe.forEach(t),kVo=i(U),r6=n(U,"LI",{});var fEe=s(r6);Qde=n(fEe,"STRONG",{});var JJr=s(Qde);RVo=r(JJr,"tapas"),JJr.forEach(t),SVo=r(fEe," \u2014 "),XN=n(fEe,"A",{href:!0});var YJr=s(XN);PVo=r(YJr,"TFTapasForMaskedLM"),YJr.forEach(t),$Vo=r(fEe," (TAPAS model)"),fEe.forEach(t),IVo=i(U),t6=n(U,"LI",{});var mEe=s(t6);Hde=n(mEe,"STRONG",{});var KJr=s(Hde);jVo=r(KJr,"transfo-xl"),KJr.forEach(t),NVo=r(mEe," \u2014 "),zN=n(mEe,"A",{href:!0});var ZJr=s(zN);DVo=r(ZJr,"TFTransfoXLLMHeadModel"),ZJr.forEach(t),qVo=r(mEe," (Transformer-XL model)"),mEe.forEach(t),GVo=i(U),a6=n(U,"LI",{});var gEe=s(a6);Ude=n(gEe,"STRONG",{});var eYr=s(Ude);OVo=r(eYr,"xlm"),eYr.forEach(t),XVo=r(gEe," \u2014 "),VN=n(gEe,"A",{href:!0});var oYr=s(VN);zVo=r(oYr,"TFXLMWithLMHeadModel"),oYr.forEach(t),VVo=r(gEe," (XLM model)"),gEe.forEach(t),WVo=i(U),n6=n(U,"LI",{});var hEe=s(n6);Jde=n(hEe,"STRONG",{});var rYr=s(Jde);QVo=r(rYr,"xlm-roberta"),rYr.forEach(t),HVo=r(hEe," \u2014 "),WN=n(hEe,"A",{href:!0});var tYr=s(WN);UVo=r(tYr,"TFXLMRobertaForMaskedLM"),tYr.forEach(t),JVo=r(hEe," (XLM-RoBERTa model)"),hEe.forEach(t),YVo=i(U),s6=n(U,"LI",{});var pEe=s(s6);Yde=n(pEe,"STRONG",{});var aYr=s(Yde);KVo=r(aYr,"xlnet"),aYr.forEach(t),ZVo=r(pEe," \u2014 "),QN=n(pEe,"A",{href:!0});var nYr=s(QN);eWo=r(nYr,"TFXLNetLMHeadModel"),nYr.forEach(t),oWo=r(pEe," (XLNet model)"),pEe.forEach(t),U.forEach(t),rWo=i(la),Kde=n(la,"P",{});var sYr=s(Kde);tWo=r(sYr,"Examples:"),sYr.forEach(t),aWo=i(la),m(i3.$$.fragment,la),la.forEach(t),El.forEach(t),M0e=i(d),Jd=n(d,"H2",{class:!0});var x9e=s(Jd);l6=n(x9e,"A",{id:!0,class:!0,href:!0});var lYr=s(l6);Zde=n(lYr,"SPAN",{});var iYr=s(Zde);m(d3.$$.fragment,iYr),iYr.forEach(t),lYr.forEach(t),nWo=i(x9e),ece=n(x9e,"SPAN",{});var dYr=s(ece);sWo=r(dYr,"TFAutoModelForCausalLM"),dYr.forEach(t),x9e.forEach(t),E0e=i(d),gr=n(d,"DIV",{class:!0});var wl=s(gr);m(c3.$$.fragment,wl),lWo=i(wl),Yd=n(wl,"P",{});var QX=s(Yd);iWo=r(QX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),oce=n(QX,"CODE",{});var cYr=s(oce);dWo=r(cYr,"from_pretrained()"),cYr.forEach(t),cWo=r(QX,"class method or the "),rce=n(QX,"CODE",{});var fYr=s(rce);fWo=r(fYr,"from_config()"),fYr.forEach(t),mWo=r(QX,`class
method.`),QX.forEach(t),gWo=i(wl),f3=n(wl,"P",{});var k9e=s(f3);hWo=r(k9e,"This class cannot be instantiated directly using "),tce=n(k9e,"CODE",{});var mYr=s(tce);pWo=r(mYr,"__init__()"),mYr.forEach(t),_Wo=r(k9e," (throws an error)."),k9e.forEach(t),uWo=i(wl),nt=n(wl,"DIV",{class:!0});var Al=s(nt);m(m3.$$.fragment,Al),bWo=i(Al),ace=n(Al,"P",{});var gYr=s(ace);vWo=r(gYr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),gYr.forEach(t),TWo=i(Al),Kd=n(Al,"P",{});var HX=s(Kd);FWo=r(HX,`Note:
Loading a model from its configuration file does `),nce=n(HX,"STRONG",{});var hYr=s(nce);CWo=r(hYr,"not"),hYr.forEach(t),MWo=r(HX,` load the model weights. It only affects the
model\u2019s configuration. Use `),sce=n(HX,"CODE",{});var pYr=s(sce);EWo=r(pYr,"from_pretrained()"),pYr.forEach(t),yWo=r(HX,"to load the model weights."),HX.forEach(t),wWo=i(Al),lce=n(Al,"P",{});var _Yr=s(lce);AWo=r(_Yr,"Examples:"),_Yr.forEach(t),LWo=i(Al),m(g3.$$.fragment,Al),Al.forEach(t),BWo=i(wl),go=n(wl,"DIV",{class:!0});var ia=s(go);m(h3.$$.fragment,ia),xWo=i(ia),ice=n(ia,"P",{});var uYr=s(ice);kWo=r(uYr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),uYr.forEach(t),RWo=i(ia),an=n(ia,"P",{});var aC=s(an);SWo=r(aC,"The model class to instantiate is selected based on the "),dce=n(aC,"CODE",{});var bYr=s(dce);PWo=r(bYr,"model_type"),bYr.forEach(t),$Wo=r(aC,` property of the config object (either
passed as an argument or loaded from `),cce=n(aC,"CODE",{});var vYr=s(cce);IWo=r(vYr,"pretrained_model_name_or_path"),vYr.forEach(t),jWo=r(aC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fce=n(aC,"CODE",{});var TYr=s(fce);NWo=r(TYr,"pretrained_model_name_or_path"),TYr.forEach(t),DWo=r(aC,":"),aC.forEach(t),qWo=i(ia),he=n(ia,"UL",{});var Ce=s(he);i6=n(Ce,"LI",{});var _Ee=s(i6);mce=n(_Ee,"STRONG",{});var FYr=s(mce);GWo=r(FYr,"bert"),FYr.forEach(t),OWo=r(_Ee," \u2014 "),HN=n(_Ee,"A",{href:!0});var CYr=s(HN);XWo=r(CYr,"TFBertLMHeadModel"),CYr.forEach(t),zWo=r(_Ee," (BERT model)"),_Ee.forEach(t),VWo=i(Ce),d6=n(Ce,"LI",{});var uEe=s(d6);gce=n(uEe,"STRONG",{});var MYr=s(gce);WWo=r(MYr,"ctrl"),MYr.forEach(t),QWo=r(uEe," \u2014 "),UN=n(uEe,"A",{href:!0});var EYr=s(UN);HWo=r(EYr,"TFCTRLLMHeadModel"),EYr.forEach(t),UWo=r(uEe," (CTRL model)"),uEe.forEach(t),JWo=i(Ce),c6=n(Ce,"LI",{});var bEe=s(c6);hce=n(bEe,"STRONG",{});var yYr=s(hce);YWo=r(yYr,"gpt2"),yYr.forEach(t),KWo=r(bEe," \u2014 "),JN=n(bEe,"A",{href:!0});var wYr=s(JN);ZWo=r(wYr,"TFGPT2LMHeadModel"),wYr.forEach(t),eQo=r(bEe," (OpenAI GPT-2 model)"),bEe.forEach(t),oQo=i(Ce),f6=n(Ce,"LI",{});var vEe=s(f6);pce=n(vEe,"STRONG",{});var AYr=s(pce);rQo=r(AYr,"openai-gpt"),AYr.forEach(t),tQo=r(vEe," \u2014 "),YN=n(vEe,"A",{href:!0});var LYr=s(YN);aQo=r(LYr,"TFOpenAIGPTLMHeadModel"),LYr.forEach(t),nQo=r(vEe," (OpenAI GPT model)"),vEe.forEach(t),sQo=i(Ce),m6=n(Ce,"LI",{});var TEe=s(m6);_ce=n(TEe,"STRONG",{});var BYr=s(_ce);lQo=r(BYr,"rembert"),BYr.forEach(t),iQo=r(TEe," \u2014 "),KN=n(TEe,"A",{href:!0});var xYr=s(KN);dQo=r(xYr,"TFRemBertForCausalLM"),xYr.forEach(t),cQo=r(TEe," (RemBERT model)"),TEe.forEach(t),fQo=i(Ce),g6=n(Ce,"LI",{});var FEe=s(g6);uce=n(FEe,"STRONG",{});var kYr=s(uce);mQo=r(kYr,"roberta"),kYr.forEach(t),gQo=r(FEe," \u2014 "),ZN=n(FEe,"A",{href:!0});var RYr=s(ZN);hQo=r(RYr,"TFRobertaForCausalLM"),RYr.forEach(t),pQo=r(FEe," (RoBERTa model)"),FEe.forEach(t),_Qo=i(Ce),h6=n(Ce,"LI",{});var CEe=s(h6);bce=n(CEe,"STRONG",{});var SYr=s(bce);uQo=r(SYr,"roformer"),SYr.forEach(t),bQo=r(CEe," \u2014 "),eD=n(CEe,"A",{href:!0});var PYr=s(eD);vQo=r(PYr,"TFRoFormerForCausalLM"),PYr.forEach(t),TQo=r(CEe," (RoFormer model)"),CEe.forEach(t),FQo=i(Ce),p6=n(Ce,"LI",{});var MEe=s(p6);vce=n(MEe,"STRONG",{});var $Yr=s(vce);CQo=r($Yr,"transfo-xl"),$Yr.forEach(t),MQo=r(MEe," \u2014 "),oD=n(MEe,"A",{href:!0});var IYr=s(oD);EQo=r(IYr,"TFTransfoXLLMHeadModel"),IYr.forEach(t),yQo=r(MEe," (Transformer-XL model)"),MEe.forEach(t),wQo=i(Ce),_6=n(Ce,"LI",{});var EEe=s(_6);Tce=n(EEe,"STRONG",{});var jYr=s(Tce);AQo=r(jYr,"xlm"),jYr.forEach(t),LQo=r(EEe," \u2014 "),rD=n(EEe,"A",{href:!0});var NYr=s(rD);BQo=r(NYr,"TFXLMWithLMHeadModel"),NYr.forEach(t),xQo=r(EEe," (XLM model)"),EEe.forEach(t),kQo=i(Ce),u6=n(Ce,"LI",{});var yEe=s(u6);Fce=n(yEe,"STRONG",{});var DYr=s(Fce);RQo=r(DYr,"xlnet"),DYr.forEach(t),SQo=r(yEe," \u2014 "),tD=n(yEe,"A",{href:!0});var qYr=s(tD);PQo=r(qYr,"TFXLNetLMHeadModel"),qYr.forEach(t),$Qo=r(yEe," (XLNet model)"),yEe.forEach(t),Ce.forEach(t),IQo=i(ia),Cce=n(ia,"P",{});var GYr=s(Cce);jQo=r(GYr,"Examples:"),GYr.forEach(t),NQo=i(ia),m(p3.$$.fragment,ia),ia.forEach(t),wl.forEach(t),y0e=i(d),Zd=n(d,"H2",{class:!0});var R9e=s(Zd);b6=n(R9e,"A",{id:!0,class:!0,href:!0});var OYr=s(b6);Mce=n(OYr,"SPAN",{});var XYr=s(Mce);m(_3.$$.fragment,XYr),XYr.forEach(t),OYr.forEach(t),DQo=i(R9e),Ece=n(R9e,"SPAN",{});var zYr=s(Ece);qQo=r(zYr,"TFAutoModelForImageClassification"),zYr.forEach(t),R9e.forEach(t),w0e=i(d),hr=n(d,"DIV",{class:!0});var Ll=s(hr);m(u3.$$.fragment,Ll),GQo=i(Ll),ec=n(Ll,"P",{});var UX=s(ec);OQo=r(UX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),yce=n(UX,"CODE",{});var VYr=s(yce);XQo=r(VYr,"from_pretrained()"),VYr.forEach(t),zQo=r(UX,"class method or the "),wce=n(UX,"CODE",{});var WYr=s(wce);VQo=r(WYr,"from_config()"),WYr.forEach(t),WQo=r(UX,`class
method.`),UX.forEach(t),QQo=i(Ll),b3=n(Ll,"P",{});var S9e=s(b3);HQo=r(S9e,"This class cannot be instantiated directly using "),Ace=n(S9e,"CODE",{});var QYr=s(Ace);UQo=r(QYr,"__init__()"),QYr.forEach(t),JQo=r(S9e," (throws an error)."),S9e.forEach(t),YQo=i(Ll),st=n(Ll,"DIV",{class:!0});var Bl=s(st);m(v3.$$.fragment,Bl),KQo=i(Bl),Lce=n(Bl,"P",{});var HYr=s(Lce);ZQo=r(HYr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),HYr.forEach(t),eHo=i(Bl),oc=n(Bl,"P",{});var JX=s(oc);oHo=r(JX,`Note:
Loading a model from its configuration file does `),Bce=n(JX,"STRONG",{});var UYr=s(Bce);rHo=r(UYr,"not"),UYr.forEach(t),tHo=r(JX,` load the model weights. It only affects the
model\u2019s configuration. Use `),xce=n(JX,"CODE",{});var JYr=s(xce);aHo=r(JYr,"from_pretrained()"),JYr.forEach(t),nHo=r(JX,"to load the model weights."),JX.forEach(t),sHo=i(Bl),kce=n(Bl,"P",{});var YYr=s(kce);lHo=r(YYr,"Examples:"),YYr.forEach(t),iHo=i(Bl),m(T3.$$.fragment,Bl),Bl.forEach(t),dHo=i(Ll),ho=n(Ll,"DIV",{class:!0});var da=s(ho);m(F3.$$.fragment,da),cHo=i(da),Rce=n(da,"P",{});var KYr=s(Rce);fHo=r(KYr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),KYr.forEach(t),mHo=i(da),nn=n(da,"P",{});var nC=s(nn);gHo=r(nC,"The model class to instantiate is selected based on the "),Sce=n(nC,"CODE",{});var ZYr=s(Sce);hHo=r(ZYr,"model_type"),ZYr.forEach(t),pHo=r(nC,` property of the config object (either
passed as an argument or loaded from `),Pce=n(nC,"CODE",{});var eKr=s(Pce);_Ho=r(eKr,"pretrained_model_name_or_path"),eKr.forEach(t),uHo=r(nC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$ce=n(nC,"CODE",{});var oKr=s($ce);bHo=r(oKr,"pretrained_model_name_or_path"),oKr.forEach(t),vHo=r(nC,":"),nC.forEach(t),THo=i(da),Ice=n(da,"UL",{});var rKr=s(Ice);v6=n(rKr,"LI",{});var wEe=s(v6);jce=n(wEe,"STRONG",{});var tKr=s(jce);FHo=r(tKr,"vit"),tKr.forEach(t),CHo=r(wEe," \u2014 "),aD=n(wEe,"A",{href:!0});var aKr=s(aD);MHo=r(aKr,"TFViTForImageClassification"),aKr.forEach(t),EHo=r(wEe," (ViT model)"),wEe.forEach(t),rKr.forEach(t),yHo=i(da),Nce=n(da,"P",{});var nKr=s(Nce);wHo=r(nKr,"Examples:"),nKr.forEach(t),AHo=i(da),m(C3.$$.fragment,da),da.forEach(t),Ll.forEach(t),A0e=i(d),rc=n(d,"H2",{class:!0});var P9e=s(rc);T6=n(P9e,"A",{id:!0,class:!0,href:!0});var sKr=s(T6);Dce=n(sKr,"SPAN",{});var lKr=s(Dce);m(M3.$$.fragment,lKr),lKr.forEach(t),sKr.forEach(t),LHo=i(P9e),qce=n(P9e,"SPAN",{});var iKr=s(qce);BHo=r(iKr,"TFAutoModelForMaskedLM"),iKr.forEach(t),P9e.forEach(t),L0e=i(d),pr=n(d,"DIV",{class:!0});var xl=s(pr);m(E3.$$.fragment,xl),xHo=i(xl),tc=n(xl,"P",{});var YX=s(tc);kHo=r(YX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Gce=n(YX,"CODE",{});var dKr=s(Gce);RHo=r(dKr,"from_pretrained()"),dKr.forEach(t),SHo=r(YX,"class method or the "),Oce=n(YX,"CODE",{});var cKr=s(Oce);PHo=r(cKr,"from_config()"),cKr.forEach(t),$Ho=r(YX,`class
method.`),YX.forEach(t),IHo=i(xl),y3=n(xl,"P",{});var $9e=s(y3);jHo=r($9e,"This class cannot be instantiated directly using "),Xce=n($9e,"CODE",{});var fKr=s(Xce);NHo=r(fKr,"__init__()"),fKr.forEach(t),DHo=r($9e," (throws an error)."),$9e.forEach(t),qHo=i(xl),lt=n(xl,"DIV",{class:!0});var kl=s(lt);m(w3.$$.fragment,kl),GHo=i(kl),zce=n(kl,"P",{});var mKr=s(zce);OHo=r(mKr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),mKr.forEach(t),XHo=i(kl),ac=n(kl,"P",{});var KX=s(ac);zHo=r(KX,`Note:
Loading a model from its configuration file does `),Vce=n(KX,"STRONG",{});var gKr=s(Vce);VHo=r(gKr,"not"),gKr.forEach(t),WHo=r(KX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wce=n(KX,"CODE",{});var hKr=s(Wce);QHo=r(hKr,"from_pretrained()"),hKr.forEach(t),HHo=r(KX,"to load the model weights."),KX.forEach(t),UHo=i(kl),Qce=n(kl,"P",{});var pKr=s(Qce);JHo=r(pKr,"Examples:"),pKr.forEach(t),YHo=i(kl),m(A3.$$.fragment,kl),kl.forEach(t),KHo=i(xl),po=n(xl,"DIV",{class:!0});var ca=s(po);m(L3.$$.fragment,ca),ZHo=i(ca),Hce=n(ca,"P",{});var _Kr=s(Hce);eUo=r(_Kr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),_Kr.forEach(t),oUo=i(ca),sn=n(ca,"P",{});var sC=s(sn);rUo=r(sC,"The model class to instantiate is selected based on the "),Uce=n(sC,"CODE",{});var uKr=s(Uce);tUo=r(uKr,"model_type"),uKr.forEach(t),aUo=r(sC,` property of the config object (either
passed as an argument or loaded from `),Jce=n(sC,"CODE",{});var bKr=s(Jce);nUo=r(bKr,"pretrained_model_name_or_path"),bKr.forEach(t),sUo=r(sC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yce=n(sC,"CODE",{});var vKr=s(Yce);lUo=r(vKr,"pretrained_model_name_or_path"),vKr.forEach(t),iUo=r(sC,":"),sC.forEach(t),dUo=i(ca),Y=n(ca,"UL",{});var ee=s(Y);F6=n(ee,"LI",{});var AEe=s(F6);Kce=n(AEe,"STRONG",{});var TKr=s(Kce);cUo=r(TKr,"albert"),TKr.forEach(t),fUo=r(AEe," \u2014 "),nD=n(AEe,"A",{href:!0});var FKr=s(nD);mUo=r(FKr,"TFAlbertForMaskedLM"),FKr.forEach(t),gUo=r(AEe," (ALBERT model)"),AEe.forEach(t),hUo=i(ee),C6=n(ee,"LI",{});var LEe=s(C6);Zce=n(LEe,"STRONG",{});var CKr=s(Zce);pUo=r(CKr,"bert"),CKr.forEach(t),_Uo=r(LEe," \u2014 "),sD=n(LEe,"A",{href:!0});var MKr=s(sD);uUo=r(MKr,"TFBertForMaskedLM"),MKr.forEach(t),bUo=r(LEe," (BERT model)"),LEe.forEach(t),vUo=i(ee),M6=n(ee,"LI",{});var BEe=s(M6);efe=n(BEe,"STRONG",{});var EKr=s(efe);TUo=r(EKr,"camembert"),EKr.forEach(t),FUo=r(BEe," \u2014 "),lD=n(BEe,"A",{href:!0});var yKr=s(lD);CUo=r(yKr,"TFCamembertForMaskedLM"),yKr.forEach(t),MUo=r(BEe," (CamemBERT model)"),BEe.forEach(t),EUo=i(ee),E6=n(ee,"LI",{});var xEe=s(E6);ofe=n(xEe,"STRONG",{});var wKr=s(ofe);yUo=r(wKr,"convbert"),wKr.forEach(t),wUo=r(xEe," \u2014 "),iD=n(xEe,"A",{href:!0});var AKr=s(iD);AUo=r(AKr,"TFConvBertForMaskedLM"),AKr.forEach(t),LUo=r(xEe," (ConvBERT model)"),xEe.forEach(t),BUo=i(ee),y6=n(ee,"LI",{});var kEe=s(y6);rfe=n(kEe,"STRONG",{});var LKr=s(rfe);xUo=r(LKr,"deberta"),LKr.forEach(t),kUo=r(kEe," \u2014 "),dD=n(kEe,"A",{href:!0});var BKr=s(dD);RUo=r(BKr,"TFDebertaForMaskedLM"),BKr.forEach(t),SUo=r(kEe," (DeBERTa model)"),kEe.forEach(t),PUo=i(ee),w6=n(ee,"LI",{});var REe=s(w6);tfe=n(REe,"STRONG",{});var xKr=s(tfe);$Uo=r(xKr,"deberta-v2"),xKr.forEach(t),IUo=r(REe," \u2014 "),cD=n(REe,"A",{href:!0});var kKr=s(cD);jUo=r(kKr,"TFDebertaV2ForMaskedLM"),kKr.forEach(t),NUo=r(REe," (DeBERTa-v2 model)"),REe.forEach(t),DUo=i(ee),A6=n(ee,"LI",{});var SEe=s(A6);afe=n(SEe,"STRONG",{});var RKr=s(afe);qUo=r(RKr,"distilbert"),RKr.forEach(t),GUo=r(SEe," \u2014 "),fD=n(SEe,"A",{href:!0});var SKr=s(fD);OUo=r(SKr,"TFDistilBertForMaskedLM"),SKr.forEach(t),XUo=r(SEe," (DistilBERT model)"),SEe.forEach(t),zUo=i(ee),L6=n(ee,"LI",{});var PEe=s(L6);nfe=n(PEe,"STRONG",{});var PKr=s(nfe);VUo=r(PKr,"electra"),PKr.forEach(t),WUo=r(PEe," \u2014 "),mD=n(PEe,"A",{href:!0});var $Kr=s(mD);QUo=r($Kr,"TFElectraForMaskedLM"),$Kr.forEach(t),HUo=r(PEe," (ELECTRA model)"),PEe.forEach(t),UUo=i(ee),B6=n(ee,"LI",{});var $Ee=s(B6);sfe=n($Ee,"STRONG",{});var IKr=s(sfe);JUo=r(IKr,"flaubert"),IKr.forEach(t),YUo=r($Ee," \u2014 "),gD=n($Ee,"A",{href:!0});var jKr=s(gD);KUo=r(jKr,"TFFlaubertWithLMHeadModel"),jKr.forEach(t),ZUo=r($Ee," (FlauBERT model)"),$Ee.forEach(t),eJo=i(ee),x6=n(ee,"LI",{});var IEe=s(x6);lfe=n(IEe,"STRONG",{});var NKr=s(lfe);oJo=r(NKr,"funnel"),NKr.forEach(t),rJo=r(IEe," \u2014 "),hD=n(IEe,"A",{href:!0});var DKr=s(hD);tJo=r(DKr,"TFFunnelForMaskedLM"),DKr.forEach(t),aJo=r(IEe," (Funnel Transformer model)"),IEe.forEach(t),nJo=i(ee),k6=n(ee,"LI",{});var jEe=s(k6);ife=n(jEe,"STRONG",{});var qKr=s(ife);sJo=r(qKr,"layoutlm"),qKr.forEach(t),lJo=r(jEe," \u2014 "),pD=n(jEe,"A",{href:!0});var GKr=s(pD);iJo=r(GKr,"TFLayoutLMForMaskedLM"),GKr.forEach(t),dJo=r(jEe," (LayoutLM model)"),jEe.forEach(t),cJo=i(ee),R6=n(ee,"LI",{});var NEe=s(R6);dfe=n(NEe,"STRONG",{});var OKr=s(dfe);fJo=r(OKr,"longformer"),OKr.forEach(t),mJo=r(NEe," \u2014 "),_D=n(NEe,"A",{href:!0});var XKr=s(_D);gJo=r(XKr,"TFLongformerForMaskedLM"),XKr.forEach(t),hJo=r(NEe," (Longformer model)"),NEe.forEach(t),pJo=i(ee),S6=n(ee,"LI",{});var DEe=s(S6);cfe=n(DEe,"STRONG",{});var zKr=s(cfe);_Jo=r(zKr,"mobilebert"),zKr.forEach(t),uJo=r(DEe," \u2014 "),uD=n(DEe,"A",{href:!0});var VKr=s(uD);bJo=r(VKr,"TFMobileBertForMaskedLM"),VKr.forEach(t),vJo=r(DEe," (MobileBERT model)"),DEe.forEach(t),TJo=i(ee),P6=n(ee,"LI",{});var qEe=s(P6);ffe=n(qEe,"STRONG",{});var WKr=s(ffe);FJo=r(WKr,"mpnet"),WKr.forEach(t),CJo=r(qEe," \u2014 "),bD=n(qEe,"A",{href:!0});var QKr=s(bD);MJo=r(QKr,"TFMPNetForMaskedLM"),QKr.forEach(t),EJo=r(qEe," (MPNet model)"),qEe.forEach(t),yJo=i(ee),$6=n(ee,"LI",{});var GEe=s($6);mfe=n(GEe,"STRONG",{});var HKr=s(mfe);wJo=r(HKr,"rembert"),HKr.forEach(t),AJo=r(GEe," \u2014 "),vD=n(GEe,"A",{href:!0});var UKr=s(vD);LJo=r(UKr,"TFRemBertForMaskedLM"),UKr.forEach(t),BJo=r(GEe," (RemBERT model)"),GEe.forEach(t),xJo=i(ee),I6=n(ee,"LI",{});var OEe=s(I6);gfe=n(OEe,"STRONG",{});var JKr=s(gfe);kJo=r(JKr,"roberta"),JKr.forEach(t),RJo=r(OEe," \u2014 "),TD=n(OEe,"A",{href:!0});var YKr=s(TD);SJo=r(YKr,"TFRobertaForMaskedLM"),YKr.forEach(t),PJo=r(OEe," (RoBERTa model)"),OEe.forEach(t),$Jo=i(ee),j6=n(ee,"LI",{});var XEe=s(j6);hfe=n(XEe,"STRONG",{});var KKr=s(hfe);IJo=r(KKr,"roformer"),KKr.forEach(t),jJo=r(XEe," \u2014 "),FD=n(XEe,"A",{href:!0});var ZKr=s(FD);NJo=r(ZKr,"TFRoFormerForMaskedLM"),ZKr.forEach(t),DJo=r(XEe," (RoFormer model)"),XEe.forEach(t),qJo=i(ee),N6=n(ee,"LI",{});var zEe=s(N6);pfe=n(zEe,"STRONG",{});var eZr=s(pfe);GJo=r(eZr,"tapas"),eZr.forEach(t),OJo=r(zEe," \u2014 "),CD=n(zEe,"A",{href:!0});var oZr=s(CD);XJo=r(oZr,"TFTapasForMaskedLM"),oZr.forEach(t),zJo=r(zEe," (TAPAS model)"),zEe.forEach(t),VJo=i(ee),D6=n(ee,"LI",{});var VEe=s(D6);_fe=n(VEe,"STRONG",{});var rZr=s(_fe);WJo=r(rZr,"xlm"),rZr.forEach(t),QJo=r(VEe," \u2014 "),MD=n(VEe,"A",{href:!0});var tZr=s(MD);HJo=r(tZr,"TFXLMWithLMHeadModel"),tZr.forEach(t),UJo=r(VEe," (XLM model)"),VEe.forEach(t),JJo=i(ee),q6=n(ee,"LI",{});var WEe=s(q6);ufe=n(WEe,"STRONG",{});var aZr=s(ufe);YJo=r(aZr,"xlm-roberta"),aZr.forEach(t),KJo=r(WEe," \u2014 "),ED=n(WEe,"A",{href:!0});var nZr=s(ED);ZJo=r(nZr,"TFXLMRobertaForMaskedLM"),nZr.forEach(t),eYo=r(WEe," (XLM-RoBERTa model)"),WEe.forEach(t),ee.forEach(t),oYo=i(ca),bfe=n(ca,"P",{});var sZr=s(bfe);rYo=r(sZr,"Examples:"),sZr.forEach(t),tYo=i(ca),m(B3.$$.fragment,ca),ca.forEach(t),xl.forEach(t),B0e=i(d),nc=n(d,"H2",{class:!0});var I9e=s(nc);G6=n(I9e,"A",{id:!0,class:!0,href:!0});var lZr=s(G6);vfe=n(lZr,"SPAN",{});var iZr=s(vfe);m(x3.$$.fragment,iZr),iZr.forEach(t),lZr.forEach(t),aYo=i(I9e),Tfe=n(I9e,"SPAN",{});var dZr=s(Tfe);nYo=r(dZr,"TFAutoModelForSeq2SeqLM"),dZr.forEach(t),I9e.forEach(t),x0e=i(d),_r=n(d,"DIV",{class:!0});var Rl=s(_r);m(k3.$$.fragment,Rl),sYo=i(Rl),sc=n(Rl,"P",{});var ZX=s(sc);lYo=r(ZX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Ffe=n(ZX,"CODE",{});var cZr=s(Ffe);iYo=r(cZr,"from_pretrained()"),cZr.forEach(t),dYo=r(ZX,"class method or the "),Cfe=n(ZX,"CODE",{});var fZr=s(Cfe);cYo=r(fZr,"from_config()"),fZr.forEach(t),fYo=r(ZX,`class
method.`),ZX.forEach(t),mYo=i(Rl),R3=n(Rl,"P",{});var j9e=s(R3);gYo=r(j9e,"This class cannot be instantiated directly using "),Mfe=n(j9e,"CODE",{});var mZr=s(Mfe);hYo=r(mZr,"__init__()"),mZr.forEach(t),pYo=r(j9e," (throws an error)."),j9e.forEach(t),_Yo=i(Rl),it=n(Rl,"DIV",{class:!0});var Sl=s(it);m(S3.$$.fragment,Sl),uYo=i(Sl),Efe=n(Sl,"P",{});var gZr=s(Efe);bYo=r(gZr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),gZr.forEach(t),vYo=i(Sl),lc=n(Sl,"P",{});var ez=s(lc);TYo=r(ez,`Note:
Loading a model from its configuration file does `),yfe=n(ez,"STRONG",{});var hZr=s(yfe);FYo=r(hZr,"not"),hZr.forEach(t),CYo=r(ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),wfe=n(ez,"CODE",{});var pZr=s(wfe);MYo=r(pZr,"from_pretrained()"),pZr.forEach(t),EYo=r(ez,"to load the model weights."),ez.forEach(t),yYo=i(Sl),Afe=n(Sl,"P",{});var _Zr=s(Afe);wYo=r(_Zr,"Examples:"),_Zr.forEach(t),AYo=i(Sl),m(P3.$$.fragment,Sl),Sl.forEach(t),LYo=i(Rl),_o=n(Rl,"DIV",{class:!0});var fa=s(_o);m($3.$$.fragment,fa),BYo=i(fa),Lfe=n(fa,"P",{});var uZr=s(Lfe);xYo=r(uZr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),uZr.forEach(t),kYo=i(fa),ln=n(fa,"P",{});var lC=s(ln);RYo=r(lC,"The model class to instantiate is selected based on the "),Bfe=n(lC,"CODE",{});var bZr=s(Bfe);SYo=r(bZr,"model_type"),bZr.forEach(t),PYo=r(lC,` property of the config object (either
passed as an argument or loaded from `),xfe=n(lC,"CODE",{});var vZr=s(xfe);$Yo=r(vZr,"pretrained_model_name_or_path"),vZr.forEach(t),IYo=r(lC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kfe=n(lC,"CODE",{});var TZr=s(kfe);jYo=r(TZr,"pretrained_model_name_or_path"),TZr.forEach(t),NYo=r(lC,":"),lC.forEach(t),DYo=i(fa),pe=n(fa,"UL",{});var Me=s(pe);O6=n(Me,"LI",{});var QEe=s(O6);Rfe=n(QEe,"STRONG",{});var FZr=s(Rfe);qYo=r(FZr,"bart"),FZr.forEach(t),GYo=r(QEe," \u2014 "),yD=n(QEe,"A",{href:!0});var CZr=s(yD);OYo=r(CZr,"TFBartForConditionalGeneration"),CZr.forEach(t),XYo=r(QEe," (BART model)"),QEe.forEach(t),zYo=i(Me),X6=n(Me,"LI",{});var HEe=s(X6);Sfe=n(HEe,"STRONG",{});var MZr=s(Sfe);VYo=r(MZr,"blenderbot"),MZr.forEach(t),WYo=r(HEe," \u2014 "),wD=n(HEe,"A",{href:!0});var EZr=s(wD);QYo=r(EZr,"TFBlenderbotForConditionalGeneration"),EZr.forEach(t),HYo=r(HEe," (Blenderbot model)"),HEe.forEach(t),UYo=i(Me),z6=n(Me,"LI",{});var UEe=s(z6);Pfe=n(UEe,"STRONG",{});var yZr=s(Pfe);JYo=r(yZr,"blenderbot-small"),yZr.forEach(t),YYo=r(UEe," \u2014 "),AD=n(UEe,"A",{href:!0});var wZr=s(AD);KYo=r(wZr,"TFBlenderbotSmallForConditionalGeneration"),wZr.forEach(t),ZYo=r(UEe," (BlenderbotSmall model)"),UEe.forEach(t),eKo=i(Me),V6=n(Me,"LI",{});var JEe=s(V6);$fe=n(JEe,"STRONG",{});var AZr=s($fe);oKo=r(AZr,"encoder-decoder"),AZr.forEach(t),rKo=r(JEe," \u2014 "),LD=n(JEe,"A",{href:!0});var LZr=s(LD);tKo=r(LZr,"TFEncoderDecoderModel"),LZr.forEach(t),aKo=r(JEe," (Encoder decoder model)"),JEe.forEach(t),nKo=i(Me),W6=n(Me,"LI",{});var YEe=s(W6);Ife=n(YEe,"STRONG",{});var BZr=s(Ife);sKo=r(BZr,"led"),BZr.forEach(t),lKo=r(YEe," \u2014 "),BD=n(YEe,"A",{href:!0});var xZr=s(BD);iKo=r(xZr,"TFLEDForConditionalGeneration"),xZr.forEach(t),dKo=r(YEe," (LED model)"),YEe.forEach(t),cKo=i(Me),Q6=n(Me,"LI",{});var KEe=s(Q6);jfe=n(KEe,"STRONG",{});var kZr=s(jfe);fKo=r(kZr,"marian"),kZr.forEach(t),mKo=r(KEe," \u2014 "),xD=n(KEe,"A",{href:!0});var RZr=s(xD);gKo=r(RZr,"TFMarianMTModel"),RZr.forEach(t),hKo=r(KEe," (Marian model)"),KEe.forEach(t),pKo=i(Me),H6=n(Me,"LI",{});var ZEe=s(H6);Nfe=n(ZEe,"STRONG",{});var SZr=s(Nfe);_Ko=r(SZr,"mbart"),SZr.forEach(t),uKo=r(ZEe," \u2014 "),kD=n(ZEe,"A",{href:!0});var PZr=s(kD);bKo=r(PZr,"TFMBartForConditionalGeneration"),PZr.forEach(t),vKo=r(ZEe," (mBART model)"),ZEe.forEach(t),TKo=i(Me),U6=n(Me,"LI",{});var e3e=s(U6);Dfe=n(e3e,"STRONG",{});var $Zr=s(Dfe);FKo=r($Zr,"mt5"),$Zr.forEach(t),CKo=r(e3e," \u2014 "),RD=n(e3e,"A",{href:!0});var IZr=s(RD);MKo=r(IZr,"TFMT5ForConditionalGeneration"),IZr.forEach(t),EKo=r(e3e," (mT5 model)"),e3e.forEach(t),yKo=i(Me),J6=n(Me,"LI",{});var o3e=s(J6);qfe=n(o3e,"STRONG",{});var jZr=s(qfe);wKo=r(jZr,"pegasus"),jZr.forEach(t),AKo=r(o3e," \u2014 "),SD=n(o3e,"A",{href:!0});var NZr=s(SD);LKo=r(NZr,"TFPegasusForConditionalGeneration"),NZr.forEach(t),BKo=r(o3e," (Pegasus model)"),o3e.forEach(t),xKo=i(Me),Y6=n(Me,"LI",{});var r3e=s(Y6);Gfe=n(r3e,"STRONG",{});var DZr=s(Gfe);kKo=r(DZr,"t5"),DZr.forEach(t),RKo=r(r3e," \u2014 "),PD=n(r3e,"A",{href:!0});var qZr=s(PD);SKo=r(qZr,"TFT5ForConditionalGeneration"),qZr.forEach(t),PKo=r(r3e," (T5 model)"),r3e.forEach(t),Me.forEach(t),$Ko=i(fa),Ofe=n(fa,"P",{});var GZr=s(Ofe);IKo=r(GZr,"Examples:"),GZr.forEach(t),jKo=i(fa),m(I3.$$.fragment,fa),fa.forEach(t),Rl.forEach(t),k0e=i(d),ic=n(d,"H2",{class:!0});var N9e=s(ic);K6=n(N9e,"A",{id:!0,class:!0,href:!0});var OZr=s(K6);Xfe=n(OZr,"SPAN",{});var XZr=s(Xfe);m(j3.$$.fragment,XZr),XZr.forEach(t),OZr.forEach(t),NKo=i(N9e),zfe=n(N9e,"SPAN",{});var zZr=s(zfe);DKo=r(zZr,"TFAutoModelForSequenceClassification"),zZr.forEach(t),N9e.forEach(t),R0e=i(d),ur=n(d,"DIV",{class:!0});var Pl=s(ur);m(N3.$$.fragment,Pl),qKo=i(Pl),dc=n(Pl,"P",{});var oz=s(dc);GKo=r(oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Vfe=n(oz,"CODE",{});var VZr=s(Vfe);OKo=r(VZr,"from_pretrained()"),VZr.forEach(t),XKo=r(oz,"class method or the "),Wfe=n(oz,"CODE",{});var WZr=s(Wfe);zKo=r(WZr,"from_config()"),WZr.forEach(t),VKo=r(oz,`class
method.`),oz.forEach(t),WKo=i(Pl),D3=n(Pl,"P",{});var D9e=s(D3);QKo=r(D9e,"This class cannot be instantiated directly using "),Qfe=n(D9e,"CODE",{});var QZr=s(Qfe);HKo=r(QZr,"__init__()"),QZr.forEach(t),UKo=r(D9e," (throws an error)."),D9e.forEach(t),JKo=i(Pl),dt=n(Pl,"DIV",{class:!0});var $l=s(dt);m(q3.$$.fragment,$l),YKo=i($l),Hfe=n($l,"P",{});var HZr=s(Hfe);KKo=r(HZr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),HZr.forEach(t),ZKo=i($l),cc=n($l,"P",{});var rz=s(cc);eZo=r(rz,`Note:
Loading a model from its configuration file does `),Ufe=n(rz,"STRONG",{});var UZr=s(Ufe);oZo=r(UZr,"not"),UZr.forEach(t),rZo=r(rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jfe=n(rz,"CODE",{});var JZr=s(Jfe);tZo=r(JZr,"from_pretrained()"),JZr.forEach(t),aZo=r(rz,"to load the model weights."),rz.forEach(t),nZo=i($l),Yfe=n($l,"P",{});var YZr=s(Yfe);sZo=r(YZr,"Examples:"),YZr.forEach(t),lZo=i($l),m(G3.$$.fragment,$l),$l.forEach(t),iZo=i(Pl),uo=n(Pl,"DIV",{class:!0});var ma=s(uo);m(O3.$$.fragment,ma),dZo=i(ma),Kfe=n(ma,"P",{});var KZr=s(Kfe);cZo=r(KZr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),KZr.forEach(t),fZo=i(ma),dn=n(ma,"P",{});var iC=s(dn);mZo=r(iC,"The model class to instantiate is selected based on the "),Zfe=n(iC,"CODE",{});var ZZr=s(Zfe);gZo=r(ZZr,"model_type"),ZZr.forEach(t),hZo=r(iC,` property of the config object (either
passed as an argument or loaded from `),eme=n(iC,"CODE",{});var eet=s(eme);pZo=r(eet,"pretrained_model_name_or_path"),eet.forEach(t),_Zo=r(iC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ome=n(iC,"CODE",{});var oet=s(ome);uZo=r(oet,"pretrained_model_name_or_path"),oet.forEach(t),bZo=r(iC,":"),iC.forEach(t),vZo=i(ma),X=n(ma,"UL",{});var W=s(X);Z6=n(W,"LI",{});var t3e=s(Z6);rme=n(t3e,"STRONG",{});var ret=s(rme);TZo=r(ret,"albert"),ret.forEach(t),FZo=r(t3e," \u2014 "),$D=n(t3e,"A",{href:!0});var tet=s($D);CZo=r(tet,"TFAlbertForSequenceClassification"),tet.forEach(t),MZo=r(t3e," (ALBERT model)"),t3e.forEach(t),EZo=i(W),eT=n(W,"LI",{});var a3e=s(eT);tme=n(a3e,"STRONG",{});var aet=s(tme);yZo=r(aet,"bert"),aet.forEach(t),wZo=r(a3e," \u2014 "),ID=n(a3e,"A",{href:!0});var net=s(ID);AZo=r(net,"TFBertForSequenceClassification"),net.forEach(t),LZo=r(a3e," (BERT model)"),a3e.forEach(t),BZo=i(W),oT=n(W,"LI",{});var n3e=s(oT);ame=n(n3e,"STRONG",{});var set=s(ame);xZo=r(set,"camembert"),set.forEach(t),kZo=r(n3e," \u2014 "),jD=n(n3e,"A",{href:!0});var iet=s(jD);RZo=r(iet,"TFCamembertForSequenceClassification"),iet.forEach(t),SZo=r(n3e," (CamemBERT model)"),n3e.forEach(t),PZo=i(W),rT=n(W,"LI",{});var s3e=s(rT);nme=n(s3e,"STRONG",{});var det=s(nme);$Zo=r(det,"convbert"),det.forEach(t),IZo=r(s3e," \u2014 "),ND=n(s3e,"A",{href:!0});var cet=s(ND);jZo=r(cet,"TFConvBertForSequenceClassification"),cet.forEach(t),NZo=r(s3e," (ConvBERT model)"),s3e.forEach(t),DZo=i(W),tT=n(W,"LI",{});var l3e=s(tT);sme=n(l3e,"STRONG",{});var fet=s(sme);qZo=r(fet,"ctrl"),fet.forEach(t),GZo=r(l3e," \u2014 "),DD=n(l3e,"A",{href:!0});var met=s(DD);OZo=r(met,"TFCTRLForSequenceClassification"),met.forEach(t),XZo=r(l3e," (CTRL model)"),l3e.forEach(t),zZo=i(W),aT=n(W,"LI",{});var i3e=s(aT);lme=n(i3e,"STRONG",{});var get=s(lme);VZo=r(get,"deberta"),get.forEach(t),WZo=r(i3e," \u2014 "),qD=n(i3e,"A",{href:!0});var het=s(qD);QZo=r(het,"TFDebertaForSequenceClassification"),het.forEach(t),HZo=r(i3e," (DeBERTa model)"),i3e.forEach(t),UZo=i(W),nT=n(W,"LI",{});var d3e=s(nT);ime=n(d3e,"STRONG",{});var pet=s(ime);JZo=r(pet,"deberta-v2"),pet.forEach(t),YZo=r(d3e," \u2014 "),GD=n(d3e,"A",{href:!0});var _et=s(GD);KZo=r(_et,"TFDebertaV2ForSequenceClassification"),_et.forEach(t),ZZo=r(d3e," (DeBERTa-v2 model)"),d3e.forEach(t),eer=i(W),sT=n(W,"LI",{});var c3e=s(sT);dme=n(c3e,"STRONG",{});var uet=s(dme);oer=r(uet,"distilbert"),uet.forEach(t),rer=r(c3e," \u2014 "),OD=n(c3e,"A",{href:!0});var bet=s(OD);ter=r(bet,"TFDistilBertForSequenceClassification"),bet.forEach(t),aer=r(c3e," (DistilBERT model)"),c3e.forEach(t),ner=i(W),lT=n(W,"LI",{});var f3e=s(lT);cme=n(f3e,"STRONG",{});var vet=s(cme);ser=r(vet,"electra"),vet.forEach(t),ler=r(f3e," \u2014 "),XD=n(f3e,"A",{href:!0});var Tet=s(XD);ier=r(Tet,"TFElectraForSequenceClassification"),Tet.forEach(t),der=r(f3e," (ELECTRA model)"),f3e.forEach(t),cer=i(W),iT=n(W,"LI",{});var m3e=s(iT);fme=n(m3e,"STRONG",{});var Fet=s(fme);fer=r(Fet,"flaubert"),Fet.forEach(t),mer=r(m3e," \u2014 "),zD=n(m3e,"A",{href:!0});var Cet=s(zD);ger=r(Cet,"TFFlaubertForSequenceClassification"),Cet.forEach(t),her=r(m3e," (FlauBERT model)"),m3e.forEach(t),per=i(W),dT=n(W,"LI",{});var g3e=s(dT);mme=n(g3e,"STRONG",{});var Met=s(mme);_er=r(Met,"funnel"),Met.forEach(t),uer=r(g3e," \u2014 "),VD=n(g3e,"A",{href:!0});var Eet=s(VD);ber=r(Eet,"TFFunnelForSequenceClassification"),Eet.forEach(t),ver=r(g3e," (Funnel Transformer model)"),g3e.forEach(t),Ter=i(W),cT=n(W,"LI",{});var h3e=s(cT);gme=n(h3e,"STRONG",{});var yet=s(gme);Fer=r(yet,"gpt2"),yet.forEach(t),Cer=r(h3e," \u2014 "),WD=n(h3e,"A",{href:!0});var wet=s(WD);Mer=r(wet,"TFGPT2ForSequenceClassification"),wet.forEach(t),Eer=r(h3e," (OpenAI GPT-2 model)"),h3e.forEach(t),yer=i(W),fT=n(W,"LI",{});var p3e=s(fT);hme=n(p3e,"STRONG",{});var Aet=s(hme);wer=r(Aet,"layoutlm"),Aet.forEach(t),Aer=r(p3e," \u2014 "),QD=n(p3e,"A",{href:!0});var Let=s(QD);Ler=r(Let,"TFLayoutLMForSequenceClassification"),Let.forEach(t),Ber=r(p3e," (LayoutLM model)"),p3e.forEach(t),xer=i(W),mT=n(W,"LI",{});var _3e=s(mT);pme=n(_3e,"STRONG",{});var Bet=s(pme);ker=r(Bet,"longformer"),Bet.forEach(t),Rer=r(_3e," \u2014 "),HD=n(_3e,"A",{href:!0});var xet=s(HD);Ser=r(xet,"TFLongformerForSequenceClassification"),xet.forEach(t),Per=r(_3e," (Longformer model)"),_3e.forEach(t),$er=i(W),gT=n(W,"LI",{});var u3e=s(gT);_me=n(u3e,"STRONG",{});var ket=s(_me);Ier=r(ket,"mobilebert"),ket.forEach(t),jer=r(u3e," \u2014 "),UD=n(u3e,"A",{href:!0});var Ret=s(UD);Ner=r(Ret,"TFMobileBertForSequenceClassification"),Ret.forEach(t),Der=r(u3e," (MobileBERT model)"),u3e.forEach(t),qer=i(W),hT=n(W,"LI",{});var b3e=s(hT);ume=n(b3e,"STRONG",{});var Set=s(ume);Ger=r(Set,"mpnet"),Set.forEach(t),Oer=r(b3e," \u2014 "),JD=n(b3e,"A",{href:!0});var Pet=s(JD);Xer=r(Pet,"TFMPNetForSequenceClassification"),Pet.forEach(t),zer=r(b3e," (MPNet model)"),b3e.forEach(t),Ver=i(W),pT=n(W,"LI",{});var v3e=s(pT);bme=n(v3e,"STRONG",{});var $et=s(bme);Wer=r($et,"openai-gpt"),$et.forEach(t),Qer=r(v3e," \u2014 "),YD=n(v3e,"A",{href:!0});var Iet=s(YD);Her=r(Iet,"TFOpenAIGPTForSequenceClassification"),Iet.forEach(t),Uer=r(v3e," (OpenAI GPT model)"),v3e.forEach(t),Jer=i(W),_T=n(W,"LI",{});var T3e=s(_T);vme=n(T3e,"STRONG",{});var jet=s(vme);Yer=r(jet,"rembert"),jet.forEach(t),Ker=r(T3e," \u2014 "),KD=n(T3e,"A",{href:!0});var Net=s(KD);Zer=r(Net,"TFRemBertForSequenceClassification"),Net.forEach(t),eor=r(T3e," (RemBERT model)"),T3e.forEach(t),oor=i(W),uT=n(W,"LI",{});var F3e=s(uT);Tme=n(F3e,"STRONG",{});var Det=s(Tme);ror=r(Det,"roberta"),Det.forEach(t),tor=r(F3e," \u2014 "),ZD=n(F3e,"A",{href:!0});var qet=s(ZD);aor=r(qet,"TFRobertaForSequenceClassification"),qet.forEach(t),nor=r(F3e," (RoBERTa model)"),F3e.forEach(t),sor=i(W),bT=n(W,"LI",{});var C3e=s(bT);Fme=n(C3e,"STRONG",{});var Get=s(Fme);lor=r(Get,"roformer"),Get.forEach(t),ior=r(C3e," \u2014 "),eq=n(C3e,"A",{href:!0});var Oet=s(eq);dor=r(Oet,"TFRoFormerForSequenceClassification"),Oet.forEach(t),cor=r(C3e," (RoFormer model)"),C3e.forEach(t),mor=i(W),vT=n(W,"LI",{});var M3e=s(vT);Cme=n(M3e,"STRONG",{});var Xet=s(Cme);gor=r(Xet,"tapas"),Xet.forEach(t),hor=r(M3e," \u2014 "),oq=n(M3e,"A",{href:!0});var zet=s(oq);por=r(zet,"TFTapasForSequenceClassification"),zet.forEach(t),_or=r(M3e," (TAPAS model)"),M3e.forEach(t),uor=i(W),TT=n(W,"LI",{});var E3e=s(TT);Mme=n(E3e,"STRONG",{});var Vet=s(Mme);bor=r(Vet,"transfo-xl"),Vet.forEach(t),vor=r(E3e," \u2014 "),rq=n(E3e,"A",{href:!0});var Wet=s(rq);Tor=r(Wet,"TFTransfoXLForSequenceClassification"),Wet.forEach(t),For=r(E3e," (Transformer-XL model)"),E3e.forEach(t),Cor=i(W),FT=n(W,"LI",{});var y3e=s(FT);Eme=n(y3e,"STRONG",{});var Qet=s(Eme);Mor=r(Qet,"xlm"),Qet.forEach(t),Eor=r(y3e," \u2014 "),tq=n(y3e,"A",{href:!0});var Het=s(tq);yor=r(Het,"TFXLMForSequenceClassification"),Het.forEach(t),wor=r(y3e," (XLM model)"),y3e.forEach(t),Aor=i(W),CT=n(W,"LI",{});var w3e=s(CT);yme=n(w3e,"STRONG",{});var Uet=s(yme);Lor=r(Uet,"xlm-roberta"),Uet.forEach(t),Bor=r(w3e," \u2014 "),aq=n(w3e,"A",{href:!0});var Jet=s(aq);xor=r(Jet,"TFXLMRobertaForSequenceClassification"),Jet.forEach(t),kor=r(w3e," (XLM-RoBERTa model)"),w3e.forEach(t),Ror=i(W),MT=n(W,"LI",{});var A3e=s(MT);wme=n(A3e,"STRONG",{});var Yet=s(wme);Sor=r(Yet,"xlnet"),Yet.forEach(t),Por=r(A3e," \u2014 "),nq=n(A3e,"A",{href:!0});var Ket=s(nq);$or=r(Ket,"TFXLNetForSequenceClassification"),Ket.forEach(t),Ior=r(A3e," (XLNet model)"),A3e.forEach(t),W.forEach(t),jor=i(ma),Ame=n(ma,"P",{});var Zet=s(Ame);Nor=r(Zet,"Examples:"),Zet.forEach(t),Dor=i(ma),m(X3.$$.fragment,ma),ma.forEach(t),Pl.forEach(t),S0e=i(d),fc=n(d,"H2",{class:!0});var q9e=s(fc);ET=n(q9e,"A",{id:!0,class:!0,href:!0});var eot=s(ET);Lme=n(eot,"SPAN",{});var oot=s(Lme);m(z3.$$.fragment,oot),oot.forEach(t),eot.forEach(t),qor=i(q9e),Bme=n(q9e,"SPAN",{});var rot=s(Bme);Gor=r(rot,"TFAutoModelForMultipleChoice"),rot.forEach(t),q9e.forEach(t),P0e=i(d),br=n(d,"DIV",{class:!0});var Il=s(br);m(V3.$$.fragment,Il),Oor=i(Il),mc=n(Il,"P",{});var tz=s(mc);Xor=r(tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),xme=n(tz,"CODE",{});var tot=s(xme);zor=r(tot,"from_pretrained()"),tot.forEach(t),Vor=r(tz,"class method or the "),kme=n(tz,"CODE",{});var aot=s(kme);Wor=r(aot,"from_config()"),aot.forEach(t),Qor=r(tz,`class
method.`),tz.forEach(t),Hor=i(Il),W3=n(Il,"P",{});var G9e=s(W3);Uor=r(G9e,"This class cannot be instantiated directly using "),Rme=n(G9e,"CODE",{});var not=s(Rme);Jor=r(not,"__init__()"),not.forEach(t),Yor=r(G9e," (throws an error)."),G9e.forEach(t),Kor=i(Il),ct=n(Il,"DIV",{class:!0});var jl=s(ct);m(Q3.$$.fragment,jl),Zor=i(jl),Sme=n(jl,"P",{});var sot=s(Sme);err=r(sot,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),sot.forEach(t),orr=i(jl),gc=n(jl,"P",{});var az=s(gc);rrr=r(az,`Note:
Loading a model from its configuration file does `),Pme=n(az,"STRONG",{});var lot=s(Pme);trr=r(lot,"not"),lot.forEach(t),arr=r(az,` load the model weights. It only affects the
model\u2019s configuration. Use `),$me=n(az,"CODE",{});var iot=s($me);nrr=r(iot,"from_pretrained()"),iot.forEach(t),srr=r(az,"to load the model weights."),az.forEach(t),lrr=i(jl),Ime=n(jl,"P",{});var dot=s(Ime);irr=r(dot,"Examples:"),dot.forEach(t),drr=i(jl),m(H3.$$.fragment,jl),jl.forEach(t),crr=i(Il),bo=n(Il,"DIV",{class:!0});var ga=s(bo);m(U3.$$.fragment,ga),frr=i(ga),jme=n(ga,"P",{});var cot=s(jme);mrr=r(cot,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),cot.forEach(t),grr=i(ga),cn=n(ga,"P",{});var dC=s(cn);hrr=r(dC,"The model class to instantiate is selected based on the "),Nme=n(dC,"CODE",{});var fot=s(Nme);prr=r(fot,"model_type"),fot.forEach(t),_rr=r(dC,` property of the config object (either
passed as an argument or loaded from `),Dme=n(dC,"CODE",{});var mot=s(Dme);urr=r(mot,"pretrained_model_name_or_path"),mot.forEach(t),brr=r(dC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qme=n(dC,"CODE",{});var got=s(qme);vrr=r(got,"pretrained_model_name_or_path"),got.forEach(t),Trr=r(dC,":"),dC.forEach(t),Frr=i(ga),te=n(ga,"UL",{});var ae=s(te);yT=n(ae,"LI",{});var L3e=s(yT);Gme=n(L3e,"STRONG",{});var hot=s(Gme);Crr=r(hot,"albert"),hot.forEach(t),Mrr=r(L3e," \u2014 "),sq=n(L3e,"A",{href:!0});var pot=s(sq);Err=r(pot,"TFAlbertForMultipleChoice"),pot.forEach(t),yrr=r(L3e," (ALBERT model)"),L3e.forEach(t),wrr=i(ae),wT=n(ae,"LI",{});var B3e=s(wT);Ome=n(B3e,"STRONG",{});var _ot=s(Ome);Arr=r(_ot,"bert"),_ot.forEach(t),Lrr=r(B3e," \u2014 "),lq=n(B3e,"A",{href:!0});var uot=s(lq);Brr=r(uot,"TFBertForMultipleChoice"),uot.forEach(t),xrr=r(B3e," (BERT model)"),B3e.forEach(t),krr=i(ae),AT=n(ae,"LI",{});var x3e=s(AT);Xme=n(x3e,"STRONG",{});var bot=s(Xme);Rrr=r(bot,"camembert"),bot.forEach(t),Srr=r(x3e," \u2014 "),iq=n(x3e,"A",{href:!0});var vot=s(iq);Prr=r(vot,"TFCamembertForMultipleChoice"),vot.forEach(t),$rr=r(x3e," (CamemBERT model)"),x3e.forEach(t),Irr=i(ae),LT=n(ae,"LI",{});var k3e=s(LT);zme=n(k3e,"STRONG",{});var Tot=s(zme);jrr=r(Tot,"convbert"),Tot.forEach(t),Nrr=r(k3e," \u2014 "),dq=n(k3e,"A",{href:!0});var Fot=s(dq);Drr=r(Fot,"TFConvBertForMultipleChoice"),Fot.forEach(t),qrr=r(k3e," (ConvBERT model)"),k3e.forEach(t),Grr=i(ae),BT=n(ae,"LI",{});var R3e=s(BT);Vme=n(R3e,"STRONG",{});var Cot=s(Vme);Orr=r(Cot,"distilbert"),Cot.forEach(t),Xrr=r(R3e," \u2014 "),cq=n(R3e,"A",{href:!0});var Mot=s(cq);zrr=r(Mot,"TFDistilBertForMultipleChoice"),Mot.forEach(t),Vrr=r(R3e," (DistilBERT model)"),R3e.forEach(t),Wrr=i(ae),xT=n(ae,"LI",{});var S3e=s(xT);Wme=n(S3e,"STRONG",{});var Eot=s(Wme);Qrr=r(Eot,"electra"),Eot.forEach(t),Hrr=r(S3e," \u2014 "),fq=n(S3e,"A",{href:!0});var yot=s(fq);Urr=r(yot,"TFElectraForMultipleChoice"),yot.forEach(t),Jrr=r(S3e," (ELECTRA model)"),S3e.forEach(t),Yrr=i(ae),kT=n(ae,"LI",{});var P3e=s(kT);Qme=n(P3e,"STRONG",{});var wot=s(Qme);Krr=r(wot,"flaubert"),wot.forEach(t),Zrr=r(P3e," \u2014 "),mq=n(P3e,"A",{href:!0});var Aot=s(mq);etr=r(Aot,"TFFlaubertForMultipleChoice"),Aot.forEach(t),otr=r(P3e," (FlauBERT model)"),P3e.forEach(t),rtr=i(ae),RT=n(ae,"LI",{});var $3e=s(RT);Hme=n($3e,"STRONG",{});var Lot=s(Hme);ttr=r(Lot,"funnel"),Lot.forEach(t),atr=r($3e," \u2014 "),gq=n($3e,"A",{href:!0});var Bot=s(gq);ntr=r(Bot,"TFFunnelForMultipleChoice"),Bot.forEach(t),str=r($3e," (Funnel Transformer model)"),$3e.forEach(t),ltr=i(ae),ST=n(ae,"LI",{});var I3e=s(ST);Ume=n(I3e,"STRONG",{});var xot=s(Ume);itr=r(xot,"longformer"),xot.forEach(t),dtr=r(I3e," \u2014 "),hq=n(I3e,"A",{href:!0});var kot=s(hq);ctr=r(kot,"TFLongformerForMultipleChoice"),kot.forEach(t),ftr=r(I3e," (Longformer model)"),I3e.forEach(t),mtr=i(ae),PT=n(ae,"LI",{});var j3e=s(PT);Jme=n(j3e,"STRONG",{});var Rot=s(Jme);gtr=r(Rot,"mobilebert"),Rot.forEach(t),htr=r(j3e," \u2014 "),pq=n(j3e,"A",{href:!0});var Sot=s(pq);ptr=r(Sot,"TFMobileBertForMultipleChoice"),Sot.forEach(t),_tr=r(j3e," (MobileBERT model)"),j3e.forEach(t),utr=i(ae),$T=n(ae,"LI",{});var N3e=s($T);Yme=n(N3e,"STRONG",{});var Pot=s(Yme);btr=r(Pot,"mpnet"),Pot.forEach(t),vtr=r(N3e," \u2014 "),_q=n(N3e,"A",{href:!0});var $ot=s(_q);Ttr=r($ot,"TFMPNetForMultipleChoice"),$ot.forEach(t),Ftr=r(N3e," (MPNet model)"),N3e.forEach(t),Ctr=i(ae),IT=n(ae,"LI",{});var D3e=s(IT);Kme=n(D3e,"STRONG",{});var Iot=s(Kme);Mtr=r(Iot,"rembert"),Iot.forEach(t),Etr=r(D3e," \u2014 "),uq=n(D3e,"A",{href:!0});var jot=s(uq);ytr=r(jot,"TFRemBertForMultipleChoice"),jot.forEach(t),wtr=r(D3e," (RemBERT model)"),D3e.forEach(t),Atr=i(ae),jT=n(ae,"LI",{});var q3e=s(jT);Zme=n(q3e,"STRONG",{});var Not=s(Zme);Ltr=r(Not,"roberta"),Not.forEach(t),Btr=r(q3e," \u2014 "),bq=n(q3e,"A",{href:!0});var Dot=s(bq);xtr=r(Dot,"TFRobertaForMultipleChoice"),Dot.forEach(t),ktr=r(q3e," (RoBERTa model)"),q3e.forEach(t),Rtr=i(ae),NT=n(ae,"LI",{});var G3e=s(NT);ege=n(G3e,"STRONG",{});var qot=s(ege);Str=r(qot,"roformer"),qot.forEach(t),Ptr=r(G3e," \u2014 "),vq=n(G3e,"A",{href:!0});var Got=s(vq);$tr=r(Got,"TFRoFormerForMultipleChoice"),Got.forEach(t),Itr=r(G3e," (RoFormer model)"),G3e.forEach(t),jtr=i(ae),DT=n(ae,"LI",{});var O3e=s(DT);oge=n(O3e,"STRONG",{});var Oot=s(oge);Ntr=r(Oot,"xlm"),Oot.forEach(t),Dtr=r(O3e," \u2014 "),Tq=n(O3e,"A",{href:!0});var Xot=s(Tq);qtr=r(Xot,"TFXLMForMultipleChoice"),Xot.forEach(t),Gtr=r(O3e," (XLM model)"),O3e.forEach(t),Otr=i(ae),qT=n(ae,"LI",{});var X3e=s(qT);rge=n(X3e,"STRONG",{});var zot=s(rge);Xtr=r(zot,"xlm-roberta"),zot.forEach(t),ztr=r(X3e," \u2014 "),Fq=n(X3e,"A",{href:!0});var Vot=s(Fq);Vtr=r(Vot,"TFXLMRobertaForMultipleChoice"),Vot.forEach(t),Wtr=r(X3e," (XLM-RoBERTa model)"),X3e.forEach(t),Qtr=i(ae),GT=n(ae,"LI",{});var z3e=s(GT);tge=n(z3e,"STRONG",{});var Wot=s(tge);Htr=r(Wot,"xlnet"),Wot.forEach(t),Utr=r(z3e," \u2014 "),Cq=n(z3e,"A",{href:!0});var Qot=s(Cq);Jtr=r(Qot,"TFXLNetForMultipleChoice"),Qot.forEach(t),Ytr=r(z3e," (XLNet model)"),z3e.forEach(t),ae.forEach(t),Ktr=i(ga),age=n(ga,"P",{});var Hot=s(age);Ztr=r(Hot,"Examples:"),Hot.forEach(t),ear=i(ga),m(J3.$$.fragment,ga),ga.forEach(t),Il.forEach(t),$0e=i(d),hc=n(d,"H2",{class:!0});var O9e=s(hc);OT=n(O9e,"A",{id:!0,class:!0,href:!0});var Uot=s(OT);nge=n(Uot,"SPAN",{});var Jot=s(nge);m(Y3.$$.fragment,Jot),Jot.forEach(t),Uot.forEach(t),oar=i(O9e),sge=n(O9e,"SPAN",{});var Yot=s(sge);rar=r(Yot,"TFAutoModelForTableQuestionAnswering"),Yot.forEach(t),O9e.forEach(t),I0e=i(d),vr=n(d,"DIV",{class:!0});var Nl=s(vr);m(K3.$$.fragment,Nl),tar=i(Nl),pc=n(Nl,"P",{});var nz=s(pc);aar=r(nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),lge=n(nz,"CODE",{});var Kot=s(lge);nar=r(Kot,"from_pretrained()"),Kot.forEach(t),sar=r(nz,"class method or the "),ige=n(nz,"CODE",{});var Zot=s(ige);lar=r(Zot,"from_config()"),Zot.forEach(t),iar=r(nz,`class
method.`),nz.forEach(t),dar=i(Nl),Z3=n(Nl,"P",{});var X9e=s(Z3);car=r(X9e,"This class cannot be instantiated directly using "),dge=n(X9e,"CODE",{});var ert=s(dge);far=r(ert,"__init__()"),ert.forEach(t),mar=r(X9e," (throws an error)."),X9e.forEach(t),gar=i(Nl),ft=n(Nl,"DIV",{class:!0});var Dl=s(ft);m(ey.$$.fragment,Dl),har=i(Dl),cge=n(Dl,"P",{});var ort=s(cge);par=r(ort,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),ort.forEach(t),_ar=i(Dl),_c=n(Dl,"P",{});var sz=s(_c);uar=r(sz,`Note:
Loading a model from its configuration file does `),fge=n(sz,"STRONG",{});var rrt=s(fge);bar=r(rrt,"not"),rrt.forEach(t),Tar=r(sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mge=n(sz,"CODE",{});var trt=s(mge);Far=r(trt,"from_pretrained()"),trt.forEach(t),Car=r(sz,"to load the model weights."),sz.forEach(t),Mar=i(Dl),gge=n(Dl,"P",{});var art=s(gge);Ear=r(art,"Examples:"),art.forEach(t),yar=i(Dl),m(oy.$$.fragment,Dl),Dl.forEach(t),war=i(Nl),vo=n(Nl,"DIV",{class:!0});var ha=s(vo);m(ry.$$.fragment,ha),Aar=i(ha),hge=n(ha,"P",{});var nrt=s(hge);Lar=r(nrt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),nrt.forEach(t),Bar=i(ha),fn=n(ha,"P",{});var cC=s(fn);xar=r(cC,"The model class to instantiate is selected based on the "),pge=n(cC,"CODE",{});var srt=s(pge);kar=r(srt,"model_type"),srt.forEach(t),Rar=r(cC,` property of the config object (either
passed as an argument or loaded from `),_ge=n(cC,"CODE",{});var lrt=s(_ge);Sar=r(lrt,"pretrained_model_name_or_path"),lrt.forEach(t),Par=r(cC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),uge=n(cC,"CODE",{});var irt=s(uge);$ar=r(irt,"pretrained_model_name_or_path"),irt.forEach(t),Iar=r(cC,":"),cC.forEach(t),jar=i(ha),bge=n(ha,"UL",{});var drt=s(bge);XT=n(drt,"LI",{});var V3e=s(XT);vge=n(V3e,"STRONG",{});var crt=s(vge);Nar=r(crt,"tapas"),crt.forEach(t),Dar=r(V3e," \u2014 "),Mq=n(V3e,"A",{href:!0});var frt=s(Mq);qar=r(frt,"TFTapasForQuestionAnswering"),frt.forEach(t),Gar=r(V3e," (TAPAS model)"),V3e.forEach(t),drt.forEach(t),Oar=i(ha),Tge=n(ha,"P",{});var mrt=s(Tge);Xar=r(mrt,"Examples:"),mrt.forEach(t),zar=i(ha),m(ty.$$.fragment,ha),ha.forEach(t),Nl.forEach(t),j0e=i(d),uc=n(d,"H2",{class:!0});var z9e=s(uc);zT=n(z9e,"A",{id:!0,class:!0,href:!0});var grt=s(zT);Fge=n(grt,"SPAN",{});var hrt=s(Fge);m(ay.$$.fragment,hrt),hrt.forEach(t),grt.forEach(t),Var=i(z9e),Cge=n(z9e,"SPAN",{});var prt=s(Cge);War=r(prt,"TFAutoModelForTokenClassification"),prt.forEach(t),z9e.forEach(t),N0e=i(d),Tr=n(d,"DIV",{class:!0});var ql=s(Tr);m(ny.$$.fragment,ql),Qar=i(ql),bc=n(ql,"P",{});var lz=s(bc);Har=r(lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Mge=n(lz,"CODE",{});var _rt=s(Mge);Uar=r(_rt,"from_pretrained()"),_rt.forEach(t),Jar=r(lz,"class method or the "),Ege=n(lz,"CODE",{});var urt=s(Ege);Yar=r(urt,"from_config()"),urt.forEach(t),Kar=r(lz,`class
method.`),lz.forEach(t),Zar=i(ql),sy=n(ql,"P",{});var V9e=s(sy);enr=r(V9e,"This class cannot be instantiated directly using "),yge=n(V9e,"CODE",{});var brt=s(yge);onr=r(brt,"__init__()"),brt.forEach(t),rnr=r(V9e," (throws an error)."),V9e.forEach(t),tnr=i(ql),mt=n(ql,"DIV",{class:!0});var Gl=s(mt);m(ly.$$.fragment,Gl),anr=i(Gl),wge=n(Gl,"P",{});var vrt=s(wge);nnr=r(vrt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),vrt.forEach(t),snr=i(Gl),vc=n(Gl,"P",{});var iz=s(vc);lnr=r(iz,`Note:
Loading a model from its configuration file does `),Age=n(iz,"STRONG",{});var Trt=s(Age);inr=r(Trt,"not"),Trt.forEach(t),dnr=r(iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lge=n(iz,"CODE",{});var Frt=s(Lge);cnr=r(Frt,"from_pretrained()"),Frt.forEach(t),fnr=r(iz,"to load the model weights."),iz.forEach(t),mnr=i(Gl),Bge=n(Gl,"P",{});var Crt=s(Bge);gnr=r(Crt,"Examples:"),Crt.forEach(t),hnr=i(Gl),m(iy.$$.fragment,Gl),Gl.forEach(t),pnr=i(ql),To=n(ql,"DIV",{class:!0});var pa=s(To);m(dy.$$.fragment,pa),_nr=i(pa),xge=n(pa,"P",{});var Mrt=s(xge);unr=r(Mrt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Mrt.forEach(t),bnr=i(pa),mn=n(pa,"P",{});var fC=s(mn);vnr=r(fC,"The model class to instantiate is selected based on the "),kge=n(fC,"CODE",{});var Ert=s(kge);Tnr=r(Ert,"model_type"),Ert.forEach(t),Fnr=r(fC,` property of the config object (either
passed as an argument or loaded from `),Rge=n(fC,"CODE",{});var yrt=s(Rge);Cnr=r(yrt,"pretrained_model_name_or_path"),yrt.forEach(t),Mnr=r(fC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sge=n(fC,"CODE",{});var wrt=s(Sge);Enr=r(wrt,"pretrained_model_name_or_path"),wrt.forEach(t),ynr=r(fC,":"),fC.forEach(t),wnr=i(pa),K=n(pa,"UL",{});var oe=s(K);VT=n(oe,"LI",{});var W3e=s(VT);Pge=n(W3e,"STRONG",{});var Art=s(Pge);Anr=r(Art,"albert"),Art.forEach(t),Lnr=r(W3e," \u2014 "),Eq=n(W3e,"A",{href:!0});var Lrt=s(Eq);Bnr=r(Lrt,"TFAlbertForTokenClassification"),Lrt.forEach(t),xnr=r(W3e," (ALBERT model)"),W3e.forEach(t),knr=i(oe),WT=n(oe,"LI",{});var Q3e=s(WT);$ge=n(Q3e,"STRONG",{});var Brt=s($ge);Rnr=r(Brt,"bert"),Brt.forEach(t),Snr=r(Q3e," \u2014 "),yq=n(Q3e,"A",{href:!0});var xrt=s(yq);Pnr=r(xrt,"TFBertForTokenClassification"),xrt.forEach(t),$nr=r(Q3e," (BERT model)"),Q3e.forEach(t),Inr=i(oe),QT=n(oe,"LI",{});var H3e=s(QT);Ige=n(H3e,"STRONG",{});var krt=s(Ige);jnr=r(krt,"camembert"),krt.forEach(t),Nnr=r(H3e," \u2014 "),wq=n(H3e,"A",{href:!0});var Rrt=s(wq);Dnr=r(Rrt,"TFCamembertForTokenClassification"),Rrt.forEach(t),qnr=r(H3e," (CamemBERT model)"),H3e.forEach(t),Gnr=i(oe),HT=n(oe,"LI",{});var U3e=s(HT);jge=n(U3e,"STRONG",{});var Srt=s(jge);Onr=r(Srt,"convbert"),Srt.forEach(t),Xnr=r(U3e," \u2014 "),Aq=n(U3e,"A",{href:!0});var Prt=s(Aq);znr=r(Prt,"TFConvBertForTokenClassification"),Prt.forEach(t),Vnr=r(U3e," (ConvBERT model)"),U3e.forEach(t),Wnr=i(oe),UT=n(oe,"LI",{});var J3e=s(UT);Nge=n(J3e,"STRONG",{});var $rt=s(Nge);Qnr=r($rt,"deberta"),$rt.forEach(t),Hnr=r(J3e," \u2014 "),Lq=n(J3e,"A",{href:!0});var Irt=s(Lq);Unr=r(Irt,"TFDebertaForTokenClassification"),Irt.forEach(t),Jnr=r(J3e," (DeBERTa model)"),J3e.forEach(t),Ynr=i(oe),JT=n(oe,"LI",{});var Y3e=s(JT);Dge=n(Y3e,"STRONG",{});var jrt=s(Dge);Knr=r(jrt,"deberta-v2"),jrt.forEach(t),Znr=r(Y3e," \u2014 "),Bq=n(Y3e,"A",{href:!0});var Nrt=s(Bq);esr=r(Nrt,"TFDebertaV2ForTokenClassification"),Nrt.forEach(t),osr=r(Y3e," (DeBERTa-v2 model)"),Y3e.forEach(t),rsr=i(oe),YT=n(oe,"LI",{});var K3e=s(YT);qge=n(K3e,"STRONG",{});var Drt=s(qge);tsr=r(Drt,"distilbert"),Drt.forEach(t),asr=r(K3e," \u2014 "),xq=n(K3e,"A",{href:!0});var qrt=s(xq);nsr=r(qrt,"TFDistilBertForTokenClassification"),qrt.forEach(t),ssr=r(K3e," (DistilBERT model)"),K3e.forEach(t),lsr=i(oe),KT=n(oe,"LI",{});var Z3e=s(KT);Gge=n(Z3e,"STRONG",{});var Grt=s(Gge);isr=r(Grt,"electra"),Grt.forEach(t),dsr=r(Z3e," \u2014 "),kq=n(Z3e,"A",{href:!0});var Ort=s(kq);csr=r(Ort,"TFElectraForTokenClassification"),Ort.forEach(t),fsr=r(Z3e," (ELECTRA model)"),Z3e.forEach(t),msr=i(oe),ZT=n(oe,"LI",{});var eye=s(ZT);Oge=n(eye,"STRONG",{});var Xrt=s(Oge);gsr=r(Xrt,"flaubert"),Xrt.forEach(t),hsr=r(eye," \u2014 "),Rq=n(eye,"A",{href:!0});var zrt=s(Rq);psr=r(zrt,"TFFlaubertForTokenClassification"),zrt.forEach(t),_sr=r(eye," (FlauBERT model)"),eye.forEach(t),usr=i(oe),e7=n(oe,"LI",{});var oye=s(e7);Xge=n(oye,"STRONG",{});var Vrt=s(Xge);bsr=r(Vrt,"funnel"),Vrt.forEach(t),vsr=r(oye," \u2014 "),Sq=n(oye,"A",{href:!0});var Wrt=s(Sq);Tsr=r(Wrt,"TFFunnelForTokenClassification"),Wrt.forEach(t),Fsr=r(oye," (Funnel Transformer model)"),oye.forEach(t),Csr=i(oe),o7=n(oe,"LI",{});var rye=s(o7);zge=n(rye,"STRONG",{});var Qrt=s(zge);Msr=r(Qrt,"layoutlm"),Qrt.forEach(t),Esr=r(rye," \u2014 "),Pq=n(rye,"A",{href:!0});var Hrt=s(Pq);ysr=r(Hrt,"TFLayoutLMForTokenClassification"),Hrt.forEach(t),wsr=r(rye," (LayoutLM model)"),rye.forEach(t),Asr=i(oe),r7=n(oe,"LI",{});var tye=s(r7);Vge=n(tye,"STRONG",{});var Urt=s(Vge);Lsr=r(Urt,"longformer"),Urt.forEach(t),Bsr=r(tye," \u2014 "),$q=n(tye,"A",{href:!0});var Jrt=s($q);xsr=r(Jrt,"TFLongformerForTokenClassification"),Jrt.forEach(t),ksr=r(tye," (Longformer model)"),tye.forEach(t),Rsr=i(oe),t7=n(oe,"LI",{});var aye=s(t7);Wge=n(aye,"STRONG",{});var Yrt=s(Wge);Ssr=r(Yrt,"mobilebert"),Yrt.forEach(t),Psr=r(aye," \u2014 "),Iq=n(aye,"A",{href:!0});var Krt=s(Iq);$sr=r(Krt,"TFMobileBertForTokenClassification"),Krt.forEach(t),Isr=r(aye," (MobileBERT model)"),aye.forEach(t),jsr=i(oe),a7=n(oe,"LI",{});var nye=s(a7);Qge=n(nye,"STRONG",{});var Zrt=s(Qge);Nsr=r(Zrt,"mpnet"),Zrt.forEach(t),Dsr=r(nye," \u2014 "),jq=n(nye,"A",{href:!0});var ett=s(jq);qsr=r(ett,"TFMPNetForTokenClassification"),ett.forEach(t),Gsr=r(nye," (MPNet model)"),nye.forEach(t),Osr=i(oe),n7=n(oe,"LI",{});var sye=s(n7);Hge=n(sye,"STRONG",{});var ott=s(Hge);Xsr=r(ott,"rembert"),ott.forEach(t),zsr=r(sye," \u2014 "),Nq=n(sye,"A",{href:!0});var rtt=s(Nq);Vsr=r(rtt,"TFRemBertForTokenClassification"),rtt.forEach(t),Wsr=r(sye," (RemBERT model)"),sye.forEach(t),Qsr=i(oe),s7=n(oe,"LI",{});var lye=s(s7);Uge=n(lye,"STRONG",{});var ttt=s(Uge);Hsr=r(ttt,"roberta"),ttt.forEach(t),Usr=r(lye," \u2014 "),Dq=n(lye,"A",{href:!0});var att=s(Dq);Jsr=r(att,"TFRobertaForTokenClassification"),att.forEach(t),Ysr=r(lye," (RoBERTa model)"),lye.forEach(t),Ksr=i(oe),l7=n(oe,"LI",{});var iye=s(l7);Jge=n(iye,"STRONG",{});var ntt=s(Jge);Zsr=r(ntt,"roformer"),ntt.forEach(t),elr=r(iye," \u2014 "),qq=n(iye,"A",{href:!0});var stt=s(qq);olr=r(stt,"TFRoFormerForTokenClassification"),stt.forEach(t),rlr=r(iye," (RoFormer model)"),iye.forEach(t),tlr=i(oe),i7=n(oe,"LI",{});var dye=s(i7);Yge=n(dye,"STRONG",{});var ltt=s(Yge);alr=r(ltt,"xlm"),ltt.forEach(t),nlr=r(dye," \u2014 "),Gq=n(dye,"A",{href:!0});var itt=s(Gq);slr=r(itt,"TFXLMForTokenClassification"),itt.forEach(t),llr=r(dye," (XLM model)"),dye.forEach(t),ilr=i(oe),d7=n(oe,"LI",{});var cye=s(d7);Kge=n(cye,"STRONG",{});var dtt=s(Kge);dlr=r(dtt,"xlm-roberta"),dtt.forEach(t),clr=r(cye," \u2014 "),Oq=n(cye,"A",{href:!0});var ctt=s(Oq);flr=r(ctt,"TFXLMRobertaForTokenClassification"),ctt.forEach(t),mlr=r(cye," (XLM-RoBERTa model)"),cye.forEach(t),glr=i(oe),c7=n(oe,"LI",{});var fye=s(c7);Zge=n(fye,"STRONG",{});var ftt=s(Zge);hlr=r(ftt,"xlnet"),ftt.forEach(t),plr=r(fye," \u2014 "),Xq=n(fye,"A",{href:!0});var mtt=s(Xq);_lr=r(mtt,"TFXLNetForTokenClassification"),mtt.forEach(t),ulr=r(fye," (XLNet model)"),fye.forEach(t),oe.forEach(t),blr=i(pa),ehe=n(pa,"P",{});var gtt=s(ehe);vlr=r(gtt,"Examples:"),gtt.forEach(t),Tlr=i(pa),m(cy.$$.fragment,pa),pa.forEach(t),ql.forEach(t),D0e=i(d),Tc=n(d,"H2",{class:!0});var W9e=s(Tc);f7=n(W9e,"A",{id:!0,class:!0,href:!0});var htt=s(f7);ohe=n(htt,"SPAN",{});var ptt=s(ohe);m(fy.$$.fragment,ptt),ptt.forEach(t),htt.forEach(t),Flr=i(W9e),rhe=n(W9e,"SPAN",{});var _tt=s(rhe);Clr=r(_tt,"TFAutoModelForQuestionAnswering"),_tt.forEach(t),W9e.forEach(t),q0e=i(d),Fr=n(d,"DIV",{class:!0});var Ol=s(Fr);m(my.$$.fragment,Ol),Mlr=i(Ol),Fc=n(Ol,"P",{});var dz=s(Fc);Elr=r(dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),the=n(dz,"CODE",{});var utt=s(the);ylr=r(utt,"from_pretrained()"),utt.forEach(t),wlr=r(dz,"class method or the "),ahe=n(dz,"CODE",{});var btt=s(ahe);Alr=r(btt,"from_config()"),btt.forEach(t),Llr=r(dz,`class
method.`),dz.forEach(t),Blr=i(Ol),gy=n(Ol,"P",{});var Q9e=s(gy);xlr=r(Q9e,"This class cannot be instantiated directly using "),nhe=n(Q9e,"CODE",{});var vtt=s(nhe);klr=r(vtt,"__init__()"),vtt.forEach(t),Rlr=r(Q9e," (throws an error)."),Q9e.forEach(t),Slr=i(Ol),gt=n(Ol,"DIV",{class:!0});var Xl=s(gt);m(hy.$$.fragment,Xl),Plr=i(Xl),she=n(Xl,"P",{});var Ttt=s(she);$lr=r(Ttt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Ttt.forEach(t),Ilr=i(Xl),Cc=n(Xl,"P",{});var cz=s(Cc);jlr=r(cz,`Note:
Loading a model from its configuration file does `),lhe=n(cz,"STRONG",{});var Ftt=s(lhe);Nlr=r(Ftt,"not"),Ftt.forEach(t),Dlr=r(cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ihe=n(cz,"CODE",{});var Ctt=s(ihe);qlr=r(Ctt,"from_pretrained()"),Ctt.forEach(t),Glr=r(cz,"to load the model weights."),cz.forEach(t),Olr=i(Xl),dhe=n(Xl,"P",{});var Mtt=s(dhe);Xlr=r(Mtt,"Examples:"),Mtt.forEach(t),zlr=i(Xl),m(py.$$.fragment,Xl),Xl.forEach(t),Vlr=i(Ol),Fo=n(Ol,"DIV",{class:!0});var _a=s(Fo);m(_y.$$.fragment,_a),Wlr=i(_a),che=n(_a,"P",{});var Ett=s(che);Qlr=r(Ett,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Ett.forEach(t),Hlr=i(_a),gn=n(_a,"P",{});var mC=s(gn);Ulr=r(mC,"The model class to instantiate is selected based on the "),fhe=n(mC,"CODE",{});var ytt=s(fhe);Jlr=r(ytt,"model_type"),ytt.forEach(t),Ylr=r(mC,` property of the config object (either
passed as an argument or loaded from `),mhe=n(mC,"CODE",{});var wtt=s(mhe);Klr=r(wtt,"pretrained_model_name_or_path"),wtt.forEach(t),Zlr=r(mC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ghe=n(mC,"CODE",{});var Att=s(ghe);eir=r(Att,"pretrained_model_name_or_path"),Att.forEach(t),oir=r(mC,":"),mC.forEach(t),rir=i(_a),Z=n(_a,"UL",{});var re=s(Z);m7=n(re,"LI",{});var mye=s(m7);hhe=n(mye,"STRONG",{});var Ltt=s(hhe);tir=r(Ltt,"albert"),Ltt.forEach(t),air=r(mye," \u2014 "),zq=n(mye,"A",{href:!0});var Btt=s(zq);nir=r(Btt,"TFAlbertForQuestionAnswering"),Btt.forEach(t),sir=r(mye," (ALBERT model)"),mye.forEach(t),lir=i(re),g7=n(re,"LI",{});var gye=s(g7);phe=n(gye,"STRONG",{});var xtt=s(phe);iir=r(xtt,"bert"),xtt.forEach(t),dir=r(gye," \u2014 "),Vq=n(gye,"A",{href:!0});var ktt=s(Vq);cir=r(ktt,"TFBertForQuestionAnswering"),ktt.forEach(t),fir=r(gye," (BERT model)"),gye.forEach(t),mir=i(re),h7=n(re,"LI",{});var hye=s(h7);_he=n(hye,"STRONG",{});var Rtt=s(_he);gir=r(Rtt,"camembert"),Rtt.forEach(t),hir=r(hye," \u2014 "),Wq=n(hye,"A",{href:!0});var Stt=s(Wq);pir=r(Stt,"TFCamembertForQuestionAnswering"),Stt.forEach(t),_ir=r(hye," (CamemBERT model)"),hye.forEach(t),uir=i(re),p7=n(re,"LI",{});var pye=s(p7);uhe=n(pye,"STRONG",{});var Ptt=s(uhe);bir=r(Ptt,"convbert"),Ptt.forEach(t),vir=r(pye," \u2014 "),Qq=n(pye,"A",{href:!0});var $tt=s(Qq);Tir=r($tt,"TFConvBertForQuestionAnswering"),$tt.forEach(t),Fir=r(pye," (ConvBERT model)"),pye.forEach(t),Cir=i(re),_7=n(re,"LI",{});var _ye=s(_7);bhe=n(_ye,"STRONG",{});var Itt=s(bhe);Mir=r(Itt,"deberta"),Itt.forEach(t),Eir=r(_ye," \u2014 "),Hq=n(_ye,"A",{href:!0});var jtt=s(Hq);yir=r(jtt,"TFDebertaForQuestionAnswering"),jtt.forEach(t),wir=r(_ye," (DeBERTa model)"),_ye.forEach(t),Air=i(re),u7=n(re,"LI",{});var uye=s(u7);vhe=n(uye,"STRONG",{});var Ntt=s(vhe);Lir=r(Ntt,"deberta-v2"),Ntt.forEach(t),Bir=r(uye," \u2014 "),Uq=n(uye,"A",{href:!0});var Dtt=s(Uq);xir=r(Dtt,"TFDebertaV2ForQuestionAnswering"),Dtt.forEach(t),kir=r(uye," (DeBERTa-v2 model)"),uye.forEach(t),Rir=i(re),b7=n(re,"LI",{});var bye=s(b7);The=n(bye,"STRONG",{});var qtt=s(The);Sir=r(qtt,"distilbert"),qtt.forEach(t),Pir=r(bye," \u2014 "),Jq=n(bye,"A",{href:!0});var Gtt=s(Jq);$ir=r(Gtt,"TFDistilBertForQuestionAnswering"),Gtt.forEach(t),Iir=r(bye," (DistilBERT model)"),bye.forEach(t),jir=i(re),v7=n(re,"LI",{});var vye=s(v7);Fhe=n(vye,"STRONG",{});var Ott=s(Fhe);Nir=r(Ott,"electra"),Ott.forEach(t),Dir=r(vye," \u2014 "),Yq=n(vye,"A",{href:!0});var Xtt=s(Yq);qir=r(Xtt,"TFElectraForQuestionAnswering"),Xtt.forEach(t),Gir=r(vye," (ELECTRA model)"),vye.forEach(t),Oir=i(re),T7=n(re,"LI",{});var Tye=s(T7);Che=n(Tye,"STRONG",{});var ztt=s(Che);Xir=r(ztt,"flaubert"),ztt.forEach(t),zir=r(Tye," \u2014 "),Kq=n(Tye,"A",{href:!0});var Vtt=s(Kq);Vir=r(Vtt,"TFFlaubertForQuestionAnsweringSimple"),Vtt.forEach(t),Wir=r(Tye," (FlauBERT model)"),Tye.forEach(t),Qir=i(re),F7=n(re,"LI",{});var Fye=s(F7);Mhe=n(Fye,"STRONG",{});var Wtt=s(Mhe);Hir=r(Wtt,"funnel"),Wtt.forEach(t),Uir=r(Fye," \u2014 "),Zq=n(Fye,"A",{href:!0});var Qtt=s(Zq);Jir=r(Qtt,"TFFunnelForQuestionAnswering"),Qtt.forEach(t),Yir=r(Fye," (Funnel Transformer model)"),Fye.forEach(t),Kir=i(re),C7=n(re,"LI",{});var Cye=s(C7);Ehe=n(Cye,"STRONG",{});var Htt=s(Ehe);Zir=r(Htt,"longformer"),Htt.forEach(t),edr=r(Cye," \u2014 "),eG=n(Cye,"A",{href:!0});var Utt=s(eG);odr=r(Utt,"TFLongformerForQuestionAnswering"),Utt.forEach(t),rdr=r(Cye," (Longformer model)"),Cye.forEach(t),tdr=i(re),M7=n(re,"LI",{});var Mye=s(M7);yhe=n(Mye,"STRONG",{});var Jtt=s(yhe);adr=r(Jtt,"mobilebert"),Jtt.forEach(t),ndr=r(Mye," \u2014 "),oG=n(Mye,"A",{href:!0});var Ytt=s(oG);sdr=r(Ytt,"TFMobileBertForQuestionAnswering"),Ytt.forEach(t),ldr=r(Mye," (MobileBERT model)"),Mye.forEach(t),idr=i(re),E7=n(re,"LI",{});var Eye=s(E7);whe=n(Eye,"STRONG",{});var Ktt=s(whe);ddr=r(Ktt,"mpnet"),Ktt.forEach(t),cdr=r(Eye," \u2014 "),rG=n(Eye,"A",{href:!0});var Ztt=s(rG);fdr=r(Ztt,"TFMPNetForQuestionAnswering"),Ztt.forEach(t),mdr=r(Eye," (MPNet model)"),Eye.forEach(t),gdr=i(re),y7=n(re,"LI",{});var yye=s(y7);Ahe=n(yye,"STRONG",{});var eat=s(Ahe);hdr=r(eat,"rembert"),eat.forEach(t),pdr=r(yye," \u2014 "),tG=n(yye,"A",{href:!0});var oat=s(tG);_dr=r(oat,"TFRemBertForQuestionAnswering"),oat.forEach(t),udr=r(yye," (RemBERT model)"),yye.forEach(t),bdr=i(re),w7=n(re,"LI",{});var wye=s(w7);Lhe=n(wye,"STRONG",{});var rat=s(Lhe);vdr=r(rat,"roberta"),rat.forEach(t),Tdr=r(wye," \u2014 "),aG=n(wye,"A",{href:!0});var tat=s(aG);Fdr=r(tat,"TFRobertaForQuestionAnswering"),tat.forEach(t),Cdr=r(wye," (RoBERTa model)"),wye.forEach(t),Mdr=i(re),A7=n(re,"LI",{});var Aye=s(A7);Bhe=n(Aye,"STRONG",{});var aat=s(Bhe);Edr=r(aat,"roformer"),aat.forEach(t),ydr=r(Aye," \u2014 "),nG=n(Aye,"A",{href:!0});var nat=s(nG);wdr=r(nat,"TFRoFormerForQuestionAnswering"),nat.forEach(t),Adr=r(Aye," (RoFormer model)"),Aye.forEach(t),Ldr=i(re),L7=n(re,"LI",{});var Lye=s(L7);xhe=n(Lye,"STRONG",{});var sat=s(xhe);Bdr=r(sat,"xlm"),sat.forEach(t),xdr=r(Lye," \u2014 "),sG=n(Lye,"A",{href:!0});var lat=s(sG);kdr=r(lat,"TFXLMForQuestionAnsweringSimple"),lat.forEach(t),Rdr=r(Lye," (XLM model)"),Lye.forEach(t),Sdr=i(re),B7=n(re,"LI",{});var Bye=s(B7);khe=n(Bye,"STRONG",{});var iat=s(khe);Pdr=r(iat,"xlm-roberta"),iat.forEach(t),$dr=r(Bye," \u2014 "),lG=n(Bye,"A",{href:!0});var dat=s(lG);Idr=r(dat,"TFXLMRobertaForQuestionAnswering"),dat.forEach(t),jdr=r(Bye," (XLM-RoBERTa model)"),Bye.forEach(t),Ndr=i(re),x7=n(re,"LI",{});var xye=s(x7);Rhe=n(xye,"STRONG",{});var cat=s(Rhe);Ddr=r(cat,"xlnet"),cat.forEach(t),qdr=r(xye," \u2014 "),iG=n(xye,"A",{href:!0});var fat=s(iG);Gdr=r(fat,"TFXLNetForQuestionAnsweringSimple"),fat.forEach(t),Odr=r(xye," (XLNet model)"),xye.forEach(t),re.forEach(t),Xdr=i(_a),She=n(_a,"P",{});var mat=s(She);zdr=r(mat,"Examples:"),mat.forEach(t),Vdr=i(_a),m(uy.$$.fragment,_a),_a.forEach(t),Ol.forEach(t),G0e=i(d),Mc=n(d,"H2",{class:!0});var H9e=s(Mc);k7=n(H9e,"A",{id:!0,class:!0,href:!0});var gat=s(k7);Phe=n(gat,"SPAN",{});var hat=s(Phe);m(by.$$.fragment,hat),hat.forEach(t),gat.forEach(t),Wdr=i(H9e),$he=n(H9e,"SPAN",{});var pat=s($he);Qdr=r(pat,"TFAutoModelForVision2Seq"),pat.forEach(t),H9e.forEach(t),O0e=i(d),Cr=n(d,"DIV",{class:!0});var zl=s(Cr);m(vy.$$.fragment,zl),Hdr=i(zl),Ec=n(zl,"P",{});var fz=s(Ec);Udr=r(fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ihe=n(fz,"CODE",{});var _at=s(Ihe);Jdr=r(_at,"from_pretrained()"),_at.forEach(t),Ydr=r(fz,"class method or the "),jhe=n(fz,"CODE",{});var uat=s(jhe);Kdr=r(uat,"from_config()"),uat.forEach(t),Zdr=r(fz,`class
method.`),fz.forEach(t),ecr=i(zl),Ty=n(zl,"P",{});var U9e=s(Ty);ocr=r(U9e,"This class cannot be instantiated directly using "),Nhe=n(U9e,"CODE",{});var bat=s(Nhe);rcr=r(bat,"__init__()"),bat.forEach(t),tcr=r(U9e," (throws an error)."),U9e.forEach(t),acr=i(zl),ht=n(zl,"DIV",{class:!0});var Vl=s(ht);m(Fy.$$.fragment,Vl),ncr=i(Vl),Dhe=n(Vl,"P",{});var vat=s(Dhe);scr=r(vat,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),vat.forEach(t),lcr=i(Vl),yc=n(Vl,"P",{});var mz=s(yc);icr=r(mz,`Note:
Loading a model from its configuration file does `),qhe=n(mz,"STRONG",{});var Tat=s(qhe);dcr=r(Tat,"not"),Tat.forEach(t),ccr=r(mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ghe=n(mz,"CODE",{});var Fat=s(Ghe);fcr=r(Fat,"from_pretrained()"),Fat.forEach(t),mcr=r(mz,"to load the model weights."),mz.forEach(t),gcr=i(Vl),Ohe=n(Vl,"P",{});var Cat=s(Ohe);hcr=r(Cat,"Examples:"),Cat.forEach(t),pcr=i(Vl),m(Cy.$$.fragment,Vl),Vl.forEach(t),_cr=i(zl),Co=n(zl,"DIV",{class:!0});var ua=s(Co);m(My.$$.fragment,ua),ucr=i(ua),Xhe=n(ua,"P",{});var Mat=s(Xhe);bcr=r(Mat,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Mat.forEach(t),vcr=i(ua),hn=n(ua,"P",{});var gC=s(hn);Tcr=r(gC,"The model class to instantiate is selected based on the "),zhe=n(gC,"CODE",{});var Eat=s(zhe);Fcr=r(Eat,"model_type"),Eat.forEach(t),Ccr=r(gC,` property of the config object (either
passed as an argument or loaded from `),Vhe=n(gC,"CODE",{});var yat=s(Vhe);Mcr=r(yat,"pretrained_model_name_or_path"),yat.forEach(t),Ecr=r(gC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Whe=n(gC,"CODE",{});var wat=s(Whe);ycr=r(wat,"pretrained_model_name_or_path"),wat.forEach(t),wcr=r(gC,":"),gC.forEach(t),Acr=i(ua),Qhe=n(ua,"UL",{});var Aat=s(Qhe);R7=n(Aat,"LI",{});var kye=s(R7);Hhe=n(kye,"STRONG",{});var Lat=s(Hhe);Lcr=r(Lat,"vision-encoder-decoder"),Lat.forEach(t),Bcr=r(kye," \u2014 "),dG=n(kye,"A",{href:!0});var Bat=s(dG);xcr=r(Bat,"TFVisionEncoderDecoderModel"),Bat.forEach(t),kcr=r(kye," (Vision Encoder decoder model)"),kye.forEach(t),Aat.forEach(t),Rcr=i(ua),Uhe=n(ua,"P",{});var xat=s(Uhe);Scr=r(xat,"Examples:"),xat.forEach(t),Pcr=i(ua),m(Ey.$$.fragment,ua),ua.forEach(t),zl.forEach(t),X0e=i(d),wc=n(d,"H2",{class:!0});var J9e=s(wc);S7=n(J9e,"A",{id:!0,class:!0,href:!0});var kat=s(S7);Jhe=n(kat,"SPAN",{});var Rat=s(Jhe);m(yy.$$.fragment,Rat),Rat.forEach(t),kat.forEach(t),$cr=i(J9e),Yhe=n(J9e,"SPAN",{});var Sat=s(Yhe);Icr=r(Sat,"TFAutoModelForSpeechSeq2Seq"),Sat.forEach(t),J9e.forEach(t),z0e=i(d),Mr=n(d,"DIV",{class:!0});var Wl=s(Mr);m(wy.$$.fragment,Wl),jcr=i(Wl),Ac=n(Wl,"P",{});var gz=s(Ac);Ncr=r(gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Khe=n(gz,"CODE",{});var Pat=s(Khe);Dcr=r(Pat,"from_pretrained()"),Pat.forEach(t),qcr=r(gz,"class method or the "),Zhe=n(gz,"CODE",{});var $at=s(Zhe);Gcr=r($at,"from_config()"),$at.forEach(t),Ocr=r(gz,`class
method.`),gz.forEach(t),Xcr=i(Wl),Ay=n(Wl,"P",{});var Y9e=s(Ay);zcr=r(Y9e,"This class cannot be instantiated directly using "),epe=n(Y9e,"CODE",{});var Iat=s(epe);Vcr=r(Iat,"__init__()"),Iat.forEach(t),Wcr=r(Y9e," (throws an error)."),Y9e.forEach(t),Qcr=i(Wl),pt=n(Wl,"DIV",{class:!0});var Ql=s(pt);m(Ly.$$.fragment,Ql),Hcr=i(Ql),ope=n(Ql,"P",{});var jat=s(ope);Ucr=r(jat,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),jat.forEach(t),Jcr=i(Ql),Lc=n(Ql,"P",{});var hz=s(Lc);Ycr=r(hz,`Note:
Loading a model from its configuration file does `),rpe=n(hz,"STRONG",{});var Nat=s(rpe);Kcr=r(Nat,"not"),Nat.forEach(t),Zcr=r(hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tpe=n(hz,"CODE",{});var Dat=s(tpe);efr=r(Dat,"from_pretrained()"),Dat.forEach(t),ofr=r(hz,"to load the model weights."),hz.forEach(t),rfr=i(Ql),ape=n(Ql,"P",{});var qat=s(ape);tfr=r(qat,"Examples:"),qat.forEach(t),afr=i(Ql),m(By.$$.fragment,Ql),Ql.forEach(t),nfr=i(Wl),Mo=n(Wl,"DIV",{class:!0});var ba=s(Mo);m(xy.$$.fragment,ba),sfr=i(ba),npe=n(ba,"P",{});var Gat=s(npe);lfr=r(Gat,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Gat.forEach(t),ifr=i(ba),pn=n(ba,"P",{});var hC=s(pn);dfr=r(hC,"The model class to instantiate is selected based on the "),spe=n(hC,"CODE",{});var Oat=s(spe);cfr=r(Oat,"model_type"),Oat.forEach(t),ffr=r(hC,` property of the config object (either
passed as an argument or loaded from `),lpe=n(hC,"CODE",{});var Xat=s(lpe);mfr=r(Xat,"pretrained_model_name_or_path"),Xat.forEach(t),gfr=r(hC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ipe=n(hC,"CODE",{});var zat=s(ipe);hfr=r(zat,"pretrained_model_name_or_path"),zat.forEach(t),pfr=r(hC,":"),hC.forEach(t),_fr=i(ba),dpe=n(ba,"UL",{});var Vat=s(dpe);P7=n(Vat,"LI",{});var Rye=s(P7);cpe=n(Rye,"STRONG",{});var Wat=s(cpe);ufr=r(Wat,"speech_to_text"),Wat.forEach(t),bfr=r(Rye," \u2014 "),cG=n(Rye,"A",{href:!0});var Qat=s(cG);vfr=r(Qat,"TFSpeech2TextForConditionalGeneration"),Qat.forEach(t),Tfr=r(Rye," (Speech2Text model)"),Rye.forEach(t),Vat.forEach(t),Ffr=i(ba),fpe=n(ba,"P",{});var Hat=s(fpe);Cfr=r(Hat,"Examples:"),Hat.forEach(t),Mfr=i(ba),m(ky.$$.fragment,ba),ba.forEach(t),Wl.forEach(t),V0e=i(d),Bc=n(d,"H2",{class:!0});var K9e=s(Bc);$7=n(K9e,"A",{id:!0,class:!0,href:!0});var Uat=s($7);mpe=n(Uat,"SPAN",{});var Jat=s(mpe);m(Ry.$$.fragment,Jat),Jat.forEach(t),Uat.forEach(t),Efr=i(K9e),gpe=n(K9e,"SPAN",{});var Yat=s(gpe);yfr=r(Yat,"FlaxAutoModel"),Yat.forEach(t),K9e.forEach(t),W0e=i(d),Er=n(d,"DIV",{class:!0});var Hl=s(Er);m(Sy.$$.fragment,Hl),wfr=i(Hl),xc=n(Hl,"P",{});var pz=s(xc);Afr=r(pz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),hpe=n(pz,"CODE",{});var Kat=s(hpe);Lfr=r(Kat,"from_pretrained()"),Kat.forEach(t),Bfr=r(pz,"class method or the "),ppe=n(pz,"CODE",{});var Zat=s(ppe);xfr=r(Zat,"from_config()"),Zat.forEach(t),kfr=r(pz,`class
method.`),pz.forEach(t),Rfr=i(Hl),Py=n(Hl,"P",{});var Z9e=s(Py);Sfr=r(Z9e,"This class cannot be instantiated directly using "),_pe=n(Z9e,"CODE",{});var ent=s(_pe);Pfr=r(ent,"__init__()"),ent.forEach(t),$fr=r(Z9e," (throws an error)."),Z9e.forEach(t),Ifr=i(Hl),_t=n(Hl,"DIV",{class:!0});var Ul=s(_t);m($y.$$.fragment,Ul),jfr=i(Ul),upe=n(Ul,"P",{});var ont=s(upe);Nfr=r(ont,"Instantiates one of the base model classes of the library from a configuration."),ont.forEach(t),Dfr=i(Ul),kc=n(Ul,"P",{});var _z=s(kc);qfr=r(_z,`Note:
Loading a model from its configuration file does `),bpe=n(_z,"STRONG",{});var rnt=s(bpe);Gfr=r(rnt,"not"),rnt.forEach(t),Ofr=r(_z,` load the model weights. It only affects the
model\u2019s configuration. Use `),vpe=n(_z,"CODE",{});var tnt=s(vpe);Xfr=r(tnt,"from_pretrained()"),tnt.forEach(t),zfr=r(_z,"to load the model weights."),_z.forEach(t),Vfr=i(Ul),Tpe=n(Ul,"P",{});var ant=s(Tpe);Wfr=r(ant,"Examples:"),ant.forEach(t),Qfr=i(Ul),m(Iy.$$.fragment,Ul),Ul.forEach(t),Hfr=i(Hl),Eo=n(Hl,"DIV",{class:!0});var va=s(Eo);m(jy.$$.fragment,va),Ufr=i(va),Fpe=n(va,"P",{});var nnt=s(Fpe);Jfr=r(nnt,"Instantiate one of the base model classes of the library from a pretrained model."),nnt.forEach(t),Yfr=i(va),_n=n(va,"P",{});var pC=s(_n);Kfr=r(pC,"The model class to instantiate is selected based on the "),Cpe=n(pC,"CODE",{});var snt=s(Cpe);Zfr=r(snt,"model_type"),snt.forEach(t),emr=r(pC,` property of the config object (either
passed as an argument or loaded from `),Mpe=n(pC,"CODE",{});var lnt=s(Mpe);omr=r(lnt,"pretrained_model_name_or_path"),lnt.forEach(t),rmr=r(pC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Epe=n(pC,"CODE",{});var int=s(Epe);tmr=r(int,"pretrained_model_name_or_path"),int.forEach(t),amr=r(pC,":"),pC.forEach(t),nmr=i(va),V=n(va,"UL",{});var Q=s(V);I7=n(Q,"LI",{});var Sye=s(I7);ype=n(Sye,"STRONG",{});var dnt=s(ype);smr=r(dnt,"albert"),dnt.forEach(t),lmr=r(Sye," \u2014 "),fG=n(Sye,"A",{href:!0});var cnt=s(fG);imr=r(cnt,"FlaxAlbertModel"),cnt.forEach(t),dmr=r(Sye," (ALBERT model)"),Sye.forEach(t),cmr=i(Q),j7=n(Q,"LI",{});var Pye=s(j7);wpe=n(Pye,"STRONG",{});var fnt=s(wpe);fmr=r(fnt,"bart"),fnt.forEach(t),mmr=r(Pye," \u2014 "),mG=n(Pye,"A",{href:!0});var mnt=s(mG);gmr=r(mnt,"FlaxBartModel"),mnt.forEach(t),hmr=r(Pye," (BART model)"),Pye.forEach(t),pmr=i(Q),N7=n(Q,"LI",{});var $ye=s(N7);Ape=n($ye,"STRONG",{});var gnt=s(Ape);_mr=r(gnt,"beit"),gnt.forEach(t),umr=r($ye," \u2014 "),gG=n($ye,"A",{href:!0});var hnt=s(gG);bmr=r(hnt,"FlaxBeitModel"),hnt.forEach(t),vmr=r($ye," (BEiT model)"),$ye.forEach(t),Tmr=i(Q),D7=n(Q,"LI",{});var Iye=s(D7);Lpe=n(Iye,"STRONG",{});var pnt=s(Lpe);Fmr=r(pnt,"bert"),pnt.forEach(t),Cmr=r(Iye," \u2014 "),hG=n(Iye,"A",{href:!0});var _nt=s(hG);Mmr=r(_nt,"FlaxBertModel"),_nt.forEach(t),Emr=r(Iye," (BERT model)"),Iye.forEach(t),ymr=i(Q),q7=n(Q,"LI",{});var jye=s(q7);Bpe=n(jye,"STRONG",{});var unt=s(Bpe);wmr=r(unt,"big_bird"),unt.forEach(t),Amr=r(jye," \u2014 "),pG=n(jye,"A",{href:!0});var bnt=s(pG);Lmr=r(bnt,"FlaxBigBirdModel"),bnt.forEach(t),Bmr=r(jye," (BigBird model)"),jye.forEach(t),xmr=i(Q),G7=n(Q,"LI",{});var Nye=s(G7);xpe=n(Nye,"STRONG",{});var vnt=s(xpe);kmr=r(vnt,"blenderbot"),vnt.forEach(t),Rmr=r(Nye," \u2014 "),_G=n(Nye,"A",{href:!0});var Tnt=s(_G);Smr=r(Tnt,"FlaxBlenderbotModel"),Tnt.forEach(t),Pmr=r(Nye," (Blenderbot model)"),Nye.forEach(t),$mr=i(Q),O7=n(Q,"LI",{});var Dye=s(O7);kpe=n(Dye,"STRONG",{});var Fnt=s(kpe);Imr=r(Fnt,"blenderbot-small"),Fnt.forEach(t),jmr=r(Dye," \u2014 "),uG=n(Dye,"A",{href:!0});var Cnt=s(uG);Nmr=r(Cnt,"FlaxBlenderbotSmallModel"),Cnt.forEach(t),Dmr=r(Dye," (BlenderbotSmall model)"),Dye.forEach(t),qmr=i(Q),X7=n(Q,"LI",{});var qye=s(X7);Rpe=n(qye,"STRONG",{});var Mnt=s(Rpe);Gmr=r(Mnt,"clip"),Mnt.forEach(t),Omr=r(qye," \u2014 "),bG=n(qye,"A",{href:!0});var Ent=s(bG);Xmr=r(Ent,"FlaxCLIPModel"),Ent.forEach(t),zmr=r(qye," (CLIP model)"),qye.forEach(t),Vmr=i(Q),z7=n(Q,"LI",{});var Gye=s(z7);Spe=n(Gye,"STRONG",{});var ynt=s(Spe);Wmr=r(ynt,"distilbert"),ynt.forEach(t),Qmr=r(Gye," \u2014 "),vG=n(Gye,"A",{href:!0});var wnt=s(vG);Hmr=r(wnt,"FlaxDistilBertModel"),wnt.forEach(t),Umr=r(Gye," (DistilBERT model)"),Gye.forEach(t),Jmr=i(Q),V7=n(Q,"LI",{});var Oye=s(V7);Ppe=n(Oye,"STRONG",{});var Ant=s(Ppe);Ymr=r(Ant,"electra"),Ant.forEach(t),Kmr=r(Oye," \u2014 "),TG=n(Oye,"A",{href:!0});var Lnt=s(TG);Zmr=r(Lnt,"FlaxElectraModel"),Lnt.forEach(t),egr=r(Oye," (ELECTRA model)"),Oye.forEach(t),ogr=i(Q),W7=n(Q,"LI",{});var Xye=s(W7);$pe=n(Xye,"STRONG",{});var Bnt=s($pe);rgr=r(Bnt,"gpt2"),Bnt.forEach(t),tgr=r(Xye," \u2014 "),FG=n(Xye,"A",{href:!0});var xnt=s(FG);agr=r(xnt,"FlaxGPT2Model"),xnt.forEach(t),ngr=r(Xye," (OpenAI GPT-2 model)"),Xye.forEach(t),sgr=i(Q),Q7=n(Q,"LI",{});var zye=s(Q7);Ipe=n(zye,"STRONG",{});var knt=s(Ipe);lgr=r(knt,"gpt_neo"),knt.forEach(t),igr=r(zye," \u2014 "),CG=n(zye,"A",{href:!0});var Rnt=s(CG);dgr=r(Rnt,"FlaxGPTNeoModel"),Rnt.forEach(t),cgr=r(zye," (GPT Neo model)"),zye.forEach(t),fgr=i(Q),H7=n(Q,"LI",{});var Vye=s(H7);jpe=n(Vye,"STRONG",{});var Snt=s(jpe);mgr=r(Snt,"gptj"),Snt.forEach(t),ggr=r(Vye," \u2014 "),MG=n(Vye,"A",{href:!0});var Pnt=s(MG);hgr=r(Pnt,"FlaxGPTJModel"),Pnt.forEach(t),pgr=r(Vye," (GPT-J model)"),Vye.forEach(t),_gr=i(Q),U7=n(Q,"LI",{});var Wye=s(U7);Npe=n(Wye,"STRONG",{});var $nt=s(Npe);ugr=r($nt,"marian"),$nt.forEach(t),bgr=r(Wye," \u2014 "),EG=n(Wye,"A",{href:!0});var Int=s(EG);vgr=r(Int,"FlaxMarianModel"),Int.forEach(t),Tgr=r(Wye," (Marian model)"),Wye.forEach(t),Fgr=i(Q),J7=n(Q,"LI",{});var Qye=s(J7);Dpe=n(Qye,"STRONG",{});var jnt=s(Dpe);Cgr=r(jnt,"mbart"),jnt.forEach(t),Mgr=r(Qye," \u2014 "),yG=n(Qye,"A",{href:!0});var Nnt=s(yG);Egr=r(Nnt,"FlaxMBartModel"),Nnt.forEach(t),ygr=r(Qye," (mBART model)"),Qye.forEach(t),wgr=i(Q),Y7=n(Q,"LI",{});var Hye=s(Y7);qpe=n(Hye,"STRONG",{});var Dnt=s(qpe);Agr=r(Dnt,"mt5"),Dnt.forEach(t),Lgr=r(Hye," \u2014 "),wG=n(Hye,"A",{href:!0});var qnt=s(wG);Bgr=r(qnt,"FlaxMT5Model"),qnt.forEach(t),xgr=r(Hye," (mT5 model)"),Hye.forEach(t),kgr=i(Q),K7=n(Q,"LI",{});var Uye=s(K7);Gpe=n(Uye,"STRONG",{});var Gnt=s(Gpe);Rgr=r(Gnt,"pegasus"),Gnt.forEach(t),Sgr=r(Uye," \u2014 "),AG=n(Uye,"A",{href:!0});var Ont=s(AG);Pgr=r(Ont,"FlaxPegasusModel"),Ont.forEach(t),$gr=r(Uye," (Pegasus model)"),Uye.forEach(t),Igr=i(Q),Z7=n(Q,"LI",{});var Jye=s(Z7);Ope=n(Jye,"STRONG",{});var Xnt=s(Ope);jgr=r(Xnt,"roberta"),Xnt.forEach(t),Ngr=r(Jye," \u2014 "),LG=n(Jye,"A",{href:!0});var znt=s(LG);Dgr=r(znt,"FlaxRobertaModel"),znt.forEach(t),qgr=r(Jye," (RoBERTa model)"),Jye.forEach(t),Ggr=i(Q),e8=n(Q,"LI",{});var Yye=s(e8);Xpe=n(Yye,"STRONG",{});var Vnt=s(Xpe);Ogr=r(Vnt,"roformer"),Vnt.forEach(t),Xgr=r(Yye," \u2014 "),BG=n(Yye,"A",{href:!0});var Wnt=s(BG);zgr=r(Wnt,"FlaxRoFormerModel"),Wnt.forEach(t),Vgr=r(Yye," (RoFormer model)"),Yye.forEach(t),Wgr=i(Q),o8=n(Q,"LI",{});var Kye=s(o8);zpe=n(Kye,"STRONG",{});var Qnt=s(zpe);Qgr=r(Qnt,"t5"),Qnt.forEach(t),Hgr=r(Kye," \u2014 "),xG=n(Kye,"A",{href:!0});var Hnt=s(xG);Ugr=r(Hnt,"FlaxT5Model"),Hnt.forEach(t),Jgr=r(Kye," (T5 model)"),Kye.forEach(t),Ygr=i(Q),r8=n(Q,"LI",{});var Zye=s(r8);Vpe=n(Zye,"STRONG",{});var Unt=s(Vpe);Kgr=r(Unt,"vision-text-dual-encoder"),Unt.forEach(t),Zgr=r(Zye," \u2014 "),kG=n(Zye,"A",{href:!0});var Jnt=s(kG);ehr=r(Jnt,"FlaxVisionTextDualEncoderModel"),Jnt.forEach(t),ohr=r(Zye," (VisionTextDualEncoder model)"),Zye.forEach(t),rhr=i(Q),t8=n(Q,"LI",{});var ewe=s(t8);Wpe=n(ewe,"STRONG",{});var Ynt=s(Wpe);thr=r(Ynt,"vit"),Ynt.forEach(t),ahr=r(ewe," \u2014 "),RG=n(ewe,"A",{href:!0});var Knt=s(RG);nhr=r(Knt,"FlaxViTModel"),Knt.forEach(t),shr=r(ewe," (ViT model)"),ewe.forEach(t),lhr=i(Q),a8=n(Q,"LI",{});var owe=s(a8);Qpe=n(owe,"STRONG",{});var Znt=s(Qpe);ihr=r(Znt,"wav2vec2"),Znt.forEach(t),dhr=r(owe," \u2014 "),SG=n(owe,"A",{href:!0});var est=s(SG);chr=r(est,"FlaxWav2Vec2Model"),est.forEach(t),fhr=r(owe," (Wav2Vec2 model)"),owe.forEach(t),mhr=i(Q),n8=n(Q,"LI",{});var rwe=s(n8);Hpe=n(rwe,"STRONG",{});var ost=s(Hpe);ghr=r(ost,"xglm"),ost.forEach(t),hhr=r(rwe," \u2014 "),PG=n(rwe,"A",{href:!0});var rst=s(PG);phr=r(rst,"FlaxXGLMModel"),rst.forEach(t),_hr=r(rwe," (XGLM model)"),rwe.forEach(t),Q.forEach(t),uhr=i(va),Upe=n(va,"P",{});var tst=s(Upe);bhr=r(tst,"Examples:"),tst.forEach(t),vhr=i(va),m(Ny.$$.fragment,va),va.forEach(t),Hl.forEach(t),Q0e=i(d),Rc=n(d,"H2",{class:!0});var eBe=s(Rc);s8=n(eBe,"A",{id:!0,class:!0,href:!0});var ast=s(s8);Jpe=n(ast,"SPAN",{});var nst=s(Jpe);m(Dy.$$.fragment,nst),nst.forEach(t),ast.forEach(t),Thr=i(eBe),Ype=n(eBe,"SPAN",{});var sst=s(Ype);Fhr=r(sst,"FlaxAutoModelForCausalLM"),sst.forEach(t),eBe.forEach(t),H0e=i(d),yr=n(d,"DIV",{class:!0});var Jl=s(yr);m(qy.$$.fragment,Jl),Chr=i(Jl),Sc=n(Jl,"P",{});var uz=s(Sc);Mhr=r(uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Kpe=n(uz,"CODE",{});var lst=s(Kpe);Ehr=r(lst,"from_pretrained()"),lst.forEach(t),yhr=r(uz,"class method or the "),Zpe=n(uz,"CODE",{});var ist=s(Zpe);whr=r(ist,"from_config()"),ist.forEach(t),Ahr=r(uz,`class
method.`),uz.forEach(t),Lhr=i(Jl),Gy=n(Jl,"P",{});var oBe=s(Gy);Bhr=r(oBe,"This class cannot be instantiated directly using "),e_e=n(oBe,"CODE",{});var dst=s(e_e);xhr=r(dst,"__init__()"),dst.forEach(t),khr=r(oBe," (throws an error)."),oBe.forEach(t),Rhr=i(Jl),ut=n(Jl,"DIV",{class:!0});var Yl=s(ut);m(Oy.$$.fragment,Yl),Shr=i(Yl),o_e=n(Yl,"P",{});var cst=s(o_e);Phr=r(cst,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),cst.forEach(t),$hr=i(Yl),Pc=n(Yl,"P",{});var bz=s(Pc);Ihr=r(bz,`Note:
Loading a model from its configuration file does `),r_e=n(bz,"STRONG",{});var fst=s(r_e);jhr=r(fst,"not"),fst.forEach(t),Nhr=r(bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),t_e=n(bz,"CODE",{});var mst=s(t_e);Dhr=r(mst,"from_pretrained()"),mst.forEach(t),qhr=r(bz,"to load the model weights."),bz.forEach(t),Ghr=i(Yl),a_e=n(Yl,"P",{});var gst=s(a_e);Ohr=r(gst,"Examples:"),gst.forEach(t),Xhr=i(Yl),m(Xy.$$.fragment,Yl),Yl.forEach(t),zhr=i(Jl),yo=n(Jl,"DIV",{class:!0});var Ta=s(yo);m(zy.$$.fragment,Ta),Vhr=i(Ta),n_e=n(Ta,"P",{});var hst=s(n_e);Whr=r(hst,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),hst.forEach(t),Qhr=i(Ta),un=n(Ta,"P",{});var _C=s(un);Hhr=r(_C,"The model class to instantiate is selected based on the "),s_e=n(_C,"CODE",{});var pst=s(s_e);Uhr=r(pst,"model_type"),pst.forEach(t),Jhr=r(_C,` property of the config object (either
passed as an argument or loaded from `),l_e=n(_C,"CODE",{});var _st=s(l_e);Yhr=r(_st,"pretrained_model_name_or_path"),_st.forEach(t),Khr=r(_C,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),i_e=n(_C,"CODE",{});var ust=s(i_e);Zhr=r(ust,"pretrained_model_name_or_path"),ust.forEach(t),epr=r(_C,":"),_C.forEach(t),opr=i(Ta),bn=n(Ta,"UL",{});var uC=s(bn);l8=n(uC,"LI",{});var twe=s(l8);d_e=n(twe,"STRONG",{});var bst=s(d_e);rpr=r(bst,"gpt2"),bst.forEach(t),tpr=r(twe," \u2014 "),$G=n(twe,"A",{href:!0});var vst=s($G);apr=r(vst,"FlaxGPT2LMHeadModel"),vst.forEach(t),npr=r(twe," (OpenAI GPT-2 model)"),twe.forEach(t),spr=i(uC),i8=n(uC,"LI",{});var awe=s(i8);c_e=n(awe,"STRONG",{});var Tst=s(c_e);lpr=r(Tst,"gpt_neo"),Tst.forEach(t),ipr=r(awe," \u2014 "),IG=n(awe,"A",{href:!0});var Fst=s(IG);dpr=r(Fst,"FlaxGPTNeoForCausalLM"),Fst.forEach(t),cpr=r(awe," (GPT Neo model)"),awe.forEach(t),fpr=i(uC),d8=n(uC,"LI",{});var nwe=s(d8);f_e=n(nwe,"STRONG",{});var Cst=s(f_e);mpr=r(Cst,"gptj"),Cst.forEach(t),gpr=r(nwe," \u2014 "),jG=n(nwe,"A",{href:!0});var Mst=s(jG);hpr=r(Mst,"FlaxGPTJForCausalLM"),Mst.forEach(t),ppr=r(nwe," (GPT-J model)"),nwe.forEach(t),_pr=i(uC),c8=n(uC,"LI",{});var swe=s(c8);m_e=n(swe,"STRONG",{});var Est=s(m_e);upr=r(Est,"xglm"),Est.forEach(t),bpr=r(swe," \u2014 "),NG=n(swe,"A",{href:!0});var yst=s(NG);vpr=r(yst,"FlaxXGLMForCausalLM"),yst.forEach(t),Tpr=r(swe," (XGLM model)"),swe.forEach(t),uC.forEach(t),Fpr=i(Ta),g_e=n(Ta,"P",{});var wst=s(g_e);Cpr=r(wst,"Examples:"),wst.forEach(t),Mpr=i(Ta),m(Vy.$$.fragment,Ta),Ta.forEach(t),Jl.forEach(t),U0e=i(d),$c=n(d,"H2",{class:!0});var rBe=s($c);f8=n(rBe,"A",{id:!0,class:!0,href:!0});var Ast=s(f8);h_e=n(Ast,"SPAN",{});var Lst=s(h_e);m(Wy.$$.fragment,Lst),Lst.forEach(t),Ast.forEach(t),Epr=i(rBe),p_e=n(rBe,"SPAN",{});var Bst=s(p_e);ypr=r(Bst,"FlaxAutoModelForPreTraining"),Bst.forEach(t),rBe.forEach(t),J0e=i(d),wr=n(d,"DIV",{class:!0});var Kl=s(wr);m(Qy.$$.fragment,Kl),wpr=i(Kl),Ic=n(Kl,"P",{});var vz=s(Ic);Apr=r(vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),__e=n(vz,"CODE",{});var xst=s(__e);Lpr=r(xst,"from_pretrained()"),xst.forEach(t),Bpr=r(vz,"class method or the "),u_e=n(vz,"CODE",{});var kst=s(u_e);xpr=r(kst,"from_config()"),kst.forEach(t),kpr=r(vz,`class
method.`),vz.forEach(t),Rpr=i(Kl),Hy=n(Kl,"P",{});var tBe=s(Hy);Spr=r(tBe,"This class cannot be instantiated directly using "),b_e=n(tBe,"CODE",{});var Rst=s(b_e);Ppr=r(Rst,"__init__()"),Rst.forEach(t),$pr=r(tBe," (throws an error)."),tBe.forEach(t),Ipr=i(Kl),bt=n(Kl,"DIV",{class:!0});var Zl=s(bt);m(Uy.$$.fragment,Zl),jpr=i(Zl),v_e=n(Zl,"P",{});var Sst=s(v_e);Npr=r(Sst,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Sst.forEach(t),Dpr=i(Zl),jc=n(Zl,"P",{});var Tz=s(jc);qpr=r(Tz,`Note:
Loading a model from its configuration file does `),T_e=n(Tz,"STRONG",{});var Pst=s(T_e);Gpr=r(Pst,"not"),Pst.forEach(t),Opr=r(Tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),F_e=n(Tz,"CODE",{});var $st=s(F_e);Xpr=r($st,"from_pretrained()"),$st.forEach(t),zpr=r(Tz,"to load the model weights."),Tz.forEach(t),Vpr=i(Zl),C_e=n(Zl,"P",{});var Ist=s(C_e);Wpr=r(Ist,"Examples:"),Ist.forEach(t),Qpr=i(Zl),m(Jy.$$.fragment,Zl),Zl.forEach(t),Hpr=i(Kl),wo=n(Kl,"DIV",{class:!0});var Fa=s(wo);m(Yy.$$.fragment,Fa),Upr=i(Fa),M_e=n(Fa,"P",{});var jst=s(M_e);Jpr=r(jst,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),jst.forEach(t),Ypr=i(Fa),vn=n(Fa,"P",{});var bC=s(vn);Kpr=r(bC,"The model class to instantiate is selected based on the "),E_e=n(bC,"CODE",{});var Nst=s(E_e);Zpr=r(Nst,"model_type"),Nst.forEach(t),e_r=r(bC,` property of the config object (either
passed as an argument or loaded from `),y_e=n(bC,"CODE",{});var Dst=s(y_e);o_r=r(Dst,"pretrained_model_name_or_path"),Dst.forEach(t),r_r=r(bC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w_e=n(bC,"CODE",{});var qst=s(w_e);t_r=r(qst,"pretrained_model_name_or_path"),qst.forEach(t),a_r=r(bC,":"),bC.forEach(t),n_r=i(Fa),fe=n(Fa,"UL",{});var _e=s(fe);m8=n(_e,"LI",{});var lwe=s(m8);A_e=n(lwe,"STRONG",{});var Gst=s(A_e);s_r=r(Gst,"albert"),Gst.forEach(t),l_r=r(lwe," \u2014 "),DG=n(lwe,"A",{href:!0});var Ost=s(DG);i_r=r(Ost,"FlaxAlbertForPreTraining"),Ost.forEach(t),d_r=r(lwe," (ALBERT model)"),lwe.forEach(t),c_r=i(_e),g8=n(_e,"LI",{});var iwe=s(g8);L_e=n(iwe,"STRONG",{});var Xst=s(L_e);f_r=r(Xst,"bart"),Xst.forEach(t),m_r=r(iwe," \u2014 "),qG=n(iwe,"A",{href:!0});var zst=s(qG);g_r=r(zst,"FlaxBartForConditionalGeneration"),zst.forEach(t),h_r=r(iwe," (BART model)"),iwe.forEach(t),p_r=i(_e),h8=n(_e,"LI",{});var dwe=s(h8);B_e=n(dwe,"STRONG",{});var Vst=s(B_e);__r=r(Vst,"bert"),Vst.forEach(t),u_r=r(dwe," \u2014 "),GG=n(dwe,"A",{href:!0});var Wst=s(GG);b_r=r(Wst,"FlaxBertForPreTraining"),Wst.forEach(t),v_r=r(dwe," (BERT model)"),dwe.forEach(t),T_r=i(_e),p8=n(_e,"LI",{});var cwe=s(p8);x_e=n(cwe,"STRONG",{});var Qst=s(x_e);F_r=r(Qst,"big_bird"),Qst.forEach(t),C_r=r(cwe," \u2014 "),OG=n(cwe,"A",{href:!0});var Hst=s(OG);M_r=r(Hst,"FlaxBigBirdForPreTraining"),Hst.forEach(t),E_r=r(cwe," (BigBird model)"),cwe.forEach(t),y_r=i(_e),_8=n(_e,"LI",{});var fwe=s(_8);k_e=n(fwe,"STRONG",{});var Ust=s(k_e);w_r=r(Ust,"electra"),Ust.forEach(t),A_r=r(fwe," \u2014 "),XG=n(fwe,"A",{href:!0});var Jst=s(XG);L_r=r(Jst,"FlaxElectraForPreTraining"),Jst.forEach(t),B_r=r(fwe," (ELECTRA model)"),fwe.forEach(t),x_r=i(_e),u8=n(_e,"LI",{});var mwe=s(u8);R_e=n(mwe,"STRONG",{});var Yst=s(R_e);k_r=r(Yst,"mbart"),Yst.forEach(t),R_r=r(mwe," \u2014 "),zG=n(mwe,"A",{href:!0});var Kst=s(zG);S_r=r(Kst,"FlaxMBartForConditionalGeneration"),Kst.forEach(t),P_r=r(mwe," (mBART model)"),mwe.forEach(t),$_r=i(_e),b8=n(_e,"LI",{});var gwe=s(b8);S_e=n(gwe,"STRONG",{});var Zst=s(S_e);I_r=r(Zst,"mt5"),Zst.forEach(t),j_r=r(gwe," \u2014 "),VG=n(gwe,"A",{href:!0});var elt=s(VG);N_r=r(elt,"FlaxMT5ForConditionalGeneration"),elt.forEach(t),D_r=r(gwe," (mT5 model)"),gwe.forEach(t),q_r=i(_e),v8=n(_e,"LI",{});var hwe=s(v8);P_e=n(hwe,"STRONG",{});var olt=s(P_e);G_r=r(olt,"roberta"),olt.forEach(t),O_r=r(hwe," \u2014 "),WG=n(hwe,"A",{href:!0});var rlt=s(WG);X_r=r(rlt,"FlaxRobertaForMaskedLM"),rlt.forEach(t),z_r=r(hwe," (RoBERTa model)"),hwe.forEach(t),V_r=i(_e),T8=n(_e,"LI",{});var pwe=s(T8);$_e=n(pwe,"STRONG",{});var tlt=s($_e);W_r=r(tlt,"roformer"),tlt.forEach(t),Q_r=r(pwe," \u2014 "),QG=n(pwe,"A",{href:!0});var alt=s(QG);H_r=r(alt,"FlaxRoFormerForMaskedLM"),alt.forEach(t),U_r=r(pwe," (RoFormer model)"),pwe.forEach(t),J_r=i(_e),F8=n(_e,"LI",{});var _we=s(F8);I_e=n(_we,"STRONG",{});var nlt=s(I_e);Y_r=r(nlt,"t5"),nlt.forEach(t),K_r=r(_we," \u2014 "),HG=n(_we,"A",{href:!0});var slt=s(HG);Z_r=r(slt,"FlaxT5ForConditionalGeneration"),slt.forEach(t),eur=r(_we," (T5 model)"),_we.forEach(t),our=i(_e),C8=n(_e,"LI",{});var uwe=s(C8);j_e=n(uwe,"STRONG",{});var llt=s(j_e);rur=r(llt,"wav2vec2"),llt.forEach(t),tur=r(uwe," \u2014 "),UG=n(uwe,"A",{href:!0});var ilt=s(UG);aur=r(ilt,"FlaxWav2Vec2ForPreTraining"),ilt.forEach(t),nur=r(uwe," (Wav2Vec2 model)"),uwe.forEach(t),_e.forEach(t),sur=i(Fa),N_e=n(Fa,"P",{});var dlt=s(N_e);lur=r(dlt,"Examples:"),dlt.forEach(t),iur=i(Fa),m(Ky.$$.fragment,Fa),Fa.forEach(t),Kl.forEach(t),Y0e=i(d),Nc=n(d,"H2",{class:!0});var aBe=s(Nc);M8=n(aBe,"A",{id:!0,class:!0,href:!0});var clt=s(M8);D_e=n(clt,"SPAN",{});var flt=s(D_e);m(Zy.$$.fragment,flt),flt.forEach(t),clt.forEach(t),dur=i(aBe),q_e=n(aBe,"SPAN",{});var mlt=s(q_e);cur=r(mlt,"FlaxAutoModelForMaskedLM"),mlt.forEach(t),aBe.forEach(t),K0e=i(d),Ar=n(d,"DIV",{class:!0});var ei=s(Ar);m(ew.$$.fragment,ei),fur=i(ei),Dc=n(ei,"P",{});var Fz=s(Dc);mur=r(Fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),G_e=n(Fz,"CODE",{});var glt=s(G_e);gur=r(glt,"from_pretrained()"),glt.forEach(t),hur=r(Fz,"class method or the "),O_e=n(Fz,"CODE",{});var hlt=s(O_e);pur=r(hlt,"from_config()"),hlt.forEach(t),_ur=r(Fz,`class
method.`),Fz.forEach(t),uur=i(ei),ow=n(ei,"P",{});var nBe=s(ow);bur=r(nBe,"This class cannot be instantiated directly using "),X_e=n(nBe,"CODE",{});var plt=s(X_e);vur=r(plt,"__init__()"),plt.forEach(t),Tur=r(nBe," (throws an error)."),nBe.forEach(t),Fur=i(ei),vt=n(ei,"DIV",{class:!0});var oi=s(vt);m(rw.$$.fragment,oi),Cur=i(oi),z_e=n(oi,"P",{});var _lt=s(z_e);Mur=r(_lt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),_lt.forEach(t),Eur=i(oi),qc=n(oi,"P",{});var Cz=s(qc);yur=r(Cz,`Note:
Loading a model from its configuration file does `),V_e=n(Cz,"STRONG",{});var ult=s(V_e);wur=r(ult,"not"),ult.forEach(t),Aur=r(Cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),W_e=n(Cz,"CODE",{});var blt=s(W_e);Lur=r(blt,"from_pretrained()"),blt.forEach(t),Bur=r(Cz,"to load the model weights."),Cz.forEach(t),xur=i(oi),Q_e=n(oi,"P",{});var vlt=s(Q_e);kur=r(vlt,"Examples:"),vlt.forEach(t),Rur=i(oi),m(tw.$$.fragment,oi),oi.forEach(t),Sur=i(ei),Ao=n(ei,"DIV",{class:!0});var Ca=s(Ao);m(aw.$$.fragment,Ca),Pur=i(Ca),H_e=n(Ca,"P",{});var Tlt=s(H_e);$ur=r(Tlt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Tlt.forEach(t),Iur=i(Ca),Tn=n(Ca,"P",{});var vC=s(Tn);jur=r(vC,"The model class to instantiate is selected based on the "),U_e=n(vC,"CODE",{});var Flt=s(U_e);Nur=r(Flt,"model_type"),Flt.forEach(t),Dur=r(vC,` property of the config object (either
passed as an argument or loaded from `),J_e=n(vC,"CODE",{});var Clt=s(J_e);qur=r(Clt,"pretrained_model_name_or_path"),Clt.forEach(t),Gur=r(vC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Y_e=n(vC,"CODE",{});var Mlt=s(Y_e);Our=r(Mlt,"pretrained_model_name_or_path"),Mlt.forEach(t),Xur=r(vC,":"),vC.forEach(t),zur=i(Ca),be=n(Ca,"UL",{});var Ye=s(be);E8=n(Ye,"LI",{});var bwe=s(E8);K_e=n(bwe,"STRONG",{});var Elt=s(K_e);Vur=r(Elt,"albert"),Elt.forEach(t),Wur=r(bwe," \u2014 "),JG=n(bwe,"A",{href:!0});var ylt=s(JG);Qur=r(ylt,"FlaxAlbertForMaskedLM"),ylt.forEach(t),Hur=r(bwe," (ALBERT model)"),bwe.forEach(t),Uur=i(Ye),y8=n(Ye,"LI",{});var vwe=s(y8);Z_e=n(vwe,"STRONG",{});var wlt=s(Z_e);Jur=r(wlt,"bart"),wlt.forEach(t),Yur=r(vwe," \u2014 "),YG=n(vwe,"A",{href:!0});var Alt=s(YG);Kur=r(Alt,"FlaxBartForConditionalGeneration"),Alt.forEach(t),Zur=r(vwe," (BART model)"),vwe.forEach(t),e1r=i(Ye),w8=n(Ye,"LI",{});var Twe=s(w8);eue=n(Twe,"STRONG",{});var Llt=s(eue);o1r=r(Llt,"bert"),Llt.forEach(t),r1r=r(Twe," \u2014 "),KG=n(Twe,"A",{href:!0});var Blt=s(KG);t1r=r(Blt,"FlaxBertForMaskedLM"),Blt.forEach(t),a1r=r(Twe," (BERT model)"),Twe.forEach(t),n1r=i(Ye),A8=n(Ye,"LI",{});var Fwe=s(A8);oue=n(Fwe,"STRONG",{});var xlt=s(oue);s1r=r(xlt,"big_bird"),xlt.forEach(t),l1r=r(Fwe," \u2014 "),ZG=n(Fwe,"A",{href:!0});var klt=s(ZG);i1r=r(klt,"FlaxBigBirdForMaskedLM"),klt.forEach(t),d1r=r(Fwe," (BigBird model)"),Fwe.forEach(t),c1r=i(Ye),L8=n(Ye,"LI",{});var Cwe=s(L8);rue=n(Cwe,"STRONG",{});var Rlt=s(rue);f1r=r(Rlt,"distilbert"),Rlt.forEach(t),m1r=r(Cwe," \u2014 "),eO=n(Cwe,"A",{href:!0});var Slt=s(eO);g1r=r(Slt,"FlaxDistilBertForMaskedLM"),Slt.forEach(t),h1r=r(Cwe," (DistilBERT model)"),Cwe.forEach(t),p1r=i(Ye),B8=n(Ye,"LI",{});var Mwe=s(B8);tue=n(Mwe,"STRONG",{});var Plt=s(tue);_1r=r(Plt,"electra"),Plt.forEach(t),u1r=r(Mwe," \u2014 "),oO=n(Mwe,"A",{href:!0});var $lt=s(oO);b1r=r($lt,"FlaxElectraForMaskedLM"),$lt.forEach(t),v1r=r(Mwe," (ELECTRA model)"),Mwe.forEach(t),T1r=i(Ye),x8=n(Ye,"LI",{});var Ewe=s(x8);aue=n(Ewe,"STRONG",{});var Ilt=s(aue);F1r=r(Ilt,"mbart"),Ilt.forEach(t),C1r=r(Ewe," \u2014 "),rO=n(Ewe,"A",{href:!0});var jlt=s(rO);M1r=r(jlt,"FlaxMBartForConditionalGeneration"),jlt.forEach(t),E1r=r(Ewe," (mBART model)"),Ewe.forEach(t),y1r=i(Ye),k8=n(Ye,"LI",{});var ywe=s(k8);nue=n(ywe,"STRONG",{});var Nlt=s(nue);w1r=r(Nlt,"roberta"),Nlt.forEach(t),A1r=r(ywe," \u2014 "),tO=n(ywe,"A",{href:!0});var Dlt=s(tO);L1r=r(Dlt,"FlaxRobertaForMaskedLM"),Dlt.forEach(t),B1r=r(ywe," (RoBERTa model)"),ywe.forEach(t),x1r=i(Ye),R8=n(Ye,"LI",{});var wwe=s(R8);sue=n(wwe,"STRONG",{});var qlt=s(sue);k1r=r(qlt,"roformer"),qlt.forEach(t),R1r=r(wwe," \u2014 "),aO=n(wwe,"A",{href:!0});var Glt=s(aO);S1r=r(Glt,"FlaxRoFormerForMaskedLM"),Glt.forEach(t),P1r=r(wwe," (RoFormer model)"),wwe.forEach(t),Ye.forEach(t),$1r=i(Ca),lue=n(Ca,"P",{});var Olt=s(lue);I1r=r(Olt,"Examples:"),Olt.forEach(t),j1r=i(Ca),m(nw.$$.fragment,Ca),Ca.forEach(t),ei.forEach(t),Z0e=i(d),Gc=n(d,"H2",{class:!0});var sBe=s(Gc);S8=n(sBe,"A",{id:!0,class:!0,href:!0});var Xlt=s(S8);iue=n(Xlt,"SPAN",{});var zlt=s(iue);m(sw.$$.fragment,zlt),zlt.forEach(t),Xlt.forEach(t),N1r=i(sBe),due=n(sBe,"SPAN",{});var Vlt=s(due);D1r=r(Vlt,"FlaxAutoModelForSeq2SeqLM"),Vlt.forEach(t),sBe.forEach(t),eLe=i(d),Lr=n(d,"DIV",{class:!0});var ri=s(Lr);m(lw.$$.fragment,ri),q1r=i(ri),Oc=n(ri,"P",{});var Mz=s(Oc);G1r=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),cue=n(Mz,"CODE",{});var Wlt=s(cue);O1r=r(Wlt,"from_pretrained()"),Wlt.forEach(t),X1r=r(Mz,"class method or the "),fue=n(Mz,"CODE",{});var Qlt=s(fue);z1r=r(Qlt,"from_config()"),Qlt.forEach(t),V1r=r(Mz,`class
method.`),Mz.forEach(t),W1r=i(ri),iw=n(ri,"P",{});var lBe=s(iw);Q1r=r(lBe,"This class cannot be instantiated directly using "),mue=n(lBe,"CODE",{});var Hlt=s(mue);H1r=r(Hlt,"__init__()"),Hlt.forEach(t),U1r=r(lBe," (throws an error)."),lBe.forEach(t),J1r=i(ri),Tt=n(ri,"DIV",{class:!0});var ti=s(Tt);m(dw.$$.fragment,ti),Y1r=i(ti),gue=n(ti,"P",{});var Ult=s(gue);K1r=r(Ult,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Ult.forEach(t),Z1r=i(ti),Xc=n(ti,"P",{});var Ez=s(Xc);ebr=r(Ez,`Note:
Loading a model from its configuration file does `),hue=n(Ez,"STRONG",{});var Jlt=s(hue);obr=r(Jlt,"not"),Jlt.forEach(t),rbr=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),pue=n(Ez,"CODE",{});var Ylt=s(pue);tbr=r(Ylt,"from_pretrained()"),Ylt.forEach(t),abr=r(Ez,"to load the model weights."),Ez.forEach(t),nbr=i(ti),_ue=n(ti,"P",{});var Klt=s(_ue);sbr=r(Klt,"Examples:"),Klt.forEach(t),lbr=i(ti),m(cw.$$.fragment,ti),ti.forEach(t),ibr=i(ri),Lo=n(ri,"DIV",{class:!0});var Ma=s(Lo);m(fw.$$.fragment,Ma),dbr=i(Ma),uue=n(Ma,"P",{});var Zlt=s(uue);cbr=r(Zlt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Zlt.forEach(t),fbr=i(Ma),Fn=n(Ma,"P",{});var TC=s(Fn);mbr=r(TC,"The model class to instantiate is selected based on the "),bue=n(TC,"CODE",{});var eit=s(bue);gbr=r(eit,"model_type"),eit.forEach(t),hbr=r(TC,` property of the config object (either
passed as an argument or loaded from `),vue=n(TC,"CODE",{});var oit=s(vue);pbr=r(oit,"pretrained_model_name_or_path"),oit.forEach(t),_br=r(TC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tue=n(TC,"CODE",{});var rit=s(Tue);ubr=r(rit,"pretrained_model_name_or_path"),rit.forEach(t),bbr=r(TC,":"),TC.forEach(t),vbr=i(Ma),ve=n(Ma,"UL",{});var Ke=s(ve);P8=n(Ke,"LI",{});var Awe=s(P8);Fue=n(Awe,"STRONG",{});var tit=s(Fue);Tbr=r(tit,"bart"),tit.forEach(t),Fbr=r(Awe," \u2014 "),nO=n(Awe,"A",{href:!0});var ait=s(nO);Cbr=r(ait,"FlaxBartForConditionalGeneration"),ait.forEach(t),Mbr=r(Awe," (BART model)"),Awe.forEach(t),Ebr=i(Ke),$8=n(Ke,"LI",{});var Lwe=s($8);Cue=n(Lwe,"STRONG",{});var nit=s(Cue);ybr=r(nit,"blenderbot"),nit.forEach(t),wbr=r(Lwe," \u2014 "),sO=n(Lwe,"A",{href:!0});var sit=s(sO);Abr=r(sit,"FlaxBlenderbotForConditionalGeneration"),sit.forEach(t),Lbr=r(Lwe," (Blenderbot model)"),Lwe.forEach(t),Bbr=i(Ke),I8=n(Ke,"LI",{});var Bwe=s(I8);Mue=n(Bwe,"STRONG",{});var lit=s(Mue);xbr=r(lit,"blenderbot-small"),lit.forEach(t),kbr=r(Bwe," \u2014 "),lO=n(Bwe,"A",{href:!0});var iit=s(lO);Rbr=r(iit,"FlaxBlenderbotSmallForConditionalGeneration"),iit.forEach(t),Sbr=r(Bwe," (BlenderbotSmall model)"),Bwe.forEach(t),Pbr=i(Ke),j8=n(Ke,"LI",{});var xwe=s(j8);Eue=n(xwe,"STRONG",{});var dit=s(Eue);$br=r(dit,"encoder-decoder"),dit.forEach(t),Ibr=r(xwe," \u2014 "),iO=n(xwe,"A",{href:!0});var cit=s(iO);jbr=r(cit,"FlaxEncoderDecoderModel"),cit.forEach(t),Nbr=r(xwe," (Encoder decoder model)"),xwe.forEach(t),Dbr=i(Ke),N8=n(Ke,"LI",{});var kwe=s(N8);yue=n(kwe,"STRONG",{});var fit=s(yue);qbr=r(fit,"marian"),fit.forEach(t),Gbr=r(kwe," \u2014 "),dO=n(kwe,"A",{href:!0});var mit=s(dO);Obr=r(mit,"FlaxMarianMTModel"),mit.forEach(t),Xbr=r(kwe," (Marian model)"),kwe.forEach(t),zbr=i(Ke),D8=n(Ke,"LI",{});var Rwe=s(D8);wue=n(Rwe,"STRONG",{});var git=s(wue);Vbr=r(git,"mbart"),git.forEach(t),Wbr=r(Rwe," \u2014 "),cO=n(Rwe,"A",{href:!0});var hit=s(cO);Qbr=r(hit,"FlaxMBartForConditionalGeneration"),hit.forEach(t),Hbr=r(Rwe," (mBART model)"),Rwe.forEach(t),Ubr=i(Ke),q8=n(Ke,"LI",{});var Swe=s(q8);Aue=n(Swe,"STRONG",{});var pit=s(Aue);Jbr=r(pit,"mt5"),pit.forEach(t),Ybr=r(Swe," \u2014 "),fO=n(Swe,"A",{href:!0});var _it=s(fO);Kbr=r(_it,"FlaxMT5ForConditionalGeneration"),_it.forEach(t),Zbr=r(Swe," (mT5 model)"),Swe.forEach(t),e5r=i(Ke),G8=n(Ke,"LI",{});var Pwe=s(G8);Lue=n(Pwe,"STRONG",{});var uit=s(Lue);o5r=r(uit,"pegasus"),uit.forEach(t),r5r=r(Pwe," \u2014 "),mO=n(Pwe,"A",{href:!0});var bit=s(mO);t5r=r(bit,"FlaxPegasusForConditionalGeneration"),bit.forEach(t),a5r=r(Pwe," (Pegasus model)"),Pwe.forEach(t),n5r=i(Ke),O8=n(Ke,"LI",{});var $we=s(O8);Bue=n($we,"STRONG",{});var vit=s(Bue);s5r=r(vit,"t5"),vit.forEach(t),l5r=r($we," \u2014 "),gO=n($we,"A",{href:!0});var Tit=s(gO);i5r=r(Tit,"FlaxT5ForConditionalGeneration"),Tit.forEach(t),d5r=r($we," (T5 model)"),$we.forEach(t),Ke.forEach(t),c5r=i(Ma),xue=n(Ma,"P",{});var Fit=s(xue);f5r=r(Fit,"Examples:"),Fit.forEach(t),m5r=i(Ma),m(mw.$$.fragment,Ma),Ma.forEach(t),ri.forEach(t),oLe=i(d),zc=n(d,"H2",{class:!0});var iBe=s(zc);X8=n(iBe,"A",{id:!0,class:!0,href:!0});var Cit=s(X8);kue=n(Cit,"SPAN",{});var Mit=s(kue);m(gw.$$.fragment,Mit),Mit.forEach(t),Cit.forEach(t),g5r=i(iBe),Rue=n(iBe,"SPAN",{});var Eit=s(Rue);h5r=r(Eit,"FlaxAutoModelForSequenceClassification"),Eit.forEach(t),iBe.forEach(t),rLe=i(d),Br=n(d,"DIV",{class:!0});var ai=s(Br);m(hw.$$.fragment,ai),p5r=i(ai),Vc=n(ai,"P",{});var yz=s(Vc);_5r=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Sue=n(yz,"CODE",{});var yit=s(Sue);u5r=r(yit,"from_pretrained()"),yit.forEach(t),b5r=r(yz,"class method or the "),Pue=n(yz,"CODE",{});var wit=s(Pue);v5r=r(wit,"from_config()"),wit.forEach(t),T5r=r(yz,`class
method.`),yz.forEach(t),F5r=i(ai),pw=n(ai,"P",{});var dBe=s(pw);C5r=r(dBe,"This class cannot be instantiated directly using "),$ue=n(dBe,"CODE",{});var Ait=s($ue);M5r=r(Ait,"__init__()"),Ait.forEach(t),E5r=r(dBe," (throws an error)."),dBe.forEach(t),y5r=i(ai),Ft=n(ai,"DIV",{class:!0});var ni=s(Ft);m(_w.$$.fragment,ni),w5r=i(ni),Iue=n(ni,"P",{});var Lit=s(Iue);A5r=r(Lit,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lit.forEach(t),L5r=i(ni),Wc=n(ni,"P",{});var wz=s(Wc);B5r=r(wz,`Note:
Loading a model from its configuration file does `),jue=n(wz,"STRONG",{});var Bit=s(jue);x5r=r(Bit,"not"),Bit.forEach(t),k5r=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Nue=n(wz,"CODE",{});var xit=s(Nue);R5r=r(xit,"from_pretrained()"),xit.forEach(t),S5r=r(wz,"to load the model weights."),wz.forEach(t),P5r=i(ni),Due=n(ni,"P",{});var kit=s(Due);$5r=r(kit,"Examples:"),kit.forEach(t),I5r=i(ni),m(uw.$$.fragment,ni),ni.forEach(t),j5r=i(ai),Bo=n(ai,"DIV",{class:!0});var Ea=s(Bo);m(bw.$$.fragment,Ea),N5r=i(Ea),que=n(Ea,"P",{});var Rit=s(que);D5r=r(Rit,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Rit.forEach(t),q5r=i(Ea),Cn=n(Ea,"P",{});var FC=s(Cn);G5r=r(FC,"The model class to instantiate is selected based on the "),Gue=n(FC,"CODE",{});var Sit=s(Gue);O5r=r(Sit,"model_type"),Sit.forEach(t),X5r=r(FC,` property of the config object (either
passed as an argument or loaded from `),Oue=n(FC,"CODE",{});var Pit=s(Oue);z5r=r(Pit,"pretrained_model_name_or_path"),Pit.forEach(t),V5r=r(FC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xue=n(FC,"CODE",{});var $it=s(Xue);W5r=r($it,"pretrained_model_name_or_path"),$it.forEach(t),Q5r=r(FC,":"),FC.forEach(t),H5r=i(Ea),Te=n(Ea,"UL",{});var Ze=s(Te);z8=n(Ze,"LI",{});var Iwe=s(z8);zue=n(Iwe,"STRONG",{});var Iit=s(zue);U5r=r(Iit,"albert"),Iit.forEach(t),J5r=r(Iwe," \u2014 "),hO=n(Iwe,"A",{href:!0});var jit=s(hO);Y5r=r(jit,"FlaxAlbertForSequenceClassification"),jit.forEach(t),K5r=r(Iwe," (ALBERT model)"),Iwe.forEach(t),Z5r=i(Ze),V8=n(Ze,"LI",{});var jwe=s(V8);Vue=n(jwe,"STRONG",{});var Nit=s(Vue);e2r=r(Nit,"bart"),Nit.forEach(t),o2r=r(jwe," \u2014 "),pO=n(jwe,"A",{href:!0});var Dit=s(pO);r2r=r(Dit,"FlaxBartForSequenceClassification"),Dit.forEach(t),t2r=r(jwe," (BART model)"),jwe.forEach(t),a2r=i(Ze),W8=n(Ze,"LI",{});var Nwe=s(W8);Wue=n(Nwe,"STRONG",{});var qit=s(Wue);n2r=r(qit,"bert"),qit.forEach(t),s2r=r(Nwe," \u2014 "),_O=n(Nwe,"A",{href:!0});var Git=s(_O);l2r=r(Git,"FlaxBertForSequenceClassification"),Git.forEach(t),i2r=r(Nwe," (BERT model)"),Nwe.forEach(t),d2r=i(Ze),Q8=n(Ze,"LI",{});var Dwe=s(Q8);Que=n(Dwe,"STRONG",{});var Oit=s(Que);c2r=r(Oit,"big_bird"),Oit.forEach(t),f2r=r(Dwe," \u2014 "),uO=n(Dwe,"A",{href:!0});var Xit=s(uO);m2r=r(Xit,"FlaxBigBirdForSequenceClassification"),Xit.forEach(t),g2r=r(Dwe," (BigBird model)"),Dwe.forEach(t),h2r=i(Ze),H8=n(Ze,"LI",{});var qwe=s(H8);Hue=n(qwe,"STRONG",{});var zit=s(Hue);p2r=r(zit,"distilbert"),zit.forEach(t),_2r=r(qwe," \u2014 "),bO=n(qwe,"A",{href:!0});var Vit=s(bO);u2r=r(Vit,"FlaxDistilBertForSequenceClassification"),Vit.forEach(t),b2r=r(qwe," (DistilBERT model)"),qwe.forEach(t),v2r=i(Ze),U8=n(Ze,"LI",{});var Gwe=s(U8);Uue=n(Gwe,"STRONG",{});var Wit=s(Uue);T2r=r(Wit,"electra"),Wit.forEach(t),F2r=r(Gwe," \u2014 "),vO=n(Gwe,"A",{href:!0});var Qit=s(vO);C2r=r(Qit,"FlaxElectraForSequenceClassification"),Qit.forEach(t),M2r=r(Gwe," (ELECTRA model)"),Gwe.forEach(t),E2r=i(Ze),J8=n(Ze,"LI",{});var Owe=s(J8);Jue=n(Owe,"STRONG",{});var Hit=s(Jue);y2r=r(Hit,"mbart"),Hit.forEach(t),w2r=r(Owe," \u2014 "),TO=n(Owe,"A",{href:!0});var Uit=s(TO);A2r=r(Uit,"FlaxMBartForSequenceClassification"),Uit.forEach(t),L2r=r(Owe," (mBART model)"),Owe.forEach(t),B2r=i(Ze),Y8=n(Ze,"LI",{});var Xwe=s(Y8);Yue=n(Xwe,"STRONG",{});var Jit=s(Yue);x2r=r(Jit,"roberta"),Jit.forEach(t),k2r=r(Xwe," \u2014 "),FO=n(Xwe,"A",{href:!0});var Yit=s(FO);R2r=r(Yit,"FlaxRobertaForSequenceClassification"),Yit.forEach(t),S2r=r(Xwe," (RoBERTa model)"),Xwe.forEach(t),P2r=i(Ze),K8=n(Ze,"LI",{});var zwe=s(K8);Kue=n(zwe,"STRONG",{});var Kit=s(Kue);$2r=r(Kit,"roformer"),Kit.forEach(t),I2r=r(zwe," \u2014 "),CO=n(zwe,"A",{href:!0});var Zit=s(CO);j2r=r(Zit,"FlaxRoFormerForSequenceClassification"),Zit.forEach(t),N2r=r(zwe," (RoFormer model)"),zwe.forEach(t),Ze.forEach(t),D2r=i(Ea),Zue=n(Ea,"P",{});var edt=s(Zue);q2r=r(edt,"Examples:"),edt.forEach(t),G2r=i(Ea),m(vw.$$.fragment,Ea),Ea.forEach(t),ai.forEach(t),tLe=i(d),Qc=n(d,"H2",{class:!0});var cBe=s(Qc);Z8=n(cBe,"A",{id:!0,class:!0,href:!0});var odt=s(Z8);e1e=n(odt,"SPAN",{});var rdt=s(e1e);m(Tw.$$.fragment,rdt),rdt.forEach(t),odt.forEach(t),O2r=i(cBe),o1e=n(cBe,"SPAN",{});var tdt=s(o1e);X2r=r(tdt,"FlaxAutoModelForQuestionAnswering"),tdt.forEach(t),cBe.forEach(t),aLe=i(d),xr=n(d,"DIV",{class:!0});var si=s(xr);m(Fw.$$.fragment,si),z2r=i(si),Hc=n(si,"P",{});var Az=s(Hc);V2r=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),r1e=n(Az,"CODE",{});var adt=s(r1e);W2r=r(adt,"from_pretrained()"),adt.forEach(t),Q2r=r(Az,"class method or the "),t1e=n(Az,"CODE",{});var ndt=s(t1e);H2r=r(ndt,"from_config()"),ndt.forEach(t),U2r=r(Az,`class
method.`),Az.forEach(t),J2r=i(si),Cw=n(si,"P",{});var fBe=s(Cw);Y2r=r(fBe,"This class cannot be instantiated directly using "),a1e=n(fBe,"CODE",{});var sdt=s(a1e);K2r=r(sdt,"__init__()"),sdt.forEach(t),Z2r=r(fBe," (throws an error)."),fBe.forEach(t),evr=i(si),Ct=n(si,"DIV",{class:!0});var li=s(Ct);m(Mw.$$.fragment,li),ovr=i(li),n1e=n(li,"P",{});var ldt=s(n1e);rvr=r(ldt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),ldt.forEach(t),tvr=i(li),Uc=n(li,"P",{});var Lz=s(Uc);avr=r(Lz,`Note:
Loading a model from its configuration file does `),s1e=n(Lz,"STRONG",{});var idt=s(s1e);nvr=r(idt,"not"),idt.forEach(t),svr=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),l1e=n(Lz,"CODE",{});var ddt=s(l1e);lvr=r(ddt,"from_pretrained()"),ddt.forEach(t),ivr=r(Lz,"to load the model weights."),Lz.forEach(t),dvr=i(li),i1e=n(li,"P",{});var cdt=s(i1e);cvr=r(cdt,"Examples:"),cdt.forEach(t),fvr=i(li),m(Ew.$$.fragment,li),li.forEach(t),mvr=i(si),xo=n(si,"DIV",{class:!0});var ya=s(xo);m(yw.$$.fragment,ya),gvr=i(ya),d1e=n(ya,"P",{});var fdt=s(d1e);hvr=r(fdt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),fdt.forEach(t),pvr=i(ya),Mn=n(ya,"P",{});var CC=s(Mn);_vr=r(CC,"The model class to instantiate is selected based on the "),c1e=n(CC,"CODE",{});var mdt=s(c1e);uvr=r(mdt,"model_type"),mdt.forEach(t),bvr=r(CC,` property of the config object (either
passed as an argument or loaded from `),f1e=n(CC,"CODE",{});var gdt=s(f1e);vvr=r(gdt,"pretrained_model_name_or_path"),gdt.forEach(t),Tvr=r(CC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),m1e=n(CC,"CODE",{});var hdt=s(m1e);Fvr=r(hdt,"pretrained_model_name_or_path"),hdt.forEach(t),Cvr=r(CC,":"),CC.forEach(t),Mvr=i(ya),Fe=n(ya,"UL",{});var eo=s(Fe);eF=n(eo,"LI",{});var Vwe=s(eF);g1e=n(Vwe,"STRONG",{});var pdt=s(g1e);Evr=r(pdt,"albert"),pdt.forEach(t),yvr=r(Vwe," \u2014 "),MO=n(Vwe,"A",{href:!0});var _dt=s(MO);wvr=r(_dt,"FlaxAlbertForQuestionAnswering"),_dt.forEach(t),Avr=r(Vwe," (ALBERT model)"),Vwe.forEach(t),Lvr=i(eo),oF=n(eo,"LI",{});var Wwe=s(oF);h1e=n(Wwe,"STRONG",{});var udt=s(h1e);Bvr=r(udt,"bart"),udt.forEach(t),xvr=r(Wwe," \u2014 "),EO=n(Wwe,"A",{href:!0});var bdt=s(EO);kvr=r(bdt,"FlaxBartForQuestionAnswering"),bdt.forEach(t),Rvr=r(Wwe," (BART model)"),Wwe.forEach(t),Svr=i(eo),rF=n(eo,"LI",{});var Qwe=s(rF);p1e=n(Qwe,"STRONG",{});var vdt=s(p1e);Pvr=r(vdt,"bert"),vdt.forEach(t),$vr=r(Qwe," \u2014 "),yO=n(Qwe,"A",{href:!0});var Tdt=s(yO);Ivr=r(Tdt,"FlaxBertForQuestionAnswering"),Tdt.forEach(t),jvr=r(Qwe," (BERT model)"),Qwe.forEach(t),Nvr=i(eo),tF=n(eo,"LI",{});var Hwe=s(tF);_1e=n(Hwe,"STRONG",{});var Fdt=s(_1e);Dvr=r(Fdt,"big_bird"),Fdt.forEach(t),qvr=r(Hwe," \u2014 "),wO=n(Hwe,"A",{href:!0});var Cdt=s(wO);Gvr=r(Cdt,"FlaxBigBirdForQuestionAnswering"),Cdt.forEach(t),Ovr=r(Hwe," (BigBird model)"),Hwe.forEach(t),Xvr=i(eo),aF=n(eo,"LI",{});var Uwe=s(aF);u1e=n(Uwe,"STRONG",{});var Mdt=s(u1e);zvr=r(Mdt,"distilbert"),Mdt.forEach(t),Vvr=r(Uwe," \u2014 "),AO=n(Uwe,"A",{href:!0});var Edt=s(AO);Wvr=r(Edt,"FlaxDistilBertForQuestionAnswering"),Edt.forEach(t),Qvr=r(Uwe," (DistilBERT model)"),Uwe.forEach(t),Hvr=i(eo),nF=n(eo,"LI",{});var Jwe=s(nF);b1e=n(Jwe,"STRONG",{});var ydt=s(b1e);Uvr=r(ydt,"electra"),ydt.forEach(t),Jvr=r(Jwe," \u2014 "),LO=n(Jwe,"A",{href:!0});var wdt=s(LO);Yvr=r(wdt,"FlaxElectraForQuestionAnswering"),wdt.forEach(t),Kvr=r(Jwe," (ELECTRA model)"),Jwe.forEach(t),Zvr=i(eo),sF=n(eo,"LI",{});var Ywe=s(sF);v1e=n(Ywe,"STRONG",{});var Adt=s(v1e);e6r=r(Adt,"mbart"),Adt.forEach(t),o6r=r(Ywe," \u2014 "),BO=n(Ywe,"A",{href:!0});var Ldt=s(BO);r6r=r(Ldt,"FlaxMBartForQuestionAnswering"),Ldt.forEach(t),t6r=r(Ywe," (mBART model)"),Ywe.forEach(t),a6r=i(eo),lF=n(eo,"LI",{});var Kwe=s(lF);T1e=n(Kwe,"STRONG",{});var Bdt=s(T1e);n6r=r(Bdt,"roberta"),Bdt.forEach(t),s6r=r(Kwe," \u2014 "),xO=n(Kwe,"A",{href:!0});var xdt=s(xO);l6r=r(xdt,"FlaxRobertaForQuestionAnswering"),xdt.forEach(t),i6r=r(Kwe," (RoBERTa model)"),Kwe.forEach(t),d6r=i(eo),iF=n(eo,"LI",{});var Zwe=s(iF);F1e=n(Zwe,"STRONG",{});var kdt=s(F1e);c6r=r(kdt,"roformer"),kdt.forEach(t),f6r=r(Zwe," \u2014 "),kO=n(Zwe,"A",{href:!0});var Rdt=s(kO);m6r=r(Rdt,"FlaxRoFormerForQuestionAnswering"),Rdt.forEach(t),g6r=r(Zwe," (RoFormer model)"),Zwe.forEach(t),eo.forEach(t),h6r=i(ya),C1e=n(ya,"P",{});var Sdt=s(C1e);p6r=r(Sdt,"Examples:"),Sdt.forEach(t),_6r=i(ya),m(ww.$$.fragment,ya),ya.forEach(t),si.forEach(t),nLe=i(d),Jc=n(d,"H2",{class:!0});var mBe=s(Jc);dF=n(mBe,"A",{id:!0,class:!0,href:!0});var Pdt=s(dF);M1e=n(Pdt,"SPAN",{});var $dt=s(M1e);m(Aw.$$.fragment,$dt),$dt.forEach(t),Pdt.forEach(t),u6r=i(mBe),E1e=n(mBe,"SPAN",{});var Idt=s(E1e);b6r=r(Idt,"FlaxAutoModelForTokenClassification"),Idt.forEach(t),mBe.forEach(t),sLe=i(d),kr=n(d,"DIV",{class:!0});var ii=s(kr);m(Lw.$$.fragment,ii),v6r=i(ii),Yc=n(ii,"P",{});var Bz=s(Yc);T6r=r(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),y1e=n(Bz,"CODE",{});var jdt=s(y1e);F6r=r(jdt,"from_pretrained()"),jdt.forEach(t),C6r=r(Bz,"class method or the "),w1e=n(Bz,"CODE",{});var Ndt=s(w1e);M6r=r(Ndt,"from_config()"),Ndt.forEach(t),E6r=r(Bz,`class
method.`),Bz.forEach(t),y6r=i(ii),Bw=n(ii,"P",{});var gBe=s(Bw);w6r=r(gBe,"This class cannot be instantiated directly using "),A1e=n(gBe,"CODE",{});var Ddt=s(A1e);A6r=r(Ddt,"__init__()"),Ddt.forEach(t),L6r=r(gBe," (throws an error)."),gBe.forEach(t),B6r=i(ii),Mt=n(ii,"DIV",{class:!0});var di=s(Mt);m(xw.$$.fragment,di),x6r=i(di),L1e=n(di,"P",{});var qdt=s(L1e);k6r=r(qdt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),qdt.forEach(t),R6r=i(di),Kc=n(di,"P",{});var xz=s(Kc);S6r=r(xz,`Note:
Loading a model from its configuration file does `),B1e=n(xz,"STRONG",{});var Gdt=s(B1e);P6r=r(Gdt,"not"),Gdt.forEach(t),$6r=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),x1e=n(xz,"CODE",{});var Odt=s(x1e);I6r=r(Odt,"from_pretrained()"),Odt.forEach(t),j6r=r(xz,"to load the model weights."),xz.forEach(t),N6r=i(di),k1e=n(di,"P",{});var Xdt=s(k1e);D6r=r(Xdt,"Examples:"),Xdt.forEach(t),q6r=i(di),m(kw.$$.fragment,di),di.forEach(t),G6r=i(ii),ko=n(ii,"DIV",{class:!0});var wa=s(ko);m(Rw.$$.fragment,wa),O6r=i(wa),R1e=n(wa,"P",{});var zdt=s(R1e);X6r=r(zdt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),zdt.forEach(t),z6r=i(wa),En=n(wa,"P",{});var MC=s(En);V6r=r(MC,"The model class to instantiate is selected based on the "),S1e=n(MC,"CODE",{});var Vdt=s(S1e);W6r=r(Vdt,"model_type"),Vdt.forEach(t),Q6r=r(MC,` property of the config object (either
passed as an argument or loaded from `),P1e=n(MC,"CODE",{});var Wdt=s(P1e);H6r=r(Wdt,"pretrained_model_name_or_path"),Wdt.forEach(t),U6r=r(MC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$1e=n(MC,"CODE",{});var Qdt=s($1e);J6r=r(Qdt,"pretrained_model_name_or_path"),Qdt.forEach(t),Y6r=r(MC,":"),MC.forEach(t),K6r=i(wa),ao=n(wa,"UL",{});var Zt=s(ao);cF=n(Zt,"LI",{});var eAe=s(cF);I1e=n(eAe,"STRONG",{});var Hdt=s(I1e);Z6r=r(Hdt,"albert"),Hdt.forEach(t),eTr=r(eAe," \u2014 "),RO=n(eAe,"A",{href:!0});var Udt=s(RO);oTr=r(Udt,"FlaxAlbertForTokenClassification"),Udt.forEach(t),rTr=r(eAe," (ALBERT model)"),eAe.forEach(t),tTr=i(Zt),fF=n(Zt,"LI",{});var oAe=s(fF);j1e=n(oAe,"STRONG",{});var Jdt=s(j1e);aTr=r(Jdt,"bert"),Jdt.forEach(t),nTr=r(oAe," \u2014 "),SO=n(oAe,"A",{href:!0});var Ydt=s(SO);sTr=r(Ydt,"FlaxBertForTokenClassification"),Ydt.forEach(t),lTr=r(oAe," (BERT model)"),oAe.forEach(t),iTr=i(Zt),mF=n(Zt,"LI",{});var rAe=s(mF);N1e=n(rAe,"STRONG",{});var Kdt=s(N1e);dTr=r(Kdt,"big_bird"),Kdt.forEach(t),cTr=r(rAe," \u2014 "),PO=n(rAe,"A",{href:!0});var Zdt=s(PO);fTr=r(Zdt,"FlaxBigBirdForTokenClassification"),Zdt.forEach(t),mTr=r(rAe," (BigBird model)"),rAe.forEach(t),gTr=i(Zt),gF=n(Zt,"LI",{});var tAe=s(gF);D1e=n(tAe,"STRONG",{});var ect=s(D1e);hTr=r(ect,"distilbert"),ect.forEach(t),pTr=r(tAe," \u2014 "),$O=n(tAe,"A",{href:!0});var oct=s($O);_Tr=r(oct,"FlaxDistilBertForTokenClassification"),oct.forEach(t),uTr=r(tAe," (DistilBERT model)"),tAe.forEach(t),bTr=i(Zt),hF=n(Zt,"LI",{});var aAe=s(hF);q1e=n(aAe,"STRONG",{});var rct=s(q1e);vTr=r(rct,"electra"),rct.forEach(t),TTr=r(aAe," \u2014 "),IO=n(aAe,"A",{href:!0});var tct=s(IO);FTr=r(tct,"FlaxElectraForTokenClassification"),tct.forEach(t),CTr=r(aAe," (ELECTRA model)"),aAe.forEach(t),MTr=i(Zt),pF=n(Zt,"LI",{});var nAe=s(pF);G1e=n(nAe,"STRONG",{});var act=s(G1e);ETr=r(act,"roberta"),act.forEach(t),yTr=r(nAe," \u2014 "),jO=n(nAe,"A",{href:!0});var nct=s(jO);wTr=r(nct,"FlaxRobertaForTokenClassification"),nct.forEach(t),ATr=r(nAe," (RoBERTa model)"),nAe.forEach(t),LTr=i(Zt),_F=n(Zt,"LI",{});var sAe=s(_F);O1e=n(sAe,"STRONG",{});var sct=s(O1e);BTr=r(sct,"roformer"),sct.forEach(t),xTr=r(sAe," \u2014 "),NO=n(sAe,"A",{href:!0});var lct=s(NO);kTr=r(lct,"FlaxRoFormerForTokenClassification"),lct.forEach(t),RTr=r(sAe," (RoFormer model)"),sAe.forEach(t),Zt.forEach(t),STr=i(wa),X1e=n(wa,"P",{});var ict=s(X1e);PTr=r(ict,"Examples:"),ict.forEach(t),$Tr=i(wa),m(Sw.$$.fragment,wa),wa.forEach(t),ii.forEach(t),lLe=i(d),Zc=n(d,"H2",{class:!0});var hBe=s(Zc);uF=n(hBe,"A",{id:!0,class:!0,href:!0});var dct=s(uF);z1e=n(dct,"SPAN",{});var cct=s(z1e);m(Pw.$$.fragment,cct),cct.forEach(t),dct.forEach(t),ITr=i(hBe),V1e=n(hBe,"SPAN",{});var fct=s(V1e);jTr=r(fct,"FlaxAutoModelForMultipleChoice"),fct.forEach(t),hBe.forEach(t),iLe=i(d),Rr=n(d,"DIV",{class:!0});var ci=s(Rr);m($w.$$.fragment,ci),NTr=i(ci),ef=n(ci,"P",{});var kz=s(ef);DTr=r(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),W1e=n(kz,"CODE",{});var mct=s(W1e);qTr=r(mct,"from_pretrained()"),mct.forEach(t),GTr=r(kz,"class method or the "),Q1e=n(kz,"CODE",{});var gct=s(Q1e);OTr=r(gct,"from_config()"),gct.forEach(t),XTr=r(kz,`class
method.`),kz.forEach(t),zTr=i(ci),Iw=n(ci,"P",{});var pBe=s(Iw);VTr=r(pBe,"This class cannot be instantiated directly using "),H1e=n(pBe,"CODE",{});var hct=s(H1e);WTr=r(hct,"__init__()"),hct.forEach(t),QTr=r(pBe," (throws an error)."),pBe.forEach(t),HTr=i(ci),Et=n(ci,"DIV",{class:!0});var fi=s(Et);m(jw.$$.fragment,fi),UTr=i(fi),U1e=n(fi,"P",{});var pct=s(U1e);JTr=r(pct,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),pct.forEach(t),YTr=i(fi),of=n(fi,"P",{});var Rz=s(of);KTr=r(Rz,`Note:
Loading a model from its configuration file does `),J1e=n(Rz,"STRONG",{});var _ct=s(J1e);ZTr=r(_ct,"not"),_ct.forEach(t),e7r=r(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Y1e=n(Rz,"CODE",{});var uct=s(Y1e);o7r=r(uct,"from_pretrained()"),uct.forEach(t),r7r=r(Rz,"to load the model weights."),Rz.forEach(t),t7r=i(fi),K1e=n(fi,"P",{});var bct=s(K1e);a7r=r(bct,"Examples:"),bct.forEach(t),n7r=i(fi),m(Nw.$$.fragment,fi),fi.forEach(t),s7r=i(ci),Ro=n(ci,"DIV",{class:!0});var Aa=s(Ro);m(Dw.$$.fragment,Aa),l7r=i(Aa),Z1e=n(Aa,"P",{});var vct=s(Z1e);i7r=r(vct,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),vct.forEach(t),d7r=i(Aa),yn=n(Aa,"P",{});var EC=s(yn);c7r=r(EC,"The model class to instantiate is selected based on the "),ebe=n(EC,"CODE",{});var Tct=s(ebe);f7r=r(Tct,"model_type"),Tct.forEach(t),m7r=r(EC,` property of the config object (either
passed as an argument or loaded from `),obe=n(EC,"CODE",{});var Fct=s(obe);g7r=r(Fct,"pretrained_model_name_or_path"),Fct.forEach(t),h7r=r(EC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rbe=n(EC,"CODE",{});var Cct=s(rbe);p7r=r(Cct,"pretrained_model_name_or_path"),Cct.forEach(t),_7r=r(EC,":"),EC.forEach(t),u7r=i(Aa),no=n(Aa,"UL",{});var ea=s(no);bF=n(ea,"LI",{});var lAe=s(bF);tbe=n(lAe,"STRONG",{});var Mct=s(tbe);b7r=r(Mct,"albert"),Mct.forEach(t),v7r=r(lAe," \u2014 "),DO=n(lAe,"A",{href:!0});var Ect=s(DO);T7r=r(Ect,"FlaxAlbertForMultipleChoice"),Ect.forEach(t),F7r=r(lAe," (ALBERT model)"),lAe.forEach(t),C7r=i(ea),vF=n(ea,"LI",{});var iAe=s(vF);abe=n(iAe,"STRONG",{});var yct=s(abe);M7r=r(yct,"bert"),yct.forEach(t),E7r=r(iAe," \u2014 "),qO=n(iAe,"A",{href:!0});var wct=s(qO);y7r=r(wct,"FlaxBertForMultipleChoice"),wct.forEach(t),w7r=r(iAe," (BERT model)"),iAe.forEach(t),A7r=i(ea),TF=n(ea,"LI",{});var dAe=s(TF);nbe=n(dAe,"STRONG",{});var Act=s(nbe);L7r=r(Act,"big_bird"),Act.forEach(t),B7r=r(dAe," \u2014 "),GO=n(dAe,"A",{href:!0});var Lct=s(GO);x7r=r(Lct,"FlaxBigBirdForMultipleChoice"),Lct.forEach(t),k7r=r(dAe," (BigBird model)"),dAe.forEach(t),R7r=i(ea),FF=n(ea,"LI",{});var cAe=s(FF);sbe=n(cAe,"STRONG",{});var Bct=s(sbe);S7r=r(Bct,"distilbert"),Bct.forEach(t),P7r=r(cAe," \u2014 "),OO=n(cAe,"A",{href:!0});var xct=s(OO);$7r=r(xct,"FlaxDistilBertForMultipleChoice"),xct.forEach(t),I7r=r(cAe," (DistilBERT model)"),cAe.forEach(t),j7r=i(ea),CF=n(ea,"LI",{});var fAe=s(CF);lbe=n(fAe,"STRONG",{});var kct=s(lbe);N7r=r(kct,"electra"),kct.forEach(t),D7r=r(fAe," \u2014 "),XO=n(fAe,"A",{href:!0});var Rct=s(XO);q7r=r(Rct,"FlaxElectraForMultipleChoice"),Rct.forEach(t),G7r=r(fAe," (ELECTRA model)"),fAe.forEach(t),O7r=i(ea),MF=n(ea,"LI",{});var mAe=s(MF);ibe=n(mAe,"STRONG",{});var Sct=s(ibe);X7r=r(Sct,"roberta"),Sct.forEach(t),z7r=r(mAe," \u2014 "),zO=n(mAe,"A",{href:!0});var Pct=s(zO);V7r=r(Pct,"FlaxRobertaForMultipleChoice"),Pct.forEach(t),W7r=r(mAe," (RoBERTa model)"),mAe.forEach(t),Q7r=i(ea),EF=n(ea,"LI",{});var gAe=s(EF);dbe=n(gAe,"STRONG",{});var $ct=s(dbe);H7r=r($ct,"roformer"),$ct.forEach(t),U7r=r(gAe," \u2014 "),VO=n(gAe,"A",{href:!0});var Ict=s(VO);J7r=r(Ict,"FlaxRoFormerForMultipleChoice"),Ict.forEach(t),Y7r=r(gAe," (RoFormer model)"),gAe.forEach(t),ea.forEach(t),K7r=i(Aa),cbe=n(Aa,"P",{});var jct=s(cbe);Z7r=r(jct,"Examples:"),jct.forEach(t),e8r=i(Aa),m(qw.$$.fragment,Aa),Aa.forEach(t),ci.forEach(t),dLe=i(d),rf=n(d,"H2",{class:!0});var _Be=s(rf);yF=n(_Be,"A",{id:!0,class:!0,href:!0});var Nct=s(yF);fbe=n(Nct,"SPAN",{});var Dct=s(fbe);m(Gw.$$.fragment,Dct),Dct.forEach(t),Nct.forEach(t),o8r=i(_Be),mbe=n(_Be,"SPAN",{});var qct=s(mbe);r8r=r(qct,"FlaxAutoModelForNextSentencePrediction"),qct.forEach(t),_Be.forEach(t),cLe=i(d),Sr=n(d,"DIV",{class:!0});var mi=s(Sr);m(Ow.$$.fragment,mi),t8r=i(mi),tf=n(mi,"P",{});var Sz=s(tf);a8r=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),gbe=n(Sz,"CODE",{});var Gct=s(gbe);n8r=r(Gct,"from_pretrained()"),Gct.forEach(t),s8r=r(Sz,"class method or the "),hbe=n(Sz,"CODE",{});var Oct=s(hbe);l8r=r(Oct,"from_config()"),Oct.forEach(t),i8r=r(Sz,`class
method.`),Sz.forEach(t),d8r=i(mi),Xw=n(mi,"P",{});var uBe=s(Xw);c8r=r(uBe,"This class cannot be instantiated directly using "),pbe=n(uBe,"CODE",{});var Xct=s(pbe);f8r=r(Xct,"__init__()"),Xct.forEach(t),m8r=r(uBe," (throws an error)."),uBe.forEach(t),g8r=i(mi),yt=n(mi,"DIV",{class:!0});var gi=s(yt);m(zw.$$.fragment,gi),h8r=i(gi),_be=n(gi,"P",{});var zct=s(_be);p8r=r(zct,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),zct.forEach(t),_8r=i(gi),af=n(gi,"P",{});var Pz=s(af);u8r=r(Pz,`Note:
Loading a model from its configuration file does `),ube=n(Pz,"STRONG",{});var Vct=s(ube);b8r=r(Vct,"not"),Vct.forEach(t),v8r=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),bbe=n(Pz,"CODE",{});var Wct=s(bbe);T8r=r(Wct,"from_pretrained()"),Wct.forEach(t),F8r=r(Pz,"to load the model weights."),Pz.forEach(t),C8r=i(gi),vbe=n(gi,"P",{});var Qct=s(vbe);M8r=r(Qct,"Examples:"),Qct.forEach(t),E8r=i(gi),m(Vw.$$.fragment,gi),gi.forEach(t),y8r=i(mi),So=n(mi,"DIV",{class:!0});var La=s(So);m(Ww.$$.fragment,La),w8r=i(La),Tbe=n(La,"P",{});var Hct=s(Tbe);A8r=r(Hct,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Hct.forEach(t),L8r=i(La),wn=n(La,"P",{});var yC=s(wn);B8r=r(yC,"The model class to instantiate is selected based on the "),Fbe=n(yC,"CODE",{});var Uct=s(Fbe);x8r=r(Uct,"model_type"),Uct.forEach(t),k8r=r(yC,` property of the config object (either
passed as an argument or loaded from `),Cbe=n(yC,"CODE",{});var Jct=s(Cbe);R8r=r(Jct,"pretrained_model_name_or_path"),Jct.forEach(t),S8r=r(yC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mbe=n(yC,"CODE",{});var Yct=s(Mbe);P8r=r(Yct,"pretrained_model_name_or_path"),Yct.forEach(t),$8r=r(yC,":"),yC.forEach(t),I8r=i(La),Ebe=n(La,"UL",{});var Kct=s(Ebe);wF=n(Kct,"LI",{});var hAe=s(wF);ybe=n(hAe,"STRONG",{});var Zct=s(ybe);j8r=r(Zct,"bert"),Zct.forEach(t),N8r=r(hAe," \u2014 "),WO=n(hAe,"A",{href:!0});var eft=s(WO);D8r=r(eft,"FlaxBertForNextSentencePrediction"),eft.forEach(t),q8r=r(hAe," (BERT model)"),hAe.forEach(t),Kct.forEach(t),G8r=i(La),wbe=n(La,"P",{});var oft=s(wbe);O8r=r(oft,"Examples:"),oft.forEach(t),X8r=i(La),m(Qw.$$.fragment,La),La.forEach(t),mi.forEach(t),fLe=i(d),nf=n(d,"H2",{class:!0});var bBe=s(nf);AF=n(bBe,"A",{id:!0,class:!0,href:!0});var rft=s(AF);Abe=n(rft,"SPAN",{});var tft=s(Abe);m(Hw.$$.fragment,tft),tft.forEach(t),rft.forEach(t),z8r=i(bBe),Lbe=n(bBe,"SPAN",{});var aft=s(Lbe);V8r=r(aft,"FlaxAutoModelForImageClassification"),aft.forEach(t),bBe.forEach(t),mLe=i(d),Pr=n(d,"DIV",{class:!0});var hi=s(Pr);m(Uw.$$.fragment,hi),W8r=i(hi),sf=n(hi,"P",{});var $z=s(sf);Q8r=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Bbe=n($z,"CODE",{});var nft=s(Bbe);H8r=r(nft,"from_pretrained()"),nft.forEach(t),U8r=r($z,"class method or the "),xbe=n($z,"CODE",{});var sft=s(xbe);J8r=r(sft,"from_config()"),sft.forEach(t),Y8r=r($z,`class
method.`),$z.forEach(t),K8r=i(hi),Jw=n(hi,"P",{});var vBe=s(Jw);Z8r=r(vBe,"This class cannot be instantiated directly using "),kbe=n(vBe,"CODE",{});var lft=s(kbe);eFr=r(lft,"__init__()"),lft.forEach(t),oFr=r(vBe," (throws an error)."),vBe.forEach(t),rFr=i(hi),wt=n(hi,"DIV",{class:!0});var pi=s(wt);m(Yw.$$.fragment,pi),tFr=i(pi),Rbe=n(pi,"P",{});var ift=s(Rbe);aFr=r(ift,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),ift.forEach(t),nFr=i(pi),lf=n(pi,"P",{});var Iz=s(lf);sFr=r(Iz,`Note:
Loading a model from its configuration file does `),Sbe=n(Iz,"STRONG",{});var dft=s(Sbe);lFr=r(dft,"not"),dft.forEach(t),iFr=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=n(Iz,"CODE",{});var cft=s(Pbe);dFr=r(cft,"from_pretrained()"),cft.forEach(t),cFr=r(Iz,"to load the model weights."),Iz.forEach(t),fFr=i(pi),$be=n(pi,"P",{});var fft=s($be);mFr=r(fft,"Examples:"),fft.forEach(t),gFr=i(pi),m(Kw.$$.fragment,pi),pi.forEach(t),hFr=i(hi),Po=n(hi,"DIV",{class:!0});var Ba=s(Po);m(Zw.$$.fragment,Ba),pFr=i(Ba),Ibe=n(Ba,"P",{});var mft=s(Ibe);_Fr=r(mft,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),mft.forEach(t),uFr=i(Ba),An=n(Ba,"P",{});var wC=s(An);bFr=r(wC,"The model class to instantiate is selected based on the "),jbe=n(wC,"CODE",{});var gft=s(jbe);vFr=r(gft,"model_type"),gft.forEach(t),TFr=r(wC,` property of the config object (either
passed as an argument or loaded from `),Nbe=n(wC,"CODE",{});var hft=s(Nbe);FFr=r(hft,"pretrained_model_name_or_path"),hft.forEach(t),CFr=r(wC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dbe=n(wC,"CODE",{});var pft=s(Dbe);MFr=r(pft,"pretrained_model_name_or_path"),pft.forEach(t),EFr=r(wC,":"),wC.forEach(t),yFr=i(Ba),eA=n(Ba,"UL",{});var TBe=s(eA);LF=n(TBe,"LI",{});var pAe=s(LF);qbe=n(pAe,"STRONG",{});var _ft=s(qbe);wFr=r(_ft,"beit"),_ft.forEach(t),AFr=r(pAe," \u2014 "),QO=n(pAe,"A",{href:!0});var uft=s(QO);LFr=r(uft,"FlaxBeitForImageClassification"),uft.forEach(t),BFr=r(pAe," (BEiT model)"),pAe.forEach(t),xFr=i(TBe),BF=n(TBe,"LI",{});var _Ae=s(BF);Gbe=n(_Ae,"STRONG",{});var bft=s(Gbe);kFr=r(bft,"vit"),bft.forEach(t),RFr=r(_Ae," \u2014 "),HO=n(_Ae,"A",{href:!0});var vft=s(HO);SFr=r(vft,"FlaxViTForImageClassification"),vft.forEach(t),PFr=r(_Ae," (ViT model)"),_Ae.forEach(t),TBe.forEach(t),$Fr=i(Ba),Obe=n(Ba,"P",{});var Tft=s(Obe);IFr=r(Tft,"Examples:"),Tft.forEach(t),jFr=i(Ba),m(oA.$$.fragment,Ba),Ba.forEach(t),hi.forEach(t),gLe=i(d),df=n(d,"H2",{class:!0});var FBe=s(df);xF=n(FBe,"A",{id:!0,class:!0,href:!0});var Fft=s(xF);Xbe=n(Fft,"SPAN",{});var Cft=s(Xbe);m(rA.$$.fragment,Cft),Cft.forEach(t),Fft.forEach(t),NFr=i(FBe),zbe=n(FBe,"SPAN",{});var Mft=s(zbe);DFr=r(Mft,"FlaxAutoModelForVision2Seq"),Mft.forEach(t),FBe.forEach(t),hLe=i(d),$r=n(d,"DIV",{class:!0});var _i=s($r);m(tA.$$.fragment,_i),qFr=i(_i),cf=n(_i,"P",{});var jz=s(cf);GFr=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Vbe=n(jz,"CODE",{});var Eft=s(Vbe);OFr=r(Eft,"from_pretrained()"),Eft.forEach(t),XFr=r(jz,"class method or the "),Wbe=n(jz,"CODE",{});var yft=s(Wbe);zFr=r(yft,"from_config()"),yft.forEach(t),VFr=r(jz,`class
method.`),jz.forEach(t),WFr=i(_i),aA=n(_i,"P",{});var CBe=s(aA);QFr=r(CBe,"This class cannot be instantiated directly using "),Qbe=n(CBe,"CODE",{});var wft=s(Qbe);HFr=r(wft,"__init__()"),wft.forEach(t),UFr=r(CBe," (throws an error)."),CBe.forEach(t),JFr=i(_i),At=n(_i,"DIV",{class:!0});var ui=s(At);m(nA.$$.fragment,ui),YFr=i(ui),Hbe=n(ui,"P",{});var Aft=s(Hbe);KFr=r(Aft,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Aft.forEach(t),ZFr=i(ui),ff=n(ui,"P",{});var Nz=s(ff);eCr=r(Nz,`Note:
Loading a model from its configuration file does `),Ube=n(Nz,"STRONG",{});var Lft=s(Ube);oCr=r(Lft,"not"),Lft.forEach(t),rCr=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jbe=n(Nz,"CODE",{});var Bft=s(Jbe);tCr=r(Bft,"from_pretrained()"),Bft.forEach(t),aCr=r(Nz,"to load the model weights."),Nz.forEach(t),nCr=i(ui),Ybe=n(ui,"P",{});var xft=s(Ybe);sCr=r(xft,"Examples:"),xft.forEach(t),lCr=i(ui),m(sA.$$.fragment,ui),ui.forEach(t),iCr=i(_i),$o=n(_i,"DIV",{class:!0});var xa=s($o);m(lA.$$.fragment,xa),dCr=i(xa),Kbe=n(xa,"P",{});var kft=s(Kbe);cCr=r(kft,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),kft.forEach(t),fCr=i(xa),Ln=n(xa,"P",{});var AC=s(Ln);mCr=r(AC,"The model class to instantiate is selected based on the "),Zbe=n(AC,"CODE",{});var Rft=s(Zbe);gCr=r(Rft,"model_type"),Rft.forEach(t),hCr=r(AC,` property of the config object (either
passed as an argument or loaded from `),e5e=n(AC,"CODE",{});var Sft=s(e5e);pCr=r(Sft,"pretrained_model_name_or_path"),Sft.forEach(t),_Cr=r(AC,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o5e=n(AC,"CODE",{});var Pft=s(o5e);uCr=r(Pft,"pretrained_model_name_or_path"),Pft.forEach(t),bCr=r(AC,":"),AC.forEach(t),vCr=i(xa),r5e=n(xa,"UL",{});var $ft=s(r5e);kF=n($ft,"LI",{});var uAe=s(kF);t5e=n(uAe,"STRONG",{});var Ift=s(t5e);TCr=r(Ift,"vision-encoder-decoder"),Ift.forEach(t),FCr=r(uAe," \u2014 "),UO=n(uAe,"A",{href:!0});var jft=s(UO);CCr=r(jft,"FlaxVisionEncoderDecoderModel"),jft.forEach(t),MCr=r(uAe," (Vision Encoder decoder model)"),uAe.forEach(t),$ft.forEach(t),ECr=i(xa),a5e=n(xa,"P",{});var Nft=s(a5e);yCr=r(Nft,"Examples:"),Nft.forEach(t),wCr=i(xa),m(iA.$$.fragment,xa),xa.forEach(t),_i.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(Qft)),c(me,"id","auto-classes"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#auto-classes"),c(le,"class","relative group"),c(Bn,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoConfig"),c(kn,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoModel"),c(Rn,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoTokenizer"),c(yi,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertModel"),c(uf,"id","extending-the-auto-classes"),c(uf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uf,"href","#extending-the-auto-classes"),c(wi,"class","relative group"),c(vf,"id","transformers.AutoConfig"),c(vf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vf,"href","#transformers.AutoConfig"),c(Ai,"class","relative group"),c(f0,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(m0,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertConfig"),c(g0,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartConfig"),c(h0,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitConfig"),c(p0,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertConfig"),c(_0,"href","/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(u0,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdConfig"),c(b0,"href","/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(v0,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(T0,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(F0,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertConfig"),c(C0,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineConfig"),c(M0,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPConfig"),c(E0,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertConfig"),c(y0,"href","/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextConfig"),c(w0,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLConfig"),c(A0,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaConfig"),c(L0,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(B0,"href","/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTConfig"),c(x0,"href","/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrConfig"),c(k0,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertConfig"),c(R0,"href","/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRConfig"),c(S0,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraConfig"),c(P0,"href","/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c($0,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertConfig"),c(I0,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetConfig"),c(j0,"href","/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTConfig"),c(N0,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelConfig"),c(D0,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Config"),c(q0,"href","/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(G0,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJConfig"),c(O0,"href","/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertConfig"),c(X0,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertConfig"),c(z0,"href","/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(V0,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(W0,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(Q0,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDConfig"),c(H0,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerConfig"),c(U0,"href","/docs/transformers/pr_15678/en/model_doc/luke#transformers.LukeConfig"),c(J0,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertConfig"),c(Y0,"href","/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100Config"),c(K0,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianConfig"),c(Z0,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartConfig"),c(eL,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(oL,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(rL,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetConfig"),c(tL,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Config"),c(aL,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(nL,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(sL,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusConfig"),c(lL,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverConfig"),c(iL,"href","/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(dL,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(cL,"href","/docs/transformers/pr_15678/en/model_doc/rag#transformers.RagConfig"),c(fL,"href","/docs/transformers/pr_15678/en/model_doc/realm#transformers.RealmConfig"),c(mL,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerConfig"),c(gL,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertConfig"),c(hL,"href","/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertConfig"),c(pL,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaConfig"),c(_L,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerConfig"),c(uL,"href","/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerConfig"),c(bL,"href","/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWConfig"),c(vL,"href","/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDConfig"),c(TL,"href","/docs/transformers/pr_15678/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(FL,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(CL,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(ML,"href","/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterConfig"),c(EL,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(yL,"href","/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinConfig"),c(wL,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Config"),c(AL,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasConfig"),c(LL,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(BL,"href","/docs/transformers/pr_15678/en/model_doc/trocr#transformers.TrOCRConfig"),c(xL,"href","/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(kL,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(RL,"href","/docs/transformers/pr_15678/en/model_doc/vilt#transformers.ViltConfig"),c(SL,"href","/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(PL,"href","/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c($L,"href","/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(IL,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTConfig"),c(jL,"href","/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(NL,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(DL,"href","/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMConfig"),c(qL,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMConfig"),c(GL,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMConfig"),c(OL,"href","/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(XL,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(zL,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(VL,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetConfig"),c(WL,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoConfig"),c(io,"class","docstring"),c(og,"class","docstring"),c(qo,"class","docstring"),c(rg,"id","transformers.AutoTokenizer"),c(rg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rg,"href","#transformers.AutoTokenizer"),c(Bi,"class","relative group"),c(QL,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(HL,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertTokenizer"),c(UL,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(JL,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartTokenizer"),c(YL,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartTokenizerFast"),c(KL,"href","/docs/transformers/pr_15678/en/model_doc/barthez#transformers.BarthezTokenizer"),c(ZL,"href","/docs/transformers/pr_15678/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(e9,"href","/docs/transformers/pr_15678/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(o9,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertTokenizer"),c(r9,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertTokenizerFast"),c(t9,"href","/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(a9,"href","/docs/transformers/pr_15678/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(n9,"href","/docs/transformers/pr_15678/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(s9,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(l9,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(i9,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(d9,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(c9,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(f9,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(m9,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(g9,"href","/docs/transformers/pr_15678/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(h9,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertTokenizer"),c(p9,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(_9,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineTokenizer"),c(u9,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPTokenizer"),c(b9,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(v9,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(T9,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(F9,"href","/docs/transformers/pr_15678/en/model_doc/cpm#transformers.CpmTokenizer"),c(C9,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(M9,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaTokenizer"),c(E9,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(y9,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(w9,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(A9,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(L9,"href","/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(B9,"href","/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(x9,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraTokenizer"),c(k9,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(R9,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(S9,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetTokenizer"),c(P9,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetTokenizerFast"),c($9,"href","/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(I9,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelTokenizer"),c(j9,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(N9,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(D9,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(q9,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(G9,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(O9,"href","/docs/transformers/pr_15678/en/model_doc/herbert#transformers.HerbertTokenizer"),c(X9,"href","/docs/transformers/pr_15678/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(z9,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(V9,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaTokenizer"),c(W9,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Q9,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(H9,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(U9,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(J9,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(Y9,"href","/docs/transformers/pr_15678/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(K9,"href","/docs/transformers/pr_15678/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(Z9,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDTokenizer"),c(eB,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDTokenizerFast"),c(oB,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerTokenizer"),c(rB,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(tB,"href","/docs/transformers/pr_15678/en/model_doc/luke#transformers.LukeTokenizer"),c(aB,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(nB,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(sB,"href","/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(lB,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianTokenizer"),c(iB,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartTokenizer"),c(dB,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(cB,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(fB,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(mB,"href","/docs/transformers/pr_15678/en/model_doc/mluke#transformers.MLukeTokenizer"),c(gB,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(hB,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(pB,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(_B,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(uB,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.T5Tokenizer"),c(bB,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.T5TokenizerFast"),c(vB,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(TB,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(FB,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(CB,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(MB,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(EB,"href","/docs/transformers/pr_15678/en/model_doc/phobert#transformers.PhobertTokenizer"),c(yB,"href","/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(wB,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertTokenizer"),c(AB,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertTokenizerFast"),c(LB,"href","/docs/transformers/pr_15678/en/model_doc/rag#transformers.RagTokenizer"),c(BB,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerTokenizer"),c(xB,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(kB,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertTokenizer"),c(RB,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(SB,"href","/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(PB,"href","/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c($B,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaTokenizer"),c(IB,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(jB,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(NB,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(DB,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(qB,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(GB,"href","/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterTokenizer"),c(OB,"href","/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(XB,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(zB,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(VB,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.T5Tokenizer"),c(WB,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.T5TokenizerFast"),c(QB,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasTokenizer"),c(HB,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(UB,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(JB,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(YB,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMTokenizer"),c(KB,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(ZB,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMTokenizer"),c(ex,"href","/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(ox,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(rx,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(tx,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(ax,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(co,"class","docstring"),c(kg,"class","docstring"),c(Go,"class","docstring"),c(Rg,"id","transformers.AutoFeatureExtractor"),c(Rg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rg,"href","#transformers.AutoFeatureExtractor"),c(xi,"class","relative group"),c(nx,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(sx,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(lx,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(ix,"href","/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(dx,"href","/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(cx,"href","/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(fx,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(mx,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(gx,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(hx,"href","/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(px,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(_x,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ux,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(bx,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(vx,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(Hg,"class","docstring"),c(Oo,"class","docstring"),c(Ug,"id","transformers.AutoProcessor"),c(Ug,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ug,"href","#transformers.AutoProcessor"),c(ki,"class","relative group"),c(Tx,"href","/docs/transformers/pr_15678/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(Fx,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPProcessor"),c(Cx,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(Mx,"href","/docs/transformers/pr_15678/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(Ex,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(yx,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(wx,"href","/docs/transformers/pr_15678/en/model_doc/trocr#transformers.TrOCRProcessor"),c(Ax,"href","/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(Lx,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(nh,"class","docstring"),c(Xo,"class","docstring"),c(sh,"id","transformers.AutoModel"),c(sh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sh,"href","#transformers.AutoModel"),c(Si,"class","relative group"),c(Ir,"class","docstring"),c(Bx,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertModel"),c(xx,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartModel"),c(kx,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitModel"),c(Rx,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertModel"),c(Sx,"href","/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(Px,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdModel"),c($x,"href","/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Ix,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(jx,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Nx,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertModel"),c(Dx,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineModel"),c(qx,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.CLIPModel"),c(Gx,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertModel"),c(Ox,"href","/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextModel"),c(Xx,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLModel"),c(zx,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaModel"),c(Vx,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(Wx,"href","/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTModel"),c(Qx,"href","/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrModel"),c(Hx,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertModel"),c(Ux,"href","/docs/transformers/pr_15678/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Jx,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraModel"),c(Yx,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertModel"),c(Kx,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetModel"),c(Zx,"href","/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTModel"),c(ek,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelModel"),c(ok,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelBaseModel"),c(rk,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2Model"),c(tk,"href","/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(ak,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJModel"),c(nk,"href","/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertModel"),c(sk,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertModel"),c(lk,"href","/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(ik,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(dk,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(ck,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDModel"),c(fk,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerModel"),c(mk,"href","/docs/transformers/pr_15678/en/model_doc/luke#transformers.LukeModel"),c(gk,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertModel"),c(hk,"href","/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100Model"),c(pk,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianModel"),c(_k,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartModel"),c(uk,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(bk,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertModel"),c(vk,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetModel"),c(Tk,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5Model"),c(Fk,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerModel"),c(Ck,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(Mk,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusModel"),c(Ek,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverModel"),c(yk,"href","/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(wk,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertModel"),c(Ak,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerModel"),c(Lk,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertModel"),c(Bk,"href","/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertModel"),c(xk,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaModel"),c(kk,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerModel"),c(Rk,"href","/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerModel"),c(Sk,"href","/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWModel"),c(Pk,"href","/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDModel"),c($k,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(Ik,"href","/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterModel"),c(jk,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(Nk,"href","/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinModel"),c(Dk,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5Model"),c(qk,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasModel"),c(Gk,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(Ok,"href","/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechModel"),c(Xk,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(zk,"href","/docs/transformers/pr_15678/en/model_doc/vilt#transformers.ViltModel"),c(Vk,"href","/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(Wk,"href","/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertModel"),c(Qk,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTModel"),c(Hk,"href","/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(Uk,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(Jk,"href","/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMModel"),c(Yk,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMModel"),c(Kk,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMModel"),c(Zk,"href","/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(eR,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(oR,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(rR,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetModel"),c(tR,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoModel"),c(xe,"class","docstring"),c(zo,"class","docstring"),c($p,"id","transformers.AutoModelForPreTraining"),c($p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($p,"href","#transformers.AutoModelForPreTraining"),c(Ii,"class","relative group"),c(jr,"class","docstring"),c(aR,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForPreTraining"),c(nR,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(sR,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForPreTraining"),c(lR,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(iR,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(dR,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(cR,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(fR,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(mR,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(gR,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForPreTraining"),c(hR,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(pR,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForPreTraining"),c(_R,"href","/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(uR,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(bR,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(vR,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(TR,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(FR,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(CR,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(MR,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(ER,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(yR,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(wR,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(AR,"href","/docs/transformers/pr_15678/en/model_doc/retribert#transformers.RetriBertModel"),c(LR,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(BR,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(xR,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(kR,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(RR,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(SR,"href","/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(PR,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c($R,"href","/docs/transformers/pr_15678/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(IR,"href","/docs/transformers/pr_15678/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(jR,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(NR,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(DR,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(qR,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(GR,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(F_,"id","transformers.AutoModelForCausalLM"),c(F_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(F_,"href","#transformers.AutoModelForCausalLM"),c(Di,"class","relative group"),c(Nr,"class","docstring"),c(OR,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForCausalLM"),c(XR,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertLMHeadModel"),c(zR,"href","/docs/transformers/pr_15678/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(VR,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(WR,"href","/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(QR,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(HR,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(UR,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(JR,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(YR,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForCausalLM"),c(KR,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(ZR,"href","/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(eS,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(oS,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianForCausalLM"),c(rS,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForCausalLM"),c(tS,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(aS,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(nS,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(sS,"href","/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(lS,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(iS,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(dS,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(cS,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(fS,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(mS,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(gS,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(hS,"href","/docs/transformers/pr_15678/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(pS,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(_S,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(uS,"href","/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(bS,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(vS,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(TS,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Wo,"class","docstring"),c(ru,"id","transformers.AutoModelForMaskedLM"),c(ru,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ru,"href","#transformers.AutoModelForMaskedLM"),c(Oi,"class","relative group"),c(Dr,"class","docstring"),c(FS,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(CS,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(MS,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForMaskedLM"),c(ES,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(yS,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(wS,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(AS,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(LS,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(BS,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(xS,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(kS,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(RS,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(SS,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(PS,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForMaskedLM"),c($S,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(IS,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(jS,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(NS,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(DS,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(qS,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(GS,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(OS,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(XS,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(zS,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(VS,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(WS,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(QS,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(HS,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(US,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(JS,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(YS,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(KS,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(ZS,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Qo,"class","docstring"),c(Nu,"id","transformers.AutoModelForSeq2SeqLM"),c(Nu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nu,"href","#transformers.AutoModelForSeq2SeqLM"),c(Vi,"class","relative group"),c(qr,"class","docstring"),c(eP,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(oP,"href","/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(rP,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(tP,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(aP,"href","/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(nP,"href","/docs/transformers/pr_15678/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(sP,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(lP,"href","/docs/transformers/pr_15678/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(iP,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.MarianMTModel"),c(dP,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(cP,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(fP,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(mP,"href","/docs/transformers/pr_15678/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(gP,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(hP,"href","/docs/transformers/pr_15678/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Ho,"class","docstring"),c(o1,"id","transformers.AutoModelForSequenceClassification"),c(o1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(o1,"href","#transformers.AutoModelForSequenceClassification"),c(Hi,"class","relative group"),c(Gr,"class","docstring"),c(pP,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(_P,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForSequenceClassification"),c(uP,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForSequenceClassification"),c(bP,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(vP,"href","/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(TP,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(FP,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(CP,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(MP,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(EP,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(yP,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(wP,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(AP,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(LP,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(BP,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(xP,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(kP,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(RP,"href","/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(SP,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(PP,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c($P,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(IP,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(jP,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDForSequenceClassification"),c(NP,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(DP,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(qP,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(GP,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(OP,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(XP,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(zP,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(VP,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(WP,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(QP,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(HP,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(UP,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(JP,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(YP,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(KP,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(ZP,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(e$,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(o$,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(r$,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(t$,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(a$,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Uo,"class","docstring"),c(Q1,"id","transformers.AutoModelForMultipleChoice"),c(Q1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q1,"href","#transformers.AutoModelForMultipleChoice"),c(Yi,"class","relative group"),c(Or,"class","docstring"),c(n$,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(s$,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForMultipleChoice"),c(l$,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(i$,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(d$,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(c$,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(f$,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(m$,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(g$,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(h$,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(p$,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(_$,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(u$,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(b$,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(v$,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(T$,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(F$,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(C$,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(M$,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(E$,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(y$,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(w$,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(A$,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(L$,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(B$,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(x$,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(k$,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Jo,"class","docstring"),c(Cb,"id","transformers.AutoModelForNextSentencePrediction"),c(Cb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cb,"href","#transformers.AutoModelForNextSentencePrediction"),c(ed,"class","relative group"),c(Xr,"class","docstring"),c(R$,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(S$,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(P$,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c($$,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(I$,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Yo,"class","docstring"),c(Bb,"id","transformers.AutoModelForTokenClassification"),c(Bb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bb,"href","#transformers.AutoModelForTokenClassification"),c(td,"class","relative group"),c(zr,"class","docstring"),c(j$,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(N$,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForTokenClassification"),c(D$,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(q$,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(G$,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForTokenClassification"),c(O$,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(X$,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(z$,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(V$,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(W$,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(Q$,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(H$,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(U$,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(J$,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(Y$,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(K$,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(Z$,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(eI,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(oI,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(rI,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(tI,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(aI,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(nI,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(sI,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(lI,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(iI,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(dI,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(cI,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(fI,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(mI,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(gI,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(hI,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Ko,"class","docstring"),c(d5,"id","transformers.AutoModelForQuestionAnswering"),c(d5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(d5,"href","#transformers.AutoModelForQuestionAnswering"),c(sd,"class","relative group"),c(Vr,"class","docstring"),c(pI,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(_I,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(uI,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(bI,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(vI,"href","/docs/transformers/pr_15678/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(TI,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(FI,"href","/docs/transformers/pr_15678/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(CI,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(MI,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(EI,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(yI,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(wI,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(AI,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(LI,"href","/docs/transformers/pr_15678/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(BI,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(xI,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(kI,"href","/docs/transformers/pr_15678/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(RI,"href","/docs/transformers/pr_15678/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(SI,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(PI,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c($I,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(II,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(jI,"href","/docs/transformers/pr_15678/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(NI,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(DI,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(qI,"href","/docs/transformers/pr_15678/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(GI,"href","/docs/transformers/pr_15678/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(OI,"href","/docs/transformers/pr_15678/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(XI,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(zI,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(VI,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(WI,"href","/docs/transformers/pr_15678/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(QI,"href","/docs/transformers/pr_15678/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(HI,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(UI,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(JI,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(YI,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(KI,"href","/docs/transformers/pr_15678/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(Zo,"class","docstring"),c(U5,"id","transformers.AutoModelForTableQuestionAnswering"),c(U5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U5,"href","#transformers.AutoModelForTableQuestionAnswering"),c(dd,"class","relative group"),c(Wr,"class","docstring"),c(ZI,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(er,"class","docstring"),c(K5,"id","transformers.AutoModelForImageClassification"),c(K5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K5,"href","#transformers.AutoModelForImageClassification"),c(md,"class","relative group"),c(Qr,"class","docstring"),c(ej,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitForImageClassification"),c(oj,"href","/docs/transformers/pr_15678/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(rj,"href","/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTForImageClassification"),c(tj,"href","/docs/transformers/pr_15678/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(aj,"href","/docs/transformers/pr_15678/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(nj,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(sj,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(lj,"href","/docs/transformers/pr_15678/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(ij,"href","/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(dj,"href","/docs/transformers/pr_15678/en/model_doc/swin#transformers.SwinForImageClassification"),c(cj,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(or,"class","docstring"),c(s2,"id","transformers.AutoModelForVision2Seq"),c(s2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s2,"href","#transformers.AutoModelForVision2Seq"),c(pd,"class","relative group"),c(Hr,"class","docstring"),c(fj,"href","/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(rr,"class","docstring"),c(d2,"id","transformers.AutoModelForAudioClassification"),c(d2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(d2,"href","#transformers.AutoModelForAudioClassification"),c(bd,"class","relative group"),c(Ur,"class","docstring"),c(mj,"href","/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(gj,"href","/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(hj,"href","/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(pj,"href","/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(_j,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(uj,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(bj,"href","/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(tr,"class","docstring"),c(b2,"id","transformers.AutoModelForAudioFrameClassification"),c(b2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b2,"href","#transformers.AutoModelForAudioFrameClassification"),c(Fd,"class","relative group"),c(Jr,"class","docstring"),c(vj,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(Tj,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(Fj,"href","/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(ar,"class","docstring"),c(M2,"id","transformers.AutoModelForCTC"),c(M2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M2,"href","#transformers.AutoModelForCTC"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(Cj,"href","/docs/transformers/pr_15678/en/model_doc/hubert#transformers.HubertForCTC"),c(Mj,"href","/docs/transformers/pr_15678/en/model_doc/sew#transformers.SEWForCTC"),c(Ej,"href","/docs/transformers/pr_15678/en/model_doc/sew-d#transformers.SEWDForCTC"),c(yj,"href","/docs/transformers/pr_15678/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(wj,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(Aj,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(Lj,"href","/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(nr,"class","docstring"),c(R2,"id","transformers.AutoModelForSpeechSeq2Seq"),c(R2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(R2,"href","#transformers.AutoModelForSpeechSeq2Seq"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(Bj,"href","/docs/transformers/pr_15678/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(xj,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(sr,"class","docstring"),c(I2,"id","transformers.AutoModelForAudioXVector"),c(I2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I2,"href","#transformers.AutoModelForAudioXVector"),c(kd,"class","relative group"),c(Zr,"class","docstring"),c(kj,"href","/docs/transformers/pr_15678/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(Rj,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(Sj,"href","/docs/transformers/pr_15678/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(lr,"class","docstring"),c(G2,"id","transformers.AutoModelForObjectDetection"),c(G2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G2,"href","#transformers.AutoModelForObjectDetection"),c($d,"class","relative group"),c(et,"class","docstring"),c(Pj,"href","/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrForObjectDetection"),c(He,"class","docstring"),c(ir,"class","docstring"),c(z2,"id","transformers.AutoModelForImageSegmentation"),c(z2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z2,"href","#transformers.AutoModelForImageSegmentation"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c($j,"href","/docs/transformers/pr_15678/en/model_doc/detr#transformers.DetrForSegmentation"),c(Ue,"class","docstring"),c(dr,"class","docstring"),c(Q2,"id","transformers.AutoModelForSemanticSegmentation"),c(Q2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q2,"href","#transformers.AutoModelForSemanticSegmentation"),c(Gd,"class","relative group"),c(rt,"class","docstring"),c(Ij,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(jj,"href","/docs/transformers/pr_15678/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Je,"class","docstring"),c(cr,"class","docstring"),c(Y2,"id","transformers.TFAutoModel"),c(Y2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y2,"href","#transformers.TFAutoModel"),c(zd,"class","relative group"),c(tt,"class","docstring"),c(Nj,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertModel"),c(Dj,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.TFBartModel"),c(qj,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertModel"),c(Gj,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(Oj,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(Xj,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertModel"),c(zj,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.TFCLIPModel"),c(Vj,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertModel"),c(Wj,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLModel"),c(Qj,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaModel"),c(Hj,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(Uj,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(Jj,"href","/docs/transformers/pr_15678/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(Yj,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraModel"),c(Kj,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(Zj,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelModel"),c(eN,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(oN,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2Model"),c(rN,"href","/docs/transformers/pr_15678/en/model_doc/hubert#transformers.TFHubertModel"),c(tN,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(aN,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.TFLEDModel"),c(nN,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerModel"),c(sN,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.TFLxmertModel"),c(lN,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.TFMarianModel"),c(iN,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.TFMBartModel"),c(dN,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(cN,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetModel"),c(fN,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.TFMT5Model"),c(mN,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(gN,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.TFPegasusModel"),c(hN,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertModel"),c(pN,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaModel"),c(_N,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerModel"),c(uN,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(bN,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.TFT5Model"),c(vN,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasModel"),c(TN,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(FN,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.TFViTModel"),c(CN,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(MN,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMModel"),c(EN,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(yN,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetModel"),c(fo,"class","docstring"),c(fr,"class","docstring"),c(Nv,"id","transformers.TFAutoModelForPreTraining"),c(Nv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nv,"href","#transformers.TFAutoModelForPreTraining"),c(Qd,"class","relative group"),c(at,"class","docstring"),c(wN,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(AN,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(LN,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForPreTraining"),c(BN,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(xN,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(kN,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(RN,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(SN,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(PN,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c($N,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(IN,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(jN,"href","/docs/transformers/pr_15678/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(NN,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(DN,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(qN,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(GN,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(ON,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(XN,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(zN,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(VN,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(WN,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(QN,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(mo,"class","docstring"),c(mr,"class","docstring"),c(l6,"id","transformers.TFAutoModelForCausalLM"),c(l6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(l6,"href","#transformers.TFAutoModelForCausalLM"),c(Jd,"class","relative group"),c(nt,"class","docstring"),c(HN,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(UN,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(JN,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(YN,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(KN,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(ZN,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(eD,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(oD,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(rD,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(tD,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(b6,"id","transformers.TFAutoModelForImageClassification"),c(b6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b6,"href","#transformers.TFAutoModelForImageClassification"),c(Zd,"class","relative group"),c(st,"class","docstring"),c(aD,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.TFViTForImageClassification"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(T6,"id","transformers.TFAutoModelForMaskedLM"),c(T6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T6,"href","#transformers.TFAutoModelForMaskedLM"),c(rc,"class","relative group"),c(lt,"class","docstring"),c(nD,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(sD,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(lD,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(iD,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(dD,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(cD,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(fD,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(mD,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(gD,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(hD,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(pD,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(_D,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(uD,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(bD,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(vD,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(TD,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(FD,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(CD,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(MD,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(ED,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(po,"class","docstring"),c(pr,"class","docstring"),c(G6,"id","transformers.TFAutoModelForSeq2SeqLM"),c(G6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G6,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(nc,"class","relative group"),c(it,"class","docstring"),c(yD,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(wD,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(AD,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(LD,"href","/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(BD,"href","/docs/transformers/pr_15678/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(xD,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.TFMarianMTModel"),c(kD,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(RD,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(SD,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(PD,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(K6,"id","transformers.TFAutoModelForSequenceClassification"),c(K6,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K6,"href","#transformers.TFAutoModelForSequenceClassification"),c(ic,"class","relative group"),c(dt,"class","docstring"),c($D,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(ID,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(jD,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(ND,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(DD,"href","/docs/transformers/pr_15678/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(qD,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(GD,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(OD,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(XD,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(zD,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(VD,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(WD,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(QD,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(HD,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(UD,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(JD,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(YD,"href","/docs/transformers/pr_15678/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(KD,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(ZD,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(eq,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(oq,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(rq,"href","/docs/transformers/pr_15678/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(tq,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(aq,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(nq,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(ET,"id","transformers.TFAutoModelForMultipleChoice"),c(ET,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ET,"href","#transformers.TFAutoModelForMultipleChoice"),c(fc,"class","relative group"),c(ct,"class","docstring"),c(sq,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(lq,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(iq,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(dq,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(cq,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(fq,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(mq,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(gq,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(hq,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(pq,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(_q,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(uq,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(bq,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(vq,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(Tq,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(Fq,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(Cq,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(bo,"class","docstring"),c(br,"class","docstring"),c(OT,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(OT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(OT,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(hc,"class","relative group"),c(ft,"class","docstring"),c(Mq,"href","/docs/transformers/pr_15678/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(zT,"id","transformers.TFAutoModelForTokenClassification"),c(zT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zT,"href","#transformers.TFAutoModelForTokenClassification"),c(uc,"class","relative group"),c(mt,"class","docstring"),c(Eq,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(yq,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(wq,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(Aq,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(Lq,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(Bq,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(xq,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(kq,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(Rq,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(Sq,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(Pq,"href","/docs/transformers/pr_15678/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c($q,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(Iq,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(jq,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(Nq,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(Dq,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(qq,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(Gq,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(Oq,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(Xq,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(f7,"id","transformers.TFAutoModelForQuestionAnswering"),c(f7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(f7,"href","#transformers.TFAutoModelForQuestionAnswering"),c(Tc,"class","relative group"),c(gt,"class","docstring"),c(zq,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(Vq,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(Wq,"href","/docs/transformers/pr_15678/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(Qq,"href","/docs/transformers/pr_15678/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(Hq,"href","/docs/transformers/pr_15678/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(Uq,"href","/docs/transformers/pr_15678/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(Jq,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(Yq,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(Kq,"href","/docs/transformers/pr_15678/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(Zq,"href","/docs/transformers/pr_15678/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(eG,"href","/docs/transformers/pr_15678/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(oG,"href","/docs/transformers/pr_15678/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(rG,"href","/docs/transformers/pr_15678/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(tG,"href","/docs/transformers/pr_15678/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(aG,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(nG,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(sG,"href","/docs/transformers/pr_15678/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(lG,"href","/docs/transformers/pr_15678/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(iG,"href","/docs/transformers/pr_15678/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(k7,"id","transformers.TFAutoModelForVision2Seq"),c(k7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k7,"href","#transformers.TFAutoModelForVision2Seq"),c(Mc,"class","relative group"),c(ht,"class","docstring"),c(dG,"href","/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(S7,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(S7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S7,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(wc,"class","relative group"),c(pt,"class","docstring"),c(cG,"href","/docs/transformers/pr_15678/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c($7,"id","transformers.FlaxAutoModel"),c($7,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($7,"href","#transformers.FlaxAutoModel"),c(Bc,"class","relative group"),c(_t,"class","docstring"),c(fG,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertModel"),c(mG,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartModel"),c(gG,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.FlaxBeitModel"),c(hG,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertModel"),c(pG,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(_G,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(uG,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(bG,"href","/docs/transformers/pr_15678/en/model_doc/clip#transformers.FlaxCLIPModel"),c(vG,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(TG,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraModel"),c(FG,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(CG,"href","/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(MG,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(EG,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.FlaxMarianModel"),c(yG,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartModel"),c(wG,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.FlaxMT5Model"),c(AG,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(LG,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(BG,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(xG,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.FlaxT5Model"),c(kG,"href","/docs/transformers/pr_15678/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(RG,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.FlaxViTModel"),c(SG,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(PG,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(s8,"id","transformers.FlaxAutoModelForCausalLM"),c(s8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s8,"href","#transformers.FlaxAutoModelForCausalLM"),c(Rc,"class","relative group"),c(ut,"class","docstring"),c($G,"href","/docs/transformers/pr_15678/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(IG,"href","/docs/transformers/pr_15678/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(jG,"href","/docs/transformers/pr_15678/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(NG,"href","/docs/transformers/pr_15678/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(f8,"id","transformers.FlaxAutoModelForPreTraining"),c(f8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(f8,"href","#transformers.FlaxAutoModelForPreTraining"),c($c,"class","relative group"),c(bt,"class","docstring"),c(DG,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(qG,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(GG,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(OG,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(XG,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(zG,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(VG,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(WG,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(QG,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(HG,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(UG,"href","/docs/transformers/pr_15678/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(M8,"id","transformers.FlaxAutoModelForMaskedLM"),c(M8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M8,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Nc,"class","relative group"),c(vt,"class","docstring"),c(JG,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(YG,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(KG,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(ZG,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(eO,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(oO,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(rO,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(tO,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(aO,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(S8,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(S8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S8,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Gc,"class","relative group"),c(Tt,"class","docstring"),c(nO,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(sO,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(lO,"href","/docs/transformers/pr_15678/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(iO,"href","/docs/transformers/pr_15678/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(dO,"href","/docs/transformers/pr_15678/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(cO,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(fO,"href","/docs/transformers/pr_15678/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(mO,"href","/docs/transformers/pr_15678/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(gO,"href","/docs/transformers/pr_15678/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(X8,"id","transformers.FlaxAutoModelForSequenceClassification"),c(X8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X8,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(zc,"class","relative group"),c(Ft,"class","docstring"),c(hO,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(pO,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(_O,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(uO,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(bO,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(vO,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(TO,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(FO,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(CO,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(Z8,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(Z8,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z8,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(Qc,"class","relative group"),c(Ct,"class","docstring"),c(MO,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(EO,"href","/docs/transformers/pr_15678/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(yO,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(wO,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(AO,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(LO,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(BO,"href","/docs/transformers/pr_15678/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(xO,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(kO,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(dF,"id","transformers.FlaxAutoModelForTokenClassification"),c(dF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dF,"href","#transformers.FlaxAutoModelForTokenClassification"),c(Jc,"class","relative group"),c(Mt,"class","docstring"),c(RO,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(SO,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(PO,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c($O,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(IO,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(jO,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(NO,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(uF,"id","transformers.FlaxAutoModelForMultipleChoice"),c(uF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uF,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(Zc,"class","relative group"),c(Et,"class","docstring"),c(DO,"href","/docs/transformers/pr_15678/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(qO,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(GO,"href","/docs/transformers/pr_15678/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(OO,"href","/docs/transformers/pr_15678/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(XO,"href","/docs/transformers/pr_15678/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(zO,"href","/docs/transformers/pr_15678/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(VO,"href","/docs/transformers/pr_15678/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c(yF,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(yF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yF,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(rf,"class","relative group"),c(yt,"class","docstring"),c(WO,"href","/docs/transformers/pr_15678/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(AF,"id","transformers.FlaxAutoModelForImageClassification"),c(AF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(AF,"href","#transformers.FlaxAutoModelForImageClassification"),c(nf,"class","relative group"),c(wt,"class","docstring"),c(QO,"href","/docs/transformers/pr_15678/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(HO,"href","/docs/transformers/pr_15678/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(xF,"id","transformers.FlaxAutoModelForVision2Seq"),c(xF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xF,"href","#transformers.FlaxAutoModelForVision2Seq"),c(df,"class","relative group"),c(At,"class","docstring"),c(UO,"href","/docs/transformers/pr_15678/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c($o,"class","docstring"),c($r,"class","docstring")},m(d,u){e(document.head,J),b(d,Ae,u),b(d,le,u),e(le,me),e(me,oo),g(ce,oo,null),e(le,ue),e(le,No),e(No,vi),b(d,gf,u),b(d,ra,u),e(ra,Ti),e(ra,Fi),e(Fi,LC),e(ra,hf),b(d,Ee,u),b(d,so,u),e(so,Ci),e(so,Bn),e(Bn,BC),e(so,xn),e(so,kn),e(kn,xC),e(so,Mi),e(so,Rn),e(Rn,kC),e(so,Ei),b(d,pf,u),g(ka,d,u),b(d,lo,u),b(d,ge,u),e(ge,n0),e(ge,yi),e(yi,s0),e(ge,l0),b(d,Do,u),b(d,Ra,u),e(Ra,i0),e(Ra,_f),e(_f,d0),e(Ra,MBe),b(d,bAe,u),b(d,wi,u),e(wi,uf),e(uf,Dz),g(RC,Dz,null),e(wi,EBe),e(wi,qz),e(qz,yBe),b(d,vAe,u),b(d,Sn,u),e(Sn,wBe),e(Sn,Gz),e(Gz,ABe),e(Sn,LBe),e(Sn,Oz),e(Oz,BBe),e(Sn,xBe),b(d,TAe,u),g(SC,d,u),b(d,FAe,u),b(d,c0,u),e(c0,kBe),b(d,CAe,u),g(bf,d,u),b(d,MAe,u),b(d,Ai,u),e(Ai,vf),e(vf,Xz),g(PC,Xz,null),e(Ai,RBe),e(Ai,zz),e(zz,SBe),b(d,EAe,u),b(d,qo,u),g($C,qo,null),e(qo,PBe),e(qo,IC),e(IC,$Be),e(IC,f0),e(f0,IBe),e(IC,jBe),e(qo,NBe),e(qo,jC),e(jC,DBe),e(jC,Vz),e(Vz,qBe),e(jC,GBe),e(qo,OBe),e(qo,io),g(NC,io,null),e(io,XBe),e(io,Wz),e(Wz,zBe),e(io,VBe),e(io,Li),e(Li,WBe),e(Li,Qz),e(Qz,QBe),e(Li,HBe),e(Li,Hz),e(Hz,UBe),e(Li,JBe),e(io,YBe),e(io,v),e(v,Tf),e(Tf,Uz),e(Uz,KBe),e(Tf,ZBe),e(Tf,m0),e(m0,exe),e(Tf,oxe),e(v,rxe),e(v,Ff),e(Ff,Jz),e(Jz,txe),e(Ff,axe),e(Ff,g0),e(g0,nxe),e(Ff,sxe),e(v,lxe),e(v,Cf),e(Cf,Yz),e(Yz,ixe),e(Cf,dxe),e(Cf,h0),e(h0,cxe),e(Cf,fxe),e(v,mxe),e(v,Mf),e(Mf,Kz),e(Kz,gxe),e(Mf,hxe),e(Mf,p0),e(p0,pxe),e(Mf,_xe),e(v,uxe),e(v,Ef),e(Ef,Zz),e(Zz,bxe),e(Ef,vxe),e(Ef,_0),e(_0,Txe),e(Ef,Fxe),e(v,Cxe),e(v,yf),e(yf,eV),e(eV,Mxe),e(yf,Exe),e(yf,u0),e(u0,yxe),e(yf,wxe),e(v,Axe),e(v,wf),e(wf,oV),e(oV,Lxe),e(wf,Bxe),e(wf,b0),e(b0,xxe),e(wf,kxe),e(v,Rxe),e(v,Af),e(Af,rV),e(rV,Sxe),e(Af,Pxe),e(Af,v0),e(v0,$xe),e(Af,Ixe),e(v,jxe),e(v,Lf),e(Lf,tV),e(tV,Nxe),e(Lf,Dxe),e(Lf,T0),e(T0,qxe),e(Lf,Gxe),e(v,Oxe),e(v,Bf),e(Bf,aV),e(aV,Xxe),e(Bf,zxe),e(Bf,F0),e(F0,Vxe),e(Bf,Wxe),e(v,Qxe),e(v,xf),e(xf,nV),e(nV,Hxe),e(xf,Uxe),e(xf,C0),e(C0,Jxe),e(xf,Yxe),e(v,Kxe),e(v,kf),e(kf,sV),e(sV,Zxe),e(kf,eke),e(kf,M0),e(M0,oke),e(kf,rke),e(v,tke),e(v,Rf),e(Rf,lV),e(lV,ake),e(Rf,nke),e(Rf,E0),e(E0,ske),e(Rf,lke),e(v,ike),e(v,Sf),e(Sf,iV),e(iV,dke),e(Sf,cke),e(Sf,y0),e(y0,fke),e(Sf,mke),e(v,gke),e(v,Pf),e(Pf,dV),e(dV,hke),e(Pf,pke),e(Pf,w0),e(w0,_ke),e(Pf,uke),e(v,bke),e(v,$f),e($f,cV),e(cV,vke),e($f,Tke),e($f,A0),e(A0,Fke),e($f,Cke),e(v,Mke),e(v,If),e(If,fV),e(fV,Eke),e(If,yke),e(If,L0),e(L0,wke),e(If,Ake),e(v,Lke),e(v,jf),e(jf,mV),e(mV,Bke),e(jf,xke),e(jf,B0),e(B0,kke),e(jf,Rke),e(v,Ske),e(v,Nf),e(Nf,gV),e(gV,Pke),e(Nf,$ke),e(Nf,x0),e(x0,Ike),e(Nf,jke),e(v,Nke),e(v,Df),e(Df,hV),e(hV,Dke),e(Df,qke),e(Df,k0),e(k0,Gke),e(Df,Oke),e(v,Xke),e(v,qf),e(qf,pV),e(pV,zke),e(qf,Vke),e(qf,R0),e(R0,Wke),e(qf,Qke),e(v,Hke),e(v,Gf),e(Gf,_V),e(_V,Uke),e(Gf,Jke),e(Gf,S0),e(S0,Yke),e(Gf,Kke),e(v,Zke),e(v,Of),e(Of,uV),e(uV,eRe),e(Of,oRe),e(Of,P0),e(P0,rRe),e(Of,tRe),e(v,aRe),e(v,Xf),e(Xf,bV),e(bV,nRe),e(Xf,sRe),e(Xf,$0),e($0,lRe),e(Xf,iRe),e(v,dRe),e(v,zf),e(zf,vV),e(vV,cRe),e(zf,fRe),e(zf,I0),e(I0,mRe),e(zf,gRe),e(v,hRe),e(v,Vf),e(Vf,TV),e(TV,pRe),e(Vf,_Re),e(Vf,j0),e(j0,uRe),e(Vf,bRe),e(v,vRe),e(v,Wf),e(Wf,FV),e(FV,TRe),e(Wf,FRe),e(Wf,N0),e(N0,CRe),e(Wf,MRe),e(v,ERe),e(v,Qf),e(Qf,CV),e(CV,yRe),e(Qf,wRe),e(Qf,D0),e(D0,ARe),e(Qf,LRe),e(v,BRe),e(v,Hf),e(Hf,MV),e(MV,xRe),e(Hf,kRe),e(Hf,q0),e(q0,RRe),e(Hf,SRe),e(v,PRe),e(v,Uf),e(Uf,EV),e(EV,$Re),e(Uf,IRe),e(Uf,G0),e(G0,jRe),e(Uf,NRe),e(v,DRe),e(v,Jf),e(Jf,yV),e(yV,qRe),e(Jf,GRe),e(Jf,O0),e(O0,ORe),e(Jf,XRe),e(v,zRe),e(v,Yf),e(Yf,wV),e(wV,VRe),e(Yf,WRe),e(Yf,X0),e(X0,QRe),e(Yf,HRe),e(v,URe),e(v,Kf),e(Kf,AV),e(AV,JRe),e(Kf,YRe),e(Kf,z0),e(z0,KRe),e(Kf,ZRe),e(v,eSe),e(v,Zf),e(Zf,LV),e(LV,oSe),e(Zf,rSe),e(Zf,V0),e(V0,tSe),e(Zf,aSe),e(v,nSe),e(v,em),e(em,BV),e(BV,sSe),e(em,lSe),e(em,W0),e(W0,iSe),e(em,dSe),e(v,cSe),e(v,om),e(om,xV),e(xV,fSe),e(om,mSe),e(om,Q0),e(Q0,gSe),e(om,hSe),e(v,pSe),e(v,rm),e(rm,kV),e(kV,_Se),e(rm,uSe),e(rm,H0),e(H0,bSe),e(rm,vSe),e(v,TSe),e(v,tm),e(tm,RV),e(RV,FSe),e(tm,CSe),e(tm,U0),e(U0,MSe),e(tm,ESe),e(v,ySe),e(v,am),e(am,SV),e(SV,wSe),e(am,ASe),e(am,J0),e(J0,LSe),e(am,BSe),e(v,xSe),e(v,nm),e(nm,PV),e(PV,kSe),e(nm,RSe),e(nm,Y0),e(Y0,SSe),e(nm,PSe),e(v,$Se),e(v,sm),e(sm,$V),e($V,ISe),e(sm,jSe),e(sm,K0),e(K0,NSe),e(sm,DSe),e(v,qSe),e(v,lm),e(lm,IV),e(IV,GSe),e(lm,OSe),e(lm,Z0),e(Z0,XSe),e(lm,zSe),e(v,VSe),e(v,im),e(im,jV),e(jV,WSe),e(im,QSe),e(im,eL),e(eL,HSe),e(im,USe),e(v,JSe),e(v,dm),e(dm,NV),e(NV,YSe),e(dm,KSe),e(dm,oL),e(oL,ZSe),e(dm,ePe),e(v,oPe),e(v,cm),e(cm,DV),e(DV,rPe),e(cm,tPe),e(cm,rL),e(rL,aPe),e(cm,nPe),e(v,sPe),e(v,fm),e(fm,qV),e(qV,lPe),e(fm,iPe),e(fm,tL),e(tL,dPe),e(fm,cPe),e(v,fPe),e(v,mm),e(mm,GV),e(GV,mPe),e(mm,gPe),e(mm,aL),e(aL,hPe),e(mm,pPe),e(v,_Pe),e(v,gm),e(gm,OV),e(OV,uPe),e(gm,bPe),e(gm,nL),e(nL,vPe),e(gm,TPe),e(v,FPe),e(v,hm),e(hm,XV),e(XV,CPe),e(hm,MPe),e(hm,sL),e(sL,EPe),e(hm,yPe),e(v,wPe),e(v,pm),e(pm,zV),e(zV,APe),e(pm,LPe),e(pm,lL),e(lL,BPe),e(pm,xPe),e(v,kPe),e(v,_m),e(_m,VV),e(VV,RPe),e(_m,SPe),e(_m,iL),e(iL,PPe),e(_m,$Pe),e(v,IPe),e(v,um),e(um,WV),e(WV,jPe),e(um,NPe),e(um,dL),e(dL,DPe),e(um,qPe),e(v,GPe),e(v,bm),e(bm,QV),e(QV,OPe),e(bm,XPe),e(bm,cL),e(cL,zPe),e(bm,VPe),e(v,WPe),e(v,vm),e(vm,HV),e(HV,QPe),e(vm,HPe),e(vm,fL),e(fL,UPe),e(vm,JPe),e(v,YPe),e(v,Tm),e(Tm,UV),e(UV,KPe),e(Tm,ZPe),e(Tm,mL),e(mL,e$e),e(Tm,o$e),e(v,r$e),e(v,Fm),e(Fm,JV),e(JV,t$e),e(Fm,a$e),e(Fm,gL),e(gL,n$e),e(Fm,s$e),e(v,l$e),e(v,Cm),e(Cm,YV),e(YV,i$e),e(Cm,d$e),e(Cm,hL),e(hL,c$e),e(Cm,f$e),e(v,m$e),e(v,Mm),e(Mm,KV),e(KV,g$e),e(Mm,h$e),e(Mm,pL),e(pL,p$e),e(Mm,_$e),e(v,u$e),e(v,Em),e(Em,ZV),e(ZV,b$e),e(Em,v$e),e(Em,_L),e(_L,T$e),e(Em,F$e),e(v,C$e),e(v,ym),e(ym,eW),e(eW,M$e),e(ym,E$e),e(ym,uL),e(uL,y$e),e(ym,w$e),e(v,A$e),e(v,wm),e(wm,oW),e(oW,L$e),e(wm,B$e),e(wm,bL),e(bL,x$e),e(wm,k$e),e(v,R$e),e(v,Am),e(Am,rW),e(rW,S$e),e(Am,P$e),e(Am,vL),e(vL,$$e),e(Am,I$e),e(v,j$e),e(v,Lm),e(Lm,tW),e(tW,N$e),e(Lm,D$e),e(Lm,TL),e(TL,q$e),e(Lm,G$e),e(v,O$e),e(v,Bm),e(Bm,aW),e(aW,X$e),e(Bm,z$e),e(Bm,FL),e(FL,V$e),e(Bm,W$e),e(v,Q$e),e(v,xm),e(xm,nW),e(nW,H$e),e(xm,U$e),e(xm,CL),e(CL,J$e),e(xm,Y$e),e(v,K$e),e(v,km),e(km,sW),e(sW,Z$e),e(km,eIe),e(km,ML),e(ML,oIe),e(km,rIe),e(v,tIe),e(v,Rm),e(Rm,lW),e(lW,aIe),e(Rm,nIe),e(Rm,EL),e(EL,sIe),e(Rm,lIe),e(v,iIe),e(v,Sm),e(Sm,iW),e(iW,dIe),e(Sm,cIe),e(Sm,yL),e(yL,fIe),e(Sm,mIe),e(v,gIe),e(v,Pm),e(Pm,dW),e(dW,hIe),e(Pm,pIe),e(Pm,wL),e(wL,_Ie),e(Pm,uIe),e(v,bIe),e(v,$m),e($m,cW),e(cW,vIe),e($m,TIe),e($m,AL),e(AL,FIe),e($m,CIe),e(v,MIe),e(v,Im),e(Im,fW),e(fW,EIe),e(Im,yIe),e(Im,LL),e(LL,wIe),e(Im,AIe),e(v,LIe),e(v,jm),e(jm,mW),e(mW,BIe),e(jm,xIe),e(jm,BL),e(BL,kIe),e(jm,RIe),e(v,SIe),e(v,Nm),e(Nm,gW),e(gW,PIe),e(Nm,$Ie),e(Nm,xL),e(xL,IIe),e(Nm,jIe),e(v,NIe),e(v,Dm),e(Dm,hW),e(hW,DIe),e(Dm,qIe),e(Dm,kL),e(kL,GIe),e(Dm,OIe),e(v,XIe),e(v,qm),e(qm,pW),e(pW,zIe),e(qm,VIe),e(qm,RL),e(RL,WIe),e(qm,QIe),e(v,HIe),e(v,Gm),e(Gm,_W),e(_W,UIe),e(Gm,JIe),e(Gm,SL),e(SL,YIe),e(Gm,KIe),e(v,ZIe),e(v,Om),e(Om,uW),e(uW,eje),e(Om,oje),e(Om,PL),e(PL,rje),e(Om,tje),e(v,aje),e(v,Xm),e(Xm,bW),e(bW,nje),e(Xm,sje),e(Xm,$L),e($L,lje),e(Xm,ije),e(v,dje),e(v,zm),e(zm,vW),e(vW,cje),e(zm,fje),e(zm,IL),e(IL,mje),e(zm,gje),e(v,hje),e(v,Vm),e(Vm,TW),e(TW,pje),e(Vm,_je),e(Vm,jL),e(jL,uje),e(Vm,bje),e(v,vje),e(v,Wm),e(Wm,FW),e(FW,Tje),e(Wm,Fje),e(Wm,NL),e(NL,Cje),e(Wm,Mje),e(v,Eje),e(v,Qm),e(Qm,CW),e(CW,yje),e(Qm,wje),e(Qm,DL),e(DL,Aje),e(Qm,Lje),e(v,Bje),e(v,Hm),e(Hm,MW),e(MW,xje),e(Hm,kje),e(Hm,qL),e(qL,Rje),e(Hm,Sje),e(v,Pje),e(v,Um),e(Um,EW),e(EW,$je),e(Um,Ije),e(Um,GL),e(GL,jje),e(Um,Nje),e(v,Dje),e(v,Jm),e(Jm,yW),e(yW,qje),e(Jm,Gje),e(Jm,OL),e(OL,Oje),e(Jm,Xje),e(v,zje),e(v,Ym),e(Ym,wW),e(wW,Vje),e(Ym,Wje),e(Ym,XL),e(XL,Qje),e(Ym,Hje),e(v,Uje),e(v,Km),e(Km,AW),e(AW,Jje),e(Km,Yje),e(Km,zL),e(zL,Kje),e(Km,Zje),e(v,eNe),e(v,Zm),e(Zm,LW),e(LW,oNe),e(Zm,rNe),e(Zm,VL),e(VL,tNe),e(Zm,aNe),e(v,nNe),e(v,eg),e(eg,BW),e(BW,sNe),e(eg,lNe),e(eg,WL),e(WL,iNe),e(eg,dNe),e(io,cNe),e(io,xW),e(xW,fNe),e(io,mNe),g(DC,io,null),e(qo,gNe),e(qo,og),g(qC,og,null),e(og,hNe),e(og,kW),e(kW,pNe),b(d,yAe,u),b(d,Bi,u),e(Bi,rg),e(rg,RW),g(GC,RW,null),e(Bi,_Ne),e(Bi,SW),e(SW,uNe),b(d,wAe,u),b(d,Go,u),g(OC,Go,null),e(Go,bNe),e(Go,XC),e(XC,vNe),e(XC,QL),e(QL,TNe),e(XC,FNe),e(Go,CNe),e(Go,zC),e(zC,MNe),e(zC,PW),e(PW,ENe),e(zC,yNe),e(Go,wNe),e(Go,co),g(VC,co,null),e(co,ANe),e(co,$W),e($W,LNe),e(co,BNe),e(co,Sa),e(Sa,xNe),e(Sa,IW),e(IW,kNe),e(Sa,RNe),e(Sa,jW),e(jW,SNe),e(Sa,PNe),e(Sa,NW),e(NW,$Ne),e(Sa,INe),e(co,jNe),e(co,M),e(M,Pn),e(Pn,DW),e(DW,NNe),e(Pn,DNe),e(Pn,HL),e(HL,qNe),e(Pn,GNe),e(Pn,UL),e(UL,ONe),e(Pn,XNe),e(M,zNe),e(M,$n),e($n,qW),e(qW,VNe),e($n,WNe),e($n,JL),e(JL,QNe),e($n,HNe),e($n,YL),e(YL,UNe),e($n,JNe),e(M,YNe),e(M,In),e(In,GW),e(GW,KNe),e(In,ZNe),e(In,KL),e(KL,eDe),e(In,oDe),e(In,ZL),e(ZL,rDe),e(In,tDe),e(M,aDe),e(M,tg),e(tg,OW),e(OW,nDe),e(tg,sDe),e(tg,e9),e(e9,lDe),e(tg,iDe),e(M,dDe),e(M,jn),e(jn,XW),e(XW,cDe),e(jn,fDe),e(jn,o9),e(o9,mDe),e(jn,gDe),e(jn,r9),e(r9,hDe),e(jn,pDe),e(M,_De),e(M,ag),e(ag,zW),e(zW,uDe),e(ag,bDe),e(ag,t9),e(t9,vDe),e(ag,TDe),e(M,FDe),e(M,ng),e(ng,VW),e(VW,CDe),e(ng,MDe),e(ng,a9),e(a9,EDe),e(ng,yDe),e(M,wDe),e(M,sg),e(sg,WW),e(WW,ADe),e(sg,LDe),e(sg,n9),e(n9,BDe),e(sg,xDe),e(M,kDe),e(M,Nn),e(Nn,QW),e(QW,RDe),e(Nn,SDe),e(Nn,s9),e(s9,PDe),e(Nn,$De),e(Nn,l9),e(l9,IDe),e(Nn,jDe),e(M,NDe),e(M,Dn),e(Dn,HW),e(HW,DDe),e(Dn,qDe),e(Dn,i9),e(i9,GDe),e(Dn,ODe),e(Dn,d9),e(d9,XDe),e(Dn,zDe),e(M,VDe),e(M,qn),e(qn,UW),e(UW,WDe),e(qn,QDe),e(qn,c9),e(c9,HDe),e(qn,UDe),e(qn,f9),e(f9,JDe),e(qn,YDe),e(M,KDe),e(M,lg),e(lg,JW),e(JW,ZDe),e(lg,eqe),e(lg,m9),e(m9,oqe),e(lg,rqe),e(M,tqe),e(M,ig),e(ig,YW),e(YW,aqe),e(ig,nqe),e(ig,g9),e(g9,sqe),e(ig,lqe),e(M,iqe),e(M,Gn),e(Gn,KW),e(KW,dqe),e(Gn,cqe),e(Gn,h9),e(h9,fqe),e(Gn,mqe),e(Gn,p9),e(p9,gqe),e(Gn,hqe),e(M,pqe),e(M,dg),e(dg,ZW),e(ZW,_qe),e(dg,uqe),e(dg,_9),e(_9,bqe),e(dg,vqe),e(M,Tqe),e(M,On),e(On,eQ),e(eQ,Fqe),e(On,Cqe),e(On,u9),e(u9,Mqe),e(On,Eqe),e(On,b9),e(b9,yqe),e(On,wqe),e(M,Aqe),e(M,Xn),e(Xn,oQ),e(oQ,Lqe),e(Xn,Bqe),e(Xn,v9),e(v9,xqe),e(Xn,kqe),e(Xn,T9),e(T9,Rqe),e(Xn,Sqe),e(M,Pqe),e(M,zn),e(zn,rQ),e(rQ,$qe),e(zn,Iqe),e(zn,F9),e(F9,jqe),e(zn,Nqe),e(zn,tQ),e(tQ,Dqe),e(zn,qqe),e(M,Gqe),e(M,cg),e(cg,aQ),e(aQ,Oqe),e(cg,Xqe),e(cg,C9),e(C9,zqe),e(cg,Vqe),e(M,Wqe),e(M,Vn),e(Vn,nQ),e(nQ,Qqe),e(Vn,Hqe),e(Vn,M9),e(M9,Uqe),e(Vn,Jqe),e(Vn,E9),e(E9,Yqe),e(Vn,Kqe),e(M,Zqe),e(M,fg),e(fg,sQ),e(sQ,eGe),e(fg,oGe),e(fg,y9),e(y9,rGe),e(fg,tGe),e(M,aGe),e(M,Wn),e(Wn,lQ),e(lQ,nGe),e(Wn,sGe),e(Wn,w9),e(w9,lGe),e(Wn,iGe),e(Wn,A9),e(A9,dGe),e(Wn,cGe),e(M,fGe),e(M,Qn),e(Qn,iQ),e(iQ,mGe),e(Qn,gGe),e(Qn,L9),e(L9,hGe),e(Qn,pGe),e(Qn,B9),e(B9,_Ge),e(Qn,uGe),e(M,bGe),e(M,Hn),e(Hn,dQ),e(dQ,vGe),e(Hn,TGe),e(Hn,x9),e(x9,FGe),e(Hn,CGe),e(Hn,k9),e(k9,MGe),e(Hn,EGe),e(M,yGe),e(M,mg),e(mg,cQ),e(cQ,wGe),e(mg,AGe),e(mg,R9),e(R9,LGe),e(mg,BGe),e(M,xGe),e(M,Un),e(Un,fQ),e(fQ,kGe),e(Un,RGe),e(Un,S9),e(S9,SGe),e(Un,PGe),e(Un,P9),e(P9,$Ge),e(Un,IGe),e(M,jGe),e(M,gg),e(gg,mQ),e(mQ,NGe),e(gg,DGe),e(gg,$9),e($9,qGe),e(gg,GGe),e(M,OGe),e(M,Jn),e(Jn,gQ),e(gQ,XGe),e(Jn,zGe),e(Jn,I9),e(I9,VGe),e(Jn,WGe),e(Jn,j9),e(j9,QGe),e(Jn,HGe),e(M,UGe),e(M,Yn),e(Yn,hQ),e(hQ,JGe),e(Yn,YGe),e(Yn,N9),e(N9,KGe),e(Yn,ZGe),e(Yn,D9),e(D9,eOe),e(Yn,oOe),e(M,rOe),e(M,Kn),e(Kn,pQ),e(pQ,tOe),e(Kn,aOe),e(Kn,q9),e(q9,nOe),e(Kn,sOe),e(Kn,G9),e(G9,lOe),e(Kn,iOe),e(M,dOe),e(M,Zn),e(Zn,_Q),e(_Q,cOe),e(Zn,fOe),e(Zn,O9),e(O9,mOe),e(Zn,gOe),e(Zn,X9),e(X9,hOe),e(Zn,pOe),e(M,_Oe),e(M,hg),e(hg,uQ),e(uQ,uOe),e(hg,bOe),e(hg,z9),e(z9,vOe),e(hg,TOe),e(M,FOe),e(M,es),e(es,bQ),e(bQ,COe),e(es,MOe),e(es,V9),e(V9,EOe),e(es,yOe),e(es,W9),e(W9,wOe),e(es,AOe),e(M,LOe),e(M,os),e(os,vQ),e(vQ,BOe),e(os,xOe),e(os,Q9),e(Q9,kOe),e(os,ROe),e(os,H9),e(H9,SOe),e(os,POe),e(M,$Oe),e(M,rs),e(rs,TQ),e(TQ,IOe),e(rs,jOe),e(rs,U9),e(U9,NOe),e(rs,DOe),e(rs,J9),e(J9,qOe),e(rs,GOe),e(M,OOe),e(M,ts),e(ts,FQ),e(FQ,XOe),e(ts,zOe),e(ts,Y9),e(Y9,VOe),e(ts,WOe),e(ts,K9),e(K9,QOe),e(ts,HOe),e(M,UOe),e(M,as),e(as,CQ),e(CQ,JOe),e(as,YOe),e(as,Z9),e(Z9,KOe),e(as,ZOe),e(as,eB),e(eB,eXe),e(as,oXe),e(M,rXe),e(M,ns),e(ns,MQ),e(MQ,tXe),e(ns,aXe),e(ns,oB),e(oB,nXe),e(ns,sXe),e(ns,rB),e(rB,lXe),e(ns,iXe),e(M,dXe),e(M,pg),e(pg,EQ),e(EQ,cXe),e(pg,fXe),e(pg,tB),e(tB,mXe),e(pg,gXe),e(M,hXe),e(M,ss),e(ss,yQ),e(yQ,pXe),e(ss,_Xe),e(ss,aB),e(aB,uXe),e(ss,bXe),e(ss,nB),e(nB,vXe),e(ss,TXe),e(M,FXe),e(M,_g),e(_g,wQ),e(wQ,CXe),e(_g,MXe),e(_g,sB),e(sB,EXe),e(_g,yXe),e(M,wXe),e(M,ug),e(ug,AQ),e(AQ,AXe),e(ug,LXe),e(ug,lB),e(lB,BXe),e(ug,xXe),e(M,kXe),e(M,ls),e(ls,LQ),e(LQ,RXe),e(ls,SXe),e(ls,iB),e(iB,PXe),e(ls,$Xe),e(ls,dB),e(dB,IXe),e(ls,jXe),e(M,NXe),e(M,is),e(is,BQ),e(BQ,DXe),e(is,qXe),e(is,cB),e(cB,GXe),e(is,OXe),e(is,fB),e(fB,XXe),e(is,zXe),e(M,VXe),e(M,bg),e(bg,xQ),e(xQ,WXe),e(bg,QXe),e(bg,mB),e(mB,HXe),e(bg,UXe),e(M,JXe),e(M,ds),e(ds,kQ),e(kQ,YXe),e(ds,KXe),e(ds,gB),e(gB,ZXe),e(ds,eze),e(ds,hB),e(hB,oze),e(ds,rze),e(M,tze),e(M,cs),e(cs,RQ),e(RQ,aze),e(cs,nze),e(cs,pB),e(pB,sze),e(cs,lze),e(cs,_B),e(_B,ize),e(cs,dze),e(M,cze),e(M,fs),e(fs,SQ),e(SQ,fze),e(fs,mze),e(fs,uB),e(uB,gze),e(fs,hze),e(fs,bB),e(bB,pze),e(fs,_ze),e(M,uze),e(M,ms),e(ms,PQ),e(PQ,bze),e(ms,vze),e(ms,vB),e(vB,Tze),e(ms,Fze),e(ms,TB),e(TB,Cze),e(ms,Mze),e(M,Eze),e(M,gs),e(gs,$Q),e($Q,yze),e(gs,wze),e(gs,FB),e(FB,Aze),e(gs,Lze),e(gs,CB),e(CB,Bze),e(gs,xze),e(M,kze),e(M,vg),e(vg,IQ),e(IQ,Rze),e(vg,Sze),e(vg,MB),e(MB,Pze),e(vg,$ze),e(M,Ize),e(M,Tg),e(Tg,jQ),e(jQ,jze),e(Tg,Nze),e(Tg,EB),e(EB,Dze),e(Tg,qze),e(M,Gze),e(M,Fg),e(Fg,NQ),e(NQ,Oze),e(Fg,Xze),e(Fg,yB),e(yB,zze),e(Fg,Vze),e(M,Wze),e(M,hs),e(hs,DQ),e(DQ,Qze),e(hs,Hze),e(hs,wB),e(wB,Uze),e(hs,Jze),e(hs,AB),e(AB,Yze),e(hs,Kze),e(M,Zze),e(M,Cg),e(Cg,qQ),e(qQ,eVe),e(Cg,oVe),e(Cg,LB),e(LB,rVe),e(Cg,tVe),e(M,aVe),e(M,ps),e(ps,GQ),e(GQ,nVe),e(ps,sVe),e(ps,BB),e(BB,lVe),e(ps,iVe),e(ps,xB),e(xB,dVe),e(ps,cVe),e(M,fVe),e(M,_s),e(_s,OQ),e(OQ,mVe),e(_s,gVe),e(_s,kB),e(kB,hVe),e(_s,pVe),e(_s,RB),e(RB,_Ve),e(_s,uVe),e(M,bVe),e(M,us),e(us,XQ),e(XQ,vVe),e(us,TVe),e(us,SB),e(SB,FVe),e(us,CVe),e(us,PB),e(PB,MVe),e(us,EVe),e(M,yVe),e(M,bs),e(bs,zQ),e(zQ,wVe),e(bs,AVe),e(bs,$B),e($B,LVe),e(bs,BVe),e(bs,IB),e(IB,xVe),e(bs,kVe),e(M,RVe),e(M,vs),e(vs,VQ),e(VQ,SVe),e(vs,PVe),e(vs,jB),e(jB,$Ve),e(vs,IVe),e(vs,NB),e(NB,jVe),e(vs,NVe),e(M,DVe),e(M,Mg),e(Mg,WQ),e(WQ,qVe),e(Mg,GVe),e(Mg,DB),e(DB,OVe),e(Mg,XVe),e(M,zVe),e(M,Eg),e(Eg,QQ),e(QQ,VVe),e(Eg,WVe),e(Eg,qB),e(qB,QVe),e(Eg,HVe),e(M,UVe),e(M,Ts),e(Ts,HQ),e(HQ,JVe),e(Ts,YVe),e(Ts,GB),e(GB,KVe),e(Ts,ZVe),e(Ts,OB),e(OB,eWe),e(Ts,oWe),e(M,rWe),e(M,Fs),e(Fs,UQ),e(UQ,tWe),e(Fs,aWe),e(Fs,XB),e(XB,nWe),e(Fs,sWe),e(Fs,zB),e(zB,lWe),e(Fs,iWe),e(M,dWe),e(M,Cs),e(Cs,JQ),e(JQ,cWe),e(Cs,fWe),e(Cs,VB),e(VB,mWe),e(Cs,gWe),e(Cs,WB),e(WB,hWe),e(Cs,pWe),e(M,_We),e(M,yg),e(yg,YQ),e(YQ,uWe),e(yg,bWe),e(yg,QB),e(QB,vWe),e(yg,TWe),e(M,FWe),e(M,wg),e(wg,KQ),e(KQ,CWe),e(wg,MWe),e(wg,HB),e(HB,EWe),e(wg,yWe),e(M,wWe),e(M,Ag),e(Ag,ZQ),e(ZQ,AWe),e(Ag,LWe),e(Ag,UB),e(UB,BWe),e(Ag,xWe),e(M,kWe),e(M,Lg),e(Lg,eH),e(eH,RWe),e(Lg,SWe),e(Lg,JB),e(JB,PWe),e(Lg,$We),e(M,IWe),e(M,Ms),e(Ms,oH),e(oH,jWe),e(Ms,NWe),e(Ms,YB),e(YB,DWe),e(Ms,qWe),e(Ms,KB),e(KB,GWe),e(Ms,OWe),e(M,XWe),e(M,Bg),e(Bg,rH),e(rH,zWe),e(Bg,VWe),e(Bg,ZB),e(ZB,WWe),e(Bg,QWe),e(M,HWe),e(M,xg),e(xg,tH),e(tH,UWe),e(xg,JWe),e(xg,ex),e(ex,YWe),e(xg,KWe),e(M,ZWe),e(M,Es),e(Es,aH),e(aH,eQe),e(Es,oQe),e(Es,ox),e(ox,rQe),e(Es,tQe),e(Es,rx),e(rx,aQe),e(Es,nQe),e(M,sQe),e(M,ys),e(ys,nH),e(nH,lQe),e(ys,iQe),e(ys,tx),e(tx,dQe),e(ys,cQe),e(ys,ax),e(ax,fQe),e(ys,mQe),e(co,gQe),e(co,sH),e(sH,hQe),e(co,pQe),g(WC,co,null),e(Go,_Qe),e(Go,kg),g(QC,kg,null),e(kg,uQe),e(kg,lH),e(lH,bQe),b(d,AAe,u),b(d,xi,u),e(xi,Rg),e(Rg,iH),g(HC,iH,null),e(xi,vQe),e(xi,dH),e(dH,TQe),b(d,LAe,u),b(d,Oo,u),g(UC,Oo,null),e(Oo,FQe),e(Oo,JC),e(JC,CQe),e(JC,nx),e(nx,MQe),e(JC,EQe),e(Oo,yQe),e(Oo,YC),e(YC,wQe),e(YC,cH),e(cH,AQe),e(YC,LQe),e(Oo,BQe),e(Oo,Le),g(KC,Le,null),e(Le,xQe),e(Le,fH),e(fH,kQe),e(Le,RQe),e(Le,Pa),e(Pa,SQe),e(Pa,mH),e(mH,PQe),e(Pa,$Qe),e(Pa,gH),e(gH,IQe),e(Pa,jQe),e(Pa,hH),e(hH,NQe),e(Pa,DQe),e(Le,qQe),e(Le,se),e(se,Sg),e(Sg,pH),e(pH,GQe),e(Sg,OQe),e(Sg,sx),e(sx,XQe),e(Sg,zQe),e(se,VQe),e(se,Pg),e(Pg,_H),e(_H,WQe),e(Pg,QQe),e(Pg,lx),e(lx,HQe),e(Pg,UQe),e(se,JQe),e(se,$g),e($g,uH),e(uH,YQe),e($g,KQe),e($g,ix),e(ix,ZQe),e($g,eHe),e(se,oHe),e(se,Ig),e(Ig,bH),e(bH,rHe),e(Ig,tHe),e(Ig,dx),e(dx,aHe),e(Ig,nHe),e(se,sHe),e(se,jg),e(jg,vH),e(vH,lHe),e(jg,iHe),e(jg,cx),e(cx,dHe),e(jg,cHe),e(se,fHe),e(se,Ng),e(Ng,TH),e(TH,mHe),e(Ng,gHe),e(Ng,fx),e(fx,hHe),e(Ng,pHe),e(se,_He),e(se,Dg),e(Dg,FH),e(FH,uHe),e(Dg,bHe),e(Dg,mx),e(mx,vHe),e(Dg,THe),e(se,FHe),e(se,qg),e(qg,CH),e(CH,CHe),e(qg,MHe),e(qg,gx),e(gx,EHe),e(qg,yHe),e(se,wHe),e(se,Gg),e(Gg,MH),e(MH,AHe),e(Gg,LHe),e(Gg,hx),e(hx,BHe),e(Gg,xHe),e(se,kHe),e(se,Og),e(Og,EH),e(EH,RHe),e(Og,SHe),e(Og,px),e(px,PHe),e(Og,$He),e(se,IHe),e(se,Xg),e(Xg,yH),e(yH,jHe),e(Xg,NHe),e(Xg,_x),e(_x,DHe),e(Xg,qHe),e(se,GHe),e(se,zg),e(zg,wH),e(wH,OHe),e(zg,XHe),e(zg,ux),e(ux,zHe),e(zg,VHe),e(se,WHe),e(se,Vg),e(Vg,AH),e(AH,QHe),e(Vg,HHe),e(Vg,bx),e(bx,UHe),e(Vg,JHe),e(se,YHe),e(se,Wg),e(Wg,LH),e(LH,KHe),e(Wg,ZHe),e(Wg,vx),e(vx,eUe),e(Wg,oUe),e(Le,rUe),g(Qg,Le,null),e(Le,tUe),e(Le,BH),e(BH,aUe),e(Le,nUe),g(ZC,Le,null),e(Oo,sUe),e(Oo,Hg),g(e4,Hg,null),e(Hg,lUe),e(Hg,xH),e(xH,iUe),b(d,BAe,u),b(d,ki,u),e(ki,Ug),e(Ug,kH),g(o4,kH,null),e(ki,dUe),e(ki,RH),e(RH,cUe),b(d,xAe,u),b(d,Xo,u),g(r4,Xo,null),e(Xo,fUe),e(Xo,t4),e(t4,mUe),e(t4,Tx),e(Tx,gUe),e(t4,hUe),e(Xo,pUe),e(Xo,a4),e(a4,_Ue),e(a4,SH),e(SH,uUe),e(a4,bUe),e(Xo,vUe),e(Xo,Be),g(n4,Be,null),e(Be,TUe),e(Be,PH),e(PH,FUe),e(Be,CUe),e(Be,Ri),e(Ri,MUe),e(Ri,$H),e($H,EUe),e(Ri,yUe),e(Ri,IH),e(IH,wUe),e(Ri,AUe),e(Be,LUe),e(Be,ye),e(ye,Jg),e(Jg,jH),e(jH,BUe),e(Jg,xUe),e(Jg,Fx),e(Fx,kUe),e(Jg,RUe),e(ye,SUe),e(ye,Yg),e(Yg,NH),e(NH,PUe),e(Yg,$Ue),e(Yg,Cx),e(Cx,IUe),e(Yg,jUe),e(ye,NUe),e(ye,Kg),e(Kg,DH),e(DH,DUe),e(Kg,qUe),e(Kg,Mx),e(Mx,GUe),e(Kg,OUe),e(ye,XUe),e(ye,Zg),e(Zg,qH),e(qH,zUe),e(Zg,VUe),e(Zg,Ex),e(Ex,WUe),e(Zg,QUe),e(ye,HUe),e(ye,eh),e(eh,GH),e(GH,UUe),e(eh,JUe),e(eh,yx),e(yx,YUe),e(eh,KUe),e(ye,ZUe),e(ye,oh),e(oh,OH),e(OH,eJe),e(oh,oJe),e(oh,wx),e(wx,rJe),e(oh,tJe),e(ye,aJe),e(ye,rh),e(rh,XH),e(XH,nJe),e(rh,sJe),e(rh,Ax),e(Ax,lJe),e(rh,iJe),e(ye,dJe),e(ye,th),e(th,zH),e(zH,cJe),e(th,fJe),e(th,Lx),e(Lx,mJe),e(th,gJe),e(Be,hJe),g(ah,Be,null),e(Be,pJe),e(Be,VH),e(VH,_Je),e(Be,uJe),g(s4,Be,null),e(Xo,bJe),e(Xo,nh),g(l4,nh,null),e(nh,vJe),e(nh,WH),e(WH,TJe),b(d,kAe,u),b(d,Si,u),e(Si,sh),e(sh,QH),g(i4,QH,null),e(Si,FJe),e(Si,HH),e(HH,CJe),b(d,RAe,u),b(d,zo,u),g(d4,zo,null),e(zo,MJe),e(zo,Pi),e(Pi,EJe),e(Pi,UH),e(UH,yJe),e(Pi,wJe),e(Pi,JH),e(JH,AJe),e(Pi,LJe),e(zo,BJe),e(zo,c4),e(c4,xJe),e(c4,YH),e(YH,kJe),e(c4,RJe),e(zo,SJe),e(zo,Ir),g(f4,Ir,null),e(Ir,PJe),e(Ir,KH),e(KH,$Je),e(Ir,IJe),e(Ir,$i),e($i,jJe),e($i,ZH),e(ZH,NJe),e($i,DJe),e($i,eU),e(eU,qJe),e($i,GJe),e(Ir,OJe),e(Ir,oU),e(oU,XJe),e(Ir,zJe),g(m4,Ir,null),e(zo,VJe),e(zo,xe),g(g4,xe,null),e(xe,WJe),e(xe,rU),e(rU,QJe),e(xe,HJe),e(xe,$a),e($a,UJe),e($a,tU),e(tU,JJe),e($a,YJe),e($a,aU),e(aU,KJe),e($a,ZJe),e($a,nU),e(nU,eYe),e($a,oYe),e(xe,rYe),e(xe,F),e(F,lh),e(lh,sU),e(sU,tYe),e(lh,aYe),e(lh,Bx),e(Bx,nYe),e(lh,sYe),e(F,lYe),e(F,ih),e(ih,lU),e(lU,iYe),e(ih,dYe),e(ih,xx),e(xx,cYe),e(ih,fYe),e(F,mYe),e(F,dh),e(dh,iU),e(iU,gYe),e(dh,hYe),e(dh,kx),e(kx,pYe),e(dh,_Ye),e(F,uYe),e(F,ch),e(ch,dU),e(dU,bYe),e(ch,vYe),e(ch,Rx),e(Rx,TYe),e(ch,FYe),e(F,CYe),e(F,fh),e(fh,cU),e(cU,MYe),e(fh,EYe),e(fh,Sx),e(Sx,yYe),e(fh,wYe),e(F,AYe),e(F,mh),e(mh,fU),e(fU,LYe),e(mh,BYe),e(mh,Px),e(Px,xYe),e(mh,kYe),e(F,RYe),e(F,gh),e(gh,mU),e(mU,SYe),e(gh,PYe),e(gh,$x),e($x,$Ye),e(gh,IYe),e(F,jYe),e(F,hh),e(hh,gU),e(gU,NYe),e(hh,DYe),e(hh,Ix),e(Ix,qYe),e(hh,GYe),e(F,OYe),e(F,ph),e(ph,hU),e(hU,XYe),e(ph,zYe),e(ph,jx),e(jx,VYe),e(ph,WYe),e(F,QYe),e(F,_h),e(_h,pU),e(pU,HYe),e(_h,UYe),e(_h,Nx),e(Nx,JYe),e(_h,YYe),e(F,KYe),e(F,uh),e(uh,_U),e(_U,ZYe),e(uh,eKe),e(uh,Dx),e(Dx,oKe),e(uh,rKe),e(F,tKe),e(F,bh),e(bh,uU),e(uU,aKe),e(bh,nKe),e(bh,qx),e(qx,sKe),e(bh,lKe),e(F,iKe),e(F,vh),e(vh,bU),e(bU,dKe),e(vh,cKe),e(vh,Gx),e(Gx,fKe),e(vh,mKe),e(F,gKe),e(F,Th),e(Th,vU),e(vU,hKe),e(Th,pKe),e(Th,Ox),e(Ox,_Ke),e(Th,uKe),e(F,bKe),e(F,Fh),e(Fh,TU),e(TU,vKe),e(Fh,TKe),e(Fh,Xx),e(Xx,FKe),e(Fh,CKe),e(F,MKe),e(F,Ch),e(Ch,FU),e(FU,EKe),e(Ch,yKe),e(Ch,zx),e(zx,wKe),e(Ch,AKe),e(F,LKe),e(F,Mh),e(Mh,CU),e(CU,BKe),e(Mh,xKe),e(Mh,Vx),e(Vx,kKe),e(Mh,RKe),e(F,SKe),e(F,Eh),e(Eh,MU),e(MU,PKe),e(Eh,$Ke),e(Eh,Wx),e(Wx,IKe),e(Eh,jKe),e(F,NKe),e(F,yh),e(yh,EU),e(EU,DKe),e(yh,qKe),e(yh,Qx),e(Qx,GKe),e(yh,OKe),e(F,XKe),e(F,wh),e(wh,yU),e(yU,zKe),e(wh,VKe),e(wh,Hx),e(Hx,WKe),e(wh,QKe),e(F,HKe),e(F,Ah),e(Ah,wU),e(wU,UKe),e(Ah,JKe),e(Ah,Ux),e(Ux,YKe),e(Ah,KKe),e(F,ZKe),e(F,Lh),e(Lh,AU),e(AU,eZe),e(Lh,oZe),e(Lh,Jx),e(Jx,rZe),e(Lh,tZe),e(F,aZe),e(F,Bh),e(Bh,LU),e(LU,nZe),e(Bh,sZe),e(Bh,Yx),e(Yx,lZe),e(Bh,iZe),e(F,dZe),e(F,xh),e(xh,BU),e(BU,cZe),e(xh,fZe),e(xh,Kx),e(Kx,mZe),e(xh,gZe),e(F,hZe),e(F,kh),e(kh,xU),e(xU,pZe),e(kh,_Ze),e(kh,Zx),e(Zx,uZe),e(kh,bZe),e(F,vZe),e(F,ws),e(ws,kU),e(kU,TZe),e(ws,FZe),e(ws,ek),e(ek,CZe),e(ws,MZe),e(ws,ok),e(ok,EZe),e(ws,yZe),e(F,wZe),e(F,Rh),e(Rh,RU),e(RU,AZe),e(Rh,LZe),e(Rh,rk),e(rk,BZe),e(Rh,xZe),e(F,kZe),e(F,Sh),e(Sh,SU),e(SU,RZe),e(Sh,SZe),e(Sh,tk),e(tk,PZe),e(Sh,$Ze),e(F,IZe),e(F,Ph),e(Ph,PU),e(PU,jZe),e(Ph,NZe),e(Ph,ak),e(ak,DZe),e(Ph,qZe),e(F,GZe),e(F,$h),e($h,$U),e($U,OZe),e($h,XZe),e($h,nk),e(nk,zZe),e($h,VZe),e(F,WZe),e(F,Ih),e(Ih,IU),e(IU,QZe),e(Ih,HZe),e(Ih,sk),e(sk,UZe),e(Ih,JZe),e(F,YZe),e(F,jh),e(jh,jU),e(jU,KZe),e(jh,ZZe),e(jh,lk),e(lk,eeo),e(jh,oeo),e(F,reo),e(F,Nh),e(Nh,NU),e(NU,teo),e(Nh,aeo),e(Nh,ik),e(ik,neo),e(Nh,seo),e(F,leo),e(F,Dh),e(Dh,DU),e(DU,ieo),e(Dh,deo),e(Dh,dk),e(dk,ceo),e(Dh,feo),e(F,meo),e(F,qh),e(qh,qU),e(qU,geo),e(qh,heo),e(qh,ck),e(ck,peo),e(qh,_eo),e(F,ueo),e(F,Gh),e(Gh,GU),e(GU,beo),e(Gh,veo),e(Gh,fk),e(fk,Teo),e(Gh,Feo),e(F,Ceo),e(F,Oh),e(Oh,OU),e(OU,Meo),e(Oh,Eeo),e(Oh,mk),e(mk,yeo),e(Oh,weo),e(F,Aeo),e(F,Xh),e(Xh,XU),e(XU,Leo),e(Xh,Beo),e(Xh,gk),e(gk,xeo),e(Xh,keo),e(F,Reo),e(F,zh),e(zh,zU),e(zU,Seo),e(zh,Peo),e(zh,hk),e(hk,$eo),e(zh,Ieo),e(F,jeo),e(F,Vh),e(Vh,VU),e(VU,Neo),e(Vh,Deo),e(Vh,pk),e(pk,qeo),e(Vh,Geo),e(F,Oeo),e(F,Wh),e(Wh,WU),e(WU,Xeo),e(Wh,zeo),e(Wh,_k),e(_k,Veo),e(Wh,Weo),e(F,Qeo),e(F,Qh),e(Qh,QU),e(QU,Heo),e(Qh,Ueo),e(Qh,uk),e(uk,Jeo),e(Qh,Yeo),e(F,Keo),e(F,Hh),e(Hh,HU),e(HU,Zeo),e(Hh,eoo),e(Hh,bk),e(bk,ooo),e(Hh,roo),e(F,too),e(F,Uh),e(Uh,UU),e(UU,aoo),e(Uh,noo),e(Uh,vk),e(vk,soo),e(Uh,loo),e(F,ioo),e(F,Jh),e(Jh,JU),e(JU,doo),e(Jh,coo),e(Jh,Tk),e(Tk,foo),e(Jh,moo),e(F,goo),e(F,Yh),e(Yh,YU),e(YU,hoo),e(Yh,poo),e(Yh,Fk),e(Fk,_oo),e(Yh,uoo),e(F,boo),e(F,Kh),e(Kh,KU),e(KU,voo),e(Kh,Too),e(Kh,Ck),e(Ck,Foo),e(Kh,Coo),e(F,Moo),e(F,Zh),e(Zh,ZU),e(ZU,Eoo),e(Zh,yoo),e(Zh,Mk),e(Mk,woo),e(Zh,Aoo),e(F,Loo),e(F,ep),e(ep,eJ),e(eJ,Boo),e(ep,xoo),e(ep,Ek),e(Ek,koo),e(ep,Roo),e(F,Soo),e(F,op),e(op,oJ),e(oJ,Poo),e(op,$oo),e(op,yk),e(yk,Ioo),e(op,joo),e(F,Noo),e(F,rp),e(rp,rJ),e(rJ,Doo),e(rp,qoo),e(rp,wk),e(wk,Goo),e(rp,Ooo),e(F,Xoo),e(F,tp),e(tp,tJ),e(tJ,zoo),e(tp,Voo),e(tp,Ak),e(Ak,Woo),e(tp,Qoo),e(F,Hoo),e(F,ap),e(ap,aJ),e(aJ,Uoo),e(ap,Joo),e(ap,Lk),e(Lk,Yoo),e(ap,Koo),e(F,Zoo),e(F,np),e(np,nJ),e(nJ,ero),e(np,oro),e(np,Bk),e(Bk,rro),e(np,tro),e(F,aro),e(F,sp),e(sp,sJ),e(sJ,nro),e(sp,sro),e(sp,xk),e(xk,lro),e(sp,iro),e(F,dro),e(F,lp),e(lp,lJ),e(lJ,cro),e(lp,fro),e(lp,kk),e(kk,mro),e(lp,gro),e(F,hro),e(F,ip),e(ip,iJ),e(iJ,pro),e(ip,_ro),e(ip,Rk),e(Rk,uro),e(ip,bro),e(F,vro),e(F,dp),e(dp,dJ),e(dJ,Tro),e(dp,Fro),e(dp,Sk),e(Sk,Cro),e(dp,Mro),e(F,Ero),e(F,cp),e(cp,cJ),e(cJ,yro),e(cp,wro),e(cp,Pk),e(Pk,Aro),e(cp,Lro),e(F,Bro),e(F,fp),e(fp,fJ),e(fJ,xro),e(fp,kro),e(fp,$k),e($k,Rro),e(fp,Sro),e(F,Pro),e(F,mp),e(mp,mJ),e(mJ,$ro),e(mp,Iro),e(mp,Ik),e(Ik,jro),e(mp,Nro),e(F,Dro),e(F,gp),e(gp,gJ),e(gJ,qro),e(gp,Gro),e(gp,jk),e(jk,Oro),e(gp,Xro),e(F,zro),e(F,hp),e(hp,hJ),e(hJ,Vro),e(hp,Wro),e(hp,Nk),e(Nk,Qro),e(hp,Hro),e(F,Uro),e(F,pp),e(pp,pJ),e(pJ,Jro),e(pp,Yro),e(pp,Dk),e(Dk,Kro),e(pp,Zro),e(F,eto),e(F,_p),e(_p,_J),e(_J,oto),e(_p,rto),e(_p,qk),e(qk,tto),e(_p,ato),e(F,nto),e(F,up),e(up,uJ),e(uJ,sto),e(up,lto),e(up,Gk),e(Gk,ito),e(up,dto),e(F,cto),e(F,bp),e(bp,bJ),e(bJ,fto),e(bp,mto),e(bp,Ok),e(Ok,gto),e(bp,hto),e(F,pto),e(F,vp),e(vp,vJ),e(vJ,_to),e(vp,uto),e(vp,Xk),e(Xk,bto),e(vp,vto),e(F,Tto),e(F,Tp),e(Tp,TJ),e(TJ,Fto),e(Tp,Cto),e(Tp,zk),e(zk,Mto),e(Tp,Eto),e(F,yto),e(F,Fp),e(Fp,FJ),e(FJ,wto),e(Fp,Ato),e(Fp,Vk),e(Vk,Lto),e(Fp,Bto),e(F,xto),e(F,Cp),e(Cp,CJ),e(CJ,kto),e(Cp,Rto),e(Cp,Wk),e(Wk,Sto),e(Cp,Pto),e(F,$to),e(F,Mp),e(Mp,MJ),e(MJ,Ito),e(Mp,jto),e(Mp,Qk),e(Qk,Nto),e(Mp,Dto),e(F,qto),e(F,Ep),e(Ep,EJ),e(EJ,Gto),e(Ep,Oto),e(Ep,Hk),e(Hk,Xto),e(Ep,zto),e(F,Vto),e(F,yp),e(yp,yJ),e(yJ,Wto),e(yp,Qto),e(yp,Uk),e(Uk,Hto),e(yp,Uto),e(F,Jto),e(F,wp),e(wp,wJ),e(wJ,Yto),e(wp,Kto),e(wp,Jk),e(Jk,Zto),e(wp,eao),e(F,oao),e(F,Ap),e(Ap,AJ),e(AJ,rao),e(Ap,tao),e(Ap,Yk),e(Yk,aao),e(Ap,nao),e(F,sao),e(F,Lp),e(Lp,LJ),e(LJ,lao),e(Lp,iao),e(Lp,Kk),e(Kk,dao),e(Lp,cao),e(F,fao),e(F,Bp),e(Bp,BJ),e(BJ,mao),e(Bp,gao),e(Bp,Zk),e(Zk,hao),e(Bp,pao),e(F,_ao),e(F,xp),e(xp,xJ),e(xJ,uao),e(xp,bao),e(xp,eR),e(eR,vao),e(xp,Tao),e(F,Fao),e(F,kp),e(kp,kJ),e(kJ,Cao),e(kp,Mao),e(kp,oR),e(oR,Eao),e(kp,yao),e(F,wao),e(F,Rp),e(Rp,RJ),e(RJ,Aao),e(Rp,Lao),e(Rp,rR),e(rR,Bao),e(Rp,xao),e(F,kao),e(F,Sp),e(Sp,SJ),e(SJ,Rao),e(Sp,Sao),e(Sp,tR),e(tR,Pao),e(Sp,$ao),e(xe,Iao),e(xe,Pp),e(Pp,jao),e(Pp,PJ),e(PJ,Nao),e(Pp,Dao),e(Pp,$J),e($J,qao),e(xe,Gao),e(xe,IJ),e(IJ,Oao),e(xe,Xao),g(h4,xe,null),b(d,SAe,u),b(d,Ii,u),e(Ii,$p),e($p,jJ),g(p4,jJ,null),e(Ii,zao),e(Ii,NJ),e(NJ,Vao),b(d,PAe,u),b(d,Vo,u),g(_4,Vo,null),e(Vo,Wao),e(Vo,ji),e(ji,Qao),e(ji,DJ),e(DJ,Hao),e(ji,Uao),e(ji,qJ),e(qJ,Jao),e(ji,Yao),e(Vo,Kao),e(Vo,u4),e(u4,Zao),e(u4,GJ),e(GJ,eno),e(u4,ono),e(Vo,rno),e(Vo,jr),g(b4,jr,null),e(jr,tno),e(jr,OJ),e(OJ,ano),e(jr,nno),e(jr,Ni),e(Ni,sno),e(Ni,XJ),e(XJ,lno),e(Ni,ino),e(Ni,zJ),e(zJ,dno),e(Ni,cno),e(jr,fno),e(jr,VJ),e(VJ,mno),e(jr,gno),g(v4,jr,null),e(Vo,hno),e(Vo,ke),g(T4,ke,null),e(ke,pno),e(ke,WJ),e(WJ,_no),e(ke,uno),e(ke,Ia),e(Ia,bno),e(Ia,QJ),e(QJ,vno),e(Ia,Tno),e(Ia,HJ),e(HJ,Fno),e(Ia,Cno),e(Ia,UJ),e(UJ,Mno),e(Ia,Eno),e(ke,yno),e(ke,k),e(k,Ip),e(Ip,JJ),e(JJ,wno),e(Ip,Ano),e(Ip,aR),e(aR,Lno),e(Ip,Bno),e(k,xno),e(k,jp),e(jp,YJ),e(YJ,kno),e(jp,Rno),e(jp,nR),e(nR,Sno),e(jp,Pno),e(k,$no),e(k,Np),e(Np,KJ),e(KJ,Ino),e(Np,jno),e(Np,sR),e(sR,Nno),e(Np,Dno),e(k,qno),e(k,Dp),e(Dp,ZJ),e(ZJ,Gno),e(Dp,Ono),e(Dp,lR),e(lR,Xno),e(Dp,zno),e(k,Vno),e(k,qp),e(qp,eY),e(eY,Wno),e(qp,Qno),e(qp,iR),e(iR,Hno),e(qp,Uno),e(k,Jno),e(k,Gp),e(Gp,oY),e(oY,Yno),e(Gp,Kno),e(Gp,dR),e(dR,Zno),e(Gp,eso),e(k,oso),e(k,Op),e(Op,rY),e(rY,rso),e(Op,tso),e(Op,cR),e(cR,aso),e(Op,nso),e(k,sso),e(k,Xp),e(Xp,tY),e(tY,lso),e(Xp,iso),e(Xp,fR),e(fR,dso),e(Xp,cso),e(k,fso),e(k,zp),e(zp,aY),e(aY,mso),e(zp,gso),e(zp,mR),e(mR,hso),e(zp,pso),e(k,_so),e(k,Vp),e(Vp,nY),e(nY,uso),e(Vp,bso),e(Vp,gR),e(gR,vso),e(Vp,Tso),e(k,Fso),e(k,Wp),e(Wp,sY),e(sY,Cso),e(Wp,Mso),e(Wp,hR),e(hR,Eso),e(Wp,yso),e(k,wso),e(k,Qp),e(Qp,lY),e(lY,Aso),e(Qp,Lso),e(Qp,pR),e(pR,Bso),e(Qp,xso),e(k,kso),e(k,Hp),e(Hp,iY),e(iY,Rso),e(Hp,Sso),e(Hp,_R),e(_R,Pso),e(Hp,$so),e(k,Iso),e(k,Up),e(Up,dY),e(dY,jso),e(Up,Nso),e(Up,uR),e(uR,Dso),e(Up,qso),e(k,Gso),e(k,Jp),e(Jp,cY),e(cY,Oso),e(Jp,Xso),e(Jp,bR),e(bR,zso),e(Jp,Vso),e(k,Wso),e(k,Yp),e(Yp,fY),e(fY,Qso),e(Yp,Hso),e(Yp,vR),e(vR,Uso),e(Yp,Jso),e(k,Yso),e(k,Kp),e(Kp,mY),e(mY,Kso),e(Kp,Zso),e(Kp,TR),e(TR,elo),e(Kp,olo),e(k,rlo),e(k,Zp),e(Zp,gY),e(gY,tlo),e(Zp,alo),e(Zp,FR),e(FR,nlo),e(Zp,slo),e(k,llo),e(k,e_),e(e_,hY),e(hY,ilo),e(e_,dlo),e(e_,CR),e(CR,clo),e(e_,flo),e(k,mlo),e(k,o_),e(o_,pY),e(pY,glo),e(o_,hlo),e(o_,MR),e(MR,plo),e(o_,_lo),e(k,ulo),e(k,r_),e(r_,_Y),e(_Y,blo),e(r_,vlo),e(r_,ER),e(ER,Tlo),e(r_,Flo),e(k,Clo),e(k,t_),e(t_,uY),e(uY,Mlo),e(t_,Elo),e(t_,yR),e(yR,ylo),e(t_,wlo),e(k,Alo),e(k,a_),e(a_,bY),e(bY,Llo),e(a_,Blo),e(a_,wR),e(wR,xlo),e(a_,klo),e(k,Rlo),e(k,n_),e(n_,vY),e(vY,Slo),e(n_,Plo),e(n_,AR),e(AR,$lo),e(n_,Ilo),e(k,jlo),e(k,s_),e(s_,TY),e(TY,Nlo),e(s_,Dlo),e(s_,LR),e(LR,qlo),e(s_,Glo),e(k,Olo),e(k,l_),e(l_,FY),e(FY,Xlo),e(l_,zlo),e(l_,BR),e(BR,Vlo),e(l_,Wlo),e(k,Qlo),e(k,i_),e(i_,CY),e(CY,Hlo),e(i_,Ulo),e(i_,xR),e(xR,Jlo),e(i_,Ylo),e(k,Klo),e(k,d_),e(d_,MY),e(MY,Zlo),e(d_,eio),e(d_,kR),e(kR,oio),e(d_,rio),e(k,tio),e(k,c_),e(c_,EY),e(EY,aio),e(c_,nio),e(c_,RR),e(RR,sio),e(c_,lio),e(k,iio),e(k,f_),e(f_,yY),e(yY,dio),e(f_,cio),e(f_,SR),e(SR,fio),e(f_,mio),e(k,gio),e(k,m_),e(m_,wY),e(wY,hio),e(m_,pio),e(m_,PR),e(PR,_io),e(m_,uio),e(k,bio),e(k,g_),e(g_,AY),e(AY,vio),e(g_,Tio),e(g_,$R),e($R,Fio),e(g_,Cio),e(k,Mio),e(k,h_),e(h_,LY),e(LY,Eio),e(h_,yio),e(h_,IR),e(IR,wio),e(h_,Aio),e(k,Lio),e(k,p_),e(p_,BY),e(BY,Bio),e(p_,xio),e(p_,jR),e(jR,kio),e(p_,Rio),e(k,Sio),e(k,__),e(__,xY),e(xY,Pio),e(__,$io),e(__,NR),e(NR,Iio),e(__,jio),e(k,Nio),e(k,u_),e(u_,kY),e(kY,Dio),e(u_,qio),e(u_,DR),e(DR,Gio),e(u_,Oio),e(k,Xio),e(k,b_),e(b_,RY),e(RY,zio),e(b_,Vio),e(b_,qR),e(qR,Wio),e(b_,Qio),e(k,Hio),e(k,v_),e(v_,SY),e(SY,Uio),e(v_,Jio),e(v_,GR),e(GR,Yio),e(v_,Kio),e(ke,Zio),e(ke,T_),e(T_,edo),e(T_,PY),e(PY,odo),e(T_,rdo),e(T_,$Y),e($Y,tdo),e(ke,ado),e(ke,IY),e(IY,ndo),e(ke,sdo),g(F4,ke,null),b(d,$Ae,u),b(d,Di,u),e(Di,F_),e(F_,jY),g(C4,jY,null),e(Di,ldo),e(Di,NY),e(NY,ido),b(d,IAe,u),b(d,Wo,u),g(M4,Wo,null),e(Wo,ddo),e(Wo,qi),e(qi,cdo),e(qi,DY),e(DY,fdo),e(qi,mdo),e(qi,qY),e(qY,gdo),e(qi,hdo),e(Wo,pdo),e(Wo,E4),e(E4,_do),e(E4,GY),e(GY,udo),e(E4,bdo),e(Wo,vdo),e(Wo,Nr),g(y4,Nr,null),e(Nr,Tdo),e(Nr,OY),e(OY,Fdo),e(Nr,Cdo),e(Nr,Gi),e(Gi,Mdo),e(Gi,XY),e(XY,Edo),e(Gi,ydo),e(Gi,zY),e(zY,wdo),e(Gi,Ado),e(Nr,Ldo),e(Nr,VY),e(VY,Bdo),e(Nr,xdo),g(w4,Nr,null),e(Wo,kdo),e(Wo,Re),g(A4,Re,null),e(Re,Rdo),e(Re,WY),e(WY,Sdo),e(Re,Pdo),e(Re,ja),e(ja,$do),e(ja,QY),e(QY,Ido),e(ja,jdo),e(ja,HY),e(HY,Ndo),e(ja,Ddo),e(ja,UY),e(UY,qdo),e(ja,Gdo),e(Re,Odo),e(Re,I),e(I,C_),e(C_,JY),e(JY,Xdo),e(C_,zdo),e(C_,OR),e(OR,Vdo),e(C_,Wdo),e(I,Qdo),e(I,M_),e(M_,YY),e(YY,Hdo),e(M_,Udo),e(M_,XR),e(XR,Jdo),e(M_,Ydo),e(I,Kdo),e(I,E_),e(E_,KY),e(KY,Zdo),e(E_,eco),e(E_,zR),e(zR,oco),e(E_,rco),e(I,tco),e(I,y_),e(y_,ZY),e(ZY,aco),e(y_,nco),e(y_,VR),e(VR,sco),e(y_,lco),e(I,ico),e(I,w_),e(w_,eK),e(eK,dco),e(w_,cco),e(w_,WR),e(WR,fco),e(w_,mco),e(I,gco),e(I,A_),e(A_,oK),e(oK,hco),e(A_,pco),e(A_,QR),e(QR,_co),e(A_,uco),e(I,bco),e(I,L_),e(L_,rK),e(rK,vco),e(L_,Tco),e(L_,HR),e(HR,Fco),e(L_,Cco),e(I,Mco),e(I,B_),e(B_,tK),e(tK,Eco),e(B_,yco),e(B_,UR),e(UR,wco),e(B_,Aco),e(I,Lco),e(I,x_),e(x_,aK),e(aK,Bco),e(x_,xco),e(x_,JR),e(JR,kco),e(x_,Rco),e(I,Sco),e(I,k_),e(k_,nK),e(nK,Pco),e(k_,$co),e(k_,YR),e(YR,Ico),e(k_,jco),e(I,Nco),e(I,R_),e(R_,sK),e(sK,Dco),e(R_,qco),e(R_,KR),e(KR,Gco),e(R_,Oco),e(I,Xco),e(I,S_),e(S_,lK),e(lK,zco),e(S_,Vco),e(S_,ZR),e(ZR,Wco),e(S_,Qco),e(I,Hco),e(I,P_),e(P_,iK),e(iK,Uco),e(P_,Jco),e(P_,eS),e(eS,Yco),e(P_,Kco),e(I,Zco),e(I,$_),e($_,dK),e(dK,efo),e($_,ofo),e($_,oS),e(oS,rfo),e($_,tfo),e(I,afo),e(I,I_),e(I_,cK),e(cK,nfo),e(I_,sfo),e(I_,rS),e(rS,lfo),e(I_,ifo),e(I,dfo),e(I,j_),e(j_,fK),e(fK,cfo),e(j_,ffo),e(j_,tS),e(tS,mfo),e(j_,gfo),e(I,hfo),e(I,N_),e(N_,mK),e(mK,pfo),e(N_,_fo),e(N_,aS),e(aS,ufo),e(N_,bfo),e(I,vfo),e(I,D_),e(D_,gK),e(gK,Tfo),e(D_,Ffo),e(D_,nS),e(nS,Cfo),e(D_,Mfo),e(I,Efo),e(I,q_),e(q_,hK),e(hK,yfo),e(q_,wfo),e(q_,sS),e(sS,Afo),e(q_,Lfo),e(I,Bfo),e(I,G_),e(G_,pK),e(pK,xfo),e(G_,kfo),e(G_,lS),e(lS,Rfo),e(G_,Sfo),e(I,Pfo),e(I,O_),e(O_,_K),e(_K,$fo),e(O_,Ifo),e(O_,iS),e(iS,jfo),e(O_,Nfo),e(I,Dfo),e(I,X_),e(X_,uK),e(uK,qfo),e(X_,Gfo),e(X_,dS),e(dS,Ofo),e(X_,Xfo),e(I,zfo),e(I,z_),e(z_,bK),e(bK,Vfo),e(z_,Wfo),e(z_,cS),e(cS,Qfo),e(z_,Hfo),e(I,Ufo),e(I,V_),e(V_,vK),e(vK,Jfo),e(V_,Yfo),e(V_,fS),e(fS,Kfo),e(V_,Zfo),e(I,emo),e(I,W_),e(W_,TK),e(TK,omo),e(W_,rmo),e(W_,mS),e(mS,tmo),e(W_,amo),e(I,nmo),e(I,Q_),e(Q_,FK),e(FK,smo),e(Q_,lmo),e(Q_,gS),e(gS,imo),e(Q_,dmo),e(I,cmo),e(I,H_),e(H_,CK),e(CK,fmo),e(H_,mmo),e(H_,hS),e(hS,gmo),e(H_,hmo),e(I,pmo),e(I,U_),e(U_,MK),e(MK,_mo),e(U_,umo),e(U_,pS),e(pS,bmo),e(U_,vmo),e(I,Tmo),e(I,J_),e(J_,EK),e(EK,Fmo),e(J_,Cmo),e(J_,_S),e(_S,Mmo),e(J_,Emo),e(I,ymo),e(I,Y_),e(Y_,yK),e(yK,wmo),e(Y_,Amo),e(Y_,uS),e(uS,Lmo),e(Y_,Bmo),e(I,xmo),e(I,K_),e(K_,wK),e(wK,kmo),e(K_,Rmo),e(K_,bS),e(bS,Smo),e(K_,Pmo),e(I,$mo),e(I,Z_),e(Z_,AK),e(AK,Imo),e(Z_,jmo),e(Z_,vS),e(vS,Nmo),e(Z_,Dmo),e(I,qmo),e(I,eu),e(eu,LK),e(LK,Gmo),e(eu,Omo),e(eu,TS),e(TS,Xmo),e(eu,zmo),e(Re,Vmo),e(Re,ou),e(ou,Wmo),e(ou,BK),e(BK,Qmo),e(ou,Hmo),e(ou,xK),e(xK,Umo),e(Re,Jmo),e(Re,kK),e(kK,Ymo),e(Re,Kmo),g(L4,Re,null),b(d,jAe,u),b(d,Oi,u),e(Oi,ru),e(ru,RK),g(B4,RK,null),e(Oi,Zmo),e(Oi,SK),e(SK,ego),b(d,NAe,u),b(d,Qo,u),g(x4,Qo,null),e(Qo,ogo),e(Qo,Xi),e(Xi,rgo),e(Xi,PK),e(PK,tgo),e(Xi,ago),e(Xi,$K),e($K,ngo),e(Xi,sgo),e(Qo,lgo),e(Qo,k4),e(k4,igo),e(k4,IK),e(IK,dgo),e(k4,cgo),e(Qo,fgo),e(Qo,Dr),g(R4,Dr,null),e(Dr,mgo),e(Dr,jK),e(jK,ggo),e(Dr,hgo),e(Dr,zi),e(zi,pgo),e(zi,NK),e(NK,_go),e(zi,ugo),e(zi,DK),e(DK,bgo),e(zi,vgo),e(Dr,Tgo),e(Dr,qK),e(qK,Fgo),e(Dr,Cgo),g(S4,Dr,null),e(Qo,Mgo),e(Qo,Se),g(P4,Se,null),e(Se,Ego),e(Se,GK),e(GK,ygo),e(Se,wgo),e(Se,Na),e(Na,Ago),e(Na,OK),e(OK,Lgo),e(Na,Bgo),e(Na,XK),e(XK,xgo),e(Na,kgo),e(Na,zK),e(zK,Rgo),e(Na,Sgo),e(Se,Pgo),e(Se,$),e($,tu),e(tu,VK),e(VK,$go),e(tu,Igo),e(tu,FS),e(FS,jgo),e(tu,Ngo),e($,Dgo),e($,au),e(au,WK),e(WK,qgo),e(au,Ggo),e(au,CS),e(CS,Ogo),e(au,Xgo),e($,zgo),e($,nu),e(nu,QK),e(QK,Vgo),e(nu,Wgo),e(nu,MS),e(MS,Qgo),e(nu,Hgo),e($,Ugo),e($,su),e(su,HK),e(HK,Jgo),e(su,Ygo),e(su,ES),e(ES,Kgo),e(su,Zgo),e($,eho),e($,lu),e(lu,UK),e(UK,oho),e(lu,rho),e(lu,yS),e(yS,tho),e(lu,aho),e($,nho),e($,iu),e(iu,JK),e(JK,sho),e(iu,lho),e(iu,wS),e(wS,iho),e(iu,dho),e($,cho),e($,du),e(du,YK),e(YK,fho),e(du,mho),e(du,AS),e(AS,gho),e(du,hho),e($,pho),e($,cu),e(cu,KK),e(KK,_ho),e(cu,uho),e(cu,LS),e(LS,bho),e(cu,vho),e($,Tho),e($,fu),e(fu,ZK),e(ZK,Fho),e(fu,Cho),e(fu,BS),e(BS,Mho),e(fu,Eho),e($,yho),e($,mu),e(mu,eZ),e(eZ,who),e(mu,Aho),e(mu,xS),e(xS,Lho),e(mu,Bho),e($,xho),e($,gu),e(gu,oZ),e(oZ,kho),e(gu,Rho),e(gu,kS),e(kS,Sho),e(gu,Pho),e($,$ho),e($,hu),e(hu,rZ),e(rZ,Iho),e(hu,jho),e(hu,RS),e(RS,Nho),e(hu,Dho),e($,qho),e($,pu),e(pu,tZ),e(tZ,Gho),e(pu,Oho),e(pu,SS),e(SS,Xho),e(pu,zho),e($,Vho),e($,_u),e(_u,aZ),e(aZ,Who),e(_u,Qho),e(_u,PS),e(PS,Hho),e(_u,Uho),e($,Jho),e($,uu),e(uu,nZ),e(nZ,Yho),e(uu,Kho),e(uu,$S),e($S,Zho),e(uu,epo),e($,opo),e($,bu),e(bu,sZ),e(sZ,rpo),e(bu,tpo),e(bu,IS),e(IS,apo),e(bu,npo),e($,spo),e($,vu),e(vu,lZ),e(lZ,lpo),e(vu,ipo),e(vu,jS),e(jS,dpo),e(vu,cpo),e($,fpo),e($,Tu),e(Tu,iZ),e(iZ,mpo),e(Tu,gpo),e(Tu,NS),e(NS,hpo),e(Tu,ppo),e($,_po),e($,Fu),e(Fu,dZ),e(dZ,upo),e(Fu,bpo),e(Fu,DS),e(DS,vpo),e(Fu,Tpo),e($,Fpo),e($,Cu),e(Cu,cZ),e(cZ,Cpo),e(Cu,Mpo),e(Cu,qS),e(qS,Epo),e(Cu,ypo),e($,wpo),e($,Mu),e(Mu,fZ),e(fZ,Apo),e(Mu,Lpo),e(Mu,GS),e(GS,Bpo),e(Mu,xpo),e($,kpo),e($,Eu),e(Eu,mZ),e(mZ,Rpo),e(Eu,Spo),e(Eu,OS),e(OS,Ppo),e(Eu,$po),e($,Ipo),e($,yu),e(yu,gZ),e(gZ,jpo),e(yu,Npo),e(yu,XS),e(XS,Dpo),e(yu,qpo),e($,Gpo),e($,wu),e(wu,hZ),e(hZ,Opo),e(wu,Xpo),e(wu,zS),e(zS,zpo),e(wu,Vpo),e($,Wpo),e($,Au),e(Au,pZ),e(pZ,Qpo),e(Au,Hpo),e(Au,VS),e(VS,Upo),e(Au,Jpo),e($,Ypo),e($,Lu),e(Lu,_Z),e(_Z,Kpo),e(Lu,Zpo),e(Lu,WS),e(WS,e_o),e(Lu,o_o),e($,r_o),e($,Bu),e(Bu,uZ),e(uZ,t_o),e(Bu,a_o),e(Bu,QS),e(QS,n_o),e(Bu,s_o),e($,l_o),e($,xu),e(xu,bZ),e(bZ,i_o),e(xu,d_o),e(xu,HS),e(HS,c_o),e(xu,f_o),e($,m_o),e($,ku),e(ku,vZ),e(vZ,g_o),e(ku,h_o),e(ku,US),e(US,p_o),e(ku,__o),e($,u_o),e($,Ru),e(Ru,TZ),e(TZ,b_o),e(Ru,v_o),e(Ru,FZ),e(FZ,T_o),e(Ru,F_o),e($,C_o),e($,Su),e(Su,CZ),e(CZ,M_o),e(Su,E_o),e(Su,JS),e(JS,y_o),e(Su,w_o),e($,A_o),e($,Pu),e(Pu,MZ),e(MZ,L_o),e(Pu,B_o),e(Pu,YS),e(YS,x_o),e(Pu,k_o),e($,R_o),e($,$u),e($u,EZ),e(EZ,S_o),e($u,P_o),e($u,KS),e(KS,$_o),e($u,I_o),e($,j_o),e($,Iu),e(Iu,yZ),e(yZ,N_o),e(Iu,D_o),e(Iu,ZS),e(ZS,q_o),e(Iu,G_o),e(Se,O_o),e(Se,ju),e(ju,X_o),e(ju,wZ),e(wZ,z_o),e(ju,V_o),e(ju,AZ),e(AZ,W_o),e(Se,Q_o),e(Se,LZ),e(LZ,H_o),e(Se,U_o),g($4,Se,null),b(d,DAe,u),b(d,Vi,u),e(Vi,Nu),e(Nu,BZ),g(I4,BZ,null),e(Vi,J_o),e(Vi,xZ),e(xZ,Y_o),b(d,qAe,u),b(d,Ho,u),g(j4,Ho,null),e(Ho,K_o),e(Ho,Wi),e(Wi,Z_o),e(Wi,kZ),e(kZ,euo),e(Wi,ouo),e(Wi,RZ),e(RZ,ruo),e(Wi,tuo),e(Ho,auo),e(Ho,N4),e(N4,nuo),e(N4,SZ),e(SZ,suo),e(N4,luo),e(Ho,iuo),e(Ho,qr),g(D4,qr,null),e(qr,duo),e(qr,PZ),e(PZ,cuo),e(qr,fuo),e(qr,Qi),e(Qi,muo),e(Qi,$Z),e($Z,guo),e(Qi,huo),e(Qi,IZ),e(IZ,puo),e(Qi,_uo),e(qr,uuo),e(qr,jZ),e(jZ,buo),e(qr,vuo),g(q4,qr,null),e(Ho,Tuo),e(Ho,Pe),g(G4,Pe,null),e(Pe,Fuo),e(Pe,NZ),e(NZ,Cuo),e(Pe,Muo),e(Pe,Da),e(Da,Euo),e(Da,DZ),e(DZ,yuo),e(Da,wuo),e(Da,qZ),e(qZ,Auo),e(Da,Luo),e(Da,GZ),e(GZ,Buo),e(Da,xuo),e(Pe,kuo),e(Pe,ne),e(ne,Du),e(Du,OZ),e(OZ,Ruo),e(Du,Suo),e(Du,eP),e(eP,Puo),e(Du,$uo),e(ne,Iuo),e(ne,qu),e(qu,XZ),e(XZ,juo),e(qu,Nuo),e(qu,oP),e(oP,Duo),e(qu,quo),e(ne,Guo),e(ne,Gu),e(Gu,zZ),e(zZ,Ouo),e(Gu,Xuo),e(Gu,rP),e(rP,zuo),e(Gu,Vuo),e(ne,Wuo),e(ne,Ou),e(Ou,VZ),e(VZ,Quo),e(Ou,Huo),e(Ou,tP),e(tP,Uuo),e(Ou,Juo),e(ne,Yuo),e(ne,Xu),e(Xu,WZ),e(WZ,Kuo),e(Xu,Zuo),e(Xu,aP),e(aP,e1o),e(Xu,o1o),e(ne,r1o),e(ne,zu),e(zu,QZ),e(QZ,t1o),e(zu,a1o),e(zu,nP),e(nP,n1o),e(zu,s1o),e(ne,l1o),e(ne,Vu),e(Vu,HZ),e(HZ,i1o),e(Vu,d1o),e(Vu,sP),e(sP,c1o),e(Vu,f1o),e(ne,m1o),e(ne,Wu),e(Wu,UZ),e(UZ,g1o),e(Wu,h1o),e(Wu,lP),e(lP,p1o),e(Wu,_1o),e(ne,u1o),e(ne,Qu),e(Qu,JZ),e(JZ,b1o),e(Qu,v1o),e(Qu,iP),e(iP,T1o),e(Qu,F1o),e(ne,C1o),e(ne,Hu),e(Hu,YZ),e(YZ,M1o),e(Hu,E1o),e(Hu,dP),e(dP,y1o),e(Hu,w1o),e(ne,A1o),e(ne,Uu),e(Uu,KZ),e(KZ,L1o),e(Uu,B1o),e(Uu,cP),e(cP,x1o),e(Uu,k1o),e(ne,R1o),e(ne,Ju),e(Ju,ZZ),e(ZZ,S1o),e(Ju,P1o),e(Ju,fP),e(fP,$1o),e(Ju,I1o),e(ne,j1o),e(ne,Yu),e(Yu,eee),e(eee,N1o),e(Yu,D1o),e(Yu,mP),e(mP,q1o),e(Yu,G1o),e(ne,O1o),e(ne,Ku),e(Ku,oee),e(oee,X1o),e(Ku,z1o),e(Ku,gP),e(gP,V1o),e(Ku,W1o),e(ne,Q1o),e(ne,Zu),e(Zu,ree),e(ree,H1o),e(Zu,U1o),e(Zu,hP),e(hP,J1o),e(Zu,Y1o),e(Pe,K1o),e(Pe,e1),e(e1,Z1o),e(e1,tee),e(tee,ebo),e(e1,obo),e(e1,aee),e(aee,rbo),e(Pe,tbo),e(Pe,nee),e(nee,abo),e(Pe,nbo),g(O4,Pe,null),b(d,GAe,u),b(d,Hi,u),e(Hi,o1),e(o1,see),g(X4,see,null),e(Hi,sbo),e(Hi,lee),e(lee,lbo),b(d,OAe,u),b(d,Uo,u),g(z4,Uo,null),e(Uo,ibo),e(Uo,Ui),e(Ui,dbo),e(Ui,iee),e(iee,cbo),e(Ui,fbo),e(Ui,dee),e(dee,mbo),e(Ui,gbo),e(Uo,hbo),e(Uo,V4),e(V4,pbo),e(V4,cee),e(cee,_bo),e(V4,ubo),e(Uo,bbo),e(Uo,Gr),g(W4,Gr,null),e(Gr,vbo),e(Gr,fee),e(fee,Tbo),e(Gr,Fbo),e(Gr,Ji),e(Ji,Cbo),e(Ji,mee),e(mee,Mbo),e(Ji,Ebo),e(Ji,gee),e(gee,ybo),e(Ji,wbo),e(Gr,Abo),e(Gr,hee),e(hee,Lbo),e(Gr,Bbo),g(Q4,Gr,null),e(Uo,xbo),e(Uo,$e),g(H4,$e,null),e($e,kbo),e($e,pee),e(pee,Rbo),e($e,Sbo),e($e,qa),e(qa,Pbo),e(qa,_ee),e(_ee,$bo),e(qa,Ibo),e(qa,uee),e(uee,jbo),e(qa,Nbo),e(qa,bee),e(bee,Dbo),e(qa,qbo),e($e,Gbo),e($e,A),e(A,r1),e(r1,vee),e(vee,Obo),e(r1,Xbo),e(r1,pP),e(pP,zbo),e(r1,Vbo),e(A,Wbo),e(A,t1),e(t1,Tee),e(Tee,Qbo),e(t1,Hbo),e(t1,_P),e(_P,Ubo),e(t1,Jbo),e(A,Ybo),e(A,a1),e(a1,Fee),e(Fee,Kbo),e(a1,Zbo),e(a1,uP),e(uP,e5o),e(a1,o5o),e(A,r5o),e(A,n1),e(n1,Cee),e(Cee,t5o),e(n1,a5o),e(n1,bP),e(bP,n5o),e(n1,s5o),e(A,l5o),e(A,s1),e(s1,Mee),e(Mee,i5o),e(s1,d5o),e(s1,vP),e(vP,c5o),e(s1,f5o),e(A,m5o),e(A,l1),e(l1,Eee),e(Eee,g5o),e(l1,h5o),e(l1,TP),e(TP,p5o),e(l1,_5o),e(A,u5o),e(A,i1),e(i1,yee),e(yee,b5o),e(i1,v5o),e(i1,FP),e(FP,T5o),e(i1,F5o),e(A,C5o),e(A,d1),e(d1,wee),e(wee,M5o),e(d1,E5o),e(d1,CP),e(CP,y5o),e(d1,w5o),e(A,A5o),e(A,c1),e(c1,Aee),e(Aee,L5o),e(c1,B5o),e(c1,MP),e(MP,x5o),e(c1,k5o),e(A,R5o),e(A,f1),e(f1,Lee),e(Lee,S5o),e(f1,P5o),e(f1,EP),e(EP,$5o),e(f1,I5o),e(A,j5o),e(A,m1),e(m1,Bee),e(Bee,N5o),e(m1,D5o),e(m1,yP),e(yP,q5o),e(m1,G5o),e(A,O5o),e(A,g1),e(g1,xee),e(xee,X5o),e(g1,z5o),e(g1,wP),e(wP,V5o),e(g1,W5o),e(A,Q5o),e(A,h1),e(h1,kee),e(kee,H5o),e(h1,U5o),e(h1,AP),e(AP,J5o),e(h1,Y5o),e(A,K5o),e(A,p1),e(p1,Ree),e(Ree,Z5o),e(p1,e2o),e(p1,LP),e(LP,o2o),e(p1,r2o),e(A,t2o),e(A,_1),e(_1,See),e(See,a2o),e(_1,n2o),e(_1,BP),e(BP,s2o),e(_1,l2o),e(A,i2o),e(A,u1),e(u1,Pee),e(Pee,d2o),e(u1,c2o),e(u1,xP),e(xP,f2o),e(u1,m2o),e(A,g2o),e(A,b1),e(b1,$ee),e($ee,h2o),e(b1,p2o),e(b1,kP),e(kP,_2o),e(b1,u2o),e(A,b2o),e(A,v1),e(v1,Iee),e(Iee,v2o),e(v1,T2o),e(v1,RP),e(RP,F2o),e(v1,C2o),e(A,M2o),e(A,T1),e(T1,jee),e(jee,E2o),e(T1,y2o),e(T1,SP),e(SP,w2o),e(T1,A2o),e(A,L2o),e(A,F1),e(F1,Nee),e(Nee,B2o),e(F1,x2o),e(F1,PP),e(PP,k2o),e(F1,R2o),e(A,S2o),e(A,C1),e(C1,Dee),e(Dee,P2o),e(C1,$2o),e(C1,$P),e($P,I2o),e(C1,j2o),e(A,N2o),e(A,M1),e(M1,qee),e(qee,D2o),e(M1,q2o),e(M1,IP),e(IP,G2o),e(M1,O2o),e(A,X2o),e(A,E1),e(E1,Gee),e(Gee,z2o),e(E1,V2o),e(E1,jP),e(jP,W2o),e(E1,Q2o),e(A,H2o),e(A,y1),e(y1,Oee),e(Oee,U2o),e(y1,J2o),e(y1,NP),e(NP,Y2o),e(y1,K2o),e(A,Z2o),e(A,w1),e(w1,Xee),e(Xee,evo),e(w1,ovo),e(w1,DP),e(DP,rvo),e(w1,tvo),e(A,avo),e(A,A1),e(A1,zee),e(zee,nvo),e(A1,svo),e(A1,qP),e(qP,lvo),e(A1,ivo),e(A,dvo),e(A,L1),e(L1,Vee),e(Vee,cvo),e(L1,fvo),e(L1,GP),e(GP,mvo),e(L1,gvo),e(A,hvo),e(A,B1),e(B1,Wee),e(Wee,pvo),e(B1,_vo),e(B1,OP),e(OP,uvo),e(B1,bvo),e(A,vvo),e(A,x1),e(x1,Qee),e(Qee,Tvo),e(x1,Fvo),e(x1,XP),e(XP,Cvo),e(x1,Mvo),e(A,Evo),e(A,k1),e(k1,Hee),e(Hee,yvo),e(k1,wvo),e(k1,zP),e(zP,Avo),e(k1,Lvo),e(A,Bvo),e(A,R1),e(R1,Uee),e(Uee,xvo),e(R1,kvo),e(R1,VP),e(VP,Rvo),e(R1,Svo),e(A,Pvo),e(A,S1),e(S1,Jee),e(Jee,$vo),e(S1,Ivo),e(S1,WP),e(WP,jvo),e(S1,Nvo),e(A,Dvo),e(A,P1),e(P1,Yee),e(Yee,qvo),e(P1,Gvo),e(P1,QP),e(QP,Ovo),e(P1,Xvo),e(A,zvo),e(A,$1),e($1,Kee),e(Kee,Vvo),e($1,Wvo),e($1,HP),e(HP,Qvo),e($1,Hvo),e(A,Uvo),e(A,I1),e(I1,Zee),e(Zee,Jvo),e(I1,Yvo),e(I1,UP),e(UP,Kvo),e(I1,Zvo),e(A,e6o),e(A,j1),e(j1,eoe),e(eoe,o6o),e(j1,r6o),e(j1,JP),e(JP,t6o),e(j1,a6o),e(A,n6o),e(A,N1),e(N1,ooe),e(ooe,s6o),e(N1,l6o),e(N1,YP),e(YP,i6o),e(N1,d6o),e(A,c6o),e(A,D1),e(D1,roe),e(roe,f6o),e(D1,m6o),e(D1,KP),e(KP,g6o),e(D1,h6o),e(A,p6o),e(A,q1),e(q1,toe),e(toe,_6o),e(q1,u6o),e(q1,ZP),e(ZP,b6o),e(q1,v6o),e(A,T6o),e(A,G1),e(G1,aoe),e(aoe,F6o),e(G1,C6o),e(G1,e$),e(e$,M6o),e(G1,E6o),e(A,y6o),e(A,O1),e(O1,noe),e(noe,w6o),e(O1,A6o),e(O1,o$),e(o$,L6o),e(O1,B6o),e(A,x6o),e(A,X1),e(X1,soe),e(soe,k6o),e(X1,R6o),e(X1,r$),e(r$,S6o),e(X1,P6o),e(A,$6o),e(A,z1),e(z1,loe),e(loe,I6o),e(z1,j6o),e(z1,t$),e(t$,N6o),e(z1,D6o),e(A,q6o),e(A,V1),e(V1,ioe),e(ioe,G6o),e(V1,O6o),e(V1,a$),e(a$,X6o),e(V1,z6o),e($e,V6o),e($e,W1),e(W1,W6o),e(W1,doe),e(doe,Q6o),e(W1,H6o),e(W1,coe),e(coe,U6o),e($e,J6o),e($e,foe),e(foe,Y6o),e($e,K6o),g(U4,$e,null),b(d,XAe,u),b(d,Yi,u),e(Yi,Q1),e(Q1,moe),g(J4,moe,null),e(Yi,Z6o),e(Yi,goe),e(goe,eTo),b(d,zAe,u),b(d,Jo,u),g(Y4,Jo,null),e(Jo,oTo),e(Jo,Ki),e(Ki,rTo),e(Ki,hoe),e(hoe,tTo),e(Ki,aTo),e(Ki,poe),e(poe,nTo),e(Ki,sTo),e(Jo,lTo),e(Jo,K4),e(K4,iTo),e(K4,_oe),e(_oe,dTo),e(K4,cTo),e(Jo,fTo),e(Jo,Or),g(Z4,Or,null),e(Or,mTo),e(Or,uoe),e(uoe,gTo),e(Or,hTo),e(Or,Zi),e(Zi,pTo),e(Zi,boe),e(boe,_To),e(Zi,uTo),e(Zi,voe),e(voe,bTo),e(Zi,vTo),e(Or,TTo),e(Or,Toe),e(Toe,FTo),e(Or,CTo),g(eM,Or,null),e(Jo,MTo),e(Jo,Ie),g(oM,Ie,null),e(Ie,ETo),e(Ie,Foe),e(Foe,yTo),e(Ie,wTo),e(Ie,Ga),e(Ga,ATo),e(Ga,Coe),e(Coe,LTo),e(Ga,BTo),e(Ga,Moe),e(Moe,xTo),e(Ga,kTo),e(Ga,Eoe),e(Eoe,RTo),e(Ga,STo),e(Ie,PTo),e(Ie,G),e(G,H1),e(H1,yoe),e(yoe,$To),e(H1,ITo),e(H1,n$),e(n$,jTo),e(H1,NTo),e(G,DTo),e(G,U1),e(U1,woe),e(woe,qTo),e(U1,GTo),e(U1,s$),e(s$,OTo),e(U1,XTo),e(G,zTo),e(G,J1),e(J1,Aoe),e(Aoe,VTo),e(J1,WTo),e(J1,l$),e(l$,QTo),e(J1,HTo),e(G,UTo),e(G,Y1),e(Y1,Loe),e(Loe,JTo),e(Y1,YTo),e(Y1,i$),e(i$,KTo),e(Y1,ZTo),e(G,e7o),e(G,K1),e(K1,Boe),e(Boe,o7o),e(K1,r7o),e(K1,d$),e(d$,t7o),e(K1,a7o),e(G,n7o),e(G,Z1),e(Z1,xoe),e(xoe,s7o),e(Z1,l7o),e(Z1,c$),e(c$,i7o),e(Z1,d7o),e(G,c7o),e(G,eb),e(eb,koe),e(koe,f7o),e(eb,m7o),e(eb,f$),e(f$,g7o),e(eb,h7o),e(G,p7o),e(G,ob),e(ob,Roe),e(Roe,_7o),e(ob,u7o),e(ob,m$),e(m$,b7o),e(ob,v7o),e(G,T7o),e(G,rb),e(rb,Soe),e(Soe,F7o),e(rb,C7o),e(rb,g$),e(g$,M7o),e(rb,E7o),e(G,y7o),e(G,tb),e(tb,Poe),e(Poe,w7o),e(tb,A7o),e(tb,h$),e(h$,L7o),e(tb,B7o),e(G,x7o),e(G,ab),e(ab,$oe),e($oe,k7o),e(ab,R7o),e(ab,p$),e(p$,S7o),e(ab,P7o),e(G,$7o),e(G,nb),e(nb,Ioe),e(Ioe,I7o),e(nb,j7o),e(nb,_$),e(_$,N7o),e(nb,D7o),e(G,q7o),e(G,sb),e(sb,joe),e(joe,G7o),e(sb,O7o),e(sb,u$),e(u$,X7o),e(sb,z7o),e(G,V7o),e(G,lb),e(lb,Noe),e(Noe,W7o),e(lb,Q7o),e(lb,b$),e(b$,H7o),e(lb,U7o),e(G,J7o),e(G,ib),e(ib,Doe),e(Doe,Y7o),e(ib,K7o),e(ib,v$),e(v$,Z7o),e(ib,e8o),e(G,o8o),e(G,db),e(db,qoe),e(qoe,r8o),e(db,t8o),e(db,T$),e(T$,a8o),e(db,n8o),e(G,s8o),e(G,cb),e(cb,Goe),e(Goe,l8o),e(cb,i8o),e(cb,F$),e(F$,d8o),e(cb,c8o),e(G,f8o),e(G,fb),e(fb,Ooe),e(Ooe,m8o),e(fb,g8o),e(fb,C$),e(C$,h8o),e(fb,p8o),e(G,_8o),e(G,mb),e(mb,Xoe),e(Xoe,u8o),e(mb,b8o),e(mb,M$),e(M$,v8o),e(mb,T8o),e(G,F8o),e(G,gb),e(gb,zoe),e(zoe,C8o),e(gb,M8o),e(gb,E$),e(E$,E8o),e(gb,y8o),e(G,w8o),e(G,hb),e(hb,Voe),e(Voe,A8o),e(hb,L8o),e(hb,y$),e(y$,B8o),e(hb,x8o),e(G,k8o),e(G,pb),e(pb,Woe),e(Woe,R8o),e(pb,S8o),e(pb,w$),e(w$,P8o),e(pb,$8o),e(G,I8o),e(G,_b),e(_b,Qoe),e(Qoe,j8o),e(_b,N8o),e(_b,A$),e(A$,D8o),e(_b,q8o),e(G,G8o),e(G,ub),e(ub,Hoe),e(Hoe,O8o),e(ub,X8o),e(ub,L$),e(L$,z8o),e(ub,V8o),e(G,W8o),e(G,bb),e(bb,Uoe),e(Uoe,Q8o),e(bb,H8o),e(bb,B$),e(B$,U8o),e(bb,J8o),e(G,Y8o),e(G,vb),e(vb,Joe),e(Joe,K8o),e(vb,Z8o),e(vb,x$),e(x$,eFo),e(vb,oFo),e(G,rFo),e(G,Tb),e(Tb,Yoe),e(Yoe,tFo),e(Tb,aFo),e(Tb,k$),e(k$,nFo),e(Tb,sFo),e(Ie,lFo),e(Ie,Fb),e(Fb,iFo),e(Fb,Koe),e(Koe,dFo),e(Fb,cFo),e(Fb,Zoe),e(Zoe,fFo),e(Ie,mFo),e(Ie,ere),e(ere,gFo),e(Ie,hFo),g(rM,Ie,null),b(d,VAe,u),b(d,ed,u),e(ed,Cb),e(Cb,ore),g(tM,ore,null),e(ed,pFo),e(ed,rre),e(rre,_Fo),b(d,WAe,u),b(d,Yo,u),g(aM,Yo,null),e(Yo,uFo),e(Yo,od),e(od,bFo),e(od,tre),e(tre,vFo),e(od,TFo),e(od,are),e(are,FFo),e(od,CFo),e(Yo,MFo),e(Yo,nM),e(nM,EFo),e(nM,nre),e(nre,yFo),e(nM,wFo),e(Yo,AFo),e(Yo,Xr),g(sM,Xr,null),e(Xr,LFo),e(Xr,sre),e(sre,BFo),e(Xr,xFo),e(Xr,rd),e(rd,kFo),e(rd,lre),e(lre,RFo),e(rd,SFo),e(rd,ire),e(ire,PFo),e(rd,$Fo),e(Xr,IFo),e(Xr,dre),e(dre,jFo),e(Xr,NFo),g(lM,Xr,null),e(Yo,DFo),e(Yo,je),g(iM,je,null),e(je,qFo),e(je,cre),e(cre,GFo),e(je,OFo),e(je,Oa),e(Oa,XFo),e(Oa,fre),e(fre,zFo),e(Oa,VFo),e(Oa,mre),e(mre,WFo),e(Oa,QFo),e(Oa,gre),e(gre,HFo),e(Oa,UFo),e(je,JFo),e(je,oa),e(oa,Mb),e(Mb,hre),e(hre,YFo),e(Mb,KFo),e(Mb,R$),e(R$,ZFo),e(Mb,eCo),e(oa,oCo),e(oa,Eb),e(Eb,pre),e(pre,rCo),e(Eb,tCo),e(Eb,S$),e(S$,aCo),e(Eb,nCo),e(oa,sCo),e(oa,yb),e(yb,_re),e(_re,lCo),e(yb,iCo),e(yb,P$),e(P$,dCo),e(yb,cCo),e(oa,fCo),e(oa,wb),e(wb,ure),e(ure,mCo),e(wb,gCo),e(wb,$$),e($$,hCo),e(wb,pCo),e(oa,_Co),e(oa,Ab),e(Ab,bre),e(bre,uCo),e(Ab,bCo),e(Ab,I$),e(I$,vCo),e(Ab,TCo),e(je,FCo),e(je,Lb),e(Lb,CCo),e(Lb,vre),e(vre,MCo),e(Lb,ECo),e(Lb,Tre),e(Tre,yCo),e(je,wCo),e(je,Fre),e(Fre,ACo),e(je,LCo),g(dM,je,null),b(d,QAe,u),b(d,td,u),e(td,Bb),e(Bb,Cre),g(cM,Cre,null),e(td,BCo),e(td,Mre),e(Mre,xCo),b(d,HAe,u),b(d,Ko,u),g(fM,Ko,null),e(Ko,kCo),e(Ko,ad),e(ad,RCo),e(ad,Ere),e(Ere,SCo),e(ad,PCo),e(ad,yre),e(yre,$Co),e(ad,ICo),e(Ko,jCo),e(Ko,mM),e(mM,NCo),e(mM,wre),e(wre,DCo),e(mM,qCo),e(Ko,GCo),e(Ko,zr),g(gM,zr,null),e(zr,OCo),e(zr,Are),e(Are,XCo),e(zr,zCo),e(zr,nd),e(nd,VCo),e(nd,Lre),e(Lre,WCo),e(nd,QCo),e(nd,Bre),e(Bre,HCo),e(nd,UCo),e(zr,JCo),e(zr,xre),e(xre,YCo),e(zr,KCo),g(hM,zr,null),e(Ko,ZCo),e(Ko,Ne),g(pM,Ne,null),e(Ne,e4o),e(Ne,kre),e(kre,o4o),e(Ne,r4o),e(Ne,Xa),e(Xa,t4o),e(Xa,Rre),e(Rre,a4o),e(Xa,n4o),e(Xa,Sre),e(Sre,s4o),e(Xa,l4o),e(Xa,Pre),e(Pre,i4o),e(Xa,d4o),e(Ne,c4o),e(Ne,N),e(N,xb),e(xb,$re),e($re,f4o),e(xb,m4o),e(xb,j$),e(j$,g4o),e(xb,h4o),e(N,p4o),e(N,kb),e(kb,Ire),e(Ire,_4o),e(kb,u4o),e(kb,N$),e(N$,b4o),e(kb,v4o),e(N,T4o),e(N,Rb),e(Rb,jre),e(jre,F4o),e(Rb,C4o),e(Rb,D$),e(D$,M4o),e(Rb,E4o),e(N,y4o),e(N,Sb),e(Sb,Nre),e(Nre,w4o),e(Sb,A4o),e(Sb,q$),e(q$,L4o),e(Sb,B4o),e(N,x4o),e(N,Pb),e(Pb,Dre),e(Dre,k4o),e(Pb,R4o),e(Pb,G$),e(G$,S4o),e(Pb,P4o),e(N,$4o),e(N,$b),e($b,qre),e(qre,I4o),e($b,j4o),e($b,O$),e(O$,N4o),e($b,D4o),e(N,q4o),e(N,Ib),e(Ib,Gre),e(Gre,G4o),e(Ib,O4o),e(Ib,X$),e(X$,X4o),e(Ib,z4o),e(N,V4o),e(N,jb),e(jb,Ore),e(Ore,W4o),e(jb,Q4o),e(jb,z$),e(z$,H4o),e(jb,U4o),e(N,J4o),e(N,Nb),e(Nb,Xre),e(Xre,Y4o),e(Nb,K4o),e(Nb,V$),e(V$,Z4o),e(Nb,eMo),e(N,oMo),e(N,Db),e(Db,zre),e(zre,rMo),e(Db,tMo),e(Db,W$),e(W$,aMo),e(Db,nMo),e(N,sMo),e(N,qb),e(qb,Vre),e(Vre,lMo),e(qb,iMo),e(qb,Q$),e(Q$,dMo),e(qb,cMo),e(N,fMo),e(N,Gb),e(Gb,Wre),e(Wre,mMo),e(Gb,gMo),e(Gb,H$),e(H$,hMo),e(Gb,pMo),e(N,_Mo),e(N,Ob),e(Ob,Qre),e(Qre,uMo),e(Ob,bMo),e(Ob,U$),e(U$,vMo),e(Ob,TMo),e(N,FMo),e(N,Xb),e(Xb,Hre),e(Hre,CMo),e(Xb,MMo),e(Xb,J$),e(J$,EMo),e(Xb,yMo),e(N,wMo),e(N,zb),e(zb,Ure),e(Ure,AMo),e(zb,LMo),e(zb,Y$),e(Y$,BMo),e(zb,xMo),e(N,kMo),e(N,Vb),e(Vb,Jre),e(Jre,RMo),e(Vb,SMo),e(Vb,K$),e(K$,PMo),e(Vb,$Mo),e(N,IMo),e(N,Wb),e(Wb,Yre),e(Yre,jMo),e(Wb,NMo),e(Wb,Z$),e(Z$,DMo),e(Wb,qMo),e(N,GMo),e(N,Qb),e(Qb,Kre),e(Kre,OMo),e(Qb,XMo),e(Qb,eI),e(eI,zMo),e(Qb,VMo),e(N,WMo),e(N,Hb),e(Hb,Zre),e(Zre,QMo),e(Hb,HMo),e(Hb,oI),e(oI,UMo),e(Hb,JMo),e(N,YMo),e(N,Ub),e(Ub,ete),e(ete,KMo),e(Ub,ZMo),e(Ub,rI),e(rI,eEo),e(Ub,oEo),e(N,rEo),e(N,Jb),e(Jb,ote),e(ote,tEo),e(Jb,aEo),e(Jb,tI),e(tI,nEo),e(Jb,sEo),e(N,lEo),e(N,Yb),e(Yb,rte),e(rte,iEo),e(Yb,dEo),e(Yb,aI),e(aI,cEo),e(Yb,fEo),e(N,mEo),e(N,Kb),e(Kb,tte),e(tte,gEo),e(Kb,hEo),e(Kb,nI),e(nI,pEo),e(Kb,_Eo),e(N,uEo),e(N,Zb),e(Zb,ate),e(ate,bEo),e(Zb,vEo),e(Zb,sI),e(sI,TEo),e(Zb,FEo),e(N,CEo),e(N,e5),e(e5,nte),e(nte,MEo),e(e5,EEo),e(e5,lI),e(lI,yEo),e(e5,wEo),e(N,AEo),e(N,o5),e(o5,ste),e(ste,LEo),e(o5,BEo),e(o5,iI),e(iI,xEo),e(o5,kEo),e(N,REo),e(N,r5),e(r5,lte),e(lte,SEo),e(r5,PEo),e(r5,dI),e(dI,$Eo),e(r5,IEo),e(N,jEo),e(N,t5),e(t5,ite),e(ite,NEo),e(t5,DEo),e(t5,cI),e(cI,qEo),e(t5,GEo),e(N,OEo),e(N,a5),e(a5,dte),e(dte,XEo),e(a5,zEo),e(a5,fI),e(fI,VEo),e(a5,WEo),e(N,QEo),e(N,n5),e(n5,cte),e(cte,HEo),e(n5,UEo),e(n5,mI),e(mI,JEo),e(n5,YEo),e(N,KEo),e(N,s5),e(s5,fte),e(fte,ZEo),e(s5,e3o),e(s5,gI),e(gI,o3o),e(s5,r3o),e(N,t3o),e(N,l5),e(l5,mte),e(mte,a3o),e(l5,n3o),e(l5,hI),e(hI,s3o),e(l5,l3o),e(Ne,i3o),e(Ne,i5),e(i5,d3o),e(i5,gte),e(gte,c3o),e(i5,f3o),e(i5,hte),e(hte,m3o),e(Ne,g3o),e(Ne,pte),e(pte,h3o),e(Ne,p3o),g(_M,Ne,null),b(d,UAe,u),b(d,sd,u),e(sd,d5),e(d5,_te),g(uM,_te,null),e(sd,_3o),e(sd,ute),e(ute,u3o),b(d,JAe,u),b(d,Zo,u),g(bM,Zo,null),e(Zo,b3o),e(Zo,ld),e(ld,v3o),e(ld,bte),e(bte,T3o),e(ld,F3o),e(ld,vte),e(vte,C3o),e(ld,M3o),e(Zo,E3o),e(Zo,vM),e(vM,y3o),e(vM,Tte),e(Tte,w3o),e(vM,A3o),e(Zo,L3o),e(Zo,Vr),g(TM,Vr,null),e(Vr,B3o),e(Vr,Fte),e(Fte,x3o),e(Vr,k3o),e(Vr,id),e(id,R3o),e(id,Cte),e(Cte,S3o),e(id,P3o),e(id,Mte),e(Mte,$3o),e(id,I3o),e(Vr,j3o),e(Vr,Ete),e(Ete,N3o),e(Vr,D3o),g(FM,Vr,null),e(Zo,q3o),e(Zo,De),g(CM,De,null),e(De,G3o),e(De,yte),e(yte,O3o),e(De,X3o),e(De,za),e(za,z3o),e(za,wte),e(wte,V3o),e(za,W3o),e(za,Ate),e(Ate,Q3o),e(za,H3o),e(za,Lte),e(Lte,U3o),e(za,J3o),e(De,Y3o),e(De,R),e(R,c5),e(c5,Bte),e(Bte,K3o),e(c5,Z3o),e(c5,pI),e(pI,eyo),e(c5,oyo),e(R,ryo),e(R,f5),e(f5,xte),e(xte,tyo),e(f5,ayo),e(f5,_I),e(_I,nyo),e(f5,syo),e(R,lyo),e(R,m5),e(m5,kte),e(kte,iyo),e(m5,dyo),e(m5,uI),e(uI,cyo),e(m5,fyo),e(R,myo),e(R,g5),e(g5,Rte),e(Rte,gyo),e(g5,hyo),e(g5,bI),e(bI,pyo),e(g5,_yo),e(R,uyo),e(R,h5),e(h5,Ste),e(Ste,byo),e(h5,vyo),e(h5,vI),e(vI,Tyo),e(h5,Fyo),e(R,Cyo),e(R,p5),e(p5,Pte),e(Pte,Myo),e(p5,Eyo),e(p5,TI),e(TI,yyo),e(p5,wyo),e(R,Ayo),e(R,_5),e(_5,$te),e($te,Lyo),e(_5,Byo),e(_5,FI),e(FI,xyo),e(_5,kyo),e(R,Ryo),e(R,u5),e(u5,Ite),e(Ite,Syo),e(u5,Pyo),e(u5,CI),e(CI,$yo),e(u5,Iyo),e(R,jyo),e(R,b5),e(b5,jte),e(jte,Nyo),e(b5,Dyo),e(b5,MI),e(MI,qyo),e(b5,Gyo),e(R,Oyo),e(R,v5),e(v5,Nte),e(Nte,Xyo),e(v5,zyo),e(v5,EI),e(EI,Vyo),e(v5,Wyo),e(R,Qyo),e(R,T5),e(T5,Dte),e(Dte,Hyo),e(T5,Uyo),e(T5,yI),e(yI,Jyo),e(T5,Yyo),e(R,Kyo),e(R,F5),e(F5,qte),e(qte,Zyo),e(F5,ewo),e(F5,wI),e(wI,owo),e(F5,rwo),e(R,two),e(R,C5),e(C5,Gte),e(Gte,awo),e(C5,nwo),e(C5,AI),e(AI,swo),e(C5,lwo),e(R,iwo),e(R,M5),e(M5,Ote),e(Ote,dwo),e(M5,cwo),e(M5,LI),e(LI,fwo),e(M5,mwo),e(R,gwo),e(R,E5),e(E5,Xte),e(Xte,hwo),e(E5,pwo),e(E5,BI),e(BI,_wo),e(E5,uwo),e(R,bwo),e(R,y5),e(y5,zte),e(zte,vwo),e(y5,Two),e(y5,xI),e(xI,Fwo),e(y5,Cwo),e(R,Mwo),e(R,w5),e(w5,Vte),e(Vte,Ewo),e(w5,ywo),e(w5,kI),e(kI,wwo),e(w5,Awo),e(R,Lwo),e(R,A5),e(A5,Wte),e(Wte,Bwo),e(A5,xwo),e(A5,RI),e(RI,kwo),e(A5,Rwo),e(R,Swo),e(R,L5),e(L5,Qte),e(Qte,Pwo),e(L5,$wo),e(L5,SI),e(SI,Iwo),e(L5,jwo),e(R,Nwo),e(R,B5),e(B5,Hte),e(Hte,Dwo),e(B5,qwo),e(B5,PI),e(PI,Gwo),e(B5,Owo),e(R,Xwo),e(R,x5),e(x5,Ute),e(Ute,zwo),e(x5,Vwo),e(x5,$I),e($I,Wwo),e(x5,Qwo),e(R,Hwo),e(R,k5),e(k5,Jte),e(Jte,Uwo),e(k5,Jwo),e(k5,II),e(II,Ywo),e(k5,Kwo),e(R,Zwo),e(R,R5),e(R5,Yte),e(Yte,eAo),e(R5,oAo),e(R5,jI),e(jI,rAo),e(R5,tAo),e(R,aAo),e(R,S5),e(S5,Kte),e(Kte,nAo),e(S5,sAo),e(S5,NI),e(NI,lAo),e(S5,iAo),e(R,dAo),e(R,P5),e(P5,Zte),e(Zte,cAo),e(P5,fAo),e(P5,DI),e(DI,mAo),e(P5,gAo),e(R,hAo),e(R,$5),e($5,eae),e(eae,pAo),e($5,_Ao),e($5,qI),e(qI,uAo),e($5,bAo),e(R,vAo),e(R,I5),e(I5,oae),e(oae,TAo),e(I5,FAo),e(I5,GI),e(GI,CAo),e(I5,MAo),e(R,EAo),e(R,j5),e(j5,rae),e(rae,yAo),e(j5,wAo),e(j5,OI),e(OI,AAo),e(j5,LAo),e(R,BAo),e(R,N5),e(N5,tae),e(tae,xAo),e(N5,kAo),e(N5,XI),e(XI,RAo),e(N5,SAo),e(R,PAo),e(R,D5),e(D5,aae),e(aae,$Ao),e(D5,IAo),e(D5,zI),e(zI,jAo),e(D5,NAo),e(R,DAo),e(R,q5),e(q5,nae),e(nae,qAo),e(q5,GAo),e(q5,VI),e(VI,OAo),e(q5,XAo),e(R,zAo),e(R,G5),e(G5,sae),e(sae,VAo),e(G5,WAo),e(G5,WI),e(WI,QAo),e(G5,HAo),e(R,UAo),e(R,O5),e(O5,lae),e(lae,JAo),e(O5,YAo),e(O5,QI),e(QI,KAo),e(O5,ZAo),e(R,e0o),e(R,X5),e(X5,iae),e(iae,o0o),e(X5,r0o),e(X5,HI),e(HI,t0o),e(X5,a0o),e(R,n0o),e(R,z5),e(z5,dae),e(dae,s0o),e(z5,l0o),e(z5,UI),e(UI,i0o),e(z5,d0o),e(R,c0o),e(R,V5),e(V5,cae),e(cae,f0o),e(V5,m0o),e(V5,JI),e(JI,g0o),e(V5,h0o),e(R,p0o),e(R,W5),e(W5,fae),e(fae,_0o),e(W5,u0o),e(W5,YI),e(YI,b0o),e(W5,v0o),e(R,T0o),e(R,Q5),e(Q5,mae),e(mae,F0o),e(Q5,C0o),e(Q5,KI),e(KI,M0o),e(Q5,E0o),e(De,y0o),e(De,H5),e(H5,w0o),e(H5,gae),e(gae,A0o),e(H5,L0o),e(H5,hae),e(hae,B0o),e(De,x0o),e(De,pae),e(pae,k0o),e(De,R0o),g(MM,De,null),b(d,YAe,u),b(d,dd,u),e(dd,U5),e(U5,_ae),g(EM,_ae,null),e(dd,S0o),e(dd,uae),e(uae,P0o),b(d,KAe,u),b(d,er,u),g(yM,er,null),e(er,$0o),e(er,cd),e(cd,I0o),e(cd,bae),e(bae,j0o),e(cd,N0o),e(cd,vae),e(vae,D0o),e(cd,q0o),e(er,G0o),e(er,wM),e(wM,O0o),e(wM,Tae),e(Tae,X0o),e(wM,z0o),e(er,V0o),e(er,Wr),g(AM,Wr,null),e(Wr,W0o),e(Wr,Fae),e(Fae,Q0o),e(Wr,H0o),e(Wr,fd),e(fd,U0o),e(fd,Cae),e(Cae,J0o),e(fd,Y0o),e(fd,Mae),e(Mae,K0o),e(fd,Z0o),e(Wr,eLo),e(Wr,Eae),e(Eae,oLo),e(Wr,rLo),g(LM,Wr,null),e(er,tLo),e(er,qe),g(BM,qe,null),e(qe,aLo),e(qe,yae),e(yae,nLo),e(qe,sLo),e(qe,Va),e(Va,lLo),e(Va,wae),e(wae,iLo),e(Va,dLo),e(Va,Aae),e(Aae,cLo),e(Va,fLo),e(Va,Lae),e(Lae,mLo),e(Va,gLo),e(qe,hLo),e(qe,Bae),e(Bae,J5),e(J5,xae),e(xae,pLo),e(J5,_Lo),e(J5,ZI),e(ZI,uLo),e(J5,bLo),e(qe,vLo),e(qe,Y5),e(Y5,TLo),e(Y5,kae),e(kae,FLo),e(Y5,CLo),e(Y5,Rae),e(Rae,MLo),e(qe,ELo),e(qe,Sae),e(Sae,yLo),e(qe,wLo),g(xM,qe,null),b(d,ZAe,u),b(d,md,u),e(md,K5),e(K5,Pae),g(kM,Pae,null),e(md,ALo),e(md,$ae),e($ae,LLo),b(d,e0e,u),b(d,or,u),g(RM,or,null),e(or,BLo),e(or,gd),e(gd,xLo),e(gd,Iae),e(Iae,kLo),e(gd,RLo),e(gd,jae),e(jae,SLo),e(gd,PLo),e(or,$Lo),e(or,SM),e(SM,ILo),e(SM,Nae),e(Nae,jLo),e(SM,NLo),e(or,DLo),e(or,Qr),g(PM,Qr,null),e(Qr,qLo),e(Qr,Dae),e(Dae,GLo),e(Qr,OLo),e(Qr,hd),e(hd,XLo),e(hd,qae),e(qae,zLo),e(hd,VLo),e(hd,Gae),e(Gae,WLo),e(hd,QLo),e(Qr,HLo),e(Qr,Oae),e(Oae,ULo),e(Qr,JLo),g($M,Qr,null),e(or,YLo),e(or,Ge),g(IM,Ge,null),e(Ge,KLo),e(Ge,Xae),e(Xae,ZLo),e(Ge,e9o),e(Ge,Wa),e(Wa,o9o),e(Wa,zae),e(zae,r9o),e(Wa,t9o),e(Wa,Vae),e(Vae,a9o),e(Wa,n9o),e(Wa,Wae),e(Wae,s9o),e(Wa,l9o),e(Ge,i9o),e(Ge,we),e(we,Z5),e(Z5,Qae),e(Qae,d9o),e(Z5,c9o),e(Z5,ej),e(ej,f9o),e(Z5,m9o),e(we,g9o),e(we,e2),e(e2,Hae),e(Hae,h9o),e(e2,p9o),e(e2,oj),e(oj,_9o),e(e2,u9o),e(we,b9o),e(we,As),e(As,Uae),e(Uae,v9o),e(As,T9o),e(As,rj),e(rj,F9o),e(As,C9o),e(As,tj),e(tj,M9o),e(As,E9o),e(we,y9o),e(we,o2),e(o2,Jae),e(Jae,w9o),e(o2,A9o),e(o2,aj),e(aj,L9o),e(o2,B9o),e(we,x9o),e(we,ta),e(ta,Yae),e(Yae,k9o),e(ta,R9o),e(ta,nj),e(nj,S9o),e(ta,P9o),e(ta,sj),e(sj,$9o),e(ta,I9o),e(ta,lj),e(lj,j9o),e(ta,N9o),e(we,D9o),e(we,r2),e(r2,Kae),e(Kae,q9o),e(r2,G9o),e(r2,ij),e(ij,O9o),e(r2,X9o),e(we,z9o),e(we,t2),e(t2,Zae),e(Zae,V9o),e(t2,W9o),e(t2,dj),e(dj,Q9o),e(t2,H9o),e(we,U9o),e(we,a2),e(a2,ene),e(ene,J9o),e(a2,Y9o),e(a2,cj),e(cj,K9o),e(a2,Z9o),e(Ge,eBo),e(Ge,n2),e(n2,oBo),e(n2,one),e(one,rBo),e(n2,tBo),e(n2,rne),e(rne,aBo),e(Ge,nBo),e(Ge,tne),e(tne,sBo),e(Ge,lBo),g(jM,Ge,null),b(d,o0e,u),b(d,pd,u),e(pd,s2),e(s2,ane),g(NM,ane,null),e(pd,iBo),e(pd,nne),e(nne,dBo),b(d,r0e,u),b(d,rr,u),g(DM,rr,null),e(rr,cBo),e(rr,_d),e(_d,fBo),e(_d,sne),e(sne,mBo),e(_d,gBo),e(_d,lne),e(lne,hBo),e(_d,pBo),e(rr,_Bo),e(rr,qM),e(qM,uBo),e(qM,ine),e(ine,bBo),e(qM,vBo),e(rr,TBo),e(rr,Hr),g(GM,Hr,null),e(Hr,FBo),e(Hr,dne),e(dne,CBo),e(Hr,MBo),e(Hr,ud),e(ud,EBo),e(ud,cne),e(cne,yBo),e(ud,wBo),e(ud,fne),e(fne,ABo),e(ud,LBo),e(Hr,BBo),e(Hr,mne),e(mne,xBo),e(Hr,kBo),g(OM,Hr,null),e(rr,RBo),e(rr,Oe),g(XM,Oe,null),e(Oe,SBo),e(Oe,gne),e(gne,PBo),e(Oe,$Bo),e(Oe,Qa),e(Qa,IBo),e(Qa,hne),e(hne,jBo),e(Qa,NBo),e(Qa,pne),e(pne,DBo),e(Qa,qBo),e(Qa,_ne),e(_ne,GBo),e(Qa,OBo),e(Oe,XBo),e(Oe,une),e(une,l2),e(l2,bne),e(bne,zBo),e(l2,VBo),e(l2,fj),e(fj,WBo),e(l2,QBo),e(Oe,HBo),e(Oe,i2),e(i2,UBo),e(i2,vne),e(vne,JBo),e(i2,YBo),e(i2,Tne),e(Tne,KBo),e(Oe,ZBo),e(Oe,Fne),e(Fne,exo),e(Oe,oxo),g(zM,Oe,null),b(d,t0e,u),b(d,bd,u),e(bd,d2),e(d2,Cne),g(VM,Cne,null),e(bd,rxo),e(bd,Mne),e(Mne,txo),b(d,a0e,u),b(d,tr,u),g(WM,tr,null),e(tr,axo),e(tr,vd),e(vd,nxo),e(vd,Ene),e(Ene,sxo),e(vd,lxo),e(vd,yne),e(yne,ixo),e(vd,dxo),e(tr,cxo),e(tr,QM),e(QM,fxo),e(QM,wne),e(wne,mxo),e(QM,gxo),e(tr,hxo),e(tr,Ur),g(HM,Ur,null),e(Ur,pxo),e(Ur,Ane),e(Ane,_xo),e(Ur,uxo),e(Ur,Td),e(Td,bxo),e(Td,Lne),e(Lne,vxo),e(Td,Txo),e(Td,Bne),e(Bne,Fxo),e(Td,Cxo),e(Ur,Mxo),e(Ur,xne),e(xne,Exo),e(Ur,yxo),g(UM,Ur,null),e(tr,wxo),e(tr,Xe),g(JM,Xe,null),e(Xe,Axo),e(Xe,kne),e(kne,Lxo),e(Xe,Bxo),e(Xe,Ha),e(Ha,xxo),e(Ha,Rne),e(Rne,kxo),e(Ha,Rxo),e(Ha,Sne),e(Sne,Sxo),e(Ha,Pxo),e(Ha,Pne),e(Pne,$xo),e(Ha,Ixo),e(Xe,jxo),e(Xe,ro),e(ro,c2),e(c2,$ne),e($ne,Nxo),e(c2,Dxo),e(c2,mj),e(mj,qxo),e(c2,Gxo),e(ro,Oxo),e(ro,f2),e(f2,Ine),e(Ine,Xxo),e(f2,zxo),e(f2,gj),e(gj,Vxo),e(f2,Wxo),e(ro,Qxo),e(ro,m2),e(m2,jne),e(jne,Hxo),e(m2,Uxo),e(m2,hj),e(hj,Jxo),e(m2,Yxo),e(ro,Kxo),e(ro,g2),e(g2,Nne),e(Nne,Zxo),e(g2,eko),e(g2,pj),e(pj,oko),e(g2,rko),e(ro,tko),e(ro,h2),e(h2,Dne),e(Dne,ako),e(h2,nko),e(h2,_j),e(_j,sko),e(h2,lko),e(ro,iko),e(ro,p2),e(p2,qne),e(qne,dko),e(p2,cko),e(p2,uj),e(uj,fko),e(p2,mko),e(ro,gko),e(ro,_2),e(_2,Gne),e(Gne,hko),e(_2,pko),e(_2,bj),e(bj,_ko),e(_2,uko),e(Xe,bko),e(Xe,u2),e(u2,vko),e(u2,One),e(One,Tko),e(u2,Fko),e(u2,Xne),e(Xne,Cko),e(Xe,Mko),e(Xe,zne),e(zne,Eko),e(Xe,yko),g(YM,Xe,null),b(d,n0e,u),b(d,Fd,u),e(Fd,b2),e(b2,Vne),g(KM,Vne,null),e(Fd,wko),e(Fd,Wne),e(Wne,Ako),b(d,s0e,u),b(d,ar,u),g(ZM,ar,null),e(ar,Lko),e(ar,Cd),e(Cd,Bko),e(Cd,Qne),e(Qne,xko),e(Cd,kko),e(Cd,Hne),e(Hne,Rko),e(Cd,Sko),e(ar,Pko),e(ar,eE),e(eE,$ko),e(eE,Une),e(Une,Iko),e(eE,jko),e(ar,Nko),e(ar,Jr),g(oE,Jr,null),e(Jr,Dko),e(Jr,Jne),e(Jne,qko),e(Jr,Gko),e(Jr,Md),e(Md,Oko),e(Md,Yne),e(Yne,Xko),e(Md,zko),e(Md,Kne),e(Kne,Vko),e(Md,Wko),e(Jr,Qko),e(Jr,Zne),e(Zne,Hko),e(Jr,Uko),g(rE,Jr,null),e(ar,Jko),e(ar,ze),g(tE,ze,null),e(ze,Yko),e(ze,ese),e(ese,Kko),e(ze,Zko),e(ze,Ua),e(Ua,eRo),e(Ua,ose),e(ose,oRo),e(Ua,rRo),e(Ua,rse),e(rse,tRo),e(Ua,aRo),e(Ua,tse),e(tse,nRo),e(Ua,sRo),e(ze,lRo),e(ze,Ed),e(Ed,v2),e(v2,ase),e(ase,iRo),e(v2,dRo),e(v2,vj),e(vj,cRo),e(v2,fRo),e(Ed,mRo),e(Ed,T2),e(T2,nse),e(nse,gRo),e(T2,hRo),e(T2,Tj),e(Tj,pRo),e(T2,_Ro),e(Ed,uRo),e(Ed,F2),e(F2,sse),e(sse,bRo),e(F2,vRo),e(F2,Fj),e(Fj,TRo),e(F2,FRo),e(ze,CRo),e(ze,C2),e(C2,MRo),e(C2,lse),e(lse,ERo),e(C2,yRo),e(C2,ise),e(ise,wRo),e(ze,ARo),e(ze,dse),e(dse,LRo),e(ze,BRo),g(aE,ze,null),b(d,l0e,u),b(d,yd,u),e(yd,M2),e(M2,cse),g(nE,cse,null),e(yd,xRo),e(yd,fse),e(fse,kRo),b(d,i0e,u),b(d,nr,u),g(sE,nr,null),e(nr,RRo),e(nr,wd),e(wd,SRo),e(wd,mse),e(mse,PRo),e(wd,$Ro),e(wd,gse),e(gse,IRo),e(wd,jRo),e(nr,NRo),e(nr,lE),e(lE,DRo),e(lE,hse),e(hse,qRo),e(lE,GRo),e(nr,ORo),e(nr,Yr),g(iE,Yr,null),e(Yr,XRo),e(Yr,pse),e(pse,zRo),e(Yr,VRo),e(Yr,Ad),e(Ad,WRo),e(Ad,_se),e(_se,QRo),e(Ad,HRo),e(Ad,use),e(use,URo),e(Ad,JRo),e(Yr,YRo),e(Yr,bse),e(bse,KRo),e(Yr,ZRo),g(dE,Yr,null),e(nr,eSo),e(nr,Ve),g(cE,Ve,null),e(Ve,oSo),e(Ve,vse),e(vse,rSo),e(Ve,tSo),e(Ve,Ja),e(Ja,aSo),e(Ja,Tse),e(Tse,nSo),e(Ja,sSo),e(Ja,Fse),e(Fse,lSo),e(Ja,iSo),e(Ja,Cse),e(Cse,dSo),e(Ja,cSo),e(Ve,fSo),e(Ve,to),e(to,E2),e(E2,Mse),e(Mse,mSo),e(E2,gSo),e(E2,Cj),e(Cj,hSo),e(E2,pSo),e(to,_So),e(to,y2),e(y2,Ese),e(Ese,uSo),e(y2,bSo),e(y2,Mj),e(Mj,vSo),e(y2,TSo),e(to,FSo),e(to,w2),e(w2,yse),e(yse,CSo),e(w2,MSo),e(w2,Ej),e(Ej,ESo),e(w2,ySo),e(to,wSo),e(to,A2),e(A2,wse),e(wse,ASo),e(A2,LSo),e(A2,yj),e(yj,BSo),e(A2,xSo),e(to,kSo),e(to,L2),e(L2,Ase),e(Ase,RSo),e(L2,SSo),e(L2,wj),e(wj,PSo),e(L2,$So),e(to,ISo),e(to,B2),e(B2,Lse),e(Lse,jSo),e(B2,NSo),e(B2,Aj),e(Aj,DSo),e(B2,qSo),e(to,GSo),e(to,x2),e(x2,Bse),e(Bse,OSo),e(x2,XSo),e(x2,Lj),e(Lj,zSo),e(x2,VSo),e(Ve,WSo),e(Ve,k2),e(k2,QSo),e(k2,xse),e(xse,HSo),e(k2,USo),e(k2,kse),e(kse,JSo),e(Ve,YSo),e(Ve,Rse),e(Rse,KSo),e(Ve,ZSo),g(fE,Ve,null),b(d,d0e,u),b(d,Ld,u),e(Ld,R2),e(R2,Sse),g(mE,Sse,null),e(Ld,ePo),e(Ld,Pse),e(Pse,oPo),b(d,c0e,u),b(d,sr,u),g(gE,sr,null),e(sr,rPo),e(sr,Bd),e(Bd,tPo),e(Bd,$se),e($se,aPo),e(Bd,nPo),e(Bd,Ise),e(Ise,sPo),e(Bd,lPo),e(sr,iPo),e(sr,hE),e(hE,dPo),e(hE,jse),e(jse,cPo),e(hE,fPo),e(sr,mPo),e(sr,Kr),g(pE,Kr,null),e(Kr,gPo),e(Kr,Nse),e(Nse,hPo),e(Kr,pPo),e(Kr,xd),e(xd,_Po),e(xd,Dse),e(Dse,uPo),e(xd,bPo),e(xd,qse),e(qse,vPo),e(xd,TPo),e(Kr,FPo),e(Kr,Gse),e(Gse,CPo),e(Kr,MPo),g(_E,Kr,null),e(sr,EPo),e(sr,We),g(uE,We,null),e(We,yPo),e(We,Ose),e(Ose,wPo),e(We,APo),e(We,Ya),e(Ya,LPo),e(Ya,Xse),e(Xse,BPo),e(Ya,xPo),e(Ya,zse),e(zse,kPo),e(Ya,RPo),e(Ya,Vse),e(Vse,SPo),e(Ya,PPo),e(We,$Po),e(We,bE),e(bE,S2),e(S2,Wse),e(Wse,IPo),e(S2,jPo),e(S2,Bj),e(Bj,NPo),e(S2,DPo),e(bE,qPo),e(bE,P2),e(P2,Qse),e(Qse,GPo),e(P2,OPo),e(P2,xj),e(xj,XPo),e(P2,zPo),e(We,VPo),e(We,$2),e($2,WPo),e($2,Hse),e(Hse,QPo),e($2,HPo),e($2,Use),e(Use,UPo),e(We,JPo),e(We,Jse),e(Jse,YPo),e(We,KPo),g(vE,We,null),b(d,f0e,u),b(d,kd,u),e(kd,I2),e(I2,Yse),g(TE,Yse,null),e(kd,ZPo),e(kd,Kse),e(Kse,e$o),b(d,m0e,u),b(d,lr,u),g(FE,lr,null),e(lr,o$o),e(lr,Rd),e(Rd,r$o),e(Rd,Zse),e(Zse,t$o),e(Rd,a$o),e(Rd,ele),e(ele,n$o),e(Rd,s$o),e(lr,l$o),e(lr,CE),e(CE,i$o),e(CE,ole),e(ole,d$o),e(CE,c$o),e(lr,f$o),e(lr,Zr),g(ME,Zr,null),e(Zr,m$o),e(Zr,rle),e(rle,g$o),e(Zr,h$o),e(Zr,Sd),e(Sd,p$o),e(Sd,tle),e(tle,_$o),e(Sd,u$o),e(Sd,ale),e(ale,b$o),e(Sd,v$o),e(Zr,T$o),e(Zr,nle),e(nle,F$o),e(Zr,C$o),g(EE,Zr,null),e(lr,M$o),e(lr,Qe),g(yE,Qe,null),e(Qe,E$o),e(Qe,sle),e(sle,y$o),e(Qe,w$o),e(Qe,Ka),e(Ka,A$o),e(Ka,lle),e(lle,L$o),e(Ka,B$o),e(Ka,ile),e(ile,x$o),e(Ka,k$o),e(Ka,dle),e(dle,R$o),e(Ka,S$o),e(Qe,P$o),e(Qe,Pd),e(Pd,j2),e(j2,cle),e(cle,$$o),e(j2,I$o),e(j2,kj),e(kj,j$o),e(j2,N$o),e(Pd,D$o),e(Pd,N2),e(N2,fle),e(fle,q$o),e(N2,G$o),e(N2,Rj),e(Rj,O$o),e(N2,X$o),e(Pd,z$o),e(Pd,D2),e(D2,mle),e(mle,V$o),e(D2,W$o),e(D2,Sj),e(Sj,Q$o),e(D2,H$o),e(Qe,U$o),e(Qe,q2),e(q2,J$o),e(q2,gle),e(gle,Y$o),e(q2,K$o),e(q2,hle),e(hle,Z$o),e(Qe,eIo),e(Qe,ple),e(ple,oIo),e(Qe,rIo),g(wE,Qe,null),b(d,g0e,u),b(d,$d,u),e($d,G2),e(G2,_le),g(AE,_le,null),e($d,tIo),e($d,ule),e(ule,aIo),b(d,h0e,u),b(d,ir,u),g(LE,ir,null),e(ir,nIo),e(ir,Id),e(Id,sIo),e(Id,ble),e(ble,lIo),e(Id,iIo),e(Id,vle),e(vle,dIo),e(Id,cIo),e(ir,fIo),e(ir,BE),e(BE,mIo),e(BE,Tle),e(Tle,gIo),e(BE,hIo),e(ir,pIo),e(ir,et),g(xE,et,null),e(et,_Io),e(et,Fle),e(Fle,uIo),e(et,bIo),e(et,jd),e(jd,vIo),e(jd,Cle),e(Cle,TIo),e(jd,FIo),e(jd,Mle),e(Mle,CIo),e(jd,MIo),e(et,EIo),e(et,Ele),e(Ele,yIo),e(et,wIo),g(kE,et,null),e(ir,AIo),e(ir,He),g(RE,He,null),e(He,LIo),e(He,yle),e(yle,BIo),e(He,xIo),e(He,Za),e(Za,kIo),e(Za,wle),e(wle,RIo),e(Za,SIo),e(Za,Ale),e(Ale,PIo),e(Za,$Io),e(Za,Lle),e(Lle,IIo),e(Za,jIo),e(He,NIo),e(He,Ble),e(Ble,O2),e(O2,xle),e(xle,DIo),e(O2,qIo),e(O2,Pj),e(Pj,GIo),e(O2,OIo),e(He,XIo),e(He,X2),e(X2,zIo),e(X2,kle),e(kle,VIo),e(X2,WIo),e(X2,Rle),e(Rle,QIo),e(He,HIo),e(He,Sle),e(Sle,UIo),e(He,JIo),g(SE,He,null),b(d,p0e,u),b(d,Nd,u),e(Nd,z2),e(z2,Ple),g(PE,Ple,null),e(Nd,YIo),e(Nd,$le),e($le,KIo),b(d,_0e,u),b(d,dr,u),g($E,dr,null),e(dr,ZIo),e(dr,Dd),e(Dd,ejo),e(Dd,Ile),e(Ile,ojo),e(Dd,rjo),e(Dd,jle),e(jle,tjo),e(Dd,ajo),e(dr,njo),e(dr,IE),e(IE,sjo),e(IE,Nle),e(Nle,ljo),e(IE,ijo),e(dr,djo),e(dr,ot),g(jE,ot,null),e(ot,cjo),e(ot,Dle),e(Dle,fjo),e(ot,mjo),e(ot,qd),e(qd,gjo),e(qd,qle),e(qle,hjo),e(qd,pjo),e(qd,Gle),e(Gle,_jo),e(qd,ujo),e(ot,bjo),e(ot,Ole),e(Ole,vjo),e(ot,Tjo),g(NE,ot,null),e(dr,Fjo),e(dr,Ue),g(DE,Ue,null),e(Ue,Cjo),e(Ue,Xle),e(Xle,Mjo),e(Ue,Ejo),e(Ue,en),e(en,yjo),e(en,zle),e(zle,wjo),e(en,Ajo),e(en,Vle),e(Vle,Ljo),e(en,Bjo),e(en,Wle),e(Wle,xjo),e(en,kjo),e(Ue,Rjo),e(Ue,Qle),e(Qle,V2),e(V2,Hle),e(Hle,Sjo),e(V2,Pjo),e(V2,$j),e($j,$jo),e(V2,Ijo),e(Ue,jjo),e(Ue,W2),e(W2,Njo),e(W2,Ule),e(Ule,Djo),e(W2,qjo),e(W2,Jle),e(Jle,Gjo),e(Ue,Ojo),e(Ue,Yle),e(Yle,Xjo),e(Ue,zjo),g(qE,Ue,null),b(d,u0e,u),b(d,Gd,u),e(Gd,Q2),e(Q2,Kle),g(GE,Kle,null),e(Gd,Vjo),e(Gd,Zle),e(Zle,Wjo),b(d,b0e,u),b(d,cr,u),g(OE,cr,null),e(cr,Qjo),e(cr,Od),e(Od,Hjo),e(Od,eie),e(eie,Ujo),e(Od,Jjo),e(Od,oie),e(oie,Yjo),e(Od,Kjo),e(cr,Zjo),e(cr,XE),e(XE,eNo),e(XE,rie),e(rie,oNo),e(XE,rNo),e(cr,tNo),e(cr,rt),g(zE,rt,null),e(rt,aNo),e(rt,tie),e(tie,nNo),e(rt,sNo),e(rt,Xd),e(Xd,lNo),e(Xd,aie),e(aie,iNo),e(Xd,dNo),e(Xd,nie),e(nie,cNo),e(Xd,fNo),e(rt,mNo),e(rt,sie),e(sie,gNo),e(rt,hNo),g(VE,rt,null),e(cr,pNo),e(cr,Je),g(WE,Je,null),e(Je,_No),e(Je,lie),e(lie,uNo),e(Je,bNo),e(Je,on),e(on,vNo),e(on,iie),e(iie,TNo),e(on,FNo),e(on,die),e(die,CNo),e(on,MNo),e(on,cie),e(cie,ENo),e(on,yNo),e(Je,wNo),e(Je,QE),e(QE,H2),e(H2,fie),e(fie,ANo),e(H2,LNo),e(H2,Ij),e(Ij,BNo),e(H2,xNo),e(QE,kNo),e(QE,U2),e(U2,mie),e(mie,RNo),e(U2,SNo),e(U2,jj),e(jj,PNo),e(U2,$No),e(Je,INo),e(Je,J2),e(J2,jNo),e(J2,gie),e(gie,NNo),e(J2,DNo),e(J2,hie),e(hie,qNo),e(Je,GNo),e(Je,pie),e(pie,ONo),e(Je,XNo),g(HE,Je,null),b(d,v0e,u),b(d,zd,u),e(zd,Y2),e(Y2,_ie),g(UE,_ie,null),e(zd,zNo),e(zd,uie),e(uie,VNo),b(d,T0e,u),b(d,fr,u),g(JE,fr,null),e(fr,WNo),e(fr,Vd),e(Vd,QNo),e(Vd,bie),e(bie,HNo),e(Vd,UNo),e(Vd,vie),e(vie,JNo),e(Vd,YNo),e(fr,KNo),e(fr,YE),e(YE,ZNo),e(YE,Tie),e(Tie,eDo),e(YE,oDo),e(fr,rDo),e(fr,tt),g(KE,tt,null),e(tt,tDo),e(tt,Fie),e(Fie,aDo),e(tt,nDo),e(tt,Wd),e(Wd,sDo),e(Wd,Cie),e(Cie,lDo),e(Wd,iDo),e(Wd,Mie),e(Mie,dDo),e(Wd,cDo),e(tt,fDo),e(tt,Eie),e(Eie,mDo),e(tt,gDo),g(ZE,tt,null),e(fr,hDo),e(fr,fo),g(e3,fo,null),e(fo,pDo),e(fo,yie),e(yie,_Do),e(fo,uDo),e(fo,rn),e(rn,bDo),e(rn,wie),e(wie,vDo),e(rn,TDo),e(rn,Aie),e(Aie,FDo),e(rn,CDo),e(rn,Lie),e(Lie,MDo),e(rn,EDo),e(fo,yDo),e(fo,B),e(B,K2),e(K2,Bie),e(Bie,wDo),e(K2,ADo),e(K2,Nj),e(Nj,LDo),e(K2,BDo),e(B,xDo),e(B,Z2),e(Z2,xie),e(xie,kDo),e(Z2,RDo),e(Z2,Dj),e(Dj,SDo),e(Z2,PDo),e(B,$Do),e(B,ev),e(ev,kie),e(kie,IDo),e(ev,jDo),e(ev,qj),e(qj,NDo),e(ev,DDo),e(B,qDo),e(B,ov),e(ov,Rie),e(Rie,GDo),e(ov,ODo),e(ov,Gj),e(Gj,XDo),e(ov,zDo),e(B,VDo),e(B,rv),e(rv,Sie),e(Sie,WDo),e(rv,QDo),e(rv,Oj),e(Oj,HDo),e(rv,UDo),e(B,JDo),e(B,tv),e(tv,Pie),e(Pie,YDo),e(tv,KDo),e(tv,Xj),e(Xj,ZDo),e(tv,eqo),e(B,oqo),e(B,av),e(av,$ie),e($ie,rqo),e(av,tqo),e(av,zj),e(zj,aqo),e(av,nqo),e(B,sqo),e(B,nv),e(nv,Iie),e(Iie,lqo),e(nv,iqo),e(nv,Vj),e(Vj,dqo),e(nv,cqo),e(B,fqo),e(B,sv),e(sv,jie),e(jie,mqo),e(sv,gqo),e(sv,Wj),e(Wj,hqo),e(sv,pqo),e(B,_qo),e(B,lv),e(lv,Nie),e(Nie,uqo),e(lv,bqo),e(lv,Qj),e(Qj,vqo),e(lv,Tqo),e(B,Fqo),e(B,iv),e(iv,Die),e(Die,Cqo),e(iv,Mqo),e(iv,Hj),e(Hj,Eqo),e(iv,yqo),e(B,wqo),e(B,dv),e(dv,qie),e(qie,Aqo),e(dv,Lqo),e(dv,Uj),e(Uj,Bqo),e(dv,xqo),e(B,kqo),e(B,cv),e(cv,Gie),e(Gie,Rqo),e(cv,Sqo),e(cv,Jj),e(Jj,Pqo),e(cv,$qo),e(B,Iqo),e(B,fv),e(fv,Oie),e(Oie,jqo),e(fv,Nqo),e(fv,Yj),e(Yj,Dqo),e(fv,qqo),e(B,Gqo),e(B,mv),e(mv,Xie),e(Xie,Oqo),e(mv,Xqo),e(mv,Kj),e(Kj,zqo),e(mv,Vqo),e(B,Wqo),e(B,Ls),e(Ls,zie),e(zie,Qqo),e(Ls,Hqo),e(Ls,Zj),e(Zj,Uqo),e(Ls,Jqo),e(Ls,eN),e(eN,Yqo),e(Ls,Kqo),e(B,Zqo),e(B,gv),e(gv,Vie),e(Vie,eGo),e(gv,oGo),e(gv,oN),e(oN,rGo),e(gv,tGo),e(B,aGo),e(B,hv),e(hv,Wie),e(Wie,nGo),e(hv,sGo),e(hv,rN),e(rN,lGo),e(hv,iGo),e(B,dGo),e(B,pv),e(pv,Qie),e(Qie,cGo),e(pv,fGo),e(pv,tN),e(tN,mGo),e(pv,gGo),e(B,hGo),e(B,_v),e(_v,Hie),e(Hie,pGo),e(_v,_Go),e(_v,aN),e(aN,uGo),e(_v,bGo),e(B,vGo),e(B,uv),e(uv,Uie),e(Uie,TGo),e(uv,FGo),e(uv,nN),e(nN,CGo),e(uv,MGo),e(B,EGo),e(B,bv),e(bv,Jie),e(Jie,yGo),e(bv,wGo),e(bv,sN),e(sN,AGo),e(bv,LGo),e(B,BGo),e(B,vv),e(vv,Yie),e(Yie,xGo),e(vv,kGo),e(vv,lN),e(lN,RGo),e(vv,SGo),e(B,PGo),e(B,Tv),e(Tv,Kie),e(Kie,$Go),e(Tv,IGo),e(Tv,iN),e(iN,jGo),e(Tv,NGo),e(B,DGo),e(B,Fv),e(Fv,Zie),e(Zie,qGo),e(Fv,GGo),e(Fv,dN),e(dN,OGo),e(Fv,XGo),e(B,zGo),e(B,Cv),e(Cv,ede),e(ede,VGo),e(Cv,WGo),e(Cv,cN),e(cN,QGo),e(Cv,HGo),e(B,UGo),e(B,Mv),e(Mv,ode),e(ode,JGo),e(Mv,YGo),e(Mv,fN),e(fN,KGo),e(Mv,ZGo),e(B,eOo),e(B,Ev),e(Ev,rde),e(rde,oOo),e(Ev,rOo),e(Ev,mN),e(mN,tOo),e(Ev,aOo),e(B,nOo),e(B,yv),e(yv,tde),e(tde,sOo),e(yv,lOo),e(yv,gN),e(gN,iOo),e(yv,dOo),e(B,cOo),e(B,wv),e(wv,ade),e(ade,fOo),e(wv,mOo),e(wv,hN),e(hN,gOo),e(wv,hOo),e(B,pOo),e(B,Av),e(Av,nde),e(nde,_Oo),e(Av,uOo),e(Av,pN),e(pN,bOo),e(Av,vOo),e(B,TOo),e(B,Lv),e(Lv,sde),e(sde,FOo),e(Lv,COo),e(Lv,_N),e(_N,MOo),e(Lv,EOo),e(B,yOo),e(B,Bv),e(Bv,lde),e(lde,wOo),e(Bv,AOo),e(Bv,uN),e(uN,LOo),e(Bv,BOo),e(B,xOo),e(B,xv),e(xv,ide),e(ide,kOo),e(xv,ROo),e(xv,bN),e(bN,SOo),e(xv,POo),e(B,$Oo),e(B,kv),e(kv,dde),e(dde,IOo),e(kv,jOo),e(kv,vN),e(vN,NOo),e(kv,DOo),e(B,qOo),e(B,Rv),e(Rv,cde),e(cde,GOo),e(Rv,OOo),e(Rv,TN),e(TN,XOo),e(Rv,zOo),e(B,VOo),e(B,Sv),e(Sv,fde),e(fde,WOo),e(Sv,QOo),e(Sv,FN),e(FN,HOo),e(Sv,UOo),e(B,JOo),e(B,Pv),e(Pv,mde),e(mde,YOo),e(Pv,KOo),e(Pv,CN),e(CN,ZOo),e(Pv,eXo),e(B,oXo),e(B,$v),e($v,gde),e(gde,rXo),e($v,tXo),e($v,MN),e(MN,aXo),e($v,nXo),e(B,sXo),e(B,Iv),e(Iv,hde),e(hde,lXo),e(Iv,iXo),e(Iv,EN),e(EN,dXo),e(Iv,cXo),e(B,fXo),e(B,jv),e(jv,pde),e(pde,mXo),e(jv,gXo),e(jv,yN),e(yN,hXo),e(jv,pXo),e(fo,_Xo),e(fo,_de),e(_de,uXo),e(fo,bXo),g(o3,fo,null),b(d,F0e,u),b(d,Qd,u),e(Qd,Nv),e(Nv,ude),g(r3,ude,null),e(Qd,vXo),e(Qd,bde),e(bde,TXo),b(d,C0e,u),b(d,mr,u),g(t3,mr,null),e(mr,FXo),e(mr,Hd),e(Hd,CXo),e(Hd,vde),e(vde,MXo),e(Hd,EXo),e(Hd,Tde),e(Tde,yXo),e(Hd,wXo),e(mr,AXo),e(mr,a3),e(a3,LXo),e(a3,Fde),e(Fde,BXo),e(a3,xXo),e(mr,kXo),e(mr,at),g(n3,at,null),e(at,RXo),e(at,Cde),e(Cde,SXo),e(at,PXo),e(at,Ud),e(Ud,$Xo),e(Ud,Mde),e(Mde,IXo),e(Ud,jXo),e(Ud,Ede),e(Ede,NXo),e(Ud,DXo),e(at,qXo),e(at,yde),e(yde,GXo),e(at,OXo),g(s3,at,null),e(mr,XXo),e(mr,mo),g(l3,mo,null),e(mo,zXo),e(mo,wde),e(wde,VXo),e(mo,WXo),e(mo,tn),e(tn,QXo),e(tn,Ade),e(Ade,HXo),e(tn,UXo),e(tn,Lde),e(Lde,JXo),e(tn,YXo),e(tn,Bde),e(Bde,KXo),e(tn,ZXo),e(mo,ezo),e(mo,H),e(H,Dv),e(Dv,xde),e(xde,ozo),e(Dv,rzo),e(Dv,wN),e(wN,tzo),e(Dv,azo),e(H,nzo),e(H,qv),e(qv,kde),e(kde,szo),e(qv,lzo),e(qv,AN),e(AN,izo),e(qv,dzo),e(H,czo),e(H,Gv),e(Gv,Rde),e(Rde,fzo),e(Gv,mzo),e(Gv,LN),e(LN,gzo),e(Gv,hzo),e(H,pzo),e(H,Ov),e(Ov,Sde),e(Sde,_zo),e(Ov,uzo),e(Ov,BN),e(BN,bzo),e(Ov,vzo),e(H,Tzo),e(H,Xv),e(Xv,Pde),e(Pde,Fzo),e(Xv,Czo),e(Xv,xN),e(xN,Mzo),e(Xv,Ezo),e(H,yzo),e(H,zv),e(zv,$de),e($de,wzo),e(zv,Azo),e(zv,kN),e(kN,Lzo),e(zv,Bzo),e(H,xzo),e(H,Vv),e(Vv,Ide),e(Ide,kzo),e(Vv,Rzo),e(Vv,RN),e(RN,Szo),e(Vv,Pzo),e(H,$zo),e(H,Wv),e(Wv,jde),e(jde,Izo),e(Wv,jzo),e(Wv,SN),e(SN,Nzo),e(Wv,Dzo),e(H,qzo),e(H,Qv),e(Qv,Nde),e(Nde,Gzo),e(Qv,Ozo),e(Qv,PN),e(PN,Xzo),e(Qv,zzo),e(H,Vzo),e(H,Hv),e(Hv,Dde),e(Dde,Wzo),e(Hv,Qzo),e(Hv,$N),e($N,Hzo),e(Hv,Uzo),e(H,Jzo),e(H,Uv),e(Uv,qde),e(qde,Yzo),e(Uv,Kzo),e(Uv,IN),e(IN,Zzo),e(Uv,eVo),e(H,oVo),e(H,Jv),e(Jv,Gde),e(Gde,rVo),e(Jv,tVo),e(Jv,jN),e(jN,aVo),e(Jv,nVo),e(H,sVo),e(H,Yv),e(Yv,Ode),e(Ode,lVo),e(Yv,iVo),e(Yv,NN),e(NN,dVo),e(Yv,cVo),e(H,fVo),e(H,Kv),e(Kv,Xde),e(Xde,mVo),e(Kv,gVo),e(Kv,DN),e(DN,hVo),e(Kv,pVo),e(H,_Vo),e(H,Zv),e(Zv,zde),e(zde,uVo),e(Zv,bVo),e(Zv,qN),e(qN,vVo),e(Zv,TVo),e(H,FVo),e(H,e6),e(e6,Vde),e(Vde,CVo),e(e6,MVo),e(e6,GN),e(GN,EVo),e(e6,yVo),e(H,wVo),e(H,o6),e(o6,Wde),e(Wde,AVo),e(o6,LVo),e(o6,ON),e(ON,BVo),e(o6,xVo),e(H,kVo),e(H,r6),e(r6,Qde),e(Qde,RVo),e(r6,SVo),e(r6,XN),e(XN,PVo),e(r6,$Vo),e(H,IVo),e(H,t6),e(t6,Hde),e(Hde,jVo),e(t6,NVo),e(t6,zN),e(zN,DVo),e(t6,qVo),e(H,GVo),e(H,a6),e(a6,Ude),e(Ude,OVo),e(a6,XVo),e(a6,VN),e(VN,zVo),e(a6,VVo),e(H,WVo),e(H,n6),e(n6,Jde),e(Jde,QVo),e(n6,HVo),e(n6,WN),e(WN,UVo),e(n6,JVo),e(H,YVo),e(H,s6),e(s6,Yde),e(Yde,KVo),e(s6,ZVo),e(s6,QN),e(QN,eWo),e(s6,oWo),e(mo,rWo),e(mo,Kde),e(Kde,tWo),e(mo,aWo),g(i3,mo,null),b(d,M0e,u),b(d,Jd,u),e(Jd,l6),e(l6,Zde),g(d3,Zde,null),e(Jd,nWo),e(Jd,ece),e(ece,sWo),b(d,E0e,u),b(d,gr,u),g(c3,gr,null),e(gr,lWo),e(gr,Yd),e(Yd,iWo),e(Yd,oce),e(oce,dWo),e(Yd,cWo),e(Yd,rce),e(rce,fWo),e(Yd,mWo),e(gr,gWo),e(gr,f3),e(f3,hWo),e(f3,tce),e(tce,pWo),e(f3,_Wo),e(gr,uWo),e(gr,nt),g(m3,nt,null),e(nt,bWo),e(nt,ace),e(ace,vWo),e(nt,TWo),e(nt,Kd),e(Kd,FWo),e(Kd,nce),e(nce,CWo),e(Kd,MWo),e(Kd,sce),e(sce,EWo),e(Kd,yWo),e(nt,wWo),e(nt,lce),e(lce,AWo),e(nt,LWo),g(g3,nt,null),e(gr,BWo),e(gr,go),g(h3,go,null),e(go,xWo),e(go,ice),e(ice,kWo),e(go,RWo),e(go,an),e(an,SWo),e(an,dce),e(dce,PWo),e(an,$Wo),e(an,cce),e(cce,IWo),e(an,jWo),e(an,fce),e(fce,NWo),e(an,DWo),e(go,qWo),e(go,he),e(he,i6),e(i6,mce),e(mce,GWo),e(i6,OWo),e(i6,HN),e(HN,XWo),e(i6,zWo),e(he,VWo),e(he,d6),e(d6,gce),e(gce,WWo),e(d6,QWo),e(d6,UN),e(UN,HWo),e(d6,UWo),e(he,JWo),e(he,c6),e(c6,hce),e(hce,YWo),e(c6,KWo),e(c6,JN),e(JN,ZWo),e(c6,eQo),e(he,oQo),e(he,f6),e(f6,pce),e(pce,rQo),e(f6,tQo),e(f6,YN),e(YN,aQo),e(f6,nQo),e(he,sQo),e(he,m6),e(m6,_ce),e(_ce,lQo),e(m6,iQo),e(m6,KN),e(KN,dQo),e(m6,cQo),e(he,fQo),e(he,g6),e(g6,uce),e(uce,mQo),e(g6,gQo),e(g6,ZN),e(ZN,hQo),e(g6,pQo),e(he,_Qo),e(he,h6),e(h6,bce),e(bce,uQo),e(h6,bQo),e(h6,eD),e(eD,vQo),e(h6,TQo),e(he,FQo),e(he,p6),e(p6,vce),e(vce,CQo),e(p6,MQo),e(p6,oD),e(oD,EQo),e(p6,yQo),e(he,wQo),e(he,_6),e(_6,Tce),e(Tce,AQo),e(_6,LQo),e(_6,rD),e(rD,BQo),e(_6,xQo),e(he,kQo),e(he,u6),e(u6,Fce),e(Fce,RQo),e(u6,SQo),e(u6,tD),e(tD,PQo),e(u6,$Qo),e(go,IQo),e(go,Cce),e(Cce,jQo),e(go,NQo),g(p3,go,null),b(d,y0e,u),b(d,Zd,u),e(Zd,b6),e(b6,Mce),g(_3,Mce,null),e(Zd,DQo),e(Zd,Ece),e(Ece,qQo),b(d,w0e,u),b(d,hr,u),g(u3,hr,null),e(hr,GQo),e(hr,ec),e(ec,OQo),e(ec,yce),e(yce,XQo),e(ec,zQo),e(ec,wce),e(wce,VQo),e(ec,WQo),e(hr,QQo),e(hr,b3),e(b3,HQo),e(b3,Ace),e(Ace,UQo),e(b3,JQo),e(hr,YQo),e(hr,st),g(v3,st,null),e(st,KQo),e(st,Lce),e(Lce,ZQo),e(st,eHo),e(st,oc),e(oc,oHo),e(oc,Bce),e(Bce,rHo),e(oc,tHo),e(oc,xce),e(xce,aHo),e(oc,nHo),e(st,sHo),e(st,kce),e(kce,lHo),e(st,iHo),g(T3,st,null),e(hr,dHo),e(hr,ho),g(F3,ho,null),e(ho,cHo),e(ho,Rce),e(Rce,fHo),e(ho,mHo),e(ho,nn),e(nn,gHo),e(nn,Sce),e(Sce,hHo),e(nn,pHo),e(nn,Pce),e(Pce,_Ho),e(nn,uHo),e(nn,$ce),e($ce,bHo),e(nn,vHo),e(ho,THo),e(ho,Ice),e(Ice,v6),e(v6,jce),e(jce,FHo),e(v6,CHo),e(v6,aD),e(aD,MHo),e(v6,EHo),e(ho,yHo),e(ho,Nce),e(Nce,wHo),e(ho,AHo),g(C3,ho,null),b(d,A0e,u),b(d,rc,u),e(rc,T6),e(T6,Dce),g(M3,Dce,null),e(rc,LHo),e(rc,qce),e(qce,BHo),b(d,L0e,u),b(d,pr,u),g(E3,pr,null),e(pr,xHo),e(pr,tc),e(tc,kHo),e(tc,Gce),e(Gce,RHo),e(tc,SHo),e(tc,Oce),e(Oce,PHo),e(tc,$Ho),e(pr,IHo),e(pr,y3),e(y3,jHo),e(y3,Xce),e(Xce,NHo),e(y3,DHo),e(pr,qHo),e(pr,lt),g(w3,lt,null),e(lt,GHo),e(lt,zce),e(zce,OHo),e(lt,XHo),e(lt,ac),e(ac,zHo),e(ac,Vce),e(Vce,VHo),e(ac,WHo),e(ac,Wce),e(Wce,QHo),e(ac,HHo),e(lt,UHo),e(lt,Qce),e(Qce,JHo),e(lt,YHo),g(A3,lt,null),e(pr,KHo),e(pr,po),g(L3,po,null),e(po,ZHo),e(po,Hce),e(Hce,eUo),e(po,oUo),e(po,sn),e(sn,rUo),e(sn,Uce),e(Uce,tUo),e(sn,aUo),e(sn,Jce),e(Jce,nUo),e(sn,sUo),e(sn,Yce),e(Yce,lUo),e(sn,iUo),e(po,dUo),e(po,Y),e(Y,F6),e(F6,Kce),e(Kce,cUo),e(F6,fUo),e(F6,nD),e(nD,mUo),e(F6,gUo),e(Y,hUo),e(Y,C6),e(C6,Zce),e(Zce,pUo),e(C6,_Uo),e(C6,sD),e(sD,uUo),e(C6,bUo),e(Y,vUo),e(Y,M6),e(M6,efe),e(efe,TUo),e(M6,FUo),e(M6,lD),e(lD,CUo),e(M6,MUo),e(Y,EUo),e(Y,E6),e(E6,ofe),e(ofe,yUo),e(E6,wUo),e(E6,iD),e(iD,AUo),e(E6,LUo),e(Y,BUo),e(Y,y6),e(y6,rfe),e(rfe,xUo),e(y6,kUo),e(y6,dD),e(dD,RUo),e(y6,SUo),e(Y,PUo),e(Y,w6),e(w6,tfe),e(tfe,$Uo),e(w6,IUo),e(w6,cD),e(cD,jUo),e(w6,NUo),e(Y,DUo),e(Y,A6),e(A6,afe),e(afe,qUo),e(A6,GUo),e(A6,fD),e(fD,OUo),e(A6,XUo),e(Y,zUo),e(Y,L6),e(L6,nfe),e(nfe,VUo),e(L6,WUo),e(L6,mD),e(mD,QUo),e(L6,HUo),e(Y,UUo),e(Y,B6),e(B6,sfe),e(sfe,JUo),e(B6,YUo),e(B6,gD),e(gD,KUo),e(B6,ZUo),e(Y,eJo),e(Y,x6),e(x6,lfe),e(lfe,oJo),e(x6,rJo),e(x6,hD),e(hD,tJo),e(x6,aJo),e(Y,nJo),e(Y,k6),e(k6,ife),e(ife,sJo),e(k6,lJo),e(k6,pD),e(pD,iJo),e(k6,dJo),e(Y,cJo),e(Y,R6),e(R6,dfe),e(dfe,fJo),e(R6,mJo),e(R6,_D),e(_D,gJo),e(R6,hJo),e(Y,pJo),e(Y,S6),e(S6,cfe),e(cfe,_Jo),e(S6,uJo),e(S6,uD),e(uD,bJo),e(S6,vJo),e(Y,TJo),e(Y,P6),e(P6,ffe),e(ffe,FJo),e(P6,CJo),e(P6,bD),e(bD,MJo),e(P6,EJo),e(Y,yJo),e(Y,$6),e($6,mfe),e(mfe,wJo),e($6,AJo),e($6,vD),e(vD,LJo),e($6,BJo),e(Y,xJo),e(Y,I6),e(I6,gfe),e(gfe,kJo),e(I6,RJo),e(I6,TD),e(TD,SJo),e(I6,PJo),e(Y,$Jo),e(Y,j6),e(j6,hfe),e(hfe,IJo),e(j6,jJo),e(j6,FD),e(FD,NJo),e(j6,DJo),e(Y,qJo),e(Y,N6),e(N6,pfe),e(pfe,GJo),e(N6,OJo),e(N6,CD),e(CD,XJo),e(N6,zJo),e(Y,VJo),e(Y,D6),e(D6,_fe),e(_fe,WJo),e(D6,QJo),e(D6,MD),e(MD,HJo),e(D6,UJo),e(Y,JJo),e(Y,q6),e(q6,ufe),e(ufe,YJo),e(q6,KJo),e(q6,ED),e(ED,ZJo),e(q6,eYo),e(po,oYo),e(po,bfe),e(bfe,rYo),e(po,tYo),g(B3,po,null),b(d,B0e,u),b(d,nc,u),e(nc,G6),e(G6,vfe),g(x3,vfe,null),e(nc,aYo),e(nc,Tfe),e(Tfe,nYo),b(d,x0e,u),b(d,_r,u),g(k3,_r,null),e(_r,sYo),e(_r,sc),e(sc,lYo),e(sc,Ffe),e(Ffe,iYo),e(sc,dYo),e(sc,Cfe),e(Cfe,cYo),e(sc,fYo),e(_r,mYo),e(_r,R3),e(R3,gYo),e(R3,Mfe),e(Mfe,hYo),e(R3,pYo),e(_r,_Yo),e(_r,it),g(S3,it,null),e(it,uYo),e(it,Efe),e(Efe,bYo),e(it,vYo),e(it,lc),e(lc,TYo),e(lc,yfe),e(yfe,FYo),e(lc,CYo),e(lc,wfe),e(wfe,MYo),e(lc,EYo),e(it,yYo),e(it,Afe),e(Afe,wYo),e(it,AYo),g(P3,it,null),e(_r,LYo),e(_r,_o),g($3,_o,null),e(_o,BYo),e(_o,Lfe),e(Lfe,xYo),e(_o,kYo),e(_o,ln),e(ln,RYo),e(ln,Bfe),e(Bfe,SYo),e(ln,PYo),e(ln,xfe),e(xfe,$Yo),e(ln,IYo),e(ln,kfe),e(kfe,jYo),e(ln,NYo),e(_o,DYo),e(_o,pe),e(pe,O6),e(O6,Rfe),e(Rfe,qYo),e(O6,GYo),e(O6,yD),e(yD,OYo),e(O6,XYo),e(pe,zYo),e(pe,X6),e(X6,Sfe),e(Sfe,VYo),e(X6,WYo),e(X6,wD),e(wD,QYo),e(X6,HYo),e(pe,UYo),e(pe,z6),e(z6,Pfe),e(Pfe,JYo),e(z6,YYo),e(z6,AD),e(AD,KYo),e(z6,ZYo),e(pe,eKo),e(pe,V6),e(V6,$fe),e($fe,oKo),e(V6,rKo),e(V6,LD),e(LD,tKo),e(V6,aKo),e(pe,nKo),e(pe,W6),e(W6,Ife),e(Ife,sKo),e(W6,lKo),e(W6,BD),e(BD,iKo),e(W6,dKo),e(pe,cKo),e(pe,Q6),e(Q6,jfe),e(jfe,fKo),e(Q6,mKo),e(Q6,xD),e(xD,gKo),e(Q6,hKo),e(pe,pKo),e(pe,H6),e(H6,Nfe),e(Nfe,_Ko),e(H6,uKo),e(H6,kD),e(kD,bKo),e(H6,vKo),e(pe,TKo),e(pe,U6),e(U6,Dfe),e(Dfe,FKo),e(U6,CKo),e(U6,RD),e(RD,MKo),e(U6,EKo),e(pe,yKo),e(pe,J6),e(J6,qfe),e(qfe,wKo),e(J6,AKo),e(J6,SD),e(SD,LKo),e(J6,BKo),e(pe,xKo),e(pe,Y6),e(Y6,Gfe),e(Gfe,kKo),e(Y6,RKo),e(Y6,PD),e(PD,SKo),e(Y6,PKo),e(_o,$Ko),e(_o,Ofe),e(Ofe,IKo),e(_o,jKo),g(I3,_o,null),b(d,k0e,u),b(d,ic,u),e(ic,K6),e(K6,Xfe),g(j3,Xfe,null),e(ic,NKo),e(ic,zfe),e(zfe,DKo),b(d,R0e,u),b(d,ur,u),g(N3,ur,null),e(ur,qKo),e(ur,dc),e(dc,GKo),e(dc,Vfe),e(Vfe,OKo),e(dc,XKo),e(dc,Wfe),e(Wfe,zKo),e(dc,VKo),e(ur,WKo),e(ur,D3),e(D3,QKo),e(D3,Qfe),e(Qfe,HKo),e(D3,UKo),e(ur,JKo),e(ur,dt),g(q3,dt,null),e(dt,YKo),e(dt,Hfe),e(Hfe,KKo),e(dt,ZKo),e(dt,cc),e(cc,eZo),e(cc,Ufe),e(Ufe,oZo),e(cc,rZo),e(cc,Jfe),e(Jfe,tZo),e(cc,aZo),e(dt,nZo),e(dt,Yfe),e(Yfe,sZo),e(dt,lZo),g(G3,dt,null),e(ur,iZo),e(ur,uo),g(O3,uo,null),e(uo,dZo),e(uo,Kfe),e(Kfe,cZo),e(uo,fZo),e(uo,dn),e(dn,mZo),e(dn,Zfe),e(Zfe,gZo),e(dn,hZo),e(dn,eme),e(eme,pZo),e(dn,_Zo),e(dn,ome),e(ome,uZo),e(dn,bZo),e(uo,vZo),e(uo,X),e(X,Z6),e(Z6,rme),e(rme,TZo),e(Z6,FZo),e(Z6,$D),e($D,CZo),e(Z6,MZo),e(X,EZo),e(X,eT),e(eT,tme),e(tme,yZo),e(eT,wZo),e(eT,ID),e(ID,AZo),e(eT,LZo),e(X,BZo),e(X,oT),e(oT,ame),e(ame,xZo),e(oT,kZo),e(oT,jD),e(jD,RZo),e(oT,SZo),e(X,PZo),e(X,rT),e(rT,nme),e(nme,$Zo),e(rT,IZo),e(rT,ND),e(ND,jZo),e(rT,NZo),e(X,DZo),e(X,tT),e(tT,sme),e(sme,qZo),e(tT,GZo),e(tT,DD),e(DD,OZo),e(tT,XZo),e(X,zZo),e(X,aT),e(aT,lme),e(lme,VZo),e(aT,WZo),e(aT,qD),e(qD,QZo),e(aT,HZo),e(X,UZo),e(X,nT),e(nT,ime),e(ime,JZo),e(nT,YZo),e(nT,GD),e(GD,KZo),e(nT,ZZo),e(X,eer),e(X,sT),e(sT,dme),e(dme,oer),e(sT,rer),e(sT,OD),e(OD,ter),e(sT,aer),e(X,ner),e(X,lT),e(lT,cme),e(cme,ser),e(lT,ler),e(lT,XD),e(XD,ier),e(lT,der),e(X,cer),e(X,iT),e(iT,fme),e(fme,fer),e(iT,mer),e(iT,zD),e(zD,ger),e(iT,her),e(X,per),e(X,dT),e(dT,mme),e(mme,_er),e(dT,uer),e(dT,VD),e(VD,ber),e(dT,ver),e(X,Ter),e(X,cT),e(cT,gme),e(gme,Fer),e(cT,Cer),e(cT,WD),e(WD,Mer),e(cT,Eer),e(X,yer),e(X,fT),e(fT,hme),e(hme,wer),e(fT,Aer),e(fT,QD),e(QD,Ler),e(fT,Ber),e(X,xer),e(X,mT),e(mT,pme),e(pme,ker),e(mT,Rer),e(mT,HD),e(HD,Ser),e(mT,Per),e(X,$er),e(X,gT),e(gT,_me),e(_me,Ier),e(gT,jer),e(gT,UD),e(UD,Ner),e(gT,Der),e(X,qer),e(X,hT),e(hT,ume),e(ume,Ger),e(hT,Oer),e(hT,JD),e(JD,Xer),e(hT,zer),e(X,Ver),e(X,pT),e(pT,bme),e(bme,Wer),e(pT,Qer),e(pT,YD),e(YD,Her),e(pT,Uer),e(X,Jer),e(X,_T),e(_T,vme),e(vme,Yer),e(_T,Ker),e(_T,KD),e(KD,Zer),e(_T,eor),e(X,oor),e(X,uT),e(uT,Tme),e(Tme,ror),e(uT,tor),e(uT,ZD),e(ZD,aor),e(uT,nor),e(X,sor),e(X,bT),e(bT,Fme),e(Fme,lor),e(bT,ior),e(bT,eq),e(eq,dor),e(bT,cor),e(X,mor),e(X,vT),e(vT,Cme),e(Cme,gor),e(vT,hor),e(vT,oq),e(oq,por),e(vT,_or),e(X,uor),e(X,TT),e(TT,Mme),e(Mme,bor),e(TT,vor),e(TT,rq),e(rq,Tor),e(TT,For),e(X,Cor),e(X,FT),e(FT,Eme),e(Eme,Mor),e(FT,Eor),e(FT,tq),e(tq,yor),e(FT,wor),e(X,Aor),e(X,CT),e(CT,yme),e(yme,Lor),e(CT,Bor),e(CT,aq),e(aq,xor),e(CT,kor),e(X,Ror),e(X,MT),e(MT,wme),e(wme,Sor),e(MT,Por),e(MT,nq),e(nq,$or),e(MT,Ior),e(uo,jor),e(uo,Ame),e(Ame,Nor),e(uo,Dor),g(X3,uo,null),b(d,S0e,u),b(d,fc,u),e(fc,ET),e(ET,Lme),g(z3,Lme,null),e(fc,qor),e(fc,Bme),e(Bme,Gor),b(d,P0e,u),b(d,br,u),g(V3,br,null),e(br,Oor),e(br,mc),e(mc,Xor),e(mc,xme),e(xme,zor),e(mc,Vor),e(mc,kme),e(kme,Wor),e(mc,Qor),e(br,Hor),e(br,W3),e(W3,Uor),e(W3,Rme),e(Rme,Jor),e(W3,Yor),e(br,Kor),e(br,ct),g(Q3,ct,null),e(ct,Zor),e(ct,Sme),e(Sme,err),e(ct,orr),e(ct,gc),e(gc,rrr),e(gc,Pme),e(Pme,trr),e(gc,arr),e(gc,$me),e($me,nrr),e(gc,srr),e(ct,lrr),e(ct,Ime),e(Ime,irr),e(ct,drr),g(H3,ct,null),e(br,crr),e(br,bo),g(U3,bo,null),e(bo,frr),e(bo,jme),e(jme,mrr),e(bo,grr),e(bo,cn),e(cn,hrr),e(cn,Nme),e(Nme,prr),e(cn,_rr),e(cn,Dme),e(Dme,urr),e(cn,brr),e(cn,qme),e(qme,vrr),e(cn,Trr),e(bo,Frr),e(bo,te),e(te,yT),e(yT,Gme),e(Gme,Crr),e(yT,Mrr),e(yT,sq),e(sq,Err),e(yT,yrr),e(te,wrr),e(te,wT),e(wT,Ome),e(Ome,Arr),e(wT,Lrr),e(wT,lq),e(lq,Brr),e(wT,xrr),e(te,krr),e(te,AT),e(AT,Xme),e(Xme,Rrr),e(AT,Srr),e(AT,iq),e(iq,Prr),e(AT,$rr),e(te,Irr),e(te,LT),e(LT,zme),e(zme,jrr),e(LT,Nrr),e(LT,dq),e(dq,Drr),e(LT,qrr),e(te,Grr),e(te,BT),e(BT,Vme),e(Vme,Orr),e(BT,Xrr),e(BT,cq),e(cq,zrr),e(BT,Vrr),e(te,Wrr),e(te,xT),e(xT,Wme),e(Wme,Qrr),e(xT,Hrr),e(xT,fq),e(fq,Urr),e(xT,Jrr),e(te,Yrr),e(te,kT),e(kT,Qme),e(Qme,Krr),e(kT,Zrr),e(kT,mq),e(mq,etr),e(kT,otr),e(te,rtr),e(te,RT),e(RT,Hme),e(Hme,ttr),e(RT,atr),e(RT,gq),e(gq,ntr),e(RT,str),e(te,ltr),e(te,ST),e(ST,Ume),e(Ume,itr),e(ST,dtr),e(ST,hq),e(hq,ctr),e(ST,ftr),e(te,mtr),e(te,PT),e(PT,Jme),e(Jme,gtr),e(PT,htr),e(PT,pq),e(pq,ptr),e(PT,_tr),e(te,utr),e(te,$T),e($T,Yme),e(Yme,btr),e($T,vtr),e($T,_q),e(_q,Ttr),e($T,Ftr),e(te,Ctr),e(te,IT),e(IT,Kme),e(Kme,Mtr),e(IT,Etr),e(IT,uq),e(uq,ytr),e(IT,wtr),e(te,Atr),e(te,jT),e(jT,Zme),e(Zme,Ltr),e(jT,Btr),e(jT,bq),e(bq,xtr),e(jT,ktr),e(te,Rtr),e(te,NT),e(NT,ege),e(ege,Str),e(NT,Ptr),e(NT,vq),e(vq,$tr),e(NT,Itr),e(te,jtr),e(te,DT),e(DT,oge),e(oge,Ntr),e(DT,Dtr),e(DT,Tq),e(Tq,qtr),e(DT,Gtr),e(te,Otr),e(te,qT),e(qT,rge),e(rge,Xtr),e(qT,ztr),e(qT,Fq),e(Fq,Vtr),e(qT,Wtr),e(te,Qtr),e(te,GT),e(GT,tge),e(tge,Htr),e(GT,Utr),e(GT,Cq),e(Cq,Jtr),e(GT,Ytr),e(bo,Ktr),e(bo,age),e(age,Ztr),e(bo,ear),g(J3,bo,null),b(d,$0e,u),b(d,hc,u),e(hc,OT),e(OT,nge),g(Y3,nge,null),e(hc,oar),e(hc,sge),e(sge,rar),b(d,I0e,u),b(d,vr,u),g(K3,vr,null),e(vr,tar),e(vr,pc),e(pc,aar),e(pc,lge),e(lge,nar),e(pc,sar),e(pc,ige),e(ige,lar),e(pc,iar),e(vr,dar),e(vr,Z3),e(Z3,car),e(Z3,dge),e(dge,far),e(Z3,mar),e(vr,gar),e(vr,ft),g(ey,ft,null),e(ft,har),e(ft,cge),e(cge,par),e(ft,_ar),e(ft,_c),e(_c,uar),e(_c,fge),e(fge,bar),e(_c,Tar),e(_c,mge),e(mge,Far),e(_c,Car),e(ft,Mar),e(ft,gge),e(gge,Ear),e(ft,yar),g(oy,ft,null),e(vr,war),e(vr,vo),g(ry,vo,null),e(vo,Aar),e(vo,hge),e(hge,Lar),e(vo,Bar),e(vo,fn),e(fn,xar),e(fn,pge),e(pge,kar),e(fn,Rar),e(fn,_ge),e(_ge,Sar),e(fn,Par),e(fn,uge),e(uge,$ar),e(fn,Iar),e(vo,jar),e(vo,bge),e(bge,XT),e(XT,vge),e(vge,Nar),e(XT,Dar),e(XT,Mq),e(Mq,qar),e(XT,Gar),e(vo,Oar),e(vo,Tge),e(Tge,Xar),e(vo,zar),g(ty,vo,null),b(d,j0e,u),b(d,uc,u),e(uc,zT),e(zT,Fge),g(ay,Fge,null),e(uc,Var),e(uc,Cge),e(Cge,War),b(d,N0e,u),b(d,Tr,u),g(ny,Tr,null),e(Tr,Qar),e(Tr,bc),e(bc,Har),e(bc,Mge),e(Mge,Uar),e(bc,Jar),e(bc,Ege),e(Ege,Yar),e(bc,Kar),e(Tr,Zar),e(Tr,sy),e(sy,enr),e(sy,yge),e(yge,onr),e(sy,rnr),e(Tr,tnr),e(Tr,mt),g(ly,mt,null),e(mt,anr),e(mt,wge),e(wge,nnr),e(mt,snr),e(mt,vc),e(vc,lnr),e(vc,Age),e(Age,inr),e(vc,dnr),e(vc,Lge),e(Lge,cnr),e(vc,fnr),e(mt,mnr),e(mt,Bge),e(Bge,gnr),e(mt,hnr),g(iy,mt,null),e(Tr,pnr),e(Tr,To),g(dy,To,null),e(To,_nr),e(To,xge),e(xge,unr),e(To,bnr),e(To,mn),e(mn,vnr),e(mn,kge),e(kge,Tnr),e(mn,Fnr),e(mn,Rge),e(Rge,Cnr),e(mn,Mnr),e(mn,Sge),e(Sge,Enr),e(mn,ynr),e(To,wnr),e(To,K),e(K,VT),e(VT,Pge),e(Pge,Anr),e(VT,Lnr),e(VT,Eq),e(Eq,Bnr),e(VT,xnr),e(K,knr),e(K,WT),e(WT,$ge),e($ge,Rnr),e(WT,Snr),e(WT,yq),e(yq,Pnr),e(WT,$nr),e(K,Inr),e(K,QT),e(QT,Ige),e(Ige,jnr),e(QT,Nnr),e(QT,wq),e(wq,Dnr),e(QT,qnr),e(K,Gnr),e(K,HT),e(HT,jge),e(jge,Onr),e(HT,Xnr),e(HT,Aq),e(Aq,znr),e(HT,Vnr),e(K,Wnr),e(K,UT),e(UT,Nge),e(Nge,Qnr),e(UT,Hnr),e(UT,Lq),e(Lq,Unr),e(UT,Jnr),e(K,Ynr),e(K,JT),e(JT,Dge),e(Dge,Knr),e(JT,Znr),e(JT,Bq),e(Bq,esr),e(JT,osr),e(K,rsr),e(K,YT),e(YT,qge),e(qge,tsr),e(YT,asr),e(YT,xq),e(xq,nsr),e(YT,ssr),e(K,lsr),e(K,KT),e(KT,Gge),e(Gge,isr),e(KT,dsr),e(KT,kq),e(kq,csr),e(KT,fsr),e(K,msr),e(K,ZT),e(ZT,Oge),e(Oge,gsr),e(ZT,hsr),e(ZT,Rq),e(Rq,psr),e(ZT,_sr),e(K,usr),e(K,e7),e(e7,Xge),e(Xge,bsr),e(e7,vsr),e(e7,Sq),e(Sq,Tsr),e(e7,Fsr),e(K,Csr),e(K,o7),e(o7,zge),e(zge,Msr),e(o7,Esr),e(o7,Pq),e(Pq,ysr),e(o7,wsr),e(K,Asr),e(K,r7),e(r7,Vge),e(Vge,Lsr),e(r7,Bsr),e(r7,$q),e($q,xsr),e(r7,ksr),e(K,Rsr),e(K,t7),e(t7,Wge),e(Wge,Ssr),e(t7,Psr),e(t7,Iq),e(Iq,$sr),e(t7,Isr),e(K,jsr),e(K,a7),e(a7,Qge),e(Qge,Nsr),e(a7,Dsr),e(a7,jq),e(jq,qsr),e(a7,Gsr),e(K,Osr),e(K,n7),e(n7,Hge),e(Hge,Xsr),e(n7,zsr),e(n7,Nq),e(Nq,Vsr),e(n7,Wsr),e(K,Qsr),e(K,s7),e(s7,Uge),e(Uge,Hsr),e(s7,Usr),e(s7,Dq),e(Dq,Jsr),e(s7,Ysr),e(K,Ksr),e(K,l7),e(l7,Jge),e(Jge,Zsr),e(l7,elr),e(l7,qq),e(qq,olr),e(l7,rlr),e(K,tlr),e(K,i7),e(i7,Yge),e(Yge,alr),e(i7,nlr),e(i7,Gq),e(Gq,slr),e(i7,llr),e(K,ilr),e(K,d7),e(d7,Kge),e(Kge,dlr),e(d7,clr),e(d7,Oq),e(Oq,flr),e(d7,mlr),e(K,glr),e(K,c7),e(c7,Zge),e(Zge,hlr),e(c7,plr),e(c7,Xq),e(Xq,_lr),e(c7,ulr),e(To,blr),e(To,ehe),e(ehe,vlr),e(To,Tlr),g(cy,To,null),b(d,D0e,u),b(d,Tc,u),e(Tc,f7),e(f7,ohe),g(fy,ohe,null),e(Tc,Flr),e(Tc,rhe),e(rhe,Clr),b(d,q0e,u),b(d,Fr,u),g(my,Fr,null),e(Fr,Mlr),e(Fr,Fc),e(Fc,Elr),e(Fc,the),e(the,ylr),e(Fc,wlr),e(Fc,ahe),e(ahe,Alr),e(Fc,Llr),e(Fr,Blr),e(Fr,gy),e(gy,xlr),e(gy,nhe),e(nhe,klr),e(gy,Rlr),e(Fr,Slr),e(Fr,gt),g(hy,gt,null),e(gt,Plr),e(gt,she),e(she,$lr),e(gt,Ilr),e(gt,Cc),e(Cc,jlr),e(Cc,lhe),e(lhe,Nlr),e(Cc,Dlr),e(Cc,ihe),e(ihe,qlr),e(Cc,Glr),e(gt,Olr),e(gt,dhe),e(dhe,Xlr),e(gt,zlr),g(py,gt,null),e(Fr,Vlr),e(Fr,Fo),g(_y,Fo,null),e(Fo,Wlr),e(Fo,che),e(che,Qlr),e(Fo,Hlr),e(Fo,gn),e(gn,Ulr),e(gn,fhe),e(fhe,Jlr),e(gn,Ylr),e(gn,mhe),e(mhe,Klr),e(gn,Zlr),e(gn,ghe),e(ghe,eir),e(gn,oir),e(Fo,rir),e(Fo,Z),e(Z,m7),e(m7,hhe),e(hhe,tir),e(m7,air),e(m7,zq),e(zq,nir),e(m7,sir),e(Z,lir),e(Z,g7),e(g7,phe),e(phe,iir),e(g7,dir),e(g7,Vq),e(Vq,cir),e(g7,fir),e(Z,mir),e(Z,h7),e(h7,_he),e(_he,gir),e(h7,hir),e(h7,Wq),e(Wq,pir),e(h7,_ir),e(Z,uir),e(Z,p7),e(p7,uhe),e(uhe,bir),e(p7,vir),e(p7,Qq),e(Qq,Tir),e(p7,Fir),e(Z,Cir),e(Z,_7),e(_7,bhe),e(bhe,Mir),e(_7,Eir),e(_7,Hq),e(Hq,yir),e(_7,wir),e(Z,Air),e(Z,u7),e(u7,vhe),e(vhe,Lir),e(u7,Bir),e(u7,Uq),e(Uq,xir),e(u7,kir),e(Z,Rir),e(Z,b7),e(b7,The),e(The,Sir),e(b7,Pir),e(b7,Jq),e(Jq,$ir),e(b7,Iir),e(Z,jir),e(Z,v7),e(v7,Fhe),e(Fhe,Nir),e(v7,Dir),e(v7,Yq),e(Yq,qir),e(v7,Gir),e(Z,Oir),e(Z,T7),e(T7,Che),e(Che,Xir),e(T7,zir),e(T7,Kq),e(Kq,Vir),e(T7,Wir),e(Z,Qir),e(Z,F7),e(F7,Mhe),e(Mhe,Hir),e(F7,Uir),e(F7,Zq),e(Zq,Jir),e(F7,Yir),e(Z,Kir),e(Z,C7),e(C7,Ehe),e(Ehe,Zir),e(C7,edr),e(C7,eG),e(eG,odr),e(C7,rdr),e(Z,tdr),e(Z,M7),e(M7,yhe),e(yhe,adr),e(M7,ndr),e(M7,oG),e(oG,sdr),e(M7,ldr),e(Z,idr),e(Z,E7),e(E7,whe),e(whe,ddr),e(E7,cdr),e(E7,rG),e(rG,fdr),e(E7,mdr),e(Z,gdr),e(Z,y7),e(y7,Ahe),e(Ahe,hdr),e(y7,pdr),e(y7,tG),e(tG,_dr),e(y7,udr),e(Z,bdr),e(Z,w7),e(w7,Lhe),e(Lhe,vdr),e(w7,Tdr),e(w7,aG),e(aG,Fdr),e(w7,Cdr),e(Z,Mdr),e(Z,A7),e(A7,Bhe),e(Bhe,Edr),e(A7,ydr),e(A7,nG),e(nG,wdr),e(A7,Adr),e(Z,Ldr),e(Z,L7),e(L7,xhe),e(xhe,Bdr),e(L7,xdr),e(L7,sG),e(sG,kdr),e(L7,Rdr),e(Z,Sdr),e(Z,B7),e(B7,khe),e(khe,Pdr),e(B7,$dr),e(B7,lG),e(lG,Idr),e(B7,jdr),e(Z,Ndr),e(Z,x7),e(x7,Rhe),e(Rhe,Ddr),e(x7,qdr),e(x7,iG),e(iG,Gdr),e(x7,Odr),e(Fo,Xdr),e(Fo,She),e(She,zdr),e(Fo,Vdr),g(uy,Fo,null),b(d,G0e,u),b(d,Mc,u),e(Mc,k7),e(k7,Phe),g(by,Phe,null),e(Mc,Wdr),e(Mc,$he),e($he,Qdr),b(d,O0e,u),b(d,Cr,u),g(vy,Cr,null),e(Cr,Hdr),e(Cr,Ec),e(Ec,Udr),e(Ec,Ihe),e(Ihe,Jdr),e(Ec,Ydr),e(Ec,jhe),e(jhe,Kdr),e(Ec,Zdr),e(Cr,ecr),e(Cr,Ty),e(Ty,ocr),e(Ty,Nhe),e(Nhe,rcr),e(Ty,tcr),e(Cr,acr),e(Cr,ht),g(Fy,ht,null),e(ht,ncr),e(ht,Dhe),e(Dhe,scr),e(ht,lcr),e(ht,yc),e(yc,icr),e(yc,qhe),e(qhe,dcr),e(yc,ccr),e(yc,Ghe),e(Ghe,fcr),e(yc,mcr),e(ht,gcr),e(ht,Ohe),e(Ohe,hcr),e(ht,pcr),g(Cy,ht,null),e(Cr,_cr),e(Cr,Co),g(My,Co,null),e(Co,ucr),e(Co,Xhe),e(Xhe,bcr),e(Co,vcr),e(Co,hn),e(hn,Tcr),e(hn,zhe),e(zhe,Fcr),e(hn,Ccr),e(hn,Vhe),e(Vhe,Mcr),e(hn,Ecr),e(hn,Whe),e(Whe,ycr),e(hn,wcr),e(Co,Acr),e(Co,Qhe),e(Qhe,R7),e(R7,Hhe),e(Hhe,Lcr),e(R7,Bcr),e(R7,dG),e(dG,xcr),e(R7,kcr),e(Co,Rcr),e(Co,Uhe),e(Uhe,Scr),e(Co,Pcr),g(Ey,Co,null),b(d,X0e,u),b(d,wc,u),e(wc,S7),e(S7,Jhe),g(yy,Jhe,null),e(wc,$cr),e(wc,Yhe),e(Yhe,Icr),b(d,z0e,u),b(d,Mr,u),g(wy,Mr,null),e(Mr,jcr),e(Mr,Ac),e(Ac,Ncr),e(Ac,Khe),e(Khe,Dcr),e(Ac,qcr),e(Ac,Zhe),e(Zhe,Gcr),e(Ac,Ocr),e(Mr,Xcr),e(Mr,Ay),e(Ay,zcr),e(Ay,epe),e(epe,Vcr),e(Ay,Wcr),e(Mr,Qcr),e(Mr,pt),g(Ly,pt,null),e(pt,Hcr),e(pt,ope),e(ope,Ucr),e(pt,Jcr),e(pt,Lc),e(Lc,Ycr),e(Lc,rpe),e(rpe,Kcr),e(Lc,Zcr),e(Lc,tpe),e(tpe,efr),e(Lc,ofr),e(pt,rfr),e(pt,ape),e(ape,tfr),e(pt,afr),g(By,pt,null),e(Mr,nfr),e(Mr,Mo),g(xy,Mo,null),e(Mo,sfr),e(Mo,npe),e(npe,lfr),e(Mo,ifr),e(Mo,pn),e(pn,dfr),e(pn,spe),e(spe,cfr),e(pn,ffr),e(pn,lpe),e(lpe,mfr),e(pn,gfr),e(pn,ipe),e(ipe,hfr),e(pn,pfr),e(Mo,_fr),e(Mo,dpe),e(dpe,P7),e(P7,cpe),e(cpe,ufr),e(P7,bfr),e(P7,cG),e(cG,vfr),e(P7,Tfr),e(Mo,Ffr),e(Mo,fpe),e(fpe,Cfr),e(Mo,Mfr),g(ky,Mo,null),b(d,V0e,u),b(d,Bc,u),e(Bc,$7),e($7,mpe),g(Ry,mpe,null),e(Bc,Efr),e(Bc,gpe),e(gpe,yfr),b(d,W0e,u),b(d,Er,u),g(Sy,Er,null),e(Er,wfr),e(Er,xc),e(xc,Afr),e(xc,hpe),e(hpe,Lfr),e(xc,Bfr),e(xc,ppe),e(ppe,xfr),e(xc,kfr),e(Er,Rfr),e(Er,Py),e(Py,Sfr),e(Py,_pe),e(_pe,Pfr),e(Py,$fr),e(Er,Ifr),e(Er,_t),g($y,_t,null),e(_t,jfr),e(_t,upe),e(upe,Nfr),e(_t,Dfr),e(_t,kc),e(kc,qfr),e(kc,bpe),e(bpe,Gfr),e(kc,Ofr),e(kc,vpe),e(vpe,Xfr),e(kc,zfr),e(_t,Vfr),e(_t,Tpe),e(Tpe,Wfr),e(_t,Qfr),g(Iy,_t,null),e(Er,Hfr),e(Er,Eo),g(jy,Eo,null),e(Eo,Ufr),e(Eo,Fpe),e(Fpe,Jfr),e(Eo,Yfr),e(Eo,_n),e(_n,Kfr),e(_n,Cpe),e(Cpe,Zfr),e(_n,emr),e(_n,Mpe),e(Mpe,omr),e(_n,rmr),e(_n,Epe),e(Epe,tmr),e(_n,amr),e(Eo,nmr),e(Eo,V),e(V,I7),e(I7,ype),e(ype,smr),e(I7,lmr),e(I7,fG),e(fG,imr),e(I7,dmr),e(V,cmr),e(V,j7),e(j7,wpe),e(wpe,fmr),e(j7,mmr),e(j7,mG),e(mG,gmr),e(j7,hmr),e(V,pmr),e(V,N7),e(N7,Ape),e(Ape,_mr),e(N7,umr),e(N7,gG),e(gG,bmr),e(N7,vmr),e(V,Tmr),e(V,D7),e(D7,Lpe),e(Lpe,Fmr),e(D7,Cmr),e(D7,hG),e(hG,Mmr),e(D7,Emr),e(V,ymr),e(V,q7),e(q7,Bpe),e(Bpe,wmr),e(q7,Amr),e(q7,pG),e(pG,Lmr),e(q7,Bmr),e(V,xmr),e(V,G7),e(G7,xpe),e(xpe,kmr),e(G7,Rmr),e(G7,_G),e(_G,Smr),e(G7,Pmr),e(V,$mr),e(V,O7),e(O7,kpe),e(kpe,Imr),e(O7,jmr),e(O7,uG),e(uG,Nmr),e(O7,Dmr),e(V,qmr),e(V,X7),e(X7,Rpe),e(Rpe,Gmr),e(X7,Omr),e(X7,bG),e(bG,Xmr),e(X7,zmr),e(V,Vmr),e(V,z7),e(z7,Spe),e(Spe,Wmr),e(z7,Qmr),e(z7,vG),e(vG,Hmr),e(z7,Umr),e(V,Jmr),e(V,V7),e(V7,Ppe),e(Ppe,Ymr),e(V7,Kmr),e(V7,TG),e(TG,Zmr),e(V7,egr),e(V,ogr),e(V,W7),e(W7,$pe),e($pe,rgr),e(W7,tgr),e(W7,FG),e(FG,agr),e(W7,ngr),e(V,sgr),e(V,Q7),e(Q7,Ipe),e(Ipe,lgr),e(Q7,igr),e(Q7,CG),e(CG,dgr),e(Q7,cgr),e(V,fgr),e(V,H7),e(H7,jpe),e(jpe,mgr),e(H7,ggr),e(H7,MG),e(MG,hgr),e(H7,pgr),e(V,_gr),e(V,U7),e(U7,Npe),e(Npe,ugr),e(U7,bgr),e(U7,EG),e(EG,vgr),e(U7,Tgr),e(V,Fgr),e(V,J7),e(J7,Dpe),e(Dpe,Cgr),e(J7,Mgr),e(J7,yG),e(yG,Egr),e(J7,ygr),e(V,wgr),e(V,Y7),e(Y7,qpe),e(qpe,Agr),e(Y7,Lgr),e(Y7,wG),e(wG,Bgr),e(Y7,xgr),e(V,kgr),e(V,K7),e(K7,Gpe),e(Gpe,Rgr),e(K7,Sgr),e(K7,AG),e(AG,Pgr),e(K7,$gr),e(V,Igr),e(V,Z7),e(Z7,Ope),e(Ope,jgr),e(Z7,Ngr),e(Z7,LG),e(LG,Dgr),e(Z7,qgr),e(V,Ggr),e(V,e8),e(e8,Xpe),e(Xpe,Ogr),e(e8,Xgr),e(e8,BG),e(BG,zgr),e(e8,Vgr),e(V,Wgr),e(V,o8),e(o8,zpe),e(zpe,Qgr),e(o8,Hgr),e(o8,xG),e(xG,Ugr),e(o8,Jgr),e(V,Ygr),e(V,r8),e(r8,Vpe),e(Vpe,Kgr),e(r8,Zgr),e(r8,kG),e(kG,ehr),e(r8,ohr),e(V,rhr),e(V,t8),e(t8,Wpe),e(Wpe,thr),e(t8,ahr),e(t8,RG),e(RG,nhr),e(t8,shr),e(V,lhr),e(V,a8),e(a8,Qpe),e(Qpe,ihr),e(a8,dhr),e(a8,SG),e(SG,chr),e(a8,fhr),e(V,mhr),e(V,n8),e(n8,Hpe),e(Hpe,ghr),e(n8,hhr),e(n8,PG),e(PG,phr),e(n8,_hr),e(Eo,uhr),e(Eo,Upe),e(Upe,bhr),e(Eo,vhr),g(Ny,Eo,null),b(d,Q0e,u),b(d,Rc,u),e(Rc,s8),e(s8,Jpe),g(Dy,Jpe,null),e(Rc,Thr),e(Rc,Ype),e(Ype,Fhr),b(d,H0e,u),b(d,yr,u),g(qy,yr,null),e(yr,Chr),e(yr,Sc),e(Sc,Mhr),e(Sc,Kpe),e(Kpe,Ehr),e(Sc,yhr),e(Sc,Zpe),e(Zpe,whr),e(Sc,Ahr),e(yr,Lhr),e(yr,Gy),e(Gy,Bhr),e(Gy,e_e),e(e_e,xhr),e(Gy,khr),e(yr,Rhr),e(yr,ut),g(Oy,ut,null),e(ut,Shr),e(ut,o_e),e(o_e,Phr),e(ut,$hr),e(ut,Pc),e(Pc,Ihr),e(Pc,r_e),e(r_e,jhr),e(Pc,Nhr),e(Pc,t_e),e(t_e,Dhr),e(Pc,qhr),e(ut,Ghr),e(ut,a_e),e(a_e,Ohr),e(ut,Xhr),g(Xy,ut,null),e(yr,zhr),e(yr,yo),g(zy,yo,null),e(yo,Vhr),e(yo,n_e),e(n_e,Whr),e(yo,Qhr),e(yo,un),e(un,Hhr),e(un,s_e),e(s_e,Uhr),e(un,Jhr),e(un,l_e),e(l_e,Yhr),e(un,Khr),e(un,i_e),e(i_e,Zhr),e(un,epr),e(yo,opr),e(yo,bn),e(bn,l8),e(l8,d_e),e(d_e,rpr),e(l8,tpr),e(l8,$G),e($G,apr),e(l8,npr),e(bn,spr),e(bn,i8),e(i8,c_e),e(c_e,lpr),e(i8,ipr),e(i8,IG),e(IG,dpr),e(i8,cpr),e(bn,fpr),e(bn,d8),e(d8,f_e),e(f_e,mpr),e(d8,gpr),e(d8,jG),e(jG,hpr),e(d8,ppr),e(bn,_pr),e(bn,c8),e(c8,m_e),e(m_e,upr),e(c8,bpr),e(c8,NG),e(NG,vpr),e(c8,Tpr),e(yo,Fpr),e(yo,g_e),e(g_e,Cpr),e(yo,Mpr),g(Vy,yo,null),b(d,U0e,u),b(d,$c,u),e($c,f8),e(f8,h_e),g(Wy,h_e,null),e($c,Epr),e($c,p_e),e(p_e,ypr),b(d,J0e,u),b(d,wr,u),g(Qy,wr,null),e(wr,wpr),e(wr,Ic),e(Ic,Apr),e(Ic,__e),e(__e,Lpr),e(Ic,Bpr),e(Ic,u_e),e(u_e,xpr),e(Ic,kpr),e(wr,Rpr),e(wr,Hy),e(Hy,Spr),e(Hy,b_e),e(b_e,Ppr),e(Hy,$pr),e(wr,Ipr),e(wr,bt),g(Uy,bt,null),e(bt,jpr),e(bt,v_e),e(v_e,Npr),e(bt,Dpr),e(bt,jc),e(jc,qpr),e(jc,T_e),e(T_e,Gpr),e(jc,Opr),e(jc,F_e),e(F_e,Xpr),e(jc,zpr),e(bt,Vpr),e(bt,C_e),e(C_e,Wpr),e(bt,Qpr),g(Jy,bt,null),e(wr,Hpr),e(wr,wo),g(Yy,wo,null),e(wo,Upr),e(wo,M_e),e(M_e,Jpr),e(wo,Ypr),e(wo,vn),e(vn,Kpr),e(vn,E_e),e(E_e,Zpr),e(vn,e_r),e(vn,y_e),e(y_e,o_r),e(vn,r_r),e(vn,w_e),e(w_e,t_r),e(vn,a_r),e(wo,n_r),e(wo,fe),e(fe,m8),e(m8,A_e),e(A_e,s_r),e(m8,l_r),e(m8,DG),e(DG,i_r),e(m8,d_r),e(fe,c_r),e(fe,g8),e(g8,L_e),e(L_e,f_r),e(g8,m_r),e(g8,qG),e(qG,g_r),e(g8,h_r),e(fe,p_r),e(fe,h8),e(h8,B_e),e(B_e,__r),e(h8,u_r),e(h8,GG),e(GG,b_r),e(h8,v_r),e(fe,T_r),e(fe,p8),e(p8,x_e),e(x_e,F_r),e(p8,C_r),e(p8,OG),e(OG,M_r),e(p8,E_r),e(fe,y_r),e(fe,_8),e(_8,k_e),e(k_e,w_r),e(_8,A_r),e(_8,XG),e(XG,L_r),e(_8,B_r),e(fe,x_r),e(fe,u8),e(u8,R_e),e(R_e,k_r),e(u8,R_r),e(u8,zG),e(zG,S_r),e(u8,P_r),e(fe,$_r),e(fe,b8),e(b8,S_e),e(S_e,I_r),e(b8,j_r),e(b8,VG),e(VG,N_r),e(b8,D_r),e(fe,q_r),e(fe,v8),e(v8,P_e),e(P_e,G_r),e(v8,O_r),e(v8,WG),e(WG,X_r),e(v8,z_r),e(fe,V_r),e(fe,T8),e(T8,$_e),e($_e,W_r),e(T8,Q_r),e(T8,QG),e(QG,H_r),e(T8,U_r),e(fe,J_r),e(fe,F8),e(F8,I_e),e(I_e,Y_r),e(F8,K_r),e(F8,HG),e(HG,Z_r),e(F8,eur),e(fe,our),e(fe,C8),e(C8,j_e),e(j_e,rur),e(C8,tur),e(C8,UG),e(UG,aur),e(C8,nur),e(wo,sur),e(wo,N_e),e(N_e,lur),e(wo,iur),g(Ky,wo,null),b(d,Y0e,u),b(d,Nc,u),e(Nc,M8),e(M8,D_e),g(Zy,D_e,null),e(Nc,dur),e(Nc,q_e),e(q_e,cur),b(d,K0e,u),b(d,Ar,u),g(ew,Ar,null),e(Ar,fur),e(Ar,Dc),e(Dc,mur),e(Dc,G_e),e(G_e,gur),e(Dc,hur),e(Dc,O_e),e(O_e,pur),e(Dc,_ur),e(Ar,uur),e(Ar,ow),e(ow,bur),e(ow,X_e),e(X_e,vur),e(ow,Tur),e(Ar,Fur),e(Ar,vt),g(rw,vt,null),e(vt,Cur),e(vt,z_e),e(z_e,Mur),e(vt,Eur),e(vt,qc),e(qc,yur),e(qc,V_e),e(V_e,wur),e(qc,Aur),e(qc,W_e),e(W_e,Lur),e(qc,Bur),e(vt,xur),e(vt,Q_e),e(Q_e,kur),e(vt,Rur),g(tw,vt,null),e(Ar,Sur),e(Ar,Ao),g(aw,Ao,null),e(Ao,Pur),e(Ao,H_e),e(H_e,$ur),e(Ao,Iur),e(Ao,Tn),e(Tn,jur),e(Tn,U_e),e(U_e,Nur),e(Tn,Dur),e(Tn,J_e),e(J_e,qur),e(Tn,Gur),e(Tn,Y_e),e(Y_e,Our),e(Tn,Xur),e(Ao,zur),e(Ao,be),e(be,E8),e(E8,K_e),e(K_e,Vur),e(E8,Wur),e(E8,JG),e(JG,Qur),e(E8,Hur),e(be,Uur),e(be,y8),e(y8,Z_e),e(Z_e,Jur),e(y8,Yur),e(y8,YG),e(YG,Kur),e(y8,Zur),e(be,e1r),e(be,w8),e(w8,eue),e(eue,o1r),e(w8,r1r),e(w8,KG),e(KG,t1r),e(w8,a1r),e(be,n1r),e(be,A8),e(A8,oue),e(oue,s1r),e(A8,l1r),e(A8,ZG),e(ZG,i1r),e(A8,d1r),e(be,c1r),e(be,L8),e(L8,rue),e(rue,f1r),e(L8,m1r),e(L8,eO),e(eO,g1r),e(L8,h1r),e(be,p1r),e(be,B8),e(B8,tue),e(tue,_1r),e(B8,u1r),e(B8,oO),e(oO,b1r),e(B8,v1r),e(be,T1r),e(be,x8),e(x8,aue),e(aue,F1r),e(x8,C1r),e(x8,rO),e(rO,M1r),e(x8,E1r),e(be,y1r),e(be,k8),e(k8,nue),e(nue,w1r),e(k8,A1r),e(k8,tO),e(tO,L1r),e(k8,B1r),e(be,x1r),e(be,R8),e(R8,sue),e(sue,k1r),e(R8,R1r),e(R8,aO),e(aO,S1r),e(R8,P1r),e(Ao,$1r),e(Ao,lue),e(lue,I1r),e(Ao,j1r),g(nw,Ao,null),b(d,Z0e,u),b(d,Gc,u),e(Gc,S8),e(S8,iue),g(sw,iue,null),e(Gc,N1r),e(Gc,due),e(due,D1r),b(d,eLe,u),b(d,Lr,u),g(lw,Lr,null),e(Lr,q1r),e(Lr,Oc),e(Oc,G1r),e(Oc,cue),e(cue,O1r),e(Oc,X1r),e(Oc,fue),e(fue,z1r),e(Oc,V1r),e(Lr,W1r),e(Lr,iw),e(iw,Q1r),e(iw,mue),e(mue,H1r),e(iw,U1r),e(Lr,J1r),e(Lr,Tt),g(dw,Tt,null),e(Tt,Y1r),e(Tt,gue),e(gue,K1r),e(Tt,Z1r),e(Tt,Xc),e(Xc,ebr),e(Xc,hue),e(hue,obr),e(Xc,rbr),e(Xc,pue),e(pue,tbr),e(Xc,abr),e(Tt,nbr),e(Tt,_ue),e(_ue,sbr),e(Tt,lbr),g(cw,Tt,null),e(Lr,ibr),e(Lr,Lo),g(fw,Lo,null),e(Lo,dbr),e(Lo,uue),e(uue,cbr),e(Lo,fbr),e(Lo,Fn),e(Fn,mbr),e(Fn,bue),e(bue,gbr),e(Fn,hbr),e(Fn,vue),e(vue,pbr),e(Fn,_br),e(Fn,Tue),e(Tue,ubr),e(Fn,bbr),e(Lo,vbr),e(Lo,ve),e(ve,P8),e(P8,Fue),e(Fue,Tbr),e(P8,Fbr),e(P8,nO),e(nO,Cbr),e(P8,Mbr),e(ve,Ebr),e(ve,$8),e($8,Cue),e(Cue,ybr),e($8,wbr),e($8,sO),e(sO,Abr),e($8,Lbr),e(ve,Bbr),e(ve,I8),e(I8,Mue),e(Mue,xbr),e(I8,kbr),e(I8,lO),e(lO,Rbr),e(I8,Sbr),e(ve,Pbr),e(ve,j8),e(j8,Eue),e(Eue,$br),e(j8,Ibr),e(j8,iO),e(iO,jbr),e(j8,Nbr),e(ve,Dbr),e(ve,N8),e(N8,yue),e(yue,qbr),e(N8,Gbr),e(N8,dO),e(dO,Obr),e(N8,Xbr),e(ve,zbr),e(ve,D8),e(D8,wue),e(wue,Vbr),e(D8,Wbr),e(D8,cO),e(cO,Qbr),e(D8,Hbr),e(ve,Ubr),e(ve,q8),e(q8,Aue),e(Aue,Jbr),e(q8,Ybr),e(q8,fO),e(fO,Kbr),e(q8,Zbr),e(ve,e5r),e(ve,G8),e(G8,Lue),e(Lue,o5r),e(G8,r5r),e(G8,mO),e(mO,t5r),e(G8,a5r),e(ve,n5r),e(ve,O8),e(O8,Bue),e(Bue,s5r),e(O8,l5r),e(O8,gO),e(gO,i5r),e(O8,d5r),e(Lo,c5r),e(Lo,xue),e(xue,f5r),e(Lo,m5r),g(mw,Lo,null),b(d,oLe,u),b(d,zc,u),e(zc,X8),e(X8,kue),g(gw,kue,null),e(zc,g5r),e(zc,Rue),e(Rue,h5r),b(d,rLe,u),b(d,Br,u),g(hw,Br,null),e(Br,p5r),e(Br,Vc),e(Vc,_5r),e(Vc,Sue),e(Sue,u5r),e(Vc,b5r),e(Vc,Pue),e(Pue,v5r),e(Vc,T5r),e(Br,F5r),e(Br,pw),e(pw,C5r),e(pw,$ue),e($ue,M5r),e(pw,E5r),e(Br,y5r),e(Br,Ft),g(_w,Ft,null),e(Ft,w5r),e(Ft,Iue),e(Iue,A5r),e(Ft,L5r),e(Ft,Wc),e(Wc,B5r),e(Wc,jue),e(jue,x5r),e(Wc,k5r),e(Wc,Nue),e(Nue,R5r),e(Wc,S5r),e(Ft,P5r),e(Ft,Due),e(Due,$5r),e(Ft,I5r),g(uw,Ft,null),e(Br,j5r),e(Br,Bo),g(bw,Bo,null),e(Bo,N5r),e(Bo,que),e(que,D5r),e(Bo,q5r),e(Bo,Cn),e(Cn,G5r),e(Cn,Gue),e(Gue,O5r),e(Cn,X5r),e(Cn,Oue),e(Oue,z5r),e(Cn,V5r),e(Cn,Xue),e(Xue,W5r),e(Cn,Q5r),e(Bo,H5r),e(Bo,Te),e(Te,z8),e(z8,zue),e(zue,U5r),e(z8,J5r),e(z8,hO),e(hO,Y5r),e(z8,K5r),e(Te,Z5r),e(Te,V8),e(V8,Vue),e(Vue,e2r),e(V8,o2r),e(V8,pO),e(pO,r2r),e(V8,t2r),e(Te,a2r),e(Te,W8),e(W8,Wue),e(Wue,n2r),e(W8,s2r),e(W8,_O),e(_O,l2r),e(W8,i2r),e(Te,d2r),e(Te,Q8),e(Q8,Que),e(Que,c2r),e(Q8,f2r),e(Q8,uO),e(uO,m2r),e(Q8,g2r),e(Te,h2r),e(Te,H8),e(H8,Hue),e(Hue,p2r),e(H8,_2r),e(H8,bO),e(bO,u2r),e(H8,b2r),e(Te,v2r),e(Te,U8),e(U8,Uue),e(Uue,T2r),e(U8,F2r),e(U8,vO),e(vO,C2r),e(U8,M2r),e(Te,E2r),e(Te,J8),e(J8,Jue),e(Jue,y2r),e(J8,w2r),e(J8,TO),e(TO,A2r),e(J8,L2r),e(Te,B2r),e(Te,Y8),e(Y8,Yue),e(Yue,x2r),e(Y8,k2r),e(Y8,FO),e(FO,R2r),e(Y8,S2r),e(Te,P2r),e(Te,K8),e(K8,Kue),e(Kue,$2r),e(K8,I2r),e(K8,CO),e(CO,j2r),e(K8,N2r),e(Bo,D2r),e(Bo,Zue),e(Zue,q2r),e(Bo,G2r),g(vw,Bo,null),b(d,tLe,u),b(d,Qc,u),e(Qc,Z8),e(Z8,e1e),g(Tw,e1e,null),e(Qc,O2r),e(Qc,o1e),e(o1e,X2r),b(d,aLe,u),b(d,xr,u),g(Fw,xr,null),e(xr,z2r),e(xr,Hc),e(Hc,V2r),e(Hc,r1e),e(r1e,W2r),e(Hc,Q2r),e(Hc,t1e),e(t1e,H2r),e(Hc,U2r),e(xr,J2r),e(xr,Cw),e(Cw,Y2r),e(Cw,a1e),e(a1e,K2r),e(Cw,Z2r),e(xr,evr),e(xr,Ct),g(Mw,Ct,null),e(Ct,ovr),e(Ct,n1e),e(n1e,rvr),e(Ct,tvr),e(Ct,Uc),e(Uc,avr),e(Uc,s1e),e(s1e,nvr),e(Uc,svr),e(Uc,l1e),e(l1e,lvr),e(Uc,ivr),e(Ct,dvr),e(Ct,i1e),e(i1e,cvr),e(Ct,fvr),g(Ew,Ct,null),e(xr,mvr),e(xr,xo),g(yw,xo,null),e(xo,gvr),e(xo,d1e),e(d1e,hvr),e(xo,pvr),e(xo,Mn),e(Mn,_vr),e(Mn,c1e),e(c1e,uvr),e(Mn,bvr),e(Mn,f1e),e(f1e,vvr),e(Mn,Tvr),e(Mn,m1e),e(m1e,Fvr),e(Mn,Cvr),e(xo,Mvr),e(xo,Fe),e(Fe,eF),e(eF,g1e),e(g1e,Evr),e(eF,yvr),e(eF,MO),e(MO,wvr),e(eF,Avr),e(Fe,Lvr),e(Fe,oF),e(oF,h1e),e(h1e,Bvr),e(oF,xvr),e(oF,EO),e(EO,kvr),e(oF,Rvr),e(Fe,Svr),e(Fe,rF),e(rF,p1e),e(p1e,Pvr),e(rF,$vr),e(rF,yO),e(yO,Ivr),e(rF,jvr),e(Fe,Nvr),e(Fe,tF),e(tF,_1e),e(_1e,Dvr),e(tF,qvr),e(tF,wO),e(wO,Gvr),e(tF,Ovr),e(Fe,Xvr),e(Fe,aF),e(aF,u1e),e(u1e,zvr),e(aF,Vvr),e(aF,AO),e(AO,Wvr),e(aF,Qvr),e(Fe,Hvr),e(Fe,nF),e(nF,b1e),e(b1e,Uvr),e(nF,Jvr),e(nF,LO),e(LO,Yvr),e(nF,Kvr),e(Fe,Zvr),e(Fe,sF),e(sF,v1e),e(v1e,e6r),e(sF,o6r),e(sF,BO),e(BO,r6r),e(sF,t6r),e(Fe,a6r),e(Fe,lF),e(lF,T1e),e(T1e,n6r),e(lF,s6r),e(lF,xO),e(xO,l6r),e(lF,i6r),e(Fe,d6r),e(Fe,iF),e(iF,F1e),e(F1e,c6r),e(iF,f6r),e(iF,kO),e(kO,m6r),e(iF,g6r),e(xo,h6r),e(xo,C1e),e(C1e,p6r),e(xo,_6r),g(ww,xo,null),b(d,nLe,u),b(d,Jc,u),e(Jc,dF),e(dF,M1e),g(Aw,M1e,null),e(Jc,u6r),e(Jc,E1e),e(E1e,b6r),b(d,sLe,u),b(d,kr,u),g(Lw,kr,null),e(kr,v6r),e(kr,Yc),e(Yc,T6r),e(Yc,y1e),e(y1e,F6r),e(Yc,C6r),e(Yc,w1e),e(w1e,M6r),e(Yc,E6r),e(kr,y6r),e(kr,Bw),e(Bw,w6r),e(Bw,A1e),e(A1e,A6r),e(Bw,L6r),e(kr,B6r),e(kr,Mt),g(xw,Mt,null),e(Mt,x6r),e(Mt,L1e),e(L1e,k6r),e(Mt,R6r),e(Mt,Kc),e(Kc,S6r),e(Kc,B1e),e(B1e,P6r),e(Kc,$6r),e(Kc,x1e),e(x1e,I6r),e(Kc,j6r),e(Mt,N6r),e(Mt,k1e),e(k1e,D6r),e(Mt,q6r),g(kw,Mt,null),e(kr,G6r),e(kr,ko),g(Rw,ko,null),e(ko,O6r),e(ko,R1e),e(R1e,X6r),e(ko,z6r),e(ko,En),e(En,V6r),e(En,S1e),e(S1e,W6r),e(En,Q6r),e(En,P1e),e(P1e,H6r),e(En,U6r),e(En,$1e),e($1e,J6r),e(En,Y6r),e(ko,K6r),e(ko,ao),e(ao,cF),e(cF,I1e),e(I1e,Z6r),e(cF,eTr),e(cF,RO),e(RO,oTr),e(cF,rTr),e(ao,tTr),e(ao,fF),e(fF,j1e),e(j1e,aTr),e(fF,nTr),e(fF,SO),e(SO,sTr),e(fF,lTr),e(ao,iTr),e(ao,mF),e(mF,N1e),e(N1e,dTr),e(mF,cTr),e(mF,PO),e(PO,fTr),e(mF,mTr),e(ao,gTr),e(ao,gF),e(gF,D1e),e(D1e,hTr),e(gF,pTr),e(gF,$O),e($O,_Tr),e(gF,uTr),e(ao,bTr),e(ao,hF),e(hF,q1e),e(q1e,vTr),e(hF,TTr),e(hF,IO),e(IO,FTr),e(hF,CTr),e(ao,MTr),e(ao,pF),e(pF,G1e),e(G1e,ETr),e(pF,yTr),e(pF,jO),e(jO,wTr),e(pF,ATr),e(ao,LTr),e(ao,_F),e(_F,O1e),e(O1e,BTr),e(_F,xTr),e(_F,NO),e(NO,kTr),e(_F,RTr),e(ko,STr),e(ko,X1e),e(X1e,PTr),e(ko,$Tr),g(Sw,ko,null),b(d,lLe,u),b(d,Zc,u),e(Zc,uF),e(uF,z1e),g(Pw,z1e,null),e(Zc,ITr),e(Zc,V1e),e(V1e,jTr),b(d,iLe,u),b(d,Rr,u),g($w,Rr,null),e(Rr,NTr),e(Rr,ef),e(ef,DTr),e(ef,W1e),e(W1e,qTr),e(ef,GTr),e(ef,Q1e),e(Q1e,OTr),e(ef,XTr),e(Rr,zTr),e(Rr,Iw),e(Iw,VTr),e(Iw,H1e),e(H1e,WTr),e(Iw,QTr),e(Rr,HTr),e(Rr,Et),g(jw,Et,null),e(Et,UTr),e(Et,U1e),e(U1e,JTr),e(Et,YTr),e(Et,of),e(of,KTr),e(of,J1e),e(J1e,ZTr),e(of,e7r),e(of,Y1e),e(Y1e,o7r),e(of,r7r),e(Et,t7r),e(Et,K1e),e(K1e,a7r),e(Et,n7r),g(Nw,Et,null),e(Rr,s7r),e(Rr,Ro),g(Dw,Ro,null),e(Ro,l7r),e(Ro,Z1e),e(Z1e,i7r),e(Ro,d7r),e(Ro,yn),e(yn,c7r),e(yn,ebe),e(ebe,f7r),e(yn,m7r),e(yn,obe),e(obe,g7r),e(yn,h7r),e(yn,rbe),e(rbe,p7r),e(yn,_7r),e(Ro,u7r),e(Ro,no),e(no,bF),e(bF,tbe),e(tbe,b7r),e(bF,v7r),e(bF,DO),e(DO,T7r),e(bF,F7r),e(no,C7r),e(no,vF),e(vF,abe),e(abe,M7r),e(vF,E7r),e(vF,qO),e(qO,y7r),e(vF,w7r),e(no,A7r),e(no,TF),e(TF,nbe),e(nbe,L7r),e(TF,B7r),e(TF,GO),e(GO,x7r),e(TF,k7r),e(no,R7r),e(no,FF),e(FF,sbe),e(sbe,S7r),e(FF,P7r),e(FF,OO),e(OO,$7r),e(FF,I7r),e(no,j7r),e(no,CF),e(CF,lbe),e(lbe,N7r),e(CF,D7r),e(CF,XO),e(XO,q7r),e(CF,G7r),e(no,O7r),e(no,MF),e(MF,ibe),e(ibe,X7r),e(MF,z7r),e(MF,zO),e(zO,V7r),e(MF,W7r),e(no,Q7r),e(no,EF),e(EF,dbe),e(dbe,H7r),e(EF,U7r),e(EF,VO),e(VO,J7r),e(EF,Y7r),e(Ro,K7r),e(Ro,cbe),e(cbe,Z7r),e(Ro,e8r),g(qw,Ro,null),b(d,dLe,u),b(d,rf,u),e(rf,yF),e(yF,fbe),g(Gw,fbe,null),e(rf,o8r),e(rf,mbe),e(mbe,r8r),b(d,cLe,u),b(d,Sr,u),g(Ow,Sr,null),e(Sr,t8r),e(Sr,tf),e(tf,a8r),e(tf,gbe),e(gbe,n8r),e(tf,s8r),e(tf,hbe),e(hbe,l8r),e(tf,i8r),e(Sr,d8r),e(Sr,Xw),e(Xw,c8r),e(Xw,pbe),e(pbe,f8r),e(Xw,m8r),e(Sr,g8r),e(Sr,yt),g(zw,yt,null),e(yt,h8r),e(yt,_be),e(_be,p8r),e(yt,_8r),e(yt,af),e(af,u8r),e(af,ube),e(ube,b8r),e(af,v8r),e(af,bbe),e(bbe,T8r),e(af,F8r),e(yt,C8r),e(yt,vbe),e(vbe,M8r),e(yt,E8r),g(Vw,yt,null),e(Sr,y8r),e(Sr,So),g(Ww,So,null),e(So,w8r),e(So,Tbe),e(Tbe,A8r),e(So,L8r),e(So,wn),e(wn,B8r),e(wn,Fbe),e(Fbe,x8r),e(wn,k8r),e(wn,Cbe),e(Cbe,R8r),e(wn,S8r),e(wn,Mbe),e(Mbe,P8r),e(wn,$8r),e(So,I8r),e(So,Ebe),e(Ebe,wF),e(wF,ybe),e(ybe,j8r),e(wF,N8r),e(wF,WO),e(WO,D8r),e(wF,q8r),e(So,G8r),e(So,wbe),e(wbe,O8r),e(So,X8r),g(Qw,So,null),b(d,fLe,u),b(d,nf,u),e(nf,AF),e(AF,Abe),g(Hw,Abe,null),e(nf,z8r),e(nf,Lbe),e(Lbe,V8r),b(d,mLe,u),b(d,Pr,u),g(Uw,Pr,null),e(Pr,W8r),e(Pr,sf),e(sf,Q8r),e(sf,Bbe),e(Bbe,H8r),e(sf,U8r),e(sf,xbe),e(xbe,J8r),e(sf,Y8r),e(Pr,K8r),e(Pr,Jw),e(Jw,Z8r),e(Jw,kbe),e(kbe,eFr),e(Jw,oFr),e(Pr,rFr),e(Pr,wt),g(Yw,wt,null),e(wt,tFr),e(wt,Rbe),e(Rbe,aFr),e(wt,nFr),e(wt,lf),e(lf,sFr),e(lf,Sbe),e(Sbe,lFr),e(lf,iFr),e(lf,Pbe),e(Pbe,dFr),e(lf,cFr),e(wt,fFr),e(wt,$be),e($be,mFr),e(wt,gFr),g(Kw,wt,null),e(Pr,hFr),e(Pr,Po),g(Zw,Po,null),e(Po,pFr),e(Po,Ibe),e(Ibe,_Fr),e(Po,uFr),e(Po,An),e(An,bFr),e(An,jbe),e(jbe,vFr),e(An,TFr),e(An,Nbe),e(Nbe,FFr),e(An,CFr),e(An,Dbe),e(Dbe,MFr),e(An,EFr),e(Po,yFr),e(Po,eA),e(eA,LF),e(LF,qbe),e(qbe,wFr),e(LF,AFr),e(LF,QO),e(QO,LFr),e(LF,BFr),e(eA,xFr),e(eA,BF),e(BF,Gbe),e(Gbe,kFr),e(BF,RFr),e(BF,HO),e(HO,SFr),e(BF,PFr),e(Po,$Fr),e(Po,Obe),e(Obe,IFr),e(Po,jFr),g(oA,Po,null),b(d,gLe,u),b(d,df,u),e(df,xF),e(xF,Xbe),g(rA,Xbe,null),e(df,NFr),e(df,zbe),e(zbe,DFr),b(d,hLe,u),b(d,$r,u),g(tA,$r,null),e($r,qFr),e($r,cf),e(cf,GFr),e(cf,Vbe),e(Vbe,OFr),e(cf,XFr),e(cf,Wbe),e(Wbe,zFr),e(cf,VFr),e($r,WFr),e($r,aA),e(aA,QFr),e(aA,Qbe),e(Qbe,HFr),e(aA,UFr),e($r,JFr),e($r,At),g(nA,At,null),e(At,YFr),e(At,Hbe),e(Hbe,KFr),e(At,ZFr),e(At,ff),e(ff,eCr),e(ff,Ube),e(Ube,oCr),e(ff,rCr),e(ff,Jbe),e(Jbe,tCr),e(ff,aCr),e(At,nCr),e(At,Ybe),e(Ybe,sCr),e(At,lCr),g(sA,At,null),e($r,iCr),e($r,$o),g(lA,$o,null),e($o,dCr),e($o,Kbe),e(Kbe,cCr),e($o,fCr),e($o,Ln),e(Ln,mCr),e(Ln,Zbe),e(Zbe,gCr),e(Ln,hCr),e(Ln,e5e),e(e5e,pCr),e(Ln,_Cr),e(Ln,o5e),e(o5e,uCr),e(Ln,bCr),e($o,vCr),e($o,r5e),e(r5e,kF),e(kF,t5e),e(t5e,TCr),e(kF,FCr),e(kF,UO),e(UO,CCr),e(kF,MCr),e($o,ECr),e($o,a5e),e(a5e,yCr),e($o,wCr),g(iA,$o,null),pLe=!0},p(d,[u]){const dA={};u&2&&(dA.$$scope={dirty:u,ctx:d}),bf.$set(dA);const n5e={};u&2&&(n5e.$$scope={dirty:u,ctx:d}),Qg.$set(n5e);const s5e={};u&2&&(s5e.$$scope={dirty:u,ctx:d}),ah.$set(s5e)},i(d){pLe||(h(ce.$$.fragment,d),h(ka.$$.fragment,d),h(RC.$$.fragment,d),h(SC.$$.fragment,d),h(bf.$$.fragment,d),h(PC.$$.fragment,d),h($C.$$.fragment,d),h(NC.$$.fragment,d),h(DC.$$.fragment,d),h(qC.$$.fragment,d),h(GC.$$.fragment,d),h(OC.$$.fragment,d),h(VC.$$.fragment,d),h(WC.$$.fragment,d),h(QC.$$.fragment,d),h(HC.$$.fragment,d),h(UC.$$.fragment,d),h(KC.$$.fragment,d),h(Qg.$$.fragment,d),h(ZC.$$.fragment,d),h(e4.$$.fragment,d),h(o4.$$.fragment,d),h(r4.$$.fragment,d),h(n4.$$.fragment,d),h(ah.$$.fragment,d),h(s4.$$.fragment,d),h(l4.$$.fragment,d),h(i4.$$.fragment,d),h(d4.$$.fragment,d),h(f4.$$.fragment,d),h(m4.$$.fragment,d),h(g4.$$.fragment,d),h(h4.$$.fragment,d),h(p4.$$.fragment,d),h(_4.$$.fragment,d),h(b4.$$.fragment,d),h(v4.$$.fragment,d),h(T4.$$.fragment,d),h(F4.$$.fragment,d),h(C4.$$.fragment,d),h(M4.$$.fragment,d),h(y4.$$.fragment,d),h(w4.$$.fragment,d),h(A4.$$.fragment,d),h(L4.$$.fragment,d),h(B4.$$.fragment,d),h(x4.$$.fragment,d),h(R4.$$.fragment,d),h(S4.$$.fragment,d),h(P4.$$.fragment,d),h($4.$$.fragment,d),h(I4.$$.fragment,d),h(j4.$$.fragment,d),h(D4.$$.fragment,d),h(q4.$$.fragment,d),h(G4.$$.fragment,d),h(O4.$$.fragment,d),h(X4.$$.fragment,d),h(z4.$$.fragment,d),h(W4.$$.fragment,d),h(Q4.$$.fragment,d),h(H4.$$.fragment,d),h(U4.$$.fragment,d),h(J4.$$.fragment,d),h(Y4.$$.fragment,d),h(Z4.$$.fragment,d),h(eM.$$.fragment,d),h(oM.$$.fragment,d),h(rM.$$.fragment,d),h(tM.$$.fragment,d),h(aM.$$.fragment,d),h(sM.$$.fragment,d),h(lM.$$.fragment,d),h(iM.$$.fragment,d),h(dM.$$.fragment,d),h(cM.$$.fragment,d),h(fM.$$.fragment,d),h(gM.$$.fragment,d),h(hM.$$.fragment,d),h(pM.$$.fragment,d),h(_M.$$.fragment,d),h(uM.$$.fragment,d),h(bM.$$.fragment,d),h(TM.$$.fragment,d),h(FM.$$.fragment,d),h(CM.$$.fragment,d),h(MM.$$.fragment,d),h(EM.$$.fragment,d),h(yM.$$.fragment,d),h(AM.$$.fragment,d),h(LM.$$.fragment,d),h(BM.$$.fragment,d),h(xM.$$.fragment,d),h(kM.$$.fragment,d),h(RM.$$.fragment,d),h(PM.$$.fragment,d),h($M.$$.fragment,d),h(IM.$$.fragment,d),h(jM.$$.fragment,d),h(NM.$$.fragment,d),h(DM.$$.fragment,d),h(GM.$$.fragment,d),h(OM.$$.fragment,d),h(XM.$$.fragment,d),h(zM.$$.fragment,d),h(VM.$$.fragment,d),h(WM.$$.fragment,d),h(HM.$$.fragment,d),h(UM.$$.fragment,d),h(JM.$$.fragment,d),h(YM.$$.fragment,d),h(KM.$$.fragment,d),h(ZM.$$.fragment,d),h(oE.$$.fragment,d),h(rE.$$.fragment,d),h(tE.$$.fragment,d),h(aE.$$.fragment,d),h(nE.$$.fragment,d),h(sE.$$.fragment,d),h(iE.$$.fragment,d),h(dE.$$.fragment,d),h(cE.$$.fragment,d),h(fE.$$.fragment,d),h(mE.$$.fragment,d),h(gE.$$.fragment,d),h(pE.$$.fragment,d),h(_E.$$.fragment,d),h(uE.$$.fragment,d),h(vE.$$.fragment,d),h(TE.$$.fragment,d),h(FE.$$.fragment,d),h(ME.$$.fragment,d),h(EE.$$.fragment,d),h(yE.$$.fragment,d),h(wE.$$.fragment,d),h(AE.$$.fragment,d),h(LE.$$.fragment,d),h(xE.$$.fragment,d),h(kE.$$.fragment,d),h(RE.$$.fragment,d),h(SE.$$.fragment,d),h(PE.$$.fragment,d),h($E.$$.fragment,d),h(jE.$$.fragment,d),h(NE.$$.fragment,d),h(DE.$$.fragment,d),h(qE.$$.fragment,d),h(GE.$$.fragment,d),h(OE.$$.fragment,d),h(zE.$$.fragment,d),h(VE.$$.fragment,d),h(WE.$$.fragment,d),h(HE.$$.fragment,d),h(UE.$$.fragment,d),h(JE.$$.fragment,d),h(KE.$$.fragment,d),h(ZE.$$.fragment,d),h(e3.$$.fragment,d),h(o3.$$.fragment,d),h(r3.$$.fragment,d),h(t3.$$.fragment,d),h(n3.$$.fragment,d),h(s3.$$.fragment,d),h(l3.$$.fragment,d),h(i3.$$.fragment,d),h(d3.$$.fragment,d),h(c3.$$.fragment,d),h(m3.$$.fragment,d),h(g3.$$.fragment,d),h(h3.$$.fragment,d),h(p3.$$.fragment,d),h(_3.$$.fragment,d),h(u3.$$.fragment,d),h(v3.$$.fragment,d),h(T3.$$.fragment,d),h(F3.$$.fragment,d),h(C3.$$.fragment,d),h(M3.$$.fragment,d),h(E3.$$.fragment,d),h(w3.$$.fragment,d),h(A3.$$.fragment,d),h(L3.$$.fragment,d),h(B3.$$.fragment,d),h(x3.$$.fragment,d),h(k3.$$.fragment,d),h(S3.$$.fragment,d),h(P3.$$.fragment,d),h($3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(N3.$$.fragment,d),h(q3.$$.fragment,d),h(G3.$$.fragment,d),h(O3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(V3.$$.fragment,d),h(Q3.$$.fragment,d),h(H3.$$.fragment,d),h(U3.$$.fragment,d),h(J3.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(ny.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(my.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(uy.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(xy.$$.fragment,d),h(ky.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(qy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(rw.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(nw.$$.fragment,d),h(sw.$$.fragment,d),h(lw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(fw.$$.fragment,d),h(mw.$$.fragment,d),h(gw.$$.fragment,d),h(hw.$$.fragment,d),h(_w.$$.fragment,d),h(uw.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Lw.$$.fragment,d),h(xw.$$.fragment,d),h(kw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Ow.$$.fragment,d),h(zw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(oA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(nA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),pLe=!0)},o(d){p(ce.$$.fragment,d),p(ka.$$.fragment,d),p(RC.$$.fragment,d),p(SC.$$.fragment,d),p(bf.$$.fragment,d),p(PC.$$.fragment,d),p($C.$$.fragment,d),p(NC.$$.fragment,d),p(DC.$$.fragment,d),p(qC.$$.fragment,d),p(GC.$$.fragment,d),p(OC.$$.fragment,d),p(VC.$$.fragment,d),p(WC.$$.fragment,d),p(QC.$$.fragment,d),p(HC.$$.fragment,d),p(UC.$$.fragment,d),p(KC.$$.fragment,d),p(Qg.$$.fragment,d),p(ZC.$$.fragment,d),p(e4.$$.fragment,d),p(o4.$$.fragment,d),p(r4.$$.fragment,d),p(n4.$$.fragment,d),p(ah.$$.fragment,d),p(s4.$$.fragment,d),p(l4.$$.fragment,d),p(i4.$$.fragment,d),p(d4.$$.fragment,d),p(f4.$$.fragment,d),p(m4.$$.fragment,d),p(g4.$$.fragment,d),p(h4.$$.fragment,d),p(p4.$$.fragment,d),p(_4.$$.fragment,d),p(b4.$$.fragment,d),p(v4.$$.fragment,d),p(T4.$$.fragment,d),p(F4.$$.fragment,d),p(C4.$$.fragment,d),p(M4.$$.fragment,d),p(y4.$$.fragment,d),p(w4.$$.fragment,d),p(A4.$$.fragment,d),p(L4.$$.fragment,d),p(B4.$$.fragment,d),p(x4.$$.fragment,d),p(R4.$$.fragment,d),p(S4.$$.fragment,d),p(P4.$$.fragment,d),p($4.$$.fragment,d),p(I4.$$.fragment,d),p(j4.$$.fragment,d),p(D4.$$.fragment,d),p(q4.$$.fragment,d),p(G4.$$.fragment,d),p(O4.$$.fragment,d),p(X4.$$.fragment,d),p(z4.$$.fragment,d),p(W4.$$.fragment,d),p(Q4.$$.fragment,d),p(H4.$$.fragment,d),p(U4.$$.fragment,d),p(J4.$$.fragment,d),p(Y4.$$.fragment,d),p(Z4.$$.fragment,d),p(eM.$$.fragment,d),p(oM.$$.fragment,d),p(rM.$$.fragment,d),p(tM.$$.fragment,d),p(aM.$$.fragment,d),p(sM.$$.fragment,d),p(lM.$$.fragment,d),p(iM.$$.fragment,d),p(dM.$$.fragment,d),p(cM.$$.fragment,d),p(fM.$$.fragment,d),p(gM.$$.fragment,d),p(hM.$$.fragment,d),p(pM.$$.fragment,d),p(_M.$$.fragment,d),p(uM.$$.fragment,d),p(bM.$$.fragment,d),p(TM.$$.fragment,d),p(FM.$$.fragment,d),p(CM.$$.fragment,d),p(MM.$$.fragment,d),p(EM.$$.fragment,d),p(yM.$$.fragment,d),p(AM.$$.fragment,d),p(LM.$$.fragment,d),p(BM.$$.fragment,d),p(xM.$$.fragment,d),p(kM.$$.fragment,d),p(RM.$$.fragment,d),p(PM.$$.fragment,d),p($M.$$.fragment,d),p(IM.$$.fragment,d),p(jM.$$.fragment,d),p(NM.$$.fragment,d),p(DM.$$.fragment,d),p(GM.$$.fragment,d),p(OM.$$.fragment,d),p(XM.$$.fragment,d),p(zM.$$.fragment,d),p(VM.$$.fragment,d),p(WM.$$.fragment,d),p(HM.$$.fragment,d),p(UM.$$.fragment,d),p(JM.$$.fragment,d),p(YM.$$.fragment,d),p(KM.$$.fragment,d),p(ZM.$$.fragment,d),p(oE.$$.fragment,d),p(rE.$$.fragment,d),p(tE.$$.fragment,d),p(aE.$$.fragment,d),p(nE.$$.fragment,d),p(sE.$$.fragment,d),p(iE.$$.fragment,d),p(dE.$$.fragment,d),p(cE.$$.fragment,d),p(fE.$$.fragment,d),p(mE.$$.fragment,d),p(gE.$$.fragment,d),p(pE.$$.fragment,d),p(_E.$$.fragment,d),p(uE.$$.fragment,d),p(vE.$$.fragment,d),p(TE.$$.fragment,d),p(FE.$$.fragment,d),p(ME.$$.fragment,d),p(EE.$$.fragment,d),p(yE.$$.fragment,d),p(wE.$$.fragment,d),p(AE.$$.fragment,d),p(LE.$$.fragment,d),p(xE.$$.fragment,d),p(kE.$$.fragment,d),p(RE.$$.fragment,d),p(SE.$$.fragment,d),p(PE.$$.fragment,d),p($E.$$.fragment,d),p(jE.$$.fragment,d),p(NE.$$.fragment,d),p(DE.$$.fragment,d),p(qE.$$.fragment,d),p(GE.$$.fragment,d),p(OE.$$.fragment,d),p(zE.$$.fragment,d),p(VE.$$.fragment,d),p(WE.$$.fragment,d),p(HE.$$.fragment,d),p(UE.$$.fragment,d),p(JE.$$.fragment,d),p(KE.$$.fragment,d),p(ZE.$$.fragment,d),p(e3.$$.fragment,d),p(o3.$$.fragment,d),p(r3.$$.fragment,d),p(t3.$$.fragment,d),p(n3.$$.fragment,d),p(s3.$$.fragment,d),p(l3.$$.fragment,d),p(i3.$$.fragment,d),p(d3.$$.fragment,d),p(c3.$$.fragment,d),p(m3.$$.fragment,d),p(g3.$$.fragment,d),p(h3.$$.fragment,d),p(p3.$$.fragment,d),p(_3.$$.fragment,d),p(u3.$$.fragment,d),p(v3.$$.fragment,d),p(T3.$$.fragment,d),p(F3.$$.fragment,d),p(C3.$$.fragment,d),p(M3.$$.fragment,d),p(E3.$$.fragment,d),p(w3.$$.fragment,d),p(A3.$$.fragment,d),p(L3.$$.fragment,d),p(B3.$$.fragment,d),p(x3.$$.fragment,d),p(k3.$$.fragment,d),p(S3.$$.fragment,d),p(P3.$$.fragment,d),p($3.$$.fragment,d),p(I3.$$.fragment,d),p(j3.$$.fragment,d),p(N3.$$.fragment,d),p(q3.$$.fragment,d),p(G3.$$.fragment,d),p(O3.$$.fragment,d),p(X3.$$.fragment,d),p(z3.$$.fragment,d),p(V3.$$.fragment,d),p(Q3.$$.fragment,d),p(H3.$$.fragment,d),p(U3.$$.fragment,d),p(J3.$$.fragment,d),p(Y3.$$.fragment,d),p(K3.$$.fragment,d),p(ey.$$.fragment,d),p(oy.$$.fragment,d),p(ry.$$.fragment,d),p(ty.$$.fragment,d),p(ay.$$.fragment,d),p(ny.$$.fragment,d),p(ly.$$.fragment,d),p(iy.$$.fragment,d),p(dy.$$.fragment,d),p(cy.$$.fragment,d),p(fy.$$.fragment,d),p(my.$$.fragment,d),p(hy.$$.fragment,d),p(py.$$.fragment,d),p(_y.$$.fragment,d),p(uy.$$.fragment,d),p(by.$$.fragment,d),p(vy.$$.fragment,d),p(Fy.$$.fragment,d),p(Cy.$$.fragment,d),p(My.$$.fragment,d),p(Ey.$$.fragment,d),p(yy.$$.fragment,d),p(wy.$$.fragment,d),p(Ly.$$.fragment,d),p(By.$$.fragment,d),p(xy.$$.fragment,d),p(ky.$$.fragment,d),p(Ry.$$.fragment,d),p(Sy.$$.fragment,d),p($y.$$.fragment,d),p(Iy.$$.fragment,d),p(jy.$$.fragment,d),p(Ny.$$.fragment,d),p(Dy.$$.fragment,d),p(qy.$$.fragment,d),p(Oy.$$.fragment,d),p(Xy.$$.fragment,d),p(zy.$$.fragment,d),p(Vy.$$.fragment,d),p(Wy.$$.fragment,d),p(Qy.$$.fragment,d),p(Uy.$$.fragment,d),p(Jy.$$.fragment,d),p(Yy.$$.fragment,d),p(Ky.$$.fragment,d),p(Zy.$$.fragment,d),p(ew.$$.fragment,d),p(rw.$$.fragment,d),p(tw.$$.fragment,d),p(aw.$$.fragment,d),p(nw.$$.fragment,d),p(sw.$$.fragment,d),p(lw.$$.fragment,d),p(dw.$$.fragment,d),p(cw.$$.fragment,d),p(fw.$$.fragment,d),p(mw.$$.fragment,d),p(gw.$$.fragment,d),p(hw.$$.fragment,d),p(_w.$$.fragment,d),p(uw.$$.fragment,d),p(bw.$$.fragment,d),p(vw.$$.fragment,d),p(Tw.$$.fragment,d),p(Fw.$$.fragment,d),p(Mw.$$.fragment,d),p(Ew.$$.fragment,d),p(yw.$$.fragment,d),p(ww.$$.fragment,d),p(Aw.$$.fragment,d),p(Lw.$$.fragment,d),p(xw.$$.fragment,d),p(kw.$$.fragment,d),p(Rw.$$.fragment,d),p(Sw.$$.fragment,d),p(Pw.$$.fragment,d),p($w.$$.fragment,d),p(jw.$$.fragment,d),p(Nw.$$.fragment,d),p(Dw.$$.fragment,d),p(qw.$$.fragment,d),p(Gw.$$.fragment,d),p(Ow.$$.fragment,d),p(zw.$$.fragment,d),p(Vw.$$.fragment,d),p(Ww.$$.fragment,d),p(Qw.$$.fragment,d),p(Hw.$$.fragment,d),p(Uw.$$.fragment,d),p(Yw.$$.fragment,d),p(Kw.$$.fragment,d),p(Zw.$$.fragment,d),p(oA.$$.fragment,d),p(rA.$$.fragment,d),p(tA.$$.fragment,d),p(nA.$$.fragment,d),p(sA.$$.fragment,d),p(lA.$$.fragment,d),p(iA.$$.fragment,d),pLe=!1},d(d){t(J),d&&t(Ae),d&&t(le),_(ce),d&&t(gf),d&&t(ra),d&&t(Ee),d&&t(so),d&&t(pf),_(ka,d),d&&t(lo),d&&t(ge),d&&t(Do),d&&t(Ra),d&&t(bAe),d&&t(wi),_(RC),d&&t(vAe),d&&t(Sn),d&&t(TAe),_(SC,d),d&&t(FAe),d&&t(c0),d&&t(CAe),_(bf,d),d&&t(MAe),d&&t(Ai),_(PC),d&&t(EAe),d&&t(qo),_($C),_(NC),_(DC),_(qC),d&&t(yAe),d&&t(Bi),_(GC),d&&t(wAe),d&&t(Go),_(OC),_(VC),_(WC),_(QC),d&&t(AAe),d&&t(xi),_(HC),d&&t(LAe),d&&t(Oo),_(UC),_(KC),_(Qg),_(ZC),_(e4),d&&t(BAe),d&&t(ki),_(o4),d&&t(xAe),d&&t(Xo),_(r4),_(n4),_(ah),_(s4),_(l4),d&&t(kAe),d&&t(Si),_(i4),d&&t(RAe),d&&t(zo),_(d4),_(f4),_(m4),_(g4),_(h4),d&&t(SAe),d&&t(Ii),_(p4),d&&t(PAe),d&&t(Vo),_(_4),_(b4),_(v4),_(T4),_(F4),d&&t($Ae),d&&t(Di),_(C4),d&&t(IAe),d&&t(Wo),_(M4),_(y4),_(w4),_(A4),_(L4),d&&t(jAe),d&&t(Oi),_(B4),d&&t(NAe),d&&t(Qo),_(x4),_(R4),_(S4),_(P4),_($4),d&&t(DAe),d&&t(Vi),_(I4),d&&t(qAe),d&&t(Ho),_(j4),_(D4),_(q4),_(G4),_(O4),d&&t(GAe),d&&t(Hi),_(X4),d&&t(OAe),d&&t(Uo),_(z4),_(W4),_(Q4),_(H4),_(U4),d&&t(XAe),d&&t(Yi),_(J4),d&&t(zAe),d&&t(Jo),_(Y4),_(Z4),_(eM),_(oM),_(rM),d&&t(VAe),d&&t(ed),_(tM),d&&t(WAe),d&&t(Yo),_(aM),_(sM),_(lM),_(iM),_(dM),d&&t(QAe),d&&t(td),_(cM),d&&t(HAe),d&&t(Ko),_(fM),_(gM),_(hM),_(pM),_(_M),d&&t(UAe),d&&t(sd),_(uM),d&&t(JAe),d&&t(Zo),_(bM),_(TM),_(FM),_(CM),_(MM),d&&t(YAe),d&&t(dd),_(EM),d&&t(KAe),d&&t(er),_(yM),_(AM),_(LM),_(BM),_(xM),d&&t(ZAe),d&&t(md),_(kM),d&&t(e0e),d&&t(or),_(RM),_(PM),_($M),_(IM),_(jM),d&&t(o0e),d&&t(pd),_(NM),d&&t(r0e),d&&t(rr),_(DM),_(GM),_(OM),_(XM),_(zM),d&&t(t0e),d&&t(bd),_(VM),d&&t(a0e),d&&t(tr),_(WM),_(HM),_(UM),_(JM),_(YM),d&&t(n0e),d&&t(Fd),_(KM),d&&t(s0e),d&&t(ar),_(ZM),_(oE),_(rE),_(tE),_(aE),d&&t(l0e),d&&t(yd),_(nE),d&&t(i0e),d&&t(nr),_(sE),_(iE),_(dE),_(cE),_(fE),d&&t(d0e),d&&t(Ld),_(mE),d&&t(c0e),d&&t(sr),_(gE),_(pE),_(_E),_(uE),_(vE),d&&t(f0e),d&&t(kd),_(TE),d&&t(m0e),d&&t(lr),_(FE),_(ME),_(EE),_(yE),_(wE),d&&t(g0e),d&&t($d),_(AE),d&&t(h0e),d&&t(ir),_(LE),_(xE),_(kE),_(RE),_(SE),d&&t(p0e),d&&t(Nd),_(PE),d&&t(_0e),d&&t(dr),_($E),_(jE),_(NE),_(DE),_(qE),d&&t(u0e),d&&t(Gd),_(GE),d&&t(b0e),d&&t(cr),_(OE),_(zE),_(VE),_(WE),_(HE),d&&t(v0e),d&&t(zd),_(UE),d&&t(T0e),d&&t(fr),_(JE),_(KE),_(ZE),_(e3),_(o3),d&&t(F0e),d&&t(Qd),_(r3),d&&t(C0e),d&&t(mr),_(t3),_(n3),_(s3),_(l3),_(i3),d&&t(M0e),d&&t(Jd),_(d3),d&&t(E0e),d&&t(gr),_(c3),_(m3),_(g3),_(h3),_(p3),d&&t(y0e),d&&t(Zd),_(_3),d&&t(w0e),d&&t(hr),_(u3),_(v3),_(T3),_(F3),_(C3),d&&t(A0e),d&&t(rc),_(M3),d&&t(L0e),d&&t(pr),_(E3),_(w3),_(A3),_(L3),_(B3),d&&t(B0e),d&&t(nc),_(x3),d&&t(x0e),d&&t(_r),_(k3),_(S3),_(P3),_($3),_(I3),d&&t(k0e),d&&t(ic),_(j3),d&&t(R0e),d&&t(ur),_(N3),_(q3),_(G3),_(O3),_(X3),d&&t(S0e),d&&t(fc),_(z3),d&&t(P0e),d&&t(br),_(V3),_(Q3),_(H3),_(U3),_(J3),d&&t($0e),d&&t(hc),_(Y3),d&&t(I0e),d&&t(vr),_(K3),_(ey),_(oy),_(ry),_(ty),d&&t(j0e),d&&t(uc),_(ay),d&&t(N0e),d&&t(Tr),_(ny),_(ly),_(iy),_(dy),_(cy),d&&t(D0e),d&&t(Tc),_(fy),d&&t(q0e),d&&t(Fr),_(my),_(hy),_(py),_(_y),_(uy),d&&t(G0e),d&&t(Mc),_(by),d&&t(O0e),d&&t(Cr),_(vy),_(Fy),_(Cy),_(My),_(Ey),d&&t(X0e),d&&t(wc),_(yy),d&&t(z0e),d&&t(Mr),_(wy),_(Ly),_(By),_(xy),_(ky),d&&t(V0e),d&&t(Bc),_(Ry),d&&t(W0e),d&&t(Er),_(Sy),_($y),_(Iy),_(jy),_(Ny),d&&t(Q0e),d&&t(Rc),_(Dy),d&&t(H0e),d&&t(yr),_(qy),_(Oy),_(Xy),_(zy),_(Vy),d&&t(U0e),d&&t($c),_(Wy),d&&t(J0e),d&&t(wr),_(Qy),_(Uy),_(Jy),_(Yy),_(Ky),d&&t(Y0e),d&&t(Nc),_(Zy),d&&t(K0e),d&&t(Ar),_(ew),_(rw),_(tw),_(aw),_(nw),d&&t(Z0e),d&&t(Gc),_(sw),d&&t(eLe),d&&t(Lr),_(lw),_(dw),_(cw),_(fw),_(mw),d&&t(oLe),d&&t(zc),_(gw),d&&t(rLe),d&&t(Br),_(hw),_(_w),_(uw),_(bw),_(vw),d&&t(tLe),d&&t(Qc),_(Tw),d&&t(aLe),d&&t(xr),_(Fw),_(Mw),_(Ew),_(yw),_(ww),d&&t(nLe),d&&t(Jc),_(Aw),d&&t(sLe),d&&t(kr),_(Lw),_(xw),_(kw),_(Rw),_(Sw),d&&t(lLe),d&&t(Zc),_(Pw),d&&t(iLe),d&&t(Rr),_($w),_(jw),_(Nw),_(Dw),_(qw),d&&t(dLe),d&&t(rf),_(Gw),d&&t(cLe),d&&t(Sr),_(Ow),_(zw),_(Vw),_(Ww),_(Qw),d&&t(fLe),d&&t(nf),_(Hw),d&&t(mLe),d&&t(Pr),_(Uw),_(Yw),_(Kw),_(Zw),_(oA),d&&t(gLe),d&&t(df),_(rA),d&&t(hLe),d&&t($r),_(tA),_(nA),_(sA),_(lA),_(iA)}}}const Qft={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function Hft(bi,J,Ae){let{fw:le}=J;return bi.$$set=me=>{"fw"in me&&Ae(0,le=me.fw)},[le]}class omt extends Dft{constructor(J){super();qft(this,J,Hft,Wft,Gft,{fw:0})}}export{omt as default,Qft as metadata};
