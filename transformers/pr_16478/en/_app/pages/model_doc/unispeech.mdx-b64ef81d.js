import{S as Si,i as Ti,s as Ui,e as n,k as l,w as _,t as r,M as ki,c as a,d as o,m as d,a as s,x as v,h as i,b as c,F as e,g as h,y as b,q as w,o as y,B as S,v as $i}from"../../chunks/vendor-6b77c823.js";import{T as wn}from"../../chunks/Tip-39098574.js";import{D as W}from"../../chunks/Docstring-abef54e3.js";import{C as Tt}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Te}from"../../chunks/IconCopyLink-7a11ce68.js";function Ci(L){let m,k,u,T,U;return{c(){m=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){m=a(f,"P",{});var g=s(m);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){h(f,m,g),e(m,k),e(m,u),e(u,T),e(m,U)},d(f){f&&o(m)}}}function ji(L){let m,k,u,T,U;return{c(){m=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){m=a(f,"P",{});var g=s(m);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){h(f,m,g),e(m,k),e(m,u),e(u,T),e(m,U)},d(f){f&&o(m)}}}function xi(L){let m,k,u,T,U;return{c(){m=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){m=a(f,"P",{});var g=s(m);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){h(f,m,g),e(m,k),e(m,u),e(u,T),e(m,U)},d(f){f&&o(m)}}}function Fi(L){let m,k,u,T,U;return{c(){m=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){m=a(f,"P",{});var g=s(m);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){h(f,m,g),e(m,k),e(m,u),e(u,T),e(m,U)},d(f){f&&o(m)}}}function qi(L){let m,k,u,T,U,f,g,$,yn,$o,R,de,Rt,Ue,Sn,Qt,Tn,Co,pe,Un,ke,kn,$n,jo,Ut,Cn,xo,kt,Xt,jn,Fo,$t,xn,qo,he,$e,Fn,Ct,qn,Pn,En,Ce,Mn,jt,zn,Dn,Po,N,An,je,On,Wn,xe,Ln,Nn,Eo,Q,me,Zt,Fe,Vn,Jt,In,Mo,F,qe,Bn,X,Hn,xt,Kn,Yn,Pe,Rn,Qn,Xn,Z,Zn,Ft,Jn,Gn,qt,ea,ta,oa,Gt,na,aa,Ee,zo,J,ue,eo,Me,sa,to,ra,Do,G,ze,ia,De,ca,oo,la,da,Ao,ee,Ae,pa,Oe,ha,no,ma,ua,Oo,te,fe,ao,We,fa,so,ga,Wo,q,Le,_a,Ne,va,Ve,ba,wa,ya,Ie,Sa,Pt,Ta,Ua,ka,Be,$a,He,Ca,ja,xa,M,Ke,Fa,oe,qa,Et,Pa,Ea,ro,Ma,za,Da,ge,Aa,io,Oa,Wa,Ye,Lo,ne,_e,co,Re,La,lo,Na,No,P,Qe,Va,ae,Ia,po,Ba,Ha,Xe,Ka,Ya,Ra,Ze,Qa,Mt,Xa,Za,Ja,Je,Ga,Ge,es,ts,os,j,et,ns,se,as,zt,ss,rs,ho,is,cs,ls,ve,ds,mo,ps,hs,tt,ms,ot,Vo,re,be,uo,nt,us,fo,fs,Io,C,at,gs,go,_s,vs,st,bs,rt,ws,ys,Ss,it,Ts,Dt,Us,ks,$s,ct,Cs,lt,js,xs,Fs,x,dt,qs,ie,Ps,At,Es,Ms,_o,zs,Ds,As,we,Os,vo,Ws,Ls,pt,Ns,ht,Bo,ce,ye,bo,mt,Vs,wo,Is,Ho,E,ut,Bs,ft,Hs,gt,Ks,Ys,Rs,_t,Qs,Ot,Xs,Zs,Js,vt,Gs,bt,er,tr,or,z,wt,nr,le,ar,Wt,sr,rr,yo,ir,cr,lr,Se,dr,So,pr,hr,yt,Ko;return f=new Te({}),Ue=new Te({}),Fe=new Te({}),qe=new W({props:{name:"class transformers.UniSpeechConfig",anchor:"transformers.UniSpeechConfig",parameters:[{name:"vocab_size",val:" = 32"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"feat_proj_dropout",val:" = 0.0"},{name:"feat_quantizer_dropout",val:" = 0.0"},{name:"final_dropout",val:" = 0.1"},{name:"layerdrop",val:" = 0.1"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"feat_extract_norm",val:" = 'group'"},{name:"feat_extract_activation",val:" = 'gelu'"},{name:"conv_dim",val:" = (512, 512, 512, 512, 512, 512, 512)"},{name:"conv_stride",val:" = (5, 2, 2, 2, 2, 2, 2)"},{name:"conv_kernel",val:" = (10, 3, 3, 3, 3, 2, 2)"},{name:"conv_bias",val:" = False"},{name:"num_conv_pos_embeddings",val:" = 128"},{name:"num_conv_pos_embedding_groups",val:" = 16"},{name:"do_stable_layer_norm",val:" = False"},{name:"apply_spec_augment",val:" = True"},{name:"mask_time_prob",val:" = 0.05"},{name:"mask_time_length",val:" = 10"},{name:"mask_time_min_masks",val:" = 2"},{name:"mask_feature_prob",val:" = 0.0"},{name:"mask_feature_length",val:" = 10"},{name:"mask_feature_min_masks",val:" = 0"},{name:"num_codevectors_per_group",val:" = 320"},{name:"num_codevector_groups",val:" = 2"},{name:"contrastive_logits_temperature",val:" = 0.1"},{name:"num_negatives",val:" = 100"},{name:"codevector_dim",val:" = 256"},{name:"proj_codevector_dim",val:" = 256"},{name:"diversity_loss_weight",val:" = 0.1"},{name:"ctc_loss_reduction",val:" = 'mean'"},{name:"ctc_zero_infinity",val:" = False"},{name:"use_weighted_layer_sum",val:" = False"},{name:"classifier_proj_size",val:" = 256"},{name:"num_ctc_classes",val:" = 80"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"replace_prob",val:" = 0.5"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/configuration_unispeech.py#L32",parametersDescription:[{anchor:"transformers.UniSpeechConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Vocabulary size of the UniSpeech model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a>. Vocabulary size of the model. Defines the
different tokens that can be represented by the <em>inputs_ids</em> passed to the forward method of
<a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a>.`,name:"vocab_size"},{anchor:"transformers.UniSpeechConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.UniSpeechConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.UniSpeechConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.UniSpeechConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.UniSpeechConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.UniSpeechConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.UniSpeechConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.UniSpeechConfig.final_dropout",description:`<strong>final_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the final projection layer of <a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"final_dropout"},{anchor:"transformers.UniSpeechConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.UniSpeechConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.UniSpeechConfig.feat_extract_norm",description:`<strong>feat_extract_norm</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;group&quot;</code>) &#x2014;
The norm to be applied to 1D convolutional layers in feature encoder. One of <code>&quot;group&quot;</code> for group
normalization of only the first 1D convolutional layer or <code>&quot;layer&quot;</code> for layer normalization of all 1D
convolutional layers.`,name:"feat_extract_norm"},{anchor:"transformers.UniSpeechConfig.feat_proj_dropout",description:`<strong>feat_proj_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for output of the feature encoder.`,name:"feat_proj_dropout"},{anchor:"transformers.UniSpeechConfig.feat_extract_activation",description:"<strong>feat_extract_activation</strong> (<code>str, </code>optional<code>, defaults to </code>&#x201C;gelu&#x201D;<code>) -- The non-linear activation function (function or string) in the 1D convolutional layers of the feature extractor. If string, </code>&#x201C;gelu&#x201D;<code>, </code>&#x201C;relu&#x201D;<code>, </code>&#x201C;selu&#x201D;<code>and</code>&#x201C;gelu_new&#x201D;` are supported.",name:"feat_extract_activation"},{anchor:"transformers.UniSpeechConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for quantized feature encoder states.`,name:"feat_quantizer_dropout"},{anchor:"transformers.UniSpeechConfig.conv_dim",description:`<strong>conv_dim</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(512, 512, 512, 512, 512, 512, 512)</code>) &#x2014;
A tuple of integers defining the number of input and output channels of each 1D convolutional layer in the
feature encoder. The length of <em>conv_dim</em> defines the number of 1D convolutional layers.`,name:"conv_dim"},{anchor:"transformers.UniSpeechConfig.conv_stride",description:`<strong>conv_stride</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(5, 2, 2, 2, 2, 2, 2)</code>) &#x2014;
A tuple of integers defining the stride of each 1D convolutional layer in the feature encoder. The length
of <em>conv_stride</em> defines the number of convolutional layers and has to match the the length of <em>conv_dim</em>.`,name:"conv_stride"},{anchor:"transformers.UniSpeechConfig.conv_kernel",description:`<strong>conv_kernel</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(10, 3, 3, 3, 3, 3, 3)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the feature encoder. The
length of <em>conv_kernel</em> defines the number of convolutional layers and has to match the the length of
<em>conv_dim</em>.`,name:"conv_kernel"},{anchor:"transformers.UniSpeechConfig.conv_bias",description:`<strong>conv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the 1D convolutional layers have a bias.`,name:"conv_bias"},{anchor:"transformers.UniSpeechConfig.num_conv_pos_embeddings",description:`<strong>num_conv_pos_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of convolutional positional embeddings. Defines the kernel size of 1D convolutional positional
embeddings layer.`,name:"num_conv_pos_embeddings"},{anchor:"transformers.UniSpeechConfig.num_conv_pos_embedding_groups",description:`<strong>num_conv_pos_embedding_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of groups of 1D convolutional positional embeddings layer.`,name:"num_conv_pos_embedding_groups"},{anchor:"transformers.UniSpeechConfig.do_stable_layer_norm",description:`<strong>do_stable_layer_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply <em>stable</em> layer norm architecture of the Transformer encoder. <code>do_stable_layer_norm is True</code> corresponds to applying layer norm before the attention layer, whereas <code>do_stable_layer_norm is False</code> corresponds to applying layer norm after the attention layer.`,name:"do_stable_layer_norm"},{anchor:"transformers.UniSpeechConfig.apply_spec_augment",description:`<strong>apply_spec_augment</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply <em>SpecAugment</em> data augmentation to the outputs of the feature encoder. For reference see
<a href="https://arxiv.org/abs/1904.08779" rel="nofollow">SpecAugment: A Simple Data Augmentation Method for Automatic Speech
Recognition</a>.`,name:"apply_spec_augment"},{anchor:"transformers.UniSpeechConfig.mask_time_prob",description:`<strong>mask_time_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking
procecure generates &#x201D;mask_time_prob<em>len(time_axis)/mask_time_length&#x201D; independent masks over the axis. If
reasoning from the propability of each feature vector to be chosen as the start of the vector span to be
masked, </em>mask_time_prob<em> should be \`prob_vector_start</em>mask_time_length<code>. Note that overlap may decrease the actual percentage of masked vectors. This is only relevant if </code>apply_spec_augment is True\`.`,name:"mask_time_prob"},{anchor:"transformers.UniSpeechConfig.mask_time_length",description:`<strong>mask_time_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the time axis.`,name:"mask_time_length"},{anchor:"transformers.UniSpeechConfig.mask_time_min_masks",description:`<strong>mask_time_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 2), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the time axis, each time step,
irrespectively of <code>mask_feature_prob</code>. Only relevant if &#x201D;mask_time_prob*len(time_axis)/mask_time_length &lt;
mask_time_min_masks&#x201D;`,name:"mask_time_min_masks"},{anchor:"transformers.UniSpeechConfig.mask_feature_prob",description:`<strong>mask_feature_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The
masking procecure generates &#x201D;mask_feature_prob<em>len(feature_axis)/mask_time_length&#x201D; independent masks over
the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector
span to be masked, </em>mask_feature_prob<em> should be \`prob_vector_start</em>mask_feature_length<code>. Note that overlap may decrease the actual percentage of masked vectors. This is only relevant if </code>apply_spec_augment is
True\`.`,name:"mask_feature_prob"},{anchor:"transformers.UniSpeechConfig.mask_feature_length",description:`<strong>mask_feature_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the feature axis.`,name:"mask_feature_length"},{anchor:"transformers.UniSpeechConfig.mask_feature_min_masks",description:`<strong>mask_feature_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 0), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the feature axis, each time
step, irrespectively of <code>mask_feature_prob</code>. Only relevant if
&#x201D;mask_feature_prob*len(feature_axis)/mask_feature_length &lt; mask_feature_min_masks&#x201D;`,name:"mask_feature_min_masks"},{anchor:"transformers.UniSpeechConfig.num_codevectors_per_group",description:`<strong>num_codevectors_per_group</strong> (<code>int</code>, <em>optional</em>, defaults to 320) &#x2014;
Number of entries in each quantization codebook (group).`,name:"num_codevectors_per_group"},{anchor:"transformers.UniSpeechConfig.num_codevector_groups",description:`<strong>num_codevector_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of codevector groups for product codevector quantization.`,name:"num_codevector_groups"},{anchor:"transformers.UniSpeechConfig.contrastive_logits_temperature",description:`<strong>contrastive_logits_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The temperature <em>kappa</em> in the contrastive loss.`,name:"contrastive_logits_temperature"},{anchor:"transformers.UniSpeechConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for the output of the feature encoder that&#x2019;s used by the quantizer.`,name:"feat_quantizer_dropout"},{anchor:"transformers.UniSpeechConfig.num_negatives",description:`<strong>num_negatives</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Number of negative samples for the contrastive loss.`,name:"num_negatives"},{anchor:"transformers.UniSpeechConfig.codevector_dim",description:`<strong>codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the quantized feature vectors.`,name:"codevector_dim"},{anchor:"transformers.UniSpeechConfig.proj_codevector_dim",description:`<strong>proj_codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the final projection of both the quantized and the transformer features.`,name:"proj_codevector_dim"},{anchor:"transformers.UniSpeechConfig.diversity_loss_weight",description:`<strong>diversity_loss_weight</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The weight of the codebook diversity loss component.`,name:"diversity_loss_weight"},{anchor:"transformers.UniSpeechConfig.ctc_loss_reduction",description:`<strong>ctc_loss_reduction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Specifies the reduction to apply to the output of <code>torch.nn.CTCLoss</code>. Only relevant when training an
instance of <a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"ctc_loss_reduction"},{anchor:"transformers.UniSpeechConfig.ctc_zero_infinity",description:`<strong>ctc_zero_infinity</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to zero infinite losses and the associated gradients of <code>torch.nn.CTCLoss</code>. Infinite losses mainly
occur when the inputs are too short to be aligned to the targets. Only relevant when training an instance
of <a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"ctc_zero_infinity"},{anchor:"transformers.UniSpeechConfig.use_weighted_layer_sum",description:`<strong>use_weighted_layer_sum</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a weighted average of layer outputs with learned weights. Only relevant when using an
instance of <a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a>.`,name:"use_weighted_layer_sum"},{anchor:"transformers.UniSpeechConfig.classifier_proj_size",description:`<strong>classifier_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the projection before token mean-pooling for classification.`,name:"classifier_proj_size"},{anchor:"transformers.UniSpeechConfig.replace_prob",description:`<strong>replace_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
Propability that transformer feature is replaced by quantized feature for pretraining.`,name:"replace_prob"}]}}),Ee=new Tt({props:{code:`from transformers import UniSpeechModel, UniSpeechConfig

# Initializing a UniSpeech facebook/unispeech-base-960h style configuration
configuration = UniSpeechConfig()

# Initializing a model from the facebook/unispeech-base-960h style configuration
model = UniSpeechModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UniSpeechModel, UniSpeechConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a UniSpeech facebook/unispeech-base-960h style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = UniSpeechConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the facebook/unispeech-base-960h style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Me=new Te({}),ze=new W({props:{name:"class transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"extract_features",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L75",parametersDescription:[{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.extract_features",description:`<strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) &#x2014;
Sequence of extracted feature vectors of the last convolutional layer of the model.`,name:"extract_features"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ae=new W({props:{name:"class transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput",anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"projected_states",val:": FloatTensor = None"},{name:"projected_quantized_states",val:": FloatTensor = None"},{name:"codevector_perplexity",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L104",parametersDescription:[{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when model is in train mode, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a href="https://arxiv.org/pdf/2006.11477.pdf" rel="nofollow">official
paper</a> . (classification) loss.`,name:"loss"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.projected_states",description:`<strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.`,name:"projected_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.projected_quantized_states",description:`<strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.`,name:"projected_quantized_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),We=new Te({}),Le=new W({props:{name:"class transformers.UniSpeechModel",anchor:"transformers.UniSpeechModel",parameters:[{name:"config",val:": UniSpeechConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1087",parametersDescription:[{anchor:"transformers.UniSpeechModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ke=new W({props:{name:"forward",anchor:"transformers.UniSpeechModel.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"mask_time_indices",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1152",parametersDescription:[{anchor:"transformers.UniSpeechModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should be used for
padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See <code>UniSpeechProcessor.__call__</code> for
details.`,name:"input_values"},{anchor:"transformers.UniSpeechModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has <code>config.return_attention_mask == True</code>. For all models whose processor has <code>config.return_attention_mask == False</code>, <code>attention_mask</code> should
<strong>not</strong> be passed to avoid degraded performance when doing batched inference. For such models
<code>input_values</code> should simply be padded with 0 and passed without <code>attention_mask</code>. Be aware that these
models also yield slightly different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16478/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) \u2014 Sequence of extracted feature vectors of the last convolutional layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new wn({props:{$$slots:{default:[Ci]},$$scope:{ctx:L}}}),Ye=new Tt({props:{code:`from transformers import Wav2Vec2Processor, UniSpeechModel
import torch
from datasets import load_dataset

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
dataset = dataset.sort("id")
sampling_rate = dataset.features["audio"].sampling_rate

processor = Wav2Vec2Processor.from_pretrained("patrickvonplaten/unispeech-large-1500h-cv-timit")
model = UniSpeechModel.from_pretrained("patrickvonplaten/unispeech-large-1500h-cv-timit")

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, UniSpeechModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/unispeech-large-1500h-cv-timit&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechModel.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/unispeech-large-1500h-cv-timit&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">292</span>, <span class="hljs-number">1024</span>]`}}),Re=new Te({}),Qe=new W({props:{name:"class transformers.UniSpeechForCTC",anchor:"transformers.UniSpeechForCTC",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1363",parametersDescription:[{anchor:"transformers.UniSpeechForCTC.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),et=new W({props:{name:"forward",anchor:"transformers.UniSpeechForCTC.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1404",parametersDescription:[{anchor:"transformers.UniSpeechForCTC.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should be used for
padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See <code>UniSpeechProcessor.__call__</code> for
details.`,name:"input_values"},{anchor:"transformers.UniSpeechForCTC.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has <code>config.return_attention_mask == True</code>. For all models whose processor has <code>config.return_attention_mask == False</code>, <code>attention_mask</code> should
<strong>not</strong> be passed to avoid degraded performance when doing batched inference. For such models
<code>input_values</code> should simply be padded with 0 and passed without <code>attention_mask</code>. Be aware that these
models also yield slightly different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechForCTC.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechForCTC.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechForCTC.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16478/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UniSpeechForCTC.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_length)</code>, <em>optional</em>) &#x2014;
Labels for connectionist temporal classification. Note that <code>target_length</code> has to be smaller or equal to
the sequence length of the output logits. Indices are selected in <code>[-100, 0, ..., config.vocab_size - 1]</code>.
All labels set to <code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16478/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16478/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new wn({props:{$$slots:{default:[ji]},$$scope:{ctx:L}}}),tt=new Tt({props:{code:`from transformers import Wav2Vec2Processor, UniSpeechForCTC
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
dataset = dataset.sort("id")
sampling_rate = dataset.features["audio"].sampling_rate

processor = Wav2Vec2Processor.from_pretrained("patrickvonplaten/unispeech-large-1500h-cv-timit")
model = UniSpeechForCTC.from_pretrained("patrickvonplaten/unispeech-large-1500h-cv-timit")

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
with torch.no_grad():
    logits = model(**inputs).logits
predicted_ids = torch.argmax(logits, dim=-1)

# transcribe speech
transcription = processor.batch_decode(predicted_ids)
transcription[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, UniSpeechForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/unispeech-large-1500h-cv-timit&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForCTC.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/unispeech-large-1500h-cv-timit&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># transcribe speech</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;mister quilter is the apposl of the midle classes and weare glad to welcom his gosepl&#x27;</span>`}}),ot=new Tt({props:{code:`with processor.as_target_processor():
    inputs["labels"] = processor(dataset[0]["text"], return_tensors="pt").input_ids

# compute loss
loss = model(**inputs).loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">17.17</span>`}}),nt=new Te({}),at=new W({props:{name:"class transformers.UniSpeechForSequenceClassification",anchor:"transformers.UniSpeechForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1494",parametersDescription:[{anchor:"transformers.UniSpeechForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),dt=new W({props:{name:"forward",anchor:"transformers.UniSpeechForSequenceClassification.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1539",parametersDescription:[{anchor:"transformers.UniSpeechForSequenceClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should be used for
padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See <code>UniSpeechProcessor.__call__</code> for
details.`,name:"input_values"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has <code>config.return_attention_mask == True</code>. For all models whose processor has <code>config.return_attention_mask == False</code>, <code>attention_mask</code> should
<strong>not</strong> be passed to avoid degraded performance when doing batched inference. For such models
<code>input_values</code> should simply be padded with 0 and passed without <code>attention_mask</code>. Be aware that these
models also yield slightly different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16478/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16478/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16478/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new wn({props:{$$slots:{default:[xi]},$$scope:{ctx:L}}}),pt=new Tt({props:{code:`from transformers import Wav2Vec2FeatureExtractor, UniSpeechForSequenceClassification
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
dataset = dataset.sort("id")
sampling_rate = dataset.features["audio"].sampling_rate

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("hf-internal-testing/tiny-random-unispeech")
model = UniSpeechForSequenceClassification.from_pretrained("hf-internal-testing/tiny-random-unispeech")

# audio file is decoded on the fly
inputs = feature_extractor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_ids = torch.argmax(logits, dim=-1).item()
predicted_label = model.config.id2label[predicted_class_ids]
predicted_label`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2FeatureExtractor, UniSpeechForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-unispeech&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/tiny-random-unispeech&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label
<span class="hljs-string">&#x27;LABEL_0&#x27;</span>`}}),ht=new Tt({props:{code:`# compute loss - target_label is e.g. "down"
target_label = model.config.id2label[0]
inputs["labels"] = torch.tensor([model.config.label2id[target_label]])
loss = model(**inputs).loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss - target_label is e.g. &quot;down&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_label = model.config.id2label[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([model.config.label2id[target_label]])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.66</span>`}}),mt=new Te({}),ut=new W({props:{name:"class transformers.UniSpeechForPreTraining",anchor:"transformers.UniSpeechForPreTraining",parameters:[{name:"config",val:": UniSpeechConfig"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1212",parametersDescription:[{anchor:"transformers.UniSpeechForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),wt=new W({props:{name:"forward",anchor:"transformers.UniSpeechForPreTraining.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16478/src/transformers/models/unispeech/modeling_unispeech.py#L1273",parametersDescription:[{anchor:"transformers.UniSpeechForPreTraining.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should be used for
padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See <code>UniSpeechProcessor.__call__</code> for
details.`,name:"input_values"},{anchor:"transformers.UniSpeechForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has <code>config.return_attention_mask == True</code>. For all models whose processor has <code>config.return_attention_mask == False</code>, <code>attention_mask</code> should
<strong>not</strong> be passed to avoid degraded performance when doing batched inference. For such models
<code>input_values</code> should simply be padded with 0 and passed without <code>attention_mask</code>. Be aware that these
models also yield slightly different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16478/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UniSpeechForPreTraining.forward.mask_time_indices",description:`<strong>mask_time_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
masked extracted features in <em>config.proj_codevector_dim</em> space.`,name:"mask_time_indices"},{anchor:"transformers.UniSpeechForPreTraining.forward.sampled_negative_indices",description:`<strong>sampled_negative_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length, num_negatives)</code>, <em>optional</em>) &#x2014;
Indices indicating which quantized target vectors are used as negative sampled vectors in contrastive loss.
Required input for pre-training.`,name:"sampled_negative_indices"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when model is in train mode, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a
  href="https://arxiv.org/pdf/2006.11477.pdf"
  rel="nofollow"
>official
paper</a> . (classification) loss.</p>
</li>
<li>
<p><strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) \u2014 Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.</p>
</li>
<li>
<p><strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) \u2014 Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new wn({props:{$$slots:{default:[Fi]},$$scope:{ctx:L}}}),yt=new Tt({props:{code:`import torch
from transformers import Wav2Vec2FeatureExtractor, UniSpeechForPreTraining
from transformers.models.unispeech.modeling_unispeech import _compute_mask_indices

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "hf-internal-testing/tiny-random-unispeech-sat"
)
model = UniSpeechForPreTraining.from_pretrained("microsoft/unispeech-large-1500h-cv")
# TODO: Add full pretraining example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2FeatureExtractor, UniSpeechForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.unispeech.modeling_unispeech <span class="hljs-keyword">import</span> _compute_mask_indices

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;hf-internal-testing/tiny-random-unispeech-sat&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForPreTraining.from_pretrained(<span class="hljs-string">&quot;microsoft/unispeech-large-1500h-cv&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># <span class="hljs-doctag">TODO:</span> Add full pretraining example</span>`}}),{c(){m=n("meta"),k=l(),u=n("h1"),T=n("a"),U=n("span"),_(f.$$.fragment),g=l(),$=n("span"),yn=r("UniSpeech"),$o=l(),R=n("h2"),de=n("a"),Rt=n("span"),_(Ue.$$.fragment),Sn=l(),Qt=n("span"),Tn=r("Overview"),Co=l(),pe=n("p"),Un=r("The UniSpeech model was proposed in "),ke=n("a"),kn=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),$n=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
Zeng, Xuedong Huang .`),jo=l(),Ut=n("p"),Cn=r("The abstract from the paper is the following:"),xo=l(),kt=n("p"),Xt=n("em"),jn=r(`In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.`),Fo=l(),$t=n("p"),xn=r("Tips:"),qo=l(),he=n("ul"),$e=n("li"),Fn=r(`UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please
use `),Ct=n("a"),qn=r("Wav2Vec2Processor"),Pn=r(" for the feature extraction."),En=l(),Ce=n("li"),Mn=r(`UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be
decoded using `),jt=n("a"),zn=r("Wav2Vec2CTCTokenizer"),Dn=r("."),Po=l(),N=n("p"),An=r("This model was contributed by "),je=n("a"),On=r("patrickvonplaten"),Wn=r(`. The Authors\u2019 code can be
found `),xe=n("a"),Ln=r("here"),Nn=r("."),Eo=l(),Q=n("h2"),me=n("a"),Zt=n("span"),_(Fe.$$.fragment),Vn=l(),Jt=n("span"),In=r("UniSpeechConfig"),Mo=l(),F=n("div"),_(qe.$$.fragment),Bn=l(),X=n("p"),Hn=r("This is the configuration class to store the configuration of a "),xt=n("a"),Kn=r("UniSpeechModel"),Yn=r(`. It is used to instantiate an
UniSpeech model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the UniSpeech
`),Pe=n("a"),Rn=r("facebook/unispeech-base-960h"),Qn=r(" architecture."),Xn=l(),Z=n("p"),Zn=r("Configuration objects inherit from "),Ft=n("a"),Jn=r("PretrainedConfig"),Gn=r(` and can be used to control the model outputs. Read the
documentation from `),qt=n("a"),ea=r("PretrainedConfig"),ta=r(" for more information."),oa=l(),Gt=n("p"),na=r("Example:"),aa=l(),_(Ee.$$.fragment),zo=l(),J=n("h2"),ue=n("a"),eo=n("span"),_(Me.$$.fragment),sa=l(),to=n("span"),ra=r("UniSpeech specific outputs"),Do=l(),G=n("div"),_(ze.$$.fragment),ia=l(),De=n("p"),ca=r("Output type of "),oo=n("code"),la=r("UniSpeechBaseModelOutput"),da=r(", with potential hidden states and attentions."),Ao=l(),ee=n("div"),_(Ae.$$.fragment),pa=l(),Oe=n("p"),ha=r("Output type of "),no=n("code"),ma=r("UniSpeechForPreTrainingOutput"),ua=r(", with potential hidden states and attentions."),Oo=l(),te=n("h2"),fe=n("a"),ao=n("span"),_(We.$$.fragment),fa=l(),so=n("span"),ga=r("UniSpeechModel"),Wo=l(),q=n("div"),_(Le.$$.fragment),_a=l(),Ne=n("p"),va=r(`The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.
UniSpeech was proposed in `),Ve=n("a"),ba=r(`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),wa=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),ya=l(),Ie=n("p"),Sa=r("This model inherits from "),Pt=n("a"),Ta=r("PreTrainedModel"),Ua=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),ka=l(),Be=n("p"),$a=r("This model is a PyTorch "),He=n("a"),Ca=r("torch.nn.Module"),ja=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),xa=l(),M=n("div"),_(Ke.$$.fragment),Fa=l(),oe=n("p"),qa=r("The "),Et=n("a"),Pa=r("UniSpeechModel"),Ea=r(" forward method, overrides the "),ro=n("code"),Ma=r("__call__"),za=r(" special method."),Da=l(),_(ge.$$.fragment),Aa=l(),io=n("p"),Oa=r("Example:"),Wa=l(),_(Ye.$$.fragment),Lo=l(),ne=n("h2"),_e=n("a"),co=n("span"),_(Re.$$.fragment),La=l(),lo=n("span"),Na=r("UniSpeechForCTC"),No=l(),P=n("div"),_(Qe.$$.fragment),Va=l(),ae=n("p"),Ia=r("UniSpeech Model with a "),po=n("code"),Ba=r("language modeling"),Ha=r(` head on top for Connectionist Temporal Classification (CTC).
UniSpeech was proposed in `),Xe=n("a"),Ka=r(`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),Ya=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Ra=l(),Ze=n("p"),Qa=r("This model inherits from "),Mt=n("a"),Xa=r("PreTrainedModel"),Za=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),Ja=l(),Je=n("p"),Ga=r("This model is a PyTorch "),Ge=n("a"),es=r("torch.nn.Module"),ts=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),os=l(),j=n("div"),_(et.$$.fragment),ns=l(),se=n("p"),as=r("The "),zt=n("a"),ss=r("UniSpeechForCTC"),rs=r(" forward method, overrides the "),ho=n("code"),is=r("__call__"),cs=r(" special method."),ls=l(),_(ve.$$.fragment),ds=l(),mo=n("p"),ps=r("Example:"),hs=l(),_(tt.$$.fragment),ms=l(),_(ot.$$.fragment),Vo=l(),re=n("h2"),be=n("a"),uo=n("span"),_(nt.$$.fragment),us=l(),fo=n("span"),fs=r("UniSpeechForSequenceClassification"),Io=l(),C=n("div"),_(at.$$.fragment),gs=l(),go=n("p"),_s=r(`UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`),vs=l(),st=n("p"),bs=r("UniSpeech was proposed in "),rt=n("a"),ws=r(`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),ys=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Ss=l(),it=n("p"),Ts=r("This model inherits from "),Dt=n("a"),Us=r("PreTrainedModel"),ks=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),$s=l(),ct=n("p"),Cs=r("This model is a PyTorch "),lt=n("a"),js=r("torch.nn.Module"),xs=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Fs=l(),x=n("div"),_(dt.$$.fragment),qs=l(),ie=n("p"),Ps=r("The "),At=n("a"),Es=r("UniSpeechForSequenceClassification"),Ms=r(" forward method, overrides the "),_o=n("code"),zs=r("__call__"),Ds=r(" special method."),As=l(),_(we.$$.fragment),Os=l(),vo=n("p"),Ws=r("Example:"),Ls=l(),_(pt.$$.fragment),Ns=l(),_(ht.$$.fragment),Bo=l(),ce=n("h2"),ye=n("a"),bo=n("span"),_(mt.$$.fragment),Vs=l(),wo=n("span"),Is=r("UniSpeechForPreTraining"),Ho=l(),E=n("div"),_(ut.$$.fragment),Bs=l(),ft=n("p"),Hs=r(`UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
UniSpeech was proposed in `),gt=n("a"),Ks=r(`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),Ys=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Rs=l(),_t=n("p"),Qs=r("This model inherits from "),Ot=n("a"),Xs=r("PreTrainedModel"),Zs=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),Js=l(),vt=n("p"),Gs=r("This model is a PyTorch "),bt=n("a"),er=r("torch.nn.Module"),tr=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),or=l(),z=n("div"),_(wt.$$.fragment),nr=l(),le=n("p"),ar=r("The "),Wt=n("a"),sr=r("UniSpeechForPreTraining"),rr=r(" forward method, overrides the "),yo=n("code"),ir=r("__call__"),cr=r(" special method."),lr=l(),_(Se.$$.fragment),dr=l(),So=n("p"),pr=r("Example:"),hr=l(),_(yt.$$.fragment),this.h()},l(t){const p=ki('[data-svelte="svelte-1phssyn"]',document.head);m=a(p,"META",{name:!0,content:!0}),p.forEach(o),k=d(t),u=a(t,"H1",{class:!0});var St=s(u);T=a(St,"A",{id:!0,class:!0,href:!0});var To=s(T);U=a(To,"SPAN",{});var Uo=s(U);v(f.$$.fragment,Uo),Uo.forEach(o),To.forEach(o),g=d(St),$=a(St,"SPAN",{});var ko=s($);yn=i(ko,"UniSpeech"),ko.forEach(o),St.forEach(o),$o=d(t),R=a(t,"H2",{class:!0});var Yo=s(R);de=a(Yo,"A",{id:!0,class:!0,href:!0});var mr=s(de);Rt=a(mr,"SPAN",{});var ur=s(Rt);v(Ue.$$.fragment,ur),ur.forEach(o),mr.forEach(o),Sn=d(Yo),Qt=a(Yo,"SPAN",{});var fr=s(Qt);Tn=i(fr,"Overview"),fr.forEach(o),Yo.forEach(o),Co=d(t),pe=a(t,"P",{});var Ro=s(pe);Un=i(Ro,"The UniSpeech model was proposed in "),ke=a(Ro,"A",{href:!0,rel:!0});var gr=s(ke);kn=i(gr,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),gr.forEach(o),$n=i(Ro,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
Zeng, Xuedong Huang .`),Ro.forEach(o),jo=d(t),Ut=a(t,"P",{});var _r=s(Ut);Cn=i(_r,"The abstract from the paper is the following:"),_r.forEach(o),xo=d(t),kt=a(t,"P",{});var vr=s(kt);Xt=a(vr,"EM",{});var br=s(Xt);jn=i(br,`In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.`),br.forEach(o),vr.forEach(o),Fo=d(t),$t=a(t,"P",{});var wr=s($t);xn=i(wr,"Tips:"),wr.forEach(o),qo=d(t),he=a(t,"UL",{});var Qo=s(he);$e=a(Qo,"LI",{});var Xo=s($e);Fn=i(Xo,`UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please
use `),Ct=a(Xo,"A",{href:!0});var yr=s(Ct);qn=i(yr,"Wav2Vec2Processor"),yr.forEach(o),Pn=i(Xo," for the feature extraction."),Xo.forEach(o),En=d(Qo),Ce=a(Qo,"LI",{});var Zo=s(Ce);Mn=i(Zo,`UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be
decoded using `),jt=a(Zo,"A",{href:!0});var Sr=s(jt);zn=i(Sr,"Wav2Vec2CTCTokenizer"),Sr.forEach(o),Dn=i(Zo,"."),Zo.forEach(o),Qo.forEach(o),Po=d(t),N=a(t,"P",{});var Lt=s(N);An=i(Lt,"This model was contributed by "),je=a(Lt,"A",{href:!0,rel:!0});var Tr=s(je);On=i(Tr,"patrickvonplaten"),Tr.forEach(o),Wn=i(Lt,`. The Authors\u2019 code can be
found `),xe=a(Lt,"A",{href:!0,rel:!0});var Ur=s(xe);Ln=i(Ur,"here"),Ur.forEach(o),Nn=i(Lt,"."),Lt.forEach(o),Eo=d(t),Q=a(t,"H2",{class:!0});var Jo=s(Q);me=a(Jo,"A",{id:!0,class:!0,href:!0});var kr=s(me);Zt=a(kr,"SPAN",{});var $r=s(Zt);v(Fe.$$.fragment,$r),$r.forEach(o),kr.forEach(o),Vn=d(Jo),Jt=a(Jo,"SPAN",{});var Cr=s(Jt);In=i(Cr,"UniSpeechConfig"),Cr.forEach(o),Jo.forEach(o),Mo=d(t),F=a(t,"DIV",{class:!0});var V=s(F);v(qe.$$.fragment,V),Bn=d(V),X=a(V,"P",{});var Nt=s(X);Hn=i(Nt,"This is the configuration class to store the configuration of a "),xt=a(Nt,"A",{href:!0});var jr=s(xt);Kn=i(jr,"UniSpeechModel"),jr.forEach(o),Yn=i(Nt,`. It is used to instantiate an
UniSpeech model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the UniSpeech
`),Pe=a(Nt,"A",{href:!0,rel:!0});var xr=s(Pe);Rn=i(xr,"facebook/unispeech-base-960h"),xr.forEach(o),Qn=i(Nt," architecture."),Nt.forEach(o),Xn=d(V),Z=a(V,"P",{});var Vt=s(Z);Zn=i(Vt,"Configuration objects inherit from "),Ft=a(Vt,"A",{href:!0});var Fr=s(Ft);Jn=i(Fr,"PretrainedConfig"),Fr.forEach(o),Gn=i(Vt,` and can be used to control the model outputs. Read the
documentation from `),qt=a(Vt,"A",{href:!0});var qr=s(qt);ea=i(qr,"PretrainedConfig"),qr.forEach(o),ta=i(Vt," for more information."),Vt.forEach(o),oa=d(V),Gt=a(V,"P",{});var Pr=s(Gt);na=i(Pr,"Example:"),Pr.forEach(o),aa=d(V),v(Ee.$$.fragment,V),V.forEach(o),zo=d(t),J=a(t,"H2",{class:!0});var Go=s(J);ue=a(Go,"A",{id:!0,class:!0,href:!0});var Er=s(ue);eo=a(Er,"SPAN",{});var Mr=s(eo);v(Me.$$.fragment,Mr),Mr.forEach(o),Er.forEach(o),sa=d(Go),to=a(Go,"SPAN",{});var zr=s(to);ra=i(zr,"UniSpeech specific outputs"),zr.forEach(o),Go.forEach(o),Do=d(t),G=a(t,"DIV",{class:!0});var en=s(G);v(ze.$$.fragment,en),ia=d(en),De=a(en,"P",{});var tn=s(De);ca=i(tn,"Output type of "),oo=a(tn,"CODE",{});var Dr=s(oo);la=i(Dr,"UniSpeechBaseModelOutput"),Dr.forEach(o),da=i(tn,", with potential hidden states and attentions."),tn.forEach(o),en.forEach(o),Ao=d(t),ee=a(t,"DIV",{class:!0});var on=s(ee);v(Ae.$$.fragment,on),pa=d(on),Oe=a(on,"P",{});var nn=s(Oe);ha=i(nn,"Output type of "),no=a(nn,"CODE",{});var Ar=s(no);ma=i(Ar,"UniSpeechForPreTrainingOutput"),Ar.forEach(o),ua=i(nn,", with potential hidden states and attentions."),nn.forEach(o),on.forEach(o),Oo=d(t),te=a(t,"H2",{class:!0});var an=s(te);fe=a(an,"A",{id:!0,class:!0,href:!0});var Or=s(fe);ao=a(Or,"SPAN",{});var Wr=s(ao);v(We.$$.fragment,Wr),Wr.forEach(o),Or.forEach(o),fa=d(an),so=a(an,"SPAN",{});var Lr=s(so);ga=i(Lr,"UniSpeechModel"),Lr.forEach(o),an.forEach(o),Wo=d(t),q=a(t,"DIV",{class:!0});var I=s(q);v(Le.$$.fragment,I),_a=d(I),Ne=a(I,"P",{});var sn=s(Ne);va=i(sn,`The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.
UniSpeech was proposed in `),Ve=a(sn,"A",{href:!0,rel:!0});var Nr=s(Ve);ba=i(Nr,`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),Nr.forEach(o),wa=i(sn,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),sn.forEach(o),ya=d(I),Ie=a(I,"P",{});var rn=s(Ie);Sa=i(rn,"This model inherits from "),Pt=a(rn,"A",{href:!0});var Vr=s(Pt);Ta=i(Vr,"PreTrainedModel"),Vr.forEach(o),Ua=i(rn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),rn.forEach(o),ka=d(I),Be=a(I,"P",{});var cn=s(Be);$a=i(cn,"This model is a PyTorch "),He=a(cn,"A",{href:!0,rel:!0});var Ir=s(He);Ca=i(Ir,"torch.nn.Module"),Ir.forEach(o),ja=i(cn,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),cn.forEach(o),xa=d(I),M=a(I,"DIV",{class:!0});var B=s(M);v(Ke.$$.fragment,B),Fa=d(B),oe=a(B,"P",{});var It=s(oe);qa=i(It,"The "),Et=a(It,"A",{href:!0});var Br=s(Et);Pa=i(Br,"UniSpeechModel"),Br.forEach(o),Ea=i(It," forward method, overrides the "),ro=a(It,"CODE",{});var Hr=s(ro);Ma=i(Hr,"__call__"),Hr.forEach(o),za=i(It," special method."),It.forEach(o),Da=d(B),v(ge.$$.fragment,B),Aa=d(B),io=a(B,"P",{});var Kr=s(io);Oa=i(Kr,"Example:"),Kr.forEach(o),Wa=d(B),v(Ye.$$.fragment,B),B.forEach(o),I.forEach(o),Lo=d(t),ne=a(t,"H2",{class:!0});var ln=s(ne);_e=a(ln,"A",{id:!0,class:!0,href:!0});var Yr=s(_e);co=a(Yr,"SPAN",{});var Rr=s(co);v(Re.$$.fragment,Rr),Rr.forEach(o),Yr.forEach(o),La=d(ln),lo=a(ln,"SPAN",{});var Qr=s(lo);Na=i(Qr,"UniSpeechForCTC"),Qr.forEach(o),ln.forEach(o),No=d(t),P=a(t,"DIV",{class:!0});var H=s(P);v(Qe.$$.fragment,H),Va=d(H),ae=a(H,"P",{});var Bt=s(ae);Ia=i(Bt,"UniSpeech Model with a "),po=a(Bt,"CODE",{});var Xr=s(po);Ba=i(Xr,"language modeling"),Xr.forEach(o),Ha=i(Bt,` head on top for Connectionist Temporal Classification (CTC).
UniSpeech was proposed in `),Xe=a(Bt,"A",{href:!0,rel:!0});var Zr=s(Xe);Ka=i(Zr,`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),Zr.forEach(o),Ya=i(Bt,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Bt.forEach(o),Ra=d(H),Ze=a(H,"P",{});var dn=s(Ze);Qa=i(dn,"This model inherits from "),Mt=a(dn,"A",{href:!0});var Jr=s(Mt);Xa=i(Jr,"PreTrainedModel"),Jr.forEach(o),Za=i(dn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),dn.forEach(o),Ja=d(H),Je=a(H,"P",{});var pn=s(Je);Ga=i(pn,"This model is a PyTorch "),Ge=a(pn,"A",{href:!0,rel:!0});var Gr=s(Ge);es=i(Gr,"torch.nn.Module"),Gr.forEach(o),ts=i(pn,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),pn.forEach(o),os=d(H),j=a(H,"DIV",{class:!0});var D=s(j);v(et.$$.fragment,D),ns=d(D),se=a(D,"P",{});var Ht=s(se);as=i(Ht,"The "),zt=a(Ht,"A",{href:!0});var ei=s(zt);ss=i(ei,"UniSpeechForCTC"),ei.forEach(o),rs=i(Ht," forward method, overrides the "),ho=a(Ht,"CODE",{});var ti=s(ho);is=i(ti,"__call__"),ti.forEach(o),cs=i(Ht," special method."),Ht.forEach(o),ls=d(D),v(ve.$$.fragment,D),ds=d(D),mo=a(D,"P",{});var oi=s(mo);ps=i(oi,"Example:"),oi.forEach(o),hs=d(D),v(tt.$$.fragment,D),ms=d(D),v(ot.$$.fragment,D),D.forEach(o),H.forEach(o),Vo=d(t),re=a(t,"H2",{class:!0});var hn=s(re);be=a(hn,"A",{id:!0,class:!0,href:!0});var ni=s(be);uo=a(ni,"SPAN",{});var ai=s(uo);v(nt.$$.fragment,ai),ai.forEach(o),ni.forEach(o),us=d(hn),fo=a(hn,"SPAN",{});var si=s(fo);fs=i(si,"UniSpeechForSequenceClassification"),si.forEach(o),hn.forEach(o),Io=d(t),C=a(t,"DIV",{class:!0});var A=s(C);v(at.$$.fragment,A),gs=d(A),go=a(A,"P",{});var ri=s(go);_s=i(ri,`UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`),ri.forEach(o),vs=d(A),st=a(A,"P",{});var mn=s(st);bs=i(mn,"UniSpeech was proposed in "),rt=a(mn,"A",{href:!0,rel:!0});var ii=s(rt);ws=i(ii,`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),ii.forEach(o),ys=i(mn,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),mn.forEach(o),Ss=d(A),it=a(A,"P",{});var un=s(it);Ts=i(un,"This model inherits from "),Dt=a(un,"A",{href:!0});var ci=s(Dt);Us=i(ci,"PreTrainedModel"),ci.forEach(o),ks=i(un,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),un.forEach(o),$s=d(A),ct=a(A,"P",{});var fn=s(ct);Cs=i(fn,"This model is a PyTorch "),lt=a(fn,"A",{href:!0,rel:!0});var li=s(lt);js=i(li,"torch.nn.Module"),li.forEach(o),xs=i(fn,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),fn.forEach(o),Fs=d(A),x=a(A,"DIV",{class:!0});var O=s(x);v(dt.$$.fragment,O),qs=d(O),ie=a(O,"P",{});var Kt=s(ie);Ps=i(Kt,"The "),At=a(Kt,"A",{href:!0});var di=s(At);Es=i(di,"UniSpeechForSequenceClassification"),di.forEach(o),Ms=i(Kt," forward method, overrides the "),_o=a(Kt,"CODE",{});var pi=s(_o);zs=i(pi,"__call__"),pi.forEach(o),Ds=i(Kt," special method."),Kt.forEach(o),As=d(O),v(we.$$.fragment,O),Os=d(O),vo=a(O,"P",{});var hi=s(vo);Ws=i(hi,"Example:"),hi.forEach(o),Ls=d(O),v(pt.$$.fragment,O),Ns=d(O),v(ht.$$.fragment,O),O.forEach(o),A.forEach(o),Bo=d(t),ce=a(t,"H2",{class:!0});var gn=s(ce);ye=a(gn,"A",{id:!0,class:!0,href:!0});var mi=s(ye);bo=a(mi,"SPAN",{});var ui=s(bo);v(mt.$$.fragment,ui),ui.forEach(o),mi.forEach(o),Vs=d(gn),wo=a(gn,"SPAN",{});var fi=s(wo);Is=i(fi,"UniSpeechForPreTraining"),fi.forEach(o),gn.forEach(o),Ho=d(t),E=a(t,"DIV",{class:!0});var K=s(E);v(ut.$$.fragment,K),Bs=d(K),ft=a(K,"P",{});var _n=s(ft);Hs=i(_n,`UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
UniSpeech was proposed in `),gt=a(_n,"A",{href:!0,rel:!0});var gi=s(gt);Ks=i(gi,`UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled
Data`),gi.forEach(o),Ys=i(_n,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),_n.forEach(o),Rs=d(K),_t=a(K,"P",{});var vn=s(_t);Qs=i(vn,"This model inherits from "),Ot=a(vn,"A",{href:!0});var _i=s(Ot);Xs=i(_i,"PreTrainedModel"),_i.forEach(o),Zs=i(vn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving etc.).`),vn.forEach(o),Js=d(K),vt=a(K,"P",{});var bn=s(vt);Gs=i(bn,"This model is a PyTorch "),bt=a(bn,"A",{href:!0,rel:!0});var vi=s(bt);er=i(vi,"torch.nn.Module"),vi.forEach(o),tr=i(bn,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),bn.forEach(o),or=d(K),z=a(K,"DIV",{class:!0});var Y=s(z);v(wt.$$.fragment,Y),nr=d(Y),le=a(Y,"P",{});var Yt=s(le);ar=i(Yt,"The "),Wt=a(Yt,"A",{href:!0});var bi=s(Wt);sr=i(bi,"UniSpeechForPreTraining"),bi.forEach(o),rr=i(Yt," forward method, overrides the "),yo=a(Yt,"CODE",{});var wi=s(yo);ir=i(wi,"__call__"),wi.forEach(o),cr=i(Yt," special method."),Yt.forEach(o),lr=d(Y),v(Se.$$.fragment,Y),dr=d(Y),So=a(Y,"P",{});var yi=s(So);pr=i(yi,"Example:"),yi.forEach(o),hr=d(Y),v(yt.$$.fragment,Y),Y.forEach(o),K.forEach(o),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(Pi)),c(T,"id","unispeech"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#unispeech"),c(u,"class","relative group"),c(de,"id","overview"),c(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(de,"href","#overview"),c(R,"class","relative group"),c(ke,"href","https://arxiv.org/abs/2101.07597"),c(ke,"rel","nofollow"),c(Ct,"href","/docs/transformers/pr_16478/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(jt,"href","/docs/transformers/pr_16478/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(je,"href","https://huggingface.co/patrickvonplaten"),c(je,"rel","nofollow"),c(xe,"href","https://github.com/microsoft/UniSpeech/tree/main/UniSpeech"),c(xe,"rel","nofollow"),c(me,"id","transformers.UniSpeechConfig"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#transformers.UniSpeechConfig"),c(Q,"class","relative group"),c(xt,"href","/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechModel"),c(Pe,"href","https://huggingface.co/facebook/unispeech-base-960h"),c(Pe,"rel","nofollow"),c(Ft,"href","/docs/transformers/pr_16478/en/main_classes/configuration#transformers.PretrainedConfig"),c(qt,"href","/docs/transformers/pr_16478/en/main_classes/configuration#transformers.PretrainedConfig"),c(F,"class","docstring"),c(ue,"id","transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"),c(J,"class","relative group"),c(G,"class","docstring"),c(ee,"class","docstring"),c(fe,"id","transformers.UniSpeechModel"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#transformers.UniSpeechModel"),c(te,"class","relative group"),c(Ve,"href","https://arxiv.org/abs/2101.07597"),c(Ve,"rel","nofollow"),c(Pt,"href","/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel"),c(He,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(He,"rel","nofollow"),c(Et,"href","/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechModel"),c(M,"class","docstring"),c(q,"class","docstring"),c(_e,"id","transformers.UniSpeechForCTC"),c(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_e,"href","#transformers.UniSpeechForCTC"),c(ne,"class","relative group"),c(Xe,"href","https://arxiv.org/abs/2101.07597"),c(Xe,"rel","nofollow"),c(Mt,"href","/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel"),c(Ge,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ge,"rel","nofollow"),c(zt,"href","/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(j,"class","docstring"),c(P,"class","docstring"),c(be,"id","transformers.UniSpeechForSequenceClassification"),c(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(be,"href","#transformers.UniSpeechForSequenceClassification"),c(re,"class","relative group"),c(rt,"href","https://arxiv.org/abs/2101.07597"),c(rt,"rel","nofollow"),c(Dt,"href","/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel"),c(lt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(lt,"rel","nofollow"),c(At,"href","/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(x,"class","docstring"),c(C,"class","docstring"),c(ye,"id","transformers.UniSpeechForPreTraining"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#transformers.UniSpeechForPreTraining"),c(ce,"class","relative group"),c(gt,"href","https://arxiv.org/abs/2101.07597"),c(gt,"rel","nofollow"),c(Ot,"href","/docs/transformers/pr_16478/en/main_classes/model#transformers.PreTrainedModel"),c(bt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(bt,"rel","nofollow"),c(Wt,"href","/docs/transformers/pr_16478/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(z,"class","docstring"),c(E,"class","docstring")},m(t,p){e(document.head,m),h(t,k,p),h(t,u,p),e(u,T),e(T,U),b(f,U,null),e(u,g),e(u,$),e($,yn),h(t,$o,p),h(t,R,p),e(R,de),e(de,Rt),b(Ue,Rt,null),e(R,Sn),e(R,Qt),e(Qt,Tn),h(t,Co,p),h(t,pe,p),e(pe,Un),e(pe,ke),e(ke,kn),e(pe,$n),h(t,jo,p),h(t,Ut,p),e(Ut,Cn),h(t,xo,p),h(t,kt,p),e(kt,Xt),e(Xt,jn),h(t,Fo,p),h(t,$t,p),e($t,xn),h(t,qo,p),h(t,he,p),e(he,$e),e($e,Fn),e($e,Ct),e(Ct,qn),e($e,Pn),e(he,En),e(he,Ce),e(Ce,Mn),e(Ce,jt),e(jt,zn),e(Ce,Dn),h(t,Po,p),h(t,N,p),e(N,An),e(N,je),e(je,On),e(N,Wn),e(N,xe),e(xe,Ln),e(N,Nn),h(t,Eo,p),h(t,Q,p),e(Q,me),e(me,Zt),b(Fe,Zt,null),e(Q,Vn),e(Q,Jt),e(Jt,In),h(t,Mo,p),h(t,F,p),b(qe,F,null),e(F,Bn),e(F,X),e(X,Hn),e(X,xt),e(xt,Kn),e(X,Yn),e(X,Pe),e(Pe,Rn),e(X,Qn),e(F,Xn),e(F,Z),e(Z,Zn),e(Z,Ft),e(Ft,Jn),e(Z,Gn),e(Z,qt),e(qt,ea),e(Z,ta),e(F,oa),e(F,Gt),e(Gt,na),e(F,aa),b(Ee,F,null),h(t,zo,p),h(t,J,p),e(J,ue),e(ue,eo),b(Me,eo,null),e(J,sa),e(J,to),e(to,ra),h(t,Do,p),h(t,G,p),b(ze,G,null),e(G,ia),e(G,De),e(De,ca),e(De,oo),e(oo,la),e(De,da),h(t,Ao,p),h(t,ee,p),b(Ae,ee,null),e(ee,pa),e(ee,Oe),e(Oe,ha),e(Oe,no),e(no,ma),e(Oe,ua),h(t,Oo,p),h(t,te,p),e(te,fe),e(fe,ao),b(We,ao,null),e(te,fa),e(te,so),e(so,ga),h(t,Wo,p),h(t,q,p),b(Le,q,null),e(q,_a),e(q,Ne),e(Ne,va),e(Ne,Ve),e(Ve,ba),e(Ne,wa),e(q,ya),e(q,Ie),e(Ie,Sa),e(Ie,Pt),e(Pt,Ta),e(Ie,Ua),e(q,ka),e(q,Be),e(Be,$a),e(Be,He),e(He,Ca),e(Be,ja),e(q,xa),e(q,M),b(Ke,M,null),e(M,Fa),e(M,oe),e(oe,qa),e(oe,Et),e(Et,Pa),e(oe,Ea),e(oe,ro),e(ro,Ma),e(oe,za),e(M,Da),b(ge,M,null),e(M,Aa),e(M,io),e(io,Oa),e(M,Wa),b(Ye,M,null),h(t,Lo,p),h(t,ne,p),e(ne,_e),e(_e,co),b(Re,co,null),e(ne,La),e(ne,lo),e(lo,Na),h(t,No,p),h(t,P,p),b(Qe,P,null),e(P,Va),e(P,ae),e(ae,Ia),e(ae,po),e(po,Ba),e(ae,Ha),e(ae,Xe),e(Xe,Ka),e(ae,Ya),e(P,Ra),e(P,Ze),e(Ze,Qa),e(Ze,Mt),e(Mt,Xa),e(Ze,Za),e(P,Ja),e(P,Je),e(Je,Ga),e(Je,Ge),e(Ge,es),e(Je,ts),e(P,os),e(P,j),b(et,j,null),e(j,ns),e(j,se),e(se,as),e(se,zt),e(zt,ss),e(se,rs),e(se,ho),e(ho,is),e(se,cs),e(j,ls),b(ve,j,null),e(j,ds),e(j,mo),e(mo,ps),e(j,hs),b(tt,j,null),e(j,ms),b(ot,j,null),h(t,Vo,p),h(t,re,p),e(re,be),e(be,uo),b(nt,uo,null),e(re,us),e(re,fo),e(fo,fs),h(t,Io,p),h(t,C,p),b(at,C,null),e(C,gs),e(C,go),e(go,_s),e(C,vs),e(C,st),e(st,bs),e(st,rt),e(rt,ws),e(st,ys),e(C,Ss),e(C,it),e(it,Ts),e(it,Dt),e(Dt,Us),e(it,ks),e(C,$s),e(C,ct),e(ct,Cs),e(ct,lt),e(lt,js),e(ct,xs),e(C,Fs),e(C,x),b(dt,x,null),e(x,qs),e(x,ie),e(ie,Ps),e(ie,At),e(At,Es),e(ie,Ms),e(ie,_o),e(_o,zs),e(ie,Ds),e(x,As),b(we,x,null),e(x,Os),e(x,vo),e(vo,Ws),e(x,Ls),b(pt,x,null),e(x,Ns),b(ht,x,null),h(t,Bo,p),h(t,ce,p),e(ce,ye),e(ye,bo),b(mt,bo,null),e(ce,Vs),e(ce,wo),e(wo,Is),h(t,Ho,p),h(t,E,p),b(ut,E,null),e(E,Bs),e(E,ft),e(ft,Hs),e(ft,gt),e(gt,Ks),e(ft,Ys),e(E,Rs),e(E,_t),e(_t,Qs),e(_t,Ot),e(Ot,Xs),e(_t,Zs),e(E,Js),e(E,vt),e(vt,Gs),e(vt,bt),e(bt,er),e(vt,tr),e(E,or),e(E,z),b(wt,z,null),e(z,nr),e(z,le),e(le,ar),e(le,Wt),e(Wt,sr),e(le,rr),e(le,yo),e(yo,ir),e(le,cr),e(z,lr),b(Se,z,null),e(z,dr),e(z,So),e(So,pr),e(z,hr),b(yt,z,null),Ko=!0},p(t,[p]){const St={};p&2&&(St.$$scope={dirty:p,ctx:t}),ge.$set(St);const To={};p&2&&(To.$$scope={dirty:p,ctx:t}),ve.$set(To);const Uo={};p&2&&(Uo.$$scope={dirty:p,ctx:t}),we.$set(Uo);const ko={};p&2&&(ko.$$scope={dirty:p,ctx:t}),Se.$set(ko)},i(t){Ko||(w(f.$$.fragment,t),w(Ue.$$.fragment,t),w(Fe.$$.fragment,t),w(qe.$$.fragment,t),w(Ee.$$.fragment,t),w(Me.$$.fragment,t),w(ze.$$.fragment,t),w(Ae.$$.fragment,t),w(We.$$.fragment,t),w(Le.$$.fragment,t),w(Ke.$$.fragment,t),w(ge.$$.fragment,t),w(Ye.$$.fragment,t),w(Re.$$.fragment,t),w(Qe.$$.fragment,t),w(et.$$.fragment,t),w(ve.$$.fragment,t),w(tt.$$.fragment,t),w(ot.$$.fragment,t),w(nt.$$.fragment,t),w(at.$$.fragment,t),w(dt.$$.fragment,t),w(we.$$.fragment,t),w(pt.$$.fragment,t),w(ht.$$.fragment,t),w(mt.$$.fragment,t),w(ut.$$.fragment,t),w(wt.$$.fragment,t),w(Se.$$.fragment,t),w(yt.$$.fragment,t),Ko=!0)},o(t){y(f.$$.fragment,t),y(Ue.$$.fragment,t),y(Fe.$$.fragment,t),y(qe.$$.fragment,t),y(Ee.$$.fragment,t),y(Me.$$.fragment,t),y(ze.$$.fragment,t),y(Ae.$$.fragment,t),y(We.$$.fragment,t),y(Le.$$.fragment,t),y(Ke.$$.fragment,t),y(ge.$$.fragment,t),y(Ye.$$.fragment,t),y(Re.$$.fragment,t),y(Qe.$$.fragment,t),y(et.$$.fragment,t),y(ve.$$.fragment,t),y(tt.$$.fragment,t),y(ot.$$.fragment,t),y(nt.$$.fragment,t),y(at.$$.fragment,t),y(dt.$$.fragment,t),y(we.$$.fragment,t),y(pt.$$.fragment,t),y(ht.$$.fragment,t),y(mt.$$.fragment,t),y(ut.$$.fragment,t),y(wt.$$.fragment,t),y(Se.$$.fragment,t),y(yt.$$.fragment,t),Ko=!1},d(t){o(m),t&&o(k),t&&o(u),S(f),t&&o($o),t&&o(R),S(Ue),t&&o(Co),t&&o(pe),t&&o(jo),t&&o(Ut),t&&o(xo),t&&o(kt),t&&o(Fo),t&&o($t),t&&o(qo),t&&o(he),t&&o(Po),t&&o(N),t&&o(Eo),t&&o(Q),S(Fe),t&&o(Mo),t&&o(F),S(qe),S(Ee),t&&o(zo),t&&o(J),S(Me),t&&o(Do),t&&o(G),S(ze),t&&o(Ao),t&&o(ee),S(Ae),t&&o(Oo),t&&o(te),S(We),t&&o(Wo),t&&o(q),S(Le),S(Ke),S(ge),S(Ye),t&&o(Lo),t&&o(ne),S(Re),t&&o(No),t&&o(P),S(Qe),S(et),S(ve),S(tt),S(ot),t&&o(Vo),t&&o(re),S(nt),t&&o(Io),t&&o(C),S(at),S(dt),S(we),S(pt),S(ht),t&&o(Bo),t&&o(ce),S(mt),t&&o(Ho),t&&o(E),S(ut),S(wt),S(Se),S(yt)}}}const Pi={local:"unispeech",sections:[{local:"overview",title:"Overview"},{local:"transformers.UniSpeechConfig",title:"UniSpeechConfig"},{local:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",title:"UniSpeech specific outputs"},{local:"transformers.UniSpeechModel",title:"UniSpeechModel"},{local:"transformers.UniSpeechForCTC",title:"UniSpeechForCTC"},{local:"transformers.UniSpeechForSequenceClassification",title:"UniSpeechForSequenceClassification"},{local:"transformers.UniSpeechForPreTraining",title:"UniSpeechForPreTraining"}],title:"UniSpeech"};function Ei(L){return $i(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Wi extends Si{constructor(m){super();Ti(this,m,Ei,qi,Ui,{})}}export{Wi as default,Pi as metadata};
