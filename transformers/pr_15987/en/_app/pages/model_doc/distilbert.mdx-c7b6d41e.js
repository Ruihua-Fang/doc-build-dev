import{S as ry,i as ay,s as iy,e as o,k as l,w as T,t as a,M as ly,c as n,d as t,m as d,a as r,x as k,h as i,b as c,F as e,g as u,y as w,q as $,o as D,B as F}from"../../chunks/vendor-4833417e.js";import{T as me}from"../../chunks/Tip-fffd6df1.js";import{D as U}from"../../chunks/Docstring-4f315ed9.js";import{C as ye}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as ve}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function dy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function cy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function py(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function hy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function uy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function fy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function my(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le;return{c(){p=o("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),b=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=o("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),M=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=o("code"),pe=a("model(inputs)"),re=a("."),N=l(),q=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),J=l(),z=o("ul"),x=o("li"),he=a("a single Tensor with "),W=o("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=o("code"),ae=a("model(inputs_ids)"),ee=l(),A=o("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),P=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=o("code"),le=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var E=r(p);y=i(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(t),g=d(h),v=n(h,"UL",{});var K=r(v);b=n(K,"LI",{});var ge=r(b);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var be=r(B);de=i(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),V=d(h),M=n(h,"P",{});var I=r(M);G=i(I,"This second option is useful when using "),S=n(I,"CODE",{});var _e=r(S);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);pe=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),N=d(h),q=n(h,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),J=d(h),z=n(h,"UL",{});var C=r(z);x=n(C,"LI",{});var Q=r(x);he=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);se=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),P=n(C,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),C.forEach(t)},m(h,E){u(h,p,E),e(p,y),u(h,g,E),u(h,v,E),e(v,b),e(b,_),e(v,f),e(v,B),e(B,de),u(h,V,E),u(h,M,E),e(M,G),e(M,S),e(S,X),e(M,ce),e(M,O),e(O,pe),e(M,re),u(h,N,E),u(h,q,E),e(q,Y),u(h,J,E),u(h,z,E),e(z,x),e(x,he),e(x,W),e(W,se),e(x,ue),e(x,R),e(R,ae),e(z,ee),e(z,A),e(A,ie),e(A,L),e(L,oe),e(z,fe),e(z,P),e(P,te),e(P,H),e(H,le)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(M),h&&t(N),h&&t(q),h&&t(J),h&&t(z)}}}function gy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function _y(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le;return{c(){p=o("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),b=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=o("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),M=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=o("code"),pe=a("model(inputs)"),re=a("."),N=l(),q=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),J=l(),z=o("ul"),x=o("li"),he=a("a single Tensor with "),W=o("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=o("code"),ae=a("model(inputs_ids)"),ee=l(),A=o("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),P=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=o("code"),le=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var E=r(p);y=i(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(t),g=d(h),v=n(h,"UL",{});var K=r(v);b=n(K,"LI",{});var ge=r(b);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var be=r(B);de=i(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),V=d(h),M=n(h,"P",{});var I=r(M);G=i(I,"This second option is useful when using "),S=n(I,"CODE",{});var _e=r(S);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);pe=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),N=d(h),q=n(h,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),J=d(h),z=n(h,"UL",{});var C=r(z);x=n(C,"LI",{});var Q=r(x);he=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);se=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),P=n(C,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),C.forEach(t)},m(h,E){u(h,p,E),e(p,y),u(h,g,E),u(h,v,E),e(v,b),e(b,_),e(v,f),e(v,B),e(B,de),u(h,V,E),u(h,M,E),e(M,G),e(M,S),e(S,X),e(M,ce),e(M,O),e(O,pe),e(M,re),u(h,N,E),u(h,q,E),e(q,Y),u(h,J,E),u(h,z,E),e(z,x),e(x,he),e(x,W),e(W,se),e(x,ue),e(x,R),e(R,ae),e(z,ee),e(z,A),e(A,ie),e(A,L),e(L,oe),e(z,fe),e(z,P),e(P,te),e(P,H),e(H,le)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(M),h&&t(N),h&&t(q),h&&t(J),h&&t(z)}}}function vy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function by(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le;return{c(){p=o("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),b=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=o("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),M=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=o("code"),pe=a("model(inputs)"),re=a("."),N=l(),q=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),J=l(),z=o("ul"),x=o("li"),he=a("a single Tensor with "),W=o("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=o("code"),ae=a("model(inputs_ids)"),ee=l(),A=o("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),P=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=o("code"),le=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var E=r(p);y=i(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(t),g=d(h),v=n(h,"UL",{});var K=r(v);b=n(K,"LI",{});var ge=r(b);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var be=r(B);de=i(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),V=d(h),M=n(h,"P",{});var I=r(M);G=i(I,"This second option is useful when using "),S=n(I,"CODE",{});var _e=r(S);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);pe=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),N=d(h),q=n(h,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),J=d(h),z=n(h,"UL",{});var C=r(z);x=n(C,"LI",{});var Q=r(x);he=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);se=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),P=n(C,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),C.forEach(t)},m(h,E){u(h,p,E),e(p,y),u(h,g,E),u(h,v,E),e(v,b),e(b,_),e(v,f),e(v,B),e(B,de),u(h,V,E),u(h,M,E),e(M,G),e(M,S),e(S,X),e(M,ce),e(M,O),e(O,pe),e(M,re),u(h,N,E),u(h,q,E),e(q,Y),u(h,J,E),u(h,z,E),e(z,x),e(x,he),e(x,W),e(W,se),e(x,ue),e(x,R),e(R,ae),e(z,ee),e(z,A),e(A,ie),e(A,L),e(L,oe),e(z,fe),e(z,P),e(P,te),e(P,H),e(H,le)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(M),h&&t(N),h&&t(q),h&&t(J),h&&t(z)}}}function Ty(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function ky(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le;return{c(){p=o("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),b=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=o("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),M=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=o("code"),pe=a("model(inputs)"),re=a("."),N=l(),q=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),J=l(),z=o("ul"),x=o("li"),he=a("a single Tensor with "),W=o("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=o("code"),ae=a("model(inputs_ids)"),ee=l(),A=o("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),P=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=o("code"),le=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var E=r(p);y=i(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(t),g=d(h),v=n(h,"UL",{});var K=r(v);b=n(K,"LI",{});var ge=r(b);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var be=r(B);de=i(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),V=d(h),M=n(h,"P",{});var I=r(M);G=i(I,"This second option is useful when using "),S=n(I,"CODE",{});var _e=r(S);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);pe=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),N=d(h),q=n(h,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),J=d(h),z=n(h,"UL",{});var C=r(z);x=n(C,"LI",{});var Q=r(x);he=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);se=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),P=n(C,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),C.forEach(t)},m(h,E){u(h,p,E),e(p,y),u(h,g,E),u(h,v,E),e(v,b),e(b,_),e(v,f),e(v,B),e(B,de),u(h,V,E),u(h,M,E),e(M,G),e(M,S),e(S,X),e(M,ce),e(M,O),e(O,pe),e(M,re),u(h,N,E),u(h,q,E),e(q,Y),u(h,J,E),u(h,z,E),e(z,x),e(x,he),e(x,W),e(W,se),e(x,ue),e(x,R),e(R,ae),e(z,ee),e(z,A),e(A,ie),e(A,L),e(L,oe),e(z,fe),e(z,P),e(P,te),e(P,H),e(H,le)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(M),h&&t(N),h&&t(q),h&&t(J),h&&t(z)}}}function wy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function $y(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le;return{c(){p=o("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),b=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=o("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),M=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=o("code"),pe=a("model(inputs)"),re=a("."),N=l(),q=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),J=l(),z=o("ul"),x=o("li"),he=a("a single Tensor with "),W=o("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=o("code"),ae=a("model(inputs_ids)"),ee=l(),A=o("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),P=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=o("code"),le=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var E=r(p);y=i(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(t),g=d(h),v=n(h,"UL",{});var K=r(v);b=n(K,"LI",{});var ge=r(b);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var be=r(B);de=i(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),V=d(h),M=n(h,"P",{});var I=r(M);G=i(I,"This second option is useful when using "),S=n(I,"CODE",{});var _e=r(S);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);pe=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),N=d(h),q=n(h,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),J=d(h),z=n(h,"UL",{});var C=r(z);x=n(C,"LI",{});var Q=r(x);he=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);se=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),P=n(C,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),C.forEach(t)},m(h,E){u(h,p,E),e(p,y),u(h,g,E),u(h,v,E),e(v,b),e(b,_),e(v,f),e(v,B),e(B,de),u(h,V,E),u(h,M,E),e(M,G),e(M,S),e(S,X),e(M,ce),e(M,O),e(O,pe),e(M,re),u(h,N,E),u(h,q,E),e(q,Y),u(h,J,E),u(h,z,E),e(z,x),e(x,he),e(x,W),e(W,se),e(x,ue),e(x,R),e(R,ae),e(z,ee),e(z,A),e(A,ie),e(A,L),e(L,oe),e(z,fe),e(z,P),e(P,te),e(P,H),e(H,le)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(M),h&&t(N),h&&t(q),h&&t(J),h&&t(z)}}}function Dy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function Fy(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le;return{c(){p=o("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),v=o("ul"),b=o("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=o("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),V=l(),M=o("p"),G=a("This second option is useful when using "),S=o("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=o("code"),pe=a("model(inputs)"),re=a("."),N=l(),q=o("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),J=l(),z=o("ul"),x=o("li"),he=a("a single Tensor with "),W=o("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=o("code"),ae=a("model(inputs_ids)"),ee=l(),A=o("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o("code"),oe=a("model([input_ids, attention_mask])"),fe=l(),P=o("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=o("code"),le=a('model({"input_ids": input_ids})')},l(h){p=n(h,"P",{});var E=r(p);y=i(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(t),g=d(h),v=n(h,"UL",{});var K=r(v);b=n(K,"LI",{});var ge=r(b);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var be=r(B);de=i(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),V=d(h),M=n(h,"P",{});var I=r(M);G=i(I,"This second option is useful when using "),S=n(I,"CODE",{});var _e=r(S);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);pe=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),N=d(h),q=n(h,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),J=d(h),z=n(h,"UL",{});var C=r(z);x=n(C,"LI",{});var Q=r(x);he=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);se=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(C),A=n(C,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n(Z,"CODE",{});var De=r(L);oe=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),P=n(C,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),C.forEach(t)},m(h,E){u(h,p,E),e(p,y),u(h,g,E),u(h,v,E),e(v,b),e(b,_),e(v,f),e(v,B),e(B,de),u(h,V,E),u(h,M,E),e(M,G),e(M,S),e(S,X),e(M,ce),e(M,O),e(O,pe),e(M,re),u(h,N,E),u(h,q,E),e(q,Y),u(h,J,E),u(h,z,E),e(z,x),e(x,he),e(x,W),e(W,se),e(x,ue),e(x,R),e(R,ae),e(z,ee),e(z,A),e(A,ie),e(A,L),e(L,oe),e(z,fe),e(z,P),e(P,te),e(P,H),e(H,le)},d(h){h&&t(p),h&&t(g),h&&t(v),h&&t(V),h&&t(M),h&&t(N),h&&t(q),h&&t(J),h&&t(z)}}}function yy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function By(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function Ey(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function My(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function zy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function xy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function jy(j){let p,y,g,v,b;return{c(){p=o("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o("code"),v=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=n(_,"P",{});var f=r(p);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);v=i(B,"Module"),B.forEach(t),b=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,p,f),e(p,y),e(p,g),e(g,v),e(p,b)},d(_){_&&t(p)}}}function Cy(j){let p,y,g,v,b,_,f,B,de,V,M,G,S,X,ce,O,pe,re,N,q,Y,J,z,x,he,W,se,ue,R,ae,ee,A,ie,L,oe,fe,P,te,H,le,h,E,K,ge,be,I,_e,Te,ke,C,Q,we,$e,Z,De,ne,Fe,hu,up,bt,uu,Yo,fu,mu,Zo,gu,_u,en,vu,bu,fp,as,Ks,_l,tn,Tu,vl,ku,mp,Se,sn,wu,yt,$u,di,Du,Fu,ci,yu,Bu,on,Eu,Mu,zu,is,xu,pi,ju,Cu,hi,Pu,qu,Au,bl,Iu,Nu,nn,gp,ls,Gs,Tl,rn,Lu,kl,Su,_p,_t,an,Ou,wl,Wu,Ru,Xs,ui,Hu,Qu,fi,Uu,Vu,Ju,ln,Ku,mi,Gu,Xu,vp,ds,Ys,$l,dn,Yu,Dl,Zu,bp,vt,cn,ef,pn,tf,Fl,sf,of,nf,Zs,gi,rf,af,_i,lf,df,cf,hn,pf,vi,hf,uf,Tp,cs,eo,yl,un,ff,Bl,mf,kp,Oe,fn,gf,El,_f,vf,mn,bf,bi,Tf,kf,wf,gn,$f,_n,Df,Ff,yf,Ve,vn,Bf,ps,Ef,Ti,Mf,zf,Ml,xf,jf,Cf,to,Pf,zl,qf,Af,bn,wp,hs,so,xl,Tn,If,jl,Nf,$p,We,kn,Lf,wn,Sf,Cl,Of,Wf,Rf,$n,Hf,ki,Qf,Uf,Vf,Dn,Jf,Fn,Kf,Gf,Xf,Je,yn,Yf,us,Zf,wi,em,tm,Pl,sm,om,nm,oo,rm,ql,am,im,Bn,Dp,fs,no,Al,En,lm,Il,dm,Fp,Re,Mn,cm,Nl,pm,hm,zn,um,$i,fm,mm,gm,xn,_m,jn,vm,bm,Tm,Ce,Cn,km,ms,wm,Di,$m,Dm,Ll,Fm,ym,Bm,ro,Em,Sl,Mm,zm,Pn,xm,Ol,jm,Cm,qn,yp,gs,ao,Wl,An,Pm,Rl,qm,Bp,He,In,Am,Hl,Im,Nm,Nn,Lm,Fi,Sm,Om,Wm,Ln,Rm,Sn,Hm,Qm,Um,Ke,On,Vm,_s,Jm,yi,Km,Gm,Ql,Xm,Ym,Zm,io,eg,Ul,tg,sg,Wn,Ep,vs,lo,Vl,Rn,og,Jl,ng,Mp,Qe,Hn,rg,Kl,ag,ig,Qn,lg,Bi,dg,cg,pg,Un,hg,Vn,ug,fg,mg,Ge,Jn,gg,bs,_g,Ei,vg,bg,Gl,Tg,kg,wg,co,$g,Xl,Dg,Fg,Kn,zp,Ts,po,Yl,Gn,yg,Zl,Bg,xp,Ue,Xn,Eg,ks,Mg,ed,zg,xg,td,jg,Cg,Pg,Yn,qg,Mi,Ag,Ig,Ng,Zn,Lg,er,Sg,Og,Wg,Xe,tr,Rg,ws,Hg,zi,Qg,Ug,sd,Vg,Jg,Kg,ho,Gg,od,Xg,Yg,sr,jp,$s,uo,nd,or,Zg,rd,e_,Cp,Pe,nr,t_,ad,s_,o_,rr,n_,xi,r_,a_,i_,ar,l_,ir,d_,c_,p_,fo,h_,Ye,lr,u_,Ds,f_,ji,m_,g_,id,__,v_,b_,mo,T_,ld,k_,w_,dr,Pp,Fs,go,dd,cr,$_,cd,D_,qp,qe,pr,F_,hr,y_,pd,B_,E_,M_,ur,z_,Ci,x_,j_,C_,fr,P_,mr,q_,A_,I_,_o,N_,Ze,gr,L_,ys,S_,Pi,O_,W_,hd,R_,H_,Q_,vo,U_,ud,V_,J_,_r,Ap,Bs,bo,fd,vr,K_,md,G_,Ip,Ae,br,X_,gd,Y_,Z_,Tr,ev,qi,tv,sv,ov,kr,nv,wr,rv,av,iv,To,lv,et,$r,dv,Es,cv,Ai,pv,hv,_d,uv,fv,mv,ko,gv,vd,_v,vv,Dr,Np,Ms,wo,bd,Fr,bv,Td,Tv,Lp,Ie,yr,kv,kd,wv,$v,Br,Dv,Ii,Fv,yv,Bv,Er,Ev,Mr,Mv,zv,xv,$o,jv,tt,zr,Cv,zs,Pv,Ni,qv,Av,wd,Iv,Nv,Lv,Do,Sv,$d,Ov,Wv,xr,Sp,xs,Fo,Dd,jr,Rv,Fd,Hv,Op,Ne,Cr,Qv,yd,Uv,Vv,Pr,Jv,Li,Kv,Gv,Xv,qr,Yv,Ar,Zv,eb,tb,yo,sb,st,Ir,ob,js,nb,Si,rb,ab,Bd,ib,lb,db,Bo,cb,Ed,pb,hb,Nr,Wp,Cs,Eo,Md,Lr,ub,zd,fb,Rp,Le,Sr,mb,Ps,gb,xd,_b,vb,jd,bb,Tb,kb,Or,wb,Oi,$b,Db,Fb,Wr,yb,Rr,Bb,Eb,Mb,Mo,zb,ot,Hr,xb,qs,jb,Wi,Cb,Pb,Cd,qb,Ab,Ib,zo,Nb,Pd,Lb,Sb,Qr,Hp,As,xo,qd,Ur,Ob,Ad,Wb,Qp,Be,Vr,Rb,Id,Hb,Qb,Jr,Ub,Ri,Vb,Jb,Kb,Kr,Gb,Gr,Xb,Yb,Zb,Nd,e1,t1,Bt,Ld,Xr,s1,o1,Sd,Yr,n1,r1,Od,Zr,a1,i1,Wd,ea,l1,d1,nt,ta,c1,Is,p1,Rd,h1,u1,Hd,f1,m1,g1,jo,_1,Qd,v1,b1,sa,Up,Ns,Co,Ud,oa,T1,Vd,k1,Vp,Ee,na,w1,ra,$1,Jd,D1,F1,y1,aa,B1,Hi,E1,M1,z1,ia,x1,la,j1,C1,P1,Kd,q1,A1,Et,Gd,da,I1,N1,Xd,ca,L1,S1,Yd,pa,O1,W1,Zd,ha,R1,H1,rt,ua,Q1,Ls,U1,ec,V1,J1,tc,K1,G1,X1,Po,Y1,sc,Z1,eT,fa,Jp,Ss,qo,oc,ma,tT,nc,sT,Kp,Me,ga,oT,rc,nT,rT,_a,aT,Qi,iT,lT,dT,va,cT,ba,pT,hT,uT,ac,fT,mT,Mt,ic,Ta,gT,_T,lc,ka,vT,bT,dc,wa,TT,kT,cc,$a,wT,$T,at,Da,DT,Os,FT,pc,yT,BT,hc,ET,MT,zT,Ao,xT,uc,jT,CT,Fa,Gp,Ws,Io,fc,ya,PT,mc,qT,Xp,ze,Ba,AT,gc,IT,NT,Ea,LT,Ui,ST,OT,WT,Ma,RT,za,HT,QT,UT,_c,VT,JT,zt,vc,xa,KT,GT,bc,ja,XT,YT,Tc,Ca,ZT,ek,kc,Pa,tk,sk,it,qa,ok,Rs,nk,wc,rk,ak,$c,ik,lk,dk,No,ck,Dc,pk,hk,Aa,Yp,Hs,Lo,Fc,Ia,uk,yc,fk,Zp,xe,Na,mk,Bc,gk,_k,La,vk,Vi,bk,Tk,kk,Sa,wk,Oa,$k,Dk,Fk,Ec,yk,Bk,xt,Mc,Wa,Ek,Mk,zc,Ra,zk,xk,xc,Ha,jk,Ck,jc,Qa,Pk,qk,lt,Ua,Ak,Qs,Ik,Cc,Nk,Lk,Pc,Sk,Ok,Wk,So,Rk,qc,Hk,Qk,Va,eh,Us,Oo,Ac,Ja,Uk,Ic,Vk,th,je,Ka,Jk,Vs,Kk,Nc,Gk,Xk,Lc,Yk,Zk,ew,Ga,tw,Ji,sw,ow,nw,Xa,rw,Ya,aw,iw,lw,Sc,dw,cw,jt,Oc,Za,pw,hw,Wc,ei,uw,fw,Rc,ti,mw,gw,Hc,si,_w,vw,dt,oi,bw,Js,Tw,Qc,kw,ww,Uc,$w,Dw,Fw,Wo,yw,Vc,Bw,Ew,ni,sh;return _=new ve({}),X=new ve({}),tn=new ve({}),sn=new U({props:{name:"class transformers.DistilBertConfig",anchor:"transformers.DistilBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"max_position_embeddings",val:" = 512"},{name:"sinusoidal_pos_embds",val:" = False"},{name:"n_layers",val:" = 6"},{name:"n_heads",val:" = 12"},{name:"dim",val:" = 768"},{name:"hidden_dim",val:" = 3072"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"qa_dropout",val:" = 0.1"},{name:"seq_classif_dropout",val:" = 0.2"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/configuration_distilbert.py#L37",parametersDescription:[{anchor:"transformers.DistilBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> or <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DistilBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DistilBertConfig.sinusoidal_pos_embds",description:`<strong>sinusoidal_pos_embds</strong> (<code>boolean</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sinusoidal positional embeddings.`,name:"sinusoidal_pos_embds"},{anchor:"transformers.DistilBertConfig.n_layers",description:`<strong>n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layers"},{anchor:"transformers.DistilBertConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_heads"},{anchor:"transformers.DistilBertConfig.dim",description:`<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"dim"},{anchor:"transformers.DistilBertConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
The size of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"hidden_dim"},{anchor:"transformers.DistilBertConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DistilBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DistilBertConfig.activation",description:`<strong>activation</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation"},{anchor:"transformers.DistilBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DistilBertConfig.qa_dropout",description:`<strong>qa_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilities used in the question answering model <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a>.`,name:"qa_dropout"},{anchor:"transformers.DistilBertConfig.seq_classif_dropout",description:`<strong>seq_classif_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The dropout probabilities used in the sequence classification and the multiple choice model
<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>.`,name:"seq_classif_dropout"}]}}),nn=new ye({props:{code:`from transformers import DistilBertModel, DistilBertConfig

# Initializing a DistilBERT configuration
configuration = DistilBertConfig()

# Initializing a model from the configuration
model = DistilBertModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertModel, DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DistilBERT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DistilBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),rn=new ve({}),an=new U({props:{name:"class transformers.DistilBertTokenizer",anchor:"transformers.DistilBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/tokenization_distilbert.py#L56"}}),dn=new ve({}),cn=new U({props:{name:"class transformers.DistilBertTokenizerFast",anchor:"transformers.DistilBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/tokenization_distilbert_fast.py#L65"}}),un=new ve({}),fn=new U({props:{name:"class transformers.DistilBertModel",anchor:"transformers.DistilBertModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L435",parametersDescription:[{anchor:"transformers.DistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vn=new U({props:{name:"forward",anchor:"transformers.DistilBertModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L507",parametersDescription:[{anchor:"transformers.DistilBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),to=new me({props:{$$slots:{default:[dy]},$$scope:{ctx:j}}}),bn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertModel
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Tn=new ve({}),kn=new U({props:{name:"class transformers.DistilBertForMaskedLM",anchor:"transformers.DistilBertForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L563",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yn=new U({props:{name:"forward",anchor:"transformers.DistilBertForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L603",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),oo=new me({props:{$$slots:{default:[cy]},$$scope:{ctx:j}}}),Bn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMaskedLM
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),En=new ve({}),Mn=new U({props:{name:"class transformers.DistilBertForSequenceClassification",anchor:"transformers.DistilBertForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L667",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Cn=new U({props:{name:"forward",anchor:"transformers.DistilBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L701",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ro=new me({props:{$$slots:{default:[py]},$$scope:{ctx:j}}}),Pn=new ye({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

torch.manual_seed(0)
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),qn=new ye({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

torch.manual_seed(0)
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", problem_type="multi_label_classification", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),An=new ve({}),In=new U({props:{name:"class transformers.DistilBertForMultipleChoice",anchor:"transformers.DistilBertForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L997",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),On=new U({props:{name:"forward",anchor:"transformers.DistilBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L1029",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),io=new me({props:{$$slots:{default:[hy]},$$scope:{ctx:j}}}),Wn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMultipleChoice
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")
model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Rn=new ve({}),Hn=new U({props:{name:"class transformers.DistilBertForTokenClassification",anchor:"transformers.DistilBertForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L902",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Jn=new U({props:{name:"forward",anchor:"transformers.DistilBertForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L934",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),co=new me({props:{$$slots:{default:[uy]},$$scope:{ctx:j}}}),Kn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForTokenClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Gn=new ve({}),Xn=new U({props:{name:"class transformers.DistilBertForQuestionAnswering",anchor:"transformers.DistilBertForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L785",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),tr=new U({props:{name:"forward",anchor:"transformers.DistilBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_distilbert.py#L817",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ho=new me({props:{$$slots:{default:[fy]},$$scope:{ctx:j}}}),sr=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch

torch.manual_seed(0)
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
round(loss.item(), 2)


start_scores = outputs.start_logits
list(start_scores.shape)


end_scores = outputs.end_logits
list(end_scores.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(start_scores.shape)


<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(end_scores.shape)
`}}),or=new ve({}),nr=new U({props:{name:"class transformers.TFDistilBertModel",anchor:"transformers.TFDistilBertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L536",parametersDescription:[{anchor:"transformers.TFDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fo=new me({props:{$$slots:{default:[my]},$$scope:{ctx:j}}}),lr=new U({props:{name:"call",anchor:"transformers.TFDistilBertModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L541",parametersDescription:[{anchor:"transformers.TFDistilBertModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),mo=new me({props:{$$slots:{default:[gy]},$$scope:{ctx:j}}}),dr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),cr=new ve({}),pr=new U({props:{name:"class transformers.TFDistilBertForMaskedLM",anchor:"transformers.TFDistilBertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L636",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_o=new me({props:{$$slots:{default:[_y]},$$scope:{ctx:j}}}),gr=new U({props:{name:"call",anchor:"transformers.TFDistilBertForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L656",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),vo=new me({props:{$$slots:{default:[vy]},$$scope:{ctx:j}}}),_r=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),vr=new ve({}),br=new U({props:{name:"class transformers.TFDistilBertForSequenceClassification",anchor:"transformers.TFDistilBertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L740",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),To=new me({props:{$$slots:{default:[by]},$$scope:{ctx:j}}}),$r=new U({props:{name:"call",anchor:"transformers.TFDistilBertForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L757",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ko=new me({props:{$$slots:{default:[Ty]},$$scope:{ctx:j}}}),Dr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Fr=new ve({}),yr=new U({props:{name:"class transformers.TFDistilBertForMultipleChoice",anchor:"transformers.TFDistilBertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L931",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),$o=new me({props:{$$slots:{default:[ky]},$$scope:{ctx:j}}}),zr=new U({props:{name:"call",anchor:"transformers.TFDistilBertForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L957",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Do=new me({props:{$$slots:{default:[wy]},$$scope:{ctx:j}}}),xr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMultipleChoice
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),jr=new ve({}),Cr=new U({props:{name:"class transformers.TFDistilBertForTokenClassification",anchor:"transformers.TFDistilBertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L841",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yo=new me({props:{$$slots:{default:[$y]},$$scope:{ctx:j}}}),Ir=new U({props:{name:"call",anchor:"transformers.TFDistilBertForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L852",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Bo=new me({props:{$$slots:{default:[Dy]},$$scope:{ctx:j}}}),Nr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Lr=new ve({}),Sr=new U({props:{name:"class transformers.TFDistilBertForQuestionAnswering",anchor:"transformers.TFDistilBertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1073",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new me({props:{$$slots:{default:[Fy]},$$scope:{ctx:j}}}),Hr=new U({props:{name:"call",anchor:"transformers.TFDistilBertForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1084",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),zo=new me({props:{$$slots:{default:[yy]},$$scope:{ctx:j}}}),Qr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),Ur=new ve({}),Vr=new U({props:{name:"class transformers.FlaxDistilBertModel",anchor:"transformers.FlaxDistilBertModel",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L527",parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ta=new U({props:{name:"__call__",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}]}}),jo=new me({props:{$$slots:{default:[By]},$$scope:{ctx:j}}}),sa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),oa=new ve({}),na=new U({props:{name:"class transformers.FlaxDistilBertForMaskedLM",anchor:"transformers.FlaxDistilBertForMaskedLM",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L600",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ua=new U({props:{name:"__call__",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Po=new me({props:{$$slots:{default:[Ey]},$$scope:{ctx:j}}}),fa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMaskedLM

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ma=new ve({}),ga=new U({props:{name:"class transformers.FlaxDistilBertForSequenceClassification",anchor:"transformers.FlaxDistilBertForSequenceClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L669",parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Da=new U({props:{name:"__call__",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Ao=new me({props:{$$slots:{default:[My]},$$scope:{ctx:j}}}),Fa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ya=new ve({}),Ba=new U({props:{name:"class transformers.FlaxDistilBertForMultipleChoice",anchor:"transformers.FlaxDistilBertForMultipleChoice",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L749",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qa=new U({props:{name:"__call__",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),No=new me({props:{$$slots:{default:[zy]},$$scope:{ctx:j}}}),Aa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMultipleChoice

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ia=new ve({}),Na=new U({props:{name:"class transformers.FlaxDistilBertForTokenClassification",anchor:"transformers.FlaxDistilBertForTokenClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L815",parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ua=new U({props:{name:"__call__",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),So=new me({props:{$$slots:{default:[xy]},$$scope:{ctx:j}}}),Va=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ja=new ve({}),Ka=new U({props:{name:"class transformers.FlaxDistilBertForQuestionAnswering",anchor:"transformers.FlaxDistilBertForQuestionAnswering",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L885",parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),oi=new U({props:{name:"__call__",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Wo=new me({props:{$$slots:{default:[jy]},$$scope:{ctx:j}}}),ni=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){p=o("meta"),y=l(),g=o("h1"),v=o("a"),b=o("span"),T(_.$$.fragment),f=l(),B=o("span"),de=a("DistilBERT"),V=l(),M=o("h2"),G=o("a"),S=o("span"),T(X.$$.fragment),ce=l(),O=o("span"),pe=a("Overview"),re=l(),N=o("p"),q=a("The DistilBERT model was proposed in the blog post "),Y=o("a"),J=a(`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),z=a(", and the paper "),x=o("a"),he=a(`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),W=a(`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),se=o("em"),ue=a("bert-base-uncased"),R=a(`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),ae=l(),ee=o("p"),A=a("The abstract from the paper is the following:"),ie=l(),L=o("p"),oe=o("em"),fe=a(`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),P=l(),te=o("p"),H=a("Tips:"),le=l(),h=o("ul"),E=o("li"),K=a("DistilBERT doesn\u2019t have "),ge=o("code"),be=a("token_type_ids"),I=a(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),_e=o("code"),Te=a("tokenizer.sep_token"),ke=a(" (or "),C=o("code"),Q=a("[SEP]"),we=a(")."),$e=l(),Z=o("li"),De=a("DistilBERT doesn\u2019t have options to select the input positions ("),ne=o("code"),Fe=a("position_ids"),hu=a(` input). This could be added if
necessary though, just let us know if you need this option.`),up=l(),bt=o("p"),uu=a("This model was contributed by "),Yo=o("a"),fu=a("victorsanh"),mu=a(`. This model jax version was
contributed by `),Zo=o("a"),gu=a("kamalkraj"),_u=a(". The original code can be found "),en=o("a"),vu=a("here"),bu=a("."),fp=l(),as=o("h2"),Ks=o("a"),_l=o("span"),T(tn.$$.fragment),Tu=l(),vl=o("span"),ku=a("DistilBertConfig"),mp=l(),Se=o("div"),T(sn.$$.fragment),wu=l(),yt=o("p"),$u=a("This is the configuration class to store the configuration of a "),di=o("a"),Du=a("DistilBertModel"),Fu=a(" or a "),ci=o("a"),yu=a("TFDistilBertModel"),Bu=a(`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),on=o("a"),Eu=a("distilbert-base-uncased"),Mu=a(" architecture."),zu=l(),is=o("p"),xu=a("Configuration objects inherit from "),pi=o("a"),ju=a("PretrainedConfig"),Cu=a(` and can be used to control the model outputs. Read the
documentation from `),hi=o("a"),Pu=a("PretrainedConfig"),qu=a(" for more information."),Au=l(),bl=o("p"),Iu=a("Examples:"),Nu=l(),T(nn.$$.fragment),gp=l(),ls=o("h2"),Gs=o("a"),Tl=o("span"),T(rn.$$.fragment),Lu=l(),kl=o("span"),Su=a("DistilBertTokenizer"),_p=l(),_t=o("div"),T(an.$$.fragment),Ou=l(),wl=o("p"),Wu=a("Construct a DistilBERT tokenizer."),Ru=l(),Xs=o("p"),ui=o("a"),Hu=a("DistilBertTokenizer"),Qu=a(" is identical to "),fi=o("a"),Uu=a("BertTokenizer"),Vu=a(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),Ju=l(),ln=o("p"),Ku=a("Refer to superclass "),mi=o("a"),Gu=a("BertTokenizer"),Xu=a(" for usage examples and documentation concerning parameters."),vp=l(),ds=o("h2"),Ys=o("a"),$l=o("span"),T(dn.$$.fragment),Yu=l(),Dl=o("span"),Zu=a("DistilBertTokenizerFast"),bp=l(),vt=o("div"),T(cn.$$.fragment),ef=l(),pn=o("p"),tf=a("Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fl=o("em"),sf=a("tokenizers"),of=a(" library)."),nf=l(),Zs=o("p"),gi=o("a"),rf=a("DistilBertTokenizerFast"),af=a(" is identical to "),_i=o("a"),lf=a("BertTokenizerFast"),df=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),cf=l(),hn=o("p"),pf=a("Refer to superclass "),vi=o("a"),hf=a("BertTokenizerFast"),uf=a(" for usage examples and documentation concerning parameters."),Tp=l(),cs=o("h2"),eo=o("a"),yl=o("span"),T(un.$$.fragment),ff=l(),Bl=o("span"),mf=a("DistilBertModel"),kp=l(),Oe=o("div"),T(fn.$$.fragment),gf=l(),El=o("p"),_f=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),vf=l(),mn=o("p"),bf=a("This model inherits from "),bi=o("a"),Tf=a("PreTrainedModel"),kf=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wf=l(),gn=o("p"),$f=a("This model is also a PyTorch "),_n=o("a"),Df=a("torch.nn.Module"),Ff=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),yf=l(),Ve=o("div"),T(vn.$$.fragment),Bf=l(),ps=o("p"),Ef=a("The "),Ti=o("a"),Mf=a("DistilBertModel"),zf=a(" forward method, overrides the "),Ml=o("code"),xf=a("__call__"),jf=a(" special method."),Cf=l(),T(to.$$.fragment),Pf=l(),zl=o("p"),qf=a("Example:"),Af=l(),T(bn.$$.fragment),wp=l(),hs=o("h2"),so=o("a"),xl=o("span"),T(Tn.$$.fragment),If=l(),jl=o("span"),Nf=a("DistilBertForMaskedLM"),$p=l(),We=o("div"),T(kn.$$.fragment),Lf=l(),wn=o("p"),Sf=a("DistilBert Model with a "),Cl=o("code"),Of=a("masked language modeling"),Wf=a(" head on top."),Rf=l(),$n=o("p"),Hf=a("This model inherits from "),ki=o("a"),Qf=a("PreTrainedModel"),Uf=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vf=l(),Dn=o("p"),Jf=a("This model is also a PyTorch "),Fn=o("a"),Kf=a("torch.nn.Module"),Gf=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xf=l(),Je=o("div"),T(yn.$$.fragment),Yf=l(),us=o("p"),Zf=a("The "),wi=o("a"),em=a("DistilBertForMaskedLM"),tm=a(" forward method, overrides the "),Pl=o("code"),sm=a("__call__"),om=a(" special method."),nm=l(),T(oo.$$.fragment),rm=l(),ql=o("p"),am=a("Example:"),im=l(),T(Bn.$$.fragment),Dp=l(),fs=o("h2"),no=o("a"),Al=o("span"),T(En.$$.fragment),lm=l(),Il=o("span"),dm=a("DistilBertForSequenceClassification"),Fp=l(),Re=o("div"),T(Mn.$$.fragment),cm=l(),Nl=o("p"),pm=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),hm=l(),zn=o("p"),um=a("This model inherits from "),$i=o("a"),fm=a("PreTrainedModel"),mm=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gm=l(),xn=o("p"),_m=a("This model is also a PyTorch "),jn=o("a"),vm=a("torch.nn.Module"),bm=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Tm=l(),Ce=o("div"),T(Cn.$$.fragment),km=l(),ms=o("p"),wm=a("The "),Di=o("a"),$m=a("DistilBertForSequenceClassification"),Dm=a(" forward method, overrides the "),Ll=o("code"),Fm=a("__call__"),ym=a(" special method."),Bm=l(),T(ro.$$.fragment),Em=l(),Sl=o("p"),Mm=a("Example of single-label classification:"),zm=l(),T(Pn.$$.fragment),xm=l(),Ol=o("p"),jm=a("Example of multi-label classification:"),Cm=l(),T(qn.$$.fragment),yp=l(),gs=o("h2"),ao=o("a"),Wl=o("span"),T(An.$$.fragment),Pm=l(),Rl=o("span"),qm=a("DistilBertForMultipleChoice"),Bp=l(),He=o("div"),T(In.$$.fragment),Am=l(),Hl=o("p"),Im=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Nm=l(),Nn=o("p"),Lm=a("This model inherits from "),Fi=o("a"),Sm=a("PreTrainedModel"),Om=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wm=l(),Ln=o("p"),Rm=a("This model is also a PyTorch "),Sn=o("a"),Hm=a("torch.nn.Module"),Qm=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Um=l(),Ke=o("div"),T(On.$$.fragment),Vm=l(),_s=o("p"),Jm=a("The "),yi=o("a"),Km=a("DistilBertForMultipleChoice"),Gm=a(" forward method, overrides the "),Ql=o("code"),Xm=a("__call__"),Ym=a(" special method."),Zm=l(),T(io.$$.fragment),eg=l(),Ul=o("p"),tg=a("Examples:"),sg=l(),T(Wn.$$.fragment),Ep=l(),vs=o("h2"),lo=o("a"),Vl=o("span"),T(Rn.$$.fragment),og=l(),Jl=o("span"),ng=a("DistilBertForTokenClassification"),Mp=l(),Qe=o("div"),T(Hn.$$.fragment),rg=l(),Kl=o("p"),ag=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),ig=l(),Qn=o("p"),lg=a("This model inherits from "),Bi=o("a"),dg=a("PreTrainedModel"),cg=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pg=l(),Un=o("p"),hg=a("This model is also a PyTorch "),Vn=o("a"),ug=a("torch.nn.Module"),fg=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),mg=l(),Ge=o("div"),T(Jn.$$.fragment),gg=l(),bs=o("p"),_g=a("The "),Ei=o("a"),vg=a("DistilBertForTokenClassification"),bg=a(" forward method, overrides the "),Gl=o("code"),Tg=a("__call__"),kg=a(" special method."),wg=l(),T(co.$$.fragment),$g=l(),Xl=o("p"),Dg=a("Example:"),Fg=l(),T(Kn.$$.fragment),zp=l(),Ts=o("h2"),po=o("a"),Yl=o("span"),T(Gn.$$.fragment),yg=l(),Zl=o("span"),Bg=a("DistilBertForQuestionAnswering"),xp=l(),Ue=o("div"),T(Xn.$$.fragment),Eg=l(),ks=o("p"),Mg=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),ed=o("code"),zg=a("span start logits"),xg=a(" and "),td=o("code"),jg=a("span end logits"),Cg=a(")."),Pg=l(),Yn=o("p"),qg=a("This model inherits from "),Mi=o("a"),Ag=a("PreTrainedModel"),Ig=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ng=l(),Zn=o("p"),Lg=a("This model is also a PyTorch "),er=o("a"),Sg=a("torch.nn.Module"),Og=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Wg=l(),Xe=o("div"),T(tr.$$.fragment),Rg=l(),ws=o("p"),Hg=a("The "),zi=o("a"),Qg=a("DistilBertForQuestionAnswering"),Ug=a(" forward method, overrides the "),sd=o("code"),Vg=a("__call__"),Jg=a(" special method."),Kg=l(),T(ho.$$.fragment),Gg=l(),od=o("p"),Xg=a("Example:"),Yg=l(),T(sr.$$.fragment),jp=l(),$s=o("h2"),uo=o("a"),nd=o("span"),T(or.$$.fragment),Zg=l(),rd=o("span"),e_=a("TFDistilBertModel"),Cp=l(),Pe=o("div"),T(nr.$$.fragment),t_=l(),ad=o("p"),s_=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),o_=l(),rr=o("p"),n_=a("This model inherits from "),xi=o("a"),r_=a("TFPreTrainedModel"),a_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),i_=l(),ar=o("p"),l_=a("This model is also a "),ir=o("a"),d_=a("tf.keras.Model"),c_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),p_=l(),T(fo.$$.fragment),h_=l(),Ye=o("div"),T(lr.$$.fragment),u_=l(),Ds=o("p"),f_=a("The "),ji=o("a"),m_=a("TFDistilBertModel"),g_=a(" forward method, overrides the "),id=o("code"),__=a("__call__"),v_=a(" special method."),b_=l(),T(mo.$$.fragment),T_=l(),ld=o("p"),k_=a("Example:"),w_=l(),T(dr.$$.fragment),Pp=l(),Fs=o("h2"),go=o("a"),dd=o("span"),T(cr.$$.fragment),$_=l(),cd=o("span"),D_=a("TFDistilBertForMaskedLM"),qp=l(),qe=o("div"),T(pr.$$.fragment),F_=l(),hr=o("p"),y_=a("DistilBert Model with a "),pd=o("code"),B_=a("masked language modeling"),E_=a(" head on top."),M_=l(),ur=o("p"),z_=a("This model inherits from "),Ci=o("a"),x_=a("TFPreTrainedModel"),j_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C_=l(),fr=o("p"),P_=a("This model is also a "),mr=o("a"),q_=a("tf.keras.Model"),A_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),I_=l(),T(_o.$$.fragment),N_=l(),Ze=o("div"),T(gr.$$.fragment),L_=l(),ys=o("p"),S_=a("The "),Pi=o("a"),O_=a("TFDistilBertForMaskedLM"),W_=a(" forward method, overrides the "),hd=o("code"),R_=a("__call__"),H_=a(" special method."),Q_=l(),T(vo.$$.fragment),U_=l(),ud=o("p"),V_=a("Example:"),J_=l(),T(_r.$$.fragment),Ap=l(),Bs=o("h2"),bo=o("a"),fd=o("span"),T(vr.$$.fragment),K_=l(),md=o("span"),G_=a("TFDistilBertForSequenceClassification"),Ip=l(),Ae=o("div"),T(br.$$.fragment),X_=l(),gd=o("p"),Y_=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Z_=l(),Tr=o("p"),ev=a("This model inherits from "),qi=o("a"),tv=a("TFPreTrainedModel"),sv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ov=l(),kr=o("p"),nv=a("This model is also a "),wr=o("a"),rv=a("tf.keras.Model"),av=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),iv=l(),T(To.$$.fragment),lv=l(),et=o("div"),T($r.$$.fragment),dv=l(),Es=o("p"),cv=a("The "),Ai=o("a"),pv=a("TFDistilBertForSequenceClassification"),hv=a(" forward method, overrides the "),_d=o("code"),uv=a("__call__"),fv=a(" special method."),mv=l(),T(ko.$$.fragment),gv=l(),vd=o("p"),_v=a("Example:"),vv=l(),T(Dr.$$.fragment),Np=l(),Ms=o("h2"),wo=o("a"),bd=o("span"),T(Fr.$$.fragment),bv=l(),Td=o("span"),Tv=a("TFDistilBertForMultipleChoice"),Lp=l(),Ie=o("div"),T(yr.$$.fragment),kv=l(),kd=o("p"),wv=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),$v=l(),Br=o("p"),Dv=a("This model inherits from "),Ii=o("a"),Fv=a("TFPreTrainedModel"),yv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bv=l(),Er=o("p"),Ev=a("This model is also a "),Mr=o("a"),Mv=a("tf.keras.Model"),zv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),xv=l(),T($o.$$.fragment),jv=l(),tt=o("div"),T(zr.$$.fragment),Cv=l(),zs=o("p"),Pv=a("The "),Ni=o("a"),qv=a("TFDistilBertForMultipleChoice"),Av=a(" forward method, overrides the "),wd=o("code"),Iv=a("__call__"),Nv=a(" special method."),Lv=l(),T(Do.$$.fragment),Sv=l(),$d=o("p"),Ov=a("Example:"),Wv=l(),T(xr.$$.fragment),Sp=l(),xs=o("h2"),Fo=o("a"),Dd=o("span"),T(jr.$$.fragment),Rv=l(),Fd=o("span"),Hv=a("TFDistilBertForTokenClassification"),Op=l(),Ne=o("div"),T(Cr.$$.fragment),Qv=l(),yd=o("p"),Uv=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Vv=l(),Pr=o("p"),Jv=a("This model inherits from "),Li=o("a"),Kv=a("TFPreTrainedModel"),Gv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xv=l(),qr=o("p"),Yv=a("This model is also a "),Ar=o("a"),Zv=a("tf.keras.Model"),eb=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),tb=l(),T(yo.$$.fragment),sb=l(),st=o("div"),T(Ir.$$.fragment),ob=l(),js=o("p"),nb=a("The "),Si=o("a"),rb=a("TFDistilBertForTokenClassification"),ab=a(" forward method, overrides the "),Bd=o("code"),ib=a("__call__"),lb=a(" special method."),db=l(),T(Bo.$$.fragment),cb=l(),Ed=o("p"),pb=a("Example:"),hb=l(),T(Nr.$$.fragment),Wp=l(),Cs=o("h2"),Eo=o("a"),Md=o("span"),T(Lr.$$.fragment),ub=l(),zd=o("span"),fb=a("TFDistilBertForQuestionAnswering"),Rp=l(),Le=o("div"),T(Sr.$$.fragment),mb=l(),Ps=o("p"),gb=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),xd=o("code"),_b=a("span start logits"),vb=a(" and "),jd=o("code"),bb=a("span end logits"),Tb=a(")."),kb=l(),Or=o("p"),wb=a("This model inherits from "),Oi=o("a"),$b=a("TFPreTrainedModel"),Db=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Fb=l(),Wr=o("p"),yb=a("This model is also a "),Rr=o("a"),Bb=a("tf.keras.Model"),Eb=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Mb=l(),T(Mo.$$.fragment),zb=l(),ot=o("div"),T(Hr.$$.fragment),xb=l(),qs=o("p"),jb=a("The "),Wi=o("a"),Cb=a("TFDistilBertForQuestionAnswering"),Pb=a(" forward method, overrides the "),Cd=o("code"),qb=a("__call__"),Ab=a(" special method."),Ib=l(),T(zo.$$.fragment),Nb=l(),Pd=o("p"),Lb=a("Example:"),Sb=l(),T(Qr.$$.fragment),Hp=l(),As=o("h2"),xo=o("a"),qd=o("span"),T(Ur.$$.fragment),Ob=l(),Ad=o("span"),Wb=a("FlaxDistilBertModel"),Qp=l(),Be=o("div"),T(Vr.$$.fragment),Rb=l(),Id=o("p"),Hb=a("The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),Qb=l(),Jr=o("p"),Ub=a("This model inherits from "),Ri=o("a"),Vb=a("FlaxPreTrainedModel"),Jb=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Kb=l(),Kr=o("p"),Gb=a("This model is also a Flax Linen "),Gr=o("a"),Xb=a("flax.linen.Module"),Yb=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Zb=l(),Nd=o("p"),e1=a("Finally, this model supports inherent JAX features such as:"),t1=l(),Bt=o("ul"),Ld=o("li"),Xr=o("a"),s1=a("Just-In-Time (JIT) compilation"),o1=l(),Sd=o("li"),Yr=o("a"),n1=a("Automatic Differentiation"),r1=l(),Od=o("li"),Zr=o("a"),a1=a("Vectorization"),i1=l(),Wd=o("li"),ea=o("a"),l1=a("Parallelization"),d1=l(),nt=o("div"),T(ta.$$.fragment),c1=l(),Is=o("p"),p1=a("The "),Rd=o("code"),h1=a("FlaxDistilBertPreTrainedModel"),u1=a("forward method, overrides the "),Hd=o("code"),f1=a("__call__"),m1=a(" special method."),g1=l(),T(jo.$$.fragment),_1=l(),Qd=o("p"),v1=a("Example:"),b1=l(),T(sa.$$.fragment),Up=l(),Ns=o("h2"),Co=o("a"),Ud=o("span"),T(oa.$$.fragment),T1=l(),Vd=o("span"),k1=a("FlaxDistilBertForMaskedLM"),Vp=l(),Ee=o("div"),T(na.$$.fragment),w1=l(),ra=o("p"),$1=a("DistilBert Model with a "),Jd=o("code"),D1=a("language modeling"),F1=a(" head on top."),y1=l(),aa=o("p"),B1=a("This model inherits from "),Hi=o("a"),E1=a("FlaxPreTrainedModel"),M1=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),z1=l(),ia=o("p"),x1=a("This model is also a Flax Linen "),la=o("a"),j1=a("flax.linen.Module"),C1=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),P1=l(),Kd=o("p"),q1=a("Finally, this model supports inherent JAX features such as:"),A1=l(),Et=o("ul"),Gd=o("li"),da=o("a"),I1=a("Just-In-Time (JIT) compilation"),N1=l(),Xd=o("li"),ca=o("a"),L1=a("Automatic Differentiation"),S1=l(),Yd=o("li"),pa=o("a"),O1=a("Vectorization"),W1=l(),Zd=o("li"),ha=o("a"),R1=a("Parallelization"),H1=l(),rt=o("div"),T(ua.$$.fragment),Q1=l(),Ls=o("p"),U1=a("The "),ec=o("code"),V1=a("FlaxDistilBertPreTrainedModel"),J1=a("forward method, overrides the "),tc=o("code"),K1=a("__call__"),G1=a(" special method."),X1=l(),T(Po.$$.fragment),Y1=l(),sc=o("p"),Z1=a("Example:"),eT=l(),T(fa.$$.fragment),Jp=l(),Ss=o("h2"),qo=o("a"),oc=o("span"),T(ma.$$.fragment),tT=l(),nc=o("span"),sT=a("FlaxDistilBertForSequenceClassification"),Kp=l(),Me=o("div"),T(ga.$$.fragment),oT=l(),rc=o("p"),nT=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),rT=l(),_a=o("p"),aT=a("This model inherits from "),Qi=o("a"),iT=a("FlaxPreTrainedModel"),lT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),dT=l(),va=o("p"),cT=a("This model is also a Flax Linen "),ba=o("a"),pT=a("flax.linen.Module"),hT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),uT=l(),ac=o("p"),fT=a("Finally, this model supports inherent JAX features such as:"),mT=l(),Mt=o("ul"),ic=o("li"),Ta=o("a"),gT=a("Just-In-Time (JIT) compilation"),_T=l(),lc=o("li"),ka=o("a"),vT=a("Automatic Differentiation"),bT=l(),dc=o("li"),wa=o("a"),TT=a("Vectorization"),kT=l(),cc=o("li"),$a=o("a"),wT=a("Parallelization"),$T=l(),at=o("div"),T(Da.$$.fragment),DT=l(),Os=o("p"),FT=a("The "),pc=o("code"),yT=a("FlaxDistilBertPreTrainedModel"),BT=a("forward method, overrides the "),hc=o("code"),ET=a("__call__"),MT=a(" special method."),zT=l(),T(Ao.$$.fragment),xT=l(),uc=o("p"),jT=a("Example:"),CT=l(),T(Fa.$$.fragment),Gp=l(),Ws=o("h2"),Io=o("a"),fc=o("span"),T(ya.$$.fragment),PT=l(),mc=o("span"),qT=a("FlaxDistilBertForMultipleChoice"),Xp=l(),ze=o("div"),T(Ba.$$.fragment),AT=l(),gc=o("p"),IT=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),NT=l(),Ea=o("p"),LT=a("This model inherits from "),Ui=o("a"),ST=a("FlaxPreTrainedModel"),OT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),WT=l(),Ma=o("p"),RT=a("This model is also a Flax Linen "),za=o("a"),HT=a("flax.linen.Module"),QT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),UT=l(),_c=o("p"),VT=a("Finally, this model supports inherent JAX features such as:"),JT=l(),zt=o("ul"),vc=o("li"),xa=o("a"),KT=a("Just-In-Time (JIT) compilation"),GT=l(),bc=o("li"),ja=o("a"),XT=a("Automatic Differentiation"),YT=l(),Tc=o("li"),Ca=o("a"),ZT=a("Vectorization"),ek=l(),kc=o("li"),Pa=o("a"),tk=a("Parallelization"),sk=l(),it=o("div"),T(qa.$$.fragment),ok=l(),Rs=o("p"),nk=a("The "),wc=o("code"),rk=a("FlaxDistilBertPreTrainedModel"),ak=a("forward method, overrides the "),$c=o("code"),ik=a("__call__"),lk=a(" special method."),dk=l(),T(No.$$.fragment),ck=l(),Dc=o("p"),pk=a("Example:"),hk=l(),T(Aa.$$.fragment),Yp=l(),Hs=o("h2"),Lo=o("a"),Fc=o("span"),T(Ia.$$.fragment),uk=l(),yc=o("span"),fk=a("FlaxDistilBertForTokenClassification"),Zp=l(),xe=o("div"),T(Na.$$.fragment),mk=l(),Bc=o("p"),gk=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),_k=l(),La=o("p"),vk=a("This model inherits from "),Vi=o("a"),bk=a("FlaxPreTrainedModel"),Tk=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),kk=l(),Sa=o("p"),wk=a("This model is also a Flax Linen "),Oa=o("a"),$k=a("flax.linen.Module"),Dk=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Fk=l(),Ec=o("p"),yk=a("Finally, this model supports inherent JAX features such as:"),Bk=l(),xt=o("ul"),Mc=o("li"),Wa=o("a"),Ek=a("Just-In-Time (JIT) compilation"),Mk=l(),zc=o("li"),Ra=o("a"),zk=a("Automatic Differentiation"),xk=l(),xc=o("li"),Ha=o("a"),jk=a("Vectorization"),Ck=l(),jc=o("li"),Qa=o("a"),Pk=a("Parallelization"),qk=l(),lt=o("div"),T(Ua.$$.fragment),Ak=l(),Qs=o("p"),Ik=a("The "),Cc=o("code"),Nk=a("FlaxDistilBertPreTrainedModel"),Lk=a("forward method, overrides the "),Pc=o("code"),Sk=a("__call__"),Ok=a(" special method."),Wk=l(),T(So.$$.fragment),Rk=l(),qc=o("p"),Hk=a("Example:"),Qk=l(),T(Va.$$.fragment),eh=l(),Us=o("h2"),Oo=o("a"),Ac=o("span"),T(Ja.$$.fragment),Uk=l(),Ic=o("span"),Vk=a("FlaxDistilBertForQuestionAnswering"),th=l(),je=o("div"),T(Ka.$$.fragment),Jk=l(),Vs=o("p"),Kk=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Nc=o("code"),Gk=a("span start logits"),Xk=a(" and "),Lc=o("code"),Yk=a("span end logits"),Zk=a(")."),ew=l(),Ga=o("p"),tw=a("This model inherits from "),Ji=o("a"),sw=a("FlaxPreTrainedModel"),ow=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),nw=l(),Xa=o("p"),rw=a("This model is also a Flax Linen "),Ya=o("a"),aw=a("flax.linen.Module"),iw=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),lw=l(),Sc=o("p"),dw=a("Finally, this model supports inherent JAX features such as:"),cw=l(),jt=o("ul"),Oc=o("li"),Za=o("a"),pw=a("Just-In-Time (JIT) compilation"),hw=l(),Wc=o("li"),ei=o("a"),uw=a("Automatic Differentiation"),fw=l(),Rc=o("li"),ti=o("a"),mw=a("Vectorization"),gw=l(),Hc=o("li"),si=o("a"),_w=a("Parallelization"),vw=l(),dt=o("div"),T(oi.$$.fragment),bw=l(),Js=o("p"),Tw=a("The "),Qc=o("code"),kw=a("FlaxDistilBertPreTrainedModel"),ww=a("forward method, overrides the "),Uc=o("code"),$w=a("__call__"),Dw=a(" special method."),Fw=l(),T(Wo.$$.fragment),yw=l(),Vc=o("p"),Bw=a("Example:"),Ew=l(),T(ni.$$.fragment),this.h()},l(s){const m=ly('[data-svelte="svelte-1phssyn"]',document.head);p=n(m,"META",{name:!0,content:!0}),m.forEach(t),y=d(s),g=n(s,"H1",{class:!0});var ri=r(g);v=n(ri,"A",{id:!0,class:!0,href:!0});var Jc=r(v);b=n(Jc,"SPAN",{});var Kc=r(b);k(_.$$.fragment,Kc),Kc.forEach(t),Jc.forEach(t),f=d(ri),B=n(ri,"SPAN",{});var Gc=r(B);de=i(Gc,"DistilBERT"),Gc.forEach(t),ri.forEach(t),V=d(s),M=n(s,"H2",{class:!0});var ai=r(M);G=n(ai,"A",{id:!0,class:!0,href:!0});var Xc=r(G);S=n(Xc,"SPAN",{});var Yc=r(S);k(X.$$.fragment,Yc),Yc.forEach(t),Xc.forEach(t),ce=d(ai),O=n(ai,"SPAN",{});var Zc=r(O);pe=i(Zc,"Overview"),Zc.forEach(t),ai.forEach(t),re=d(s),N=n(s,"P",{});var Ct=r(N);q=i(Ct,"The DistilBERT model was proposed in the blog post "),Y=n(Ct,"A",{href:!0,rel:!0});var ep=r(Y);J=i(ep,`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),ep.forEach(t),z=i(Ct,", and the paper "),x=n(Ct,"A",{href:!0,rel:!0});var tp=r(x);he=i(tp,`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),tp.forEach(t),W=i(Ct,`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),se=n(Ct,"EM",{});var sp=r(se);ue=i(sp,"bert-base-uncased"),sp.forEach(t),R=i(Ct,`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),Ct.forEach(t),ae=d(s),ee=n(s,"P",{});var op=r(ee);A=i(op,"The abstract from the paper is the following:"),op.forEach(t),ie=d(s),L=n(s,"P",{});var np=r(L);oe=n(np,"EM",{});var rp=r(oe);fe=i(rp,`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),rp.forEach(t),np.forEach(t),P=d(s),te=n(s,"P",{});var ap=r(te);H=i(ap,"Tips:"),ap.forEach(t),le=d(s),h=n(s,"UL",{});var ii=r(h);E=n(ii,"LI",{});var Pt=r(E);K=i(Pt,"DistilBERT doesn\u2019t have "),ge=n(Pt,"CODE",{});var ip=r(ge);be=i(ip,"token_type_ids"),ip.forEach(t),I=i(Pt,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),_e=n(Pt,"CODE",{});var lp=r(_e);Te=i(lp,"tokenizer.sep_token"),lp.forEach(t),ke=i(Pt," (or "),C=n(Pt,"CODE",{});var dp=r(C);Q=i(dp,"[SEP]"),dp.forEach(t),we=i(Pt,")."),Pt.forEach(t),$e=d(ii),Z=n(ii,"LI",{});var li=r(Z);De=i(li,"DistilBERT doesn\u2019t have options to select the input positions ("),ne=n(li,"CODE",{});var cp=r(ne);Fe=i(cp,"position_ids"),cp.forEach(t),hu=i(li,` input). This could be added if
necessary though, just let us know if you need this option.`),li.forEach(t),ii.forEach(t),up=d(s),bt=n(s,"P",{});var qt=r(bt);uu=i(qt,"This model was contributed by "),Yo=n(qt,"A",{href:!0,rel:!0});var Mw=r(Yo);fu=i(Mw,"victorsanh"),Mw.forEach(t),mu=i(qt,`. This model jax version was
contributed by `),Zo=n(qt,"A",{href:!0,rel:!0});var zw=r(Zo);gu=i(zw,"kamalkraj"),zw.forEach(t),_u=i(qt,". The original code can be found "),en=n(qt,"A",{href:!0,rel:!0});var xw=r(en);vu=i(xw,"here"),xw.forEach(t),bu=i(qt,"."),qt.forEach(t),fp=d(s),as=n(s,"H2",{class:!0});var oh=r(as);Ks=n(oh,"A",{id:!0,class:!0,href:!0});var jw=r(Ks);_l=n(jw,"SPAN",{});var Cw=r(_l);k(tn.$$.fragment,Cw),Cw.forEach(t),jw.forEach(t),Tu=d(oh),vl=n(oh,"SPAN",{});var Pw=r(vl);ku=i(Pw,"DistilBertConfig"),Pw.forEach(t),oh.forEach(t),mp=d(s),Se=n(s,"DIV",{class:!0});var At=r(Se);k(sn.$$.fragment,At),wu=d(At),yt=n(At,"P",{});var Ro=r(yt);$u=i(Ro,"This is the configuration class to store the configuration of a "),di=n(Ro,"A",{href:!0});var qw=r(di);Du=i(qw,"DistilBertModel"),qw.forEach(t),Fu=i(Ro," or a "),ci=n(Ro,"A",{href:!0});var Aw=r(ci);yu=i(Aw,"TFDistilBertModel"),Aw.forEach(t),Bu=i(Ro,`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),on=n(Ro,"A",{href:!0,rel:!0});var Iw=r(on);Eu=i(Iw,"distilbert-base-uncased"),Iw.forEach(t),Mu=i(Ro," architecture."),Ro.forEach(t),zu=d(At),is=n(At,"P",{});var Ki=r(is);xu=i(Ki,"Configuration objects inherit from "),pi=n(Ki,"A",{href:!0});var Nw=r(pi);ju=i(Nw,"PretrainedConfig"),Nw.forEach(t),Cu=i(Ki,` and can be used to control the model outputs. Read the
documentation from `),hi=n(Ki,"A",{href:!0});var Lw=r(hi);Pu=i(Lw,"PretrainedConfig"),Lw.forEach(t),qu=i(Ki," for more information."),Ki.forEach(t),Au=d(At),bl=n(At,"P",{});var Sw=r(bl);Iu=i(Sw,"Examples:"),Sw.forEach(t),Nu=d(At),k(nn.$$.fragment,At),At.forEach(t),gp=d(s),ls=n(s,"H2",{class:!0});var nh=r(ls);Gs=n(nh,"A",{id:!0,class:!0,href:!0});var Ow=r(Gs);Tl=n(Ow,"SPAN",{});var Ww=r(Tl);k(rn.$$.fragment,Ww),Ww.forEach(t),Ow.forEach(t),Lu=d(nh),kl=n(nh,"SPAN",{});var Rw=r(kl);Su=i(Rw,"DistilBertTokenizer"),Rw.forEach(t),nh.forEach(t),_p=d(s),_t=n(s,"DIV",{class:!0});var Ho=r(_t);k(an.$$.fragment,Ho),Ou=d(Ho),wl=n(Ho,"P",{});var Hw=r(wl);Wu=i(Hw,"Construct a DistilBERT tokenizer."),Hw.forEach(t),Ru=d(Ho),Xs=n(Ho,"P",{});var pp=r(Xs);ui=n(pp,"A",{href:!0});var Qw=r(ui);Hu=i(Qw,"DistilBertTokenizer"),Qw.forEach(t),Qu=i(pp," is identical to "),fi=n(pp,"A",{href:!0});var Uw=r(fi);Uu=i(Uw,"BertTokenizer"),Uw.forEach(t),Vu=i(pp,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),pp.forEach(t),Ju=d(Ho),ln=n(Ho,"P",{});var rh=r(ln);Ku=i(rh,"Refer to superclass "),mi=n(rh,"A",{href:!0});var Vw=r(mi);Gu=i(Vw,"BertTokenizer"),Vw.forEach(t),Xu=i(rh," for usage examples and documentation concerning parameters."),rh.forEach(t),Ho.forEach(t),vp=d(s),ds=n(s,"H2",{class:!0});var ah=r(ds);Ys=n(ah,"A",{id:!0,class:!0,href:!0});var Jw=r(Ys);$l=n(Jw,"SPAN",{});var Kw=r($l);k(dn.$$.fragment,Kw),Kw.forEach(t),Jw.forEach(t),Yu=d(ah),Dl=n(ah,"SPAN",{});var Gw=r(Dl);Zu=i(Gw,"DistilBertTokenizerFast"),Gw.forEach(t),ah.forEach(t),bp=d(s),vt=n(s,"DIV",{class:!0});var Qo=r(vt);k(cn.$$.fragment,Qo),ef=d(Qo),pn=n(Qo,"P",{});var ih=r(pn);tf=i(ih,"Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fl=n(ih,"EM",{});var Xw=r(Fl);sf=i(Xw,"tokenizers"),Xw.forEach(t),of=i(ih," library)."),ih.forEach(t),nf=d(Qo),Zs=n(Qo,"P",{});var hp=r(Zs);gi=n(hp,"A",{href:!0});var Yw=r(gi);rf=i(Yw,"DistilBertTokenizerFast"),Yw.forEach(t),af=i(hp," is identical to "),_i=n(hp,"A",{href:!0});var Zw=r(_i);lf=i(Zw,"BertTokenizerFast"),Zw.forEach(t),df=i(hp,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),hp.forEach(t),cf=d(Qo),hn=n(Qo,"P",{});var lh=r(hn);pf=i(lh,"Refer to superclass "),vi=n(lh,"A",{href:!0});var e$=r(vi);hf=i(e$,"BertTokenizerFast"),e$.forEach(t),uf=i(lh," for usage examples and documentation concerning parameters."),lh.forEach(t),Qo.forEach(t),Tp=d(s),cs=n(s,"H2",{class:!0});var dh=r(cs);eo=n(dh,"A",{id:!0,class:!0,href:!0});var t$=r(eo);yl=n(t$,"SPAN",{});var s$=r(yl);k(un.$$.fragment,s$),s$.forEach(t),t$.forEach(t),ff=d(dh),Bl=n(dh,"SPAN",{});var o$=r(Bl);mf=i(o$,"DistilBertModel"),o$.forEach(t),dh.forEach(t),kp=d(s),Oe=n(s,"DIV",{class:!0});var It=r(Oe);k(fn.$$.fragment,It),gf=d(It),El=n(It,"P",{});var n$=r(El);_f=i(n$,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),n$.forEach(t),vf=d(It),mn=n(It,"P",{});var ch=r(mn);bf=i(ch,"This model inherits from "),bi=n(ch,"A",{href:!0});var r$=r(bi);Tf=i(r$,"PreTrainedModel"),r$.forEach(t),kf=i(ch,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ch.forEach(t),wf=d(It),gn=n(It,"P",{});var ph=r(gn);$f=i(ph,"This model is also a PyTorch "),_n=n(ph,"A",{href:!0,rel:!0});var a$=r(_n);Df=i(a$,"torch.nn.Module"),a$.forEach(t),Ff=i(ph,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ph.forEach(t),yf=d(It),Ve=n(It,"DIV",{class:!0});var Nt=r(Ve);k(vn.$$.fragment,Nt),Bf=d(Nt),ps=n(Nt,"P",{});var Gi=r(ps);Ef=i(Gi,"The "),Ti=n(Gi,"A",{href:!0});var i$=r(Ti);Mf=i(i$,"DistilBertModel"),i$.forEach(t),zf=i(Gi," forward method, overrides the "),Ml=n(Gi,"CODE",{});var l$=r(Ml);xf=i(l$,"__call__"),l$.forEach(t),jf=i(Gi," special method."),Gi.forEach(t),Cf=d(Nt),k(to.$$.fragment,Nt),Pf=d(Nt),zl=n(Nt,"P",{});var d$=r(zl);qf=i(d$,"Example:"),d$.forEach(t),Af=d(Nt),k(bn.$$.fragment,Nt),Nt.forEach(t),It.forEach(t),wp=d(s),hs=n(s,"H2",{class:!0});var hh=r(hs);so=n(hh,"A",{id:!0,class:!0,href:!0});var c$=r(so);xl=n(c$,"SPAN",{});var p$=r(xl);k(Tn.$$.fragment,p$),p$.forEach(t),c$.forEach(t),If=d(hh),jl=n(hh,"SPAN",{});var h$=r(jl);Nf=i(h$,"DistilBertForMaskedLM"),h$.forEach(t),hh.forEach(t),$p=d(s),We=n(s,"DIV",{class:!0});var Lt=r(We);k(kn.$$.fragment,Lt),Lf=d(Lt),wn=n(Lt,"P",{});var uh=r(wn);Sf=i(uh,"DistilBert Model with a "),Cl=n(uh,"CODE",{});var u$=r(Cl);Of=i(u$,"masked language modeling"),u$.forEach(t),Wf=i(uh," head on top."),uh.forEach(t),Rf=d(Lt),$n=n(Lt,"P",{});var fh=r($n);Hf=i(fh,"This model inherits from "),ki=n(fh,"A",{href:!0});var f$=r(ki);Qf=i(f$,"PreTrainedModel"),f$.forEach(t),Uf=i(fh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),fh.forEach(t),Vf=d(Lt),Dn=n(Lt,"P",{});var mh=r(Dn);Jf=i(mh,"This model is also a PyTorch "),Fn=n(mh,"A",{href:!0,rel:!0});var m$=r(Fn);Kf=i(m$,"torch.nn.Module"),m$.forEach(t),Gf=i(mh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),mh.forEach(t),Xf=d(Lt),Je=n(Lt,"DIV",{class:!0});var St=r(Je);k(yn.$$.fragment,St),Yf=d(St),us=n(St,"P",{});var Xi=r(us);Zf=i(Xi,"The "),wi=n(Xi,"A",{href:!0});var g$=r(wi);em=i(g$,"DistilBertForMaskedLM"),g$.forEach(t),tm=i(Xi," forward method, overrides the "),Pl=n(Xi,"CODE",{});var _$=r(Pl);sm=i(_$,"__call__"),_$.forEach(t),om=i(Xi," special method."),Xi.forEach(t),nm=d(St),k(oo.$$.fragment,St),rm=d(St),ql=n(St,"P",{});var v$=r(ql);am=i(v$,"Example:"),v$.forEach(t),im=d(St),k(Bn.$$.fragment,St),St.forEach(t),Lt.forEach(t),Dp=d(s),fs=n(s,"H2",{class:!0});var gh=r(fs);no=n(gh,"A",{id:!0,class:!0,href:!0});var b$=r(no);Al=n(b$,"SPAN",{});var T$=r(Al);k(En.$$.fragment,T$),T$.forEach(t),b$.forEach(t),lm=d(gh),Il=n(gh,"SPAN",{});var k$=r(Il);dm=i(k$,"DistilBertForSequenceClassification"),k$.forEach(t),gh.forEach(t),Fp=d(s),Re=n(s,"DIV",{class:!0});var Ot=r(Re);k(Mn.$$.fragment,Ot),cm=d(Ot),Nl=n(Ot,"P",{});var w$=r(Nl);pm=i(w$,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),w$.forEach(t),hm=d(Ot),zn=n(Ot,"P",{});var _h=r(zn);um=i(_h,"This model inherits from "),$i=n(_h,"A",{href:!0});var $$=r($i);fm=i($$,"PreTrainedModel"),$$.forEach(t),mm=i(_h,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_h.forEach(t),gm=d(Ot),xn=n(Ot,"P",{});var vh=r(xn);_m=i(vh,"This model is also a PyTorch "),jn=n(vh,"A",{href:!0,rel:!0});var D$=r(jn);vm=i(D$,"torch.nn.Module"),D$.forEach(t),bm=i(vh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),vh.forEach(t),Tm=d(Ot),Ce=n(Ot,"DIV",{class:!0});var ct=r(Ce);k(Cn.$$.fragment,ct),km=d(ct),ms=n(ct,"P",{});var Yi=r(ms);wm=i(Yi,"The "),Di=n(Yi,"A",{href:!0});var F$=r(Di);$m=i(F$,"DistilBertForSequenceClassification"),F$.forEach(t),Dm=i(Yi," forward method, overrides the "),Ll=n(Yi,"CODE",{});var y$=r(Ll);Fm=i(y$,"__call__"),y$.forEach(t),ym=i(Yi," special method."),Yi.forEach(t),Bm=d(ct),k(ro.$$.fragment,ct),Em=d(ct),Sl=n(ct,"P",{});var B$=r(Sl);Mm=i(B$,"Example of single-label classification:"),B$.forEach(t),zm=d(ct),k(Pn.$$.fragment,ct),xm=d(ct),Ol=n(ct,"P",{});var E$=r(Ol);jm=i(E$,"Example of multi-label classification:"),E$.forEach(t),Cm=d(ct),k(qn.$$.fragment,ct),ct.forEach(t),Ot.forEach(t),yp=d(s),gs=n(s,"H2",{class:!0});var bh=r(gs);ao=n(bh,"A",{id:!0,class:!0,href:!0});var M$=r(ao);Wl=n(M$,"SPAN",{});var z$=r(Wl);k(An.$$.fragment,z$),z$.forEach(t),M$.forEach(t),Pm=d(bh),Rl=n(bh,"SPAN",{});var x$=r(Rl);qm=i(x$,"DistilBertForMultipleChoice"),x$.forEach(t),bh.forEach(t),Bp=d(s),He=n(s,"DIV",{class:!0});var Wt=r(He);k(In.$$.fragment,Wt),Am=d(Wt),Hl=n(Wt,"P",{});var j$=r(Hl);Im=i(j$,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),j$.forEach(t),Nm=d(Wt),Nn=n(Wt,"P",{});var Th=r(Nn);Lm=i(Th,"This model inherits from "),Fi=n(Th,"A",{href:!0});var C$=r(Fi);Sm=i(C$,"PreTrainedModel"),C$.forEach(t),Om=i(Th,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Th.forEach(t),Wm=d(Wt),Ln=n(Wt,"P",{});var kh=r(Ln);Rm=i(kh,"This model is also a PyTorch "),Sn=n(kh,"A",{href:!0,rel:!0});var P$=r(Sn);Hm=i(P$,"torch.nn.Module"),P$.forEach(t),Qm=i(kh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kh.forEach(t),Um=d(Wt),Ke=n(Wt,"DIV",{class:!0});var Rt=r(Ke);k(On.$$.fragment,Rt),Vm=d(Rt),_s=n(Rt,"P",{});var Zi=r(_s);Jm=i(Zi,"The "),yi=n(Zi,"A",{href:!0});var q$=r(yi);Km=i(q$,"DistilBertForMultipleChoice"),q$.forEach(t),Gm=i(Zi," forward method, overrides the "),Ql=n(Zi,"CODE",{});var A$=r(Ql);Xm=i(A$,"__call__"),A$.forEach(t),Ym=i(Zi," special method."),Zi.forEach(t),Zm=d(Rt),k(io.$$.fragment,Rt),eg=d(Rt),Ul=n(Rt,"P",{});var I$=r(Ul);tg=i(I$,"Examples:"),I$.forEach(t),sg=d(Rt),k(Wn.$$.fragment,Rt),Rt.forEach(t),Wt.forEach(t),Ep=d(s),vs=n(s,"H2",{class:!0});var wh=r(vs);lo=n(wh,"A",{id:!0,class:!0,href:!0});var N$=r(lo);Vl=n(N$,"SPAN",{});var L$=r(Vl);k(Rn.$$.fragment,L$),L$.forEach(t),N$.forEach(t),og=d(wh),Jl=n(wh,"SPAN",{});var S$=r(Jl);ng=i(S$,"DistilBertForTokenClassification"),S$.forEach(t),wh.forEach(t),Mp=d(s),Qe=n(s,"DIV",{class:!0});var Ht=r(Qe);k(Hn.$$.fragment,Ht),rg=d(Ht),Kl=n(Ht,"P",{});var O$=r(Kl);ag=i(O$,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),O$.forEach(t),ig=d(Ht),Qn=n(Ht,"P",{});var $h=r(Qn);lg=i($h,"This model inherits from "),Bi=n($h,"A",{href:!0});var W$=r(Bi);dg=i(W$,"PreTrainedModel"),W$.forEach(t),cg=i($h,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$h.forEach(t),pg=d(Ht),Un=n(Ht,"P",{});var Dh=r(Un);hg=i(Dh,"This model is also a PyTorch "),Vn=n(Dh,"A",{href:!0,rel:!0});var R$=r(Vn);ug=i(R$,"torch.nn.Module"),R$.forEach(t),fg=i(Dh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Dh.forEach(t),mg=d(Ht),Ge=n(Ht,"DIV",{class:!0});var Qt=r(Ge);k(Jn.$$.fragment,Qt),gg=d(Qt),bs=n(Qt,"P",{});var el=r(bs);_g=i(el,"The "),Ei=n(el,"A",{href:!0});var H$=r(Ei);vg=i(H$,"DistilBertForTokenClassification"),H$.forEach(t),bg=i(el," forward method, overrides the "),Gl=n(el,"CODE",{});var Q$=r(Gl);Tg=i(Q$,"__call__"),Q$.forEach(t),kg=i(el," special method."),el.forEach(t),wg=d(Qt),k(co.$$.fragment,Qt),$g=d(Qt),Xl=n(Qt,"P",{});var U$=r(Xl);Dg=i(U$,"Example:"),U$.forEach(t),Fg=d(Qt),k(Kn.$$.fragment,Qt),Qt.forEach(t),Ht.forEach(t),zp=d(s),Ts=n(s,"H2",{class:!0});var Fh=r(Ts);po=n(Fh,"A",{id:!0,class:!0,href:!0});var V$=r(po);Yl=n(V$,"SPAN",{});var J$=r(Yl);k(Gn.$$.fragment,J$),J$.forEach(t),V$.forEach(t),yg=d(Fh),Zl=n(Fh,"SPAN",{});var K$=r(Zl);Bg=i(K$,"DistilBertForQuestionAnswering"),K$.forEach(t),Fh.forEach(t),xp=d(s),Ue=n(s,"DIV",{class:!0});var Ut=r(Ue);k(Xn.$$.fragment,Ut),Eg=d(Ut),ks=n(Ut,"P",{});var tl=r(ks);Mg=i(tl,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),ed=n(tl,"CODE",{});var G$=r(ed);zg=i(G$,"span start logits"),G$.forEach(t),xg=i(tl," and "),td=n(tl,"CODE",{});var X$=r(td);jg=i(X$,"span end logits"),X$.forEach(t),Cg=i(tl,")."),tl.forEach(t),Pg=d(Ut),Yn=n(Ut,"P",{});var yh=r(Yn);qg=i(yh,"This model inherits from "),Mi=n(yh,"A",{href:!0});var Y$=r(Mi);Ag=i(Y$,"PreTrainedModel"),Y$.forEach(t),Ig=i(yh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yh.forEach(t),Ng=d(Ut),Zn=n(Ut,"P",{});var Bh=r(Zn);Lg=i(Bh,"This model is also a PyTorch "),er=n(Bh,"A",{href:!0,rel:!0});var Z$=r(er);Sg=i(Z$,"torch.nn.Module"),Z$.forEach(t),Og=i(Bh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bh.forEach(t),Wg=d(Ut),Xe=n(Ut,"DIV",{class:!0});var Vt=r(Xe);k(tr.$$.fragment,Vt),Rg=d(Vt),ws=n(Vt,"P",{});var sl=r(ws);Hg=i(sl,"The "),zi=n(sl,"A",{href:!0});var eD=r(zi);Qg=i(eD,"DistilBertForQuestionAnswering"),eD.forEach(t),Ug=i(sl," forward method, overrides the "),sd=n(sl,"CODE",{});var tD=r(sd);Vg=i(tD,"__call__"),tD.forEach(t),Jg=i(sl," special method."),sl.forEach(t),Kg=d(Vt),k(ho.$$.fragment,Vt),Gg=d(Vt),od=n(Vt,"P",{});var sD=r(od);Xg=i(sD,"Example:"),sD.forEach(t),Yg=d(Vt),k(sr.$$.fragment,Vt),Vt.forEach(t),Ut.forEach(t),jp=d(s),$s=n(s,"H2",{class:!0});var Eh=r($s);uo=n(Eh,"A",{id:!0,class:!0,href:!0});var oD=r(uo);nd=n(oD,"SPAN",{});var nD=r(nd);k(or.$$.fragment,nD),nD.forEach(t),oD.forEach(t),Zg=d(Eh),rd=n(Eh,"SPAN",{});var rD=r(rd);e_=i(rD,"TFDistilBertModel"),rD.forEach(t),Eh.forEach(t),Cp=d(s),Pe=n(s,"DIV",{class:!0});var Tt=r(Pe);k(nr.$$.fragment,Tt),t_=d(Tt),ad=n(Tt,"P",{});var aD=r(ad);s_=i(aD,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),aD.forEach(t),o_=d(Tt),rr=n(Tt,"P",{});var Mh=r(rr);n_=i(Mh,"This model inherits from "),xi=n(Mh,"A",{href:!0});var iD=r(xi);r_=i(iD,"TFPreTrainedModel"),iD.forEach(t),a_=i(Mh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Mh.forEach(t),i_=d(Tt),ar=n(Tt,"P",{});var zh=r(ar);l_=i(zh,"This model is also a "),ir=n(zh,"A",{href:!0,rel:!0});var lD=r(ir);d_=i(lD,"tf.keras.Model"),lD.forEach(t),c_=i(zh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),zh.forEach(t),p_=d(Tt),k(fo.$$.fragment,Tt),h_=d(Tt),Ye=n(Tt,"DIV",{class:!0});var Jt=r(Ye);k(lr.$$.fragment,Jt),u_=d(Jt),Ds=n(Jt,"P",{});var ol=r(Ds);f_=i(ol,"The "),ji=n(ol,"A",{href:!0});var dD=r(ji);m_=i(dD,"TFDistilBertModel"),dD.forEach(t),g_=i(ol," forward method, overrides the "),id=n(ol,"CODE",{});var cD=r(id);__=i(cD,"__call__"),cD.forEach(t),v_=i(ol," special method."),ol.forEach(t),b_=d(Jt),k(mo.$$.fragment,Jt),T_=d(Jt),ld=n(Jt,"P",{});var pD=r(ld);k_=i(pD,"Example:"),pD.forEach(t),w_=d(Jt),k(dr.$$.fragment,Jt),Jt.forEach(t),Tt.forEach(t),Pp=d(s),Fs=n(s,"H2",{class:!0});var xh=r(Fs);go=n(xh,"A",{id:!0,class:!0,href:!0});var hD=r(go);dd=n(hD,"SPAN",{});var uD=r(dd);k(cr.$$.fragment,uD),uD.forEach(t),hD.forEach(t),$_=d(xh),cd=n(xh,"SPAN",{});var fD=r(cd);D_=i(fD,"TFDistilBertForMaskedLM"),fD.forEach(t),xh.forEach(t),qp=d(s),qe=n(s,"DIV",{class:!0});var kt=r(qe);k(pr.$$.fragment,kt),F_=d(kt),hr=n(kt,"P",{});var jh=r(hr);y_=i(jh,"DistilBert Model with a "),pd=n(jh,"CODE",{});var mD=r(pd);B_=i(mD,"masked language modeling"),mD.forEach(t),E_=i(jh," head on top."),jh.forEach(t),M_=d(kt),ur=n(kt,"P",{});var Ch=r(ur);z_=i(Ch,"This model inherits from "),Ci=n(Ch,"A",{href:!0});var gD=r(Ci);x_=i(gD,"TFPreTrainedModel"),gD.forEach(t),j_=i(Ch,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ch.forEach(t),C_=d(kt),fr=n(kt,"P",{});var Ph=r(fr);P_=i(Ph,"This model is also a "),mr=n(Ph,"A",{href:!0,rel:!0});var _D=r(mr);q_=i(_D,"tf.keras.Model"),_D.forEach(t),A_=i(Ph,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ph.forEach(t),I_=d(kt),k(_o.$$.fragment,kt),N_=d(kt),Ze=n(kt,"DIV",{class:!0});var Kt=r(Ze);k(gr.$$.fragment,Kt),L_=d(Kt),ys=n(Kt,"P",{});var nl=r(ys);S_=i(nl,"The "),Pi=n(nl,"A",{href:!0});var vD=r(Pi);O_=i(vD,"TFDistilBertForMaskedLM"),vD.forEach(t),W_=i(nl," forward method, overrides the "),hd=n(nl,"CODE",{});var bD=r(hd);R_=i(bD,"__call__"),bD.forEach(t),H_=i(nl," special method."),nl.forEach(t),Q_=d(Kt),k(vo.$$.fragment,Kt),U_=d(Kt),ud=n(Kt,"P",{});var TD=r(ud);V_=i(TD,"Example:"),TD.forEach(t),J_=d(Kt),k(_r.$$.fragment,Kt),Kt.forEach(t),kt.forEach(t),Ap=d(s),Bs=n(s,"H2",{class:!0});var qh=r(Bs);bo=n(qh,"A",{id:!0,class:!0,href:!0});var kD=r(bo);fd=n(kD,"SPAN",{});var wD=r(fd);k(vr.$$.fragment,wD),wD.forEach(t),kD.forEach(t),K_=d(qh),md=n(qh,"SPAN",{});var $D=r(md);G_=i($D,"TFDistilBertForSequenceClassification"),$D.forEach(t),qh.forEach(t),Ip=d(s),Ae=n(s,"DIV",{class:!0});var wt=r(Ae);k(br.$$.fragment,wt),X_=d(wt),gd=n(wt,"P",{});var DD=r(gd);Y_=i(DD,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),DD.forEach(t),Z_=d(wt),Tr=n(wt,"P",{});var Ah=r(Tr);ev=i(Ah,"This model inherits from "),qi=n(Ah,"A",{href:!0});var FD=r(qi);tv=i(FD,"TFPreTrainedModel"),FD.forEach(t),sv=i(Ah,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ah.forEach(t),ov=d(wt),kr=n(wt,"P",{});var Ih=r(kr);nv=i(Ih,"This model is also a "),wr=n(Ih,"A",{href:!0,rel:!0});var yD=r(wr);rv=i(yD,"tf.keras.Model"),yD.forEach(t),av=i(Ih,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ih.forEach(t),iv=d(wt),k(To.$$.fragment,wt),lv=d(wt),et=n(wt,"DIV",{class:!0});var Gt=r(et);k($r.$$.fragment,Gt),dv=d(Gt),Es=n(Gt,"P",{});var rl=r(Es);cv=i(rl,"The "),Ai=n(rl,"A",{href:!0});var BD=r(Ai);pv=i(BD,"TFDistilBertForSequenceClassification"),BD.forEach(t),hv=i(rl," forward method, overrides the "),_d=n(rl,"CODE",{});var ED=r(_d);uv=i(ED,"__call__"),ED.forEach(t),fv=i(rl," special method."),rl.forEach(t),mv=d(Gt),k(ko.$$.fragment,Gt),gv=d(Gt),vd=n(Gt,"P",{});var MD=r(vd);_v=i(MD,"Example:"),MD.forEach(t),vv=d(Gt),k(Dr.$$.fragment,Gt),Gt.forEach(t),wt.forEach(t),Np=d(s),Ms=n(s,"H2",{class:!0});var Nh=r(Ms);wo=n(Nh,"A",{id:!0,class:!0,href:!0});var zD=r(wo);bd=n(zD,"SPAN",{});var xD=r(bd);k(Fr.$$.fragment,xD),xD.forEach(t),zD.forEach(t),bv=d(Nh),Td=n(Nh,"SPAN",{});var jD=r(Td);Tv=i(jD,"TFDistilBertForMultipleChoice"),jD.forEach(t),Nh.forEach(t),Lp=d(s),Ie=n(s,"DIV",{class:!0});var $t=r(Ie);k(yr.$$.fragment,$t),kv=d($t),kd=n($t,"P",{});var CD=r(kd);wv=i(CD,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),CD.forEach(t),$v=d($t),Br=n($t,"P",{});var Lh=r(Br);Dv=i(Lh,"This model inherits from "),Ii=n(Lh,"A",{href:!0});var PD=r(Ii);Fv=i(PD,"TFPreTrainedModel"),PD.forEach(t),yv=i(Lh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lh.forEach(t),Bv=d($t),Er=n($t,"P",{});var Sh=r(Er);Ev=i(Sh,"This model is also a "),Mr=n(Sh,"A",{href:!0,rel:!0});var qD=r(Mr);Mv=i(qD,"tf.keras.Model"),qD.forEach(t),zv=i(Sh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Sh.forEach(t),xv=d($t),k($o.$$.fragment,$t),jv=d($t),tt=n($t,"DIV",{class:!0});var Xt=r(tt);k(zr.$$.fragment,Xt),Cv=d(Xt),zs=n(Xt,"P",{});var al=r(zs);Pv=i(al,"The "),Ni=n(al,"A",{href:!0});var AD=r(Ni);qv=i(AD,"TFDistilBertForMultipleChoice"),AD.forEach(t),Av=i(al," forward method, overrides the "),wd=n(al,"CODE",{});var ID=r(wd);Iv=i(ID,"__call__"),ID.forEach(t),Nv=i(al," special method."),al.forEach(t),Lv=d(Xt),k(Do.$$.fragment,Xt),Sv=d(Xt),$d=n(Xt,"P",{});var ND=r($d);Ov=i(ND,"Example:"),ND.forEach(t),Wv=d(Xt),k(xr.$$.fragment,Xt),Xt.forEach(t),$t.forEach(t),Sp=d(s),xs=n(s,"H2",{class:!0});var Oh=r(xs);Fo=n(Oh,"A",{id:!0,class:!0,href:!0});var LD=r(Fo);Dd=n(LD,"SPAN",{});var SD=r(Dd);k(jr.$$.fragment,SD),SD.forEach(t),LD.forEach(t),Rv=d(Oh),Fd=n(Oh,"SPAN",{});var OD=r(Fd);Hv=i(OD,"TFDistilBertForTokenClassification"),OD.forEach(t),Oh.forEach(t),Op=d(s),Ne=n(s,"DIV",{class:!0});var Dt=r(Ne);k(Cr.$$.fragment,Dt),Qv=d(Dt),yd=n(Dt,"P",{});var WD=r(yd);Uv=i(WD,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),WD.forEach(t),Vv=d(Dt),Pr=n(Dt,"P",{});var Wh=r(Pr);Jv=i(Wh,"This model inherits from "),Li=n(Wh,"A",{href:!0});var RD=r(Li);Kv=i(RD,"TFPreTrainedModel"),RD.forEach(t),Gv=i(Wh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wh.forEach(t),Xv=d(Dt),qr=n(Dt,"P",{});var Rh=r(qr);Yv=i(Rh,"This model is also a "),Ar=n(Rh,"A",{href:!0,rel:!0});var HD=r(Ar);Zv=i(HD,"tf.keras.Model"),HD.forEach(t),eb=i(Rh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Rh.forEach(t),tb=d(Dt),k(yo.$$.fragment,Dt),sb=d(Dt),st=n(Dt,"DIV",{class:!0});var Yt=r(st);k(Ir.$$.fragment,Yt),ob=d(Yt),js=n(Yt,"P",{});var il=r(js);nb=i(il,"The "),Si=n(il,"A",{href:!0});var QD=r(Si);rb=i(QD,"TFDistilBertForTokenClassification"),QD.forEach(t),ab=i(il," forward method, overrides the "),Bd=n(il,"CODE",{});var UD=r(Bd);ib=i(UD,"__call__"),UD.forEach(t),lb=i(il," special method."),il.forEach(t),db=d(Yt),k(Bo.$$.fragment,Yt),cb=d(Yt),Ed=n(Yt,"P",{});var VD=r(Ed);pb=i(VD,"Example:"),VD.forEach(t),hb=d(Yt),k(Nr.$$.fragment,Yt),Yt.forEach(t),Dt.forEach(t),Wp=d(s),Cs=n(s,"H2",{class:!0});var Hh=r(Cs);Eo=n(Hh,"A",{id:!0,class:!0,href:!0});var JD=r(Eo);Md=n(JD,"SPAN",{});var KD=r(Md);k(Lr.$$.fragment,KD),KD.forEach(t),JD.forEach(t),ub=d(Hh),zd=n(Hh,"SPAN",{});var GD=r(zd);fb=i(GD,"TFDistilBertForQuestionAnswering"),GD.forEach(t),Hh.forEach(t),Rp=d(s),Le=n(s,"DIV",{class:!0});var Ft=r(Le);k(Sr.$$.fragment,Ft),mb=d(Ft),Ps=n(Ft,"P",{});var ll=r(Ps);gb=i(ll,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),xd=n(ll,"CODE",{});var XD=r(xd);_b=i(XD,"span start logits"),XD.forEach(t),vb=i(ll," and "),jd=n(ll,"CODE",{});var YD=r(jd);bb=i(YD,"span end logits"),YD.forEach(t),Tb=i(ll,")."),ll.forEach(t),kb=d(Ft),Or=n(Ft,"P",{});var Qh=r(Or);wb=i(Qh,"This model inherits from "),Oi=n(Qh,"A",{href:!0});var ZD=r(Oi);$b=i(ZD,"TFPreTrainedModel"),ZD.forEach(t),Db=i(Qh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qh.forEach(t),Fb=d(Ft),Wr=n(Ft,"P",{});var Uh=r(Wr);yb=i(Uh,"This model is also a "),Rr=n(Uh,"A",{href:!0,rel:!0});var e2=r(Rr);Bb=i(e2,"tf.keras.Model"),e2.forEach(t),Eb=i(Uh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Uh.forEach(t),Mb=d(Ft),k(Mo.$$.fragment,Ft),zb=d(Ft),ot=n(Ft,"DIV",{class:!0});var Zt=r(ot);k(Hr.$$.fragment,Zt),xb=d(Zt),qs=n(Zt,"P",{});var dl=r(qs);jb=i(dl,"The "),Wi=n(dl,"A",{href:!0});var t2=r(Wi);Cb=i(t2,"TFDistilBertForQuestionAnswering"),t2.forEach(t),Pb=i(dl," forward method, overrides the "),Cd=n(dl,"CODE",{});var s2=r(Cd);qb=i(s2,"__call__"),s2.forEach(t),Ab=i(dl," special method."),dl.forEach(t),Ib=d(Zt),k(zo.$$.fragment,Zt),Nb=d(Zt),Pd=n(Zt,"P",{});var o2=r(Pd);Lb=i(o2,"Example:"),o2.forEach(t),Sb=d(Zt),k(Qr.$$.fragment,Zt),Zt.forEach(t),Ft.forEach(t),Hp=d(s),As=n(s,"H2",{class:!0});var Vh=r(As);xo=n(Vh,"A",{id:!0,class:!0,href:!0});var n2=r(xo);qd=n(n2,"SPAN",{});var r2=r(qd);k(Ur.$$.fragment,r2),r2.forEach(t),n2.forEach(t),Ob=d(Vh),Ad=n(Vh,"SPAN",{});var a2=r(Ad);Wb=i(a2,"FlaxDistilBertModel"),a2.forEach(t),Vh.forEach(t),Qp=d(s),Be=n(s,"DIV",{class:!0});var pt=r(Be);k(Vr.$$.fragment,pt),Rb=d(pt),Id=n(pt,"P",{});var i2=r(Id);Hb=i(i2,"The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),i2.forEach(t),Qb=d(pt),Jr=n(pt,"P",{});var Jh=r(Jr);Ub=i(Jh,"This model inherits from "),Ri=n(Jh,"A",{href:!0});var l2=r(Ri);Vb=i(l2,"FlaxPreTrainedModel"),l2.forEach(t),Jb=i(Jh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Jh.forEach(t),Kb=d(pt),Kr=n(pt,"P",{});var Kh=r(Kr);Gb=i(Kh,"This model is also a Flax Linen "),Gr=n(Kh,"A",{href:!0,rel:!0});var d2=r(Gr);Xb=i(d2,"flax.linen.Module"),d2.forEach(t),Yb=i(Kh,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Kh.forEach(t),Zb=d(pt),Nd=n(pt,"P",{});var c2=r(Nd);e1=i(c2,"Finally, this model supports inherent JAX features such as:"),c2.forEach(t),t1=d(pt),Bt=n(pt,"UL",{});var Uo=r(Bt);Ld=n(Uo,"LI",{});var p2=r(Ld);Xr=n(p2,"A",{href:!0,rel:!0});var h2=r(Xr);s1=i(h2,"Just-In-Time (JIT) compilation"),h2.forEach(t),p2.forEach(t),o1=d(Uo),Sd=n(Uo,"LI",{});var u2=r(Sd);Yr=n(u2,"A",{href:!0,rel:!0});var f2=r(Yr);n1=i(f2,"Automatic Differentiation"),f2.forEach(t),u2.forEach(t),r1=d(Uo),Od=n(Uo,"LI",{});var m2=r(Od);Zr=n(m2,"A",{href:!0,rel:!0});var g2=r(Zr);a1=i(g2,"Vectorization"),g2.forEach(t),m2.forEach(t),i1=d(Uo),Wd=n(Uo,"LI",{});var _2=r(Wd);ea=n(_2,"A",{href:!0,rel:!0});var v2=r(ea);l1=i(v2,"Parallelization"),v2.forEach(t),_2.forEach(t),Uo.forEach(t),d1=d(pt),nt=n(pt,"DIV",{class:!0});var es=r(nt);k(ta.$$.fragment,es),c1=d(es),Is=n(es,"P",{});var cl=r(Is);p1=i(cl,"The "),Rd=n(cl,"CODE",{});var b2=r(Rd);h1=i(b2,"FlaxDistilBertPreTrainedModel"),b2.forEach(t),u1=i(cl,"forward method, overrides the "),Hd=n(cl,"CODE",{});var T2=r(Hd);f1=i(T2,"__call__"),T2.forEach(t),m1=i(cl," special method."),cl.forEach(t),g1=d(es),k(jo.$$.fragment,es),_1=d(es),Qd=n(es,"P",{});var k2=r(Qd);v1=i(k2,"Example:"),k2.forEach(t),b1=d(es),k(sa.$$.fragment,es),es.forEach(t),pt.forEach(t),Up=d(s),Ns=n(s,"H2",{class:!0});var Gh=r(Ns);Co=n(Gh,"A",{id:!0,class:!0,href:!0});var w2=r(Co);Ud=n(w2,"SPAN",{});var $2=r(Ud);k(oa.$$.fragment,$2),$2.forEach(t),w2.forEach(t),T1=d(Gh),Vd=n(Gh,"SPAN",{});var D2=r(Vd);k1=i(D2,"FlaxDistilBertForMaskedLM"),D2.forEach(t),Gh.forEach(t),Vp=d(s),Ee=n(s,"DIV",{class:!0});var ht=r(Ee);k(na.$$.fragment,ht),w1=d(ht),ra=n(ht,"P",{});var Xh=r(ra);$1=i(Xh,"DistilBert Model with a "),Jd=n(Xh,"CODE",{});var F2=r(Jd);D1=i(F2,"language modeling"),F2.forEach(t),F1=i(Xh," head on top."),Xh.forEach(t),y1=d(ht),aa=n(ht,"P",{});var Yh=r(aa);B1=i(Yh,"This model inherits from "),Hi=n(Yh,"A",{href:!0});var y2=r(Hi);E1=i(y2,"FlaxPreTrainedModel"),y2.forEach(t),M1=i(Yh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Yh.forEach(t),z1=d(ht),ia=n(ht,"P",{});var Zh=r(ia);x1=i(Zh,"This model is also a Flax Linen "),la=n(Zh,"A",{href:!0,rel:!0});var B2=r(la);j1=i(B2,"flax.linen.Module"),B2.forEach(t),C1=i(Zh,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Zh.forEach(t),P1=d(ht),Kd=n(ht,"P",{});var E2=r(Kd);q1=i(E2,"Finally, this model supports inherent JAX features such as:"),E2.forEach(t),A1=d(ht),Et=n(ht,"UL",{});var Vo=r(Et);Gd=n(Vo,"LI",{});var M2=r(Gd);da=n(M2,"A",{href:!0,rel:!0});var z2=r(da);I1=i(z2,"Just-In-Time (JIT) compilation"),z2.forEach(t),M2.forEach(t),N1=d(Vo),Xd=n(Vo,"LI",{});var x2=r(Xd);ca=n(x2,"A",{href:!0,rel:!0});var j2=r(ca);L1=i(j2,"Automatic Differentiation"),j2.forEach(t),x2.forEach(t),S1=d(Vo),Yd=n(Vo,"LI",{});var C2=r(Yd);pa=n(C2,"A",{href:!0,rel:!0});var P2=r(pa);O1=i(P2,"Vectorization"),P2.forEach(t),C2.forEach(t),W1=d(Vo),Zd=n(Vo,"LI",{});var q2=r(Zd);ha=n(q2,"A",{href:!0,rel:!0});var A2=r(ha);R1=i(A2,"Parallelization"),A2.forEach(t),q2.forEach(t),Vo.forEach(t),H1=d(ht),rt=n(ht,"DIV",{class:!0});var ts=r(rt);k(ua.$$.fragment,ts),Q1=d(ts),Ls=n(ts,"P",{});var pl=r(Ls);U1=i(pl,"The "),ec=n(pl,"CODE",{});var I2=r(ec);V1=i(I2,"FlaxDistilBertPreTrainedModel"),I2.forEach(t),J1=i(pl,"forward method, overrides the "),tc=n(pl,"CODE",{});var N2=r(tc);K1=i(N2,"__call__"),N2.forEach(t),G1=i(pl," special method."),pl.forEach(t),X1=d(ts),k(Po.$$.fragment,ts),Y1=d(ts),sc=n(ts,"P",{});var L2=r(sc);Z1=i(L2,"Example:"),L2.forEach(t),eT=d(ts),k(fa.$$.fragment,ts),ts.forEach(t),ht.forEach(t),Jp=d(s),Ss=n(s,"H2",{class:!0});var eu=r(Ss);qo=n(eu,"A",{id:!0,class:!0,href:!0});var S2=r(qo);oc=n(S2,"SPAN",{});var O2=r(oc);k(ma.$$.fragment,O2),O2.forEach(t),S2.forEach(t),tT=d(eu),nc=n(eu,"SPAN",{});var W2=r(nc);sT=i(W2,"FlaxDistilBertForSequenceClassification"),W2.forEach(t),eu.forEach(t),Kp=d(s),Me=n(s,"DIV",{class:!0});var ut=r(Me);k(ga.$$.fragment,ut),oT=d(ut),rc=n(ut,"P",{});var R2=r(rc);nT=i(R2,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),R2.forEach(t),rT=d(ut),_a=n(ut,"P",{});var tu=r(_a);aT=i(tu,"This model inherits from "),Qi=n(tu,"A",{href:!0});var H2=r(Qi);iT=i(H2,"FlaxPreTrainedModel"),H2.forEach(t),lT=i(tu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),tu.forEach(t),dT=d(ut),va=n(ut,"P",{});var su=r(va);cT=i(su,"This model is also a Flax Linen "),ba=n(su,"A",{href:!0,rel:!0});var Q2=r(ba);pT=i(Q2,"flax.linen.Module"),Q2.forEach(t),hT=i(su,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),su.forEach(t),uT=d(ut),ac=n(ut,"P",{});var U2=r(ac);fT=i(U2,"Finally, this model supports inherent JAX features such as:"),U2.forEach(t),mT=d(ut),Mt=n(ut,"UL",{});var Jo=r(Mt);ic=n(Jo,"LI",{});var V2=r(ic);Ta=n(V2,"A",{href:!0,rel:!0});var J2=r(Ta);gT=i(J2,"Just-In-Time (JIT) compilation"),J2.forEach(t),V2.forEach(t),_T=d(Jo),lc=n(Jo,"LI",{});var K2=r(lc);ka=n(K2,"A",{href:!0,rel:!0});var G2=r(ka);vT=i(G2,"Automatic Differentiation"),G2.forEach(t),K2.forEach(t),bT=d(Jo),dc=n(Jo,"LI",{});var X2=r(dc);wa=n(X2,"A",{href:!0,rel:!0});var Y2=r(wa);TT=i(Y2,"Vectorization"),Y2.forEach(t),X2.forEach(t),kT=d(Jo),cc=n(Jo,"LI",{});var Z2=r(cc);$a=n(Z2,"A",{href:!0,rel:!0});var eF=r($a);wT=i(eF,"Parallelization"),eF.forEach(t),Z2.forEach(t),Jo.forEach(t),$T=d(ut),at=n(ut,"DIV",{class:!0});var ss=r(at);k(Da.$$.fragment,ss),DT=d(ss),Os=n(ss,"P",{});var hl=r(Os);FT=i(hl,"The "),pc=n(hl,"CODE",{});var tF=r(pc);yT=i(tF,"FlaxDistilBertPreTrainedModel"),tF.forEach(t),BT=i(hl,"forward method, overrides the "),hc=n(hl,"CODE",{});var sF=r(hc);ET=i(sF,"__call__"),sF.forEach(t),MT=i(hl," special method."),hl.forEach(t),zT=d(ss),k(Ao.$$.fragment,ss),xT=d(ss),uc=n(ss,"P",{});var oF=r(uc);jT=i(oF,"Example:"),oF.forEach(t),CT=d(ss),k(Fa.$$.fragment,ss),ss.forEach(t),ut.forEach(t),Gp=d(s),Ws=n(s,"H2",{class:!0});var ou=r(Ws);Io=n(ou,"A",{id:!0,class:!0,href:!0});var nF=r(Io);fc=n(nF,"SPAN",{});var rF=r(fc);k(ya.$$.fragment,rF),rF.forEach(t),nF.forEach(t),PT=d(ou),mc=n(ou,"SPAN",{});var aF=r(mc);qT=i(aF,"FlaxDistilBertForMultipleChoice"),aF.forEach(t),ou.forEach(t),Xp=d(s),ze=n(s,"DIV",{class:!0});var ft=r(ze);k(Ba.$$.fragment,ft),AT=d(ft),gc=n(ft,"P",{});var iF=r(gc);IT=i(iF,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),iF.forEach(t),NT=d(ft),Ea=n(ft,"P",{});var nu=r(Ea);LT=i(nu,"This model inherits from "),Ui=n(nu,"A",{href:!0});var lF=r(Ui);ST=i(lF,"FlaxPreTrainedModel"),lF.forEach(t),OT=i(nu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),nu.forEach(t),WT=d(ft),Ma=n(ft,"P",{});var ru=r(Ma);RT=i(ru,"This model is also a Flax Linen "),za=n(ru,"A",{href:!0,rel:!0});var dF=r(za);HT=i(dF,"flax.linen.Module"),dF.forEach(t),QT=i(ru,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),ru.forEach(t),UT=d(ft),_c=n(ft,"P",{});var cF=r(_c);VT=i(cF,"Finally, this model supports inherent JAX features such as:"),cF.forEach(t),JT=d(ft),zt=n(ft,"UL",{});var Ko=r(zt);vc=n(Ko,"LI",{});var pF=r(vc);xa=n(pF,"A",{href:!0,rel:!0});var hF=r(xa);KT=i(hF,"Just-In-Time (JIT) compilation"),hF.forEach(t),pF.forEach(t),GT=d(Ko),bc=n(Ko,"LI",{});var uF=r(bc);ja=n(uF,"A",{href:!0,rel:!0});var fF=r(ja);XT=i(fF,"Automatic Differentiation"),fF.forEach(t),uF.forEach(t),YT=d(Ko),Tc=n(Ko,"LI",{});var mF=r(Tc);Ca=n(mF,"A",{href:!0,rel:!0});var gF=r(Ca);ZT=i(gF,"Vectorization"),gF.forEach(t),mF.forEach(t),ek=d(Ko),kc=n(Ko,"LI",{});var _F=r(kc);Pa=n(_F,"A",{href:!0,rel:!0});var vF=r(Pa);tk=i(vF,"Parallelization"),vF.forEach(t),_F.forEach(t),Ko.forEach(t),sk=d(ft),it=n(ft,"DIV",{class:!0});var os=r(it);k(qa.$$.fragment,os),ok=d(os),Rs=n(os,"P",{});var ul=r(Rs);nk=i(ul,"The "),wc=n(ul,"CODE",{});var bF=r(wc);rk=i(bF,"FlaxDistilBertPreTrainedModel"),bF.forEach(t),ak=i(ul,"forward method, overrides the "),$c=n(ul,"CODE",{});var TF=r($c);ik=i(TF,"__call__"),TF.forEach(t),lk=i(ul," special method."),ul.forEach(t),dk=d(os),k(No.$$.fragment,os),ck=d(os),Dc=n(os,"P",{});var kF=r(Dc);pk=i(kF,"Example:"),kF.forEach(t),hk=d(os),k(Aa.$$.fragment,os),os.forEach(t),ft.forEach(t),Yp=d(s),Hs=n(s,"H2",{class:!0});var au=r(Hs);Lo=n(au,"A",{id:!0,class:!0,href:!0});var wF=r(Lo);Fc=n(wF,"SPAN",{});var $F=r(Fc);k(Ia.$$.fragment,$F),$F.forEach(t),wF.forEach(t),uk=d(au),yc=n(au,"SPAN",{});var DF=r(yc);fk=i(DF,"FlaxDistilBertForTokenClassification"),DF.forEach(t),au.forEach(t),Zp=d(s),xe=n(s,"DIV",{class:!0});var mt=r(xe);k(Na.$$.fragment,mt),mk=d(mt),Bc=n(mt,"P",{});var FF=r(Bc);gk=i(FF,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),FF.forEach(t),_k=d(mt),La=n(mt,"P",{});var iu=r(La);vk=i(iu,"This model inherits from "),Vi=n(iu,"A",{href:!0});var yF=r(Vi);bk=i(yF,"FlaxPreTrainedModel"),yF.forEach(t),Tk=i(iu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),iu.forEach(t),kk=d(mt),Sa=n(mt,"P",{});var lu=r(Sa);wk=i(lu,"This model is also a Flax Linen "),Oa=n(lu,"A",{href:!0,rel:!0});var BF=r(Oa);$k=i(BF,"flax.linen.Module"),BF.forEach(t),Dk=i(lu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),lu.forEach(t),Fk=d(mt),Ec=n(mt,"P",{});var EF=r(Ec);yk=i(EF,"Finally, this model supports inherent JAX features such as:"),EF.forEach(t),Bk=d(mt),xt=n(mt,"UL",{});var Go=r(xt);Mc=n(Go,"LI",{});var MF=r(Mc);Wa=n(MF,"A",{href:!0,rel:!0});var zF=r(Wa);Ek=i(zF,"Just-In-Time (JIT) compilation"),zF.forEach(t),MF.forEach(t),Mk=d(Go),zc=n(Go,"LI",{});var xF=r(zc);Ra=n(xF,"A",{href:!0,rel:!0});var jF=r(Ra);zk=i(jF,"Automatic Differentiation"),jF.forEach(t),xF.forEach(t),xk=d(Go),xc=n(Go,"LI",{});var CF=r(xc);Ha=n(CF,"A",{href:!0,rel:!0});var PF=r(Ha);jk=i(PF,"Vectorization"),PF.forEach(t),CF.forEach(t),Ck=d(Go),jc=n(Go,"LI",{});var qF=r(jc);Qa=n(qF,"A",{href:!0,rel:!0});var AF=r(Qa);Pk=i(AF,"Parallelization"),AF.forEach(t),qF.forEach(t),Go.forEach(t),qk=d(mt),lt=n(mt,"DIV",{class:!0});var ns=r(lt);k(Ua.$$.fragment,ns),Ak=d(ns),Qs=n(ns,"P",{});var fl=r(Qs);Ik=i(fl,"The "),Cc=n(fl,"CODE",{});var IF=r(Cc);Nk=i(IF,"FlaxDistilBertPreTrainedModel"),IF.forEach(t),Lk=i(fl,"forward method, overrides the "),Pc=n(fl,"CODE",{});var NF=r(Pc);Sk=i(NF,"__call__"),NF.forEach(t),Ok=i(fl," special method."),fl.forEach(t),Wk=d(ns),k(So.$$.fragment,ns),Rk=d(ns),qc=n(ns,"P",{});var LF=r(qc);Hk=i(LF,"Example:"),LF.forEach(t),Qk=d(ns),k(Va.$$.fragment,ns),ns.forEach(t),mt.forEach(t),eh=d(s),Us=n(s,"H2",{class:!0});var du=r(Us);Oo=n(du,"A",{id:!0,class:!0,href:!0});var SF=r(Oo);Ac=n(SF,"SPAN",{});var OF=r(Ac);k(Ja.$$.fragment,OF),OF.forEach(t),SF.forEach(t),Uk=d(du),Ic=n(du,"SPAN",{});var WF=r(Ic);Vk=i(WF,"FlaxDistilBertForQuestionAnswering"),WF.forEach(t),du.forEach(t),th=d(s),je=n(s,"DIV",{class:!0});var gt=r(je);k(Ka.$$.fragment,gt),Jk=d(gt),Vs=n(gt,"P",{});var ml=r(Vs);Kk=i(ml,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Nc=n(ml,"CODE",{});var RF=r(Nc);Gk=i(RF,"span start logits"),RF.forEach(t),Xk=i(ml," and "),Lc=n(ml,"CODE",{});var HF=r(Lc);Yk=i(HF,"span end logits"),HF.forEach(t),Zk=i(ml,")."),ml.forEach(t),ew=d(gt),Ga=n(gt,"P",{});var cu=r(Ga);tw=i(cu,"This model inherits from "),Ji=n(cu,"A",{href:!0});var QF=r(Ji);sw=i(QF,"FlaxPreTrainedModel"),QF.forEach(t),ow=i(cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),cu.forEach(t),nw=d(gt),Xa=n(gt,"P",{});var pu=r(Xa);rw=i(pu,"This model is also a Flax Linen "),Ya=n(pu,"A",{href:!0,rel:!0});var UF=r(Ya);aw=i(UF,"flax.linen.Module"),UF.forEach(t),iw=i(pu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),pu.forEach(t),lw=d(gt),Sc=n(gt,"P",{});var VF=r(Sc);dw=i(VF,"Finally, this model supports inherent JAX features such as:"),VF.forEach(t),cw=d(gt),jt=n(gt,"UL",{});var Xo=r(jt);Oc=n(Xo,"LI",{});var JF=r(Oc);Za=n(JF,"A",{href:!0,rel:!0});var KF=r(Za);pw=i(KF,"Just-In-Time (JIT) compilation"),KF.forEach(t),JF.forEach(t),hw=d(Xo),Wc=n(Xo,"LI",{});var GF=r(Wc);ei=n(GF,"A",{href:!0,rel:!0});var XF=r(ei);uw=i(XF,"Automatic Differentiation"),XF.forEach(t),GF.forEach(t),fw=d(Xo),Rc=n(Xo,"LI",{});var YF=r(Rc);ti=n(YF,"A",{href:!0,rel:!0});var ZF=r(ti);mw=i(ZF,"Vectorization"),ZF.forEach(t),YF.forEach(t),gw=d(Xo),Hc=n(Xo,"LI",{});var ey=r(Hc);si=n(ey,"A",{href:!0,rel:!0});var ty=r(si);_w=i(ty,"Parallelization"),ty.forEach(t),ey.forEach(t),Xo.forEach(t),vw=d(gt),dt=n(gt,"DIV",{class:!0});var rs=r(dt);k(oi.$$.fragment,rs),bw=d(rs),Js=n(rs,"P",{});var gl=r(Js);Tw=i(gl,"The "),Qc=n(gl,"CODE",{});var sy=r(Qc);kw=i(sy,"FlaxDistilBertPreTrainedModel"),sy.forEach(t),ww=i(gl,"forward method, overrides the "),Uc=n(gl,"CODE",{});var oy=r(Uc);$w=i(oy,"__call__"),oy.forEach(t),Dw=i(gl," special method."),gl.forEach(t),Fw=d(rs),k(Wo.$$.fragment,rs),yw=d(rs),Vc=n(rs,"P",{});var ny=r(Vc);Bw=i(ny,"Example:"),ny.forEach(t),Ew=d(rs),k(ni.$$.fragment,rs),rs.forEach(t),gt.forEach(t),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Py)),c(v,"id","distilbert"),c(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(v,"href","#distilbert"),c(g,"class","relative group"),c(G,"id","overview"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#overview"),c(M,"class","relative group"),c(Y,"href","https://medium.com/huggingface/distilbert-8cf3380435b5"),c(Y,"rel","nofollow"),c(x,"href","https://arxiv.org/abs/1910.01108"),c(x,"rel","nofollow"),c(Yo,"href","https://huggingface.co/victorsanh"),c(Yo,"rel","nofollow"),c(Zo,"href","https://huggingface.co/kamalkraj"),c(Zo,"rel","nofollow"),c(en,"href","https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation"),c(en,"rel","nofollow"),c(Ks,"id","transformers.DistilBertConfig"),c(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ks,"href","#transformers.DistilBertConfig"),c(as,"class","relative group"),c(di,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertModel"),c(ci,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(on,"href","https://huggingface.co/distilbert-base-uncased"),c(on,"rel","nofollow"),c(pi,"href","/docs/transformers/pr_15987/en/main_classes/configuration#transformers.PretrainedConfig"),c(hi,"href","/docs/transformers/pr_15987/en/main_classes/configuration#transformers.PretrainedConfig"),c(Se,"class","docstring"),c(Gs,"id","transformers.DistilBertTokenizer"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#transformers.DistilBertTokenizer"),c(ls,"class","relative group"),c(ui,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(fi,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer"),c(mi,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer"),c(_t,"class","docstring"),c(Ys,"id","transformers.DistilBertTokenizerFast"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#transformers.DistilBertTokenizerFast"),c(ds,"class","relative group"),c(gi,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(_i,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizerFast"),c(vi,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizerFast"),c(vt,"class","docstring"),c(eo,"id","transformers.DistilBertModel"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.DistilBertModel"),c(cs,"class","relative group"),c(bi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(_n,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(_n,"rel","nofollow"),c(Ti,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertModel"),c(Ve,"class","docstring"),c(Oe,"class","docstring"),c(so,"id","transformers.DistilBertForMaskedLM"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.DistilBertForMaskedLM"),c(hs,"class","relative group"),c(ki,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(Fn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Fn,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(Je,"class","docstring"),c(We,"class","docstring"),c(no,"id","transformers.DistilBertForSequenceClassification"),c(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(no,"href","#transformers.DistilBertForSequenceClassification"),c(fs,"class","relative group"),c($i,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(jn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(jn,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(Ce,"class","docstring"),c(Re,"class","docstring"),c(ao,"id","transformers.DistilBertForMultipleChoice"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.DistilBertForMultipleChoice"),c(gs,"class","relative group"),c(Fi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(Sn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Sn,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(Ke,"class","docstring"),c(He,"class","docstring"),c(lo,"id","transformers.DistilBertForTokenClassification"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.DistilBertForTokenClassification"),c(vs,"class","relative group"),c(Bi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(Vn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Vn,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(Ge,"class","docstring"),c(Qe,"class","docstring"),c(po,"id","transformers.DistilBertForQuestionAnswering"),c(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(po,"href","#transformers.DistilBertForQuestionAnswering"),c(Ts,"class","relative group"),c(Mi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(er,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(er,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Xe,"class","docstring"),c(Ue,"class","docstring"),c(uo,"id","transformers.TFDistilBertModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFDistilBertModel"),c($s,"class","relative group"),c(xi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(ir,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ir,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(Ye,"class","docstring"),c(Pe,"class","docstring"),c(go,"id","transformers.TFDistilBertForMaskedLM"),c(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(go,"href","#transformers.TFDistilBertForMaskedLM"),c(Fs,"class","relative group"),c(Ci,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(mr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(mr,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(Ze,"class","docstring"),c(qe,"class","docstring"),c(bo,"id","transformers.TFDistilBertForSequenceClassification"),c(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bo,"href","#transformers.TFDistilBertForSequenceClassification"),c(Bs,"class","relative group"),c(qi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(wr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(wr,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(et,"class","docstring"),c(Ae,"class","docstring"),c(wo,"id","transformers.TFDistilBertForMultipleChoice"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFDistilBertForMultipleChoice"),c(Ms,"class","relative group"),c(Ii,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Mr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Mr,"rel","nofollow"),c(Ni,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(tt,"class","docstring"),c(Ie,"class","docstring"),c(Fo,"id","transformers.TFDistilBertForTokenClassification"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFDistilBertForTokenClassification"),c(xs,"class","relative group"),c(Li,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ar,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ar,"rel","nofollow"),c(Si,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(st,"class","docstring"),c(Ne,"class","docstring"),c(Eo,"id","transformers.TFDistilBertForQuestionAnswering"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFDistilBertForQuestionAnswering"),c(Cs,"class","relative group"),c(Oi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rr,"rel","nofollow"),c(Wi,"href","/docs/transformers/pr_15987/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(ot,"class","docstring"),c(Le,"class","docstring"),c(xo,"id","transformers.FlaxDistilBertModel"),c(xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xo,"href","#transformers.FlaxDistilBertModel"),c(As,"class","relative group"),c(Ri,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Gr,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Gr,"rel","nofollow"),c(Xr,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Xr,"rel","nofollow"),c(Yr,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Yr,"rel","nofollow"),c(Zr,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Zr,"rel","nofollow"),c(ea,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ea,"rel","nofollow"),c(nt,"class","docstring"),c(Be,"class","docstring"),c(Co,"id","transformers.FlaxDistilBertForMaskedLM"),c(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Co,"href","#transformers.FlaxDistilBertForMaskedLM"),c(Ns,"class","relative group"),c(Hi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(la,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(la,"rel","nofollow"),c(da,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(da,"rel","nofollow"),c(ca,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ca,"rel","nofollow"),c(pa,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(pa,"rel","nofollow"),c(ha,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ha,"rel","nofollow"),c(rt,"class","docstring"),c(Ee,"class","docstring"),c(qo,"id","transformers.FlaxDistilBertForSequenceClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.FlaxDistilBertForSequenceClassification"),c(Ss,"class","relative group"),c(Qi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(ba,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(ba,"rel","nofollow"),c(Ta,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Ta,"rel","nofollow"),c(ka,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ka,"rel","nofollow"),c(wa,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(wa,"rel","nofollow"),c($a,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c($a,"rel","nofollow"),c(at,"class","docstring"),c(Me,"class","docstring"),c(Io,"id","transformers.FlaxDistilBertForMultipleChoice"),c(Io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Io,"href","#transformers.FlaxDistilBertForMultipleChoice"),c(Ws,"class","relative group"),c(Ui,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(za,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(za,"rel","nofollow"),c(xa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(xa,"rel","nofollow"),c(ja,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ja,"rel","nofollow"),c(Ca,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Ca,"rel","nofollow"),c(Pa,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Pa,"rel","nofollow"),c(it,"class","docstring"),c(ze,"class","docstring"),c(Lo,"id","transformers.FlaxDistilBertForTokenClassification"),c(Lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lo,"href","#transformers.FlaxDistilBertForTokenClassification"),c(Hs,"class","relative group"),c(Vi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Oa,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Oa,"rel","nofollow"),c(Wa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Wa,"rel","nofollow"),c(Ra,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ra,"rel","nofollow"),c(Ha,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Ha,"rel","nofollow"),c(Qa,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Qa,"rel","nofollow"),c(lt,"class","docstring"),c(xe,"class","docstring"),c(Oo,"id","transformers.FlaxDistilBertForQuestionAnswering"),c(Oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oo,"href","#transformers.FlaxDistilBertForQuestionAnswering"),c(Us,"class","relative group"),c(Ji,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ya,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ya,"rel","nofollow"),c(Za,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Za,"rel","nofollow"),c(ei,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ei,"rel","nofollow"),c(ti,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ti,"rel","nofollow"),c(si,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(si,"rel","nofollow"),c(dt,"class","docstring"),c(je,"class","docstring")},m(s,m){e(document.head,p),u(s,y,m),u(s,g,m),e(g,v),e(v,b),w(_,b,null),e(g,f),e(g,B),e(B,de),u(s,V,m),u(s,M,m),e(M,G),e(G,S),w(X,S,null),e(M,ce),e(M,O),e(O,pe),u(s,re,m),u(s,N,m),e(N,q),e(N,Y),e(Y,J),e(N,z),e(N,x),e(x,he),e(N,W),e(N,se),e(se,ue),e(N,R),u(s,ae,m),u(s,ee,m),e(ee,A),u(s,ie,m),u(s,L,m),e(L,oe),e(oe,fe),u(s,P,m),u(s,te,m),e(te,H),u(s,le,m),u(s,h,m),e(h,E),e(E,K),e(E,ge),e(ge,be),e(E,I),e(E,_e),e(_e,Te),e(E,ke),e(E,C),e(C,Q),e(E,we),e(h,$e),e(h,Z),e(Z,De),e(Z,ne),e(ne,Fe),e(Z,hu),u(s,up,m),u(s,bt,m),e(bt,uu),e(bt,Yo),e(Yo,fu),e(bt,mu),e(bt,Zo),e(Zo,gu),e(bt,_u),e(bt,en),e(en,vu),e(bt,bu),u(s,fp,m),u(s,as,m),e(as,Ks),e(Ks,_l),w(tn,_l,null),e(as,Tu),e(as,vl),e(vl,ku),u(s,mp,m),u(s,Se,m),w(sn,Se,null),e(Se,wu),e(Se,yt),e(yt,$u),e(yt,di),e(di,Du),e(yt,Fu),e(yt,ci),e(ci,yu),e(yt,Bu),e(yt,on),e(on,Eu),e(yt,Mu),e(Se,zu),e(Se,is),e(is,xu),e(is,pi),e(pi,ju),e(is,Cu),e(is,hi),e(hi,Pu),e(is,qu),e(Se,Au),e(Se,bl),e(bl,Iu),e(Se,Nu),w(nn,Se,null),u(s,gp,m),u(s,ls,m),e(ls,Gs),e(Gs,Tl),w(rn,Tl,null),e(ls,Lu),e(ls,kl),e(kl,Su),u(s,_p,m),u(s,_t,m),w(an,_t,null),e(_t,Ou),e(_t,wl),e(wl,Wu),e(_t,Ru),e(_t,Xs),e(Xs,ui),e(ui,Hu),e(Xs,Qu),e(Xs,fi),e(fi,Uu),e(Xs,Vu),e(_t,Ju),e(_t,ln),e(ln,Ku),e(ln,mi),e(mi,Gu),e(ln,Xu),u(s,vp,m),u(s,ds,m),e(ds,Ys),e(Ys,$l),w(dn,$l,null),e(ds,Yu),e(ds,Dl),e(Dl,Zu),u(s,bp,m),u(s,vt,m),w(cn,vt,null),e(vt,ef),e(vt,pn),e(pn,tf),e(pn,Fl),e(Fl,sf),e(pn,of),e(vt,nf),e(vt,Zs),e(Zs,gi),e(gi,rf),e(Zs,af),e(Zs,_i),e(_i,lf),e(Zs,df),e(vt,cf),e(vt,hn),e(hn,pf),e(hn,vi),e(vi,hf),e(hn,uf),u(s,Tp,m),u(s,cs,m),e(cs,eo),e(eo,yl),w(un,yl,null),e(cs,ff),e(cs,Bl),e(Bl,mf),u(s,kp,m),u(s,Oe,m),w(fn,Oe,null),e(Oe,gf),e(Oe,El),e(El,_f),e(Oe,vf),e(Oe,mn),e(mn,bf),e(mn,bi),e(bi,Tf),e(mn,kf),e(Oe,wf),e(Oe,gn),e(gn,$f),e(gn,_n),e(_n,Df),e(gn,Ff),e(Oe,yf),e(Oe,Ve),w(vn,Ve,null),e(Ve,Bf),e(Ve,ps),e(ps,Ef),e(ps,Ti),e(Ti,Mf),e(ps,zf),e(ps,Ml),e(Ml,xf),e(ps,jf),e(Ve,Cf),w(to,Ve,null),e(Ve,Pf),e(Ve,zl),e(zl,qf),e(Ve,Af),w(bn,Ve,null),u(s,wp,m),u(s,hs,m),e(hs,so),e(so,xl),w(Tn,xl,null),e(hs,If),e(hs,jl),e(jl,Nf),u(s,$p,m),u(s,We,m),w(kn,We,null),e(We,Lf),e(We,wn),e(wn,Sf),e(wn,Cl),e(Cl,Of),e(wn,Wf),e(We,Rf),e(We,$n),e($n,Hf),e($n,ki),e(ki,Qf),e($n,Uf),e(We,Vf),e(We,Dn),e(Dn,Jf),e(Dn,Fn),e(Fn,Kf),e(Dn,Gf),e(We,Xf),e(We,Je),w(yn,Je,null),e(Je,Yf),e(Je,us),e(us,Zf),e(us,wi),e(wi,em),e(us,tm),e(us,Pl),e(Pl,sm),e(us,om),e(Je,nm),w(oo,Je,null),e(Je,rm),e(Je,ql),e(ql,am),e(Je,im),w(Bn,Je,null),u(s,Dp,m),u(s,fs,m),e(fs,no),e(no,Al),w(En,Al,null),e(fs,lm),e(fs,Il),e(Il,dm),u(s,Fp,m),u(s,Re,m),w(Mn,Re,null),e(Re,cm),e(Re,Nl),e(Nl,pm),e(Re,hm),e(Re,zn),e(zn,um),e(zn,$i),e($i,fm),e(zn,mm),e(Re,gm),e(Re,xn),e(xn,_m),e(xn,jn),e(jn,vm),e(xn,bm),e(Re,Tm),e(Re,Ce),w(Cn,Ce,null),e(Ce,km),e(Ce,ms),e(ms,wm),e(ms,Di),e(Di,$m),e(ms,Dm),e(ms,Ll),e(Ll,Fm),e(ms,ym),e(Ce,Bm),w(ro,Ce,null),e(Ce,Em),e(Ce,Sl),e(Sl,Mm),e(Ce,zm),w(Pn,Ce,null),e(Ce,xm),e(Ce,Ol),e(Ol,jm),e(Ce,Cm),w(qn,Ce,null),u(s,yp,m),u(s,gs,m),e(gs,ao),e(ao,Wl),w(An,Wl,null),e(gs,Pm),e(gs,Rl),e(Rl,qm),u(s,Bp,m),u(s,He,m),w(In,He,null),e(He,Am),e(He,Hl),e(Hl,Im),e(He,Nm),e(He,Nn),e(Nn,Lm),e(Nn,Fi),e(Fi,Sm),e(Nn,Om),e(He,Wm),e(He,Ln),e(Ln,Rm),e(Ln,Sn),e(Sn,Hm),e(Ln,Qm),e(He,Um),e(He,Ke),w(On,Ke,null),e(Ke,Vm),e(Ke,_s),e(_s,Jm),e(_s,yi),e(yi,Km),e(_s,Gm),e(_s,Ql),e(Ql,Xm),e(_s,Ym),e(Ke,Zm),w(io,Ke,null),e(Ke,eg),e(Ke,Ul),e(Ul,tg),e(Ke,sg),w(Wn,Ke,null),u(s,Ep,m),u(s,vs,m),e(vs,lo),e(lo,Vl),w(Rn,Vl,null),e(vs,og),e(vs,Jl),e(Jl,ng),u(s,Mp,m),u(s,Qe,m),w(Hn,Qe,null),e(Qe,rg),e(Qe,Kl),e(Kl,ag),e(Qe,ig),e(Qe,Qn),e(Qn,lg),e(Qn,Bi),e(Bi,dg),e(Qn,cg),e(Qe,pg),e(Qe,Un),e(Un,hg),e(Un,Vn),e(Vn,ug),e(Un,fg),e(Qe,mg),e(Qe,Ge),w(Jn,Ge,null),e(Ge,gg),e(Ge,bs),e(bs,_g),e(bs,Ei),e(Ei,vg),e(bs,bg),e(bs,Gl),e(Gl,Tg),e(bs,kg),e(Ge,wg),w(co,Ge,null),e(Ge,$g),e(Ge,Xl),e(Xl,Dg),e(Ge,Fg),w(Kn,Ge,null),u(s,zp,m),u(s,Ts,m),e(Ts,po),e(po,Yl),w(Gn,Yl,null),e(Ts,yg),e(Ts,Zl),e(Zl,Bg),u(s,xp,m),u(s,Ue,m),w(Xn,Ue,null),e(Ue,Eg),e(Ue,ks),e(ks,Mg),e(ks,ed),e(ed,zg),e(ks,xg),e(ks,td),e(td,jg),e(ks,Cg),e(Ue,Pg),e(Ue,Yn),e(Yn,qg),e(Yn,Mi),e(Mi,Ag),e(Yn,Ig),e(Ue,Ng),e(Ue,Zn),e(Zn,Lg),e(Zn,er),e(er,Sg),e(Zn,Og),e(Ue,Wg),e(Ue,Xe),w(tr,Xe,null),e(Xe,Rg),e(Xe,ws),e(ws,Hg),e(ws,zi),e(zi,Qg),e(ws,Ug),e(ws,sd),e(sd,Vg),e(ws,Jg),e(Xe,Kg),w(ho,Xe,null),e(Xe,Gg),e(Xe,od),e(od,Xg),e(Xe,Yg),w(sr,Xe,null),u(s,jp,m),u(s,$s,m),e($s,uo),e(uo,nd),w(or,nd,null),e($s,Zg),e($s,rd),e(rd,e_),u(s,Cp,m),u(s,Pe,m),w(nr,Pe,null),e(Pe,t_),e(Pe,ad),e(ad,s_),e(Pe,o_),e(Pe,rr),e(rr,n_),e(rr,xi),e(xi,r_),e(rr,a_),e(Pe,i_),e(Pe,ar),e(ar,l_),e(ar,ir),e(ir,d_),e(ar,c_),e(Pe,p_),w(fo,Pe,null),e(Pe,h_),e(Pe,Ye),w(lr,Ye,null),e(Ye,u_),e(Ye,Ds),e(Ds,f_),e(Ds,ji),e(ji,m_),e(Ds,g_),e(Ds,id),e(id,__),e(Ds,v_),e(Ye,b_),w(mo,Ye,null),e(Ye,T_),e(Ye,ld),e(ld,k_),e(Ye,w_),w(dr,Ye,null),u(s,Pp,m),u(s,Fs,m),e(Fs,go),e(go,dd),w(cr,dd,null),e(Fs,$_),e(Fs,cd),e(cd,D_),u(s,qp,m),u(s,qe,m),w(pr,qe,null),e(qe,F_),e(qe,hr),e(hr,y_),e(hr,pd),e(pd,B_),e(hr,E_),e(qe,M_),e(qe,ur),e(ur,z_),e(ur,Ci),e(Ci,x_),e(ur,j_),e(qe,C_),e(qe,fr),e(fr,P_),e(fr,mr),e(mr,q_),e(fr,A_),e(qe,I_),w(_o,qe,null),e(qe,N_),e(qe,Ze),w(gr,Ze,null),e(Ze,L_),e(Ze,ys),e(ys,S_),e(ys,Pi),e(Pi,O_),e(ys,W_),e(ys,hd),e(hd,R_),e(ys,H_),e(Ze,Q_),w(vo,Ze,null),e(Ze,U_),e(Ze,ud),e(ud,V_),e(Ze,J_),w(_r,Ze,null),u(s,Ap,m),u(s,Bs,m),e(Bs,bo),e(bo,fd),w(vr,fd,null),e(Bs,K_),e(Bs,md),e(md,G_),u(s,Ip,m),u(s,Ae,m),w(br,Ae,null),e(Ae,X_),e(Ae,gd),e(gd,Y_),e(Ae,Z_),e(Ae,Tr),e(Tr,ev),e(Tr,qi),e(qi,tv),e(Tr,sv),e(Ae,ov),e(Ae,kr),e(kr,nv),e(kr,wr),e(wr,rv),e(kr,av),e(Ae,iv),w(To,Ae,null),e(Ae,lv),e(Ae,et),w($r,et,null),e(et,dv),e(et,Es),e(Es,cv),e(Es,Ai),e(Ai,pv),e(Es,hv),e(Es,_d),e(_d,uv),e(Es,fv),e(et,mv),w(ko,et,null),e(et,gv),e(et,vd),e(vd,_v),e(et,vv),w(Dr,et,null),u(s,Np,m),u(s,Ms,m),e(Ms,wo),e(wo,bd),w(Fr,bd,null),e(Ms,bv),e(Ms,Td),e(Td,Tv),u(s,Lp,m),u(s,Ie,m),w(yr,Ie,null),e(Ie,kv),e(Ie,kd),e(kd,wv),e(Ie,$v),e(Ie,Br),e(Br,Dv),e(Br,Ii),e(Ii,Fv),e(Br,yv),e(Ie,Bv),e(Ie,Er),e(Er,Ev),e(Er,Mr),e(Mr,Mv),e(Er,zv),e(Ie,xv),w($o,Ie,null),e(Ie,jv),e(Ie,tt),w(zr,tt,null),e(tt,Cv),e(tt,zs),e(zs,Pv),e(zs,Ni),e(Ni,qv),e(zs,Av),e(zs,wd),e(wd,Iv),e(zs,Nv),e(tt,Lv),w(Do,tt,null),e(tt,Sv),e(tt,$d),e($d,Ov),e(tt,Wv),w(xr,tt,null),u(s,Sp,m),u(s,xs,m),e(xs,Fo),e(Fo,Dd),w(jr,Dd,null),e(xs,Rv),e(xs,Fd),e(Fd,Hv),u(s,Op,m),u(s,Ne,m),w(Cr,Ne,null),e(Ne,Qv),e(Ne,yd),e(yd,Uv),e(Ne,Vv),e(Ne,Pr),e(Pr,Jv),e(Pr,Li),e(Li,Kv),e(Pr,Gv),e(Ne,Xv),e(Ne,qr),e(qr,Yv),e(qr,Ar),e(Ar,Zv),e(qr,eb),e(Ne,tb),w(yo,Ne,null),e(Ne,sb),e(Ne,st),w(Ir,st,null),e(st,ob),e(st,js),e(js,nb),e(js,Si),e(Si,rb),e(js,ab),e(js,Bd),e(Bd,ib),e(js,lb),e(st,db),w(Bo,st,null),e(st,cb),e(st,Ed),e(Ed,pb),e(st,hb),w(Nr,st,null),u(s,Wp,m),u(s,Cs,m),e(Cs,Eo),e(Eo,Md),w(Lr,Md,null),e(Cs,ub),e(Cs,zd),e(zd,fb),u(s,Rp,m),u(s,Le,m),w(Sr,Le,null),e(Le,mb),e(Le,Ps),e(Ps,gb),e(Ps,xd),e(xd,_b),e(Ps,vb),e(Ps,jd),e(jd,bb),e(Ps,Tb),e(Le,kb),e(Le,Or),e(Or,wb),e(Or,Oi),e(Oi,$b),e(Or,Db),e(Le,Fb),e(Le,Wr),e(Wr,yb),e(Wr,Rr),e(Rr,Bb),e(Wr,Eb),e(Le,Mb),w(Mo,Le,null),e(Le,zb),e(Le,ot),w(Hr,ot,null),e(ot,xb),e(ot,qs),e(qs,jb),e(qs,Wi),e(Wi,Cb),e(qs,Pb),e(qs,Cd),e(Cd,qb),e(qs,Ab),e(ot,Ib),w(zo,ot,null),e(ot,Nb),e(ot,Pd),e(Pd,Lb),e(ot,Sb),w(Qr,ot,null),u(s,Hp,m),u(s,As,m),e(As,xo),e(xo,qd),w(Ur,qd,null),e(As,Ob),e(As,Ad),e(Ad,Wb),u(s,Qp,m),u(s,Be,m),w(Vr,Be,null),e(Be,Rb),e(Be,Id),e(Id,Hb),e(Be,Qb),e(Be,Jr),e(Jr,Ub),e(Jr,Ri),e(Ri,Vb),e(Jr,Jb),e(Be,Kb),e(Be,Kr),e(Kr,Gb),e(Kr,Gr),e(Gr,Xb),e(Kr,Yb),e(Be,Zb),e(Be,Nd),e(Nd,e1),e(Be,t1),e(Be,Bt),e(Bt,Ld),e(Ld,Xr),e(Xr,s1),e(Bt,o1),e(Bt,Sd),e(Sd,Yr),e(Yr,n1),e(Bt,r1),e(Bt,Od),e(Od,Zr),e(Zr,a1),e(Bt,i1),e(Bt,Wd),e(Wd,ea),e(ea,l1),e(Be,d1),e(Be,nt),w(ta,nt,null),e(nt,c1),e(nt,Is),e(Is,p1),e(Is,Rd),e(Rd,h1),e(Is,u1),e(Is,Hd),e(Hd,f1),e(Is,m1),e(nt,g1),w(jo,nt,null),e(nt,_1),e(nt,Qd),e(Qd,v1),e(nt,b1),w(sa,nt,null),u(s,Up,m),u(s,Ns,m),e(Ns,Co),e(Co,Ud),w(oa,Ud,null),e(Ns,T1),e(Ns,Vd),e(Vd,k1),u(s,Vp,m),u(s,Ee,m),w(na,Ee,null),e(Ee,w1),e(Ee,ra),e(ra,$1),e(ra,Jd),e(Jd,D1),e(ra,F1),e(Ee,y1),e(Ee,aa),e(aa,B1),e(aa,Hi),e(Hi,E1),e(aa,M1),e(Ee,z1),e(Ee,ia),e(ia,x1),e(ia,la),e(la,j1),e(ia,C1),e(Ee,P1),e(Ee,Kd),e(Kd,q1),e(Ee,A1),e(Ee,Et),e(Et,Gd),e(Gd,da),e(da,I1),e(Et,N1),e(Et,Xd),e(Xd,ca),e(ca,L1),e(Et,S1),e(Et,Yd),e(Yd,pa),e(pa,O1),e(Et,W1),e(Et,Zd),e(Zd,ha),e(ha,R1),e(Ee,H1),e(Ee,rt),w(ua,rt,null),e(rt,Q1),e(rt,Ls),e(Ls,U1),e(Ls,ec),e(ec,V1),e(Ls,J1),e(Ls,tc),e(tc,K1),e(Ls,G1),e(rt,X1),w(Po,rt,null),e(rt,Y1),e(rt,sc),e(sc,Z1),e(rt,eT),w(fa,rt,null),u(s,Jp,m),u(s,Ss,m),e(Ss,qo),e(qo,oc),w(ma,oc,null),e(Ss,tT),e(Ss,nc),e(nc,sT),u(s,Kp,m),u(s,Me,m),w(ga,Me,null),e(Me,oT),e(Me,rc),e(rc,nT),e(Me,rT),e(Me,_a),e(_a,aT),e(_a,Qi),e(Qi,iT),e(_a,lT),e(Me,dT),e(Me,va),e(va,cT),e(va,ba),e(ba,pT),e(va,hT),e(Me,uT),e(Me,ac),e(ac,fT),e(Me,mT),e(Me,Mt),e(Mt,ic),e(ic,Ta),e(Ta,gT),e(Mt,_T),e(Mt,lc),e(lc,ka),e(ka,vT),e(Mt,bT),e(Mt,dc),e(dc,wa),e(wa,TT),e(Mt,kT),e(Mt,cc),e(cc,$a),e($a,wT),e(Me,$T),e(Me,at),w(Da,at,null),e(at,DT),e(at,Os),e(Os,FT),e(Os,pc),e(pc,yT),e(Os,BT),e(Os,hc),e(hc,ET),e(Os,MT),e(at,zT),w(Ao,at,null),e(at,xT),e(at,uc),e(uc,jT),e(at,CT),w(Fa,at,null),u(s,Gp,m),u(s,Ws,m),e(Ws,Io),e(Io,fc),w(ya,fc,null),e(Ws,PT),e(Ws,mc),e(mc,qT),u(s,Xp,m),u(s,ze,m),w(Ba,ze,null),e(ze,AT),e(ze,gc),e(gc,IT),e(ze,NT),e(ze,Ea),e(Ea,LT),e(Ea,Ui),e(Ui,ST),e(Ea,OT),e(ze,WT),e(ze,Ma),e(Ma,RT),e(Ma,za),e(za,HT),e(Ma,QT),e(ze,UT),e(ze,_c),e(_c,VT),e(ze,JT),e(ze,zt),e(zt,vc),e(vc,xa),e(xa,KT),e(zt,GT),e(zt,bc),e(bc,ja),e(ja,XT),e(zt,YT),e(zt,Tc),e(Tc,Ca),e(Ca,ZT),e(zt,ek),e(zt,kc),e(kc,Pa),e(Pa,tk),e(ze,sk),e(ze,it),w(qa,it,null),e(it,ok),e(it,Rs),e(Rs,nk),e(Rs,wc),e(wc,rk),e(Rs,ak),e(Rs,$c),e($c,ik),e(Rs,lk),e(it,dk),w(No,it,null),e(it,ck),e(it,Dc),e(Dc,pk),e(it,hk),w(Aa,it,null),u(s,Yp,m),u(s,Hs,m),e(Hs,Lo),e(Lo,Fc),w(Ia,Fc,null),e(Hs,uk),e(Hs,yc),e(yc,fk),u(s,Zp,m),u(s,xe,m),w(Na,xe,null),e(xe,mk),e(xe,Bc),e(Bc,gk),e(xe,_k),e(xe,La),e(La,vk),e(La,Vi),e(Vi,bk),e(La,Tk),e(xe,kk),e(xe,Sa),e(Sa,wk),e(Sa,Oa),e(Oa,$k),e(Sa,Dk),e(xe,Fk),e(xe,Ec),e(Ec,yk),e(xe,Bk),e(xe,xt),e(xt,Mc),e(Mc,Wa),e(Wa,Ek),e(xt,Mk),e(xt,zc),e(zc,Ra),e(Ra,zk),e(xt,xk),e(xt,xc),e(xc,Ha),e(Ha,jk),e(xt,Ck),e(xt,jc),e(jc,Qa),e(Qa,Pk),e(xe,qk),e(xe,lt),w(Ua,lt,null),e(lt,Ak),e(lt,Qs),e(Qs,Ik),e(Qs,Cc),e(Cc,Nk),e(Qs,Lk),e(Qs,Pc),e(Pc,Sk),e(Qs,Ok),e(lt,Wk),w(So,lt,null),e(lt,Rk),e(lt,qc),e(qc,Hk),e(lt,Qk),w(Va,lt,null),u(s,eh,m),u(s,Us,m),e(Us,Oo),e(Oo,Ac),w(Ja,Ac,null),e(Us,Uk),e(Us,Ic),e(Ic,Vk),u(s,th,m),u(s,je,m),w(Ka,je,null),e(je,Jk),e(je,Vs),e(Vs,Kk),e(Vs,Nc),e(Nc,Gk),e(Vs,Xk),e(Vs,Lc),e(Lc,Yk),e(Vs,Zk),e(je,ew),e(je,Ga),e(Ga,tw),e(Ga,Ji),e(Ji,sw),e(Ga,ow),e(je,nw),e(je,Xa),e(Xa,rw),e(Xa,Ya),e(Ya,aw),e(Xa,iw),e(je,lw),e(je,Sc),e(Sc,dw),e(je,cw),e(je,jt),e(jt,Oc),e(Oc,Za),e(Za,pw),e(jt,hw),e(jt,Wc),e(Wc,ei),e(ei,uw),e(jt,fw),e(jt,Rc),e(Rc,ti),e(ti,mw),e(jt,gw),e(jt,Hc),e(Hc,si),e(si,_w),e(je,vw),e(je,dt),w(oi,dt,null),e(dt,bw),e(dt,Js),e(Js,Tw),e(Js,Qc),e(Qc,kw),e(Js,ww),e(Js,Uc),e(Uc,$w),e(Js,Dw),e(dt,Fw),w(Wo,dt,null),e(dt,yw),e(dt,Vc),e(Vc,Bw),e(dt,Ew),w(ni,dt,null),sh=!0},p(s,[m]){const ri={};m&2&&(ri.$$scope={dirty:m,ctx:s}),to.$set(ri);const Jc={};m&2&&(Jc.$$scope={dirty:m,ctx:s}),oo.$set(Jc);const Kc={};m&2&&(Kc.$$scope={dirty:m,ctx:s}),ro.$set(Kc);const Gc={};m&2&&(Gc.$$scope={dirty:m,ctx:s}),io.$set(Gc);const ai={};m&2&&(ai.$$scope={dirty:m,ctx:s}),co.$set(ai);const Xc={};m&2&&(Xc.$$scope={dirty:m,ctx:s}),ho.$set(Xc);const Yc={};m&2&&(Yc.$$scope={dirty:m,ctx:s}),fo.$set(Yc);const Zc={};m&2&&(Zc.$$scope={dirty:m,ctx:s}),mo.$set(Zc);const Ct={};m&2&&(Ct.$$scope={dirty:m,ctx:s}),_o.$set(Ct);const ep={};m&2&&(ep.$$scope={dirty:m,ctx:s}),vo.$set(ep);const tp={};m&2&&(tp.$$scope={dirty:m,ctx:s}),To.$set(tp);const sp={};m&2&&(sp.$$scope={dirty:m,ctx:s}),ko.$set(sp);const op={};m&2&&(op.$$scope={dirty:m,ctx:s}),$o.$set(op);const np={};m&2&&(np.$$scope={dirty:m,ctx:s}),Do.$set(np);const rp={};m&2&&(rp.$$scope={dirty:m,ctx:s}),yo.$set(rp);const ap={};m&2&&(ap.$$scope={dirty:m,ctx:s}),Bo.$set(ap);const ii={};m&2&&(ii.$$scope={dirty:m,ctx:s}),Mo.$set(ii);const Pt={};m&2&&(Pt.$$scope={dirty:m,ctx:s}),zo.$set(Pt);const ip={};m&2&&(ip.$$scope={dirty:m,ctx:s}),jo.$set(ip);const lp={};m&2&&(lp.$$scope={dirty:m,ctx:s}),Po.$set(lp);const dp={};m&2&&(dp.$$scope={dirty:m,ctx:s}),Ao.$set(dp);const li={};m&2&&(li.$$scope={dirty:m,ctx:s}),No.$set(li);const cp={};m&2&&(cp.$$scope={dirty:m,ctx:s}),So.$set(cp);const qt={};m&2&&(qt.$$scope={dirty:m,ctx:s}),Wo.$set(qt)},i(s){sh||($(_.$$.fragment,s),$(X.$$.fragment,s),$(tn.$$.fragment,s),$(sn.$$.fragment,s),$(nn.$$.fragment,s),$(rn.$$.fragment,s),$(an.$$.fragment,s),$(dn.$$.fragment,s),$(cn.$$.fragment,s),$(un.$$.fragment,s),$(fn.$$.fragment,s),$(vn.$$.fragment,s),$(to.$$.fragment,s),$(bn.$$.fragment,s),$(Tn.$$.fragment,s),$(kn.$$.fragment,s),$(yn.$$.fragment,s),$(oo.$$.fragment,s),$(Bn.$$.fragment,s),$(En.$$.fragment,s),$(Mn.$$.fragment,s),$(Cn.$$.fragment,s),$(ro.$$.fragment,s),$(Pn.$$.fragment,s),$(qn.$$.fragment,s),$(An.$$.fragment,s),$(In.$$.fragment,s),$(On.$$.fragment,s),$(io.$$.fragment,s),$(Wn.$$.fragment,s),$(Rn.$$.fragment,s),$(Hn.$$.fragment,s),$(Jn.$$.fragment,s),$(co.$$.fragment,s),$(Kn.$$.fragment,s),$(Gn.$$.fragment,s),$(Xn.$$.fragment,s),$(tr.$$.fragment,s),$(ho.$$.fragment,s),$(sr.$$.fragment,s),$(or.$$.fragment,s),$(nr.$$.fragment,s),$(fo.$$.fragment,s),$(lr.$$.fragment,s),$(mo.$$.fragment,s),$(dr.$$.fragment,s),$(cr.$$.fragment,s),$(pr.$$.fragment,s),$(_o.$$.fragment,s),$(gr.$$.fragment,s),$(vo.$$.fragment,s),$(_r.$$.fragment,s),$(vr.$$.fragment,s),$(br.$$.fragment,s),$(To.$$.fragment,s),$($r.$$.fragment,s),$(ko.$$.fragment,s),$(Dr.$$.fragment,s),$(Fr.$$.fragment,s),$(yr.$$.fragment,s),$($o.$$.fragment,s),$(zr.$$.fragment,s),$(Do.$$.fragment,s),$(xr.$$.fragment,s),$(jr.$$.fragment,s),$(Cr.$$.fragment,s),$(yo.$$.fragment,s),$(Ir.$$.fragment,s),$(Bo.$$.fragment,s),$(Nr.$$.fragment,s),$(Lr.$$.fragment,s),$(Sr.$$.fragment,s),$(Mo.$$.fragment,s),$(Hr.$$.fragment,s),$(zo.$$.fragment,s),$(Qr.$$.fragment,s),$(Ur.$$.fragment,s),$(Vr.$$.fragment,s),$(ta.$$.fragment,s),$(jo.$$.fragment,s),$(sa.$$.fragment,s),$(oa.$$.fragment,s),$(na.$$.fragment,s),$(ua.$$.fragment,s),$(Po.$$.fragment,s),$(fa.$$.fragment,s),$(ma.$$.fragment,s),$(ga.$$.fragment,s),$(Da.$$.fragment,s),$(Ao.$$.fragment,s),$(Fa.$$.fragment,s),$(ya.$$.fragment,s),$(Ba.$$.fragment,s),$(qa.$$.fragment,s),$(No.$$.fragment,s),$(Aa.$$.fragment,s),$(Ia.$$.fragment,s),$(Na.$$.fragment,s),$(Ua.$$.fragment,s),$(So.$$.fragment,s),$(Va.$$.fragment,s),$(Ja.$$.fragment,s),$(Ka.$$.fragment,s),$(oi.$$.fragment,s),$(Wo.$$.fragment,s),$(ni.$$.fragment,s),sh=!0)},o(s){D(_.$$.fragment,s),D(X.$$.fragment,s),D(tn.$$.fragment,s),D(sn.$$.fragment,s),D(nn.$$.fragment,s),D(rn.$$.fragment,s),D(an.$$.fragment,s),D(dn.$$.fragment,s),D(cn.$$.fragment,s),D(un.$$.fragment,s),D(fn.$$.fragment,s),D(vn.$$.fragment,s),D(to.$$.fragment,s),D(bn.$$.fragment,s),D(Tn.$$.fragment,s),D(kn.$$.fragment,s),D(yn.$$.fragment,s),D(oo.$$.fragment,s),D(Bn.$$.fragment,s),D(En.$$.fragment,s),D(Mn.$$.fragment,s),D(Cn.$$.fragment,s),D(ro.$$.fragment,s),D(Pn.$$.fragment,s),D(qn.$$.fragment,s),D(An.$$.fragment,s),D(In.$$.fragment,s),D(On.$$.fragment,s),D(io.$$.fragment,s),D(Wn.$$.fragment,s),D(Rn.$$.fragment,s),D(Hn.$$.fragment,s),D(Jn.$$.fragment,s),D(co.$$.fragment,s),D(Kn.$$.fragment,s),D(Gn.$$.fragment,s),D(Xn.$$.fragment,s),D(tr.$$.fragment,s),D(ho.$$.fragment,s),D(sr.$$.fragment,s),D(or.$$.fragment,s),D(nr.$$.fragment,s),D(fo.$$.fragment,s),D(lr.$$.fragment,s),D(mo.$$.fragment,s),D(dr.$$.fragment,s),D(cr.$$.fragment,s),D(pr.$$.fragment,s),D(_o.$$.fragment,s),D(gr.$$.fragment,s),D(vo.$$.fragment,s),D(_r.$$.fragment,s),D(vr.$$.fragment,s),D(br.$$.fragment,s),D(To.$$.fragment,s),D($r.$$.fragment,s),D(ko.$$.fragment,s),D(Dr.$$.fragment,s),D(Fr.$$.fragment,s),D(yr.$$.fragment,s),D($o.$$.fragment,s),D(zr.$$.fragment,s),D(Do.$$.fragment,s),D(xr.$$.fragment,s),D(jr.$$.fragment,s),D(Cr.$$.fragment,s),D(yo.$$.fragment,s),D(Ir.$$.fragment,s),D(Bo.$$.fragment,s),D(Nr.$$.fragment,s),D(Lr.$$.fragment,s),D(Sr.$$.fragment,s),D(Mo.$$.fragment,s),D(Hr.$$.fragment,s),D(zo.$$.fragment,s),D(Qr.$$.fragment,s),D(Ur.$$.fragment,s),D(Vr.$$.fragment,s),D(ta.$$.fragment,s),D(jo.$$.fragment,s),D(sa.$$.fragment,s),D(oa.$$.fragment,s),D(na.$$.fragment,s),D(ua.$$.fragment,s),D(Po.$$.fragment,s),D(fa.$$.fragment,s),D(ma.$$.fragment,s),D(ga.$$.fragment,s),D(Da.$$.fragment,s),D(Ao.$$.fragment,s),D(Fa.$$.fragment,s),D(ya.$$.fragment,s),D(Ba.$$.fragment,s),D(qa.$$.fragment,s),D(No.$$.fragment,s),D(Aa.$$.fragment,s),D(Ia.$$.fragment,s),D(Na.$$.fragment,s),D(Ua.$$.fragment,s),D(So.$$.fragment,s),D(Va.$$.fragment,s),D(Ja.$$.fragment,s),D(Ka.$$.fragment,s),D(oi.$$.fragment,s),D(Wo.$$.fragment,s),D(ni.$$.fragment,s),sh=!1},d(s){t(p),s&&t(y),s&&t(g),F(_),s&&t(V),s&&t(M),F(X),s&&t(re),s&&t(N),s&&t(ae),s&&t(ee),s&&t(ie),s&&t(L),s&&t(P),s&&t(te),s&&t(le),s&&t(h),s&&t(up),s&&t(bt),s&&t(fp),s&&t(as),F(tn),s&&t(mp),s&&t(Se),F(sn),F(nn),s&&t(gp),s&&t(ls),F(rn),s&&t(_p),s&&t(_t),F(an),s&&t(vp),s&&t(ds),F(dn),s&&t(bp),s&&t(vt),F(cn),s&&t(Tp),s&&t(cs),F(un),s&&t(kp),s&&t(Oe),F(fn),F(vn),F(to),F(bn),s&&t(wp),s&&t(hs),F(Tn),s&&t($p),s&&t(We),F(kn),F(yn),F(oo),F(Bn),s&&t(Dp),s&&t(fs),F(En),s&&t(Fp),s&&t(Re),F(Mn),F(Cn),F(ro),F(Pn),F(qn),s&&t(yp),s&&t(gs),F(An),s&&t(Bp),s&&t(He),F(In),F(On),F(io),F(Wn),s&&t(Ep),s&&t(vs),F(Rn),s&&t(Mp),s&&t(Qe),F(Hn),F(Jn),F(co),F(Kn),s&&t(zp),s&&t(Ts),F(Gn),s&&t(xp),s&&t(Ue),F(Xn),F(tr),F(ho),F(sr),s&&t(jp),s&&t($s),F(or),s&&t(Cp),s&&t(Pe),F(nr),F(fo),F(lr),F(mo),F(dr),s&&t(Pp),s&&t(Fs),F(cr),s&&t(qp),s&&t(qe),F(pr),F(_o),F(gr),F(vo),F(_r),s&&t(Ap),s&&t(Bs),F(vr),s&&t(Ip),s&&t(Ae),F(br),F(To),F($r),F(ko),F(Dr),s&&t(Np),s&&t(Ms),F(Fr),s&&t(Lp),s&&t(Ie),F(yr),F($o),F(zr),F(Do),F(xr),s&&t(Sp),s&&t(xs),F(jr),s&&t(Op),s&&t(Ne),F(Cr),F(yo),F(Ir),F(Bo),F(Nr),s&&t(Wp),s&&t(Cs),F(Lr),s&&t(Rp),s&&t(Le),F(Sr),F(Mo),F(Hr),F(zo),F(Qr),s&&t(Hp),s&&t(As),F(Ur),s&&t(Qp),s&&t(Be),F(Vr),F(ta),F(jo),F(sa),s&&t(Up),s&&t(Ns),F(oa),s&&t(Vp),s&&t(Ee),F(na),F(ua),F(Po),F(fa),s&&t(Jp),s&&t(Ss),F(ma),s&&t(Kp),s&&t(Me),F(ga),F(Da),F(Ao),F(Fa),s&&t(Gp),s&&t(Ws),F(ya),s&&t(Xp),s&&t(ze),F(Ba),F(qa),F(No),F(Aa),s&&t(Yp),s&&t(Hs),F(Ia),s&&t(Zp),s&&t(xe),F(Na),F(Ua),F(So),F(Va),s&&t(eh),s&&t(Us),F(Ja),s&&t(th),s&&t(je),F(Ka),F(oi),F(Wo),F(ni)}}}const Py={local:"distilbert",sections:[{local:"overview",title:"Overview"},{local:"transformers.DistilBertConfig",title:"DistilBertConfig"},{local:"transformers.DistilBertTokenizer",title:"DistilBertTokenizer"},{local:"transformers.DistilBertTokenizerFast",title:"DistilBertTokenizerFast"},{local:"transformers.DistilBertModel",title:"DistilBertModel"},{local:"transformers.DistilBertForMaskedLM",title:"DistilBertForMaskedLM"},{local:"transformers.DistilBertForSequenceClassification",title:"DistilBertForSequenceClassification"},{local:"transformers.DistilBertForMultipleChoice",title:"DistilBertForMultipleChoice"},{local:"transformers.DistilBertForTokenClassification",title:"DistilBertForTokenClassification"},{local:"transformers.DistilBertForQuestionAnswering",title:"DistilBertForQuestionAnswering"},{local:"transformers.TFDistilBertModel",title:"TFDistilBertModel"},{local:"transformers.TFDistilBertForMaskedLM",title:"TFDistilBertForMaskedLM"},{local:"transformers.TFDistilBertForSequenceClassification",title:"TFDistilBertForSequenceClassification"},{local:"transformers.TFDistilBertForMultipleChoice",title:"TFDistilBertForMultipleChoice"},{local:"transformers.TFDistilBertForTokenClassification",title:"TFDistilBertForTokenClassification"},{local:"transformers.TFDistilBertForQuestionAnswering",title:"TFDistilBertForQuestionAnswering"},{local:"transformers.FlaxDistilBertModel",title:"FlaxDistilBertModel"},{local:"transformers.FlaxDistilBertForMaskedLM",title:"FlaxDistilBertForMaskedLM"},{local:"transformers.FlaxDistilBertForSequenceClassification",title:"FlaxDistilBertForSequenceClassification"},{local:"transformers.FlaxDistilBertForMultipleChoice",title:"FlaxDistilBertForMultipleChoice"},{local:"transformers.FlaxDistilBertForTokenClassification",title:"FlaxDistilBertForTokenClassification"},{local:"transformers.FlaxDistilBertForQuestionAnswering",title:"FlaxDistilBertForQuestionAnswering"}],title:"DistilBERT"};function qy(j,p,y){let{fw:g}=p;return j.$$set=v=>{"fw"in v&&y(0,g=v.fw)},[g]}class Wy extends ry{constructor(p){super();ay(this,p,qy,Cy,iy,{fw:0})}}export{Wy as default,Py as metadata};
