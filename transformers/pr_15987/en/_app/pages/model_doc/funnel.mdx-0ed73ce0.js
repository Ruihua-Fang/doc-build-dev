import{S as b$,i as y$,s as $$,e as r,k as l,w as k,t,M as E$,c as a,d as n,m as d,a as i,x as w,h as o,b as c,F as e,g as h,y as b,q as y,o as $,B as E}from"../../chunks/vendor-4833417e.js";import{T as ze}from"../../chunks/Tip-fffd6df1.js";import{D as X}from"../../chunks/Docstring-4f315ed9.js";import{C as Oe}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as qe}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function M$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function z$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function q$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function P$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function C$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function j$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function x$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function L$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function A$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function D$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function I$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function S$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function N$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function O$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function B$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function W$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function R$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function Q$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function H$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function V$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function U$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function Y$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function G$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),T=r("li"),F=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=r("p"),J=t("This second option is useful when using "),I=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),R=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),N=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),O=l(),A=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var v=i(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=a(u,"UL",{});var K=i(g);T=a(K,"LI",{});var Fe=i(T);F=o(Fe,"having all inputs as keyword arguments (like PyTorch models), or"),Fe.forEach(n),_=d(K),z=a(K,"LI",{});var we=i(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),I=a(D,"CODE",{});var Te=i(I);ne=o(Te,"tf.keras.Model.fit"),Te.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=a(D,"CODE",{});var be=i(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var ye=i(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var V=i(j);oe=o(V,"a single Tensor with "),R=a(V,"CODE",{});var $e=i(R);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),N=a(V,"CODE",{});var ve=i(N);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(x),C=a(x,"LI",{});var U=i(C);fe=o(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(U," or "),Q=a(U,"CODE",{});var ke=i(Q);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),U.forEach(n),O=d(x),A=a(x,"LI",{});var _e=i(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=a(_e,"CODE",{});var Me=i(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,T),e(T,F),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,J),e(q,I),e(I,ne),e(q,ue),e(q,S),e(S,pe),e(q,ie),h(u,Y,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,j),e(j,oe),e(j,R),e(R,le),e(j,se),e(j,N),e(N,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,O),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(Y),u&&n(L),u&&n(Z),u&&n(P)}}}function Z$(W){let p,M,m,g,T;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),T=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(F){p=a(F,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),T=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(F,_){h(F,p,_),e(p,M),e(p,m),e(m,g),e(p,T)},d(F){F&&n(p)}}}function K$(W){let p,M,m,g,T,F,_,z,ce,G,q,J,I,ne,ue,S,pe,ie,Y,L,te,Z,P,j,oe,R,le,se,N,he,de,C,fe,B,ee,ae,Q,me,O,A,re,H,ge,u,v,K,Fe,we,D,Te,be,ye,x,V,$e,ve,U,Ee,ke,_e,Me,Ya,Np,Op,Ec,xn,Bp,So,Wp,Rp,No,Qp,Hp,Mc,Kn,Bt,ul,Oo,Vp,pl,Up,zc,Cn,Bo,Yp,jn,Gp,Ga,Zp,Kp,Za,Xp,Jp,Wo,eh,nh,th,Xn,oh,Ka,sh,rh,Xa,ah,ih,qc,Jn,Wt,hl,Ro,lh,fl,dh,Pc,Pe,Qo,ch,ml,uh,ph,Rt,Ja,hh,fh,ei,mh,gh,_h,Ho,Fh,ni,Th,vh,kh,Ln,Vo,wh,gl,bh,yh,Uo,ti,$h,_l,Eh,Mh,oi,zh,Fl,qh,Ph,Qt,Yo,Ch,Go,jh,Tl,xh,Lh,Ah,wn,Zo,Dh,vl,Ih,Sh,Ko,Nh,et,Oh,kl,Bh,Wh,wl,Rh,Qh,Hh,si,Xo,Cc,nt,Ht,bl,Jo,Vh,yl,Uh,jc,Ze,es,Yh,ns,Gh,$l,Zh,Kh,Xh,Vt,ri,Jh,ef,ai,nf,tf,of,ts,sf,ii,rf,af,lf,bn,os,df,El,cf,uf,ss,pf,tt,hf,Ml,ff,mf,zl,gf,_f,xc,ot,Ut,ql,rs,Ff,Pl,Tf,Lc,st,as,vf,is,kf,li,wf,bf,Ac,rt,ls,yf,ds,$f,di,Ef,Mf,Dc,at,Yt,Cl,cs,zf,jl,qf,Ic,We,us,Pf,xl,Cf,jf,ps,xf,hs,Lf,Af,Df,fs,If,ci,Sf,Nf,Of,ms,Bf,gs,Wf,Rf,Qf,Ke,_s,Hf,it,Vf,ui,Uf,Yf,Ll,Gf,Zf,Kf,Gt,Xf,Al,Jf,em,Fs,Sc,lt,Zt,Dl,Ts,nm,Il,tm,Nc,Re,vs,om,Sl,sm,rm,ks,am,ws,im,lm,dm,bs,cm,pi,um,pm,hm,ys,fm,$s,mm,gm,_m,Xe,Es,Fm,dt,Tm,hi,vm,km,Nl,wm,bm,ym,Kt,$m,Ol,Em,Mm,Ms,Oc,ct,Xt,Bl,zs,zm,Wl,qm,Bc,ut,qs,Pm,Je,Ps,Cm,pt,jm,fi,xm,Lm,Rl,Am,Dm,Im,Jt,Sm,Ql,Nm,Om,Cs,Wc,ht,eo,Hl,js,Bm,Vl,Wm,Rc,Qe,xs,Rm,Ls,Qm,Ul,Hm,Vm,Um,As,Ym,Ds,Gm,Zm,Km,Is,Xm,mi,Jm,eg,ng,Ss,tg,Ns,og,sg,rg,en,Os,ag,ft,ig,gi,lg,dg,Yl,cg,ug,pg,no,hg,Gl,fg,mg,Bs,Qc,mt,to,Zl,Ws,gg,Kl,_g,Hc,He,Rs,Fg,Xl,Tg,vg,Qs,kg,Hs,wg,bg,yg,Vs,$g,_i,Eg,Mg,zg,Us,qg,Ys,Pg,Cg,jg,Be,Gs,xg,gt,Lg,Fi,Ag,Dg,Jl,Ig,Sg,Ng,oo,Og,ed,Bg,Wg,Zs,Rg,nd,Qg,Hg,Ks,Vc,_t,so,td,Xs,Vg,od,Ug,Uc,Ve,Js,Yg,sd,Gg,Zg,er,Kg,nr,Xg,Jg,e_,tr,n_,Ti,t_,o_,s_,or,r_,sr,a_,i_,l_,nn,rr,d_,Ft,c_,vi,u_,p_,rd,h_,f_,m_,ro,g_,ad,__,F_,ar,Yc,Tt,ao,id,ir,T_,ld,v_,Gc,Ue,lr,k_,dd,w_,b_,dr,y_,cr,$_,E_,M_,ur,z_,ki,q_,P_,C_,pr,j_,hr,x_,L_,A_,tn,fr,D_,vt,I_,wi,S_,N_,cd,O_,B_,W_,io,R_,ud,Q_,H_,mr,Zc,kt,lo,pd,gr,V_,hd,U_,Kc,Ye,_r,Y_,wt,G_,fd,Z_,K_,md,X_,J_,eF,Fr,nF,Tr,tF,oF,sF,vr,rF,bi,aF,iF,lF,kr,dF,wr,cF,uF,pF,on,br,hF,bt,fF,yi,mF,gF,gd,_F,FF,TF,co,vF,_d,kF,wF,yr,Xc,yt,uo,Fd,$r,bF,Td,yF,Jc,je,Er,$F,vd,EF,MF,Mr,zF,zr,qF,PF,CF,qr,jF,$i,xF,LF,AF,Pr,DF,Cr,IF,SF,NF,po,OF,sn,jr,BF,$t,WF,Ei,RF,QF,kd,HF,VF,UF,ho,YF,wd,GF,ZF,xr,eu,Et,fo,bd,Lr,KF,yd,XF,nu,xe,Ar,JF,$d,eT,nT,Dr,tT,Ir,oT,sT,rT,Sr,aT,Mi,iT,lT,dT,Nr,cT,Or,uT,pT,hT,mo,fT,rn,Br,mT,Mt,gT,zi,_T,FT,Ed,TT,vT,kT,go,wT,Md,bT,yT,Wr,tu,zt,_o,zd,Rr,$T,qd,ET,ou,Le,Qr,MT,Pd,zT,qT,Hr,PT,Vr,CT,jT,xT,Ur,LT,qi,AT,DT,IT,Yr,ST,Gr,NT,OT,BT,Fo,WT,an,Zr,RT,qt,QT,Pi,HT,VT,Cd,UT,YT,GT,To,ZT,jd,KT,XT,Kr,su,Pt,vo,xd,Xr,JT,Ld,ev,ru,Ae,Jr,nv,ea,tv,Ad,ov,sv,rv,na,av,ta,iv,lv,dv,oa,cv,Ci,uv,pv,hv,sa,fv,ra,mv,gv,_v,ko,Fv,ln,aa,Tv,Ct,vv,ji,kv,wv,Dd,bv,yv,$v,wo,Ev,Id,Mv,zv,ia,au,jt,bo,Sd,la,qv,Nd,Pv,iu,De,da,Cv,Od,jv,xv,ca,Lv,ua,Av,Dv,Iv,pa,Sv,xi,Nv,Ov,Bv,ha,Wv,fa,Rv,Qv,Hv,yo,Vv,dn,ma,Uv,xt,Yv,Li,Gv,Zv,Bd,Kv,Xv,Jv,$o,ek,Wd,nk,tk,ga,lu,Lt,Eo,Rd,_a,ok,Qd,sk,du,Ie,Fa,rk,Hd,ak,ik,Ta,lk,va,dk,ck,uk,ka,pk,Ai,hk,fk,mk,wa,gk,ba,_k,Fk,Tk,Mo,vk,cn,ya,kk,At,wk,Di,bk,yk,Vd,$k,Ek,Mk,zo,zk,Ud,qk,Pk,$a,cu,Dt,qo,Yd,Ea,Ck,Gd,jk,uu,Se,Ma,xk,Zd,Lk,Ak,za,Dk,qa,Ik,Sk,Nk,Pa,Ok,Ii,Bk,Wk,Rk,Ca,Qk,ja,Hk,Vk,Uk,Po,Yk,un,xa,Gk,It,Zk,Si,Kk,Xk,Kd,Jk,e1,n1,Co,t1,Xd,o1,s1,La,pu,St,jo,Jd,Aa,r1,ec,a1,hu,Ne,Da,i1,Nt,l1,nc,d1,c1,tc,u1,p1,h1,Ia,f1,Sa,m1,g1,_1,Na,F1,Ni,T1,v1,k1,Oa,w1,Ba,b1,y1,$1,xo,E1,pn,Wa,M1,Ot,z1,Oi,q1,P1,oc,C1,j1,x1,Lo,L1,sc,A1,D1,Ra,fu;return F=new qe({}),ne=new qe({}),Oo=new qe({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Ro=new qe({}),Qo=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Vo=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ko=new Oe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new qe({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new Oe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new qe({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L801",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1005",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new qe({}),us=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L894",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L910",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Gt=new ze({props:{$$slots:{default:[M$]},$$scope:{ctx:W}}}),Fs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ts=new qe({}),vs=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L971",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L988",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Kt=new ze({props:{$$slots:{default:[z$]},$$scope:{ctx:W}}}),Ms=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new qe({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1079"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1088",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Jt=new ze({props:{$$slots:{default:[q$]},$$scope:{ctx:W}}}),Cs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),js=new qe({}),xs=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1162",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Os=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1178",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),no=new ze({props:{$$slots:{default:[P$]},$$scope:{ctx:W}}}),Bs=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ws=new qe({}),Rs=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1242",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gs=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1253",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),oo=new ze({props:{$$slots:{default:[C$]},$$scope:{ctx:W}}}),Zs=new Oe({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)  # doctest:+IGNORE_OUTPUT

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)  <span class="hljs-comment"># doctest:+IGNORE_OUTPUT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Ks=new Oe({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)  # doctest:+IGNORE_OUTPUT

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)  <span class="hljs-comment"># doctest:+IGNORE_OUTPUT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Xs=new qe({}),Js=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1335",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1344",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ro=new ze({props:{$$slots:{default:[j$]},$$scope:{ctx:W}}}),ar=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ir=new qe({}),lr=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1419",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fr=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1431",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),io=new ze({props:{$$slots:{default:[x$]},$$scope:{ctx:W}}}),mr=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),gr=new qe({}),_r=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1493",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),br=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_funnel.py#L1504",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),co=new ze({props:{$$slots:{default:[L$]},$$scope:{ctx:W}}}),yr=new Oe({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

torch.manual_seed(0)  # doctest:+IGNORE_OUTPUT

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
round(loss.item(), 2)


start_scores = outputs.start_logits
list(start_scores.shape)


end_scores = outputs.end_logits
list(end_scores.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)  <span class="hljs-comment"># doctest:+IGNORE_OUTPUT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(start_scores.shape)


<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(end_scores.shape)
`}}),$r=new qe({}),Er=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1122",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),po=new ze({props:{$$slots:{default:[A$]},$$scope:{ctx:W}}}),jr=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1127",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),ho=new ze({props:{$$slots:{default:[D$]},$$scope:{ctx:W}}}),xr=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lr=new qe({}),Ar=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1183",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[I$]},$$scope:{ctx:W}}}),Br=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1188",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),go=new ze({props:{$$slots:{default:[S$]},$$scope:{ctx:W}}}),Wr=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Rr=new qe({}),Qr=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1246",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Fo=new ze({props:{$$slots:{default:[N$]},$$scope:{ctx:W}}}),Zr=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1253",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),To=new ze({props:{$$slots:{default:[O$]},$$scope:{ctx:W}}}),Kr=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Xr=new qe({}),Jr=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1325",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ko=new ze({props:{$$slots:{default:[B$]},$$scope:{ctx:W}}}),aa=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1339",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),wo=new ze({props:{$$slots:{default:[W$]},$$scope:{ctx:W}}}),ia=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),la=new qe({}),da=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1420",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yo=new ze({props:{$$slots:{default:[R$]},$$scope:{ctx:W}}}),ma=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1428",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),$o=new ze({props:{$$slots:{default:[Q$]},$$scope:{ctx:W}}}),ga=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),_a=new qe({}),Fa=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1510",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[H$]},$$scope:{ctx:W}}}),ya=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1527",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),zo=new ze({props:{$$slots:{default:[V$]},$$scope:{ctx:W}}}),$a=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ea=new qe({}),Ma=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1645",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[U$]},$$scope:{ctx:W}}}),xa=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1656",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Co=new ze({props:{$$slots:{default:[Y$]},$$scope:{ctx:W}}}),La=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Aa=new qe({}),Da=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1737",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[G$]},$$scope:{ctx:W}}}),Wa=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15987/src/transformers/models/funnel/modeling_tf_funnel.py#L1747",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15987/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15987/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15987/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Lo=new ze({props:{$$slots:{default:[Z$]},$$scope:{ctx:W}}}),Ra=new Oe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=r("meta"),M=l(),m=r("h1"),g=r("a"),T=r("span"),k(F.$$.fragment),_=l(),z=r("span"),ce=t("Funnel Transformer"),G=l(),q=r("h2"),J=r("a"),I=r("span"),k(ne.$$.fragment),ue=l(),S=r("span"),pe=t("Overview"),ie=l(),Y=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),Z=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),j=l(),oe=r("p"),R=t("The abstract from the paper is the following:"),le=l(),se=r("p"),N=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),Q=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),O=r("li"),A=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),H=t("FunnelModel"),ge=t(", "),u=r("a"),v=t("FunnelForPreTraining"),K=t(`,
`),Fe=r("a"),we=t("FunnelForMaskedLM"),D=t(", "),Te=r("a"),be=t("FunnelForTokenClassification"),ye=t(` and
class:`),x=r("em"),V=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),ve=r("a"),U=t("FunnelBaseModel"),Ee=t(", "),ke=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Ya=r("a"),Np=t("FunnelForMultipleChoice"),Op=t("."),Ec=l(),xn=r("p"),Bp=t("This model was contributed by "),So=r("a"),Wp=t("sgugger"),Rp=t(". The original code can be found "),No=r("a"),Qp=t("here"),Hp=t("."),Mc=l(),Kn=r("h2"),Bt=r("a"),ul=r("span"),k(Oo.$$.fragment),Vp=l(),pl=r("span"),Up=t("FunnelConfig"),zc=l(),Cn=r("div"),k(Bo.$$.fragment),Yp=l(),jn=r("p"),Gp=t("This is the configuration class to store the configuration of a "),Ga=r("a"),Zp=t("FunnelModel"),Kp=t(" or a "),Za=r("a"),Xp=t("TFBertModel"),Jp=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),eh=t("funnel-transformer/small"),nh=t(" architecture."),th=l(),Xn=r("p"),oh=t("Configuration objects inherit from "),Ka=r("a"),sh=t("PretrainedConfig"),rh=t(` and can be used to control the model outputs. Read the
documentation from `),Xa=r("a"),ah=t("PretrainedConfig"),ih=t(" for more information."),qc=l(),Jn=r("h2"),Wt=r("a"),hl=r("span"),k(Ro.$$.fragment),lh=l(),fl=r("span"),dh=t("FunnelTokenizer"),Pc=l(),Pe=r("div"),k(Qo.$$.fragment),ch=l(),ml=r("p"),uh=t("Construct a Funnel Transformer tokenizer."),ph=l(),Rt=r("p"),Ja=r("a"),hh=t("FunnelTokenizer"),fh=t(" is identical to "),ei=r("a"),mh=t("BertTokenizer"),gh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),_h=l(),Ho=r("p"),Fh=t("Refer to superclass "),ni=r("a"),Th=t("BertTokenizer"),vh=t(" for usage examples and documentation concerning parameters."),kh=l(),Ln=r("div"),k(Vo.$$.fragment),wh=l(),gl=r("p"),bh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),yh=l(),Uo=r("ul"),ti=r("li"),$h=t("single sequence: "),_l=r("code"),Eh=t("[CLS] X [SEP]"),Mh=l(),oi=r("li"),zh=t("pair of sequences: "),Fl=r("code"),qh=t("[CLS] A [SEP] B [SEP]"),Ph=l(),Qt=r("div"),k(Yo.$$.fragment),Ch=l(),Go=r("p"),jh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Tl=r("code"),xh=t("prepare_for_model"),Lh=t(" method."),Ah=l(),wn=r("div"),k(Zo.$$.fragment),Dh=l(),vl=r("p"),Ih=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Sh=l(),k(Ko.$$.fragment),Nh=l(),et=r("p"),Oh=t("If "),kl=r("code"),Bh=t("token_ids_1"),Wh=t(" is "),wl=r("code"),Rh=t("None"),Qh=t(", this method only returns the first portion of the mask (0s)."),Hh=l(),si=r("div"),k(Xo.$$.fragment),Cc=l(),nt=r("h2"),Ht=r("a"),bl=r("span"),k(Jo.$$.fragment),Vh=l(),yl=r("span"),Uh=t("FunnelTokenizerFast"),jc=l(),Ze=r("div"),k(es.$$.fragment),Yh=l(),ns=r("p"),Gh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=r("em"),Zh=t("tokenizers"),Kh=t(" library)."),Xh=l(),Vt=r("p"),ri=r("a"),Jh=t("FunnelTokenizerFast"),ef=t(" is identical to "),ai=r("a"),nf=t("BertTokenizerFast"),tf=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),of=l(),ts=r("p"),sf=t("Refer to superclass "),ii=r("a"),rf=t("BertTokenizerFast"),af=t(" for usage examples and documentation concerning parameters."),lf=l(),bn=r("div"),k(os.$$.fragment),df=l(),El=r("p"),cf=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),uf=l(),k(ss.$$.fragment),pf=l(),tt=r("p"),hf=t("If "),Ml=r("code"),ff=t("token_ids_1"),mf=t(" is "),zl=r("code"),gf=t("None"),_f=t(", this method only returns the first portion of the mask (0s)."),xc=l(),ot=r("h2"),Ut=r("a"),ql=r("span"),k(rs.$$.fragment),Ff=l(),Pl=r("span"),Tf=t("Funnel specific outputs"),Lc=l(),st=r("div"),k(as.$$.fragment),vf=l(),is=r("p"),kf=t("Output type of "),li=r("a"),wf=t("FunnelForPreTraining"),bf=t("."),Ac=l(),rt=r("div"),k(ls.$$.fragment),yf=l(),ds=r("p"),$f=t("Output type of "),di=r("a"),Ef=t("FunnelForPreTraining"),Mf=t("."),Dc=l(),at=r("h2"),Yt=r("a"),Cl=r("span"),k(cs.$$.fragment),zf=l(),jl=r("span"),qf=t("FunnelBaseModel"),Ic=l(),We=r("div"),k(us.$$.fragment),Pf=l(),xl=r("p"),Cf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),jf=l(),ps=r("p"),xf=t("The Funnel Transformer model was proposed in "),hs=r("a"),Lf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Af=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Df=l(),fs=r("p"),If=t("This model inherits from "),ci=r("a"),Sf=t("PreTrainedModel"),Nf=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Of=l(),ms=r("p"),Bf=t("This model is also a PyTorch "),gs=r("a"),Wf=t("torch.nn.Module"),Rf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qf=l(),Ke=r("div"),k(_s.$$.fragment),Hf=l(),it=r("p"),Vf=t("The "),ui=r("a"),Uf=t("FunnelBaseModel"),Yf=t(" forward method, overrides the "),Ll=r("code"),Gf=t("__call__"),Zf=t(" special method."),Kf=l(),k(Gt.$$.fragment),Xf=l(),Al=r("p"),Jf=t("Example:"),em=l(),k(Fs.$$.fragment),Sc=l(),lt=r("h2"),Zt=r("a"),Dl=r("span"),k(Ts.$$.fragment),nm=l(),Il=r("span"),tm=t("FunnelModel"),Nc=l(),Re=r("div"),k(vs.$$.fragment),om=l(),Sl=r("p"),sm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),rm=l(),ks=r("p"),am=t("The Funnel Transformer model was proposed in "),ws=r("a"),im=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dm=l(),bs=r("p"),cm=t("This model inherits from "),pi=r("a"),um=t("PreTrainedModel"),pm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hm=l(),ys=r("p"),fm=t("This model is also a PyTorch "),$s=r("a"),mm=t("torch.nn.Module"),gm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_m=l(),Xe=r("div"),k(Es.$$.fragment),Fm=l(),dt=r("p"),Tm=t("The "),hi=r("a"),vm=t("FunnelModel"),km=t(" forward method, overrides the "),Nl=r("code"),wm=t("__call__"),bm=t(" special method."),ym=l(),k(Kt.$$.fragment),$m=l(),Ol=r("p"),Em=t("Example:"),Mm=l(),k(Ms.$$.fragment),Oc=l(),ct=r("h2"),Xt=r("a"),Bl=r("span"),k(zs.$$.fragment),zm=l(),Wl=r("span"),qm=t("FunnelModelForPreTraining"),Bc=l(),ut=r("div"),k(qs.$$.fragment),Pm=l(),Je=r("div"),k(Ps.$$.fragment),Cm=l(),pt=r("p"),jm=t("The "),fi=r("a"),xm=t("FunnelForPreTraining"),Lm=t(" forward method, overrides the "),Rl=r("code"),Am=t("__call__"),Dm=t(" special method."),Im=l(),k(Jt.$$.fragment),Sm=l(),Ql=r("p"),Nm=t("Examples:"),Om=l(),k(Cs.$$.fragment),Wc=l(),ht=r("h2"),eo=r("a"),Hl=r("span"),k(js.$$.fragment),Bm=l(),Vl=r("span"),Wm=t("FunnelForMaskedLM"),Rc=l(),Qe=r("div"),k(xs.$$.fragment),Rm=l(),Ls=r("p"),Qm=t("Funnel Transformer Model with a "),Ul=r("code"),Hm=t("language modeling"),Vm=t(" head on top."),Um=l(),As=r("p"),Ym=t("The Funnel Transformer model was proposed in "),Ds=r("a"),Gm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Zm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Km=l(),Is=r("p"),Xm=t("This model inherits from "),mi=r("a"),Jm=t("PreTrainedModel"),eg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ng=l(),Ss=r("p"),tg=t("This model is also a PyTorch "),Ns=r("a"),og=t("torch.nn.Module"),sg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rg=l(),en=r("div"),k(Os.$$.fragment),ag=l(),ft=r("p"),ig=t("The "),gi=r("a"),lg=t("FunnelForMaskedLM"),dg=t(" forward method, overrides the "),Yl=r("code"),cg=t("__call__"),ug=t(" special method."),pg=l(),k(no.$$.fragment),hg=l(),Gl=r("p"),fg=t("Example:"),mg=l(),k(Bs.$$.fragment),Qc=l(),mt=r("h2"),to=r("a"),Zl=r("span"),k(Ws.$$.fragment),gg=l(),Kl=r("span"),_g=t("FunnelForSequenceClassification"),Hc=l(),He=r("div"),k(Rs.$$.fragment),Fg=l(),Xl=r("p"),Tg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),vg=l(),Qs=r("p"),kg=t("The Funnel Transformer model was proposed in "),Hs=r("a"),wg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),bg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),yg=l(),Vs=r("p"),$g=t("This model inherits from "),_i=r("a"),Eg=t("PreTrainedModel"),Mg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zg=l(),Us=r("p"),qg=t("This model is also a PyTorch "),Ys=r("a"),Pg=t("torch.nn.Module"),Cg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jg=l(),Be=r("div"),k(Gs.$$.fragment),xg=l(),gt=r("p"),Lg=t("The "),Fi=r("a"),Ag=t("FunnelForSequenceClassification"),Dg=t(" forward method, overrides the "),Jl=r("code"),Ig=t("__call__"),Sg=t(" special method."),Ng=l(),k(oo.$$.fragment),Og=l(),ed=r("p"),Bg=t("Example of single-label classification:"),Wg=l(),k(Zs.$$.fragment),Rg=l(),nd=r("p"),Qg=t("Example of multi-label classification:"),Hg=l(),k(Ks.$$.fragment),Vc=l(),_t=r("h2"),so=r("a"),td=r("span"),k(Xs.$$.fragment),Vg=l(),od=r("span"),Ug=t("FunnelForMultipleChoice"),Uc=l(),Ve=r("div"),k(Js.$$.fragment),Yg=l(),sd=r("p"),Gg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Zg=l(),er=r("p"),Kg=t("The Funnel Transformer model was proposed in "),nr=r("a"),Xg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Jg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),e_=l(),tr=r("p"),n_=t("This model inherits from "),Ti=r("a"),t_=t("PreTrainedModel"),o_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s_=l(),or=r("p"),r_=t("This model is also a PyTorch "),sr=r("a"),a_=t("torch.nn.Module"),i_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),l_=l(),nn=r("div"),k(rr.$$.fragment),d_=l(),Ft=r("p"),c_=t("The "),vi=r("a"),u_=t("FunnelForMultipleChoice"),p_=t(" forward method, overrides the "),rd=r("code"),h_=t("__call__"),f_=t(" special method."),m_=l(),k(ro.$$.fragment),g_=l(),ad=r("p"),__=t("Example:"),F_=l(),k(ar.$$.fragment),Yc=l(),Tt=r("h2"),ao=r("a"),id=r("span"),k(ir.$$.fragment),T_=l(),ld=r("span"),v_=t("FunnelForTokenClassification"),Gc=l(),Ue=r("div"),k(lr.$$.fragment),k_=l(),dd=r("p"),w_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),b_=l(),dr=r("p"),y_=t("The Funnel Transformer model was proposed in "),cr=r("a"),$_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),E_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),M_=l(),ur=r("p"),z_=t("This model inherits from "),ki=r("a"),q_=t("PreTrainedModel"),P_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C_=l(),pr=r("p"),j_=t("This model is also a PyTorch "),hr=r("a"),x_=t("torch.nn.Module"),L_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),A_=l(),tn=r("div"),k(fr.$$.fragment),D_=l(),vt=r("p"),I_=t("The "),wi=r("a"),S_=t("FunnelForTokenClassification"),N_=t(" forward method, overrides the "),cd=r("code"),O_=t("__call__"),B_=t(" special method."),W_=l(),k(io.$$.fragment),R_=l(),ud=r("p"),Q_=t("Example:"),H_=l(),k(mr.$$.fragment),Zc=l(),kt=r("h2"),lo=r("a"),pd=r("span"),k(gr.$$.fragment),V_=l(),hd=r("span"),U_=t("FunnelForQuestionAnswering"),Kc=l(),Ye=r("div"),k(_r.$$.fragment),Y_=l(),wt=r("p"),G_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=r("code"),Z_=t("span start logits"),K_=t(" and "),md=r("code"),X_=t("span end logits"),J_=t(")."),eF=l(),Fr=r("p"),nF=t("The Funnel Transformer model was proposed in "),Tr=r("a"),tF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),oF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sF=l(),vr=r("p"),rF=t("This model inherits from "),bi=r("a"),aF=t("PreTrainedModel"),iF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lF=l(),kr=r("p"),dF=t("This model is also a PyTorch "),wr=r("a"),cF=t("torch.nn.Module"),uF=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),pF=l(),on=r("div"),k(br.$$.fragment),hF=l(),bt=r("p"),fF=t("The "),yi=r("a"),mF=t("FunnelForQuestionAnswering"),gF=t(" forward method, overrides the "),gd=r("code"),_F=t("__call__"),FF=t(" special method."),TF=l(),k(co.$$.fragment),vF=l(),_d=r("p"),kF=t("Example:"),wF=l(),k(yr.$$.fragment),Xc=l(),yt=r("h2"),uo=r("a"),Fd=r("span"),k($r.$$.fragment),bF=l(),Td=r("span"),yF=t("TFFunnelBaseModel"),Jc=l(),je=r("div"),k(Er.$$.fragment),$F=l(),vd=r("p"),EF=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),MF=l(),Mr=r("p"),zF=t("The Funnel Transformer model was proposed in "),zr=r("a"),qF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),PF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),CF=l(),qr=r("p"),jF=t("This model inherits from "),$i=r("a"),xF=t("TFPreTrainedModel"),LF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),AF=l(),Pr=r("p"),DF=t("This model is also a "),Cr=r("a"),IF=t("tf.keras.Model"),SF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),NF=l(),k(po.$$.fragment),OF=l(),sn=r("div"),k(jr.$$.fragment),BF=l(),$t=r("p"),WF=t("The "),Ei=r("a"),RF=t("TFFunnelBaseModel"),QF=t(" forward method, overrides the "),kd=r("code"),HF=t("__call__"),VF=t(" special method."),UF=l(),k(ho.$$.fragment),YF=l(),wd=r("p"),GF=t("Example:"),ZF=l(),k(xr.$$.fragment),eu=l(),Et=r("h2"),fo=r("a"),bd=r("span"),k(Lr.$$.fragment),KF=l(),yd=r("span"),XF=t("TFFunnelModel"),nu=l(),xe=r("div"),k(Ar.$$.fragment),JF=l(),$d=r("p"),eT=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),nT=l(),Dr=r("p"),tT=t("The Funnel Transformer model was proposed in "),Ir=r("a"),oT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rT=l(),Sr=r("p"),aT=t("This model inherits from "),Mi=r("a"),iT=t("TFPreTrainedModel"),lT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dT=l(),Nr=r("p"),cT=t("This model is also a "),Or=r("a"),uT=t("tf.keras.Model"),pT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hT=l(),k(mo.$$.fragment),fT=l(),rn=r("div"),k(Br.$$.fragment),mT=l(),Mt=r("p"),gT=t("The "),zi=r("a"),_T=t("TFFunnelModel"),FT=t(" forward method, overrides the "),Ed=r("code"),TT=t("__call__"),vT=t(" special method."),kT=l(),k(go.$$.fragment),wT=l(),Md=r("p"),bT=t("Example:"),yT=l(),k(Wr.$$.fragment),tu=l(),zt=r("h2"),_o=r("a"),zd=r("span"),k(Rr.$$.fragment),$T=l(),qd=r("span"),ET=t("TFFunnelModelForPreTraining"),ou=l(),Le=r("div"),k(Qr.$$.fragment),MT=l(),Pd=r("p"),zT=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),qT=l(),Hr=r("p"),PT=t("The Funnel Transformer model was proposed in "),Vr=r("a"),CT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),jT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xT=l(),Ur=r("p"),LT=t("This model inherits from "),qi=r("a"),AT=t("TFPreTrainedModel"),DT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),IT=l(),Yr=r("p"),ST=t("This model is also a "),Gr=r("a"),NT=t("tf.keras.Model"),OT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),BT=l(),k(Fo.$$.fragment),WT=l(),an=r("div"),k(Zr.$$.fragment),RT=l(),qt=r("p"),QT=t("The "),Pi=r("a"),HT=t("TFFunnelForPreTraining"),VT=t(" forward method, overrides the "),Cd=r("code"),UT=t("__call__"),YT=t(" special method."),GT=l(),k(To.$$.fragment),ZT=l(),jd=r("p"),KT=t("Examples:"),XT=l(),k(Kr.$$.fragment),su=l(),Pt=r("h2"),vo=r("a"),xd=r("span"),k(Xr.$$.fragment),JT=l(),Ld=r("span"),ev=t("TFFunnelForMaskedLM"),ru=l(),Ae=r("div"),k(Jr.$$.fragment),nv=l(),ea=r("p"),tv=t("Funnel Model with a "),Ad=r("code"),ov=t("language modeling"),sv=t(" head on top."),rv=l(),na=r("p"),av=t("The Funnel Transformer model was proposed in "),ta=r("a"),iv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dv=l(),oa=r("p"),cv=t("This model inherits from "),Ci=r("a"),uv=t("TFPreTrainedModel"),pv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hv=l(),sa=r("p"),fv=t("This model is also a "),ra=r("a"),mv=t("tf.keras.Model"),gv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_v=l(),k(ko.$$.fragment),Fv=l(),ln=r("div"),k(aa.$$.fragment),Tv=l(),Ct=r("p"),vv=t("The "),ji=r("a"),kv=t("TFFunnelForMaskedLM"),wv=t(" forward method, overrides the "),Dd=r("code"),bv=t("__call__"),yv=t(" special method."),$v=l(),k(wo.$$.fragment),Ev=l(),Id=r("p"),Mv=t("Example:"),zv=l(),k(ia.$$.fragment),au=l(),jt=r("h2"),bo=r("a"),Sd=r("span"),k(la.$$.fragment),qv=l(),Nd=r("span"),Pv=t("TFFunnelForSequenceClassification"),iu=l(),De=r("div"),k(da.$$.fragment),Cv=l(),Od=r("p"),jv=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),xv=l(),ca=r("p"),Lv=t("The Funnel Transformer model was proposed in "),ua=r("a"),Av=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Dv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Iv=l(),pa=r("p"),Sv=t("This model inherits from "),xi=r("a"),Nv=t("TFPreTrainedModel"),Ov=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bv=l(),ha=r("p"),Wv=t("This model is also a "),fa=r("a"),Rv=t("tf.keras.Model"),Qv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Hv=l(),k(yo.$$.fragment),Vv=l(),dn=r("div"),k(ma.$$.fragment),Uv=l(),xt=r("p"),Yv=t("The "),Li=r("a"),Gv=t("TFFunnelForSequenceClassification"),Zv=t(" forward method, overrides the "),Bd=r("code"),Kv=t("__call__"),Xv=t(" special method."),Jv=l(),k($o.$$.fragment),ek=l(),Wd=r("p"),nk=t("Example:"),tk=l(),k(ga.$$.fragment),lu=l(),Lt=r("h2"),Eo=r("a"),Rd=r("span"),k(_a.$$.fragment),ok=l(),Qd=r("span"),sk=t("TFFunnelForMultipleChoice"),du=l(),Ie=r("div"),k(Fa.$$.fragment),rk=l(),Hd=r("p"),ak=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ik=l(),Ta=r("p"),lk=t("The Funnel Transformer model was proposed in "),va=r("a"),dk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ck=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),uk=l(),ka=r("p"),pk=t("This model inherits from "),Ai=r("a"),hk=t("TFPreTrainedModel"),fk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mk=l(),wa=r("p"),gk=t("This model is also a "),ba=r("a"),_k=t("tf.keras.Model"),Fk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Tk=l(),k(Mo.$$.fragment),vk=l(),cn=r("div"),k(ya.$$.fragment),kk=l(),At=r("p"),wk=t("The "),Di=r("a"),bk=t("TFFunnelForMultipleChoice"),yk=t(" forward method, overrides the "),Vd=r("code"),$k=t("__call__"),Ek=t(" special method."),Mk=l(),k(zo.$$.fragment),zk=l(),Ud=r("p"),qk=t("Example:"),Pk=l(),k($a.$$.fragment),cu=l(),Dt=r("h2"),qo=r("a"),Yd=r("span"),k(Ea.$$.fragment),Ck=l(),Gd=r("span"),jk=t("TFFunnelForTokenClassification"),uu=l(),Se=r("div"),k(Ma.$$.fragment),xk=l(),Zd=r("p"),Lk=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ak=l(),za=r("p"),Dk=t("The Funnel Transformer model was proposed in "),qa=r("a"),Ik=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Nk=l(),Pa=r("p"),Ok=t("This model inherits from "),Ii=r("a"),Bk=t("TFPreTrainedModel"),Wk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rk=l(),Ca=r("p"),Qk=t("This model is also a "),ja=r("a"),Hk=t("tf.keras.Model"),Vk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Uk=l(),k(Po.$$.fragment),Yk=l(),un=r("div"),k(xa.$$.fragment),Gk=l(),It=r("p"),Zk=t("The "),Si=r("a"),Kk=t("TFFunnelForTokenClassification"),Xk=t(" forward method, overrides the "),Kd=r("code"),Jk=t("__call__"),e1=t(" special method."),n1=l(),k(Co.$$.fragment),t1=l(),Xd=r("p"),o1=t("Example:"),s1=l(),k(La.$$.fragment),pu=l(),St=r("h2"),jo=r("a"),Jd=r("span"),k(Aa.$$.fragment),r1=l(),ec=r("span"),a1=t("TFFunnelForQuestionAnswering"),hu=l(),Ne=r("div"),k(Da.$$.fragment),i1=l(),Nt=r("p"),l1=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=r("code"),d1=t("span start logits"),c1=t(" and "),tc=r("code"),u1=t("span end logits"),p1=t(")."),h1=l(),Ia=r("p"),f1=t("The Funnel Transformer model was proposed in "),Sa=r("a"),m1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),g1=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_1=l(),Na=r("p"),F1=t("This model inherits from "),Ni=r("a"),T1=t("TFPreTrainedModel"),v1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),k1=l(),Oa=r("p"),w1=t("This model is also a "),Ba=r("a"),b1=t("tf.keras.Model"),y1=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),$1=l(),k(xo.$$.fragment),E1=l(),pn=r("div"),k(Wa.$$.fragment),M1=l(),Ot=r("p"),z1=t("The "),Oi=r("a"),q1=t("TFFunnelForQuestionAnswering"),P1=t(" forward method, overrides the "),oc=r("code"),C1=t("__call__"),j1=t(" special method."),x1=l(),k(Lo.$$.fragment),L1=l(),sc=r("p"),A1=t("Example:"),D1=l(),k(Ra.$$.fragment),this.h()},l(s){const f=E$('[data-svelte="svelte-1phssyn"]',document.head);p=a(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(s),m=a(s,"H1",{class:!0});var Qa=i(m);g=a(Qa,"A",{id:!0,class:!0,href:!0});var rc=i(g);T=a(rc,"SPAN",{});var ac=i(T);w(F.$$.fragment,ac),ac.forEach(n),rc.forEach(n),_=d(Qa),z=a(Qa,"SPAN",{});var ic=i(z);ce=o(ic,"Funnel Transformer"),ic.forEach(n),Qa.forEach(n),G=d(s),q=a(s,"H2",{class:!0});var Ha=i(q);J=a(Ha,"A",{id:!0,class:!0,href:!0});var lc=i(J);I=a(lc,"SPAN",{});var dc=i(I);w(ne.$$.fragment,dc),dc.forEach(n),lc.forEach(n),ue=d(Ha),S=a(Ha,"SPAN",{});var cc=i(S);pe=o(cc,"Overview"),cc.forEach(n),Ha.forEach(n),ie=d(s),Y=a(s,"P",{});var Va=i(Y);L=o(Va,"The Funnel Transformer model was proposed in the paper "),te=a(Va,"A",{href:!0,rel:!0});var uc=i(te);Z=o(uc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),uc.forEach(n),P=o(Va,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Va.forEach(n),j=d(s),oe=a(s,"P",{});var pc=i(oe);R=o(pc,"The abstract from the paper is the following:"),pc.forEach(n),le=d(s),se=a(s,"P",{});var hc=i(se);N=a(hc,"EM",{});var fc=i(N);he=o(fc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),fc.forEach(n),hc.forEach(n),de=d(s),C=a(s,"P",{});var mc=i(C);fe=o(mc,"Tips:"),mc.forEach(n),B=d(s),ee=a(s,"UL",{});var Ua=i(ee);ae=a(Ua,"LI",{});var gc=i(ae);Q=o(gc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),gc.forEach(n),me=d(Ua),O=a(Ua,"LI",{});var Ce=i(O);A=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(Ce,"A",{href:!0});var _c=i(re);H=o(_c,"FunnelModel"),_c.forEach(n),ge=o(Ce,", "),u=a(Ce,"A",{href:!0});var Fc=i(u);v=o(Fc,"FunnelForPreTraining"),Fc.forEach(n),K=o(Ce,`,
`),Fe=a(Ce,"A",{href:!0});var Tc=i(Fe);we=o(Tc,"FunnelForMaskedLM"),Tc.forEach(n),D=o(Ce,", "),Te=a(Ce,"A",{href:!0});var vc=i(Te);be=o(vc,"FunnelForTokenClassification"),vc.forEach(n),ye=o(Ce,` and
class:`),x=a(Ce,"EM",{});var kc=i(x);V=o(kc,"~transformers.FunnelForQuestionAnswering"),kc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),ve=a(Ce,"A",{href:!0});var wc=i(ve);U=o(wc,"FunnelBaseModel"),wc.forEach(n),Ee=o(Ce,", "),ke=a(Ce,"A",{href:!0});var bc=i(ke);_e=o(bc,"FunnelForSequenceClassification"),bc.forEach(n),Me=o(Ce,` and
`),Ya=a(Ce,"A",{href:!0});var N1=i(Ya);Np=o(N1,"FunnelForMultipleChoice"),N1.forEach(n),Op=o(Ce,"."),Ce.forEach(n),Ua.forEach(n),Ec=d(s),xn=a(s,"P",{});var Bi=i(xn);Bp=o(Bi,"This model was contributed by "),So=a(Bi,"A",{href:!0,rel:!0});var O1=i(So);Wp=o(O1,"sgugger"),O1.forEach(n),Rp=o(Bi,". The original code can be found "),No=a(Bi,"A",{href:!0,rel:!0});var B1=i(No);Qp=o(B1,"here"),B1.forEach(n),Hp=o(Bi,"."),Bi.forEach(n),Mc=d(s),Kn=a(s,"H2",{class:!0});var mu=i(Kn);Bt=a(mu,"A",{id:!0,class:!0,href:!0});var W1=i(Bt);ul=a(W1,"SPAN",{});var R1=i(ul);w(Oo.$$.fragment,R1),R1.forEach(n),W1.forEach(n),Vp=d(mu),pl=a(mu,"SPAN",{});var Q1=i(pl);Up=o(Q1,"FunnelConfig"),Q1.forEach(n),mu.forEach(n),zc=d(s),Cn=a(s,"DIV",{class:!0});var Wi=i(Cn);w(Bo.$$.fragment,Wi),Yp=d(Wi),jn=a(Wi,"P",{});var Ao=i(jn);Gp=o(Ao,"This is the configuration class to store the configuration of a "),Ga=a(Ao,"A",{href:!0});var H1=i(Ga);Zp=o(H1,"FunnelModel"),H1.forEach(n),Kp=o(Ao," or a "),Za=a(Ao,"A",{href:!0});var V1=i(Za);Xp=o(V1,"TFBertModel"),V1.forEach(n),Jp=o(Ao,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Ao,"A",{href:!0,rel:!0});var U1=i(Wo);eh=o(U1,"funnel-transformer/small"),U1.forEach(n),nh=o(Ao," architecture."),Ao.forEach(n),th=d(Wi),Xn=a(Wi,"P",{});var Ri=i(Xn);oh=o(Ri,"Configuration objects inherit from "),Ka=a(Ri,"A",{href:!0});var Y1=i(Ka);sh=o(Y1,"PretrainedConfig"),Y1.forEach(n),rh=o(Ri,` and can be used to control the model outputs. Read the
documentation from `),Xa=a(Ri,"A",{href:!0});var G1=i(Xa);ah=o(G1,"PretrainedConfig"),G1.forEach(n),ih=o(Ri," for more information."),Ri.forEach(n),Wi.forEach(n),qc=d(s),Jn=a(s,"H2",{class:!0});var gu=i(Jn);Wt=a(gu,"A",{id:!0,class:!0,href:!0});var Z1=i(Wt);hl=a(Z1,"SPAN",{});var K1=i(hl);w(Ro.$$.fragment,K1),K1.forEach(n),Z1.forEach(n),lh=d(gu),fl=a(gu,"SPAN",{});var X1=i(fl);dh=o(X1,"FunnelTokenizer"),X1.forEach(n),gu.forEach(n),Pc=d(s),Pe=a(s,"DIV",{class:!0});var Ge=i(Pe);w(Qo.$$.fragment,Ge),ch=d(Ge),ml=a(Ge,"P",{});var J1=i(ml);uh=o(J1,"Construct a Funnel Transformer tokenizer."),J1.forEach(n),ph=d(Ge),Rt=a(Ge,"P",{});var yc=i(Rt);Ja=a(yc,"A",{href:!0});var ew=i(Ja);hh=o(ew,"FunnelTokenizer"),ew.forEach(n),fh=o(yc," is identical to "),ei=a(yc,"A",{href:!0});var nw=i(ei);mh=o(nw,"BertTokenizer"),nw.forEach(n),gh=o(yc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),yc.forEach(n),_h=d(Ge),Ho=a(Ge,"P",{});var _u=i(Ho);Fh=o(_u,"Refer to superclass "),ni=a(_u,"A",{href:!0});var tw=i(ni);Th=o(tw,"BertTokenizer"),tw.forEach(n),vh=o(_u," for usage examples and documentation concerning parameters."),_u.forEach(n),kh=d(Ge),Ln=a(Ge,"DIV",{class:!0});var Qi=i(Ln);w(Vo.$$.fragment,Qi),wh=d(Qi),gl=a(Qi,"P",{});var ow=i(gl);bh=o(ow,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),ow.forEach(n),yh=d(Qi),Uo=a(Qi,"UL",{});var Fu=i(Uo);ti=a(Fu,"LI",{});var I1=i(ti);$h=o(I1,"single sequence: "),_l=a(I1,"CODE",{});var sw=i(_l);Eh=o(sw,"[CLS] X [SEP]"),sw.forEach(n),I1.forEach(n),Mh=d(Fu),oi=a(Fu,"LI",{});var S1=i(oi);zh=o(S1,"pair of sequences: "),Fl=a(S1,"CODE",{});var rw=i(Fl);qh=o(rw,"[CLS] A [SEP] B [SEP]"),rw.forEach(n),S1.forEach(n),Fu.forEach(n),Qi.forEach(n),Ph=d(Ge),Qt=a(Ge,"DIV",{class:!0});var Tu=i(Qt);w(Yo.$$.fragment,Tu),Ch=d(Tu),Go=a(Tu,"P",{});var vu=i(Go);jh=o(vu,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Tl=a(vu,"CODE",{});var aw=i(Tl);xh=o(aw,"prepare_for_model"),aw.forEach(n),Lh=o(vu," method."),vu.forEach(n),Tu.forEach(n),Ah=d(Ge),wn=a(Ge,"DIV",{class:!0});var Do=i(wn);w(Zo.$$.fragment,Do),Dh=d(Do),vl=a(Do,"P",{});var iw=i(vl);Ih=o(iw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),iw.forEach(n),Sh=d(Do),w(Ko.$$.fragment,Do),Nh=d(Do),et=a(Do,"P",{});var Hi=i(et);Oh=o(Hi,"If "),kl=a(Hi,"CODE",{});var lw=i(kl);Bh=o(lw,"token_ids_1"),lw.forEach(n),Wh=o(Hi," is "),wl=a(Hi,"CODE",{});var dw=i(wl);Rh=o(dw,"None"),dw.forEach(n),Qh=o(Hi,", this method only returns the first portion of the mask (0s)."),Hi.forEach(n),Do.forEach(n),Hh=d(Ge),si=a(Ge,"DIV",{class:!0});var cw=i(si);w(Xo.$$.fragment,cw),cw.forEach(n),Ge.forEach(n),Cc=d(s),nt=a(s,"H2",{class:!0});var ku=i(nt);Ht=a(ku,"A",{id:!0,class:!0,href:!0});var uw=i(Ht);bl=a(uw,"SPAN",{});var pw=i(bl);w(Jo.$$.fragment,pw),pw.forEach(n),uw.forEach(n),Vh=d(ku),yl=a(ku,"SPAN",{});var hw=i(yl);Uh=o(hw,"FunnelTokenizerFast"),hw.forEach(n),ku.forEach(n),jc=d(s),Ze=a(s,"DIV",{class:!0});var An=i(Ze);w(es.$$.fragment,An),Yh=d(An),ns=a(An,"P",{});var wu=i(ns);Gh=o(wu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=a(wu,"EM",{});var fw=i($l);Zh=o(fw,"tokenizers"),fw.forEach(n),Kh=o(wu," library)."),wu.forEach(n),Xh=d(An),Vt=a(An,"P",{});var $c=i(Vt);ri=a($c,"A",{href:!0});var mw=i(ri);Jh=o(mw,"FunnelTokenizerFast"),mw.forEach(n),ef=o($c," is identical to "),ai=a($c,"A",{href:!0});var gw=i(ai);nf=o(gw,"BertTokenizerFast"),gw.forEach(n),tf=o($c,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$c.forEach(n),of=d(An),ts=a(An,"P",{});var bu=i(ts);sf=o(bu,"Refer to superclass "),ii=a(bu,"A",{href:!0});var _w=i(ii);rf=o(_w,"BertTokenizerFast"),_w.forEach(n),af=o(bu," for usage examples and documentation concerning parameters."),bu.forEach(n),lf=d(An),bn=a(An,"DIV",{class:!0});var Io=i(bn);w(os.$$.fragment,Io),df=d(Io),El=a(Io,"P",{});var Fw=i(El);cf=o(Fw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Fw.forEach(n),uf=d(Io),w(ss.$$.fragment,Io),pf=d(Io),tt=a(Io,"P",{});var Vi=i(tt);hf=o(Vi,"If "),Ml=a(Vi,"CODE",{});var Tw=i(Ml);ff=o(Tw,"token_ids_1"),Tw.forEach(n),mf=o(Vi," is "),zl=a(Vi,"CODE",{});var vw=i(zl);gf=o(vw,"None"),vw.forEach(n),_f=o(Vi,", this method only returns the first portion of the mask (0s)."),Vi.forEach(n),Io.forEach(n),An.forEach(n),xc=d(s),ot=a(s,"H2",{class:!0});var yu=i(ot);Ut=a(yu,"A",{id:!0,class:!0,href:!0});var kw=i(Ut);ql=a(kw,"SPAN",{});var ww=i(ql);w(rs.$$.fragment,ww),ww.forEach(n),kw.forEach(n),Ff=d(yu),Pl=a(yu,"SPAN",{});var bw=i(Pl);Tf=o(bw,"Funnel specific outputs"),bw.forEach(n),yu.forEach(n),Lc=d(s),st=a(s,"DIV",{class:!0});var $u=i(st);w(as.$$.fragment,$u),vf=d($u),is=a($u,"P",{});var Eu=i(is);kf=o(Eu,"Output type of "),li=a(Eu,"A",{href:!0});var yw=i(li);wf=o(yw,"FunnelForPreTraining"),yw.forEach(n),bf=o(Eu,"."),Eu.forEach(n),$u.forEach(n),Ac=d(s),rt=a(s,"DIV",{class:!0});var Mu=i(rt);w(ls.$$.fragment,Mu),yf=d(Mu),ds=a(Mu,"P",{});var zu=i(ds);$f=o(zu,"Output type of "),di=a(zu,"A",{href:!0});var $w=i(di);Ef=o($w,"FunnelForPreTraining"),$w.forEach(n),Mf=o(zu,"."),zu.forEach(n),Mu.forEach(n),Dc=d(s),at=a(s,"H2",{class:!0});var qu=i(at);Yt=a(qu,"A",{id:!0,class:!0,href:!0});var Ew=i(Yt);Cl=a(Ew,"SPAN",{});var Mw=i(Cl);w(cs.$$.fragment,Mw),Mw.forEach(n),Ew.forEach(n),zf=d(qu),jl=a(qu,"SPAN",{});var zw=i(jl);qf=o(zw,"FunnelBaseModel"),zw.forEach(n),qu.forEach(n),Ic=d(s),We=a(s,"DIV",{class:!0});var yn=i(We);w(us.$$.fragment,yn),Pf=d(yn),xl=a(yn,"P",{});var qw=i(xl);Cf=o(qw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qw.forEach(n),jf=d(yn),ps=a(yn,"P",{});var Pu=i(ps);xf=o(Pu,"The Funnel Transformer model was proposed in "),hs=a(Pu,"A",{href:!0,rel:!0});var Pw=i(hs);Lf=o(Pw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Pw.forEach(n),Af=o(Pu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pu.forEach(n),Df=d(yn),fs=a(yn,"P",{});var Cu=i(fs);If=o(Cu,"This model inherits from "),ci=a(Cu,"A",{href:!0});var Cw=i(ci);Sf=o(Cw,"PreTrainedModel"),Cw.forEach(n),Nf=o(Cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cu.forEach(n),Of=d(yn),ms=a(yn,"P",{});var ju=i(ms);Bf=o(ju,"This model is also a PyTorch "),gs=a(ju,"A",{href:!0,rel:!0});var jw=i(gs);Wf=o(jw,"torch.nn.Module"),jw.forEach(n),Rf=o(ju,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ju.forEach(n),Qf=d(yn),Ke=a(yn,"DIV",{class:!0});var Dn=i(Ke);w(_s.$$.fragment,Dn),Hf=d(Dn),it=a(Dn,"P",{});var Ui=i(it);Vf=o(Ui,"The "),ui=a(Ui,"A",{href:!0});var xw=i(ui);Uf=o(xw,"FunnelBaseModel"),xw.forEach(n),Yf=o(Ui," forward method, overrides the "),Ll=a(Ui,"CODE",{});var Lw=i(Ll);Gf=o(Lw,"__call__"),Lw.forEach(n),Zf=o(Ui," special method."),Ui.forEach(n),Kf=d(Dn),w(Gt.$$.fragment,Dn),Xf=d(Dn),Al=a(Dn,"P",{});var Aw=i(Al);Jf=o(Aw,"Example:"),Aw.forEach(n),em=d(Dn),w(Fs.$$.fragment,Dn),Dn.forEach(n),yn.forEach(n),Sc=d(s),lt=a(s,"H2",{class:!0});var xu=i(lt);Zt=a(xu,"A",{id:!0,class:!0,href:!0});var Dw=i(Zt);Dl=a(Dw,"SPAN",{});var Iw=i(Dl);w(Ts.$$.fragment,Iw),Iw.forEach(n),Dw.forEach(n),nm=d(xu),Il=a(xu,"SPAN",{});var Sw=i(Il);tm=o(Sw,"FunnelModel"),Sw.forEach(n),xu.forEach(n),Nc=d(s),Re=a(s,"DIV",{class:!0});var $n=i(Re);w(vs.$$.fragment,$n),om=d($n),Sl=a($n,"P",{});var Nw=i(Sl);sm=o(Nw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Nw.forEach(n),rm=d($n),ks=a($n,"P",{});var Lu=i(ks);am=o(Lu,"The Funnel Transformer model was proposed in "),ws=a(Lu,"A",{href:!0,rel:!0});var Ow=i(ws);im=o(Ow,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ow.forEach(n),lm=o(Lu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Lu.forEach(n),dm=d($n),bs=a($n,"P",{});var Au=i(bs);cm=o(Au,"This model inherits from "),pi=a(Au,"A",{href:!0});var Bw=i(pi);um=o(Bw,"PreTrainedModel"),Bw.forEach(n),pm=o(Au,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Au.forEach(n),hm=d($n),ys=a($n,"P",{});var Du=i(ys);fm=o(Du,"This model is also a PyTorch "),$s=a(Du,"A",{href:!0,rel:!0});var Ww=i($s);mm=o(Ww,"torch.nn.Module"),Ww.forEach(n),gm=o(Du,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Du.forEach(n),_m=d($n),Xe=a($n,"DIV",{class:!0});var In=i(Xe);w(Es.$$.fragment,In),Fm=d(In),dt=a(In,"P",{});var Yi=i(dt);Tm=o(Yi,"The "),hi=a(Yi,"A",{href:!0});var Rw=i(hi);vm=o(Rw,"FunnelModel"),Rw.forEach(n),km=o(Yi," forward method, overrides the "),Nl=a(Yi,"CODE",{});var Qw=i(Nl);wm=o(Qw,"__call__"),Qw.forEach(n),bm=o(Yi," special method."),Yi.forEach(n),ym=d(In),w(Kt.$$.fragment,In),$m=d(In),Ol=a(In,"P",{});var Hw=i(Ol);Em=o(Hw,"Example:"),Hw.forEach(n),Mm=d(In),w(Ms.$$.fragment,In),In.forEach(n),$n.forEach(n),Oc=d(s),ct=a(s,"H2",{class:!0});var Iu=i(ct);Xt=a(Iu,"A",{id:!0,class:!0,href:!0});var Vw=i(Xt);Bl=a(Vw,"SPAN",{});var Uw=i(Bl);w(zs.$$.fragment,Uw),Uw.forEach(n),Vw.forEach(n),zm=d(Iu),Wl=a(Iu,"SPAN",{});var Yw=i(Wl);qm=o(Yw,"FunnelModelForPreTraining"),Yw.forEach(n),Iu.forEach(n),Bc=d(s),ut=a(s,"DIV",{class:!0});var Su=i(ut);w(qs.$$.fragment,Su),Pm=d(Su),Je=a(Su,"DIV",{class:!0});var Sn=i(Je);w(Ps.$$.fragment,Sn),Cm=d(Sn),pt=a(Sn,"P",{});var Gi=i(pt);jm=o(Gi,"The "),fi=a(Gi,"A",{href:!0});var Gw=i(fi);xm=o(Gw,"FunnelForPreTraining"),Gw.forEach(n),Lm=o(Gi," forward method, overrides the "),Rl=a(Gi,"CODE",{});var Zw=i(Rl);Am=o(Zw,"__call__"),Zw.forEach(n),Dm=o(Gi," special method."),Gi.forEach(n),Im=d(Sn),w(Jt.$$.fragment,Sn),Sm=d(Sn),Ql=a(Sn,"P",{});var Kw=i(Ql);Nm=o(Kw,"Examples:"),Kw.forEach(n),Om=d(Sn),w(Cs.$$.fragment,Sn),Sn.forEach(n),Su.forEach(n),Wc=d(s),ht=a(s,"H2",{class:!0});var Nu=i(ht);eo=a(Nu,"A",{id:!0,class:!0,href:!0});var Xw=i(eo);Hl=a(Xw,"SPAN",{});var Jw=i(Hl);w(js.$$.fragment,Jw),Jw.forEach(n),Xw.forEach(n),Bm=d(Nu),Vl=a(Nu,"SPAN",{});var eb=i(Vl);Wm=o(eb,"FunnelForMaskedLM"),eb.forEach(n),Nu.forEach(n),Rc=d(s),Qe=a(s,"DIV",{class:!0});var En=i(Qe);w(xs.$$.fragment,En),Rm=d(En),Ls=a(En,"P",{});var Ou=i(Ls);Qm=o(Ou,"Funnel Transformer Model with a "),Ul=a(Ou,"CODE",{});var nb=i(Ul);Hm=o(nb,"language modeling"),nb.forEach(n),Vm=o(Ou," head on top."),Ou.forEach(n),Um=d(En),As=a(En,"P",{});var Bu=i(As);Ym=o(Bu,"The Funnel Transformer model was proposed in "),Ds=a(Bu,"A",{href:!0,rel:!0});var tb=i(Ds);Gm=o(tb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),tb.forEach(n),Zm=o(Bu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bu.forEach(n),Km=d(En),Is=a(En,"P",{});var Wu=i(Is);Xm=o(Wu,"This model inherits from "),mi=a(Wu,"A",{href:!0});var ob=i(mi);Jm=o(ob,"PreTrainedModel"),ob.forEach(n),eg=o(Wu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wu.forEach(n),ng=d(En),Ss=a(En,"P",{});var Ru=i(Ss);tg=o(Ru,"This model is also a PyTorch "),Ns=a(Ru,"A",{href:!0,rel:!0});var sb=i(Ns);og=o(sb,"torch.nn.Module"),sb.forEach(n),sg=o(Ru,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ru.forEach(n),rg=d(En),en=a(En,"DIV",{class:!0});var Nn=i(en);w(Os.$$.fragment,Nn),ag=d(Nn),ft=a(Nn,"P",{});var Zi=i(ft);ig=o(Zi,"The "),gi=a(Zi,"A",{href:!0});var rb=i(gi);lg=o(rb,"FunnelForMaskedLM"),rb.forEach(n),dg=o(Zi," forward method, overrides the "),Yl=a(Zi,"CODE",{});var ab=i(Yl);cg=o(ab,"__call__"),ab.forEach(n),ug=o(Zi," special method."),Zi.forEach(n),pg=d(Nn),w(no.$$.fragment,Nn),hg=d(Nn),Gl=a(Nn,"P",{});var ib=i(Gl);fg=o(ib,"Example:"),ib.forEach(n),mg=d(Nn),w(Bs.$$.fragment,Nn),Nn.forEach(n),En.forEach(n),Qc=d(s),mt=a(s,"H2",{class:!0});var Qu=i(mt);to=a(Qu,"A",{id:!0,class:!0,href:!0});var lb=i(to);Zl=a(lb,"SPAN",{});var db=i(Zl);w(Ws.$$.fragment,db),db.forEach(n),lb.forEach(n),gg=d(Qu),Kl=a(Qu,"SPAN",{});var cb=i(Kl);_g=o(cb,"FunnelForSequenceClassification"),cb.forEach(n),Qu.forEach(n),Hc=d(s),He=a(s,"DIV",{class:!0});var Mn=i(He);w(Rs.$$.fragment,Mn),Fg=d(Mn),Xl=a(Mn,"P",{});var ub=i(Xl);Tg=o(ub,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),ub.forEach(n),vg=d(Mn),Qs=a(Mn,"P",{});var Hu=i(Qs);kg=o(Hu,"The Funnel Transformer model was proposed in "),Hs=a(Hu,"A",{href:!0,rel:!0});var pb=i(Hs);wg=o(pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pb.forEach(n),bg=o(Hu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Hu.forEach(n),yg=d(Mn),Vs=a(Mn,"P",{});var Vu=i(Vs);$g=o(Vu,"This model inherits from "),_i=a(Vu,"A",{href:!0});var hb=i(_i);Eg=o(hb,"PreTrainedModel"),hb.forEach(n),Mg=o(Vu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vu.forEach(n),zg=d(Mn),Us=a(Mn,"P",{});var Uu=i(Us);qg=o(Uu,"This model is also a PyTorch "),Ys=a(Uu,"A",{href:!0,rel:!0});var fb=i(Ys);Pg=o(fb,"torch.nn.Module"),fb.forEach(n),Cg=o(Uu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Uu.forEach(n),jg=d(Mn),Be=a(Mn,"DIV",{class:!0});var hn=i(Be);w(Gs.$$.fragment,hn),xg=d(hn),gt=a(hn,"P",{});var Ki=i(gt);Lg=o(Ki,"The "),Fi=a(Ki,"A",{href:!0});var mb=i(Fi);Ag=o(mb,"FunnelForSequenceClassification"),mb.forEach(n),Dg=o(Ki," forward method, overrides the "),Jl=a(Ki,"CODE",{});var gb=i(Jl);Ig=o(gb,"__call__"),gb.forEach(n),Sg=o(Ki," special method."),Ki.forEach(n),Ng=d(hn),w(oo.$$.fragment,hn),Og=d(hn),ed=a(hn,"P",{});var _b=i(ed);Bg=o(_b,"Example of single-label classification:"),_b.forEach(n),Wg=d(hn),w(Zs.$$.fragment,hn),Rg=d(hn),nd=a(hn,"P",{});var Fb=i(nd);Qg=o(Fb,"Example of multi-label classification:"),Fb.forEach(n),Hg=d(hn),w(Ks.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Vc=d(s),_t=a(s,"H2",{class:!0});var Yu=i(_t);so=a(Yu,"A",{id:!0,class:!0,href:!0});var Tb=i(so);td=a(Tb,"SPAN",{});var vb=i(td);w(Xs.$$.fragment,vb),vb.forEach(n),Tb.forEach(n),Vg=d(Yu),od=a(Yu,"SPAN",{});var kb=i(od);Ug=o(kb,"FunnelForMultipleChoice"),kb.forEach(n),Yu.forEach(n),Uc=d(s),Ve=a(s,"DIV",{class:!0});var zn=i(Ve);w(Js.$$.fragment,zn),Yg=d(zn),sd=a(zn,"P",{});var wb=i(sd);Gg=o(wb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),wb.forEach(n),Zg=d(zn),er=a(zn,"P",{});var Gu=i(er);Kg=o(Gu,"The Funnel Transformer model was proposed in "),nr=a(Gu,"A",{href:!0,rel:!0});var bb=i(nr);Xg=o(bb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),bb.forEach(n),Jg=o(Gu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Gu.forEach(n),e_=d(zn),tr=a(zn,"P",{});var Zu=i(tr);n_=o(Zu,"This model inherits from "),Ti=a(Zu,"A",{href:!0});var yb=i(Ti);t_=o(yb,"PreTrainedModel"),yb.forEach(n),o_=o(Zu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zu.forEach(n),s_=d(zn),or=a(zn,"P",{});var Ku=i(or);r_=o(Ku,"This model is also a PyTorch "),sr=a(Ku,"A",{href:!0,rel:!0});var $b=i(sr);a_=o($b,"torch.nn.Module"),$b.forEach(n),i_=o(Ku,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ku.forEach(n),l_=d(zn),nn=a(zn,"DIV",{class:!0});var On=i(nn);w(rr.$$.fragment,On),d_=d(On),Ft=a(On,"P",{});var Xi=i(Ft);c_=o(Xi,"The "),vi=a(Xi,"A",{href:!0});var Eb=i(vi);u_=o(Eb,"FunnelForMultipleChoice"),Eb.forEach(n),p_=o(Xi," forward method, overrides the "),rd=a(Xi,"CODE",{});var Mb=i(rd);h_=o(Mb,"__call__"),Mb.forEach(n),f_=o(Xi," special method."),Xi.forEach(n),m_=d(On),w(ro.$$.fragment,On),g_=d(On),ad=a(On,"P",{});var zb=i(ad);__=o(zb,"Example:"),zb.forEach(n),F_=d(On),w(ar.$$.fragment,On),On.forEach(n),zn.forEach(n),Yc=d(s),Tt=a(s,"H2",{class:!0});var Xu=i(Tt);ao=a(Xu,"A",{id:!0,class:!0,href:!0});var qb=i(ao);id=a(qb,"SPAN",{});var Pb=i(id);w(ir.$$.fragment,Pb),Pb.forEach(n),qb.forEach(n),T_=d(Xu),ld=a(Xu,"SPAN",{});var Cb=i(ld);v_=o(Cb,"FunnelForTokenClassification"),Cb.forEach(n),Xu.forEach(n),Gc=d(s),Ue=a(s,"DIV",{class:!0});var qn=i(Ue);w(lr.$$.fragment,qn),k_=d(qn),dd=a(qn,"P",{});var jb=i(dd);w_=o(jb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),jb.forEach(n),b_=d(qn),dr=a(qn,"P",{});var Ju=i(dr);y_=o(Ju,"The Funnel Transformer model was proposed in "),cr=a(Ju,"A",{href:!0,rel:!0});var xb=i(cr);$_=o(xb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xb.forEach(n),E_=o(Ju," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ju.forEach(n),M_=d(qn),ur=a(qn,"P",{});var ep=i(ur);z_=o(ep,"This model inherits from "),ki=a(ep,"A",{href:!0});var Lb=i(ki);q_=o(Lb,"PreTrainedModel"),Lb.forEach(n),P_=o(ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ep.forEach(n),C_=d(qn),pr=a(qn,"P",{});var np=i(pr);j_=o(np,"This model is also a PyTorch "),hr=a(np,"A",{href:!0,rel:!0});var Ab=i(hr);x_=o(Ab,"torch.nn.Module"),Ab.forEach(n),L_=o(np,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),np.forEach(n),A_=d(qn),tn=a(qn,"DIV",{class:!0});var Bn=i(tn);w(fr.$$.fragment,Bn),D_=d(Bn),vt=a(Bn,"P",{});var Ji=i(vt);I_=o(Ji,"The "),wi=a(Ji,"A",{href:!0});var Db=i(wi);S_=o(Db,"FunnelForTokenClassification"),Db.forEach(n),N_=o(Ji," forward method, overrides the "),cd=a(Ji,"CODE",{});var Ib=i(cd);O_=o(Ib,"__call__"),Ib.forEach(n),B_=o(Ji," special method."),Ji.forEach(n),W_=d(Bn),w(io.$$.fragment,Bn),R_=d(Bn),ud=a(Bn,"P",{});var Sb=i(ud);Q_=o(Sb,"Example:"),Sb.forEach(n),H_=d(Bn),w(mr.$$.fragment,Bn),Bn.forEach(n),qn.forEach(n),Zc=d(s),kt=a(s,"H2",{class:!0});var tp=i(kt);lo=a(tp,"A",{id:!0,class:!0,href:!0});var Nb=i(lo);pd=a(Nb,"SPAN",{});var Ob=i(pd);w(gr.$$.fragment,Ob),Ob.forEach(n),Nb.forEach(n),V_=d(tp),hd=a(tp,"SPAN",{});var Bb=i(hd);U_=o(Bb,"FunnelForQuestionAnswering"),Bb.forEach(n),tp.forEach(n),Kc=d(s),Ye=a(s,"DIV",{class:!0});var Pn=i(Ye);w(_r.$$.fragment,Pn),Y_=d(Pn),wt=a(Pn,"P",{});var el=i(wt);G_=o(el,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=a(el,"CODE",{});var Wb=i(fd);Z_=o(Wb,"span start logits"),Wb.forEach(n),K_=o(el," and "),md=a(el,"CODE",{});var Rb=i(md);X_=o(Rb,"span end logits"),Rb.forEach(n),J_=o(el,")."),el.forEach(n),eF=d(Pn),Fr=a(Pn,"P",{});var op=i(Fr);nF=o(op,"The Funnel Transformer model was proposed in "),Tr=a(op,"A",{href:!0,rel:!0});var Qb=i(Tr);tF=o(Qb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Qb.forEach(n),oF=o(op," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),op.forEach(n),sF=d(Pn),vr=a(Pn,"P",{});var sp=i(vr);rF=o(sp,"This model inherits from "),bi=a(sp,"A",{href:!0});var Hb=i(bi);aF=o(Hb,"PreTrainedModel"),Hb.forEach(n),iF=o(sp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sp.forEach(n),lF=d(Pn),kr=a(Pn,"P",{});var rp=i(kr);dF=o(rp,"This model is also a PyTorch "),wr=a(rp,"A",{href:!0,rel:!0});var Vb=i(wr);cF=o(Vb,"torch.nn.Module"),Vb.forEach(n),uF=o(rp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rp.forEach(n),pF=d(Pn),on=a(Pn,"DIV",{class:!0});var Wn=i(on);w(br.$$.fragment,Wn),hF=d(Wn),bt=a(Wn,"P",{});var nl=i(bt);fF=o(nl,"The "),yi=a(nl,"A",{href:!0});var Ub=i(yi);mF=o(Ub,"FunnelForQuestionAnswering"),Ub.forEach(n),gF=o(nl," forward method, overrides the "),gd=a(nl,"CODE",{});var Yb=i(gd);_F=o(Yb,"__call__"),Yb.forEach(n),FF=o(nl," special method."),nl.forEach(n),TF=d(Wn),w(co.$$.fragment,Wn),vF=d(Wn),_d=a(Wn,"P",{});var Gb=i(_d);kF=o(Gb,"Example:"),Gb.forEach(n),wF=d(Wn),w(yr.$$.fragment,Wn),Wn.forEach(n),Pn.forEach(n),Xc=d(s),yt=a(s,"H2",{class:!0});var ap=i(yt);uo=a(ap,"A",{id:!0,class:!0,href:!0});var Zb=i(uo);Fd=a(Zb,"SPAN",{});var Kb=i(Fd);w($r.$$.fragment,Kb),Kb.forEach(n),Zb.forEach(n),bF=d(ap),Td=a(ap,"SPAN",{});var Xb=i(Td);yF=o(Xb,"TFFunnelBaseModel"),Xb.forEach(n),ap.forEach(n),Jc=d(s),je=a(s,"DIV",{class:!0});var fn=i(je);w(Er.$$.fragment,fn),$F=d(fn),vd=a(fn,"P",{});var Jb=i(vd);EF=o(Jb,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Jb.forEach(n),MF=d(fn),Mr=a(fn,"P",{});var ip=i(Mr);zF=o(ip,"The Funnel Transformer model was proposed in "),zr=a(ip,"A",{href:!0,rel:!0});var ey=i(zr);qF=o(ey,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ey.forEach(n),PF=o(ip," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ip.forEach(n),CF=d(fn),qr=a(fn,"P",{});var lp=i(qr);jF=o(lp,"This model inherits from "),$i=a(lp,"A",{href:!0});var ny=i($i);xF=o(ny,"TFPreTrainedModel"),ny.forEach(n),LF=o(lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lp.forEach(n),AF=d(fn),Pr=a(fn,"P",{});var dp=i(Pr);DF=o(dp,"This model is also a "),Cr=a(dp,"A",{href:!0,rel:!0});var ty=i(Cr);IF=o(ty,"tf.keras.Model"),ty.forEach(n),SF=o(dp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),dp.forEach(n),NF=d(fn),w(po.$$.fragment,fn),OF=d(fn),sn=a(fn,"DIV",{class:!0});var Rn=i(sn);w(jr.$$.fragment,Rn),BF=d(Rn),$t=a(Rn,"P",{});var tl=i($t);WF=o(tl,"The "),Ei=a(tl,"A",{href:!0});var oy=i(Ei);RF=o(oy,"TFFunnelBaseModel"),oy.forEach(n),QF=o(tl," forward method, overrides the "),kd=a(tl,"CODE",{});var sy=i(kd);HF=o(sy,"__call__"),sy.forEach(n),VF=o(tl," special method."),tl.forEach(n),UF=d(Rn),w(ho.$$.fragment,Rn),YF=d(Rn),wd=a(Rn,"P",{});var ry=i(wd);GF=o(ry,"Example:"),ry.forEach(n),ZF=d(Rn),w(xr.$$.fragment,Rn),Rn.forEach(n),fn.forEach(n),eu=d(s),Et=a(s,"H2",{class:!0});var cp=i(Et);fo=a(cp,"A",{id:!0,class:!0,href:!0});var ay=i(fo);bd=a(ay,"SPAN",{});var iy=i(bd);w(Lr.$$.fragment,iy),iy.forEach(n),ay.forEach(n),KF=d(cp),yd=a(cp,"SPAN",{});var ly=i(yd);XF=o(ly,"TFFunnelModel"),ly.forEach(n),cp.forEach(n),nu=d(s),xe=a(s,"DIV",{class:!0});var mn=i(xe);w(Ar.$$.fragment,mn),JF=d(mn),$d=a(mn,"P",{});var dy=i($d);eT=o(dy,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),dy.forEach(n),nT=d(mn),Dr=a(mn,"P",{});var up=i(Dr);tT=o(up,"The Funnel Transformer model was proposed in "),Ir=a(up,"A",{href:!0,rel:!0});var cy=i(Ir);oT=o(cy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),cy.forEach(n),sT=o(up," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),up.forEach(n),rT=d(mn),Sr=a(mn,"P",{});var pp=i(Sr);aT=o(pp,"This model inherits from "),Mi=a(pp,"A",{href:!0});var uy=i(Mi);iT=o(uy,"TFPreTrainedModel"),uy.forEach(n),lT=o(pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pp.forEach(n),dT=d(mn),Nr=a(mn,"P",{});var hp=i(Nr);cT=o(hp,"This model is also a "),Or=a(hp,"A",{href:!0,rel:!0});var py=i(Or);uT=o(py,"tf.keras.Model"),py.forEach(n),pT=o(hp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hp.forEach(n),hT=d(mn),w(mo.$$.fragment,mn),fT=d(mn),rn=a(mn,"DIV",{class:!0});var Qn=i(rn);w(Br.$$.fragment,Qn),mT=d(Qn),Mt=a(Qn,"P",{});var ol=i(Mt);gT=o(ol,"The "),zi=a(ol,"A",{href:!0});var hy=i(zi);_T=o(hy,"TFFunnelModel"),hy.forEach(n),FT=o(ol," forward method, overrides the "),Ed=a(ol,"CODE",{});var fy=i(Ed);TT=o(fy,"__call__"),fy.forEach(n),vT=o(ol," special method."),ol.forEach(n),kT=d(Qn),w(go.$$.fragment,Qn),wT=d(Qn),Md=a(Qn,"P",{});var my=i(Md);bT=o(my,"Example:"),my.forEach(n),yT=d(Qn),w(Wr.$$.fragment,Qn),Qn.forEach(n),mn.forEach(n),tu=d(s),zt=a(s,"H2",{class:!0});var fp=i(zt);_o=a(fp,"A",{id:!0,class:!0,href:!0});var gy=i(_o);zd=a(gy,"SPAN",{});var _y=i(zd);w(Rr.$$.fragment,_y),_y.forEach(n),gy.forEach(n),$T=d(fp),qd=a(fp,"SPAN",{});var Fy=i(qd);ET=o(Fy,"TFFunnelModelForPreTraining"),Fy.forEach(n),fp.forEach(n),ou=d(s),Le=a(s,"DIV",{class:!0});var gn=i(Le);w(Qr.$$.fragment,gn),MT=d(gn),Pd=a(gn,"P",{});var Ty=i(Pd);zT=o(Ty,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),Ty.forEach(n),qT=d(gn),Hr=a(gn,"P",{});var mp=i(Hr);PT=o(mp,"The Funnel Transformer model was proposed in "),Vr=a(mp,"A",{href:!0,rel:!0});var vy=i(Vr);CT=o(vy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),vy.forEach(n),jT=o(mp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mp.forEach(n),xT=d(gn),Ur=a(gn,"P",{});var gp=i(Ur);LT=o(gp,"This model inherits from "),qi=a(gp,"A",{href:!0});var ky=i(qi);AT=o(ky,"TFPreTrainedModel"),ky.forEach(n),DT=o(gp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gp.forEach(n),IT=d(gn),Yr=a(gn,"P",{});var _p=i(Yr);ST=o(_p,"This model is also a "),Gr=a(_p,"A",{href:!0,rel:!0});var wy=i(Gr);NT=o(wy,"tf.keras.Model"),wy.forEach(n),OT=o(_p,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_p.forEach(n),BT=d(gn),w(Fo.$$.fragment,gn),WT=d(gn),an=a(gn,"DIV",{class:!0});var Hn=i(an);w(Zr.$$.fragment,Hn),RT=d(Hn),qt=a(Hn,"P",{});var sl=i(qt);QT=o(sl,"The "),Pi=a(sl,"A",{href:!0});var by=i(Pi);HT=o(by,"TFFunnelForPreTraining"),by.forEach(n),VT=o(sl," forward method, overrides the "),Cd=a(sl,"CODE",{});var yy=i(Cd);UT=o(yy,"__call__"),yy.forEach(n),YT=o(sl," special method."),sl.forEach(n),GT=d(Hn),w(To.$$.fragment,Hn),ZT=d(Hn),jd=a(Hn,"P",{});var $y=i(jd);KT=o($y,"Examples:"),$y.forEach(n),XT=d(Hn),w(Kr.$$.fragment,Hn),Hn.forEach(n),gn.forEach(n),su=d(s),Pt=a(s,"H2",{class:!0});var Fp=i(Pt);vo=a(Fp,"A",{id:!0,class:!0,href:!0});var Ey=i(vo);xd=a(Ey,"SPAN",{});var My=i(xd);w(Xr.$$.fragment,My),My.forEach(n),Ey.forEach(n),JT=d(Fp),Ld=a(Fp,"SPAN",{});var zy=i(Ld);ev=o(zy,"TFFunnelForMaskedLM"),zy.forEach(n),Fp.forEach(n),ru=d(s),Ae=a(s,"DIV",{class:!0});var _n=i(Ae);w(Jr.$$.fragment,_n),nv=d(_n),ea=a(_n,"P",{});var Tp=i(ea);tv=o(Tp,"Funnel Model with a "),Ad=a(Tp,"CODE",{});var qy=i(Ad);ov=o(qy,"language modeling"),qy.forEach(n),sv=o(Tp," head on top."),Tp.forEach(n),rv=d(_n),na=a(_n,"P",{});var vp=i(na);av=o(vp,"The Funnel Transformer model was proposed in "),ta=a(vp,"A",{href:!0,rel:!0});var Py=i(ta);iv=o(Py,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Py.forEach(n),lv=o(vp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),vp.forEach(n),dv=d(_n),oa=a(_n,"P",{});var kp=i(oa);cv=o(kp,"This model inherits from "),Ci=a(kp,"A",{href:!0});var Cy=i(Ci);uv=o(Cy,"TFPreTrainedModel"),Cy.forEach(n),pv=o(kp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kp.forEach(n),hv=d(_n),sa=a(_n,"P",{});var wp=i(sa);fv=o(wp,"This model is also a "),ra=a(wp,"A",{href:!0,rel:!0});var jy=i(ra);mv=o(jy,"tf.keras.Model"),jy.forEach(n),gv=o(wp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wp.forEach(n),_v=d(_n),w(ko.$$.fragment,_n),Fv=d(_n),ln=a(_n,"DIV",{class:!0});var Vn=i(ln);w(aa.$$.fragment,Vn),Tv=d(Vn),Ct=a(Vn,"P",{});var rl=i(Ct);vv=o(rl,"The "),ji=a(rl,"A",{href:!0});var xy=i(ji);kv=o(xy,"TFFunnelForMaskedLM"),xy.forEach(n),wv=o(rl," forward method, overrides the "),Dd=a(rl,"CODE",{});var Ly=i(Dd);bv=o(Ly,"__call__"),Ly.forEach(n),yv=o(rl," special method."),rl.forEach(n),$v=d(Vn),w(wo.$$.fragment,Vn),Ev=d(Vn),Id=a(Vn,"P",{});var Ay=i(Id);Mv=o(Ay,"Example:"),Ay.forEach(n),zv=d(Vn),w(ia.$$.fragment,Vn),Vn.forEach(n),_n.forEach(n),au=d(s),jt=a(s,"H2",{class:!0});var bp=i(jt);bo=a(bp,"A",{id:!0,class:!0,href:!0});var Dy=i(bo);Sd=a(Dy,"SPAN",{});var Iy=i(Sd);w(la.$$.fragment,Iy),Iy.forEach(n),Dy.forEach(n),qv=d(bp),Nd=a(bp,"SPAN",{});var Sy=i(Nd);Pv=o(Sy,"TFFunnelForSequenceClassification"),Sy.forEach(n),bp.forEach(n),iu=d(s),De=a(s,"DIV",{class:!0});var Fn=i(De);w(da.$$.fragment,Fn),Cv=d(Fn),Od=a(Fn,"P",{});var Ny=i(Od);jv=o(Ny,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Ny.forEach(n),xv=d(Fn),ca=a(Fn,"P",{});var yp=i(ca);Lv=o(yp,"The Funnel Transformer model was proposed in "),ua=a(yp,"A",{href:!0,rel:!0});var Oy=i(ua);Av=o(Oy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Oy.forEach(n),Dv=o(yp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),yp.forEach(n),Iv=d(Fn),pa=a(Fn,"P",{});var $p=i(pa);Sv=o($p,"This model inherits from "),xi=a($p,"A",{href:!0});var By=i(xi);Nv=o(By,"TFPreTrainedModel"),By.forEach(n),Ov=o($p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$p.forEach(n),Bv=d(Fn),ha=a(Fn,"P",{});var Ep=i(ha);Wv=o(Ep,"This model is also a "),fa=a(Ep,"A",{href:!0,rel:!0});var Wy=i(fa);Rv=o(Wy,"tf.keras.Model"),Wy.forEach(n),Qv=o(Ep,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ep.forEach(n),Hv=d(Fn),w(yo.$$.fragment,Fn),Vv=d(Fn),dn=a(Fn,"DIV",{class:!0});var Un=i(dn);w(ma.$$.fragment,Un),Uv=d(Un),xt=a(Un,"P",{});var al=i(xt);Yv=o(al,"The "),Li=a(al,"A",{href:!0});var Ry=i(Li);Gv=o(Ry,"TFFunnelForSequenceClassification"),Ry.forEach(n),Zv=o(al," forward method, overrides the "),Bd=a(al,"CODE",{});var Qy=i(Bd);Kv=o(Qy,"__call__"),Qy.forEach(n),Xv=o(al," special method."),al.forEach(n),Jv=d(Un),w($o.$$.fragment,Un),ek=d(Un),Wd=a(Un,"P",{});var Hy=i(Wd);nk=o(Hy,"Example:"),Hy.forEach(n),tk=d(Un),w(ga.$$.fragment,Un),Un.forEach(n),Fn.forEach(n),lu=d(s),Lt=a(s,"H2",{class:!0});var Mp=i(Lt);Eo=a(Mp,"A",{id:!0,class:!0,href:!0});var Vy=i(Eo);Rd=a(Vy,"SPAN",{});var Uy=i(Rd);w(_a.$$.fragment,Uy),Uy.forEach(n),Vy.forEach(n),ok=d(Mp),Qd=a(Mp,"SPAN",{});var Yy=i(Qd);sk=o(Yy,"TFFunnelForMultipleChoice"),Yy.forEach(n),Mp.forEach(n),du=d(s),Ie=a(s,"DIV",{class:!0});var Tn=i(Ie);w(Fa.$$.fragment,Tn),rk=d(Tn),Hd=a(Tn,"P",{});var Gy=i(Hd);ak=o(Gy,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Gy.forEach(n),ik=d(Tn),Ta=a(Tn,"P",{});var zp=i(Ta);lk=o(zp,"The Funnel Transformer model was proposed in "),va=a(zp,"A",{href:!0,rel:!0});var Zy=i(va);dk=o(Zy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Zy.forEach(n),ck=o(zp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zp.forEach(n),uk=d(Tn),ka=a(Tn,"P",{});var qp=i(ka);pk=o(qp,"This model inherits from "),Ai=a(qp,"A",{href:!0});var Ky=i(Ai);hk=o(Ky,"TFPreTrainedModel"),Ky.forEach(n),fk=o(qp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qp.forEach(n),mk=d(Tn),wa=a(Tn,"P",{});var Pp=i(wa);gk=o(Pp,"This model is also a "),ba=a(Pp,"A",{href:!0,rel:!0});var Xy=i(ba);_k=o(Xy,"tf.keras.Model"),Xy.forEach(n),Fk=o(Pp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pp.forEach(n),Tk=d(Tn),w(Mo.$$.fragment,Tn),vk=d(Tn),cn=a(Tn,"DIV",{class:!0});var Yn=i(cn);w(ya.$$.fragment,Yn),kk=d(Yn),At=a(Yn,"P",{});var il=i(At);wk=o(il,"The "),Di=a(il,"A",{href:!0});var Jy=i(Di);bk=o(Jy,"TFFunnelForMultipleChoice"),Jy.forEach(n),yk=o(il," forward method, overrides the "),Vd=a(il,"CODE",{});var e$=i(Vd);$k=o(e$,"__call__"),e$.forEach(n),Ek=o(il," special method."),il.forEach(n),Mk=d(Yn),w(zo.$$.fragment,Yn),zk=d(Yn),Ud=a(Yn,"P",{});var n$=i(Ud);qk=o(n$,"Example:"),n$.forEach(n),Pk=d(Yn),w($a.$$.fragment,Yn),Yn.forEach(n),Tn.forEach(n),cu=d(s),Dt=a(s,"H2",{class:!0});var Cp=i(Dt);qo=a(Cp,"A",{id:!0,class:!0,href:!0});var t$=i(qo);Yd=a(t$,"SPAN",{});var o$=i(Yd);w(Ea.$$.fragment,o$),o$.forEach(n),t$.forEach(n),Ck=d(Cp),Gd=a(Cp,"SPAN",{});var s$=i(Gd);jk=o(s$,"TFFunnelForTokenClassification"),s$.forEach(n),Cp.forEach(n),uu=d(s),Se=a(s,"DIV",{class:!0});var vn=i(Se);w(Ma.$$.fragment,vn),xk=d(vn),Zd=a(vn,"P",{});var r$=i(Zd);Lk=o(r$,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),r$.forEach(n),Ak=d(vn),za=a(vn,"P",{});var jp=i(za);Dk=o(jp,"The Funnel Transformer model was proposed in "),qa=a(jp,"A",{href:!0,rel:!0});var a$=i(qa);Ik=o(a$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),a$.forEach(n),Sk=o(jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jp.forEach(n),Nk=d(vn),Pa=a(vn,"P",{});var xp=i(Pa);Ok=o(xp,"This model inherits from "),Ii=a(xp,"A",{href:!0});var i$=i(Ii);Bk=o(i$,"TFPreTrainedModel"),i$.forEach(n),Wk=o(xp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xp.forEach(n),Rk=d(vn),Ca=a(vn,"P",{});var Lp=i(Ca);Qk=o(Lp,"This model is also a "),ja=a(Lp,"A",{href:!0,rel:!0});var l$=i(ja);Hk=o(l$,"tf.keras.Model"),l$.forEach(n),Vk=o(Lp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lp.forEach(n),Uk=d(vn),w(Po.$$.fragment,vn),Yk=d(vn),un=a(vn,"DIV",{class:!0});var Gn=i(un);w(xa.$$.fragment,Gn),Gk=d(Gn),It=a(Gn,"P",{});var ll=i(It);Zk=o(ll,"The "),Si=a(ll,"A",{href:!0});var d$=i(Si);Kk=o(d$,"TFFunnelForTokenClassification"),d$.forEach(n),Xk=o(ll," forward method, overrides the "),Kd=a(ll,"CODE",{});var c$=i(Kd);Jk=o(c$,"__call__"),c$.forEach(n),e1=o(ll," special method."),ll.forEach(n),n1=d(Gn),w(Co.$$.fragment,Gn),t1=d(Gn),Xd=a(Gn,"P",{});var u$=i(Xd);o1=o(u$,"Example:"),u$.forEach(n),s1=d(Gn),w(La.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),pu=d(s),St=a(s,"H2",{class:!0});var Ap=i(St);jo=a(Ap,"A",{id:!0,class:!0,href:!0});var p$=i(jo);Jd=a(p$,"SPAN",{});var h$=i(Jd);w(Aa.$$.fragment,h$),h$.forEach(n),p$.forEach(n),r1=d(Ap),ec=a(Ap,"SPAN",{});var f$=i(ec);a1=o(f$,"TFFunnelForQuestionAnswering"),f$.forEach(n),Ap.forEach(n),hu=d(s),Ne=a(s,"DIV",{class:!0});var kn=i(Ne);w(Da.$$.fragment,kn),i1=d(kn),Nt=a(kn,"P",{});var dl=i(Nt);l1=o(dl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=a(dl,"CODE",{});var m$=i(nc);d1=o(m$,"span start logits"),m$.forEach(n),c1=o(dl," and "),tc=a(dl,"CODE",{});var g$=i(tc);u1=o(g$,"span end logits"),g$.forEach(n),p1=o(dl,")."),dl.forEach(n),h1=d(kn),Ia=a(kn,"P",{});var Dp=i(Ia);f1=o(Dp,"The Funnel Transformer model was proposed in "),Sa=a(Dp,"A",{href:!0,rel:!0});var _$=i(Sa);m1=o(_$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),_$.forEach(n),g1=o(Dp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Dp.forEach(n),_1=d(kn),Na=a(kn,"P",{});var Ip=i(Na);F1=o(Ip,"This model inherits from "),Ni=a(Ip,"A",{href:!0});var F$=i(Ni);T1=o(F$,"TFPreTrainedModel"),F$.forEach(n),v1=o(Ip,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ip.forEach(n),k1=d(kn),Oa=a(kn,"P",{});var Sp=i(Oa);w1=o(Sp,"This model is also a "),Ba=a(Sp,"A",{href:!0,rel:!0});var T$=i(Ba);b1=o(T$,"tf.keras.Model"),T$.forEach(n),y1=o(Sp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Sp.forEach(n),$1=d(kn),w(xo.$$.fragment,kn),E1=d(kn),pn=a(kn,"DIV",{class:!0});var Zn=i(pn);w(Wa.$$.fragment,Zn),M1=d(Zn),Ot=a(Zn,"P",{});var cl=i(Ot);z1=o(cl,"The "),Oi=a(cl,"A",{href:!0});var v$=i(Oi);q1=o(v$,"TFFunnelForQuestionAnswering"),v$.forEach(n),P1=o(cl," forward method, overrides the "),oc=a(cl,"CODE",{});var k$=i(oc);C1=o(k$,"__call__"),k$.forEach(n),j1=o(cl," special method."),cl.forEach(n),x1=d(Zn),w(Lo.$$.fragment,Zn),L1=d(Zn),sc=a(Zn,"P",{});var w$=i(sc);A1=o(w$,"Example:"),w$.forEach(n),D1=d(Zn),w(Ra.$$.fragment,Zn),Zn.forEach(n),kn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(X$)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Fe,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Te,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ve,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ke,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Ya,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(So,"href","https://huggingface.co/sgugger"),c(So,"rel","nofollow"),c(No,"href","https://github.com/laiguokun/Funnel-Transformer"),c(No,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Kn,"class","relative group"),c(Ga,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelModel"),c(Za,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(Ka,"href","/docs/transformers/pr_15987/en/main_classes/configuration#transformers.PretrainedConfig"),c(Xa,"href","/docs/transformers/pr_15987/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ja,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizer"),c(ei,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer"),c(ni,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Qt,"class","docstring"),c(wn,"class","docstring"),c(si,"class","docstring"),c(Pe,"class","docstring"),c(Ht,"id","transformers.FunnelTokenizerFast"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ri,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ai,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizerFast"),c(ii,"href","/docs/transformers/pr_15987/en/model_doc/bert#transformers.BertTokenizerFast"),c(bn,"class","docstring"),c(Ze,"class","docstring"),c(Ut,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(li,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(di,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(ci,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(ui,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ke,"class","docstring"),c(We,"class","docstring"),c(Zt,"id","transformers.FunnelModel"),c(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ws,"href","https://arxiv.org/abs/2006.03236"),c(ws,"rel","nofollow"),c(pi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(hi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Re,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(fi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(ut,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(mi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(Ns,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ns,"rel","nofollow"),c(gi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Qe,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Hs,"href","https://arxiv.org/abs/2006.03236"),c(Hs,"rel","nofollow"),c(_i,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(Ys,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ys,"rel","nofollow"),c(Fi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(He,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(nr,"href","https://arxiv.org/abs/2006.03236"),c(nr,"rel","nofollow"),c(Ti,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(sr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(sr,"rel","nofollow"),c(vi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(Ve,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(Tt,"class","relative group"),c(cr,"href","https://arxiv.org/abs/2006.03236"),c(cr,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(hr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(hr,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ue,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(kt,"class","relative group"),c(Tr,"href","https://arxiv.org/abs/2006.03236"),c(Tr,"rel","nofollow"),c(bi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.PreTrainedModel"),c(wr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(wr,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ye,"class","docstring"),c(uo,"id","transformers.TFFunnelBaseModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFFunnelBaseModel"),c(yt,"class","relative group"),c(zr,"href","https://arxiv.org/abs/2006.03236"),c(zr,"rel","nofollow"),c($i,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(je,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Ir,"href","https://arxiv.org/abs/2006.03236"),c(Ir,"rel","nofollow"),c(Mi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Or,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Or,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(xe,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Vr,"href","https://arxiv.org/abs/2006.03236"),c(Vr,"rel","nofollow"),c(qi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Gr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Gr,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(vo,"id","transformers.TFFunnelForMaskedLM"),c(vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vo,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ta,"href","https://arxiv.org/abs/2006.03236"),c(ta,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(ra,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ra,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Ae,"class","docstring"),c(bo,"id","transformers.TFFunnelForSequenceClassification"),c(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bo,"href","#transformers.TFFunnelForSequenceClassification"),c(jt,"class","relative group"),c(ua,"href","https://arxiv.org/abs/2006.03236"),c(ua,"rel","nofollow"),c(xi,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(fa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(fa,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c(va,"href","https://arxiv.org/abs/2006.03236"),c(va,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ba,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ie,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(qa,"href","https://arxiv.org/abs/2006.03236"),c(qa,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(ja,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ja,"rel","nofollow"),c(Si,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Se,"class","docstring"),c(jo,"id","transformers.TFFunnelForQuestionAnswering"),c(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jo,"href","#transformers.TFFunnelForQuestionAnswering"),c(St,"class","relative group"),c(Sa,"href","https://arxiv.org/abs/2006.03236"),c(Sa,"rel","nofollow"),c(Ni,"href","/docs/transformers/pr_15987/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ba,"rel","nofollow"),c(Oi,"href","/docs/transformers/pr_15987/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(pn,"class","docstring"),c(Ne,"class","docstring")},m(s,f){e(document.head,p),h(s,M,f),h(s,m,f),e(m,g),e(g,T),b(F,T,null),e(m,_),e(m,z),e(z,ce),h(s,G,f),h(s,q,f),e(q,J),e(J,I),b(ne,I,null),e(q,ue),e(q,S),e(S,pe),h(s,ie,f),h(s,Y,f),e(Y,L),e(Y,te),e(te,Z),e(Y,P),h(s,j,f),h(s,oe,f),e(oe,R),h(s,le,f),h(s,se,f),e(se,N),e(N,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,Q),e(ee,me),e(ee,O),e(O,A),e(O,re),e(re,H),e(O,ge),e(O,u),e(u,v),e(O,K),e(O,Fe),e(Fe,we),e(O,D),e(O,Te),e(Te,be),e(O,ye),e(O,x),e(x,V),e(O,$e),e(O,ve),e(ve,U),e(O,Ee),e(O,ke),e(ke,_e),e(O,Me),e(O,Ya),e(Ya,Np),e(O,Op),h(s,Ec,f),h(s,xn,f),e(xn,Bp),e(xn,So),e(So,Wp),e(xn,Rp),e(xn,No),e(No,Qp),e(xn,Hp),h(s,Mc,f),h(s,Kn,f),e(Kn,Bt),e(Bt,ul),b(Oo,ul,null),e(Kn,Vp),e(Kn,pl),e(pl,Up),h(s,zc,f),h(s,Cn,f),b(Bo,Cn,null),e(Cn,Yp),e(Cn,jn),e(jn,Gp),e(jn,Ga),e(Ga,Zp),e(jn,Kp),e(jn,Za),e(Za,Xp),e(jn,Jp),e(jn,Wo),e(Wo,eh),e(jn,nh),e(Cn,th),e(Cn,Xn),e(Xn,oh),e(Xn,Ka),e(Ka,sh),e(Xn,rh),e(Xn,Xa),e(Xa,ah),e(Xn,ih),h(s,qc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,hl),b(Ro,hl,null),e(Jn,lh),e(Jn,fl),e(fl,dh),h(s,Pc,f),h(s,Pe,f),b(Qo,Pe,null),e(Pe,ch),e(Pe,ml),e(ml,uh),e(Pe,ph),e(Pe,Rt),e(Rt,Ja),e(Ja,hh),e(Rt,fh),e(Rt,ei),e(ei,mh),e(Rt,gh),e(Pe,_h),e(Pe,Ho),e(Ho,Fh),e(Ho,ni),e(ni,Th),e(Ho,vh),e(Pe,kh),e(Pe,Ln),b(Vo,Ln,null),e(Ln,wh),e(Ln,gl),e(gl,bh),e(Ln,yh),e(Ln,Uo),e(Uo,ti),e(ti,$h),e(ti,_l),e(_l,Eh),e(Uo,Mh),e(Uo,oi),e(oi,zh),e(oi,Fl),e(Fl,qh),e(Pe,Ph),e(Pe,Qt),b(Yo,Qt,null),e(Qt,Ch),e(Qt,Go),e(Go,jh),e(Go,Tl),e(Tl,xh),e(Go,Lh),e(Pe,Ah),e(Pe,wn),b(Zo,wn,null),e(wn,Dh),e(wn,vl),e(vl,Ih),e(wn,Sh),b(Ko,wn,null),e(wn,Nh),e(wn,et),e(et,Oh),e(et,kl),e(kl,Bh),e(et,Wh),e(et,wl),e(wl,Rh),e(et,Qh),e(Pe,Hh),e(Pe,si),b(Xo,si,null),h(s,Cc,f),h(s,nt,f),e(nt,Ht),e(Ht,bl),b(Jo,bl,null),e(nt,Vh),e(nt,yl),e(yl,Uh),h(s,jc,f),h(s,Ze,f),b(es,Ze,null),e(Ze,Yh),e(Ze,ns),e(ns,Gh),e(ns,$l),e($l,Zh),e(ns,Kh),e(Ze,Xh),e(Ze,Vt),e(Vt,ri),e(ri,Jh),e(Vt,ef),e(Vt,ai),e(ai,nf),e(Vt,tf),e(Ze,of),e(Ze,ts),e(ts,sf),e(ts,ii),e(ii,rf),e(ts,af),e(Ze,lf),e(Ze,bn),b(os,bn,null),e(bn,df),e(bn,El),e(El,cf),e(bn,uf),b(ss,bn,null),e(bn,pf),e(bn,tt),e(tt,hf),e(tt,Ml),e(Ml,ff),e(tt,mf),e(tt,zl),e(zl,gf),e(tt,_f),h(s,xc,f),h(s,ot,f),e(ot,Ut),e(Ut,ql),b(rs,ql,null),e(ot,Ff),e(ot,Pl),e(Pl,Tf),h(s,Lc,f),h(s,st,f),b(as,st,null),e(st,vf),e(st,is),e(is,kf),e(is,li),e(li,wf),e(is,bf),h(s,Ac,f),h(s,rt,f),b(ls,rt,null),e(rt,yf),e(rt,ds),e(ds,$f),e(ds,di),e(di,Ef),e(ds,Mf),h(s,Dc,f),h(s,at,f),e(at,Yt),e(Yt,Cl),b(cs,Cl,null),e(at,zf),e(at,jl),e(jl,qf),h(s,Ic,f),h(s,We,f),b(us,We,null),e(We,Pf),e(We,xl),e(xl,Cf),e(We,jf),e(We,ps),e(ps,xf),e(ps,hs),e(hs,Lf),e(ps,Af),e(We,Df),e(We,fs),e(fs,If),e(fs,ci),e(ci,Sf),e(fs,Nf),e(We,Of),e(We,ms),e(ms,Bf),e(ms,gs),e(gs,Wf),e(ms,Rf),e(We,Qf),e(We,Ke),b(_s,Ke,null),e(Ke,Hf),e(Ke,it),e(it,Vf),e(it,ui),e(ui,Uf),e(it,Yf),e(it,Ll),e(Ll,Gf),e(it,Zf),e(Ke,Kf),b(Gt,Ke,null),e(Ke,Xf),e(Ke,Al),e(Al,Jf),e(Ke,em),b(Fs,Ke,null),h(s,Sc,f),h(s,lt,f),e(lt,Zt),e(Zt,Dl),b(Ts,Dl,null),e(lt,nm),e(lt,Il),e(Il,tm),h(s,Nc,f),h(s,Re,f),b(vs,Re,null),e(Re,om),e(Re,Sl),e(Sl,sm),e(Re,rm),e(Re,ks),e(ks,am),e(ks,ws),e(ws,im),e(ks,lm),e(Re,dm),e(Re,bs),e(bs,cm),e(bs,pi),e(pi,um),e(bs,pm),e(Re,hm),e(Re,ys),e(ys,fm),e(ys,$s),e($s,mm),e(ys,gm),e(Re,_m),e(Re,Xe),b(Es,Xe,null),e(Xe,Fm),e(Xe,dt),e(dt,Tm),e(dt,hi),e(hi,vm),e(dt,km),e(dt,Nl),e(Nl,wm),e(dt,bm),e(Xe,ym),b(Kt,Xe,null),e(Xe,$m),e(Xe,Ol),e(Ol,Em),e(Xe,Mm),b(Ms,Xe,null),h(s,Oc,f),h(s,ct,f),e(ct,Xt),e(Xt,Bl),b(zs,Bl,null),e(ct,zm),e(ct,Wl),e(Wl,qm),h(s,Bc,f),h(s,ut,f),b(qs,ut,null),e(ut,Pm),e(ut,Je),b(Ps,Je,null),e(Je,Cm),e(Je,pt),e(pt,jm),e(pt,fi),e(fi,xm),e(pt,Lm),e(pt,Rl),e(Rl,Am),e(pt,Dm),e(Je,Im),b(Jt,Je,null),e(Je,Sm),e(Je,Ql),e(Ql,Nm),e(Je,Om),b(Cs,Je,null),h(s,Wc,f),h(s,ht,f),e(ht,eo),e(eo,Hl),b(js,Hl,null),e(ht,Bm),e(ht,Vl),e(Vl,Wm),h(s,Rc,f),h(s,Qe,f),b(xs,Qe,null),e(Qe,Rm),e(Qe,Ls),e(Ls,Qm),e(Ls,Ul),e(Ul,Hm),e(Ls,Vm),e(Qe,Um),e(Qe,As),e(As,Ym),e(As,Ds),e(Ds,Gm),e(As,Zm),e(Qe,Km),e(Qe,Is),e(Is,Xm),e(Is,mi),e(mi,Jm),e(Is,eg),e(Qe,ng),e(Qe,Ss),e(Ss,tg),e(Ss,Ns),e(Ns,og),e(Ss,sg),e(Qe,rg),e(Qe,en),b(Os,en,null),e(en,ag),e(en,ft),e(ft,ig),e(ft,gi),e(gi,lg),e(ft,dg),e(ft,Yl),e(Yl,cg),e(ft,ug),e(en,pg),b(no,en,null),e(en,hg),e(en,Gl),e(Gl,fg),e(en,mg),b(Bs,en,null),h(s,Qc,f),h(s,mt,f),e(mt,to),e(to,Zl),b(Ws,Zl,null),e(mt,gg),e(mt,Kl),e(Kl,_g),h(s,Hc,f),h(s,He,f),b(Rs,He,null),e(He,Fg),e(He,Xl),e(Xl,Tg),e(He,vg),e(He,Qs),e(Qs,kg),e(Qs,Hs),e(Hs,wg),e(Qs,bg),e(He,yg),e(He,Vs),e(Vs,$g),e(Vs,_i),e(_i,Eg),e(Vs,Mg),e(He,zg),e(He,Us),e(Us,qg),e(Us,Ys),e(Ys,Pg),e(Us,Cg),e(He,jg),e(He,Be),b(Gs,Be,null),e(Be,xg),e(Be,gt),e(gt,Lg),e(gt,Fi),e(Fi,Ag),e(gt,Dg),e(gt,Jl),e(Jl,Ig),e(gt,Sg),e(Be,Ng),b(oo,Be,null),e(Be,Og),e(Be,ed),e(ed,Bg),e(Be,Wg),b(Zs,Be,null),e(Be,Rg),e(Be,nd),e(nd,Qg),e(Be,Hg),b(Ks,Be,null),h(s,Vc,f),h(s,_t,f),e(_t,so),e(so,td),b(Xs,td,null),e(_t,Vg),e(_t,od),e(od,Ug),h(s,Uc,f),h(s,Ve,f),b(Js,Ve,null),e(Ve,Yg),e(Ve,sd),e(sd,Gg),e(Ve,Zg),e(Ve,er),e(er,Kg),e(er,nr),e(nr,Xg),e(er,Jg),e(Ve,e_),e(Ve,tr),e(tr,n_),e(tr,Ti),e(Ti,t_),e(tr,o_),e(Ve,s_),e(Ve,or),e(or,r_),e(or,sr),e(sr,a_),e(or,i_),e(Ve,l_),e(Ve,nn),b(rr,nn,null),e(nn,d_),e(nn,Ft),e(Ft,c_),e(Ft,vi),e(vi,u_),e(Ft,p_),e(Ft,rd),e(rd,h_),e(Ft,f_),e(nn,m_),b(ro,nn,null),e(nn,g_),e(nn,ad),e(ad,__),e(nn,F_),b(ar,nn,null),h(s,Yc,f),h(s,Tt,f),e(Tt,ao),e(ao,id),b(ir,id,null),e(Tt,T_),e(Tt,ld),e(ld,v_),h(s,Gc,f),h(s,Ue,f),b(lr,Ue,null),e(Ue,k_),e(Ue,dd),e(dd,w_),e(Ue,b_),e(Ue,dr),e(dr,y_),e(dr,cr),e(cr,$_),e(dr,E_),e(Ue,M_),e(Ue,ur),e(ur,z_),e(ur,ki),e(ki,q_),e(ur,P_),e(Ue,C_),e(Ue,pr),e(pr,j_),e(pr,hr),e(hr,x_),e(pr,L_),e(Ue,A_),e(Ue,tn),b(fr,tn,null),e(tn,D_),e(tn,vt),e(vt,I_),e(vt,wi),e(wi,S_),e(vt,N_),e(vt,cd),e(cd,O_),e(vt,B_),e(tn,W_),b(io,tn,null),e(tn,R_),e(tn,ud),e(ud,Q_),e(tn,H_),b(mr,tn,null),h(s,Zc,f),h(s,kt,f),e(kt,lo),e(lo,pd),b(gr,pd,null),e(kt,V_),e(kt,hd),e(hd,U_),h(s,Kc,f),h(s,Ye,f),b(_r,Ye,null),e(Ye,Y_),e(Ye,wt),e(wt,G_),e(wt,fd),e(fd,Z_),e(wt,K_),e(wt,md),e(md,X_),e(wt,J_),e(Ye,eF),e(Ye,Fr),e(Fr,nF),e(Fr,Tr),e(Tr,tF),e(Fr,oF),e(Ye,sF),e(Ye,vr),e(vr,rF),e(vr,bi),e(bi,aF),e(vr,iF),e(Ye,lF),e(Ye,kr),e(kr,dF),e(kr,wr),e(wr,cF),e(kr,uF),e(Ye,pF),e(Ye,on),b(br,on,null),e(on,hF),e(on,bt),e(bt,fF),e(bt,yi),e(yi,mF),e(bt,gF),e(bt,gd),e(gd,_F),e(bt,FF),e(on,TF),b(co,on,null),e(on,vF),e(on,_d),e(_d,kF),e(on,wF),b(yr,on,null),h(s,Xc,f),h(s,yt,f),e(yt,uo),e(uo,Fd),b($r,Fd,null),e(yt,bF),e(yt,Td),e(Td,yF),h(s,Jc,f),h(s,je,f),b(Er,je,null),e(je,$F),e(je,vd),e(vd,EF),e(je,MF),e(je,Mr),e(Mr,zF),e(Mr,zr),e(zr,qF),e(Mr,PF),e(je,CF),e(je,qr),e(qr,jF),e(qr,$i),e($i,xF),e(qr,LF),e(je,AF),e(je,Pr),e(Pr,DF),e(Pr,Cr),e(Cr,IF),e(Pr,SF),e(je,NF),b(po,je,null),e(je,OF),e(je,sn),b(jr,sn,null),e(sn,BF),e(sn,$t),e($t,WF),e($t,Ei),e(Ei,RF),e($t,QF),e($t,kd),e(kd,HF),e($t,VF),e(sn,UF),b(ho,sn,null),e(sn,YF),e(sn,wd),e(wd,GF),e(sn,ZF),b(xr,sn,null),h(s,eu,f),h(s,Et,f),e(Et,fo),e(fo,bd),b(Lr,bd,null),e(Et,KF),e(Et,yd),e(yd,XF),h(s,nu,f),h(s,xe,f),b(Ar,xe,null),e(xe,JF),e(xe,$d),e($d,eT),e(xe,nT),e(xe,Dr),e(Dr,tT),e(Dr,Ir),e(Ir,oT),e(Dr,sT),e(xe,rT),e(xe,Sr),e(Sr,aT),e(Sr,Mi),e(Mi,iT),e(Sr,lT),e(xe,dT),e(xe,Nr),e(Nr,cT),e(Nr,Or),e(Or,uT),e(Nr,pT),e(xe,hT),b(mo,xe,null),e(xe,fT),e(xe,rn),b(Br,rn,null),e(rn,mT),e(rn,Mt),e(Mt,gT),e(Mt,zi),e(zi,_T),e(Mt,FT),e(Mt,Ed),e(Ed,TT),e(Mt,vT),e(rn,kT),b(go,rn,null),e(rn,wT),e(rn,Md),e(Md,bT),e(rn,yT),b(Wr,rn,null),h(s,tu,f),h(s,zt,f),e(zt,_o),e(_o,zd),b(Rr,zd,null),e(zt,$T),e(zt,qd),e(qd,ET),h(s,ou,f),h(s,Le,f),b(Qr,Le,null),e(Le,MT),e(Le,Pd),e(Pd,zT),e(Le,qT),e(Le,Hr),e(Hr,PT),e(Hr,Vr),e(Vr,CT),e(Hr,jT),e(Le,xT),e(Le,Ur),e(Ur,LT),e(Ur,qi),e(qi,AT),e(Ur,DT),e(Le,IT),e(Le,Yr),e(Yr,ST),e(Yr,Gr),e(Gr,NT),e(Yr,OT),e(Le,BT),b(Fo,Le,null),e(Le,WT),e(Le,an),b(Zr,an,null),e(an,RT),e(an,qt),e(qt,QT),e(qt,Pi),e(Pi,HT),e(qt,VT),e(qt,Cd),e(Cd,UT),e(qt,YT),e(an,GT),b(To,an,null),e(an,ZT),e(an,jd),e(jd,KT),e(an,XT),b(Kr,an,null),h(s,su,f),h(s,Pt,f),e(Pt,vo),e(vo,xd),b(Xr,xd,null),e(Pt,JT),e(Pt,Ld),e(Ld,ev),h(s,ru,f),h(s,Ae,f),b(Jr,Ae,null),e(Ae,nv),e(Ae,ea),e(ea,tv),e(ea,Ad),e(Ad,ov),e(ea,sv),e(Ae,rv),e(Ae,na),e(na,av),e(na,ta),e(ta,iv),e(na,lv),e(Ae,dv),e(Ae,oa),e(oa,cv),e(oa,Ci),e(Ci,uv),e(oa,pv),e(Ae,hv),e(Ae,sa),e(sa,fv),e(sa,ra),e(ra,mv),e(sa,gv),e(Ae,_v),b(ko,Ae,null),e(Ae,Fv),e(Ae,ln),b(aa,ln,null),e(ln,Tv),e(ln,Ct),e(Ct,vv),e(Ct,ji),e(ji,kv),e(Ct,wv),e(Ct,Dd),e(Dd,bv),e(Ct,yv),e(ln,$v),b(wo,ln,null),e(ln,Ev),e(ln,Id),e(Id,Mv),e(ln,zv),b(ia,ln,null),h(s,au,f),h(s,jt,f),e(jt,bo),e(bo,Sd),b(la,Sd,null),e(jt,qv),e(jt,Nd),e(Nd,Pv),h(s,iu,f),h(s,De,f),b(da,De,null),e(De,Cv),e(De,Od),e(Od,jv),e(De,xv),e(De,ca),e(ca,Lv),e(ca,ua),e(ua,Av),e(ca,Dv),e(De,Iv),e(De,pa),e(pa,Sv),e(pa,xi),e(xi,Nv),e(pa,Ov),e(De,Bv),e(De,ha),e(ha,Wv),e(ha,fa),e(fa,Rv),e(ha,Qv),e(De,Hv),b(yo,De,null),e(De,Vv),e(De,dn),b(ma,dn,null),e(dn,Uv),e(dn,xt),e(xt,Yv),e(xt,Li),e(Li,Gv),e(xt,Zv),e(xt,Bd),e(Bd,Kv),e(xt,Xv),e(dn,Jv),b($o,dn,null),e(dn,ek),e(dn,Wd),e(Wd,nk),e(dn,tk),b(ga,dn,null),h(s,lu,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Rd),b(_a,Rd,null),e(Lt,ok),e(Lt,Qd),e(Qd,sk),h(s,du,f),h(s,Ie,f),b(Fa,Ie,null),e(Ie,rk),e(Ie,Hd),e(Hd,ak),e(Ie,ik),e(Ie,Ta),e(Ta,lk),e(Ta,va),e(va,dk),e(Ta,ck),e(Ie,uk),e(Ie,ka),e(ka,pk),e(ka,Ai),e(Ai,hk),e(ka,fk),e(Ie,mk),e(Ie,wa),e(wa,gk),e(wa,ba),e(ba,_k),e(wa,Fk),e(Ie,Tk),b(Mo,Ie,null),e(Ie,vk),e(Ie,cn),b(ya,cn,null),e(cn,kk),e(cn,At),e(At,wk),e(At,Di),e(Di,bk),e(At,yk),e(At,Vd),e(Vd,$k),e(At,Ek),e(cn,Mk),b(zo,cn,null),e(cn,zk),e(cn,Ud),e(Ud,qk),e(cn,Pk),b($a,cn,null),h(s,cu,f),h(s,Dt,f),e(Dt,qo),e(qo,Yd),b(Ea,Yd,null),e(Dt,Ck),e(Dt,Gd),e(Gd,jk),h(s,uu,f),h(s,Se,f),b(Ma,Se,null),e(Se,xk),e(Se,Zd),e(Zd,Lk),e(Se,Ak),e(Se,za),e(za,Dk),e(za,qa),e(qa,Ik),e(za,Sk),e(Se,Nk),e(Se,Pa),e(Pa,Ok),e(Pa,Ii),e(Ii,Bk),e(Pa,Wk),e(Se,Rk),e(Se,Ca),e(Ca,Qk),e(Ca,ja),e(ja,Hk),e(Ca,Vk),e(Se,Uk),b(Po,Se,null),e(Se,Yk),e(Se,un),b(xa,un,null),e(un,Gk),e(un,It),e(It,Zk),e(It,Si),e(Si,Kk),e(It,Xk),e(It,Kd),e(Kd,Jk),e(It,e1),e(un,n1),b(Co,un,null),e(un,t1),e(un,Xd),e(Xd,o1),e(un,s1),b(La,un,null),h(s,pu,f),h(s,St,f),e(St,jo),e(jo,Jd),b(Aa,Jd,null),e(St,r1),e(St,ec),e(ec,a1),h(s,hu,f),h(s,Ne,f),b(Da,Ne,null),e(Ne,i1),e(Ne,Nt),e(Nt,l1),e(Nt,nc),e(nc,d1),e(Nt,c1),e(Nt,tc),e(tc,u1),e(Nt,p1),e(Ne,h1),e(Ne,Ia),e(Ia,f1),e(Ia,Sa),e(Sa,m1),e(Ia,g1),e(Ne,_1),e(Ne,Na),e(Na,F1),e(Na,Ni),e(Ni,T1),e(Na,v1),e(Ne,k1),e(Ne,Oa),e(Oa,w1),e(Oa,Ba),e(Ba,b1),e(Oa,y1),e(Ne,$1),b(xo,Ne,null),e(Ne,E1),e(Ne,pn),b(Wa,pn,null),e(pn,M1),e(pn,Ot),e(Ot,z1),e(Ot,Oi),e(Oi,q1),e(Ot,P1),e(Ot,oc),e(oc,C1),e(Ot,j1),e(pn,x1),b(Lo,pn,null),e(pn,L1),e(pn,sc),e(sc,A1),e(pn,D1),b(Ra,pn,null),fu=!0},p(s,[f]){const Qa={};f&2&&(Qa.$$scope={dirty:f,ctx:s}),Gt.$set(Qa);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:s}),Kt.$set(rc);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:s}),Jt.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:s}),no.$set(ic);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:s}),oo.$set(Ha);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:s}),ro.$set(lc);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:s}),io.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),co.$set(cc);const Va={};f&2&&(Va.$$scope={dirty:f,ctx:s}),po.$set(Va);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),ho.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),mo.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),go.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),Fo.$set(fc);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),To.$set(mc);const Ua={};f&2&&(Ua.$$scope={dirty:f,ctx:s}),ko.$set(Ua);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),wo.$set(gc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:s}),yo.$set(Ce);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),$o.$set(_c);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),Mo.$set(Fc);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),zo.$set(Tc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),Po.$set(vc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),Co.$set(kc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),xo.$set(wc);const bc={};f&2&&(bc.$$scope={dirty:f,ctx:s}),Lo.$set(bc)},i(s){fu||(y(F.$$.fragment,s),y(ne.$$.fragment,s),y(Oo.$$.fragment,s),y(Bo.$$.fragment,s),y(Ro.$$.fragment,s),y(Qo.$$.fragment,s),y(Vo.$$.fragment,s),y(Yo.$$.fragment,s),y(Zo.$$.fragment,s),y(Ko.$$.fragment,s),y(Xo.$$.fragment,s),y(Jo.$$.fragment,s),y(es.$$.fragment,s),y(os.$$.fragment,s),y(ss.$$.fragment,s),y(rs.$$.fragment,s),y(as.$$.fragment,s),y(ls.$$.fragment,s),y(cs.$$.fragment,s),y(us.$$.fragment,s),y(_s.$$.fragment,s),y(Gt.$$.fragment,s),y(Fs.$$.fragment,s),y(Ts.$$.fragment,s),y(vs.$$.fragment,s),y(Es.$$.fragment,s),y(Kt.$$.fragment,s),y(Ms.$$.fragment,s),y(zs.$$.fragment,s),y(qs.$$.fragment,s),y(Ps.$$.fragment,s),y(Jt.$$.fragment,s),y(Cs.$$.fragment,s),y(js.$$.fragment,s),y(xs.$$.fragment,s),y(Os.$$.fragment,s),y(no.$$.fragment,s),y(Bs.$$.fragment,s),y(Ws.$$.fragment,s),y(Rs.$$.fragment,s),y(Gs.$$.fragment,s),y(oo.$$.fragment,s),y(Zs.$$.fragment,s),y(Ks.$$.fragment,s),y(Xs.$$.fragment,s),y(Js.$$.fragment,s),y(rr.$$.fragment,s),y(ro.$$.fragment,s),y(ar.$$.fragment,s),y(ir.$$.fragment,s),y(lr.$$.fragment,s),y(fr.$$.fragment,s),y(io.$$.fragment,s),y(mr.$$.fragment,s),y(gr.$$.fragment,s),y(_r.$$.fragment,s),y(br.$$.fragment,s),y(co.$$.fragment,s),y(yr.$$.fragment,s),y($r.$$.fragment,s),y(Er.$$.fragment,s),y(po.$$.fragment,s),y(jr.$$.fragment,s),y(ho.$$.fragment,s),y(xr.$$.fragment,s),y(Lr.$$.fragment,s),y(Ar.$$.fragment,s),y(mo.$$.fragment,s),y(Br.$$.fragment,s),y(go.$$.fragment,s),y(Wr.$$.fragment,s),y(Rr.$$.fragment,s),y(Qr.$$.fragment,s),y(Fo.$$.fragment,s),y(Zr.$$.fragment,s),y(To.$$.fragment,s),y(Kr.$$.fragment,s),y(Xr.$$.fragment,s),y(Jr.$$.fragment,s),y(ko.$$.fragment,s),y(aa.$$.fragment,s),y(wo.$$.fragment,s),y(ia.$$.fragment,s),y(la.$$.fragment,s),y(da.$$.fragment,s),y(yo.$$.fragment,s),y(ma.$$.fragment,s),y($o.$$.fragment,s),y(ga.$$.fragment,s),y(_a.$$.fragment,s),y(Fa.$$.fragment,s),y(Mo.$$.fragment,s),y(ya.$$.fragment,s),y(zo.$$.fragment,s),y($a.$$.fragment,s),y(Ea.$$.fragment,s),y(Ma.$$.fragment,s),y(Po.$$.fragment,s),y(xa.$$.fragment,s),y(Co.$$.fragment,s),y(La.$$.fragment,s),y(Aa.$$.fragment,s),y(Da.$$.fragment,s),y(xo.$$.fragment,s),y(Wa.$$.fragment,s),y(Lo.$$.fragment,s),y(Ra.$$.fragment,s),fu=!0)},o(s){$(F.$$.fragment,s),$(ne.$$.fragment,s),$(Oo.$$.fragment,s),$(Bo.$$.fragment,s),$(Ro.$$.fragment,s),$(Qo.$$.fragment,s),$(Vo.$$.fragment,s),$(Yo.$$.fragment,s),$(Zo.$$.fragment,s),$(Ko.$$.fragment,s),$(Xo.$$.fragment,s),$(Jo.$$.fragment,s),$(es.$$.fragment,s),$(os.$$.fragment,s),$(ss.$$.fragment,s),$(rs.$$.fragment,s),$(as.$$.fragment,s),$(ls.$$.fragment,s),$(cs.$$.fragment,s),$(us.$$.fragment,s),$(_s.$$.fragment,s),$(Gt.$$.fragment,s),$(Fs.$$.fragment,s),$(Ts.$$.fragment,s),$(vs.$$.fragment,s),$(Es.$$.fragment,s),$(Kt.$$.fragment,s),$(Ms.$$.fragment,s),$(zs.$$.fragment,s),$(qs.$$.fragment,s),$(Ps.$$.fragment,s),$(Jt.$$.fragment,s),$(Cs.$$.fragment,s),$(js.$$.fragment,s),$(xs.$$.fragment,s),$(Os.$$.fragment,s),$(no.$$.fragment,s),$(Bs.$$.fragment,s),$(Ws.$$.fragment,s),$(Rs.$$.fragment,s),$(Gs.$$.fragment,s),$(oo.$$.fragment,s),$(Zs.$$.fragment,s),$(Ks.$$.fragment,s),$(Xs.$$.fragment,s),$(Js.$$.fragment,s),$(rr.$$.fragment,s),$(ro.$$.fragment,s),$(ar.$$.fragment,s),$(ir.$$.fragment,s),$(lr.$$.fragment,s),$(fr.$$.fragment,s),$(io.$$.fragment,s),$(mr.$$.fragment,s),$(gr.$$.fragment,s),$(_r.$$.fragment,s),$(br.$$.fragment,s),$(co.$$.fragment,s),$(yr.$$.fragment,s),$($r.$$.fragment,s),$(Er.$$.fragment,s),$(po.$$.fragment,s),$(jr.$$.fragment,s),$(ho.$$.fragment,s),$(xr.$$.fragment,s),$(Lr.$$.fragment,s),$(Ar.$$.fragment,s),$(mo.$$.fragment,s),$(Br.$$.fragment,s),$(go.$$.fragment,s),$(Wr.$$.fragment,s),$(Rr.$$.fragment,s),$(Qr.$$.fragment,s),$(Fo.$$.fragment,s),$(Zr.$$.fragment,s),$(To.$$.fragment,s),$(Kr.$$.fragment,s),$(Xr.$$.fragment,s),$(Jr.$$.fragment,s),$(ko.$$.fragment,s),$(aa.$$.fragment,s),$(wo.$$.fragment,s),$(ia.$$.fragment,s),$(la.$$.fragment,s),$(da.$$.fragment,s),$(yo.$$.fragment,s),$(ma.$$.fragment,s),$($o.$$.fragment,s),$(ga.$$.fragment,s),$(_a.$$.fragment,s),$(Fa.$$.fragment,s),$(Mo.$$.fragment,s),$(ya.$$.fragment,s),$(zo.$$.fragment,s),$($a.$$.fragment,s),$(Ea.$$.fragment,s),$(Ma.$$.fragment,s),$(Po.$$.fragment,s),$(xa.$$.fragment,s),$(Co.$$.fragment,s),$(La.$$.fragment,s),$(Aa.$$.fragment,s),$(Da.$$.fragment,s),$(xo.$$.fragment,s),$(Wa.$$.fragment,s),$(Lo.$$.fragment,s),$(Ra.$$.fragment,s),fu=!1},d(s){n(p),s&&n(M),s&&n(m),E(F),s&&n(G),s&&n(q),E(ne),s&&n(ie),s&&n(Y),s&&n(j),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Ec),s&&n(xn),s&&n(Mc),s&&n(Kn),E(Oo),s&&n(zc),s&&n(Cn),E(Bo),s&&n(qc),s&&n(Jn),E(Ro),s&&n(Pc),s&&n(Pe),E(Qo),E(Vo),E(Yo),E(Zo),E(Ko),E(Xo),s&&n(Cc),s&&n(nt),E(Jo),s&&n(jc),s&&n(Ze),E(es),E(os),E(ss),s&&n(xc),s&&n(ot),E(rs),s&&n(Lc),s&&n(st),E(as),s&&n(Ac),s&&n(rt),E(ls),s&&n(Dc),s&&n(at),E(cs),s&&n(Ic),s&&n(We),E(us),E(_s),E(Gt),E(Fs),s&&n(Sc),s&&n(lt),E(Ts),s&&n(Nc),s&&n(Re),E(vs),E(Es),E(Kt),E(Ms),s&&n(Oc),s&&n(ct),E(zs),s&&n(Bc),s&&n(ut),E(qs),E(Ps),E(Jt),E(Cs),s&&n(Wc),s&&n(ht),E(js),s&&n(Rc),s&&n(Qe),E(xs),E(Os),E(no),E(Bs),s&&n(Qc),s&&n(mt),E(Ws),s&&n(Hc),s&&n(He),E(Rs),E(Gs),E(oo),E(Zs),E(Ks),s&&n(Vc),s&&n(_t),E(Xs),s&&n(Uc),s&&n(Ve),E(Js),E(rr),E(ro),E(ar),s&&n(Yc),s&&n(Tt),E(ir),s&&n(Gc),s&&n(Ue),E(lr),E(fr),E(io),E(mr),s&&n(Zc),s&&n(kt),E(gr),s&&n(Kc),s&&n(Ye),E(_r),E(br),E(co),E(yr),s&&n(Xc),s&&n(yt),E($r),s&&n(Jc),s&&n(je),E(Er),E(po),E(jr),E(ho),E(xr),s&&n(eu),s&&n(Et),E(Lr),s&&n(nu),s&&n(xe),E(Ar),E(mo),E(Br),E(go),E(Wr),s&&n(tu),s&&n(zt),E(Rr),s&&n(ou),s&&n(Le),E(Qr),E(Fo),E(Zr),E(To),E(Kr),s&&n(su),s&&n(Pt),E(Xr),s&&n(ru),s&&n(Ae),E(Jr),E(ko),E(aa),E(wo),E(ia),s&&n(au),s&&n(jt),E(la),s&&n(iu),s&&n(De),E(da),E(yo),E(ma),E($o),E(ga),s&&n(lu),s&&n(Lt),E(_a),s&&n(du),s&&n(Ie),E(Fa),E(Mo),E(ya),E(zo),E($a),s&&n(cu),s&&n(Dt),E(Ea),s&&n(uu),s&&n(Se),E(Ma),E(Po),E(xa),E(Co),E(La),s&&n(pu),s&&n(St),E(Aa),s&&n(hu),s&&n(Ne),E(Da),E(xo),E(Wa),E(Lo),E(Ra)}}}const X$={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function J$(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class a2 extends b${constructor(p){super();y$(this,p,J$,K$,$$,{fw:0})}}export{a2 as default,X$ as metadata};
