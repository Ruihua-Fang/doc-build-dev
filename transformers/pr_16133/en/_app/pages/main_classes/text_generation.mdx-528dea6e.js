import{S as mm,i as gm,s as _m,e as s,k as l,w as m,t,M as um,c as a,d as o,m as d,a as r,x as g,h as n,b as c,F as e,g as M,y as _,q as u,o as h,B as f}from"../../chunks/vendor-4833417e.js";import{T as pm}from"../../chunks/Tip-fffd6df1.js";import{D as ue}from"../../chunks/Docstring-4f315ed9.js";import{C as ke}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as qs}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function hm(st){let p,A,y,w,C,T,he,I,q,W,P;return{c(){p=s("p"),A=t("Apart from "),y=s("code"),w=t("inputs"),C=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=s("code"),he=t("config.json"),I=t(`) which in turn defaults to the
`),q=s("a"),W=t("PretrainedConfig"),P=t(" of the model."),this.h()},l(S){p=a(S,"P",{});var k=r(p);A=n(k,"Apart from "),y=a(k,"CODE",{});var G=r(y);w=n(G,"inputs"),G.forEach(o),C=n(k,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=a(k,"CODE",{});var fe=r(T);he=n(fe,"config.json"),fe.forEach(o),I=n(k,`) which in turn defaults to the
`),q=a(k,"A",{href:!0});var ye=r(q);W=n(ye,"PretrainedConfig"),ye.forEach(o),P=n(k," of the model."),k.forEach(o),this.h()},h(){c(q,"href","/docs/transformers/pr_16133/en/main_classes/configuration#transformers.PretrainedConfig")},m(S,k){M(S,p,k),e(p,A),e(p,y),e(y,w),e(p,C),e(p,T),e(T,he),e(p,I),e(p,q),e(q,W),e(p,P)},d(S){S&&o(p)}}}function fm(st){let p,A,y,w,C,T,he,I,q,W,P;return{c(){p=s("p"),A=t("Apart from "),y=s("code"),w=t("inputs"),C=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=s("code"),he=t("config.json"),I=t(`) which in turn defaults to the
`),q=s("a"),W=t("PretrainedConfig"),P=t(" of the model."),this.h()},l(S){p=a(S,"P",{});var k=r(p);A=n(k,"Apart from "),y=a(k,"CODE",{});var G=r(y);w=n(G,"inputs"),G.forEach(o),C=n(k,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=a(k,"CODE",{});var fe=r(T);he=n(fe,"config.json"),fe.forEach(o),I=n(k,`) which in turn defaults to the
`),q=a(k,"A",{href:!0});var ye=r(q);W=n(ye,"PretrainedConfig"),ye.forEach(o),P=n(k," of the model."),k.forEach(o),this.h()},h(){c(q,"href","/docs/transformers/pr_16133/en/main_classes/configuration#transformers.PretrainedConfig")},m(S,k){M(S,p,k),e(p,A),e(p,y),e(y,w),e(p,C),e(p,T),e(T,he),e(p,I),e(p,q),e(q,W),e(p,P)},d(S){S&&o(p)}}}function bm(st){let p,A,y,w,C,T,he,I,q,W,P,S,k,G,fe,ye,Gs,kn,Ss,$s,vn,Fs,as,je,Ue,yn,at,zs,jn,As,rs,x,rt,Ps,it,Ns,Jt,Ds,Cs,Is,lt,Ws,Qt,Bs,Hs,Rs,$,B,Ln,Us,Vs,Yt,Ks,Zs,Mn,Xs,Js,wn,Qs,Ys,ea,H,Tn,ta,na,en,oa,sa,En,aa,ra,On,ia,la,da,R,qn,ca,pa,tn,ma,ga,Gn,_a,ua,Sn,ha,fa,ba,U,$n,xa,ka,nn,va,ya,Fn,ja,La,zn,Ma,wa,Ta,V,An,Ea,Oa,on,qa,Ga,Pn,Sa,$a,Nn,Fa,za,Aa,K,Dn,Pa,Na,sn,Da,Ca,Cn,Ia,Wa,In,Ba,Ha,Ra,b,dt,Ua,Wn,Va,Ka,F,Z,Bn,Za,Xa,an,Ja,Qa,Hn,Ya,er,Rn,tr,nr,or,X,Un,sr,ar,rn,rr,ir,Vn,lr,dr,Kn,cr,pr,mr,J,Zn,gr,_r,ln,ur,hr,Xn,fr,br,Jn,xr,kr,vr,Q,Qn,yr,jr,dn,Lr,Mr,Yn,wr,Tr,eo,Er,Or,qr,Y,to,Gr,Sr,cn,$r,Fr,no,zr,Ar,oo,Pr,Nr,Dr,ee,so,Cr,Ir,pn,Wr,Br,ao,Hr,Rr,ro,Ur,Vr,Kr,Ve,Zr,ct,Xr,pt,Jr,Qr,Yr,io,ei,ti,lo,ni,oi,mt,si,co,ai,ri,gt,ii,po,li,di,_t,ci,te,ut,pi,ht,mi,mo,gi,_i,ui,go,hi,fi,ft,bi,ne,bt,xi,xt,ki,_o,vi,yi,ji,uo,Li,Mi,kt,wi,oe,vt,Ti,yt,Ei,ho,Oi,qi,Gi,fo,Si,$i,jt,Fi,se,Lt,zi,Mt,Ai,bo,Pi,Ni,Di,xo,Ci,Ii,wt,Wi,ae,Tt,Bi,Et,Hi,ko,Ri,Ui,Vi,vo,Ki,Zi,Ot,Xi,re,qt,Ji,Gt,Qi,yo,Yi,el,tl,jo,nl,ol,St,is,Le,Ke,Lo,$t,sl,Mo,al,ls,be,Ft,rl,zt,il,mn,ll,dl,cl,E,At,pl,wo,ml,gl,Pt,_l,Nt,ul,hl,fl,xe,bl,To,xl,kl,Eo,vl,yl,gn,jl,Ll,Ml,Dt,wl,Ct,Tl,El,Ol,Oo,ql,Gl,It,ds,Me,Ze,qo,Wt,Sl,Go,$l,cs,z,Bt,Fl,Ht,zl,_n,Al,Pl,Nl,Rt,Dl,un,Cl,Il,Wl,we,ie,So,Bl,Hl,$o,Rl,Ul,Fo,Vl,Kl,zo,Zl,Xl,Jl,le,Ao,Ql,Yl,Po,ed,td,No,nd,od,Do,sd,ad,rd,de,Co,id,ld,Io,dd,cd,Wo,pd,md,Bo,gd,_d,ud,O,Ut,hd,Ho,fd,bd,Te,ce,Ro,xd,kd,Uo,vd,yd,Vo,jd,Ld,Ko,Md,wd,Td,pe,Zo,Ed,Od,Xo,qd,Gd,Jo,Sd,$d,Qo,Fd,zd,Ad,me,Yo,Pd,Nd,es,Dd,Cd,ts,Id,Wd,ns,Bd,Hd,Rd,Xe,Ud,Vt,Vd,Kt,Kd,Zd,Xd,os,Jd,Qd,Zt,ps;return T=new qs({}),at=new qs({}),rt=new ue({props:{name:"class transformers.generation_utils.GenerationMixin",anchor:"transformers.generation_utils.GenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L379"}}),dt=new ue({props:{name:"generate",anchor:"transformers.generation_utils.GenerationMixin.generate",parameters:[{name:"inputs",val:": typing.Optional[torch.Tensor] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"typical_p",val:": typing.Optional[float] = None"},{name:"repetition_penalty",val:": typing.Optional[float] = None"},{name:"bad_words_ids",val:": typing.Optional[typing.Iterable[int]] = None"},{name:"force_words_ids",val:": typing.Union[typing.Iterable[int], typing.Iterable[typing.Iterable[int]], NoneType] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"encoder_no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"num_return_sequences",val:": typing.Optional[int] = None"},{name:"max_time",val:": typing.Optional[float] = None"},{name:"max_new_tokens",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"num_beam_groups",val:": typing.Optional[int] = None"},{name:"diversity_penalty",val:": typing.Optional[float] = None"},{name:"prefix_allowed_tokens_fn",val:": typing.Union[typing.Callable[[int, torch.Tensor], typing.List[int]], NoneType] = None"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = []"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = []"},{name:"constraints",val:": typing.Optional[typing.List[transformers.generation_beam_constraints.Constraint]] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"remove_invalid_values",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"exponential_decay_length_penalty",val:": typing.Union[typing.Tuple[typing.Union[int, float]], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L832",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.generate.inputs",description:`<strong>inputs</strong> (<code>torch.Tensor</code> of varying shape depending on the modality, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"inputs"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to <code>model.config.max_length</code>) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_new_tokens",description:`<strong>max_new_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
The maximum numbers of tokens to generate, ignore the current number of tokens. Use either
<code>max_new_tokens</code> or <code>max_length</code> but not both, they serve the same purpose.`,name:"max_new_tokens"},{anchor:"transformers.generation_utils.GenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_utils.GenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_utils.GenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_utils.GenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the
model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer
sequences.`,name:"length_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.encoder_no_repeat_ngram_size",description:`<strong>encoder_no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size that occur in the <code>encoder_input_ids</code> cannot occur in the
<code>decoder_input_ids</code>.`,name:"encoder_no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bad_words_ids(List[List[int]],",description:`<strong>bad_words_ids(<code>List[List[int]]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the token ids of the words that
should not appear in the generated text, use <code>tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids</code>.`,name:"bad_words_ids(List[List[int]],"},{anchor:"transformers.generation_utils.GenerationMixin.generate.force_words_ids(List[List[int]]",description:`<strong>force_words_ids(<code>List[List[int]]</code></strong> or <code>List[List[List[int]]]</code>, <em>optional</em>) &#x2014;
List of token ids that must be generated. If given a <code>List[List[int]]</code>, this is treated as a simple
list of words that must be included, the opposite to <code>bad_words_ids</code>. If given <code>List[List[List[int]]]</code>,
this triggers a <a href="https://github.com/huggingface/transformers/issues/14081" rel="nofollow">disjunctive constraint</a>,
where one can allow different forms of each word.`,name:"force_words_ids(List[List[int]]"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_time(float,",description:`<strong>max_time(<code>float</code>,</strong> <em>optional</em>, defaults to None) &#x2014;
The maximum amount of time you allow the computation to run for in seconds. generation will still
finish the current pass after allocated time has been passed.`,name:"max_time(float,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape
as <code>input_ids</code> that masks the pad token. <a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_utils.GenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beam_groups",description:`<strong>num_beam_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of groups to divide <code>num_beams</code> into in order to ensure diversity among different groups of
beams. <a href="https://arxiv.org/pdf/1610.02424.pdf" rel="nofollow">this paper</a> for more details.`,name:"num_beam_groups"},{anchor:"transformers.generation_utils.GenerationMixin.generate.diversity_penalty",description:`<strong>diversity_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
This value is subtracted from a beam&#x2019;s score if it generates a token same as any beam from other group
at a particular time. Note that <code>diversity_penalty</code> is only effective if <code>group beam search</code> is
enabled.
prefix_allowed_tokens_fn &#x2014; (<code>Callable[[int, torch.Tensor], List[int]]</code>, <em>optional</em>):
If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904" rel="nofollow">Autoregressive Entity
Retrieval</a>.`,name:"diversity_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
Custom logits processors that complement the default logits processors built from arguments and a
model&#x2019;s config. If a logit processor is passed that is already created with the arguments or a model&#x2019;s
config an error is thrown. This feature is intended for advanced users.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.generate.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
Custom stopping criteria that complement the default stopping criteria built from arguments and a
model&#x2019;s config. If a stopping criteria is passed that is already created with the arguments or a
model&#x2019;s config an error is thrown. This feature is intended for advanced users.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.generate.constraints",description:`<strong>constraints</strong> (<code>List[Constraint]</code>, <em>optional</em>) &#x2014;
Custom constraints that can be added to the generation to ensure that the output will contain the use
of certain tokens as defined by <code>Constraint</code> objects, in the most sensible way possible.`,name:"constraints"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.`,name:"forced_eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.remove_invalid_values",description:`<strong>remove_invalid_values</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to remove possible <em>nan</em> and <em>inf</em> outputs of the model to prevent the generation method to
crash. Note that using <code>remove_invalid_values</code> can slow down generation.`,name:"remove_invalid_values"},{anchor:"transformers.generation_utils.GenerationMixin.generate.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)`,name:"synced_gpus"},{anchor:"transformers.generation_utils.GenerationMixin.generate.exponential_decay_length_penalty",description:`<strong>exponential_decay_length_penalty</strong> (<code>tuple(int, float)</code>, <em>optional</em>) &#x2014;
This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been
generated. The tuple shall consist of: <code>(start_index, decay_factor)</code> where <code>start_index</code> indicates
where penalty starts and <code>decay_factor</code> represents the factor of exponential decay</p>
<p>model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*.`,name:"exponential_decay_length_penalty"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> (if
<code>return_dict_in_generate=True</code> or when <code>config.return_dict_in_generate=True</code>) or a <code>torch.FloatTensor</code>.</p>
<p>If the model is <em>not</em> an encoder-decoder model (<code>model.config.is_encoder_decoder=False</code>), the possible
<a
  href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a></li>
</ul>
<p>If the model is an encoder-decoder model (<code>model.config.is_encoder_decoder=True</code>), the possible
<a
  href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a></li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> or <code>torch.LongTensor</code></p>
`}}),Ve=new pm({props:{warning:"&lcub;true}",$$slots:{default:[hm]},$$scope:{ctx:st}}}),mt=new ke({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# generate up to 30 tokens
outputs = model.generate(input_ids, do_sample=False, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n&#x27;</span>]`}}),gt=new ke({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# sample up to 30 tokens
torch.manual_seed(0)
outputs = model.generate(input_ids, do_sample=True, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get rid of discrimination,&quot; said Rep. Mark Pocan (D-Wis.).\\n\\n&quot;Just look at the&#x27;</span>]`}}),_t=new ke({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-de")

sentence = "Paris is one of the densest populated areas in Europe."
input_ids = tokenizer(sentence, return_tensors="pt").input_ids

outputs = model.generate(input_ids)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sentence = <span class="hljs-string">&quot;Paris is one of the densest populated areas in Europe.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Paris ist eines der dichtesten besiedelten Gebiete Europas.&#x27;</span>]`}}),ut=new ue({props:{name:"greedy_search",anchor:"transformers.generation_utils.GenerationMixin.greedy_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L1489",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific keyword arguments will be forwarded to the <code>forward</code> function of the model.
If model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>
or <code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),ft=new ke({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    StoppingCriteriaList,
    MaxLengthCriteria,
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "It might be possible to"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),
    ]
)
stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

outputs = model.greedy_search(
    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;It might be possible to&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">10</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.greedy_search(
<span class="hljs-meta">... </span>    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&quot;It might be possible to get a better understanding of the nature of the problem, but it&#x27;s not&quot;</span>]`}}),bt=new ue({props:{name:"sample",anchor:"transformers.generation_utils.GenerationMixin.sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L1721",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),kt=new ke({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    StoppingCriteriaList,
    MaxLengthCriteria,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
    ]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

torch.manual_seed(0)
outputs = model.sample(
    input_ids,
    logits_processor=logits_processor,
    logits_warper=logits_warper,
    stopping_criteria=stopping_criteria,
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;Today is a beautiful day, and&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">15</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.sample(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    logits_processor=logits_processor,
<span class="hljs-meta">... </span>    logits_warper=logits_warper,
<span class="hljs-meta">... </span>    stopping_criteria=stopping_criteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the&#x27;</span>]`}}),vt=new ue({props:{name:"beam_search",anchor:"transformers.generation_utils.GenerationMixin.beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L1977",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),jt=new ke({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Lt=new ue({props:{name:"beam_sample",anchor:"transformers.generation_utils.GenerationMixin.beam_sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L2289",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),wt=new ke({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

outputs = model.beam_sample(
    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id)]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_sample(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Tt=new ue({props:{name:"group_beam_search",anchor:"transformers.generation_utils.GenerationMixin.group_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L2611",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
<p>model_kwargs &#x2014;
Additional model specific kwargs that will be forwarded to the <code>forward</code> function of the model. If
model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if
<code>model.config.is_encoder_decoder=False</code> and <code>return_dict_in_generate=True</code> or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if <code>model.config.is_encoder_decoder=True</code>.</p>
`}}),Ot=new ke({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    HammingDiversityLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
    num_beam_groups=3,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.group_beam_search(
    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    HammingDiversityLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run diverse beam search using 6 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>    num_beam_groups=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        HammingDiversityLogitsProcessor(<span class="hljs-number">5.5</span>, num_beams=<span class="hljs-number">6</span>, num_beam_groups=<span class="hljs-number">3</span>),
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.group_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),qt=new ue({props:{name:"constrained_beam_search",anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"constrained_beam_scorer",val:": ConstrainedBeamSearchScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_utils.py#L2976",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.constrained_beam_scorer",description:`<strong>constrained_beam_scorer</strong> (<code>ConstrainedBeamSearchScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation, while satisfying a list of positive constraints. For more information, the
documentation of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer">ConstrainedBeamSearchScorer</a> should be read.`,name:"constrained_beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/pr_16133/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),St=new ke({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    ConstrainedBeamSearchScorer,
    PhrasalConstraint,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

constraint_str = "sind"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.constrained_beam_search(
    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    ConstrainedBeamSearchScorer,
<span class="hljs-meta">... </span>    PhrasalConstraint,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_str = <span class="hljs-string">&quot;sind&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_token_ids = tokenizer.encode(constraint_str)[:-<span class="hljs-number">1</span>]  <span class="hljs-comment"># slice to remove eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = ConstrainedBeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>, num_beams=num_beams, device=model.device, constraints=constraints
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.constrained_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt sind Sie?&#x27;</span>]`}}),$t=new qs({}),Ft=new ue({props:{name:"class transformers.generation_tf_utils.TFGenerationMixin",anchor:"transformers.generation_tf_utils.TFGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_tf_utils.py#L342"}}),At=new ue({props:{name:"generate",anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate",parameters:[{name:"input_ids",val:" = None"},{name:"max_length",val:" = None"},{name:"min_length",val:" = None"},{name:"do_sample",val:" = None"},{name:"early_stopping",val:" = None"},{name:"num_beams",val:" = None"},{name:"temperature",val:" = None"},{name:"top_k",val:" = None"},{name:"top_p",val:" = None"},{name:"repetition_penalty",val:" = None"},{name:"bad_words_ids",val:" = None"},{name:"bos_token_id",val:" = None"},{name:"pad_token_id",val:" = None"},{name:"eos_token_id",val:" = None"},{name:"length_penalty",val:" = None"},{name:"no_repeat_ngram_size",val:" = None"},{name:"num_return_sequences",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_start_token_id",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_scores",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict_in_generate",val:" = None"},{name:"forced_bos_token_id",val:" = None"},{name:"forced_eos_token_id",val:" = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_tf_utils.py#L362",parametersDescription:[{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.input_ids",description:"<strong>input_ids</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, `(batch_size, sequence_length, &#x2014;",name:"input_ids"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.feature_dim)`",description:`<strong>feature_dim)\`</strong> or <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"feature_dim)`"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.`,name:"length_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bad_words_ids(List[int],",description:`<strong>bad_words_ids(<code>List[int]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code>tokenizer.encode(bad_word, add_prefix_space=True)</code>.`,name:"bad_words_ids(List[int],"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>tf.Tensor</code> of <code>dtype=tf.int32</code> and shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code>input_ids</code> that masks the pad token.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.
model_specific_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model.`,name:"forced_eos_token_id"}],returnType:`
<p><a
  href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> or <code>tf.Tensor</code></p>
`}}),It=new ke({props:{code:`tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
outputs = model.generate(max_length=40)  # do greedy decoding
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("openai-gpt")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "openai-gpt"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5
)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True
)  # generate 3 candidates using sampling
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("ctrl")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "ctrl"
)  # Download model and configuration from huggingface.co and cache.
input_context = "Legal My neighbor is"  # "Legal" is one of the control codes for ctrl
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2
)  # generate sequences
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "gpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "My cute dog"
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ["idiot", "stupid", "shut up"]
]
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids
)  # generate sequences without allowing bad_words to be generated`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
outputs = model.generate(max_length=<span class="hljs-number">40</span>)  <span class="hljs-comment"># do greedy decoding</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-gpt&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;openai-gpt&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>, temperature=<span class="hljs-number">1.5</span>
)  <span class="hljs-comment"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#x27;The dog&#x27;</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">40</span>, temperature=<span class="hljs-number">0.7</span>, num_return_sequences=<span class="hljs-number">3</span>, do_sample=<span class="hljs-literal">True</span>
)  <span class="hljs-comment"># generate 3 candidates using sampling</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ctrl&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;ctrl&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;Legal My neighbor is&quot;</span>  <span class="hljs-comment"># &quot;Legal&quot; is one of the control codes for ctrl</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">0.7</span>, repetition_penalty=<span class="hljs-number">1.2</span>
)  <span class="hljs-comment"># generate sequences</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;My cute dog&quot;</span>
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=<span class="hljs-literal">True</span>) <span class="hljs-keyword">for</span> bad_word <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idiot&quot;</span>, <span class="hljs-string">&quot;stupid&quot;</span>, <span class="hljs-string">&quot;shut up&quot;</span>]
]
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>, bad_words_ids=bad_words_ids
)  <span class="hljs-comment"># generate sequences without allowing bad_words to be generated</span>`}}),Wt=new qs({}),Bt=new ue({props:{name:"class transformers.generation_flax_utils.FlaxGenerationMixin",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_flax_utils.py#L119"}}),Ut=new ue({props:{name:"generate",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate",parameters:[{name:"input_ids",val:": ndarray"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"prng_key",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"trace",val:": bool = True"},{name:"params",val:": typing.Union[typing.Dict[str, jax._src.numpy.lax_numpy.ndarray], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16133/src/transformers/generation_flax_utils.py#L163",parametersDescription:[{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.trace",description:`<strong>trace</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trace generation. Setting <code>trace=False</code> should only be used for debugging and will lead to a
considerably slower runtime.`,name:"trace"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.params",description:`<strong>params</strong> (<code>Dict[str, jnp.ndarray]</code>, <em>optional</em>) &#x2014;
Optionally the model parameters can be passed. Can be useful for parallelized generation.
model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*. Also accepts <code>encoder_outputs</code> to skip encoder part.`,name:"params"}],returnDescription:`
<p><a
  href="/docs/transformers/pr_16133/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a>.</p>
`}}),Xe=new pm({props:{warning:"&lcub;true}",$$slots:{default:[fm]},$$scope:{ctx:st}}}),Zt=new ke({props:{code:`from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = FlaxAutoModelForCausalLM.from_pretrained("distilgpt2")
input_context = "The dog"
# encode input context
input_ids = tokenizer(input_context, return_tensors="np").input_ids
# generate candidates using sampling
outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_context = <span class="hljs-string">&quot;The dog&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># encode input context</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_context, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate candidates using sampling</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids=input_ids, max_length=<span class="hljs-number">20</span>, top_k=<span class="hljs-number">30</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),{c(){p=s("meta"),A=l(),y=s("h1"),w=s("a"),C=s("span"),m(T.$$.fragment),he=l(),I=s("span"),q=t("Generation"),W=l(),P=s("p"),S=t("Each framework has a generate method for auto-regressive text generation implemented in their respective GenerationMixin class:"),k=l(),G=s("ul"),fe=s("li"),ye=t("PyTorch [~generation_utils.GenerationMixin.generate] is implemented in [~generation_utils.GenerationMixin]."),Gs=l(),kn=s("li"),Ss=t("TensorFlow [~generation_tf_utils.TFGenerationMixin.generate] is implemented in [~generation_tf_utils.TFGenerationMixin]."),$s=l(),vn=s("li"),Fs=t("Flax/JAX [~generation_flax_utils.FlaxGenerationMixin.generate] is implemented in [~generation_flax_utils.FlaxGenerationMixin]."),as=l(),je=s("h2"),Ue=s("a"),yn=s("span"),m(at.$$.fragment),zs=l(),jn=s("span"),As=t("GenerationMixin"),rs=l(),x=s("div"),m(rt.$$.fragment),Ps=l(),it=s("p"),Ns=t("A class containing all functions for auto-regressive text generation, to be used as a mixin in "),Jt=s("a"),Ds=t("PreTrainedModel"),Cs=t("."),Is=l(),lt=s("p"),Ws=t("The class exposes "),Qt=s("a"),Bs=t("generate()"),Hs=t(", which can be used for:"),Rs=l(),$=s("ul"),B=s("li"),Ln=s("em"),Us=t("greedy decoding"),Vs=t(" by calling "),Yt=s("a"),Ks=t("greedy_search()"),Zs=t(" if "),Mn=s("code"),Xs=t("num_beams=1"),Js=t(` and
`),wn=s("code"),Qs=t("do_sample=False"),Ys=t("."),ea=l(),H=s("li"),Tn=s("em"),ta=t("multinomial sampling"),na=t(" by calling "),en=s("a"),oa=t("sample()"),sa=t(" if "),En=s("code"),aa=t("num_beams=1"),ra=t(` and
`),On=s("code"),ia=t("do_sample=True"),la=t("."),da=l(),R=s("li"),qn=s("em"),ca=t("beam-search decoding"),pa=t(" by calling "),tn=s("a"),ma=t("beam_search()"),ga=t(" if "),Gn=s("code"),_a=t("num_beams>1"),ua=t(` and
`),Sn=s("code"),ha=t("do_sample=False"),fa=t("."),ba=l(),U=s("li"),$n=s("em"),xa=t("beam-search multinomial sampling"),ka=t(" by calling "),nn=s("a"),va=t("beam_sample()"),ya=t(` if
`),Fn=s("code"),ja=t("num_beams>1"),La=t(" and "),zn=s("code"),Ma=t("do_sample=True"),wa=t("."),Ta=l(),V=s("li"),An=s("em"),Ea=t("diverse beam-search decoding"),Oa=t(" by calling "),on=s("a"),qa=t("group_beam_search()"),Ga=t(`, if
`),Pn=s("code"),Sa=t("num_beams>1"),$a=t(" and "),Nn=s("code"),Fa=t("num_beam_groups>1"),za=t("."),Aa=l(),K=s("li"),Dn=s("em"),Pa=t("constrained beam-search decoding"),Na=t(" by calling "),sn=s("a"),Da=t("constrained_beam_search()"),Ca=t(`,
if `),Cn=s("code"),Ia=t("constraints!=None"),Wa=t(" or "),In=s("code"),Ba=t("force_words_ids!=None"),Ha=t("."),Ra=l(),b=s("div"),m(dt.$$.fragment),Ua=l(),Wn=s("p"),Va=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Ka=l(),F=s("ul"),Z=s("li"),Bn=s("em"),Za=t("greedy decoding"),Xa=t(" by calling "),an=s("a"),Ja=t("greedy_search()"),Qa=t(" if "),Hn=s("code"),Ya=t("num_beams=1"),er=t(` and
`),Rn=s("code"),tr=t("do_sample=False"),nr=t("."),or=l(),X=s("li"),Un=s("em"),sr=t("multinomial sampling"),ar=t(" by calling "),rn=s("a"),rr=t("sample()"),ir=t(" if "),Vn=s("code"),lr=t("num_beams=1"),dr=t(` and
`),Kn=s("code"),cr=t("do_sample=True"),pr=t("."),mr=l(),J=s("li"),Zn=s("em"),gr=t("beam-search decoding"),_r=t(" by calling "),ln=s("a"),ur=t("beam_search()"),hr=t(" if "),Xn=s("code"),fr=t("num_beams>1"),br=t(` and
`),Jn=s("code"),xr=t("do_sample=False"),kr=t("."),vr=l(),Q=s("li"),Qn=s("em"),yr=t("beam-search multinomial sampling"),jr=t(" by calling "),dn=s("a"),Lr=t("beam_sample()"),Mr=t(` if
`),Yn=s("code"),wr=t("num_beams>1"),Tr=t(" and "),eo=s("code"),Er=t("do_sample=True"),Or=t("."),qr=l(),Y=s("li"),to=s("em"),Gr=t("diverse beam-search decoding"),Sr=t(" by calling "),cn=s("a"),$r=t("group_beam_search()"),Fr=t(`, if
`),no=s("code"),zr=t("num_beams>1"),Ar=t(" and "),oo=s("code"),Pr=t("num_beam_groups>1"),Nr=t("."),Dr=l(),ee=s("li"),so=s("em"),Cr=t("constrained beam-search decoding"),Ir=t(` by calling
`),pn=s("a"),Wr=t("constrained_beam_search()"),Br=t(", if "),ao=s("code"),Hr=t("constraints!=None"),Rr=t(` or
`),ro=s("code"),Ur=t("force_words_ids!=None"),Vr=t("."),Kr=l(),m(Ve.$$.fragment),Zr=l(),ct=s("p"),Xr=t("Most of these parameters are explained in more detail in "),pt=s("a"),Jr=t(`this blog
post`),Qr=t("."),Yr=l(),io=s("p"),ei=t("Examples:"),ti=l(),lo=s("p"),ni=t("Greedy Decoding:"),oi=l(),m(mt.$$.fragment),si=l(),co=s("p"),ai=t("Multinomial Sampling:"),ri=l(),m(gt.$$.fragment),ii=l(),po=s("p"),li=t("Beam-search decoding:"),di=l(),m(_t.$$.fragment),ci=l(),te=s("div"),m(ut.$$.fragment),pi=l(),ht=s("p"),mi=t("Generates sequences of token ids for models with a language modeling head using "),mo=s("strong"),gi=t("greedy decoding"),_i=t(` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),ui=l(),go=s("p"),hi=t("Examples:"),fi=l(),m(ft.$$.fragment),bi=l(),ne=s("div"),m(bt.$$.fragment),xi=l(),xt=s("p"),ki=t("Generates sequences of token ids for models with a language modeling head using "),_o=s("strong"),vi=t("multinomial sampling"),yi=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),ji=l(),uo=s("p"),Li=t("Examples:"),Mi=l(),m(kt.$$.fragment),wi=l(),oe=s("div"),m(vt.$$.fragment),Ti=l(),yt=s("p"),Ei=t("Generates sequences of token ids for models with a language modeling head using "),ho=s("strong"),Oi=t("beam search decoding"),qi=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Gi=l(),fo=s("p"),Si=t("Examples:"),$i=l(),m(jt.$$.fragment),Fi=l(),se=s("div"),m(Lt.$$.fragment),zi=l(),Mt=s("p"),Ai=t("Generates sequences of token ids for models with a language modeling head using "),bo=s("strong"),Pi=t(`beam search multinomial
sampling`),Ni=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Di=l(),xo=s("p"),Ci=t("Examples:"),Ii=l(),m(wt.$$.fragment),Wi=l(),ae=s("div"),m(Tt.$$.fragment),Bi=l(),Et=s("p"),Hi=t("Generates sequences of token ids for models with a language modeling head using "),ko=s("strong"),Ri=t(`diverse beam search
decoding`),Ui=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Vi=l(),vo=s("p"),Ki=t("Examples:"),Zi=l(),m(Ot.$$.fragment),Xi=l(),re=s("div"),m(qt.$$.fragment),Ji=l(),Gt=s("p"),Qi=t("Generates sequences of token ids for models with a language modeling head using "),yo=s("strong"),Yi=t(`constrained beam search
decoding`),el=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),tl=l(),jo=s("p"),nl=t("Examples:"),ol=l(),m(St.$$.fragment),is=l(),Le=s("h2"),Ke=s("a"),Lo=s("span"),m($t.$$.fragment),sl=l(),Mo=s("span"),al=t("TFGenerationMixin"),ls=l(),be=s("div"),m(Ft.$$.fragment),rl=l(),zt=s("p"),il=t("A class containing all of the functions supporting generation, to be used as a mixin in "),mn=s("a"),ll=t("TFPreTrainedModel"),dl=t("."),cl=l(),E=s("div"),m(At.$$.fragment),pl=l(),wo=s("p"),ml=t(`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),gl=l(),Pt=s("p"),_l=t("Adapted in part from "),Nt=s("a"),ul=t(`Facebook\u2019s XLM beam search
code`),hl=t("."),fl=l(),xe=s("p"),bl=t("Apart from "),To=s("code"),xl=t("input_ids"),kl=t(" and "),Eo=s("code"),vl=t("attention_mask"),yl=t(`, all the arguments below will default to the value of the attribute
of the same name inside the `),gn=s("a"),jl=t("PretrainedConfig"),Ll=t(` of the model. The default values indicated are the default
values of those config.`),Ml=l(),Dt=s("p"),wl=t("Most of these parameters are explained in more detail in "),Ct=s("a"),Tl=t(`this blog
post`),El=t("."),Ol=l(),Oo=s("p"),ql=t("Examples:"),Gl=l(),m(It.$$.fragment),ds=l(),Me=s("h2"),Ze=s("a"),qo=s("span"),m(Wt.$$.fragment),Sl=l(),Go=s("span"),$l=t("FlaxGenerationMixin"),cs=l(),z=s("div"),m(Bt.$$.fragment),Fl=l(),Ht=s("p"),zl=t(`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),_n=s("a"),Al=t("FlaxPreTrainedModel"),Pl=t("."),Nl=l(),Rt=s("p"),Dl=t("The class exposes "),un=s("a"),Cl=t("generate()"),Il=t(", which can be used for:"),Wl=l(),we=s("ul"),ie=s("li"),So=s("em"),Bl=t("greedy decoding"),Hl=t(" by calling "),$o=s("code"),Rl=t("_greedy_search()"),Ul=t(`if
`),Fo=s("code"),Vl=t("num_beams=1"),Kl=t(" and "),zo=s("code"),Zl=t("do_sample=False"),Xl=t("."),Jl=l(),le=s("li"),Ao=s("em"),Ql=t("multinomial sampling"),Yl=t(" by calling "),Po=s("code"),ed=t("_sample()"),td=t("if "),No=s("code"),nd=t("num_beams=1"),od=t(`
and `),Do=s("code"),sd=t("do_sample=True"),ad=t("."),rd=l(),de=s("li"),Co=s("em"),id=t("beam-search decoding"),ld=t(" by calling "),Io=s("code"),dd=t("_beam_search"),cd=t(" if "),Wo=s("code"),pd=t("num_beams>1"),md=t(`
and `),Bo=s("code"),gd=t("do_sample=False"),_d=t("."),ud=l(),O=s("div"),m(Ut.$$.fragment),hd=l(),Ho=s("p"),fd=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),bd=l(),Te=s("ul"),ce=s("li"),Ro=s("em"),xd=t("greedy decoding"),kd=t(" by calling "),Uo=s("code"),vd=t("_greedy_search()"),yd=t(`if
`),Vo=s("code"),jd=t("num_beams=1"),Ld=t(" and "),Ko=s("code"),Md=t("do_sample=False"),wd=t("."),Td=l(),pe=s("li"),Zo=s("em"),Ed=t("multinomial sampling"),Od=t(" by calling "),Xo=s("code"),qd=t("_sample()"),Gd=t("if "),Jo=s("code"),Sd=t("num_beams=1"),$d=t(`
and `),Qo=s("code"),Fd=t("do_sample=True"),zd=t("."),Ad=l(),me=s("li"),Yo=s("em"),Pd=t("beam-search decoding"),Nd=t(" by calling "),es=s("code"),Dd=t("_beam_search"),Cd=t(" if "),ts=s("code"),Id=t("num_beams>1"),Wd=t(`
and `),ns=s("code"),Bd=t("do_sample=False"),Hd=t("."),Rd=l(),m(Xe.$$.fragment),Ud=l(),Vt=s("p"),Vd=t("Most of these parameters are explained in more detail in "),Kt=s("a"),Kd=t(`this blog
post`),Zd=t("."),Xd=l(),os=s("p"),Jd=t("Examples:"),Qd=l(),m(Zt.$$.fragment),this.h()},l(i){const v=um('[data-svelte="svelte-1phssyn"]',document.head);p=a(v,"META",{name:!0,content:!0}),v.forEach(o),A=d(i),y=a(i,"H1",{class:!0});var Xt=r(y);w=a(Xt,"A",{id:!0,class:!0,href:!0});var ss=r(w);C=a(ss,"SPAN",{});var Yd=r(C);g(T.$$.fragment,Yd),Yd.forEach(o),ss.forEach(o),he=d(Xt),I=a(Xt,"SPAN",{});var ec=r(I);q=n(ec,"Generation"),ec.forEach(o),Xt.forEach(o),W=d(i),P=a(i,"P",{});var tc=r(P);S=n(tc,"Each framework has a generate method for auto-regressive text generation implemented in their respective GenerationMixin class:"),tc.forEach(o),k=d(i),G=a(i,"UL",{});var hn=r(G);fe=a(hn,"LI",{});var nc=r(fe);ye=n(nc,"PyTorch [~generation_utils.GenerationMixin.generate] is implemented in [~generation_utils.GenerationMixin]."),nc.forEach(o),Gs=d(hn),kn=a(hn,"LI",{});var oc=r(kn);Ss=n(oc,"TensorFlow [~generation_tf_utils.TFGenerationMixin.generate] is implemented in [~generation_tf_utils.TFGenerationMixin]."),oc.forEach(o),$s=d(hn),vn=a(hn,"LI",{});var sc=r(vn);Fs=n(sc,"Flax/JAX [~generation_flax_utils.FlaxGenerationMixin.generate] is implemented in [~generation_flax_utils.FlaxGenerationMixin]."),sc.forEach(o),hn.forEach(o),as=d(i),je=a(i,"H2",{class:!0});var ms=r(je);Ue=a(ms,"A",{id:!0,class:!0,href:!0});var ac=r(Ue);yn=a(ac,"SPAN",{});var rc=r(yn);g(at.$$.fragment,rc),rc.forEach(o),ac.forEach(o),zs=d(ms),jn=a(ms,"SPAN",{});var ic=r(jn);As=n(ic,"GenerationMixin"),ic.forEach(o),ms.forEach(o),rs=d(i),x=a(i,"DIV",{class:!0});var L=r(x);g(rt.$$.fragment,L),Ps=d(L),it=a(L,"P",{});var gs=r(it);Ns=n(gs,"A class containing all functions for auto-regressive text generation, to be used as a mixin in "),Jt=a(gs,"A",{href:!0});var lc=r(Jt);Ds=n(lc,"PreTrainedModel"),lc.forEach(o),Cs=n(gs,"."),gs.forEach(o),Is=d(L),lt=a(L,"P",{});var _s=r(lt);Ws=n(_s,"The class exposes "),Qt=a(_s,"A",{href:!0});var dc=r(Qt);Bs=n(dc,"generate()"),dc.forEach(o),Hs=n(_s,", which can be used for:"),_s.forEach(o),Rs=d(L),$=a(L,"UL",{});var ge=r($);B=a(ge,"LI",{});var Ee=r(B);Ln=a(Ee,"EM",{});var cc=r(Ln);Us=n(cc,"greedy decoding"),cc.forEach(o),Vs=n(Ee," by calling "),Yt=a(Ee,"A",{href:!0});var pc=r(Yt);Ks=n(pc,"greedy_search()"),pc.forEach(o),Zs=n(Ee," if "),Mn=a(Ee,"CODE",{});var mc=r(Mn);Xs=n(mc,"num_beams=1"),mc.forEach(o),Js=n(Ee,` and
`),wn=a(Ee,"CODE",{});var gc=r(wn);Qs=n(gc,"do_sample=False"),gc.forEach(o),Ys=n(Ee,"."),Ee.forEach(o),ea=d(ge),H=a(ge,"LI",{});var Oe=r(H);Tn=a(Oe,"EM",{});var _c=r(Tn);ta=n(_c,"multinomial sampling"),_c.forEach(o),na=n(Oe," by calling "),en=a(Oe,"A",{href:!0});var uc=r(en);oa=n(uc,"sample()"),uc.forEach(o),sa=n(Oe," if "),En=a(Oe,"CODE",{});var hc=r(En);aa=n(hc,"num_beams=1"),hc.forEach(o),ra=n(Oe,` and
`),On=a(Oe,"CODE",{});var fc=r(On);ia=n(fc,"do_sample=True"),fc.forEach(o),la=n(Oe,"."),Oe.forEach(o),da=d(ge),R=a(ge,"LI",{});var qe=r(R);qn=a(qe,"EM",{});var bc=r(qn);ca=n(bc,"beam-search decoding"),bc.forEach(o),pa=n(qe," by calling "),tn=a(qe,"A",{href:!0});var xc=r(tn);ma=n(xc,"beam_search()"),xc.forEach(o),ga=n(qe," if "),Gn=a(qe,"CODE",{});var kc=r(Gn);_a=n(kc,"num_beams>1"),kc.forEach(o),ua=n(qe,` and
`),Sn=a(qe,"CODE",{});var vc=r(Sn);ha=n(vc,"do_sample=False"),vc.forEach(o),fa=n(qe,"."),qe.forEach(o),ba=d(ge),U=a(ge,"LI",{});var Ge=r(U);$n=a(Ge,"EM",{});var yc=r($n);xa=n(yc,"beam-search multinomial sampling"),yc.forEach(o),ka=n(Ge," by calling "),nn=a(Ge,"A",{href:!0});var jc=r(nn);va=n(jc,"beam_sample()"),jc.forEach(o),ya=n(Ge,` if
`),Fn=a(Ge,"CODE",{});var Lc=r(Fn);ja=n(Lc,"num_beams>1"),Lc.forEach(o),La=n(Ge," and "),zn=a(Ge,"CODE",{});var Mc=r(zn);Ma=n(Mc,"do_sample=True"),Mc.forEach(o),wa=n(Ge,"."),Ge.forEach(o),Ta=d(ge),V=a(ge,"LI",{});var Se=r(V);An=a(Se,"EM",{});var wc=r(An);Ea=n(wc,"diverse beam-search decoding"),wc.forEach(o),Oa=n(Se," by calling "),on=a(Se,"A",{href:!0});var Tc=r(on);qa=n(Tc,"group_beam_search()"),Tc.forEach(o),Ga=n(Se,`, if
`),Pn=a(Se,"CODE",{});var Ec=r(Pn);Sa=n(Ec,"num_beams>1"),Ec.forEach(o),$a=n(Se," and "),Nn=a(Se,"CODE",{});var Oc=r(Nn);Fa=n(Oc,"num_beam_groups>1"),Oc.forEach(o),za=n(Se,"."),Se.forEach(o),Aa=d(ge),K=a(ge,"LI",{});var $e=r(K);Dn=a($e,"EM",{});var qc=r(Dn);Pa=n(qc,"constrained beam-search decoding"),qc.forEach(o),Na=n($e," by calling "),sn=a($e,"A",{href:!0});var Gc=r(sn);Da=n(Gc,"constrained_beam_search()"),Gc.forEach(o),Ca=n($e,`,
if `),Cn=a($e,"CODE",{});var Sc=r(Cn);Ia=n(Sc,"constraints!=None"),Sc.forEach(o),Wa=n($e," or "),In=a($e,"CODE",{});var $c=r(In);Ba=n($c,"force_words_ids!=None"),$c.forEach(o),Ha=n($e,"."),$e.forEach(o),ge.forEach(o),Ra=d(L),b=a(L,"DIV",{class:!0});var j=r(b);g(dt.$$.fragment,j),Ua=d(j),Wn=a(j,"P",{});var Fc=r(Wn);Va=n(Fc,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Fc.forEach(o),Ka=d(j),F=a(j,"UL",{});var _e=r(F);Z=a(_e,"LI",{});var Fe=r(Z);Bn=a(Fe,"EM",{});var zc=r(Bn);Za=n(zc,"greedy decoding"),zc.forEach(o),Xa=n(Fe," by calling "),an=a(Fe,"A",{href:!0});var Ac=r(an);Ja=n(Ac,"greedy_search()"),Ac.forEach(o),Qa=n(Fe," if "),Hn=a(Fe,"CODE",{});var Pc=r(Hn);Ya=n(Pc,"num_beams=1"),Pc.forEach(o),er=n(Fe,` and
`),Rn=a(Fe,"CODE",{});var Nc=r(Rn);tr=n(Nc,"do_sample=False"),Nc.forEach(o),nr=n(Fe,"."),Fe.forEach(o),or=d(_e),X=a(_e,"LI",{});var ze=r(X);Un=a(ze,"EM",{});var Dc=r(Un);sr=n(Dc,"multinomial sampling"),Dc.forEach(o),ar=n(ze," by calling "),rn=a(ze,"A",{href:!0});var Cc=r(rn);rr=n(Cc,"sample()"),Cc.forEach(o),ir=n(ze," if "),Vn=a(ze,"CODE",{});var Ic=r(Vn);lr=n(Ic,"num_beams=1"),Ic.forEach(o),dr=n(ze,` and
`),Kn=a(ze,"CODE",{});var Wc=r(Kn);cr=n(Wc,"do_sample=True"),Wc.forEach(o),pr=n(ze,"."),ze.forEach(o),mr=d(_e),J=a(_e,"LI",{});var Ae=r(J);Zn=a(Ae,"EM",{});var Bc=r(Zn);gr=n(Bc,"beam-search decoding"),Bc.forEach(o),_r=n(Ae," by calling "),ln=a(Ae,"A",{href:!0});var Hc=r(ln);ur=n(Hc,"beam_search()"),Hc.forEach(o),hr=n(Ae," if "),Xn=a(Ae,"CODE",{});var Rc=r(Xn);fr=n(Rc,"num_beams>1"),Rc.forEach(o),br=n(Ae,` and
`),Jn=a(Ae,"CODE",{});var Uc=r(Jn);xr=n(Uc,"do_sample=False"),Uc.forEach(o),kr=n(Ae,"."),Ae.forEach(o),vr=d(_e),Q=a(_e,"LI",{});var Pe=r(Q);Qn=a(Pe,"EM",{});var Vc=r(Qn);yr=n(Vc,"beam-search multinomial sampling"),Vc.forEach(o),jr=n(Pe," by calling "),dn=a(Pe,"A",{href:!0});var Kc=r(dn);Lr=n(Kc,"beam_sample()"),Kc.forEach(o),Mr=n(Pe,` if
`),Yn=a(Pe,"CODE",{});var Zc=r(Yn);wr=n(Zc,"num_beams>1"),Zc.forEach(o),Tr=n(Pe," and "),eo=a(Pe,"CODE",{});var Xc=r(eo);Er=n(Xc,"do_sample=True"),Xc.forEach(o),Or=n(Pe,"."),Pe.forEach(o),qr=d(_e),Y=a(_e,"LI",{});var Ne=r(Y);to=a(Ne,"EM",{});var Jc=r(to);Gr=n(Jc,"diverse beam-search decoding"),Jc.forEach(o),Sr=n(Ne," by calling "),cn=a(Ne,"A",{href:!0});var Qc=r(cn);$r=n(Qc,"group_beam_search()"),Qc.forEach(o),Fr=n(Ne,`, if
`),no=a(Ne,"CODE",{});var Yc=r(no);zr=n(Yc,"num_beams>1"),Yc.forEach(o),Ar=n(Ne," and "),oo=a(Ne,"CODE",{});var ep=r(oo);Pr=n(ep,"num_beam_groups>1"),ep.forEach(o),Nr=n(Ne,"."),Ne.forEach(o),Dr=d(_e),ee=a(_e,"LI",{});var De=r(ee);so=a(De,"EM",{});var tp=r(so);Cr=n(tp,"constrained beam-search decoding"),tp.forEach(o),Ir=n(De,` by calling
`),pn=a(De,"A",{href:!0});var np=r(pn);Wr=n(np,"constrained_beam_search()"),np.forEach(o),Br=n(De,", if "),ao=a(De,"CODE",{});var op=r(ao);Hr=n(op,"constraints!=None"),op.forEach(o),Rr=n(De,` or
`),ro=a(De,"CODE",{});var sp=r(ro);Ur=n(sp,"force_words_ids!=None"),sp.forEach(o),Vr=n(De,"."),De.forEach(o),_e.forEach(o),Kr=d(j),g(Ve.$$.fragment,j),Zr=d(j),ct=a(j,"P",{});var us=r(ct);Xr=n(us,"Most of these parameters are explained in more detail in "),pt=a(us,"A",{href:!0,rel:!0});var ap=r(pt);Jr=n(ap,`this blog
post`),ap.forEach(o),Qr=n(us,"."),us.forEach(o),Yr=d(j),io=a(j,"P",{});var rp=r(io);ei=n(rp,"Examples:"),rp.forEach(o),ti=d(j),lo=a(j,"P",{});var ip=r(lo);ni=n(ip,"Greedy Decoding:"),ip.forEach(o),oi=d(j),g(mt.$$.fragment,j),si=d(j),co=a(j,"P",{});var lp=r(co);ai=n(lp,"Multinomial Sampling:"),lp.forEach(o),ri=d(j),g(gt.$$.fragment,j),ii=d(j),po=a(j,"P",{});var dp=r(po);li=n(dp,"Beam-search decoding:"),dp.forEach(o),di=d(j),g(_t.$$.fragment,j),j.forEach(o),ci=d(L),te=a(L,"DIV",{class:!0});var Je=r(te);g(ut.$$.fragment,Je),pi=d(Je),ht=a(Je,"P",{});var hs=r(ht);mi=n(hs,"Generates sequences of token ids for models with a language modeling head using "),mo=a(hs,"STRONG",{});var cp=r(mo);gi=n(cp,"greedy decoding"),cp.forEach(o),_i=n(hs,` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),hs.forEach(o),ui=d(Je),go=a(Je,"P",{});var pp=r(go);hi=n(pp,"Examples:"),pp.forEach(o),fi=d(Je),g(ft.$$.fragment,Je),Je.forEach(o),bi=d(L),ne=a(L,"DIV",{class:!0});var Qe=r(ne);g(bt.$$.fragment,Qe),xi=d(Qe),xt=a(Qe,"P",{});var fs=r(xt);ki=n(fs,"Generates sequences of token ids for models with a language modeling head using "),_o=a(fs,"STRONG",{});var mp=r(_o);vi=n(mp,"multinomial sampling"),mp.forEach(o),yi=n(fs,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),fs.forEach(o),ji=d(Qe),uo=a(Qe,"P",{});var gp=r(uo);Li=n(gp,"Examples:"),gp.forEach(o),Mi=d(Qe),g(kt.$$.fragment,Qe),Qe.forEach(o),wi=d(L),oe=a(L,"DIV",{class:!0});var Ye=r(oe);g(vt.$$.fragment,Ye),Ti=d(Ye),yt=a(Ye,"P",{});var bs=r(yt);Ei=n(bs,"Generates sequences of token ids for models with a language modeling head using "),ho=a(bs,"STRONG",{});var _p=r(ho);Oi=n(_p,"beam search decoding"),_p.forEach(o),qi=n(bs,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),bs.forEach(o),Gi=d(Ye),fo=a(Ye,"P",{});var up=r(fo);Si=n(up,"Examples:"),up.forEach(o),$i=d(Ye),g(jt.$$.fragment,Ye),Ye.forEach(o),Fi=d(L),se=a(L,"DIV",{class:!0});var et=r(se);g(Lt.$$.fragment,et),zi=d(et),Mt=a(et,"P",{});var xs=r(Mt);Ai=n(xs,"Generates sequences of token ids for models with a language modeling head using "),bo=a(xs,"STRONG",{});var hp=r(bo);Pi=n(hp,`beam search multinomial
sampling`),hp.forEach(o),Ni=n(xs," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),xs.forEach(o),Di=d(et),xo=a(et,"P",{});var fp=r(xo);Ci=n(fp,"Examples:"),fp.forEach(o),Ii=d(et),g(wt.$$.fragment,et),et.forEach(o),Wi=d(L),ae=a(L,"DIV",{class:!0});var tt=r(ae);g(Tt.$$.fragment,tt),Bi=d(tt),Et=a(tt,"P",{});var ks=r(Et);Hi=n(ks,"Generates sequences of token ids for models with a language modeling head using "),ko=a(ks,"STRONG",{});var bp=r(ko);Ri=n(bp,`diverse beam search
decoding`),bp.forEach(o),Ui=n(ks," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),ks.forEach(o),Vi=d(tt),vo=a(tt,"P",{});var xp=r(vo);Ki=n(xp,"Examples:"),xp.forEach(o),Zi=d(tt),g(Ot.$$.fragment,tt),tt.forEach(o),Xi=d(L),re=a(L,"DIV",{class:!0});var nt=r(re);g(qt.$$.fragment,nt),Ji=d(nt),Gt=a(nt,"P",{});var vs=r(Gt);Qi=n(vs,"Generates sequences of token ids for models with a language modeling head using "),yo=a(vs,"STRONG",{});var kp=r(yo);Yi=n(kp,`constrained beam search
decoding`),kp.forEach(o),el=n(vs," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),vs.forEach(o),tl=d(nt),jo=a(nt,"P",{});var vp=r(jo);nl=n(vp,"Examples:"),vp.forEach(o),ol=d(nt),g(St.$$.fragment,nt),nt.forEach(o),L.forEach(o),is=d(i),Le=a(i,"H2",{class:!0});var ys=r(Le);Ke=a(ys,"A",{id:!0,class:!0,href:!0});var yp=r(Ke);Lo=a(yp,"SPAN",{});var jp=r(Lo);g($t.$$.fragment,jp),jp.forEach(o),yp.forEach(o),sl=d(ys),Mo=a(ys,"SPAN",{});var Lp=r(Mo);al=n(Lp,"TFGenerationMixin"),Lp.forEach(o),ys.forEach(o),ls=d(i),be=a(i,"DIV",{class:!0});var fn=r(be);g(Ft.$$.fragment,fn),rl=d(fn),zt=a(fn,"P",{});var js=r(zt);il=n(js,"A class containing all of the functions supporting generation, to be used as a mixin in "),mn=a(js,"A",{href:!0});var Mp=r(mn);ll=n(Mp,"TFPreTrainedModel"),Mp.forEach(o),dl=n(js,"."),js.forEach(o),cl=d(fn),E=a(fn,"DIV",{class:!0});var N=r(E);g(At.$$.fragment,N),pl=d(N),wo=a(N,"P",{});var wp=r(wo);ml=n(wp,`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),wp.forEach(o),gl=d(N),Pt=a(N,"P",{});var Ls=r(Pt);_l=n(Ls,"Adapted in part from "),Nt=a(Ls,"A",{href:!0,rel:!0});var Tp=r(Nt);ul=n(Tp,`Facebook\u2019s XLM beam search
code`),Tp.forEach(o),hl=n(Ls,"."),Ls.forEach(o),fl=d(N),xe=a(N,"P",{});var ot=r(xe);bl=n(ot,"Apart from "),To=a(ot,"CODE",{});var Ep=r(To);xl=n(Ep,"input_ids"),Ep.forEach(o),kl=n(ot," and "),Eo=a(ot,"CODE",{});var Op=r(Eo);vl=n(Op,"attention_mask"),Op.forEach(o),yl=n(ot,`, all the arguments below will default to the value of the attribute
of the same name inside the `),gn=a(ot,"A",{href:!0});var qp=r(gn);jl=n(qp,"PretrainedConfig"),qp.forEach(o),Ll=n(ot,` of the model. The default values indicated are the default
values of those config.`),ot.forEach(o),Ml=d(N),Dt=a(N,"P",{});var Ms=r(Dt);wl=n(Ms,"Most of these parameters are explained in more detail in "),Ct=a(Ms,"A",{href:!0,rel:!0});var Gp=r(Ct);Tl=n(Gp,`this blog
post`),Gp.forEach(o),El=n(Ms,"."),Ms.forEach(o),Ol=d(N),Oo=a(N,"P",{});var Sp=r(Oo);ql=n(Sp,"Examples:"),Sp.forEach(o),Gl=d(N),g(It.$$.fragment,N),N.forEach(o),fn.forEach(o),ds=d(i),Me=a(i,"H2",{class:!0});var ws=r(Me);Ze=a(ws,"A",{id:!0,class:!0,href:!0});var $p=r(Ze);qo=a($p,"SPAN",{});var Fp=r(qo);g(Wt.$$.fragment,Fp),Fp.forEach(o),$p.forEach(o),Sl=d(ws),Go=a(ws,"SPAN",{});var zp=r(Go);$l=n(zp,"FlaxGenerationMixin"),zp.forEach(o),ws.forEach(o),cs=d(i),z=a(i,"DIV",{class:!0});var ve=r(z);g(Bt.$$.fragment,ve),Fl=d(ve),Ht=a(ve,"P",{});var Ts=r(Ht);zl=n(Ts,`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),_n=a(Ts,"A",{href:!0});var Ap=r(_n);Al=n(Ap,"FlaxPreTrainedModel"),Ap.forEach(o),Pl=n(Ts,"."),Ts.forEach(o),Nl=d(ve),Rt=a(ve,"P",{});var Es=r(Rt);Dl=n(Es,"The class exposes "),un=a(Es,"A",{href:!0});var Pp=r(un);Cl=n(Pp,"generate()"),Pp.forEach(o),Il=n(Es,", which can be used for:"),Es.forEach(o),Wl=d(ve),we=a(ve,"UL",{});var bn=r(we);ie=a(bn,"LI",{});var Ce=r(ie);So=a(Ce,"EM",{});var Np=r(So);Bl=n(Np,"greedy decoding"),Np.forEach(o),Hl=n(Ce," by calling "),$o=a(Ce,"CODE",{});var Dp=r($o);Rl=n(Dp,"_greedy_search()"),Dp.forEach(o),Ul=n(Ce,`if
`),Fo=a(Ce,"CODE",{});var Cp=r(Fo);Vl=n(Cp,"num_beams=1"),Cp.forEach(o),Kl=n(Ce," and "),zo=a(Ce,"CODE",{});var Ip=r(zo);Zl=n(Ip,"do_sample=False"),Ip.forEach(o),Xl=n(Ce,"."),Ce.forEach(o),Jl=d(bn),le=a(bn,"LI",{});var Ie=r(le);Ao=a(Ie,"EM",{});var Wp=r(Ao);Ql=n(Wp,"multinomial sampling"),Wp.forEach(o),Yl=n(Ie," by calling "),Po=a(Ie,"CODE",{});var Bp=r(Po);ed=n(Bp,"_sample()"),Bp.forEach(o),td=n(Ie,"if "),No=a(Ie,"CODE",{});var Hp=r(No);nd=n(Hp,"num_beams=1"),Hp.forEach(o),od=n(Ie,`
and `),Do=a(Ie,"CODE",{});var Rp=r(Do);sd=n(Rp,"do_sample=True"),Rp.forEach(o),ad=n(Ie,"."),Ie.forEach(o),rd=d(bn),de=a(bn,"LI",{});var We=r(de);Co=a(We,"EM",{});var Up=r(Co);id=n(Up,"beam-search decoding"),Up.forEach(o),ld=n(We," by calling "),Io=a(We,"CODE",{});var Vp=r(Io);dd=n(Vp,"_beam_search"),Vp.forEach(o),cd=n(We," if "),Wo=a(We,"CODE",{});var Kp=r(Wo);pd=n(Kp,"num_beams>1"),Kp.forEach(o),md=n(We,`
and `),Bo=a(We,"CODE",{});var Zp=r(Bo);gd=n(Zp,"do_sample=False"),Zp.forEach(o),_d=n(We,"."),We.forEach(o),bn.forEach(o),ud=d(ve),O=a(ve,"DIV",{class:!0});var D=r(O);g(Ut.$$.fragment,D),hd=d(D),Ho=a(D,"P",{});var Xp=r(Ho);fd=n(Xp,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Xp.forEach(o),bd=d(D),Te=a(D,"UL",{});var xn=r(Te);ce=a(xn,"LI",{});var Be=r(ce);Ro=a(Be,"EM",{});var Jp=r(Ro);xd=n(Jp,"greedy decoding"),Jp.forEach(o),kd=n(Be," by calling "),Uo=a(Be,"CODE",{});var Qp=r(Uo);vd=n(Qp,"_greedy_search()"),Qp.forEach(o),yd=n(Be,`if
`),Vo=a(Be,"CODE",{});var Yp=r(Vo);jd=n(Yp,"num_beams=1"),Yp.forEach(o),Ld=n(Be," and "),Ko=a(Be,"CODE",{});var em=r(Ko);Md=n(em,"do_sample=False"),em.forEach(o),wd=n(Be,"."),Be.forEach(o),Td=d(xn),pe=a(xn,"LI",{});var He=r(pe);Zo=a(He,"EM",{});var tm=r(Zo);Ed=n(tm,"multinomial sampling"),tm.forEach(o),Od=n(He," by calling "),Xo=a(He,"CODE",{});var nm=r(Xo);qd=n(nm,"_sample()"),nm.forEach(o),Gd=n(He,"if "),Jo=a(He,"CODE",{});var om=r(Jo);Sd=n(om,"num_beams=1"),om.forEach(o),$d=n(He,`
and `),Qo=a(He,"CODE",{});var sm=r(Qo);Fd=n(sm,"do_sample=True"),sm.forEach(o),zd=n(He,"."),He.forEach(o),Ad=d(xn),me=a(xn,"LI",{});var Re=r(me);Yo=a(Re,"EM",{});var am=r(Yo);Pd=n(am,"beam-search decoding"),am.forEach(o),Nd=n(Re," by calling "),es=a(Re,"CODE",{});var rm=r(es);Dd=n(rm,"_beam_search"),rm.forEach(o),Cd=n(Re," if "),ts=a(Re,"CODE",{});var im=r(ts);Id=n(im,"num_beams>1"),im.forEach(o),Wd=n(Re,`
and `),ns=a(Re,"CODE",{});var lm=r(ns);Bd=n(lm,"do_sample=False"),lm.forEach(o),Hd=n(Re,"."),Re.forEach(o),xn.forEach(o),Rd=d(D),g(Xe.$$.fragment,D),Ud=d(D),Vt=a(D,"P",{});var Os=r(Vt);Vd=n(Os,"Most of these parameters are explained in more detail in "),Kt=a(Os,"A",{href:!0,rel:!0});var dm=r(Kt);Kd=n(dm,`this blog
post`),dm.forEach(o),Zd=n(Os,"."),Os.forEach(o),Xd=d(D),os=a(D,"P",{});var cm=r(os);Jd=n(cm,"Examples:"),cm.forEach(o),Qd=d(D),g(Zt.$$.fragment,D),D.forEach(o),ve.forEach(o),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(xm)),c(w,"id","generation"),c(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(w,"href","#generation"),c(y,"class","relative group"),c(Ue,"id","transformers.generation_utils.GenerationMixin"),c(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ue,"href","#transformers.generation_utils.GenerationMixin"),c(je,"class","relative group"),c(Jt,"href","/docs/transformers/pr_16133/en/main_classes/model#transformers.PreTrainedModel"),c(Qt,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(Yt,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(en,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(tn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(nn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(on,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(sn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(an,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(rn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(ln,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(dn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(cn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(pn,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(pt,"href","https://huggingface.co/blog/how-to-generate"),c(pt,"rel","nofollow"),c(b,"class","docstring"),c(te,"class","docstring"),c(ne,"class","docstring"),c(oe,"class","docstring"),c(se,"class","docstring"),c(ae,"class","docstring"),c(re,"class","docstring"),c(x,"class","docstring"),c(Ke,"id","transformers.generation_tf_utils.TFGenerationMixin"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.generation_tf_utils.TFGenerationMixin"),c(Le,"class","relative group"),c(mn,"href","/docs/transformers/pr_16133/en/main_classes/model#transformers.TFPreTrainedModel"),c(Nt,"href","https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529"),c(Nt,"rel","nofollow"),c(gn,"href","/docs/transformers/pr_16133/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ct,"href","https://huggingface.co/blog/how-to-generate"),c(Ct,"rel","nofollow"),c(E,"class","docstring"),c(be,"class","docstring"),c(Ze,"id","transformers.generation_flax_utils.FlaxGenerationMixin"),c(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ze,"href","#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Me,"class","relative group"),c(_n,"href","/docs/transformers/pr_16133/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(un,"href","/docs/transformers/pr_16133/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(Kt,"href","https://huggingface.co/blog/how-to-generate"),c(Kt,"rel","nofollow"),c(O,"class","docstring"),c(z,"class","docstring")},m(i,v){e(document.head,p),M(i,A,v),M(i,y,v),e(y,w),e(w,C),_(T,C,null),e(y,he),e(y,I),e(I,q),M(i,W,v),M(i,P,v),e(P,S),M(i,k,v),M(i,G,v),e(G,fe),e(fe,ye),e(G,Gs),e(G,kn),e(kn,Ss),e(G,$s),e(G,vn),e(vn,Fs),M(i,as,v),M(i,je,v),e(je,Ue),e(Ue,yn),_(at,yn,null),e(je,zs),e(je,jn),e(jn,As),M(i,rs,v),M(i,x,v),_(rt,x,null),e(x,Ps),e(x,it),e(it,Ns),e(it,Jt),e(Jt,Ds),e(it,Cs),e(x,Is),e(x,lt),e(lt,Ws),e(lt,Qt),e(Qt,Bs),e(lt,Hs),e(x,Rs),e(x,$),e($,B),e(B,Ln),e(Ln,Us),e(B,Vs),e(B,Yt),e(Yt,Ks),e(B,Zs),e(B,Mn),e(Mn,Xs),e(B,Js),e(B,wn),e(wn,Qs),e(B,Ys),e($,ea),e($,H),e(H,Tn),e(Tn,ta),e(H,na),e(H,en),e(en,oa),e(H,sa),e(H,En),e(En,aa),e(H,ra),e(H,On),e(On,ia),e(H,la),e($,da),e($,R),e(R,qn),e(qn,ca),e(R,pa),e(R,tn),e(tn,ma),e(R,ga),e(R,Gn),e(Gn,_a),e(R,ua),e(R,Sn),e(Sn,ha),e(R,fa),e($,ba),e($,U),e(U,$n),e($n,xa),e(U,ka),e(U,nn),e(nn,va),e(U,ya),e(U,Fn),e(Fn,ja),e(U,La),e(U,zn),e(zn,Ma),e(U,wa),e($,Ta),e($,V),e(V,An),e(An,Ea),e(V,Oa),e(V,on),e(on,qa),e(V,Ga),e(V,Pn),e(Pn,Sa),e(V,$a),e(V,Nn),e(Nn,Fa),e(V,za),e($,Aa),e($,K),e(K,Dn),e(Dn,Pa),e(K,Na),e(K,sn),e(sn,Da),e(K,Ca),e(K,Cn),e(Cn,Ia),e(K,Wa),e(K,In),e(In,Ba),e(K,Ha),e(x,Ra),e(x,b),_(dt,b,null),e(b,Ua),e(b,Wn),e(Wn,Va),e(b,Ka),e(b,F),e(F,Z),e(Z,Bn),e(Bn,Za),e(Z,Xa),e(Z,an),e(an,Ja),e(Z,Qa),e(Z,Hn),e(Hn,Ya),e(Z,er),e(Z,Rn),e(Rn,tr),e(Z,nr),e(F,or),e(F,X),e(X,Un),e(Un,sr),e(X,ar),e(X,rn),e(rn,rr),e(X,ir),e(X,Vn),e(Vn,lr),e(X,dr),e(X,Kn),e(Kn,cr),e(X,pr),e(F,mr),e(F,J),e(J,Zn),e(Zn,gr),e(J,_r),e(J,ln),e(ln,ur),e(J,hr),e(J,Xn),e(Xn,fr),e(J,br),e(J,Jn),e(Jn,xr),e(J,kr),e(F,vr),e(F,Q),e(Q,Qn),e(Qn,yr),e(Q,jr),e(Q,dn),e(dn,Lr),e(Q,Mr),e(Q,Yn),e(Yn,wr),e(Q,Tr),e(Q,eo),e(eo,Er),e(Q,Or),e(F,qr),e(F,Y),e(Y,to),e(to,Gr),e(Y,Sr),e(Y,cn),e(cn,$r),e(Y,Fr),e(Y,no),e(no,zr),e(Y,Ar),e(Y,oo),e(oo,Pr),e(Y,Nr),e(F,Dr),e(F,ee),e(ee,so),e(so,Cr),e(ee,Ir),e(ee,pn),e(pn,Wr),e(ee,Br),e(ee,ao),e(ao,Hr),e(ee,Rr),e(ee,ro),e(ro,Ur),e(ee,Vr),e(b,Kr),_(Ve,b,null),e(b,Zr),e(b,ct),e(ct,Xr),e(ct,pt),e(pt,Jr),e(ct,Qr),e(b,Yr),e(b,io),e(io,ei),e(b,ti),e(b,lo),e(lo,ni),e(b,oi),_(mt,b,null),e(b,si),e(b,co),e(co,ai),e(b,ri),_(gt,b,null),e(b,ii),e(b,po),e(po,li),e(b,di),_(_t,b,null),e(x,ci),e(x,te),_(ut,te,null),e(te,pi),e(te,ht),e(ht,mi),e(ht,mo),e(mo,gi),e(ht,_i),e(te,ui),e(te,go),e(go,hi),e(te,fi),_(ft,te,null),e(x,bi),e(x,ne),_(bt,ne,null),e(ne,xi),e(ne,xt),e(xt,ki),e(xt,_o),e(_o,vi),e(xt,yi),e(ne,ji),e(ne,uo),e(uo,Li),e(ne,Mi),_(kt,ne,null),e(x,wi),e(x,oe),_(vt,oe,null),e(oe,Ti),e(oe,yt),e(yt,Ei),e(yt,ho),e(ho,Oi),e(yt,qi),e(oe,Gi),e(oe,fo),e(fo,Si),e(oe,$i),_(jt,oe,null),e(x,Fi),e(x,se),_(Lt,se,null),e(se,zi),e(se,Mt),e(Mt,Ai),e(Mt,bo),e(bo,Pi),e(Mt,Ni),e(se,Di),e(se,xo),e(xo,Ci),e(se,Ii),_(wt,se,null),e(x,Wi),e(x,ae),_(Tt,ae,null),e(ae,Bi),e(ae,Et),e(Et,Hi),e(Et,ko),e(ko,Ri),e(Et,Ui),e(ae,Vi),e(ae,vo),e(vo,Ki),e(ae,Zi),_(Ot,ae,null),e(x,Xi),e(x,re),_(qt,re,null),e(re,Ji),e(re,Gt),e(Gt,Qi),e(Gt,yo),e(yo,Yi),e(Gt,el),e(re,tl),e(re,jo),e(jo,nl),e(re,ol),_(St,re,null),M(i,is,v),M(i,Le,v),e(Le,Ke),e(Ke,Lo),_($t,Lo,null),e(Le,sl),e(Le,Mo),e(Mo,al),M(i,ls,v),M(i,be,v),_(Ft,be,null),e(be,rl),e(be,zt),e(zt,il),e(zt,mn),e(mn,ll),e(zt,dl),e(be,cl),e(be,E),_(At,E,null),e(E,pl),e(E,wo),e(wo,ml),e(E,gl),e(E,Pt),e(Pt,_l),e(Pt,Nt),e(Nt,ul),e(Pt,hl),e(E,fl),e(E,xe),e(xe,bl),e(xe,To),e(To,xl),e(xe,kl),e(xe,Eo),e(Eo,vl),e(xe,yl),e(xe,gn),e(gn,jl),e(xe,Ll),e(E,Ml),e(E,Dt),e(Dt,wl),e(Dt,Ct),e(Ct,Tl),e(Dt,El),e(E,Ol),e(E,Oo),e(Oo,ql),e(E,Gl),_(It,E,null),M(i,ds,v),M(i,Me,v),e(Me,Ze),e(Ze,qo),_(Wt,qo,null),e(Me,Sl),e(Me,Go),e(Go,$l),M(i,cs,v),M(i,z,v),_(Bt,z,null),e(z,Fl),e(z,Ht),e(Ht,zl),e(Ht,_n),e(_n,Al),e(Ht,Pl),e(z,Nl),e(z,Rt),e(Rt,Dl),e(Rt,un),e(un,Cl),e(Rt,Il),e(z,Wl),e(z,we),e(we,ie),e(ie,So),e(So,Bl),e(ie,Hl),e(ie,$o),e($o,Rl),e(ie,Ul),e(ie,Fo),e(Fo,Vl),e(ie,Kl),e(ie,zo),e(zo,Zl),e(ie,Xl),e(we,Jl),e(we,le),e(le,Ao),e(Ao,Ql),e(le,Yl),e(le,Po),e(Po,ed),e(le,td),e(le,No),e(No,nd),e(le,od),e(le,Do),e(Do,sd),e(le,ad),e(we,rd),e(we,de),e(de,Co),e(Co,id),e(de,ld),e(de,Io),e(Io,dd),e(de,cd),e(de,Wo),e(Wo,pd),e(de,md),e(de,Bo),e(Bo,gd),e(de,_d),e(z,ud),e(z,O),_(Ut,O,null),e(O,hd),e(O,Ho),e(Ho,fd),e(O,bd),e(O,Te),e(Te,ce),e(ce,Ro),e(Ro,xd),e(ce,kd),e(ce,Uo),e(Uo,vd),e(ce,yd),e(ce,Vo),e(Vo,jd),e(ce,Ld),e(ce,Ko),e(Ko,Md),e(ce,wd),e(Te,Td),e(Te,pe),e(pe,Zo),e(Zo,Ed),e(pe,Od),e(pe,Xo),e(Xo,qd),e(pe,Gd),e(pe,Jo),e(Jo,Sd),e(pe,$d),e(pe,Qo),e(Qo,Fd),e(pe,zd),e(Te,Ad),e(Te,me),e(me,Yo),e(Yo,Pd),e(me,Nd),e(me,es),e(es,Dd),e(me,Cd),e(me,ts),e(ts,Id),e(me,Wd),e(me,ns),e(ns,Bd),e(me,Hd),e(O,Rd),_(Xe,O,null),e(O,Ud),e(O,Vt),e(Vt,Vd),e(Vt,Kt),e(Kt,Kd),e(Vt,Zd),e(O,Xd),e(O,os),e(os,Jd),e(O,Qd),_(Zt,O,null),ps=!0},p(i,[v]){const Xt={};v&2&&(Xt.$$scope={dirty:v,ctx:i}),Ve.$set(Xt);const ss={};v&2&&(ss.$$scope={dirty:v,ctx:i}),Xe.$set(ss)},i(i){ps||(u(T.$$.fragment,i),u(at.$$.fragment,i),u(rt.$$.fragment,i),u(dt.$$.fragment,i),u(Ve.$$.fragment,i),u(mt.$$.fragment,i),u(gt.$$.fragment,i),u(_t.$$.fragment,i),u(ut.$$.fragment,i),u(ft.$$.fragment,i),u(bt.$$.fragment,i),u(kt.$$.fragment,i),u(vt.$$.fragment,i),u(jt.$$.fragment,i),u(Lt.$$.fragment,i),u(wt.$$.fragment,i),u(Tt.$$.fragment,i),u(Ot.$$.fragment,i),u(qt.$$.fragment,i),u(St.$$.fragment,i),u($t.$$.fragment,i),u(Ft.$$.fragment,i),u(At.$$.fragment,i),u(It.$$.fragment,i),u(Wt.$$.fragment,i),u(Bt.$$.fragment,i),u(Ut.$$.fragment,i),u(Xe.$$.fragment,i),u(Zt.$$.fragment,i),ps=!0)},o(i){h(T.$$.fragment,i),h(at.$$.fragment,i),h(rt.$$.fragment,i),h(dt.$$.fragment,i),h(Ve.$$.fragment,i),h(mt.$$.fragment,i),h(gt.$$.fragment,i),h(_t.$$.fragment,i),h(ut.$$.fragment,i),h(ft.$$.fragment,i),h(bt.$$.fragment,i),h(kt.$$.fragment,i),h(vt.$$.fragment,i),h(jt.$$.fragment,i),h(Lt.$$.fragment,i),h(wt.$$.fragment,i),h(Tt.$$.fragment,i),h(Ot.$$.fragment,i),h(qt.$$.fragment,i),h(St.$$.fragment,i),h($t.$$.fragment,i),h(Ft.$$.fragment,i),h(At.$$.fragment,i),h(It.$$.fragment,i),h(Wt.$$.fragment,i),h(Bt.$$.fragment,i),h(Ut.$$.fragment,i),h(Xe.$$.fragment,i),h(Zt.$$.fragment,i),ps=!1},d(i){o(p),i&&o(A),i&&o(y),f(T),i&&o(W),i&&o(P),i&&o(k),i&&o(G),i&&o(as),i&&o(je),f(at),i&&o(rs),i&&o(x),f(rt),f(dt),f(Ve),f(mt),f(gt),f(_t),f(ut),f(ft),f(bt),f(kt),f(vt),f(jt),f(Lt),f(wt),f(Tt),f(Ot),f(qt),f(St),i&&o(is),i&&o(Le),f($t),i&&o(ls),i&&o(be),f(Ft),f(At),f(It),i&&o(ds),i&&o(Me),f(Wt),i&&o(cs),i&&o(z),f(Bt),f(Ut),f(Xe),f(Zt)}}}const xm={local:"generation",sections:[{local:"transformers.generation_utils.GenerationMixin",title:"GenerationMixin"},{local:"transformers.generation_tf_utils.TFGenerationMixin",title:"TFGenerationMixin"},{local:"transformers.generation_flax_utils.FlaxGenerationMixin",title:"FlaxGenerationMixin"}],title:"Generation"};function km(st,p,A){let{fw:y}=p;return st.$$set=w=>{"fw"in w&&A(0,y=w.fw)},[y]}class Tm extends mm{constructor(p){super();gm(this,p,km,bm,_m,{fw:0})}}export{Tm as default,xm as metadata};
