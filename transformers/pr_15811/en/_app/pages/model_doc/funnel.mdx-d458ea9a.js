import{S as F$,i as v$,s as k$,e as s,k as l,w as k,t,L as w$,c as r,d as n,m as d,a,x as w,h as o,b as c,J as e,g as h,y as b,q as y,o as $,B as E}from"../../chunks/vendor-9e2b328e.js";import{T as ze}from"../../chunks/Tip-76f97a76.js";import{D as ee}from"../../chunks/Docstring-50fd6873.js";import{C as Ne}from"../../chunks/CodeBlock-88e23343.js";import{I as qe}from"../../chunks/IconCopyLink-fd0e58fd.js";import"../../chunks/CopyButton-4ae140ab.js";function b$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function y$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function $$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function E$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function M$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function z$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function q$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function P$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function C$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function x$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function j$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function L$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function A$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function D$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function I$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function O$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function S$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function N$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function B$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function W$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Q$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function R$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function H$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),S=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var be=a(O);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(n),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,ne),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(q),u&&n(U),u&&n(L),u&&n(Z),u&&n(P)}}}function V$(W){let p,M,m,g,F;return{c(){p=s("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Y$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,ne,ue,O,pe,ie,U,L,te,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge,u,v,K,Te,we,D,Fe,be,ye,j,V,$e,ve,Y,Ee,ke,_e,Me,Va,Dp,Ip,yc,jn,Op,Io,Sp,Np,Oo,Bp,Wp,$c,Kn,Nt,ll,So,Qp,dl,Rp,Ec,Cn,No,Hp,xn,Vp,Ya,Yp,Up,Ua,Gp,Zp,Bo,Kp,Xp,Jp,Xn,eh,Ga,nh,th,Za,oh,sh,Mc,Jn,Bt,cl,Wo,rh,ul,ah,zc,Pe,Qo,ih,pl,lh,dh,Wt,Ka,ch,uh,Xa,ph,hh,fh,Ro,mh,Ja,gh,_h,Th,Ln,Ho,Fh,hl,vh,kh,Vo,ei,wh,fl,bh,yh,ni,$h,ml,Eh,Mh,Qt,Yo,zh,Uo,qh,gl,Ph,Ch,xh,wn,Go,jh,_l,Lh,Ah,Zo,Dh,et,Ih,Tl,Oh,Sh,Fl,Nh,Bh,Wh,vl,qc,nt,Rt,kl,Ko,Qh,wl,Rh,Pc,Ze,Xo,Hh,Jo,Vh,bl,Yh,Uh,Gh,Ht,ti,Zh,Kh,oi,Xh,Jh,ef,es,nf,si,tf,of,sf,bn,ns,rf,yl,af,lf,ts,df,tt,cf,$l,uf,pf,El,hf,ff,Cc,ot,Vt,Ml,os,mf,zl,gf,xc,st,ss,_f,rs,Tf,ri,Ff,vf,jc,rt,as,kf,is,wf,ai,bf,yf,Lc,at,Yt,ql,ls,$f,Pl,Ef,Ac,We,ds,Mf,Cl,zf,qf,cs,Pf,us,Cf,xf,jf,ps,Lf,ii,Af,Df,If,hs,Of,fs,Sf,Nf,Bf,Ke,ms,Wf,it,Qf,li,Rf,Hf,xl,Vf,Yf,Uf,Ut,Gf,jl,Zf,Kf,gs,Dc,lt,Gt,Ll,_s,Xf,Al,Jf,Ic,Qe,Ts,em,Dl,nm,tm,Fs,om,vs,sm,rm,am,ks,im,di,lm,dm,cm,ws,um,bs,pm,hm,fm,Xe,ys,mm,dt,gm,ci,_m,Tm,Il,Fm,vm,km,Zt,wm,Ol,bm,ym,$s,Oc,ct,Kt,Sl,Es,$m,Nl,Em,Sc,Ms,Je,zs,Mm,ut,zm,ui,qm,Pm,Bl,Cm,xm,jm,Xt,Lm,Wl,Am,Dm,qs,Nc,pt,Jt,Ql,Ps,Im,Rl,Om,Bc,Re,Cs,Sm,xs,Nm,Hl,Bm,Wm,Qm,js,Rm,Ls,Hm,Vm,Ym,As,Um,pi,Gm,Zm,Km,Ds,Xm,Is,Jm,eg,ng,en,Os,tg,ht,og,hi,sg,rg,Vl,ag,ig,lg,eo,dg,Yl,cg,ug,Ss,Wc,ft,no,Ul,Ns,pg,Gl,hg,Qc,He,Bs,fg,Zl,mg,gg,Ws,_g,Qs,Tg,Fg,vg,Rs,kg,fi,wg,bg,yg,Hs,$g,Vs,Eg,Mg,zg,Be,Ys,qg,mt,Pg,mi,Cg,xg,Kl,jg,Lg,Ag,to,Dg,Xl,Ig,Og,Us,Sg,Jl,Ng,Bg,Gs,Rc,gt,oo,ed,Zs,Wg,nd,Qg,Hc,Ve,Ks,Rg,td,Hg,Vg,Xs,Yg,Js,Ug,Gg,Zg,er,Kg,gi,Xg,Jg,e_,nr,n_,tr,t_,o_,s_,nn,or,r_,_t,a_,_i,i_,l_,od,d_,c_,u_,so,p_,sd,h_,f_,sr,Vc,Tt,ro,rd,rr,m_,ad,g_,Yc,Ye,ar,__,id,T_,F_,ir,v_,lr,k_,w_,b_,dr,y_,Ti,$_,E_,M_,cr,z_,ur,q_,P_,C_,tn,pr,x_,Ft,j_,Fi,L_,A_,ld,D_,I_,O_,ao,S_,dd,N_,B_,hr,Uc,vt,io,cd,fr,W_,ud,Q_,Gc,Ue,mr,R_,kt,H_,pd,V_,Y_,hd,U_,G_,Z_,gr,K_,_r,X_,J_,e1,Tr,n1,vi,t1,o1,s1,Fr,r1,vr,a1,i1,l1,on,kr,d1,wt,c1,ki,u1,p1,fd,h1,f1,m1,lo,g1,md,_1,T1,wr,Zc,bt,co,gd,br,F1,_d,v1,Kc,xe,yr,k1,Td,w1,b1,$r,y1,Er,$1,E1,M1,Mr,z1,wi,q1,P1,C1,zr,x1,qr,j1,L1,A1,uo,D1,sn,Pr,I1,yt,O1,bi,S1,N1,Fd,B1,W1,Q1,po,R1,vd,H1,V1,Cr,Xc,$t,ho,kd,xr,Y1,wd,U1,Jc,je,jr,G1,bd,Z1,K1,Lr,X1,Ar,J1,eT,nT,Dr,tT,yi,oT,sT,rT,Ir,aT,Or,iT,lT,dT,fo,cT,rn,Sr,uT,Et,pT,$i,hT,fT,yd,mT,gT,_T,mo,TT,$d,FT,vT,Nr,eu,Mt,go,Ed,Br,kT,Md,wT,nu,Le,Wr,bT,zd,yT,$T,Qr,ET,Rr,MT,zT,qT,Hr,PT,Ei,CT,xT,jT,Vr,LT,Yr,AT,DT,IT,_o,OT,an,Ur,ST,zt,NT,Mi,BT,WT,qd,QT,RT,HT,To,VT,Pd,YT,UT,Gr,tu,qt,Fo,Cd,Zr,GT,xd,ZT,ou,Ae,Kr,KT,Xr,XT,jd,JT,eF,nF,Jr,tF,ea,oF,sF,rF,na,aF,zi,iF,lF,dF,ta,cF,oa,uF,pF,hF,vo,fF,ln,sa,mF,Pt,gF,qi,_F,TF,Ld,FF,vF,kF,ko,wF,Ad,bF,yF,ra,su,Ct,wo,Dd,aa,$F,Id,EF,ru,De,ia,MF,Od,zF,qF,la,PF,da,CF,xF,jF,ca,LF,Pi,AF,DF,IF,ua,OF,pa,SF,NF,BF,bo,WF,dn,ha,QF,xt,RF,Ci,HF,VF,Sd,YF,UF,GF,yo,ZF,Nd,KF,XF,fa,au,jt,$o,Bd,ma,JF,Wd,ev,iu,Ie,ga,nv,Qd,tv,ov,_a,sv,Ta,rv,av,iv,Fa,lv,xi,dv,cv,uv,va,pv,ka,hv,fv,mv,Eo,gv,cn,wa,_v,Lt,Tv,ji,Fv,vv,Rd,kv,wv,bv,Mo,yv,Hd,$v,Ev,ba,lu,At,zo,Vd,ya,Mv,Yd,zv,du,Oe,$a,qv,Ud,Pv,Cv,Ea,xv,Ma,jv,Lv,Av,za,Dv,Li,Iv,Ov,Sv,qa,Nv,Pa,Bv,Wv,Qv,qo,Rv,un,Ca,Hv,Dt,Vv,Ai,Yv,Uv,Gd,Gv,Zv,Kv,Po,Xv,Zd,Jv,ek,xa,cu,It,Co,Kd,ja,nk,Xd,tk,uu,Se,La,ok,Ot,sk,Jd,rk,ak,ec,ik,lk,dk,Aa,ck,Da,uk,pk,hk,Ia,fk,Di,mk,gk,_k,Oa,Tk,Sa,Fk,vk,kk,xo,wk,pn,Na,bk,St,yk,Ii,$k,Ek,nc,Mk,zk,qk,jo,Pk,tc,Ck,xk,Ba,pu;return T=new qe({}),ne=new qe({}),So=new qe({}),No=new ee({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Wo=new qe({}),Qo=new ee({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new ee({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new ee({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new ee({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Ko=new qe({}),Xo=new ee({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),ns=new ee({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ts=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),os=new qe({}),ss=new ee({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L801",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),as=new ee({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1005",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new qe({}),ds=new ee({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L894",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ms=new ee({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L910",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ut=new ze({props:{$$slots:{default:[b$]},$$scope:{ctx:W}}}),gs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),_s=new qe({}),Ts=new ee({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L971",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ys=new ee({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L988",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new ze({props:{$$slots:{default:[y$]},$$scope:{ctx:W}}}),$s=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Es=new qe({}),zs=new ee({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1088",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xt=new ze({props:{$$slots:{default:[$$]},$$scope:{ctx:W}}}),qs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),Ps=new qe({}),Cs=new ee({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1162",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Os=new ee({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1178",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),eo=new ze({props:{$$slots:{default:[E$]},$$scope:{ctx:W}}}),Ss=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ns=new qe({}),Bs=new ee({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1242",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ys=new ee({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1253",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),to=new ze({props:{$$slots:{default:[M$]},$$scope:{ctx:W}}}),Us=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Gs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Zs=new qe({}),Ks=new ee({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1335",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),or=new ee({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1344",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),so=new ze({props:{$$slots:{default:[z$]},$$scope:{ctx:W}}}),sr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),rr=new qe({}),ar=new ee({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1419",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),pr=new ee({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1431",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ao=new ze({props:{$$slots:{default:[q$]},$$scope:{ctx:W}}}),hr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),fr=new qe({}),mr=new ee({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1493",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),kr=new ee({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_funnel.py#L1504",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),lo=new ze({props:{$$slots:{default:[P$]},$$scope:{ctx:W}}}),wr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),br=new qe({}),yr=new ee({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1122",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),uo=new ze({props:{$$slots:{default:[C$]},$$scope:{ctx:W}}}),Pr=new ee({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1127",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),po=new ze({props:{$$slots:{default:[x$]},$$scope:{ctx:W}}}),Cr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),xr=new qe({}),jr=new ee({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1183",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fo=new ze({props:{$$slots:{default:[j$]},$$scope:{ctx:W}}}),Sr=new ee({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1188",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),mo=new ze({props:{$$slots:{default:[L$]},$$scope:{ctx:W}}}),Nr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Br=new qe({}),Wr=new ee({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1246",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_o=new ze({props:{$$slots:{default:[A$]},$$scope:{ctx:W}}}),Ur=new ee({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1253",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),To=new ze({props:{$$slots:{default:[D$]},$$scope:{ctx:W}}}),Gr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Zr=new qe({}),Kr=new ee({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1325",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[I$]},$$scope:{ctx:W}}}),sa=new ee({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1339",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ko=new ze({props:{$$slots:{default:[O$]},$$scope:{ctx:W}}}),ra=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),aa=new qe({}),ia=new ee({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1420",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bo=new ze({props:{$$slots:{default:[S$]},$$scope:{ctx:W}}}),ha=new ee({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1428",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[N$]},$$scope:{ctx:W}}}),fa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ma=new qe({}),ga=new ee({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1510",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Eo=new ze({props:{$$slots:{default:[B$]},$$scope:{ctx:W}}}),wa=new ee({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1527",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Mo=new ze({props:{$$slots:{default:[W$]},$$scope:{ctx:W}}}),ba=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ya=new qe({}),$a=new ee({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1645",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qo=new ze({props:{$$slots:{default:[Q$]},$$scope:{ctx:W}}}),Ca=new ee({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1656",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Po=new ze({props:{$$slots:{default:[R$]},$$scope:{ctx:W}}}),xa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ja=new qe({}),La=new ee({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1737",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[H$]},$$scope:{ctx:W}}}),Na=new ee({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15811/src/transformers/models/funnel/modeling_tf_funnel.py#L1747",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/pr_15811/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15811/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_15811/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),jo=new ze({props:{$$slots:{default:[V$]},$$scope:{ctx:W}}}),Ba=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=s("meta"),M=l(),m=s("h1"),g=s("a"),F=s("span"),k(T.$$.fragment),_=l(),z=s("span"),ce=t("Funnel Transformer"),G=l(),q=s("h2"),X=s("a"),I=s("span"),k(ne.$$.fragment),ue=l(),O=s("span"),pe=t("Overview"),ie=l(),U=s("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=s("a"),Z=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),x=l(),oe=s("p"),Q=t("The abstract from the paper is the following:"),le=l(),se=s("p"),S=s("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=s("p"),fe=t("Tips:"),B=l(),J=s("ul"),ae=s("li"),R=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),N=s("li"),A=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=s("a"),H=t("FunnelModel"),ge=t(", "),u=s("a"),v=t("FunnelForPreTraining"),K=t(`,
`),Te=s("a"),we=t("FunnelForMaskedLM"),D=t(", "),Fe=s("a"),be=t("FunnelForTokenClassification"),ye=t(` and
class:`),j=s("em"),V=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),ve=s("a"),Y=t("FunnelBaseModel"),Ee=t(", "),ke=s("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Va=s("a"),Dp=t("FunnelForMultipleChoice"),Ip=t("."),yc=l(),jn=s("p"),Op=t("This model was contributed by "),Io=s("a"),Sp=t("sgugger"),Np=t(". The original code can be found "),Oo=s("a"),Bp=t("here"),Wp=t("."),$c=l(),Kn=s("h2"),Nt=s("a"),ll=s("span"),k(So.$$.fragment),Qp=l(),dl=s("span"),Rp=t("FunnelConfig"),Ec=l(),Cn=s("div"),k(No.$$.fragment),Hp=l(),xn=s("p"),Vp=t("This is the configuration class to store the configuration of a "),Ya=s("a"),Yp=t("FunnelModel"),Up=t(" or a "),Ua=s("a"),Gp=t("TFBertModel"),Zp=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Bo=s("a"),Kp=t("funnel-transformer/small"),Xp=t(" architecture."),Jp=l(),Xn=s("p"),eh=t("Configuration objects inherit from "),Ga=s("a"),nh=t("PretrainedConfig"),th=t(` and can be used to control the model outputs. Read the
documentation from `),Za=s("a"),oh=t("PretrainedConfig"),sh=t(" for more information."),Mc=l(),Jn=s("h2"),Bt=s("a"),cl=s("span"),k(Wo.$$.fragment),rh=l(),ul=s("span"),ah=t("FunnelTokenizer"),zc=l(),Pe=s("div"),k(Qo.$$.fragment),ih=l(),pl=s("p"),lh=t("Construct a Funnel Transformer tokenizer."),dh=l(),Wt=s("p"),Ka=s("a"),ch=t("FunnelTokenizer"),uh=t(" is identical to "),Xa=s("a"),ph=t("BertTokenizer"),hh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),fh=l(),Ro=s("p"),mh=t("Refer to superclass "),Ja=s("a"),gh=t("BertTokenizer"),_h=t(" for usage examples and documentation concerning parameters."),Th=l(),Ln=s("div"),k(Ho.$$.fragment),Fh=l(),hl=s("p"),vh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),kh=l(),Vo=s("ul"),ei=s("li"),wh=t("single sequence: "),fl=s("code"),bh=t("[CLS] X [SEP]"),yh=l(),ni=s("li"),$h=t("pair of sequences: "),ml=s("code"),Eh=t("[CLS] A [SEP] B [SEP]"),Mh=l(),Qt=s("div"),k(Yo.$$.fragment),zh=l(),Uo=s("p"),qh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),gl=s("code"),Ph=t("prepare_for_model"),Ch=t(" method."),xh=l(),wn=s("div"),k(Go.$$.fragment),jh=l(),_l=s("p"),Lh=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Ah=l(),k(Zo.$$.fragment),Dh=l(),et=s("p"),Ih=t("If "),Tl=s("code"),Oh=t("token_ids_1"),Sh=t(" is "),Fl=s("code"),Nh=t("None"),Bh=t(", this method only returns the first portion of the mask (0s)."),Wh=l(),vl=s("div"),qc=l(),nt=s("h2"),Rt=s("a"),kl=s("span"),k(Ko.$$.fragment),Qh=l(),wl=s("span"),Rh=t("FunnelTokenizerFast"),Pc=l(),Ze=s("div"),k(Xo.$$.fragment),Hh=l(),Jo=s("p"),Vh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),bl=s("em"),Yh=t("tokenizers"),Uh=t(" library)."),Gh=l(),Ht=s("p"),ti=s("a"),Zh=t("FunnelTokenizerFast"),Kh=t(" is identical to "),oi=s("a"),Xh=t("BertTokenizerFast"),Jh=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ef=l(),es=s("p"),nf=t("Refer to superclass "),si=s("a"),tf=t("BertTokenizerFast"),of=t(" for usage examples and documentation concerning parameters."),sf=l(),bn=s("div"),k(ns.$$.fragment),rf=l(),yl=s("p"),af=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),lf=l(),k(ts.$$.fragment),df=l(),tt=s("p"),cf=t("If "),$l=s("code"),uf=t("token_ids_1"),pf=t(" is "),El=s("code"),hf=t("None"),ff=t(", this method only returns the first portion of the mask (0s)."),Cc=l(),ot=s("h2"),Vt=s("a"),Ml=s("span"),k(os.$$.fragment),mf=l(),zl=s("span"),gf=t("Funnel specific outputs"),xc=l(),st=s("div"),k(ss.$$.fragment),_f=l(),rs=s("p"),Tf=t("Output type of "),ri=s("a"),Ff=t("FunnelForPreTraining"),vf=t("."),jc=l(),rt=s("div"),k(as.$$.fragment),kf=l(),is=s("p"),wf=t("Output type of "),ai=s("a"),bf=t("FunnelForPreTraining"),yf=t("."),Lc=l(),at=s("h2"),Yt=s("a"),ql=s("span"),k(ls.$$.fragment),$f=l(),Pl=s("span"),Ef=t("FunnelBaseModel"),Ac=l(),We=s("div"),k(ds.$$.fragment),Mf=l(),Cl=s("p"),zf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qf=l(),cs=s("p"),Pf=t("The Funnel Transformer model was proposed in "),us=s("a"),Cf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xf=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jf=l(),ps=s("p"),Lf=t("This model inherits from "),ii=s("a"),Af=t("PreTrainedModel"),Df=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),If=l(),hs=s("p"),Of=t("This model is also a PyTorch "),fs=s("a"),Sf=t("torch.nn.Module"),Nf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bf=l(),Ke=s("div"),k(ms.$$.fragment),Wf=l(),it=s("p"),Qf=t("The "),li=s("a"),Rf=t("FunnelBaseModel"),Hf=t(" forward method, overrides the "),xl=s("code"),Vf=t("__call__"),Yf=t(" special method."),Uf=l(),k(Ut.$$.fragment),Gf=l(),jl=s("p"),Zf=t("Example:"),Kf=l(),k(gs.$$.fragment),Dc=l(),lt=s("h2"),Gt=s("a"),Ll=s("span"),k(_s.$$.fragment),Xf=l(),Al=s("span"),Jf=t("FunnelModel"),Ic=l(),Qe=s("div"),k(Ts.$$.fragment),em=l(),Dl=s("p"),nm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),tm=l(),Fs=s("p"),om=t("The Funnel Transformer model was proposed in "),vs=s("a"),sm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),rm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),am=l(),ks=s("p"),im=t("This model inherits from "),di=s("a"),lm=t("PreTrainedModel"),dm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cm=l(),ws=s("p"),um=t("This model is also a PyTorch "),bs=s("a"),pm=t("torch.nn.Module"),hm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),fm=l(),Xe=s("div"),k(ys.$$.fragment),mm=l(),dt=s("p"),gm=t("The "),ci=s("a"),_m=t("FunnelModel"),Tm=t(" forward method, overrides the "),Il=s("code"),Fm=t("__call__"),vm=t(" special method."),km=l(),k(Zt.$$.fragment),wm=l(),Ol=s("p"),bm=t("Example:"),ym=l(),k($s.$$.fragment),Oc=l(),ct=s("h2"),Kt=s("a"),Sl=s("span"),k(Es.$$.fragment),$m=l(),Nl=s("span"),Em=t("FunnelModelForPreTraining"),Sc=l(),Ms=s("div"),Je=s("div"),k(zs.$$.fragment),Mm=l(),ut=s("p"),zm=t("The "),ui=s("a"),qm=t("FunnelForPreTraining"),Pm=t(" forward method, overrides the "),Bl=s("code"),Cm=t("__call__"),xm=t(" special method."),jm=l(),k(Xt.$$.fragment),Lm=l(),Wl=s("p"),Am=t("Examples:"),Dm=l(),k(qs.$$.fragment),Nc=l(),pt=s("h2"),Jt=s("a"),Ql=s("span"),k(Ps.$$.fragment),Im=l(),Rl=s("span"),Om=t("FunnelForMaskedLM"),Bc=l(),Re=s("div"),k(Cs.$$.fragment),Sm=l(),xs=s("p"),Nm=t("Funnel Transformer Model with a "),Hl=s("code"),Bm=t("language modeling"),Wm=t(" head on top."),Qm=l(),js=s("p"),Rm=t("The Funnel Transformer model was proposed in "),Ls=s("a"),Hm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Vm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ym=l(),As=s("p"),Um=t("This model inherits from "),pi=s("a"),Gm=t("PreTrainedModel"),Zm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Km=l(),Ds=s("p"),Xm=t("This model is also a PyTorch "),Is=s("a"),Jm=t("torch.nn.Module"),eg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ng=l(),en=s("div"),k(Os.$$.fragment),tg=l(),ht=s("p"),og=t("The "),hi=s("a"),sg=t("FunnelForMaskedLM"),rg=t(" forward method, overrides the "),Vl=s("code"),ag=t("__call__"),ig=t(" special method."),lg=l(),k(eo.$$.fragment),dg=l(),Yl=s("p"),cg=t("Example:"),ug=l(),k(Ss.$$.fragment),Wc=l(),ft=s("h2"),no=s("a"),Ul=s("span"),k(Ns.$$.fragment),pg=l(),Gl=s("span"),hg=t("FunnelForSequenceClassification"),Qc=l(),He=s("div"),k(Bs.$$.fragment),fg=l(),Zl=s("p"),mg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),gg=l(),Ws=s("p"),_g=t("The Funnel Transformer model was proposed in "),Qs=s("a"),Tg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),vg=l(),Rs=s("p"),kg=t("This model inherits from "),fi=s("a"),wg=t("PreTrainedModel"),bg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yg=l(),Hs=s("p"),$g=t("This model is also a PyTorch "),Vs=s("a"),Eg=t("torch.nn.Module"),Mg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),zg=l(),Be=s("div"),k(Ys.$$.fragment),qg=l(),mt=s("p"),Pg=t("The "),mi=s("a"),Cg=t("FunnelForSequenceClassification"),xg=t(" forward method, overrides the "),Kl=s("code"),jg=t("__call__"),Lg=t(" special method."),Ag=l(),k(to.$$.fragment),Dg=l(),Xl=s("p"),Ig=t("Example of single-label classification:"),Og=l(),k(Us.$$.fragment),Sg=l(),Jl=s("p"),Ng=t("Example of multi-label classification:"),Bg=l(),k(Gs.$$.fragment),Rc=l(),gt=s("h2"),oo=s("a"),ed=s("span"),k(Zs.$$.fragment),Wg=l(),nd=s("span"),Qg=t("FunnelForMultipleChoice"),Hc=l(),Ve=s("div"),k(Ks.$$.fragment),Rg=l(),td=s("p"),Hg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Vg=l(),Xs=s("p"),Yg=t("The Funnel Transformer model was proposed in "),Js=s("a"),Ug=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zg=l(),er=s("p"),Kg=t("This model inherits from "),gi=s("a"),Xg=t("PreTrainedModel"),Jg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),e_=l(),nr=s("p"),n_=t("This model is also a PyTorch "),tr=s("a"),t_=t("torch.nn.Module"),o_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),s_=l(),nn=s("div"),k(or.$$.fragment),r_=l(),_t=s("p"),a_=t("The "),_i=s("a"),i_=t("FunnelForMultipleChoice"),l_=t(" forward method, overrides the "),od=s("code"),d_=t("__call__"),c_=t(" special method."),u_=l(),k(so.$$.fragment),p_=l(),sd=s("p"),h_=t("Example:"),f_=l(),k(sr.$$.fragment),Vc=l(),Tt=s("h2"),ro=s("a"),rd=s("span"),k(rr.$$.fragment),m_=l(),ad=s("span"),g_=t("FunnelForTokenClassification"),Yc=l(),Ye=s("div"),k(ar.$$.fragment),__=l(),id=s("p"),T_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),F_=l(),ir=s("p"),v_=t("The Funnel Transformer model was proposed in "),lr=s("a"),k_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),w_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),b_=l(),dr=s("p"),y_=t("This model inherits from "),Ti=s("a"),$_=t("PreTrainedModel"),E_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),M_=l(),cr=s("p"),z_=t("This model is also a PyTorch "),ur=s("a"),q_=t("torch.nn.Module"),P_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),C_=l(),tn=s("div"),k(pr.$$.fragment),x_=l(),Ft=s("p"),j_=t("The "),Fi=s("a"),L_=t("FunnelForTokenClassification"),A_=t(" forward method, overrides the "),ld=s("code"),D_=t("__call__"),I_=t(" special method."),O_=l(),k(ao.$$.fragment),S_=l(),dd=s("p"),N_=t("Example:"),B_=l(),k(hr.$$.fragment),Uc=l(),vt=s("h2"),io=s("a"),cd=s("span"),k(fr.$$.fragment),W_=l(),ud=s("span"),Q_=t("FunnelForQuestionAnswering"),Gc=l(),Ue=s("div"),k(mr.$$.fragment),R_=l(),kt=s("p"),H_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),pd=s("code"),V_=t("span start logits"),Y_=t(" and "),hd=s("code"),U_=t("span end logits"),G_=t(")."),Z_=l(),gr=s("p"),K_=t("The Funnel Transformer model was proposed in "),_r=s("a"),X_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),J_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),e1=l(),Tr=s("p"),n1=t("This model inherits from "),vi=s("a"),t1=t("PreTrainedModel"),o1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s1=l(),Fr=s("p"),r1=t("This model is also a PyTorch "),vr=s("a"),a1=t("torch.nn.Module"),i1=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),l1=l(),on=s("div"),k(kr.$$.fragment),d1=l(),wt=s("p"),c1=t("The "),ki=s("a"),u1=t("FunnelForQuestionAnswering"),p1=t(" forward method, overrides the "),fd=s("code"),h1=t("__call__"),f1=t(" special method."),m1=l(),k(lo.$$.fragment),g1=l(),md=s("p"),_1=t("Example:"),T1=l(),k(wr.$$.fragment),Zc=l(),bt=s("h2"),co=s("a"),gd=s("span"),k(br.$$.fragment),F1=l(),_d=s("span"),v1=t("TFFunnelBaseModel"),Kc=l(),xe=s("div"),k(yr.$$.fragment),k1=l(),Td=s("p"),w1=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),b1=l(),$r=s("p"),y1=t("The Funnel Transformer model was proposed in "),Er=s("a"),$1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),E1=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),M1=l(),Mr=s("p"),z1=t("This model inherits from "),wi=s("a"),q1=t("TFPreTrainedModel"),P1=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C1=l(),zr=s("p"),x1=t("This model is also a "),qr=s("a"),j1=t("tf.keras.Model"),L1=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),A1=l(),k(uo.$$.fragment),D1=l(),sn=s("div"),k(Pr.$$.fragment),I1=l(),yt=s("p"),O1=t("The "),bi=s("a"),S1=t("TFFunnelBaseModel"),N1=t(" forward method, overrides the "),Fd=s("code"),B1=t("__call__"),W1=t(" special method."),Q1=l(),k(po.$$.fragment),R1=l(),vd=s("p"),H1=t("Example:"),V1=l(),k(Cr.$$.fragment),Xc=l(),$t=s("h2"),ho=s("a"),kd=s("span"),k(xr.$$.fragment),Y1=l(),wd=s("span"),U1=t("TFFunnelModel"),Jc=l(),je=s("div"),k(jr.$$.fragment),G1=l(),bd=s("p"),Z1=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),K1=l(),Lr=s("p"),X1=t("The Funnel Transformer model was proposed in "),Ar=s("a"),J1=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),eT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),nT=l(),Dr=s("p"),tT=t("This model inherits from "),yi=s("a"),oT=t("TFPreTrainedModel"),sT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rT=l(),Ir=s("p"),aT=t("This model is also a "),Or=s("a"),iT=t("tf.keras.Model"),lT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),dT=l(),k(fo.$$.fragment),cT=l(),rn=s("div"),k(Sr.$$.fragment),uT=l(),Et=s("p"),pT=t("The "),$i=s("a"),hT=t("TFFunnelModel"),fT=t(" forward method, overrides the "),yd=s("code"),mT=t("__call__"),gT=t(" special method."),_T=l(),k(mo.$$.fragment),TT=l(),$d=s("p"),FT=t("Example:"),vT=l(),k(Nr.$$.fragment),eu=l(),Mt=s("h2"),go=s("a"),Ed=s("span"),k(Br.$$.fragment),kT=l(),Md=s("span"),wT=t("TFFunnelModelForPreTraining"),nu=l(),Le=s("div"),k(Wr.$$.fragment),bT=l(),zd=s("p"),yT=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),$T=l(),Qr=s("p"),ET=t("The Funnel Transformer model was proposed in "),Rr=s("a"),MT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),zT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),qT=l(),Hr=s("p"),PT=t("This model inherits from "),Ei=s("a"),CT=t("TFPreTrainedModel"),xT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jT=l(),Vr=s("p"),LT=t("This model is also a "),Yr=s("a"),AT=t("tf.keras.Model"),DT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),IT=l(),k(_o.$$.fragment),OT=l(),an=s("div"),k(Ur.$$.fragment),ST=l(),zt=s("p"),NT=t("The "),Mi=s("a"),BT=t("TFFunnelForPreTraining"),WT=t(" forward method, overrides the "),qd=s("code"),QT=t("__call__"),RT=t(" special method."),HT=l(),k(To.$$.fragment),VT=l(),Pd=s("p"),YT=t("Examples:"),UT=l(),k(Gr.$$.fragment),tu=l(),qt=s("h2"),Fo=s("a"),Cd=s("span"),k(Zr.$$.fragment),GT=l(),xd=s("span"),ZT=t("TFFunnelForMaskedLM"),ou=l(),Ae=s("div"),k(Kr.$$.fragment),KT=l(),Xr=s("p"),XT=t("Funnel Model with a "),jd=s("code"),JT=t("language modeling"),eF=t(" head on top."),nF=l(),Jr=s("p"),tF=t("The Funnel Transformer model was proposed in "),ea=s("a"),oF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rF=l(),na=s("p"),aF=t("This model inherits from "),zi=s("a"),iF=t("TFPreTrainedModel"),lF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dF=l(),ta=s("p"),cF=t("This model is also a "),oa=s("a"),uF=t("tf.keras.Model"),pF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hF=l(),k(vo.$$.fragment),fF=l(),ln=s("div"),k(sa.$$.fragment),mF=l(),Pt=s("p"),gF=t("The "),qi=s("a"),_F=t("TFFunnelForMaskedLM"),TF=t(" forward method, overrides the "),Ld=s("code"),FF=t("__call__"),vF=t(" special method."),kF=l(),k(ko.$$.fragment),wF=l(),Ad=s("p"),bF=t("Example:"),yF=l(),k(ra.$$.fragment),su=l(),Ct=s("h2"),wo=s("a"),Dd=s("span"),k(aa.$$.fragment),$F=l(),Id=s("span"),EF=t("TFFunnelForSequenceClassification"),ru=l(),De=s("div"),k(ia.$$.fragment),MF=l(),Od=s("p"),zF=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),qF=l(),la=s("p"),PF=t("The Funnel Transformer model was proposed in "),da=s("a"),CF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jF=l(),ca=s("p"),LF=t("This model inherits from "),Pi=s("a"),AF=t("TFPreTrainedModel"),DF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),IF=l(),ua=s("p"),OF=t("This model is also a "),pa=s("a"),SF=t("tf.keras.Model"),NF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),BF=l(),k(bo.$$.fragment),WF=l(),dn=s("div"),k(ha.$$.fragment),QF=l(),xt=s("p"),RF=t("The "),Ci=s("a"),HF=t("TFFunnelForSequenceClassification"),VF=t(" forward method, overrides the "),Sd=s("code"),YF=t("__call__"),UF=t(" special method."),GF=l(),k(yo.$$.fragment),ZF=l(),Nd=s("p"),KF=t("Example:"),XF=l(),k(fa.$$.fragment),au=l(),jt=s("h2"),$o=s("a"),Bd=s("span"),k(ma.$$.fragment),JF=l(),Wd=s("span"),ev=t("TFFunnelForMultipleChoice"),iu=l(),Ie=s("div"),k(ga.$$.fragment),nv=l(),Qd=s("p"),tv=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ov=l(),_a=s("p"),sv=t("The Funnel Transformer model was proposed in "),Ta=s("a"),rv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),av=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),iv=l(),Fa=s("p"),lv=t("This model inherits from "),xi=s("a"),dv=t("TFPreTrainedModel"),cv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),uv=l(),va=s("p"),pv=t("This model is also a "),ka=s("a"),hv=t("tf.keras.Model"),fv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),mv=l(),k(Eo.$$.fragment),gv=l(),cn=s("div"),k(wa.$$.fragment),_v=l(),Lt=s("p"),Tv=t("The "),ji=s("a"),Fv=t("TFFunnelForMultipleChoice"),vv=t(" forward method, overrides the "),Rd=s("code"),kv=t("__call__"),wv=t(" special method."),bv=l(),k(Mo.$$.fragment),yv=l(),Hd=s("p"),$v=t("Example:"),Ev=l(),k(ba.$$.fragment),lu=l(),At=s("h2"),zo=s("a"),Vd=s("span"),k(ya.$$.fragment),Mv=l(),Yd=s("span"),zv=t("TFFunnelForTokenClassification"),du=l(),Oe=s("div"),k($a.$$.fragment),qv=l(),Ud=s("p"),Pv=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Cv=l(),Ea=s("p"),xv=t("The Funnel Transformer model was proposed in "),Ma=s("a"),jv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Lv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Av=l(),za=s("p"),Dv=t("This model inherits from "),Li=s("a"),Iv=t("TFPreTrainedModel"),Ov=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sv=l(),qa=s("p"),Nv=t("This model is also a "),Pa=s("a"),Bv=t("tf.keras.Model"),Wv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Qv=l(),k(qo.$$.fragment),Rv=l(),un=s("div"),k(Ca.$$.fragment),Hv=l(),Dt=s("p"),Vv=t("The "),Ai=s("a"),Yv=t("TFFunnelForTokenClassification"),Uv=t(" forward method, overrides the "),Gd=s("code"),Gv=t("__call__"),Zv=t(" special method."),Kv=l(),k(Po.$$.fragment),Xv=l(),Zd=s("p"),Jv=t("Example:"),ek=l(),k(xa.$$.fragment),cu=l(),It=s("h2"),Co=s("a"),Kd=s("span"),k(ja.$$.fragment),nk=l(),Xd=s("span"),tk=t("TFFunnelForQuestionAnswering"),uu=l(),Se=s("div"),k(La.$$.fragment),ok=l(),Ot=s("p"),sk=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Jd=s("code"),rk=t("span start logits"),ak=t(" and "),ec=s("code"),ik=t("span end logits"),lk=t(")."),dk=l(),Aa=s("p"),ck=t("The Funnel Transformer model was proposed in "),Da=s("a"),uk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),hk=l(),Ia=s("p"),fk=t("This model inherits from "),Di=s("a"),mk=t("TFPreTrainedModel"),gk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_k=l(),Oa=s("p"),Tk=t("This model is also a "),Sa=s("a"),Fk=t("tf.keras.Model"),vk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),kk=l(),k(xo.$$.fragment),wk=l(),pn=s("div"),k(Na.$$.fragment),bk=l(),St=s("p"),yk=t("The "),Ii=s("a"),$k=t("TFFunnelForQuestionAnswering"),Ek=t(" forward method, overrides the "),nc=s("code"),Mk=t("__call__"),zk=t(" special method."),qk=l(),k(jo.$$.fragment),Pk=l(),tc=s("p"),Ck=t("Example:"),xk=l(),k(Ba.$$.fragment),this.h()},l(i){const f=w$('[data-svelte="svelte-1phssyn"]',document.head);p=r(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(i),m=r(i,"H1",{class:!0});var Wa=a(m);g=r(Wa,"A",{id:!0,class:!0,href:!0});var oc=a(g);F=r(oc,"SPAN",{});var sc=a(F);w(T.$$.fragment,sc),sc.forEach(n),oc.forEach(n),_=d(Wa),z=r(Wa,"SPAN",{});var rc=a(z);ce=o(rc,"Funnel Transformer"),rc.forEach(n),Wa.forEach(n),G=d(i),q=r(i,"H2",{class:!0});var Qa=a(q);X=r(Qa,"A",{id:!0,class:!0,href:!0});var ac=a(X);I=r(ac,"SPAN",{});var ic=a(I);w(ne.$$.fragment,ic),ic.forEach(n),ac.forEach(n),ue=d(Qa),O=r(Qa,"SPAN",{});var lc=a(O);pe=o(lc,"Overview"),lc.forEach(n),Qa.forEach(n),ie=d(i),U=r(i,"P",{});var Ra=a(U);L=o(Ra,"The Funnel Transformer model was proposed in the paper "),te=r(Ra,"A",{href:!0,rel:!0});var dc=a(te);Z=o(dc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),dc.forEach(n),P=o(Ra,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Ra.forEach(n),x=d(i),oe=r(i,"P",{});var cc=a(oe);Q=o(cc,"The abstract from the paper is the following:"),cc.forEach(n),le=d(i),se=r(i,"P",{});var uc=a(se);S=r(uc,"EM",{});var pc=a(S);he=o(pc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),pc.forEach(n),uc.forEach(n),de=d(i),C=r(i,"P",{});var hc=a(C);fe=o(hc,"Tips:"),hc.forEach(n),B=d(i),J=r(i,"UL",{});var Ha=a(J);ae=r(Ha,"LI",{});var fc=a(ae);R=o(fc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),fc.forEach(n),me=d(Ha),N=r(Ha,"LI",{});var Ce=a(N);A=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r(Ce,"A",{href:!0});var mc=a(re);H=o(mc,"FunnelModel"),mc.forEach(n),ge=o(Ce,", "),u=r(Ce,"A",{href:!0});var gc=a(u);v=o(gc,"FunnelForPreTraining"),gc.forEach(n),K=o(Ce,`,
`),Te=r(Ce,"A",{href:!0});var _c=a(Te);we=o(_c,"FunnelForMaskedLM"),_c.forEach(n),D=o(Ce,", "),Fe=r(Ce,"A",{href:!0});var Tc=a(Fe);be=o(Tc,"FunnelForTokenClassification"),Tc.forEach(n),ye=o(Ce,` and
class:`),j=r(Ce,"EM",{});var Fc=a(j);V=o(Fc,"~transformers.FunnelForQuestionAnswering"),Fc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),ve=r(Ce,"A",{href:!0});var vc=a(ve);Y=o(vc,"FunnelBaseModel"),vc.forEach(n),Ee=o(Ce,", "),ke=r(Ce,"A",{href:!0});var kc=a(ke);_e=o(kc,"FunnelForSequenceClassification"),kc.forEach(n),Me=o(Ce,` and
`),Va=r(Ce,"A",{href:!0});var Ak=a(Va);Dp=o(Ak,"FunnelForMultipleChoice"),Ak.forEach(n),Ip=o(Ce,"."),Ce.forEach(n),Ha.forEach(n),yc=d(i),jn=r(i,"P",{});var Oi=a(jn);Op=o(Oi,"This model was contributed by "),Io=r(Oi,"A",{href:!0,rel:!0});var Dk=a(Io);Sp=o(Dk,"sgugger"),Dk.forEach(n),Np=o(Oi,". The original code can be found "),Oo=r(Oi,"A",{href:!0,rel:!0});var Ik=a(Oo);Bp=o(Ik,"here"),Ik.forEach(n),Wp=o(Oi,"."),Oi.forEach(n),$c=d(i),Kn=r(i,"H2",{class:!0});var hu=a(Kn);Nt=r(hu,"A",{id:!0,class:!0,href:!0});var Ok=a(Nt);ll=r(Ok,"SPAN",{});var Sk=a(ll);w(So.$$.fragment,Sk),Sk.forEach(n),Ok.forEach(n),Qp=d(hu),dl=r(hu,"SPAN",{});var Nk=a(dl);Rp=o(Nk,"FunnelConfig"),Nk.forEach(n),hu.forEach(n),Ec=d(i),Cn=r(i,"DIV",{class:!0});var Si=a(Cn);w(No.$$.fragment,Si),Hp=d(Si),xn=r(Si,"P",{});var Lo=a(xn);Vp=o(Lo,"This is the configuration class to store the configuration of a "),Ya=r(Lo,"A",{href:!0});var Bk=a(Ya);Yp=o(Bk,"FunnelModel"),Bk.forEach(n),Up=o(Lo," or a "),Ua=r(Lo,"A",{href:!0});var Wk=a(Ua);Gp=o(Wk,"TFBertModel"),Wk.forEach(n),Zp=o(Lo,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Bo=r(Lo,"A",{href:!0,rel:!0});var Qk=a(Bo);Kp=o(Qk,"funnel-transformer/small"),Qk.forEach(n),Xp=o(Lo," architecture."),Lo.forEach(n),Jp=d(Si),Xn=r(Si,"P",{});var Ni=a(Xn);eh=o(Ni,"Configuration objects inherit from "),Ga=r(Ni,"A",{href:!0});var Rk=a(Ga);nh=o(Rk,"PretrainedConfig"),Rk.forEach(n),th=o(Ni,` and can be used to control the model outputs. Read the
documentation from `),Za=r(Ni,"A",{href:!0});var Hk=a(Za);oh=o(Hk,"PretrainedConfig"),Hk.forEach(n),sh=o(Ni," for more information."),Ni.forEach(n),Si.forEach(n),Mc=d(i),Jn=r(i,"H2",{class:!0});var fu=a(Jn);Bt=r(fu,"A",{id:!0,class:!0,href:!0});var Vk=a(Bt);cl=r(Vk,"SPAN",{});var Yk=a(cl);w(Wo.$$.fragment,Yk),Yk.forEach(n),Vk.forEach(n),rh=d(fu),ul=r(fu,"SPAN",{});var Uk=a(ul);ah=o(Uk,"FunnelTokenizer"),Uk.forEach(n),fu.forEach(n),zc=d(i),Pe=r(i,"DIV",{class:!0});var Ge=a(Pe);w(Qo.$$.fragment,Ge),ih=d(Ge),pl=r(Ge,"P",{});var Gk=a(pl);lh=o(Gk,"Construct a Funnel Transformer tokenizer."),Gk.forEach(n),dh=d(Ge),Wt=r(Ge,"P",{});var wc=a(Wt);Ka=r(wc,"A",{href:!0});var Zk=a(Ka);ch=o(Zk,"FunnelTokenizer"),Zk.forEach(n),uh=o(wc," is identical to "),Xa=r(wc,"A",{href:!0});var Kk=a(Xa);ph=o(Kk,"BertTokenizer"),Kk.forEach(n),hh=o(wc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),wc.forEach(n),fh=d(Ge),Ro=r(Ge,"P",{});var mu=a(Ro);mh=o(mu,"Refer to superclass "),Ja=r(mu,"A",{href:!0});var Xk=a(Ja);gh=o(Xk,"BertTokenizer"),Xk.forEach(n),_h=o(mu," for usage examples and documentation concerning parameters."),mu.forEach(n),Th=d(Ge),Ln=r(Ge,"DIV",{class:!0});var Bi=a(Ln);w(Ho.$$.fragment,Bi),Fh=d(Bi),hl=r(Bi,"P",{});var Jk=a(hl);vh=o(Jk,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Jk.forEach(n),kh=d(Bi),Vo=r(Bi,"UL",{});var gu=a(Vo);ei=r(gu,"LI",{});var jk=a(ei);wh=o(jk,"single sequence: "),fl=r(jk,"CODE",{});var ew=a(fl);bh=o(ew,"[CLS] X [SEP]"),ew.forEach(n),jk.forEach(n),yh=d(gu),ni=r(gu,"LI",{});var Lk=a(ni);$h=o(Lk,"pair of sequences: "),ml=r(Lk,"CODE",{});var nw=a(ml);Eh=o(nw,"[CLS] A [SEP] B [SEP]"),nw.forEach(n),Lk.forEach(n),gu.forEach(n),Bi.forEach(n),Mh=d(Ge),Qt=r(Ge,"DIV",{class:!0});var _u=a(Qt);w(Yo.$$.fragment,_u),zh=d(_u),Uo=r(_u,"P",{});var Tu=a(Uo);qh=o(Tu,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),gl=r(Tu,"CODE",{});var tw=a(gl);Ph=o(tw,"prepare_for_model"),tw.forEach(n),Ch=o(Tu," method."),Tu.forEach(n),_u.forEach(n),xh=d(Ge),wn=r(Ge,"DIV",{class:!0});var Ao=a(wn);w(Go.$$.fragment,Ao),jh=d(Ao),_l=r(Ao,"P",{});var ow=a(_l);Lh=o(ow,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),ow.forEach(n),Ah=d(Ao),w(Zo.$$.fragment,Ao),Dh=d(Ao),et=r(Ao,"P",{});var Wi=a(et);Ih=o(Wi,"If "),Tl=r(Wi,"CODE",{});var sw=a(Tl);Oh=o(sw,"token_ids_1"),sw.forEach(n),Sh=o(Wi," is "),Fl=r(Wi,"CODE",{});var rw=a(Fl);Nh=o(rw,"None"),rw.forEach(n),Bh=o(Wi,", this method only returns the first portion of the mask (0s)."),Wi.forEach(n),Ao.forEach(n),Wh=d(Ge),vl=r(Ge,"DIV",{class:!0}),a(vl).forEach(n),Ge.forEach(n),qc=d(i),nt=r(i,"H2",{class:!0});var Fu=a(nt);Rt=r(Fu,"A",{id:!0,class:!0,href:!0});var aw=a(Rt);kl=r(aw,"SPAN",{});var iw=a(kl);w(Ko.$$.fragment,iw),iw.forEach(n),aw.forEach(n),Qh=d(Fu),wl=r(Fu,"SPAN",{});var lw=a(wl);Rh=o(lw,"FunnelTokenizerFast"),lw.forEach(n),Fu.forEach(n),Pc=d(i),Ze=r(i,"DIV",{class:!0});var An=a(Ze);w(Xo.$$.fragment,An),Hh=d(An),Jo=r(An,"P",{});var vu=a(Jo);Vh=o(vu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),bl=r(vu,"EM",{});var dw=a(bl);Yh=o(dw,"tokenizers"),dw.forEach(n),Uh=o(vu," library)."),vu.forEach(n),Gh=d(An),Ht=r(An,"P",{});var bc=a(Ht);ti=r(bc,"A",{href:!0});var cw=a(ti);Zh=o(cw,"FunnelTokenizerFast"),cw.forEach(n),Kh=o(bc," is identical to "),oi=r(bc,"A",{href:!0});var uw=a(oi);Xh=o(uw,"BertTokenizerFast"),uw.forEach(n),Jh=o(bc,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),bc.forEach(n),ef=d(An),es=r(An,"P",{});var ku=a(es);nf=o(ku,"Refer to superclass "),si=r(ku,"A",{href:!0});var pw=a(si);tf=o(pw,"BertTokenizerFast"),pw.forEach(n),of=o(ku," for usage examples and documentation concerning parameters."),ku.forEach(n),sf=d(An),bn=r(An,"DIV",{class:!0});var Do=a(bn);w(ns.$$.fragment,Do),rf=d(Do),yl=r(Do,"P",{});var hw=a(yl);af=o(hw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),hw.forEach(n),lf=d(Do),w(ts.$$.fragment,Do),df=d(Do),tt=r(Do,"P",{});var Qi=a(tt);cf=o(Qi,"If "),$l=r(Qi,"CODE",{});var fw=a($l);uf=o(fw,"token_ids_1"),fw.forEach(n),pf=o(Qi," is "),El=r(Qi,"CODE",{});var mw=a(El);hf=o(mw,"None"),mw.forEach(n),ff=o(Qi,", this method only returns the first portion of the mask (0s)."),Qi.forEach(n),Do.forEach(n),An.forEach(n),Cc=d(i),ot=r(i,"H2",{class:!0});var wu=a(ot);Vt=r(wu,"A",{id:!0,class:!0,href:!0});var gw=a(Vt);Ml=r(gw,"SPAN",{});var _w=a(Ml);w(os.$$.fragment,_w),_w.forEach(n),gw.forEach(n),mf=d(wu),zl=r(wu,"SPAN",{});var Tw=a(zl);gf=o(Tw,"Funnel specific outputs"),Tw.forEach(n),wu.forEach(n),xc=d(i),st=r(i,"DIV",{class:!0});var bu=a(st);w(ss.$$.fragment,bu),_f=d(bu),rs=r(bu,"P",{});var yu=a(rs);Tf=o(yu,"Output type of "),ri=r(yu,"A",{href:!0});var Fw=a(ri);Ff=o(Fw,"FunnelForPreTraining"),Fw.forEach(n),vf=o(yu,"."),yu.forEach(n),bu.forEach(n),jc=d(i),rt=r(i,"DIV",{class:!0});var $u=a(rt);w(as.$$.fragment,$u),kf=d($u),is=r($u,"P",{});var Eu=a(is);wf=o(Eu,"Output type of "),ai=r(Eu,"A",{href:!0});var vw=a(ai);bf=o(vw,"FunnelForPreTraining"),vw.forEach(n),yf=o(Eu,"."),Eu.forEach(n),$u.forEach(n),Lc=d(i),at=r(i,"H2",{class:!0});var Mu=a(at);Yt=r(Mu,"A",{id:!0,class:!0,href:!0});var kw=a(Yt);ql=r(kw,"SPAN",{});var ww=a(ql);w(ls.$$.fragment,ww),ww.forEach(n),kw.forEach(n),$f=d(Mu),Pl=r(Mu,"SPAN",{});var bw=a(Pl);Ef=o(bw,"FunnelBaseModel"),bw.forEach(n),Mu.forEach(n),Ac=d(i),We=r(i,"DIV",{class:!0});var yn=a(We);w(ds.$$.fragment,yn),Mf=d(yn),Cl=r(yn,"P",{});var yw=a(Cl);zf=o(yw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),yw.forEach(n),qf=d(yn),cs=r(yn,"P",{});var zu=a(cs);Pf=o(zu,"The Funnel Transformer model was proposed in "),us=r(zu,"A",{href:!0,rel:!0});var $w=a(us);Cf=o($w,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),$w.forEach(n),xf=o(zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zu.forEach(n),jf=d(yn),ps=r(yn,"P",{});var qu=a(ps);Lf=o(qu,"This model inherits from "),ii=r(qu,"A",{href:!0});var Ew=a(ii);Af=o(Ew,"PreTrainedModel"),Ew.forEach(n),Df=o(qu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qu.forEach(n),If=d(yn),hs=r(yn,"P",{});var Pu=a(hs);Of=o(Pu,"This model is also a PyTorch "),fs=r(Pu,"A",{href:!0,rel:!0});var Mw=a(fs);Sf=o(Mw,"torch.nn.Module"),Mw.forEach(n),Nf=o(Pu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Pu.forEach(n),Bf=d(yn),Ke=r(yn,"DIV",{class:!0});var Dn=a(Ke);w(ms.$$.fragment,Dn),Wf=d(Dn),it=r(Dn,"P",{});var Ri=a(it);Qf=o(Ri,"The "),li=r(Ri,"A",{href:!0});var zw=a(li);Rf=o(zw,"FunnelBaseModel"),zw.forEach(n),Hf=o(Ri," forward method, overrides the "),xl=r(Ri,"CODE",{});var qw=a(xl);Vf=o(qw,"__call__"),qw.forEach(n),Yf=o(Ri," special method."),Ri.forEach(n),Uf=d(Dn),w(Ut.$$.fragment,Dn),Gf=d(Dn),jl=r(Dn,"P",{});var Pw=a(jl);Zf=o(Pw,"Example:"),Pw.forEach(n),Kf=d(Dn),w(gs.$$.fragment,Dn),Dn.forEach(n),yn.forEach(n),Dc=d(i),lt=r(i,"H2",{class:!0});var Cu=a(lt);Gt=r(Cu,"A",{id:!0,class:!0,href:!0});var Cw=a(Gt);Ll=r(Cw,"SPAN",{});var xw=a(Ll);w(_s.$$.fragment,xw),xw.forEach(n),Cw.forEach(n),Xf=d(Cu),Al=r(Cu,"SPAN",{});var jw=a(Al);Jf=o(jw,"FunnelModel"),jw.forEach(n),Cu.forEach(n),Ic=d(i),Qe=r(i,"DIV",{class:!0});var $n=a(Qe);w(Ts.$$.fragment,$n),em=d($n),Dl=r($n,"P",{});var Lw=a(Dl);nm=o(Lw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Lw.forEach(n),tm=d($n),Fs=r($n,"P",{});var xu=a(Fs);om=o(xu,"The Funnel Transformer model was proposed in "),vs=r(xu,"A",{href:!0,rel:!0});var Aw=a(vs);sm=o(Aw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Aw.forEach(n),rm=o(xu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xu.forEach(n),am=d($n),ks=r($n,"P",{});var ju=a(ks);im=o(ju,"This model inherits from "),di=r(ju,"A",{href:!0});var Dw=a(di);lm=o(Dw,"PreTrainedModel"),Dw.forEach(n),dm=o(ju,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ju.forEach(n),cm=d($n),ws=r($n,"P",{});var Lu=a(ws);um=o(Lu,"This model is also a PyTorch "),bs=r(Lu,"A",{href:!0,rel:!0});var Iw=a(bs);pm=o(Iw,"torch.nn.Module"),Iw.forEach(n),hm=o(Lu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Lu.forEach(n),fm=d($n),Xe=r($n,"DIV",{class:!0});var In=a(Xe);w(ys.$$.fragment,In),mm=d(In),dt=r(In,"P",{});var Hi=a(dt);gm=o(Hi,"The "),ci=r(Hi,"A",{href:!0});var Ow=a(ci);_m=o(Ow,"FunnelModel"),Ow.forEach(n),Tm=o(Hi," forward method, overrides the "),Il=r(Hi,"CODE",{});var Sw=a(Il);Fm=o(Sw,"__call__"),Sw.forEach(n),vm=o(Hi," special method."),Hi.forEach(n),km=d(In),w(Zt.$$.fragment,In),wm=d(In),Ol=r(In,"P",{});var Nw=a(Ol);bm=o(Nw,"Example:"),Nw.forEach(n),ym=d(In),w($s.$$.fragment,In),In.forEach(n),$n.forEach(n),Oc=d(i),ct=r(i,"H2",{class:!0});var Au=a(ct);Kt=r(Au,"A",{id:!0,class:!0,href:!0});var Bw=a(Kt);Sl=r(Bw,"SPAN",{});var Ww=a(Sl);w(Es.$$.fragment,Ww),Ww.forEach(n),Bw.forEach(n),$m=d(Au),Nl=r(Au,"SPAN",{});var Qw=a(Nl);Em=o(Qw,"FunnelModelForPreTraining"),Qw.forEach(n),Au.forEach(n),Sc=d(i),Ms=r(i,"DIV",{class:!0});var Rw=a(Ms);Je=r(Rw,"DIV",{class:!0});var On=a(Je);w(zs.$$.fragment,On),Mm=d(On),ut=r(On,"P",{});var Vi=a(ut);zm=o(Vi,"The "),ui=r(Vi,"A",{href:!0});var Hw=a(ui);qm=o(Hw,"FunnelForPreTraining"),Hw.forEach(n),Pm=o(Vi," forward method, overrides the "),Bl=r(Vi,"CODE",{});var Vw=a(Bl);Cm=o(Vw,"__call__"),Vw.forEach(n),xm=o(Vi," special method."),Vi.forEach(n),jm=d(On),w(Xt.$$.fragment,On),Lm=d(On),Wl=r(On,"P",{});var Yw=a(Wl);Am=o(Yw,"Examples:"),Yw.forEach(n),Dm=d(On),w(qs.$$.fragment,On),On.forEach(n),Rw.forEach(n),Nc=d(i),pt=r(i,"H2",{class:!0});var Du=a(pt);Jt=r(Du,"A",{id:!0,class:!0,href:!0});var Uw=a(Jt);Ql=r(Uw,"SPAN",{});var Gw=a(Ql);w(Ps.$$.fragment,Gw),Gw.forEach(n),Uw.forEach(n),Im=d(Du),Rl=r(Du,"SPAN",{});var Zw=a(Rl);Om=o(Zw,"FunnelForMaskedLM"),Zw.forEach(n),Du.forEach(n),Bc=d(i),Re=r(i,"DIV",{class:!0});var En=a(Re);w(Cs.$$.fragment,En),Sm=d(En),xs=r(En,"P",{});var Iu=a(xs);Nm=o(Iu,"Funnel Transformer Model with a "),Hl=r(Iu,"CODE",{});var Kw=a(Hl);Bm=o(Kw,"language modeling"),Kw.forEach(n),Wm=o(Iu," head on top."),Iu.forEach(n),Qm=d(En),js=r(En,"P",{});var Ou=a(js);Rm=o(Ou,"The Funnel Transformer model was proposed in "),Ls=r(Ou,"A",{href:!0,rel:!0});var Xw=a(Ls);Hm=o(Xw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Xw.forEach(n),Vm=o(Ou," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ou.forEach(n),Ym=d(En),As=r(En,"P",{});var Su=a(As);Um=o(Su,"This model inherits from "),pi=r(Su,"A",{href:!0});var Jw=a(pi);Gm=o(Jw,"PreTrainedModel"),Jw.forEach(n),Zm=o(Su,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Su.forEach(n),Km=d(En),Ds=r(En,"P",{});var Nu=a(Ds);Xm=o(Nu,"This model is also a PyTorch "),Is=r(Nu,"A",{href:!0,rel:!0});var eb=a(Is);Jm=o(eb,"torch.nn.Module"),eb.forEach(n),eg=o(Nu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Nu.forEach(n),ng=d(En),en=r(En,"DIV",{class:!0});var Sn=a(en);w(Os.$$.fragment,Sn),tg=d(Sn),ht=r(Sn,"P",{});var Yi=a(ht);og=o(Yi,"The "),hi=r(Yi,"A",{href:!0});var nb=a(hi);sg=o(nb,"FunnelForMaskedLM"),nb.forEach(n),rg=o(Yi," forward method, overrides the "),Vl=r(Yi,"CODE",{});var tb=a(Vl);ag=o(tb,"__call__"),tb.forEach(n),ig=o(Yi," special method."),Yi.forEach(n),lg=d(Sn),w(eo.$$.fragment,Sn),dg=d(Sn),Yl=r(Sn,"P",{});var ob=a(Yl);cg=o(ob,"Example:"),ob.forEach(n),ug=d(Sn),w(Ss.$$.fragment,Sn),Sn.forEach(n),En.forEach(n),Wc=d(i),ft=r(i,"H2",{class:!0});var Bu=a(ft);no=r(Bu,"A",{id:!0,class:!0,href:!0});var sb=a(no);Ul=r(sb,"SPAN",{});var rb=a(Ul);w(Ns.$$.fragment,rb),rb.forEach(n),sb.forEach(n),pg=d(Bu),Gl=r(Bu,"SPAN",{});var ab=a(Gl);hg=o(ab,"FunnelForSequenceClassification"),ab.forEach(n),Bu.forEach(n),Qc=d(i),He=r(i,"DIV",{class:!0});var Mn=a(He);w(Bs.$$.fragment,Mn),fg=d(Mn),Zl=r(Mn,"P",{});var ib=a(Zl);mg=o(ib,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),ib.forEach(n),gg=d(Mn),Ws=r(Mn,"P",{});var Wu=a(Ws);_g=o(Wu,"The Funnel Transformer model was proposed in "),Qs=r(Wu,"A",{href:!0,rel:!0});var lb=a(Qs);Tg=o(lb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lb.forEach(n),Fg=o(Wu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Wu.forEach(n),vg=d(Mn),Rs=r(Mn,"P",{});var Qu=a(Rs);kg=o(Qu,"This model inherits from "),fi=r(Qu,"A",{href:!0});var db=a(fi);wg=o(db,"PreTrainedModel"),db.forEach(n),bg=o(Qu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qu.forEach(n),yg=d(Mn),Hs=r(Mn,"P",{});var Ru=a(Hs);$g=o(Ru,"This model is also a PyTorch "),Vs=r(Ru,"A",{href:!0,rel:!0});var cb=a(Vs);Eg=o(cb,"torch.nn.Module"),cb.forEach(n),Mg=o(Ru,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ru.forEach(n),zg=d(Mn),Be=r(Mn,"DIV",{class:!0});var hn=a(Be);w(Ys.$$.fragment,hn),qg=d(hn),mt=r(hn,"P",{});var Ui=a(mt);Pg=o(Ui,"The "),mi=r(Ui,"A",{href:!0});var ub=a(mi);Cg=o(ub,"FunnelForSequenceClassification"),ub.forEach(n),xg=o(Ui," forward method, overrides the "),Kl=r(Ui,"CODE",{});var pb=a(Kl);jg=o(pb,"__call__"),pb.forEach(n),Lg=o(Ui," special method."),Ui.forEach(n),Ag=d(hn),w(to.$$.fragment,hn),Dg=d(hn),Xl=r(hn,"P",{});var hb=a(Xl);Ig=o(hb,"Example of single-label classification:"),hb.forEach(n),Og=d(hn),w(Us.$$.fragment,hn),Sg=d(hn),Jl=r(hn,"P",{});var fb=a(Jl);Ng=o(fb,"Example of multi-label classification:"),fb.forEach(n),Bg=d(hn),w(Gs.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Rc=d(i),gt=r(i,"H2",{class:!0});var Hu=a(gt);oo=r(Hu,"A",{id:!0,class:!0,href:!0});var mb=a(oo);ed=r(mb,"SPAN",{});var gb=a(ed);w(Zs.$$.fragment,gb),gb.forEach(n),mb.forEach(n),Wg=d(Hu),nd=r(Hu,"SPAN",{});var _b=a(nd);Qg=o(_b,"FunnelForMultipleChoice"),_b.forEach(n),Hu.forEach(n),Hc=d(i),Ve=r(i,"DIV",{class:!0});var zn=a(Ve);w(Ks.$$.fragment,zn),Rg=d(zn),td=r(zn,"P",{});var Tb=a(td);Hg=o(Tb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Tb.forEach(n),Vg=d(zn),Xs=r(zn,"P",{});var Vu=a(Xs);Yg=o(Vu,"The Funnel Transformer model was proposed in "),Js=r(Vu,"A",{href:!0,rel:!0});var Fb=a(Js);Ug=o(Fb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fb.forEach(n),Gg=o(Vu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Vu.forEach(n),Zg=d(zn),er=r(zn,"P",{});var Yu=a(er);Kg=o(Yu,"This model inherits from "),gi=r(Yu,"A",{href:!0});var vb=a(gi);Xg=o(vb,"PreTrainedModel"),vb.forEach(n),Jg=o(Yu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yu.forEach(n),e_=d(zn),nr=r(zn,"P",{});var Uu=a(nr);n_=o(Uu,"This model is also a PyTorch "),tr=r(Uu,"A",{href:!0,rel:!0});var kb=a(tr);t_=o(kb,"torch.nn.Module"),kb.forEach(n),o_=o(Uu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Uu.forEach(n),s_=d(zn),nn=r(zn,"DIV",{class:!0});var Nn=a(nn);w(or.$$.fragment,Nn),r_=d(Nn),_t=r(Nn,"P",{});var Gi=a(_t);a_=o(Gi,"The "),_i=r(Gi,"A",{href:!0});var wb=a(_i);i_=o(wb,"FunnelForMultipleChoice"),wb.forEach(n),l_=o(Gi," forward method, overrides the "),od=r(Gi,"CODE",{});var bb=a(od);d_=o(bb,"__call__"),bb.forEach(n),c_=o(Gi," special method."),Gi.forEach(n),u_=d(Nn),w(so.$$.fragment,Nn),p_=d(Nn),sd=r(Nn,"P",{});var yb=a(sd);h_=o(yb,"Example:"),yb.forEach(n),f_=d(Nn),w(sr.$$.fragment,Nn),Nn.forEach(n),zn.forEach(n),Vc=d(i),Tt=r(i,"H2",{class:!0});var Gu=a(Tt);ro=r(Gu,"A",{id:!0,class:!0,href:!0});var $b=a(ro);rd=r($b,"SPAN",{});var Eb=a(rd);w(rr.$$.fragment,Eb),Eb.forEach(n),$b.forEach(n),m_=d(Gu),ad=r(Gu,"SPAN",{});var Mb=a(ad);g_=o(Mb,"FunnelForTokenClassification"),Mb.forEach(n),Gu.forEach(n),Yc=d(i),Ye=r(i,"DIV",{class:!0});var qn=a(Ye);w(ar.$$.fragment,qn),__=d(qn),id=r(qn,"P",{});var zb=a(id);T_=o(zb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),zb.forEach(n),F_=d(qn),ir=r(qn,"P",{});var Zu=a(ir);v_=o(Zu,"The Funnel Transformer model was proposed in "),lr=r(Zu,"A",{href:!0,rel:!0});var qb=a(lr);k_=o(qb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),qb.forEach(n),w_=o(Zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zu.forEach(n),b_=d(qn),dr=r(qn,"P",{});var Ku=a(dr);y_=o(Ku,"This model inherits from "),Ti=r(Ku,"A",{href:!0});var Pb=a(Ti);$_=o(Pb,"PreTrainedModel"),Pb.forEach(n),E_=o(Ku,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ku.forEach(n),M_=d(qn),cr=r(qn,"P",{});var Xu=a(cr);z_=o(Xu,"This model is also a PyTorch "),ur=r(Xu,"A",{href:!0,rel:!0});var Cb=a(ur);q_=o(Cb,"torch.nn.Module"),Cb.forEach(n),P_=o(Xu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xu.forEach(n),C_=d(qn),tn=r(qn,"DIV",{class:!0});var Bn=a(tn);w(pr.$$.fragment,Bn),x_=d(Bn),Ft=r(Bn,"P",{});var Zi=a(Ft);j_=o(Zi,"The "),Fi=r(Zi,"A",{href:!0});var xb=a(Fi);L_=o(xb,"FunnelForTokenClassification"),xb.forEach(n),A_=o(Zi," forward method, overrides the "),ld=r(Zi,"CODE",{});var jb=a(ld);D_=o(jb,"__call__"),jb.forEach(n),I_=o(Zi," special method."),Zi.forEach(n),O_=d(Bn),w(ao.$$.fragment,Bn),S_=d(Bn),dd=r(Bn,"P",{});var Lb=a(dd);N_=o(Lb,"Example:"),Lb.forEach(n),B_=d(Bn),w(hr.$$.fragment,Bn),Bn.forEach(n),qn.forEach(n),Uc=d(i),vt=r(i,"H2",{class:!0});var Ju=a(vt);io=r(Ju,"A",{id:!0,class:!0,href:!0});var Ab=a(io);cd=r(Ab,"SPAN",{});var Db=a(cd);w(fr.$$.fragment,Db),Db.forEach(n),Ab.forEach(n),W_=d(Ju),ud=r(Ju,"SPAN",{});var Ib=a(ud);Q_=o(Ib,"FunnelForQuestionAnswering"),Ib.forEach(n),Ju.forEach(n),Gc=d(i),Ue=r(i,"DIV",{class:!0});var Pn=a(Ue);w(mr.$$.fragment,Pn),R_=d(Pn),kt=r(Pn,"P",{});var Ki=a(kt);H_=o(Ki,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),pd=r(Ki,"CODE",{});var Ob=a(pd);V_=o(Ob,"span start logits"),Ob.forEach(n),Y_=o(Ki," and "),hd=r(Ki,"CODE",{});var Sb=a(hd);U_=o(Sb,"span end logits"),Sb.forEach(n),G_=o(Ki,")."),Ki.forEach(n),Z_=d(Pn),gr=r(Pn,"P",{});var ep=a(gr);K_=o(ep,"The Funnel Transformer model was proposed in "),_r=r(ep,"A",{href:!0,rel:!0});var Nb=a(_r);X_=o(Nb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Nb.forEach(n),J_=o(ep," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ep.forEach(n),e1=d(Pn),Tr=r(Pn,"P",{});var np=a(Tr);n1=o(np,"This model inherits from "),vi=r(np,"A",{href:!0});var Bb=a(vi);t1=o(Bb,"PreTrainedModel"),Bb.forEach(n),o1=o(np,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),np.forEach(n),s1=d(Pn),Fr=r(Pn,"P",{});var tp=a(Fr);r1=o(tp,"This model is also a PyTorch "),vr=r(tp,"A",{href:!0,rel:!0});var Wb=a(vr);a1=o(Wb,"torch.nn.Module"),Wb.forEach(n),i1=o(tp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),tp.forEach(n),l1=d(Pn),on=r(Pn,"DIV",{class:!0});var Wn=a(on);w(kr.$$.fragment,Wn),d1=d(Wn),wt=r(Wn,"P",{});var Xi=a(wt);c1=o(Xi,"The "),ki=r(Xi,"A",{href:!0});var Qb=a(ki);u1=o(Qb,"FunnelForQuestionAnswering"),Qb.forEach(n),p1=o(Xi," forward method, overrides the "),fd=r(Xi,"CODE",{});var Rb=a(fd);h1=o(Rb,"__call__"),Rb.forEach(n),f1=o(Xi," special method."),Xi.forEach(n),m1=d(Wn),w(lo.$$.fragment,Wn),g1=d(Wn),md=r(Wn,"P",{});var Hb=a(md);_1=o(Hb,"Example:"),Hb.forEach(n),T1=d(Wn),w(wr.$$.fragment,Wn),Wn.forEach(n),Pn.forEach(n),Zc=d(i),bt=r(i,"H2",{class:!0});var op=a(bt);co=r(op,"A",{id:!0,class:!0,href:!0});var Vb=a(co);gd=r(Vb,"SPAN",{});var Yb=a(gd);w(br.$$.fragment,Yb),Yb.forEach(n),Vb.forEach(n),F1=d(op),_d=r(op,"SPAN",{});var Ub=a(_d);v1=o(Ub,"TFFunnelBaseModel"),Ub.forEach(n),op.forEach(n),Kc=d(i),xe=r(i,"DIV",{class:!0});var fn=a(xe);w(yr.$$.fragment,fn),k1=d(fn),Td=r(fn,"P",{});var Gb=a(Td);w1=o(Gb,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Gb.forEach(n),b1=d(fn),$r=r(fn,"P",{});var sp=a($r);y1=o(sp,"The Funnel Transformer model was proposed in "),Er=r(sp,"A",{href:!0,rel:!0});var Zb=a(Er);$1=o(Zb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Zb.forEach(n),E1=o(sp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sp.forEach(n),M1=d(fn),Mr=r(fn,"P",{});var rp=a(Mr);z1=o(rp,"This model inherits from "),wi=r(rp,"A",{href:!0});var Kb=a(wi);q1=o(Kb,"TFPreTrainedModel"),Kb.forEach(n),P1=o(rp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rp.forEach(n),C1=d(fn),zr=r(fn,"P",{});var ap=a(zr);x1=o(ap,"This model is also a "),qr=r(ap,"A",{href:!0,rel:!0});var Xb=a(qr);j1=o(Xb,"tf.keras.Model"),Xb.forEach(n),L1=o(ap,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ap.forEach(n),A1=d(fn),w(uo.$$.fragment,fn),D1=d(fn),sn=r(fn,"DIV",{class:!0});var Qn=a(sn);w(Pr.$$.fragment,Qn),I1=d(Qn),yt=r(Qn,"P",{});var Ji=a(yt);O1=o(Ji,"The "),bi=r(Ji,"A",{href:!0});var Jb=a(bi);S1=o(Jb,"TFFunnelBaseModel"),Jb.forEach(n),N1=o(Ji," forward method, overrides the "),Fd=r(Ji,"CODE",{});var ey=a(Fd);B1=o(ey,"__call__"),ey.forEach(n),W1=o(Ji," special method."),Ji.forEach(n),Q1=d(Qn),w(po.$$.fragment,Qn),R1=d(Qn),vd=r(Qn,"P",{});var ny=a(vd);H1=o(ny,"Example:"),ny.forEach(n),V1=d(Qn),w(Cr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),Xc=d(i),$t=r(i,"H2",{class:!0});var ip=a($t);ho=r(ip,"A",{id:!0,class:!0,href:!0});var ty=a(ho);kd=r(ty,"SPAN",{});var oy=a(kd);w(xr.$$.fragment,oy),oy.forEach(n),ty.forEach(n),Y1=d(ip),wd=r(ip,"SPAN",{});var sy=a(wd);U1=o(sy,"TFFunnelModel"),sy.forEach(n),ip.forEach(n),Jc=d(i),je=r(i,"DIV",{class:!0});var mn=a(je);w(jr.$$.fragment,mn),G1=d(mn),bd=r(mn,"P",{});var ry=a(bd);Z1=o(ry,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),ry.forEach(n),K1=d(mn),Lr=r(mn,"P",{});var lp=a(Lr);X1=o(lp,"The Funnel Transformer model was proposed in "),Ar=r(lp,"A",{href:!0,rel:!0});var ay=a(Ar);J1=o(ay,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ay.forEach(n),eT=o(lp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),lp.forEach(n),nT=d(mn),Dr=r(mn,"P",{});var dp=a(Dr);tT=o(dp,"This model inherits from "),yi=r(dp,"A",{href:!0});var iy=a(yi);oT=o(iy,"TFPreTrainedModel"),iy.forEach(n),sT=o(dp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dp.forEach(n),rT=d(mn),Ir=r(mn,"P",{});var cp=a(Ir);aT=o(cp,"This model is also a "),Or=r(cp,"A",{href:!0,rel:!0});var ly=a(Or);iT=o(ly,"tf.keras.Model"),ly.forEach(n),lT=o(cp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),cp.forEach(n),dT=d(mn),w(fo.$$.fragment,mn),cT=d(mn),rn=r(mn,"DIV",{class:!0});var Rn=a(rn);w(Sr.$$.fragment,Rn),uT=d(Rn),Et=r(Rn,"P",{});var el=a(Et);pT=o(el,"The "),$i=r(el,"A",{href:!0});var dy=a($i);hT=o(dy,"TFFunnelModel"),dy.forEach(n),fT=o(el," forward method, overrides the "),yd=r(el,"CODE",{});var cy=a(yd);mT=o(cy,"__call__"),cy.forEach(n),gT=o(el," special method."),el.forEach(n),_T=d(Rn),w(mo.$$.fragment,Rn),TT=d(Rn),$d=r(Rn,"P",{});var uy=a($d);FT=o(uy,"Example:"),uy.forEach(n),vT=d(Rn),w(Nr.$$.fragment,Rn),Rn.forEach(n),mn.forEach(n),eu=d(i),Mt=r(i,"H2",{class:!0});var up=a(Mt);go=r(up,"A",{id:!0,class:!0,href:!0});var py=a(go);Ed=r(py,"SPAN",{});var hy=a(Ed);w(Br.$$.fragment,hy),hy.forEach(n),py.forEach(n),kT=d(up),Md=r(up,"SPAN",{});var fy=a(Md);wT=o(fy,"TFFunnelModelForPreTraining"),fy.forEach(n),up.forEach(n),nu=d(i),Le=r(i,"DIV",{class:!0});var gn=a(Le);w(Wr.$$.fragment,gn),bT=d(gn),zd=r(gn,"P",{});var my=a(zd);yT=o(my,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),my.forEach(n),$T=d(gn),Qr=r(gn,"P",{});var pp=a(Qr);ET=o(pp,"The Funnel Transformer model was proposed in "),Rr=r(pp,"A",{href:!0,rel:!0});var gy=a(Rr);MT=o(gy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),gy.forEach(n),zT=o(pp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pp.forEach(n),qT=d(gn),Hr=r(gn,"P",{});var hp=a(Hr);PT=o(hp,"This model inherits from "),Ei=r(hp,"A",{href:!0});var _y=a(Ei);CT=o(_y,"TFPreTrainedModel"),_y.forEach(n),xT=o(hp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hp.forEach(n),jT=d(gn),Vr=r(gn,"P",{});var fp=a(Vr);LT=o(fp,"This model is also a "),Yr=r(fp,"A",{href:!0,rel:!0});var Ty=a(Yr);AT=o(Ty,"tf.keras.Model"),Ty.forEach(n),DT=o(fp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),fp.forEach(n),IT=d(gn),w(_o.$$.fragment,gn),OT=d(gn),an=r(gn,"DIV",{class:!0});var Hn=a(an);w(Ur.$$.fragment,Hn),ST=d(Hn),zt=r(Hn,"P",{});var nl=a(zt);NT=o(nl,"The "),Mi=r(nl,"A",{href:!0});var Fy=a(Mi);BT=o(Fy,"TFFunnelForPreTraining"),Fy.forEach(n),WT=o(nl," forward method, overrides the "),qd=r(nl,"CODE",{});var vy=a(qd);QT=o(vy,"__call__"),vy.forEach(n),RT=o(nl," special method."),nl.forEach(n),HT=d(Hn),w(To.$$.fragment,Hn),VT=d(Hn),Pd=r(Hn,"P",{});var ky=a(Pd);YT=o(ky,"Examples:"),ky.forEach(n),UT=d(Hn),w(Gr.$$.fragment,Hn),Hn.forEach(n),gn.forEach(n),tu=d(i),qt=r(i,"H2",{class:!0});var mp=a(qt);Fo=r(mp,"A",{id:!0,class:!0,href:!0});var wy=a(Fo);Cd=r(wy,"SPAN",{});var by=a(Cd);w(Zr.$$.fragment,by),by.forEach(n),wy.forEach(n),GT=d(mp),xd=r(mp,"SPAN",{});var yy=a(xd);ZT=o(yy,"TFFunnelForMaskedLM"),yy.forEach(n),mp.forEach(n),ou=d(i),Ae=r(i,"DIV",{class:!0});var _n=a(Ae);w(Kr.$$.fragment,_n),KT=d(_n),Xr=r(_n,"P",{});var gp=a(Xr);XT=o(gp,"Funnel Model with a "),jd=r(gp,"CODE",{});var $y=a(jd);JT=o($y,"language modeling"),$y.forEach(n),eF=o(gp," head on top."),gp.forEach(n),nF=d(_n),Jr=r(_n,"P",{});var _p=a(Jr);tF=o(_p,"The Funnel Transformer model was proposed in "),ea=r(_p,"A",{href:!0,rel:!0});var Ey=a(ea);oF=o(Ey,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ey.forEach(n),sF=o(_p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_p.forEach(n),rF=d(_n),na=r(_n,"P",{});var Tp=a(na);aF=o(Tp,"This model inherits from "),zi=r(Tp,"A",{href:!0});var My=a(zi);iF=o(My,"TFPreTrainedModel"),My.forEach(n),lF=o(Tp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tp.forEach(n),dF=d(_n),ta=r(_n,"P",{});var Fp=a(ta);cF=o(Fp,"This model is also a "),oa=r(Fp,"A",{href:!0,rel:!0});var zy=a(oa);uF=o(zy,"tf.keras.Model"),zy.forEach(n),pF=o(Fp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Fp.forEach(n),hF=d(_n),w(vo.$$.fragment,_n),fF=d(_n),ln=r(_n,"DIV",{class:!0});var Vn=a(ln);w(sa.$$.fragment,Vn),mF=d(Vn),Pt=r(Vn,"P",{});var tl=a(Pt);gF=o(tl,"The "),qi=r(tl,"A",{href:!0});var qy=a(qi);_F=o(qy,"TFFunnelForMaskedLM"),qy.forEach(n),TF=o(tl," forward method, overrides the "),Ld=r(tl,"CODE",{});var Py=a(Ld);FF=o(Py,"__call__"),Py.forEach(n),vF=o(tl," special method."),tl.forEach(n),kF=d(Vn),w(ko.$$.fragment,Vn),wF=d(Vn),Ad=r(Vn,"P",{});var Cy=a(Ad);bF=o(Cy,"Example:"),Cy.forEach(n),yF=d(Vn),w(ra.$$.fragment,Vn),Vn.forEach(n),_n.forEach(n),su=d(i),Ct=r(i,"H2",{class:!0});var vp=a(Ct);wo=r(vp,"A",{id:!0,class:!0,href:!0});var xy=a(wo);Dd=r(xy,"SPAN",{});var jy=a(Dd);w(aa.$$.fragment,jy),jy.forEach(n),xy.forEach(n),$F=d(vp),Id=r(vp,"SPAN",{});var Ly=a(Id);EF=o(Ly,"TFFunnelForSequenceClassification"),Ly.forEach(n),vp.forEach(n),ru=d(i),De=r(i,"DIV",{class:!0});var Tn=a(De);w(ia.$$.fragment,Tn),MF=d(Tn),Od=r(Tn,"P",{});var Ay=a(Od);zF=o(Ay,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Ay.forEach(n),qF=d(Tn),la=r(Tn,"P",{});var kp=a(la);PF=o(kp,"The Funnel Transformer model was proposed in "),da=r(kp,"A",{href:!0,rel:!0});var Dy=a(da);CF=o(Dy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Dy.forEach(n),xF=o(kp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),kp.forEach(n),jF=d(Tn),ca=r(Tn,"P",{});var wp=a(ca);LF=o(wp,"This model inherits from "),Pi=r(wp,"A",{href:!0});var Iy=a(Pi);AF=o(Iy,"TFPreTrainedModel"),Iy.forEach(n),DF=o(wp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wp.forEach(n),IF=d(Tn),ua=r(Tn,"P",{});var bp=a(ua);OF=o(bp,"This model is also a "),pa=r(bp,"A",{href:!0,rel:!0});var Oy=a(pa);SF=o(Oy,"tf.keras.Model"),Oy.forEach(n),NF=o(bp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),bp.forEach(n),BF=d(Tn),w(bo.$$.fragment,Tn),WF=d(Tn),dn=r(Tn,"DIV",{class:!0});var Yn=a(dn);w(ha.$$.fragment,Yn),QF=d(Yn),xt=r(Yn,"P",{});var ol=a(xt);RF=o(ol,"The "),Ci=r(ol,"A",{href:!0});var Sy=a(Ci);HF=o(Sy,"TFFunnelForSequenceClassification"),Sy.forEach(n),VF=o(ol," forward method, overrides the "),Sd=r(ol,"CODE",{});var Ny=a(Sd);YF=o(Ny,"__call__"),Ny.forEach(n),UF=o(ol," special method."),ol.forEach(n),GF=d(Yn),w(yo.$$.fragment,Yn),ZF=d(Yn),Nd=r(Yn,"P",{});var By=a(Nd);KF=o(By,"Example:"),By.forEach(n),XF=d(Yn),w(fa.$$.fragment,Yn),Yn.forEach(n),Tn.forEach(n),au=d(i),jt=r(i,"H2",{class:!0});var yp=a(jt);$o=r(yp,"A",{id:!0,class:!0,href:!0});var Wy=a($o);Bd=r(Wy,"SPAN",{});var Qy=a(Bd);w(ma.$$.fragment,Qy),Qy.forEach(n),Wy.forEach(n),JF=d(yp),Wd=r(yp,"SPAN",{});var Ry=a(Wd);ev=o(Ry,"TFFunnelForMultipleChoice"),Ry.forEach(n),yp.forEach(n),iu=d(i),Ie=r(i,"DIV",{class:!0});var Fn=a(Ie);w(ga.$$.fragment,Fn),nv=d(Fn),Qd=r(Fn,"P",{});var Hy=a(Qd);tv=o(Hy,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Hy.forEach(n),ov=d(Fn),_a=r(Fn,"P",{});var $p=a(_a);sv=o($p,"The Funnel Transformer model was proposed in "),Ta=r($p,"A",{href:!0,rel:!0});var Vy=a(Ta);rv=o(Vy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Vy.forEach(n),av=o($p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),$p.forEach(n),iv=d(Fn),Fa=r(Fn,"P",{});var Ep=a(Fa);lv=o(Ep,"This model inherits from "),xi=r(Ep,"A",{href:!0});var Yy=a(xi);dv=o(Yy,"TFPreTrainedModel"),Yy.forEach(n),cv=o(Ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ep.forEach(n),uv=d(Fn),va=r(Fn,"P",{});var Mp=a(va);pv=o(Mp,"This model is also a "),ka=r(Mp,"A",{href:!0,rel:!0});var Uy=a(ka);hv=o(Uy,"tf.keras.Model"),Uy.forEach(n),fv=o(Mp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Mp.forEach(n),mv=d(Fn),w(Eo.$$.fragment,Fn),gv=d(Fn),cn=r(Fn,"DIV",{class:!0});var Un=a(cn);w(wa.$$.fragment,Un),_v=d(Un),Lt=r(Un,"P",{});var sl=a(Lt);Tv=o(sl,"The "),ji=r(sl,"A",{href:!0});var Gy=a(ji);Fv=o(Gy,"TFFunnelForMultipleChoice"),Gy.forEach(n),vv=o(sl," forward method, overrides the "),Rd=r(sl,"CODE",{});var Zy=a(Rd);kv=o(Zy,"__call__"),Zy.forEach(n),wv=o(sl," special method."),sl.forEach(n),bv=d(Un),w(Mo.$$.fragment,Un),yv=d(Un),Hd=r(Un,"P",{});var Ky=a(Hd);$v=o(Ky,"Example:"),Ky.forEach(n),Ev=d(Un),w(ba.$$.fragment,Un),Un.forEach(n),Fn.forEach(n),lu=d(i),At=r(i,"H2",{class:!0});var zp=a(At);zo=r(zp,"A",{id:!0,class:!0,href:!0});var Xy=a(zo);Vd=r(Xy,"SPAN",{});var Jy=a(Vd);w(ya.$$.fragment,Jy),Jy.forEach(n),Xy.forEach(n),Mv=d(zp),Yd=r(zp,"SPAN",{});var e$=a(Yd);zv=o(e$,"TFFunnelForTokenClassification"),e$.forEach(n),zp.forEach(n),du=d(i),Oe=r(i,"DIV",{class:!0});var vn=a(Oe);w($a.$$.fragment,vn),qv=d(vn),Ud=r(vn,"P",{});var n$=a(Ud);Pv=o(n$,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),n$.forEach(n),Cv=d(vn),Ea=r(vn,"P",{});var qp=a(Ea);xv=o(qp,"The Funnel Transformer model was proposed in "),Ma=r(qp,"A",{href:!0,rel:!0});var t$=a(Ma);jv=o(t$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),t$.forEach(n),Lv=o(qp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),qp.forEach(n),Av=d(vn),za=r(vn,"P",{});var Pp=a(za);Dv=o(Pp,"This model inherits from "),Li=r(Pp,"A",{href:!0});var o$=a(Li);Iv=o(o$,"TFPreTrainedModel"),o$.forEach(n),Ov=o(Pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Pp.forEach(n),Sv=d(vn),qa=r(vn,"P",{});var Cp=a(qa);Nv=o(Cp,"This model is also a "),Pa=r(Cp,"A",{href:!0,rel:!0});var s$=a(Pa);Bv=o(s$,"tf.keras.Model"),s$.forEach(n),Wv=o(Cp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Cp.forEach(n),Qv=d(vn),w(qo.$$.fragment,vn),Rv=d(vn),un=r(vn,"DIV",{class:!0});var Gn=a(un);w(Ca.$$.fragment,Gn),Hv=d(Gn),Dt=r(Gn,"P",{});var rl=a(Dt);Vv=o(rl,"The "),Ai=r(rl,"A",{href:!0});var r$=a(Ai);Yv=o(r$,"TFFunnelForTokenClassification"),r$.forEach(n),Uv=o(rl," forward method, overrides the "),Gd=r(rl,"CODE",{});var a$=a(Gd);Gv=o(a$,"__call__"),a$.forEach(n),Zv=o(rl," special method."),rl.forEach(n),Kv=d(Gn),w(Po.$$.fragment,Gn),Xv=d(Gn),Zd=r(Gn,"P",{});var i$=a(Zd);Jv=o(i$,"Example:"),i$.forEach(n),ek=d(Gn),w(xa.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),cu=d(i),It=r(i,"H2",{class:!0});var xp=a(It);Co=r(xp,"A",{id:!0,class:!0,href:!0});var l$=a(Co);Kd=r(l$,"SPAN",{});var d$=a(Kd);w(ja.$$.fragment,d$),d$.forEach(n),l$.forEach(n),nk=d(xp),Xd=r(xp,"SPAN",{});var c$=a(Xd);tk=o(c$,"TFFunnelForQuestionAnswering"),c$.forEach(n),xp.forEach(n),uu=d(i),Se=r(i,"DIV",{class:!0});var kn=a(Se);w(La.$$.fragment,kn),ok=d(kn),Ot=r(kn,"P",{});var al=a(Ot);sk=o(al,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Jd=r(al,"CODE",{});var u$=a(Jd);rk=o(u$,"span start logits"),u$.forEach(n),ak=o(al," and "),ec=r(al,"CODE",{});var p$=a(ec);ik=o(p$,"span end logits"),p$.forEach(n),lk=o(al,")."),al.forEach(n),dk=d(kn),Aa=r(kn,"P",{});var jp=a(Aa);ck=o(jp,"The Funnel Transformer model was proposed in "),Da=r(jp,"A",{href:!0,rel:!0});var h$=a(Da);uk=o(h$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),h$.forEach(n),pk=o(jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jp.forEach(n),hk=d(kn),Ia=r(kn,"P",{});var Lp=a(Ia);fk=o(Lp,"This model inherits from "),Di=r(Lp,"A",{href:!0});var f$=a(Di);mk=o(f$,"TFPreTrainedModel"),f$.forEach(n),gk=o(Lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lp.forEach(n),_k=d(kn),Oa=r(kn,"P",{});var Ap=a(Oa);Tk=o(Ap,"This model is also a "),Sa=r(Ap,"A",{href:!0,rel:!0});var m$=a(Sa);Fk=o(m$,"tf.keras.Model"),m$.forEach(n),vk=o(Ap,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ap.forEach(n),kk=d(kn),w(xo.$$.fragment,kn),wk=d(kn),pn=r(kn,"DIV",{class:!0});var Zn=a(pn);w(Na.$$.fragment,Zn),bk=d(Zn),St=r(Zn,"P",{});var il=a(St);yk=o(il,"The "),Ii=r(il,"A",{href:!0});var g$=a(Ii);$k=o(g$,"TFFunnelForQuestionAnswering"),g$.forEach(n),Ek=o(il," forward method, overrides the "),nc=r(il,"CODE",{});var _$=a(nc);Mk=o(_$,"__call__"),_$.forEach(n),zk=o(il," special method."),il.forEach(n),qk=d(Zn),w(jo.$$.fragment,Zn),Pk=d(Zn),tc=r(Zn,"P",{});var T$=a(tc);Ck=o(T$,"Example:"),T$.forEach(n),xk=d(Zn),w(Ba.$$.fragment,Zn),Zn.forEach(n),kn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(U$)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(X,"id","overview"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Fe,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ve,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ke,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Va,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(Io,"href","https://huggingface.co/sgugger"),c(Io,"rel","nofollow"),c(Oo,"href","https://github.com/laiguokun/Funnel-Transformer"),c(Oo,"rel","nofollow"),c(Nt,"id","transformers.FunnelConfig"),c(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nt,"href","#transformers.FunnelConfig"),c(Kn,"class","relative group"),c(Ya,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelModel"),c(Ua,"href","/docs/transformers/pr_15811/en/model_doc/bert#transformers.TFBertModel"),c(Bo,"href","https://huggingface.co/funnel-transformer/small"),c(Bo,"rel","nofollow"),c(Ga,"href","/docs/transformers/pr_15811/en/main_classes/configuration#transformers.PretrainedConfig"),c(Za,"href","/docs/transformers/pr_15811/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Bt,"id","transformers.FunnelTokenizer"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ka,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizer"),c(Xa,"href","/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer"),c(Ja,"href","/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Qt,"class","docstring"),c(wn,"class","docstring"),c(vl,"class","docstring"),c(Pe,"class","docstring"),c(Rt,"id","transformers.FunnelTokenizerFast"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ti,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(oi,"href","/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizerFast"),c(si,"href","/docs/transformers/pr_15811/en/model_doc/bert#transformers.BertTokenizerFast"),c(bn,"class","docstring"),c(Ze,"class","docstring"),c(Vt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(ri,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(ai,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(us,"href","https://arxiv.org/abs/2006.03236"),c(us,"rel","nofollow"),c(ii,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(fs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(fs,"rel","nofollow"),c(li,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ke,"class","docstring"),c(We,"class","docstring"),c(Gt,"id","transformers.FunnelModel"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(vs,"href","https://arxiv.org/abs/2006.03236"),c(vs,"rel","nofollow"),c(di,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(bs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(bs,"rel","nofollow"),c(ci,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Kt,"id","transformers.FunnelForPreTraining"),c(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(ui,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(Ms,"class","docstring"),c(Jt,"id","transformers.FunnelForMaskedLM"),c(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jt,"href","#transformers.FunnelForMaskedLM"),c(pt,"class","relative group"),c(Ls,"href","https://arxiv.org/abs/2006.03236"),c(Ls,"rel","nofollow"),c(pi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(hi,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Re,"class","docstring"),c(no,"id","transformers.FunnelForSequenceClassification"),c(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(no,"href","#transformers.FunnelForSequenceClassification"),c(ft,"class","relative group"),c(Qs,"href","https://arxiv.org/abs/2006.03236"),c(Qs,"rel","nofollow"),c(fi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(Vs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Vs,"rel","nofollow"),c(mi,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(He,"class","docstring"),c(oo,"id","transformers.FunnelForMultipleChoice"),c(oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oo,"href","#transformers.FunnelForMultipleChoice"),c(gt,"class","relative group"),c(Js,"href","https://arxiv.org/abs/2006.03236"),c(Js,"rel","nofollow"),c(gi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(tr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(tr,"rel","nofollow"),c(_i,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(Ve,"class","docstring"),c(ro,"id","transformers.FunnelForTokenClassification"),c(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ro,"href","#transformers.FunnelForTokenClassification"),c(Tt,"class","relative group"),c(lr,"href","https://arxiv.org/abs/2006.03236"),c(lr,"rel","nofollow"),c(Ti,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(ur,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ur,"rel","nofollow"),c(Fi,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ye,"class","docstring"),c(io,"id","transformers.FunnelForQuestionAnswering"),c(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(io,"href","#transformers.FunnelForQuestionAnswering"),c(vt,"class","relative group"),c(_r,"href","https://arxiv.org/abs/2006.03236"),c(_r,"rel","nofollow"),c(vi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.PreTrainedModel"),c(vr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(vr,"rel","nofollow"),c(ki,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ue,"class","docstring"),c(co,"id","transformers.TFFunnelBaseModel"),c(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(co,"href","#transformers.TFFunnelBaseModel"),c(bt,"class","relative group"),c(Er,"href","https://arxiv.org/abs/2006.03236"),c(Er,"rel","nofollow"),c(wi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(qr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(qr,"rel","nofollow"),c(bi,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(xe,"class","docstring"),c(ho,"id","transformers.TFFunnelModel"),c(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ho,"href","#transformers.TFFunnelModel"),c($t,"class","relative group"),c(Ar,"href","https://arxiv.org/abs/2006.03236"),c(Ar,"rel","nofollow"),c(yi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(Or,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Or,"rel","nofollow"),c($i,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(je,"class","docstring"),c(go,"id","transformers.TFFunnelForPreTraining"),c(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(go,"href","#transformers.TFFunnelForPreTraining"),c(Mt,"class","relative group"),c(Rr,"href","https://arxiv.org/abs/2006.03236"),c(Rr,"rel","nofollow"),c(Ei,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(Yr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Yr,"rel","nofollow"),c(Mi,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(Fo,"id","transformers.TFFunnelForMaskedLM"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFFunnelForMaskedLM"),c(qt,"class","relative group"),c(ea,"href","https://arxiv.org/abs/2006.03236"),c(ea,"rel","nofollow"),c(zi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(oa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(oa,"rel","nofollow"),c(qi,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Ae,"class","docstring"),c(wo,"id","transformers.TFFunnelForSequenceClassification"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFFunnelForSequenceClassification"),c(Ct,"class","relative group"),c(da,"href","https://arxiv.org/abs/2006.03236"),c(da,"rel","nofollow"),c(Pi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(pa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(pa,"rel","nofollow"),c(Ci,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c($o,"id","transformers.TFFunnelForMultipleChoice"),c($o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($o,"href","#transformers.TFFunnelForMultipleChoice"),c(jt,"class","relative group"),c(Ta,"href","https://arxiv.org/abs/2006.03236"),c(Ta,"rel","nofollow"),c(xi,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(ka,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ka,"rel","nofollow"),c(ji,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ie,"class","docstring"),c(zo,"id","transformers.TFFunnelForTokenClassification"),c(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zo,"href","#transformers.TFFunnelForTokenClassification"),c(At,"class","relative group"),c(Ma,"href","https://arxiv.org/abs/2006.03236"),c(Ma,"rel","nofollow"),c(Li,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(Pa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Pa,"rel","nofollow"),c(Ai,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Oe,"class","docstring"),c(Co,"id","transformers.TFFunnelForQuestionAnswering"),c(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Co,"href","#transformers.TFFunnelForQuestionAnswering"),c(It,"class","relative group"),c(Da,"href","https://arxiv.org/abs/2006.03236"),c(Da,"rel","nofollow"),c(Di,"href","/docs/transformers/pr_15811/en/main_classes/model#transformers.TFPreTrainedModel"),c(Sa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Sa,"rel","nofollow"),c(Ii,"href","/docs/transformers/pr_15811/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(pn,"class","docstring"),c(Se,"class","docstring")},m(i,f){e(document.head,p),h(i,M,f),h(i,m,f),e(m,g),e(g,F),b(T,F,null),e(m,_),e(m,z),e(z,ce),h(i,G,f),h(i,q,f),e(q,X),e(X,I),b(ne,I,null),e(q,ue),e(q,O),e(O,pe),h(i,ie,f),h(i,U,f),e(U,L),e(U,te),e(te,Z),e(U,P),h(i,x,f),h(i,oe,f),e(oe,Q),h(i,le,f),h(i,se,f),e(se,S),e(S,he),h(i,de,f),h(i,C,f),e(C,fe),h(i,B,f),h(i,J,f),e(J,ae),e(ae,R),e(J,me),e(J,N),e(N,A),e(N,re),e(re,H),e(N,ge),e(N,u),e(u,v),e(N,K),e(N,Te),e(Te,we),e(N,D),e(N,Fe),e(Fe,be),e(N,ye),e(N,j),e(j,V),e(N,$e),e(N,ve),e(ve,Y),e(N,Ee),e(N,ke),e(ke,_e),e(N,Me),e(N,Va),e(Va,Dp),e(N,Ip),h(i,yc,f),h(i,jn,f),e(jn,Op),e(jn,Io),e(Io,Sp),e(jn,Np),e(jn,Oo),e(Oo,Bp),e(jn,Wp),h(i,$c,f),h(i,Kn,f),e(Kn,Nt),e(Nt,ll),b(So,ll,null),e(Kn,Qp),e(Kn,dl),e(dl,Rp),h(i,Ec,f),h(i,Cn,f),b(No,Cn,null),e(Cn,Hp),e(Cn,xn),e(xn,Vp),e(xn,Ya),e(Ya,Yp),e(xn,Up),e(xn,Ua),e(Ua,Gp),e(xn,Zp),e(xn,Bo),e(Bo,Kp),e(xn,Xp),e(Cn,Jp),e(Cn,Xn),e(Xn,eh),e(Xn,Ga),e(Ga,nh),e(Xn,th),e(Xn,Za),e(Za,oh),e(Xn,sh),h(i,Mc,f),h(i,Jn,f),e(Jn,Bt),e(Bt,cl),b(Wo,cl,null),e(Jn,rh),e(Jn,ul),e(ul,ah),h(i,zc,f),h(i,Pe,f),b(Qo,Pe,null),e(Pe,ih),e(Pe,pl),e(pl,lh),e(Pe,dh),e(Pe,Wt),e(Wt,Ka),e(Ka,ch),e(Wt,uh),e(Wt,Xa),e(Xa,ph),e(Wt,hh),e(Pe,fh),e(Pe,Ro),e(Ro,mh),e(Ro,Ja),e(Ja,gh),e(Ro,_h),e(Pe,Th),e(Pe,Ln),b(Ho,Ln,null),e(Ln,Fh),e(Ln,hl),e(hl,vh),e(Ln,kh),e(Ln,Vo),e(Vo,ei),e(ei,wh),e(ei,fl),e(fl,bh),e(Vo,yh),e(Vo,ni),e(ni,$h),e(ni,ml),e(ml,Eh),e(Pe,Mh),e(Pe,Qt),b(Yo,Qt,null),e(Qt,zh),e(Qt,Uo),e(Uo,qh),e(Uo,gl),e(gl,Ph),e(Uo,Ch),e(Pe,xh),e(Pe,wn),b(Go,wn,null),e(wn,jh),e(wn,_l),e(_l,Lh),e(wn,Ah),b(Zo,wn,null),e(wn,Dh),e(wn,et),e(et,Ih),e(et,Tl),e(Tl,Oh),e(et,Sh),e(et,Fl),e(Fl,Nh),e(et,Bh),e(Pe,Wh),e(Pe,vl),h(i,qc,f),h(i,nt,f),e(nt,Rt),e(Rt,kl),b(Ko,kl,null),e(nt,Qh),e(nt,wl),e(wl,Rh),h(i,Pc,f),h(i,Ze,f),b(Xo,Ze,null),e(Ze,Hh),e(Ze,Jo),e(Jo,Vh),e(Jo,bl),e(bl,Yh),e(Jo,Uh),e(Ze,Gh),e(Ze,Ht),e(Ht,ti),e(ti,Zh),e(Ht,Kh),e(Ht,oi),e(oi,Xh),e(Ht,Jh),e(Ze,ef),e(Ze,es),e(es,nf),e(es,si),e(si,tf),e(es,of),e(Ze,sf),e(Ze,bn),b(ns,bn,null),e(bn,rf),e(bn,yl),e(yl,af),e(bn,lf),b(ts,bn,null),e(bn,df),e(bn,tt),e(tt,cf),e(tt,$l),e($l,uf),e(tt,pf),e(tt,El),e(El,hf),e(tt,ff),h(i,Cc,f),h(i,ot,f),e(ot,Vt),e(Vt,Ml),b(os,Ml,null),e(ot,mf),e(ot,zl),e(zl,gf),h(i,xc,f),h(i,st,f),b(ss,st,null),e(st,_f),e(st,rs),e(rs,Tf),e(rs,ri),e(ri,Ff),e(rs,vf),h(i,jc,f),h(i,rt,f),b(as,rt,null),e(rt,kf),e(rt,is),e(is,wf),e(is,ai),e(ai,bf),e(is,yf),h(i,Lc,f),h(i,at,f),e(at,Yt),e(Yt,ql),b(ls,ql,null),e(at,$f),e(at,Pl),e(Pl,Ef),h(i,Ac,f),h(i,We,f),b(ds,We,null),e(We,Mf),e(We,Cl),e(Cl,zf),e(We,qf),e(We,cs),e(cs,Pf),e(cs,us),e(us,Cf),e(cs,xf),e(We,jf),e(We,ps),e(ps,Lf),e(ps,ii),e(ii,Af),e(ps,Df),e(We,If),e(We,hs),e(hs,Of),e(hs,fs),e(fs,Sf),e(hs,Nf),e(We,Bf),e(We,Ke),b(ms,Ke,null),e(Ke,Wf),e(Ke,it),e(it,Qf),e(it,li),e(li,Rf),e(it,Hf),e(it,xl),e(xl,Vf),e(it,Yf),e(Ke,Uf),b(Ut,Ke,null),e(Ke,Gf),e(Ke,jl),e(jl,Zf),e(Ke,Kf),b(gs,Ke,null),h(i,Dc,f),h(i,lt,f),e(lt,Gt),e(Gt,Ll),b(_s,Ll,null),e(lt,Xf),e(lt,Al),e(Al,Jf),h(i,Ic,f),h(i,Qe,f),b(Ts,Qe,null),e(Qe,em),e(Qe,Dl),e(Dl,nm),e(Qe,tm),e(Qe,Fs),e(Fs,om),e(Fs,vs),e(vs,sm),e(Fs,rm),e(Qe,am),e(Qe,ks),e(ks,im),e(ks,di),e(di,lm),e(ks,dm),e(Qe,cm),e(Qe,ws),e(ws,um),e(ws,bs),e(bs,pm),e(ws,hm),e(Qe,fm),e(Qe,Xe),b(ys,Xe,null),e(Xe,mm),e(Xe,dt),e(dt,gm),e(dt,ci),e(ci,_m),e(dt,Tm),e(dt,Il),e(Il,Fm),e(dt,vm),e(Xe,km),b(Zt,Xe,null),e(Xe,wm),e(Xe,Ol),e(Ol,bm),e(Xe,ym),b($s,Xe,null),h(i,Oc,f),h(i,ct,f),e(ct,Kt),e(Kt,Sl),b(Es,Sl,null),e(ct,$m),e(ct,Nl),e(Nl,Em),h(i,Sc,f),h(i,Ms,f),e(Ms,Je),b(zs,Je,null),e(Je,Mm),e(Je,ut),e(ut,zm),e(ut,ui),e(ui,qm),e(ut,Pm),e(ut,Bl),e(Bl,Cm),e(ut,xm),e(Je,jm),b(Xt,Je,null),e(Je,Lm),e(Je,Wl),e(Wl,Am),e(Je,Dm),b(qs,Je,null),h(i,Nc,f),h(i,pt,f),e(pt,Jt),e(Jt,Ql),b(Ps,Ql,null),e(pt,Im),e(pt,Rl),e(Rl,Om),h(i,Bc,f),h(i,Re,f),b(Cs,Re,null),e(Re,Sm),e(Re,xs),e(xs,Nm),e(xs,Hl),e(Hl,Bm),e(xs,Wm),e(Re,Qm),e(Re,js),e(js,Rm),e(js,Ls),e(Ls,Hm),e(js,Vm),e(Re,Ym),e(Re,As),e(As,Um),e(As,pi),e(pi,Gm),e(As,Zm),e(Re,Km),e(Re,Ds),e(Ds,Xm),e(Ds,Is),e(Is,Jm),e(Ds,eg),e(Re,ng),e(Re,en),b(Os,en,null),e(en,tg),e(en,ht),e(ht,og),e(ht,hi),e(hi,sg),e(ht,rg),e(ht,Vl),e(Vl,ag),e(ht,ig),e(en,lg),b(eo,en,null),e(en,dg),e(en,Yl),e(Yl,cg),e(en,ug),b(Ss,en,null),h(i,Wc,f),h(i,ft,f),e(ft,no),e(no,Ul),b(Ns,Ul,null),e(ft,pg),e(ft,Gl),e(Gl,hg),h(i,Qc,f),h(i,He,f),b(Bs,He,null),e(He,fg),e(He,Zl),e(Zl,mg),e(He,gg),e(He,Ws),e(Ws,_g),e(Ws,Qs),e(Qs,Tg),e(Ws,Fg),e(He,vg),e(He,Rs),e(Rs,kg),e(Rs,fi),e(fi,wg),e(Rs,bg),e(He,yg),e(He,Hs),e(Hs,$g),e(Hs,Vs),e(Vs,Eg),e(Hs,Mg),e(He,zg),e(He,Be),b(Ys,Be,null),e(Be,qg),e(Be,mt),e(mt,Pg),e(mt,mi),e(mi,Cg),e(mt,xg),e(mt,Kl),e(Kl,jg),e(mt,Lg),e(Be,Ag),b(to,Be,null),e(Be,Dg),e(Be,Xl),e(Xl,Ig),e(Be,Og),b(Us,Be,null),e(Be,Sg),e(Be,Jl),e(Jl,Ng),e(Be,Bg),b(Gs,Be,null),h(i,Rc,f),h(i,gt,f),e(gt,oo),e(oo,ed),b(Zs,ed,null),e(gt,Wg),e(gt,nd),e(nd,Qg),h(i,Hc,f),h(i,Ve,f),b(Ks,Ve,null),e(Ve,Rg),e(Ve,td),e(td,Hg),e(Ve,Vg),e(Ve,Xs),e(Xs,Yg),e(Xs,Js),e(Js,Ug),e(Xs,Gg),e(Ve,Zg),e(Ve,er),e(er,Kg),e(er,gi),e(gi,Xg),e(er,Jg),e(Ve,e_),e(Ve,nr),e(nr,n_),e(nr,tr),e(tr,t_),e(nr,o_),e(Ve,s_),e(Ve,nn),b(or,nn,null),e(nn,r_),e(nn,_t),e(_t,a_),e(_t,_i),e(_i,i_),e(_t,l_),e(_t,od),e(od,d_),e(_t,c_),e(nn,u_),b(so,nn,null),e(nn,p_),e(nn,sd),e(sd,h_),e(nn,f_),b(sr,nn,null),h(i,Vc,f),h(i,Tt,f),e(Tt,ro),e(ro,rd),b(rr,rd,null),e(Tt,m_),e(Tt,ad),e(ad,g_),h(i,Yc,f),h(i,Ye,f),b(ar,Ye,null),e(Ye,__),e(Ye,id),e(id,T_),e(Ye,F_),e(Ye,ir),e(ir,v_),e(ir,lr),e(lr,k_),e(ir,w_),e(Ye,b_),e(Ye,dr),e(dr,y_),e(dr,Ti),e(Ti,$_),e(dr,E_),e(Ye,M_),e(Ye,cr),e(cr,z_),e(cr,ur),e(ur,q_),e(cr,P_),e(Ye,C_),e(Ye,tn),b(pr,tn,null),e(tn,x_),e(tn,Ft),e(Ft,j_),e(Ft,Fi),e(Fi,L_),e(Ft,A_),e(Ft,ld),e(ld,D_),e(Ft,I_),e(tn,O_),b(ao,tn,null),e(tn,S_),e(tn,dd),e(dd,N_),e(tn,B_),b(hr,tn,null),h(i,Uc,f),h(i,vt,f),e(vt,io),e(io,cd),b(fr,cd,null),e(vt,W_),e(vt,ud),e(ud,Q_),h(i,Gc,f),h(i,Ue,f),b(mr,Ue,null),e(Ue,R_),e(Ue,kt),e(kt,H_),e(kt,pd),e(pd,V_),e(kt,Y_),e(kt,hd),e(hd,U_),e(kt,G_),e(Ue,Z_),e(Ue,gr),e(gr,K_),e(gr,_r),e(_r,X_),e(gr,J_),e(Ue,e1),e(Ue,Tr),e(Tr,n1),e(Tr,vi),e(vi,t1),e(Tr,o1),e(Ue,s1),e(Ue,Fr),e(Fr,r1),e(Fr,vr),e(vr,a1),e(Fr,i1),e(Ue,l1),e(Ue,on),b(kr,on,null),e(on,d1),e(on,wt),e(wt,c1),e(wt,ki),e(ki,u1),e(wt,p1),e(wt,fd),e(fd,h1),e(wt,f1),e(on,m1),b(lo,on,null),e(on,g1),e(on,md),e(md,_1),e(on,T1),b(wr,on,null),h(i,Zc,f),h(i,bt,f),e(bt,co),e(co,gd),b(br,gd,null),e(bt,F1),e(bt,_d),e(_d,v1),h(i,Kc,f),h(i,xe,f),b(yr,xe,null),e(xe,k1),e(xe,Td),e(Td,w1),e(xe,b1),e(xe,$r),e($r,y1),e($r,Er),e(Er,$1),e($r,E1),e(xe,M1),e(xe,Mr),e(Mr,z1),e(Mr,wi),e(wi,q1),e(Mr,P1),e(xe,C1),e(xe,zr),e(zr,x1),e(zr,qr),e(qr,j1),e(zr,L1),e(xe,A1),b(uo,xe,null),e(xe,D1),e(xe,sn),b(Pr,sn,null),e(sn,I1),e(sn,yt),e(yt,O1),e(yt,bi),e(bi,S1),e(yt,N1),e(yt,Fd),e(Fd,B1),e(yt,W1),e(sn,Q1),b(po,sn,null),e(sn,R1),e(sn,vd),e(vd,H1),e(sn,V1),b(Cr,sn,null),h(i,Xc,f),h(i,$t,f),e($t,ho),e(ho,kd),b(xr,kd,null),e($t,Y1),e($t,wd),e(wd,U1),h(i,Jc,f),h(i,je,f),b(jr,je,null),e(je,G1),e(je,bd),e(bd,Z1),e(je,K1),e(je,Lr),e(Lr,X1),e(Lr,Ar),e(Ar,J1),e(Lr,eT),e(je,nT),e(je,Dr),e(Dr,tT),e(Dr,yi),e(yi,oT),e(Dr,sT),e(je,rT),e(je,Ir),e(Ir,aT),e(Ir,Or),e(Or,iT),e(Ir,lT),e(je,dT),b(fo,je,null),e(je,cT),e(je,rn),b(Sr,rn,null),e(rn,uT),e(rn,Et),e(Et,pT),e(Et,$i),e($i,hT),e(Et,fT),e(Et,yd),e(yd,mT),e(Et,gT),e(rn,_T),b(mo,rn,null),e(rn,TT),e(rn,$d),e($d,FT),e(rn,vT),b(Nr,rn,null),h(i,eu,f),h(i,Mt,f),e(Mt,go),e(go,Ed),b(Br,Ed,null),e(Mt,kT),e(Mt,Md),e(Md,wT),h(i,nu,f),h(i,Le,f),b(Wr,Le,null),e(Le,bT),e(Le,zd),e(zd,yT),e(Le,$T),e(Le,Qr),e(Qr,ET),e(Qr,Rr),e(Rr,MT),e(Qr,zT),e(Le,qT),e(Le,Hr),e(Hr,PT),e(Hr,Ei),e(Ei,CT),e(Hr,xT),e(Le,jT),e(Le,Vr),e(Vr,LT),e(Vr,Yr),e(Yr,AT),e(Vr,DT),e(Le,IT),b(_o,Le,null),e(Le,OT),e(Le,an),b(Ur,an,null),e(an,ST),e(an,zt),e(zt,NT),e(zt,Mi),e(Mi,BT),e(zt,WT),e(zt,qd),e(qd,QT),e(zt,RT),e(an,HT),b(To,an,null),e(an,VT),e(an,Pd),e(Pd,YT),e(an,UT),b(Gr,an,null),h(i,tu,f),h(i,qt,f),e(qt,Fo),e(Fo,Cd),b(Zr,Cd,null),e(qt,GT),e(qt,xd),e(xd,ZT),h(i,ou,f),h(i,Ae,f),b(Kr,Ae,null),e(Ae,KT),e(Ae,Xr),e(Xr,XT),e(Xr,jd),e(jd,JT),e(Xr,eF),e(Ae,nF),e(Ae,Jr),e(Jr,tF),e(Jr,ea),e(ea,oF),e(Jr,sF),e(Ae,rF),e(Ae,na),e(na,aF),e(na,zi),e(zi,iF),e(na,lF),e(Ae,dF),e(Ae,ta),e(ta,cF),e(ta,oa),e(oa,uF),e(ta,pF),e(Ae,hF),b(vo,Ae,null),e(Ae,fF),e(Ae,ln),b(sa,ln,null),e(ln,mF),e(ln,Pt),e(Pt,gF),e(Pt,qi),e(qi,_F),e(Pt,TF),e(Pt,Ld),e(Ld,FF),e(Pt,vF),e(ln,kF),b(ko,ln,null),e(ln,wF),e(ln,Ad),e(Ad,bF),e(ln,yF),b(ra,ln,null),h(i,su,f),h(i,Ct,f),e(Ct,wo),e(wo,Dd),b(aa,Dd,null),e(Ct,$F),e(Ct,Id),e(Id,EF),h(i,ru,f),h(i,De,f),b(ia,De,null),e(De,MF),e(De,Od),e(Od,zF),e(De,qF),e(De,la),e(la,PF),e(la,da),e(da,CF),e(la,xF),e(De,jF),e(De,ca),e(ca,LF),e(ca,Pi),e(Pi,AF),e(ca,DF),e(De,IF),e(De,ua),e(ua,OF),e(ua,pa),e(pa,SF),e(ua,NF),e(De,BF),b(bo,De,null),e(De,WF),e(De,dn),b(ha,dn,null),e(dn,QF),e(dn,xt),e(xt,RF),e(xt,Ci),e(Ci,HF),e(xt,VF),e(xt,Sd),e(Sd,YF),e(xt,UF),e(dn,GF),b(yo,dn,null),e(dn,ZF),e(dn,Nd),e(Nd,KF),e(dn,XF),b(fa,dn,null),h(i,au,f),h(i,jt,f),e(jt,$o),e($o,Bd),b(ma,Bd,null),e(jt,JF),e(jt,Wd),e(Wd,ev),h(i,iu,f),h(i,Ie,f),b(ga,Ie,null),e(Ie,nv),e(Ie,Qd),e(Qd,tv),e(Ie,ov),e(Ie,_a),e(_a,sv),e(_a,Ta),e(Ta,rv),e(_a,av),e(Ie,iv),e(Ie,Fa),e(Fa,lv),e(Fa,xi),e(xi,dv),e(Fa,cv),e(Ie,uv),e(Ie,va),e(va,pv),e(va,ka),e(ka,hv),e(va,fv),e(Ie,mv),b(Eo,Ie,null),e(Ie,gv),e(Ie,cn),b(wa,cn,null),e(cn,_v),e(cn,Lt),e(Lt,Tv),e(Lt,ji),e(ji,Fv),e(Lt,vv),e(Lt,Rd),e(Rd,kv),e(Lt,wv),e(cn,bv),b(Mo,cn,null),e(cn,yv),e(cn,Hd),e(Hd,$v),e(cn,Ev),b(ba,cn,null),h(i,lu,f),h(i,At,f),e(At,zo),e(zo,Vd),b(ya,Vd,null),e(At,Mv),e(At,Yd),e(Yd,zv),h(i,du,f),h(i,Oe,f),b($a,Oe,null),e(Oe,qv),e(Oe,Ud),e(Ud,Pv),e(Oe,Cv),e(Oe,Ea),e(Ea,xv),e(Ea,Ma),e(Ma,jv),e(Ea,Lv),e(Oe,Av),e(Oe,za),e(za,Dv),e(za,Li),e(Li,Iv),e(za,Ov),e(Oe,Sv),e(Oe,qa),e(qa,Nv),e(qa,Pa),e(Pa,Bv),e(qa,Wv),e(Oe,Qv),b(qo,Oe,null),e(Oe,Rv),e(Oe,un),b(Ca,un,null),e(un,Hv),e(un,Dt),e(Dt,Vv),e(Dt,Ai),e(Ai,Yv),e(Dt,Uv),e(Dt,Gd),e(Gd,Gv),e(Dt,Zv),e(un,Kv),b(Po,un,null),e(un,Xv),e(un,Zd),e(Zd,Jv),e(un,ek),b(xa,un,null),h(i,cu,f),h(i,It,f),e(It,Co),e(Co,Kd),b(ja,Kd,null),e(It,nk),e(It,Xd),e(Xd,tk),h(i,uu,f),h(i,Se,f),b(La,Se,null),e(Se,ok),e(Se,Ot),e(Ot,sk),e(Ot,Jd),e(Jd,rk),e(Ot,ak),e(Ot,ec),e(ec,ik),e(Ot,lk),e(Se,dk),e(Se,Aa),e(Aa,ck),e(Aa,Da),e(Da,uk),e(Aa,pk),e(Se,hk),e(Se,Ia),e(Ia,fk),e(Ia,Di),e(Di,mk),e(Ia,gk),e(Se,_k),e(Se,Oa),e(Oa,Tk),e(Oa,Sa),e(Sa,Fk),e(Oa,vk),e(Se,kk),b(xo,Se,null),e(Se,wk),e(Se,pn),b(Na,pn,null),e(pn,bk),e(pn,St),e(St,yk),e(St,Ii),e(Ii,$k),e(St,Ek),e(St,nc),e(nc,Mk),e(St,zk),e(pn,qk),b(jo,pn,null),e(pn,Pk),e(pn,tc),e(tc,Ck),e(pn,xk),b(Ba,pn,null),pu=!0},p(i,[f]){const Wa={};f&2&&(Wa.$$scope={dirty:f,ctx:i}),Ut.$set(Wa);const oc={};f&2&&(oc.$$scope={dirty:f,ctx:i}),Zt.$set(oc);const sc={};f&2&&(sc.$$scope={dirty:f,ctx:i}),Xt.$set(sc);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:i}),eo.$set(rc);const Qa={};f&2&&(Qa.$$scope={dirty:f,ctx:i}),to.$set(Qa);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:i}),so.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:i}),ao.$set(ic);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:i}),lo.$set(lc);const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:i}),uo.$set(Ra);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:i}),po.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:i}),fo.$set(cc);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:i}),mo.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:i}),_o.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:i}),To.$set(hc);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:i}),vo.$set(Ha);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:i}),ko.$set(fc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:i}),bo.$set(Ce);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:i}),yo.$set(mc);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:i}),Eo.$set(gc);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:i}),Mo.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:i}),qo.$set(Tc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:i}),Po.$set(Fc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:i}),xo.$set(vc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:i}),jo.$set(kc)},i(i){pu||(y(T.$$.fragment,i),y(ne.$$.fragment,i),y(So.$$.fragment,i),y(No.$$.fragment,i),y(Wo.$$.fragment,i),y(Qo.$$.fragment,i),y(Ho.$$.fragment,i),y(Yo.$$.fragment,i),y(Go.$$.fragment,i),y(Zo.$$.fragment,i),y(Ko.$$.fragment,i),y(Xo.$$.fragment,i),y(ns.$$.fragment,i),y(ts.$$.fragment,i),y(os.$$.fragment,i),y(ss.$$.fragment,i),y(as.$$.fragment,i),y(ls.$$.fragment,i),y(ds.$$.fragment,i),y(ms.$$.fragment,i),y(Ut.$$.fragment,i),y(gs.$$.fragment,i),y(_s.$$.fragment,i),y(Ts.$$.fragment,i),y(ys.$$.fragment,i),y(Zt.$$.fragment,i),y($s.$$.fragment,i),y(Es.$$.fragment,i),y(zs.$$.fragment,i),y(Xt.$$.fragment,i),y(qs.$$.fragment,i),y(Ps.$$.fragment,i),y(Cs.$$.fragment,i),y(Os.$$.fragment,i),y(eo.$$.fragment,i),y(Ss.$$.fragment,i),y(Ns.$$.fragment,i),y(Bs.$$.fragment,i),y(Ys.$$.fragment,i),y(to.$$.fragment,i),y(Us.$$.fragment,i),y(Gs.$$.fragment,i),y(Zs.$$.fragment,i),y(Ks.$$.fragment,i),y(or.$$.fragment,i),y(so.$$.fragment,i),y(sr.$$.fragment,i),y(rr.$$.fragment,i),y(ar.$$.fragment,i),y(pr.$$.fragment,i),y(ao.$$.fragment,i),y(hr.$$.fragment,i),y(fr.$$.fragment,i),y(mr.$$.fragment,i),y(kr.$$.fragment,i),y(lo.$$.fragment,i),y(wr.$$.fragment,i),y(br.$$.fragment,i),y(yr.$$.fragment,i),y(uo.$$.fragment,i),y(Pr.$$.fragment,i),y(po.$$.fragment,i),y(Cr.$$.fragment,i),y(xr.$$.fragment,i),y(jr.$$.fragment,i),y(fo.$$.fragment,i),y(Sr.$$.fragment,i),y(mo.$$.fragment,i),y(Nr.$$.fragment,i),y(Br.$$.fragment,i),y(Wr.$$.fragment,i),y(_o.$$.fragment,i),y(Ur.$$.fragment,i),y(To.$$.fragment,i),y(Gr.$$.fragment,i),y(Zr.$$.fragment,i),y(Kr.$$.fragment,i),y(vo.$$.fragment,i),y(sa.$$.fragment,i),y(ko.$$.fragment,i),y(ra.$$.fragment,i),y(aa.$$.fragment,i),y(ia.$$.fragment,i),y(bo.$$.fragment,i),y(ha.$$.fragment,i),y(yo.$$.fragment,i),y(fa.$$.fragment,i),y(ma.$$.fragment,i),y(ga.$$.fragment,i),y(Eo.$$.fragment,i),y(wa.$$.fragment,i),y(Mo.$$.fragment,i),y(ba.$$.fragment,i),y(ya.$$.fragment,i),y($a.$$.fragment,i),y(qo.$$.fragment,i),y(Ca.$$.fragment,i),y(Po.$$.fragment,i),y(xa.$$.fragment,i),y(ja.$$.fragment,i),y(La.$$.fragment,i),y(xo.$$.fragment,i),y(Na.$$.fragment,i),y(jo.$$.fragment,i),y(Ba.$$.fragment,i),pu=!0)},o(i){$(T.$$.fragment,i),$(ne.$$.fragment,i),$(So.$$.fragment,i),$(No.$$.fragment,i),$(Wo.$$.fragment,i),$(Qo.$$.fragment,i),$(Ho.$$.fragment,i),$(Yo.$$.fragment,i),$(Go.$$.fragment,i),$(Zo.$$.fragment,i),$(Ko.$$.fragment,i),$(Xo.$$.fragment,i),$(ns.$$.fragment,i),$(ts.$$.fragment,i),$(os.$$.fragment,i),$(ss.$$.fragment,i),$(as.$$.fragment,i),$(ls.$$.fragment,i),$(ds.$$.fragment,i),$(ms.$$.fragment,i),$(Ut.$$.fragment,i),$(gs.$$.fragment,i),$(_s.$$.fragment,i),$(Ts.$$.fragment,i),$(ys.$$.fragment,i),$(Zt.$$.fragment,i),$($s.$$.fragment,i),$(Es.$$.fragment,i),$(zs.$$.fragment,i),$(Xt.$$.fragment,i),$(qs.$$.fragment,i),$(Ps.$$.fragment,i),$(Cs.$$.fragment,i),$(Os.$$.fragment,i),$(eo.$$.fragment,i),$(Ss.$$.fragment,i),$(Ns.$$.fragment,i),$(Bs.$$.fragment,i),$(Ys.$$.fragment,i),$(to.$$.fragment,i),$(Us.$$.fragment,i),$(Gs.$$.fragment,i),$(Zs.$$.fragment,i),$(Ks.$$.fragment,i),$(or.$$.fragment,i),$(so.$$.fragment,i),$(sr.$$.fragment,i),$(rr.$$.fragment,i),$(ar.$$.fragment,i),$(pr.$$.fragment,i),$(ao.$$.fragment,i),$(hr.$$.fragment,i),$(fr.$$.fragment,i),$(mr.$$.fragment,i),$(kr.$$.fragment,i),$(lo.$$.fragment,i),$(wr.$$.fragment,i),$(br.$$.fragment,i),$(yr.$$.fragment,i),$(uo.$$.fragment,i),$(Pr.$$.fragment,i),$(po.$$.fragment,i),$(Cr.$$.fragment,i),$(xr.$$.fragment,i),$(jr.$$.fragment,i),$(fo.$$.fragment,i),$(Sr.$$.fragment,i),$(mo.$$.fragment,i),$(Nr.$$.fragment,i),$(Br.$$.fragment,i),$(Wr.$$.fragment,i),$(_o.$$.fragment,i),$(Ur.$$.fragment,i),$(To.$$.fragment,i),$(Gr.$$.fragment,i),$(Zr.$$.fragment,i),$(Kr.$$.fragment,i),$(vo.$$.fragment,i),$(sa.$$.fragment,i),$(ko.$$.fragment,i),$(ra.$$.fragment,i),$(aa.$$.fragment,i),$(ia.$$.fragment,i),$(bo.$$.fragment,i),$(ha.$$.fragment,i),$(yo.$$.fragment,i),$(fa.$$.fragment,i),$(ma.$$.fragment,i),$(ga.$$.fragment,i),$(Eo.$$.fragment,i),$(wa.$$.fragment,i),$(Mo.$$.fragment,i),$(ba.$$.fragment,i),$(ya.$$.fragment,i),$($a.$$.fragment,i),$(qo.$$.fragment,i),$(Ca.$$.fragment,i),$(Po.$$.fragment,i),$(xa.$$.fragment,i),$(ja.$$.fragment,i),$(La.$$.fragment,i),$(xo.$$.fragment,i),$(Na.$$.fragment,i),$(jo.$$.fragment,i),$(Ba.$$.fragment,i),pu=!1},d(i){n(p),i&&n(M),i&&n(m),E(T),i&&n(G),i&&n(q),E(ne),i&&n(ie),i&&n(U),i&&n(x),i&&n(oe),i&&n(le),i&&n(se),i&&n(de),i&&n(C),i&&n(B),i&&n(J),i&&n(yc),i&&n(jn),i&&n($c),i&&n(Kn),E(So),i&&n(Ec),i&&n(Cn),E(No),i&&n(Mc),i&&n(Jn),E(Wo),i&&n(zc),i&&n(Pe),E(Qo),E(Ho),E(Yo),E(Go),E(Zo),i&&n(qc),i&&n(nt),E(Ko),i&&n(Pc),i&&n(Ze),E(Xo),E(ns),E(ts),i&&n(Cc),i&&n(ot),E(os),i&&n(xc),i&&n(st),E(ss),i&&n(jc),i&&n(rt),E(as),i&&n(Lc),i&&n(at),E(ls),i&&n(Ac),i&&n(We),E(ds),E(ms),E(Ut),E(gs),i&&n(Dc),i&&n(lt),E(_s),i&&n(Ic),i&&n(Qe),E(Ts),E(ys),E(Zt),E($s),i&&n(Oc),i&&n(ct),E(Es),i&&n(Sc),i&&n(Ms),E(zs),E(Xt),E(qs),i&&n(Nc),i&&n(pt),E(Ps),i&&n(Bc),i&&n(Re),E(Cs),E(Os),E(eo),E(Ss),i&&n(Wc),i&&n(ft),E(Ns),i&&n(Qc),i&&n(He),E(Bs),E(Ys),E(to),E(Us),E(Gs),i&&n(Rc),i&&n(gt),E(Zs),i&&n(Hc),i&&n(Ve),E(Ks),E(or),E(so),E(sr),i&&n(Vc),i&&n(Tt),E(rr),i&&n(Yc),i&&n(Ye),E(ar),E(pr),E(ao),E(hr),i&&n(Uc),i&&n(vt),E(fr),i&&n(Gc),i&&n(Ue),E(mr),E(kr),E(lo),E(wr),i&&n(Zc),i&&n(bt),E(br),i&&n(Kc),i&&n(xe),E(yr),E(uo),E(Pr),E(po),E(Cr),i&&n(Xc),i&&n($t),E(xr),i&&n(Jc),i&&n(je),E(jr),E(fo),E(Sr),E(mo),E(Nr),i&&n(eu),i&&n(Mt),E(Br),i&&n(nu),i&&n(Le),E(Wr),E(_o),E(Ur),E(To),E(Gr),i&&n(tu),i&&n(qt),E(Zr),i&&n(ou),i&&n(Ae),E(Kr),E(vo),E(sa),E(ko),E(ra),i&&n(su),i&&n(Ct),E(aa),i&&n(ru),i&&n(De),E(ia),E(bo),E(ha),E(yo),E(fa),i&&n(au),i&&n(jt),E(ma),i&&n(iu),i&&n(Ie),E(ga),E(Eo),E(wa),E(Mo),E(ba),i&&n(lu),i&&n(At),E(ya),i&&n(du),i&&n(Oe),E($a),E(qo),E(Ca),E(Po),E(xa),i&&n(cu),i&&n(It),E(ja),i&&n(uu),i&&n(Se),E(La),E(xo),E(Na),E(jo),E(Ba)}}}const U$={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function G$(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class t2 extends F${constructor(p){super();v$(this,p,G$,Y$,k$,{fw:0})}}export{t2 as default,U$ as metadata};
