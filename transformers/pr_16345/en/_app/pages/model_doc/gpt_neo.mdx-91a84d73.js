import{S as Ql,i as Yl,s as Zl,e as n,k as d,w as _,t as r,M as ed,c as s,d as o,m as c,a,x as T,h as i,b as l,F as e,g as u,y as v,q as y,o as b,B as P}from"../../chunks/vendor-6b77c823.js";import{T as rn}from"../../chunks/Tip-39098574.js";import{D as V}from"../../chunks/Docstring-abef54e3.js";import{C as je}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as _e}from"../../chunks/IconCopyLink-7a11ce68.js";function td(B){let p,w,m,k,N;return{c(){p=n("p"),w=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),N=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,k),e(p,N)},d(g){g&&o(p)}}}function od(B){let p,w,m,k,N;return{c(){p=n("p"),w=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),N=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,k),e(p,N)},d(g){g&&o(p)}}}function nd(B){let p,w,m,k,N;return{c(){p=n("p"),w=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),N=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,k),e(p,N)},d(g){g&&o(p)}}}function sd(B){let p,w,m,k,N;return{c(){p=n("p"),w=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),N=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,k),e(p,N)},d(g){g&&o(p)}}}function ad(B){let p,w,m,k,N;return{c(){p=n("p"),w=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),k=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(f,"CODE",{});var $=a(m);k=i($,"Module"),$.forEach(o),N=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,k),e(p,N)},d(g){g&&o(p)}}}function rd(B){let p,w,m,k,N,g,f,$,Jn,ln,oe,Te,ro,Ae,Rn,io,Xn,dn,J,Kn,Ie,Qn,Yn,Le,Zn,es,cn,Lt,ts,pn,ve,os,Se,ns,ss,hn,ne,ye,lo,Oe,as,co,rs,un,be,is,po,ls,ds,mn,De,fn,se,Pe,ho,Be,cs,uo,ps,gn,E,We,hs,ae,us,St,ms,fs,He,gs,_s,Ts,re,vs,Ot,ys,bs,Dt,Ps,ks,ws,mo,Ns,$s,Ue,_n,ie,ke,fo,Ve,Gs,go,xs,Tn,z,Je,Ms,_o,Fs,Es,Re,zs,Bt,Cs,qs,js,Xe,As,Ke,Is,Ls,Ss,q,Qe,Os,le,Ds,Wt,Bs,Ws,To,Hs,Us,Vs,we,Js,vo,Rs,Xs,Ye,vn,de,Ne,yo,Ze,Ks,bo,Qs,yn,C,et,Ys,Po,Zs,ea,tt,ta,Ht,oa,na,sa,ot,aa,nt,ra,ia,la,j,st,da,ce,ca,Ut,pa,ha,ko,ua,ma,fa,$e,ga,wo,_a,Ta,at,bn,pe,Ge,No,rt,va,$o,ya,Pn,G,it,ba,Go,Pa,ka,Vt,Jt,wa,Na,$a,W,Ga,xo,xa,Ma,Mo,Fa,Ea,Fo,za,Ca,Eo,qa,ja,Aa,lt,Ia,Rt,La,Sa,Oa,dt,Da,ct,Ba,Wa,Ha,F,pt,Ua,he,Va,Xt,Ja,Ra,zo,Xa,Ka,Qa,xe,Ya,Co,Za,er,ht,tr,qo,or,nr,ut,kn,ue,Me,jo,mt,sr,Ao,ar,wn,x,ft,rr,Io,ir,lr,gt,dr,Kt,cr,pr,hr,_t,ur,Tt,mr,fr,gr,Lo,_r,Tr,H,So,vt,vr,yr,Oo,yt,br,Pr,Do,bt,kr,wr,Bo,Pt,Nr,$r,A,kt,Gr,me,xr,Wo,Mr,Fr,Ho,Er,zr,Cr,Fe,qr,Uo,jr,Ar,wt,Nn,fe,Ee,Vo,Nt,Ir,Jo,Lr,$n,M,$t,Sr,Ro,Or,Dr,Gt,Br,Qt,Wr,Hr,Ur,xt,Vr,Mt,Jr,Rr,Xr,Xo,Kr,Qr,U,Ko,Ft,Yr,Zr,Qo,Et,ei,ti,Yo,zt,oi,ni,Zo,Ct,si,ai,I,qt,ri,ge,ii,en,li,di,tn,ci,pi,hi,ze,ui,on,mi,fi,jt,Gn;return g=new _e({}),Ae=new _e({}),Oe=new _e({}),De=new je({props:{code:`from transformers import GPTNeoForCausalLM, GPT2Tokenizer

model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

prompt = (
    "In a shocking finding, scientists discovered a herd of unicorns living in a remote, "
    "previously unexplored valley, in the Andes Mountains. Even more surprising to the "
    "researchers was the fact that the unicorns spoke perfect English."
)

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoForCausalLM, GPT2Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;In a shocking finding, scientists discovered a herd of unicorns living in a remote, &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;previously unexplored valley, in the Andes Mountains. Even more surprising to the &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;researchers was the fact that the unicorns spoke perfect English.&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    do_sample=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    temperature=<span class="hljs-number">0.9</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_text = tokenizer.batch_decode(gen_tokens)[<span class="hljs-number">0</span>]`}}),Be=new _e({}),We=new V({props:{name:"class transformers.GPTNeoConfig",anchor:"transformers.GPTNeoConfig",parameters:[{name:"vocab_size",val:" = 50257"},{name:"max_position_embeddings",val:" = 2048"},{name:"hidden_size",val:" = 2048"},{name:"num_layers",val:" = 24"},{name:"attention_types",val:" = [[['global', 'local'], 12]]"},{name:"num_heads",val:" = 16"},{name:"intermediate_size",val:" = None"},{name:"window_size",val:" = 256"},{name:"activation_function",val:" = 'gelu_new'"},{name:"resid_dropout",val:" = 0.0"},{name:"embed_dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"summary_type",val:" = 'cls_index'"},{name:"summary_use_proj",val:" = True"},{name:"summary_activation",val:" = None"},{name:"summary_proj_to_labels",val:" = True"},{name:"summary_first_dropout",val:" = 0.1"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 50256"},{name:"eos_token_id",val:" = 50256"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/configuration_gpt_neo.py#L34",parametersDescription:[{anchor:"transformers.GPTNeoConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50257) &#x2014;
Vocabulary size of the GPT Neo model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>. Vocabulary size of the model. Defines the different
tokens that can be represented by the <em>inputs_ids</em> passed to the forward method of <a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoConfig.attention_types",description:`<strong>attention_types</strong> (<code>List</code>, <em>optional</em>, defaults to <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code>) &#x2014;
The type of attention for each layer in a <code>List</code> of the following format <code>[[[&quot;attention_type&quot;], num_layerss]]</code> e.g. for a 24 layer model <code>[[[&quot;global&quot;], 24]]</code> or <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code> Choose the
value of <code>attention_type</code> from <code>[&quot;global&quot;, &quot;local&quot;]</code>`,name:"attention_types"},{anchor:"transformers.GPTNeoConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.GPTNeoConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.GPTNeoConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GPTNeoConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.GPTNeoConfig.embed_dropout",description:`<strong>embed_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"embed_dropout"},{anchor:"transformers.GPTNeoConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.GPTNeoConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.GPTNeoConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}]}}),Ue=new je({props:{code:`from transformers import GPTNeoModel, GPTNeoConfig

# Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration
configuration = GPTNeoConfig()

# Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration
model = GPTNeoModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoModel, GPTNeoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GPTNeoConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Ve=new _e({}),Je=new V({props:{name:"class transformers.GPTNeoModel",anchor:"transformers.GPTNeoModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L475",parametersDescription:[{anchor:"transformers.GPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Qe=new V({props:{name:"forward",anchor:"transformers.GPTNeoModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L496",parametersDescription:[{anchor:"transformers.GPTNeoModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16345/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new rn({props:{$$slots:{default:[td]},$$scope:{ctx:B}}}),Ye=new je({props:{code:`from transformers import GPT2Tokenizer, GPTNeoModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ze=new _e({}),et=new V({props:{name:"class transformers.GPTNeoForCausalLM",anchor:"transformers.GPTNeoForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L663",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),st=new V({props:{name:"forward",anchor:"transformers.GPTNeoForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L713",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16345/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to <code>-100</code>
are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),$e=new rn({props:{$$slots:{default:[od]},$$scope:{ctx:B}}}),at=new je({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),rt=new _e({}),it=new V({props:{name:"class transformers.GPTNeoForSequenceClassification",anchor:"transformers.GPTNeoForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L816",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),pt=new V({props:{name:"forward",anchor:"transformers.GPTNeoForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L828",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be passed as
<code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which have
their past given to this model should not be passed as <code>input_ids</code> as they have already been computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16345/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.modeling_outputs.SequenceClassifierOutputWithPast</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),xe=new rn({props:{$$slots:{default:[nd]},$$scope:{ctx:B}}}),ht=new je({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification

torch.manual_seed(0)
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),ut=new je({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification

torch.manual_seed(0)
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = GPTNeoForSequenceClassification.from_pretrained("EleutherAI/gpt-neo-1.3B", problem_type="multi_label_classification", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),mt=new _e({}),ft=new V({props:{name:"class transformers.FlaxGPTNeoModel",anchor:"transformers.FlaxGPTNeoModel",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L581",parametersDescription:[{anchor:"transformers.FlaxGPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),kt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L391",parametersDescription:[{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16345/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Fe=new rn({props:{$$slots:{default:[sd]},$$scope:{ctx:B}}}),wt=new je({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoModel

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = FlaxGPTNeoModel.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoModel.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Nt=new _e({}),$t=new V({props:{name:"class transformers.FlaxGPTNeoForCausalLM",anchor:"transformers.FlaxGPTNeoForCausalLM",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L646",parametersDescription:[{anchor:"transformers.FlaxGPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoForCausalLM.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),qt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16345/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L391",parametersDescription:[{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See <a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_16345/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16345/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_16345/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ze=new rn({props:{$$slots:{default:[ad]},$$scope:{ctx:B}}}),jt=new je({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")
model = FlaxGPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")

inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
outputs = model(**inputs)

# retrieve logts for next token
next_token_logits = outputs.logits[:, -1]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`}}),{c(){p=n("meta"),w=d(),m=n("h1"),k=n("a"),N=n("span"),_(g.$$.fragment),f=d(),$=n("span"),Jn=r("GPT Neo"),ln=d(),oe=n("h2"),Te=n("a"),ro=n("span"),_(Ae.$$.fragment),Rn=d(),io=n("span"),Xn=r("Overview"),dn=d(),J=n("p"),Kn=r("The GPTNeo model was released in the "),Ie=n("a"),Qn=r("EleutherAI/gpt-neo"),Yn=r(` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Le=n("a"),Zn=r("Pile"),es=r(" dataset."),cn=d(),Lt=n("p"),ts=r(`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),pn=d(),ve=n("p"),os=r("This model was contributed by "),Se=n("a"),ns=r("valhalla"),ss=r("."),hn=d(),ne=n("h3"),ye=n("a"),lo=n("span"),_(Oe.$$.fragment),as=d(),co=n("span"),rs=r("Generation"),un=d(),be=n("p"),is=r("The "),po=n("code"),ls=r("generate()"),ds=r(" method can be used to generate text using GPT Neo model."),mn=d(),_(De.$$.fragment),fn=d(),se=n("h2"),Pe=n("a"),ho=n("span"),_(Be.$$.fragment),cs=d(),uo=n("span"),ps=r("GPTNeoConfig"),gn=d(),E=n("div"),_(We.$$.fragment),hs=d(),ae=n("p"),us=r("This is the configuration class to store the configuration of a "),St=n("a"),ms=r("GPTNeoModel"),fs=r(`. It is used to instantiate a GPT
Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the GPTNeo
`),He=n("a"),gs=r("gpt-neo-1.3B"),_s=r(" architecture."),Ts=d(),re=n("p"),vs=r("Configuration objects inherit from "),Ot=n("a"),ys=r("PretrainedConfig"),bs=r(` and can be used to control the model outputs. Read the
documentation from `),Dt=n("a"),Ps=r("PretrainedConfig"),ks=r(" for more information."),ws=d(),mo=n("p"),Ns=r("Example:"),$s=d(),_(Ue.$$.fragment),_n=d(),ie=n("h2"),ke=n("a"),fo=n("span"),_(Ve.$$.fragment),Gs=d(),go=n("span"),xs=r("GPTNeoModel"),Tn=d(),z=n("div"),_(Je.$$.fragment),Ms=d(),_o=n("p"),Fs=r("The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),Es=d(),Re=n("p"),zs=r("This model inherits from "),Bt=n("a"),Cs=r("PreTrainedModel"),qs=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),js=d(),Xe=n("p"),As=r("This model is also a PyTorch "),Ke=n("a"),Is=r("torch.nn.Module"),Ls=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ss=d(),q=n("div"),_(Qe.$$.fragment),Os=d(),le=n("p"),Ds=r("The "),Wt=n("a"),Bs=r("GPTNeoModel"),Ws=r(" forward method, overrides the "),To=n("code"),Hs=r("__call__"),Us=r(" special method."),Vs=d(),_(we.$$.fragment),Js=d(),vo=n("p"),Rs=r("Example:"),Xs=d(),_(Ye.$$.fragment),vn=d(),de=n("h2"),Ne=n("a"),yo=n("span"),_(Ze.$$.fragment),Ks=d(),bo=n("span"),Qs=r("GPTNeoForCausalLM"),yn=d(),C=n("div"),_(et.$$.fragment),Ys=d(),Po=n("p"),Zs=r(`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),ea=d(),tt=n("p"),ta=r("This model inherits from "),Ht=n("a"),oa=r("PreTrainedModel"),na=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sa=d(),ot=n("p"),aa=r("This model is also a PyTorch "),nt=n("a"),ra=r("torch.nn.Module"),ia=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),la=d(),j=n("div"),_(st.$$.fragment),da=d(),ce=n("p"),ca=r("The "),Ut=n("a"),pa=r("GPTNeoForCausalLM"),ha=r(" forward method, overrides the "),ko=n("code"),ua=r("__call__"),ma=r(" special method."),fa=d(),_($e.$$.fragment),ga=d(),wo=n("p"),_a=r("Example:"),Ta=d(),_(at.$$.fragment),bn=d(),pe=n("h2"),Ge=n("a"),No=n("span"),_(rt.$$.fragment),va=d(),$o=n("span"),ya=r("GPTNeoForSequenceClassification"),Pn=d(),G=n("div"),_(it.$$.fragment),ba=d(),Go=n("p"),Pa=r("The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),ka=d(),Vt=n("p"),Jt=n("a"),wa=r("GPTNeoForSequenceClassification"),Na=r(` uses the last token in order to do the classification, as other causal models
(e.g. GPT-1) do.`),$a=d(),W=n("p"),Ga=r(`Since it does classification on the last token, it requires to know the position of the last token. If a
`),xo=n("code"),xa=r("pad_token_id"),Ma=r(` is defined in the configuration, it finds the last token that is not a padding token in each row. If
no `),Mo=n("code"),Fa=r("pad_token_id"),Ea=r(` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when `),Fo=n("code"),za=r("inputs_embeds"),Ca=r(" are passed instead of "),Eo=n("code"),qa=r("input_ids"),ja=r(`, it does the same (take the last value in
each row of the batch).`),Aa=d(),lt=n("p"),Ia=r("This model inherits from "),Rt=n("a"),La=r("PreTrainedModel"),Sa=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Oa=d(),dt=n("p"),Da=r("This model is also a PyTorch "),ct=n("a"),Ba=r("torch.nn.Module"),Wa=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ha=d(),F=n("div"),_(pt.$$.fragment),Ua=d(),he=n("p"),Va=r("The "),Xt=n("a"),Ja=r("GPTNeoForSequenceClassification"),Ra=r(" forward method, overrides the "),zo=n("code"),Xa=r("__call__"),Ka=r(" special method."),Qa=d(),_(xe.$$.fragment),Ya=d(),Co=n("p"),Za=r("Example of single-label classification:"),er=d(),_(ht.$$.fragment),tr=d(),qo=n("p"),or=r("Example of multi-label classification:"),nr=d(),_(ut.$$.fragment),kn=d(),ue=n("h2"),Me=n("a"),jo=n("span"),_(mt.$$.fragment),sr=d(),Ao=n("span"),ar=r("FlaxGPTNeoModel"),wn=d(),x=n("div"),_(ft.$$.fragment),rr=d(),Io=n("p"),ir=r("The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),lr=d(),gt=n("p"),dr=r("This model inherits from "),Kt=n("a"),cr=r("FlaxPreTrainedModel"),pr=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hr=d(),_t=n("p"),ur=r(`This model is also a Flax Linen
`),Tt=n("a"),mr=r("flax.nn.Module"),fr=r(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),gr=d(),Lo=n("p"),_r=r("Finally, this model supports inherent JAX features such as:"),Tr=d(),H=n("ul"),So=n("li"),vt=n("a"),vr=r("Just-In-Time (JIT) compilation"),yr=d(),Oo=n("li"),yt=n("a"),br=r("Automatic Differentiation"),Pr=d(),Do=n("li"),bt=n("a"),kr=r("Vectorization"),wr=d(),Bo=n("li"),Pt=n("a"),Nr=r("Parallelization"),$r=d(),A=n("div"),_(kt.$$.fragment),Gr=d(),me=n("p"),xr=r("The "),Wo=n("code"),Mr=r("FlaxGPTNeoPreTrainedModel"),Fr=r("forward method, overrides the "),Ho=n("code"),Er=r("__call__"),zr=r(" special method."),Cr=d(),_(Fe.$$.fragment),qr=d(),Uo=n("p"),jr=r("Example:"),Ar=d(),_(wt.$$.fragment),Nn=d(),fe=n("h2"),Ee=n("a"),Vo=n("span"),_(Nt.$$.fragment),Ir=d(),Jo=n("span"),Lr=r("FlaxGPTNeoForCausalLM"),$n=d(),M=n("div"),_($t.$$.fragment),Sr=d(),Ro=n("p"),Or=r(`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Dr=d(),Gt=n("p"),Br=r("This model inherits from "),Qt=n("a"),Wr=r("FlaxPreTrainedModel"),Hr=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ur=d(),xt=n("p"),Vr=r(`This model is also a Flax Linen
`),Mt=n("a"),Jr=r("flax.nn.Module"),Rr=r(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Xr=d(),Xo=n("p"),Kr=r("Finally, this model supports inherent JAX features such as:"),Qr=d(),U=n("ul"),Ko=n("li"),Ft=n("a"),Yr=r("Just-In-Time (JIT) compilation"),Zr=d(),Qo=n("li"),Et=n("a"),ei=r("Automatic Differentiation"),ti=d(),Yo=n("li"),zt=n("a"),oi=r("Vectorization"),ni=d(),Zo=n("li"),Ct=n("a"),si=r("Parallelization"),ai=d(),I=n("div"),_(qt.$$.fragment),ri=d(),ge=n("p"),ii=r("The "),en=n("code"),li=r("FlaxGPTNeoPreTrainedModel"),di=r("forward method, overrides the "),tn=n("code"),ci=r("__call__"),pi=r(" special method."),hi=d(),_(ze.$$.fragment),ui=d(),on=n("p"),mi=r("Example:"),fi=d(),_(jt.$$.fragment),this.h()},l(t){const h=ed('[data-svelte="svelte-1phssyn"]',document.head);p=s(h,"META",{name:!0,content:!0}),h.forEach(o),w=c(t),m=s(t,"H1",{class:!0});var At=a(m);k=s(At,"A",{id:!0,class:!0,href:!0});var nn=a(k);N=s(nn,"SPAN",{});var sn=a(N);T(g.$$.fragment,sn),sn.forEach(o),nn.forEach(o),f=c(At),$=s(At,"SPAN",{});var an=a($);Jn=i(an,"GPT Neo"),an.forEach(o),At.forEach(o),ln=c(t),oe=s(t,"H2",{class:!0});var It=a(oe);Te=s(It,"A",{id:!0,class:!0,href:!0});var _i=a(Te);ro=s(_i,"SPAN",{});var Ti=a(ro);T(Ae.$$.fragment,Ti),Ti.forEach(o),_i.forEach(o),Rn=c(It),io=s(It,"SPAN",{});var vi=a(io);Xn=i(vi,"Overview"),vi.forEach(o),It.forEach(o),dn=c(t),J=s(t,"P",{});var Yt=a(J);Kn=i(Yt,"The GPTNeo model was released in the "),Ie=s(Yt,"A",{href:!0,rel:!0});var yi=a(Ie);Qn=i(yi,"EleutherAI/gpt-neo"),yi.forEach(o),Yn=i(Yt,` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Le=s(Yt,"A",{href:!0,rel:!0});var bi=a(Le);Zn=i(bi,"Pile"),bi.forEach(o),es=i(Yt," dataset."),Yt.forEach(o),cn=c(t),Lt=s(t,"P",{});var Pi=a(Lt);ts=i(Pi,`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),Pi.forEach(o),pn=c(t),ve=s(t,"P",{});var xn=a(ve);os=i(xn,"This model was contributed by "),Se=s(xn,"A",{href:!0,rel:!0});var ki=a(Se);ns=i(ki,"valhalla"),ki.forEach(o),ss=i(xn,"."),xn.forEach(o),hn=c(t),ne=s(t,"H3",{class:!0});var Mn=a(ne);ye=s(Mn,"A",{id:!0,class:!0,href:!0});var wi=a(ye);lo=s(wi,"SPAN",{});var Ni=a(lo);T(Oe.$$.fragment,Ni),Ni.forEach(o),wi.forEach(o),as=c(Mn),co=s(Mn,"SPAN",{});var $i=a(co);rs=i($i,"Generation"),$i.forEach(o),Mn.forEach(o),un=c(t),be=s(t,"P",{});var Fn=a(be);is=i(Fn,"The "),po=s(Fn,"CODE",{});var Gi=a(po);ls=i(Gi,"generate()"),Gi.forEach(o),ds=i(Fn," method can be used to generate text using GPT Neo model."),Fn.forEach(o),mn=c(t),T(De.$$.fragment,t),fn=c(t),se=s(t,"H2",{class:!0});var En=a(se);Pe=s(En,"A",{id:!0,class:!0,href:!0});var xi=a(Pe);ho=s(xi,"SPAN",{});var Mi=a(ho);T(Be.$$.fragment,Mi),Mi.forEach(o),xi.forEach(o),cs=c(En),uo=s(En,"SPAN",{});var Fi=a(uo);ps=i(Fi,"GPTNeoConfig"),Fi.forEach(o),En.forEach(o),gn=c(t),E=s(t,"DIV",{class:!0});var R=a(E);T(We.$$.fragment,R),hs=c(R),ae=s(R,"P",{});var Zt=a(ae);us=i(Zt,"This is the configuration class to store the configuration of a "),St=s(Zt,"A",{href:!0});var Ei=a(St);ms=i(Ei,"GPTNeoModel"),Ei.forEach(o),fs=i(Zt,`. It is used to instantiate a GPT
Neo model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the GPTNeo
`),He=s(Zt,"A",{href:!0,rel:!0});var zi=a(He);gs=i(zi,"gpt-neo-1.3B"),zi.forEach(o),_s=i(Zt," architecture."),Zt.forEach(o),Ts=c(R),re=s(R,"P",{});var eo=a(re);vs=i(eo,"Configuration objects inherit from "),Ot=s(eo,"A",{href:!0});var Ci=a(Ot);ys=i(Ci,"PretrainedConfig"),Ci.forEach(o),bs=i(eo,` and can be used to control the model outputs. Read the
documentation from `),Dt=s(eo,"A",{href:!0});var qi=a(Dt);Ps=i(qi,"PretrainedConfig"),qi.forEach(o),ks=i(eo," for more information."),eo.forEach(o),ws=c(R),mo=s(R,"P",{});var ji=a(mo);Ns=i(ji,"Example:"),ji.forEach(o),$s=c(R),T(Ue.$$.fragment,R),R.forEach(o),_n=c(t),ie=s(t,"H2",{class:!0});var zn=a(ie);ke=s(zn,"A",{id:!0,class:!0,href:!0});var Ai=a(ke);fo=s(Ai,"SPAN",{});var Ii=a(fo);T(Ve.$$.fragment,Ii),Ii.forEach(o),Ai.forEach(o),Gs=c(zn),go=s(zn,"SPAN",{});var Li=a(go);xs=i(Li,"GPTNeoModel"),Li.forEach(o),zn.forEach(o),Tn=c(t),z=s(t,"DIV",{class:!0});var X=a(z);T(Je.$$.fragment,X),Ms=c(X),_o=s(X,"P",{});var Si=a(_o);Fs=i(Si,"The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),Si.forEach(o),Es=c(X),Re=s(X,"P",{});var Cn=a(Re);zs=i(Cn,"This model inherits from "),Bt=s(Cn,"A",{href:!0});var Oi=a(Bt);Cs=i(Oi,"PreTrainedModel"),Oi.forEach(o),qs=i(Cn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cn.forEach(o),js=c(X),Xe=s(X,"P",{});var qn=a(Xe);As=i(qn,"This model is also a PyTorch "),Ke=s(qn,"A",{href:!0,rel:!0});var Di=a(Ke);Is=i(Di,"torch.nn.Module"),Di.forEach(o),Ls=i(qn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),qn.forEach(o),Ss=c(X),q=s(X,"DIV",{class:!0});var K=a(q);T(Qe.$$.fragment,K),Os=c(K),le=s(K,"P",{});var to=a(le);Ds=i(to,"The "),Wt=s(to,"A",{href:!0});var Bi=a(Wt);Bs=i(Bi,"GPTNeoModel"),Bi.forEach(o),Ws=i(to," forward method, overrides the "),To=s(to,"CODE",{});var Wi=a(To);Hs=i(Wi,"__call__"),Wi.forEach(o),Us=i(to," special method."),to.forEach(o),Vs=c(K),T(we.$$.fragment,K),Js=c(K),vo=s(K,"P",{});var Hi=a(vo);Rs=i(Hi,"Example:"),Hi.forEach(o),Xs=c(K),T(Ye.$$.fragment,K),K.forEach(o),X.forEach(o),vn=c(t),de=s(t,"H2",{class:!0});var jn=a(de);Ne=s(jn,"A",{id:!0,class:!0,href:!0});var Ui=a(Ne);yo=s(Ui,"SPAN",{});var Vi=a(yo);T(Ze.$$.fragment,Vi),Vi.forEach(o),Ui.forEach(o),Ks=c(jn),bo=s(jn,"SPAN",{});var Ji=a(bo);Qs=i(Ji,"GPTNeoForCausalLM"),Ji.forEach(o),jn.forEach(o),yn=c(t),C=s(t,"DIV",{class:!0});var Q=a(C);T(et.$$.fragment,Q),Ys=c(Q),Po=s(Q,"P",{});var Ri=a(Po);Zs=i(Ri,`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Ri.forEach(o),ea=c(Q),tt=s(Q,"P",{});var An=a(tt);ta=i(An,"This model inherits from "),Ht=s(An,"A",{href:!0});var Xi=a(Ht);oa=i(Xi,"PreTrainedModel"),Xi.forEach(o),na=i(An,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),An.forEach(o),sa=c(Q),ot=s(Q,"P",{});var In=a(ot);aa=i(In,"This model is also a PyTorch "),nt=s(In,"A",{href:!0,rel:!0});var Ki=a(nt);ra=i(Ki,"torch.nn.Module"),Ki.forEach(o),ia=i(In,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),In.forEach(o),la=c(Q),j=s(Q,"DIV",{class:!0});var Y=a(j);T(st.$$.fragment,Y),da=c(Y),ce=s(Y,"P",{});var oo=a(ce);ca=i(oo,"The "),Ut=s(oo,"A",{href:!0});var Qi=a(Ut);pa=i(Qi,"GPTNeoForCausalLM"),Qi.forEach(o),ha=i(oo," forward method, overrides the "),ko=s(oo,"CODE",{});var Yi=a(ko);ua=i(Yi,"__call__"),Yi.forEach(o),ma=i(oo," special method."),oo.forEach(o),fa=c(Y),T($e.$$.fragment,Y),ga=c(Y),wo=s(Y,"P",{});var Zi=a(wo);_a=i(Zi,"Example:"),Zi.forEach(o),Ta=c(Y),T(at.$$.fragment,Y),Y.forEach(o),Q.forEach(o),bn=c(t),pe=s(t,"H2",{class:!0});var Ln=a(pe);Ge=s(Ln,"A",{id:!0,class:!0,href:!0});var el=a(Ge);No=s(el,"SPAN",{});var tl=a(No);T(rt.$$.fragment,tl),tl.forEach(o),el.forEach(o),va=c(Ln),$o=s(Ln,"SPAN",{});var ol=a($o);ya=i(ol,"GPTNeoForSequenceClassification"),ol.forEach(o),Ln.forEach(o),Pn=c(t),G=s(t,"DIV",{class:!0});var L=a(G);T(it.$$.fragment,L),ba=c(L),Go=s(L,"P",{});var nl=a(Go);Pa=i(nl,"The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),nl.forEach(o),ka=c(L),Vt=s(L,"P",{});var gi=a(Vt);Jt=s(gi,"A",{href:!0});var sl=a(Jt);wa=i(sl,"GPTNeoForSequenceClassification"),sl.forEach(o),Na=i(gi,` uses the last token in order to do the classification, as other causal models
(e.g. GPT-1) do.`),gi.forEach(o),$a=c(L),W=s(L,"P",{});var Z=a(W);Ga=i(Z,`Since it does classification on the last token, it requires to know the position of the last token. If a
`),xo=s(Z,"CODE",{});var al=a(xo);xa=i(al,"pad_token_id"),al.forEach(o),Ma=i(Z,` is defined in the configuration, it finds the last token that is not a padding token in each row. If
no `),Mo=s(Z,"CODE",{});var rl=a(Mo);Fa=i(rl,"pad_token_id"),rl.forEach(o),Ea=i(Z,` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when `),Fo=s(Z,"CODE",{});var il=a(Fo);za=i(il,"inputs_embeds"),il.forEach(o),Ca=i(Z," are passed instead of "),Eo=s(Z,"CODE",{});var ll=a(Eo);qa=i(ll,"input_ids"),ll.forEach(o),ja=i(Z,`, it does the same (take the last value in
each row of the batch).`),Z.forEach(o),Aa=c(L),lt=s(L,"P",{});var Sn=a(lt);Ia=i(Sn,"This model inherits from "),Rt=s(Sn,"A",{href:!0});var dl=a(Rt);La=i(dl,"PreTrainedModel"),dl.forEach(o),Sa=i(Sn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sn.forEach(o),Oa=c(L),dt=s(L,"P",{});var On=a(dt);Da=i(On,"This model is also a PyTorch "),ct=s(On,"A",{href:!0,rel:!0});var cl=a(ct);Ba=i(cl,"torch.nn.Module"),cl.forEach(o),Wa=i(On,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),On.forEach(o),Ha=c(L),F=s(L,"DIV",{class:!0});var S=a(F);T(pt.$$.fragment,S),Ua=c(S),he=s(S,"P",{});var no=a(he);Va=i(no,"The "),Xt=s(no,"A",{href:!0});var pl=a(Xt);Ja=i(pl,"GPTNeoForSequenceClassification"),pl.forEach(o),Ra=i(no," forward method, overrides the "),zo=s(no,"CODE",{});var hl=a(zo);Xa=i(hl,"__call__"),hl.forEach(o),Ka=i(no," special method."),no.forEach(o),Qa=c(S),T(xe.$$.fragment,S),Ya=c(S),Co=s(S,"P",{});var ul=a(Co);Za=i(ul,"Example of single-label classification:"),ul.forEach(o),er=c(S),T(ht.$$.fragment,S),tr=c(S),qo=s(S,"P",{});var ml=a(qo);or=i(ml,"Example of multi-label classification:"),ml.forEach(o),nr=c(S),T(ut.$$.fragment,S),S.forEach(o),L.forEach(o),kn=c(t),ue=s(t,"H2",{class:!0});var Dn=a(ue);Me=s(Dn,"A",{id:!0,class:!0,href:!0});var fl=a(Me);jo=s(fl,"SPAN",{});var gl=a(jo);T(mt.$$.fragment,gl),gl.forEach(o),fl.forEach(o),sr=c(Dn),Ao=s(Dn,"SPAN",{});var _l=a(Ao);ar=i(_l,"FlaxGPTNeoModel"),_l.forEach(o),Dn.forEach(o),wn=c(t),x=s(t,"DIV",{class:!0});var O=a(x);T(ft.$$.fragment,O),rr=c(O),Io=s(O,"P",{});var Tl=a(Io);ir=i(Tl,"The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),Tl.forEach(o),lr=c(O),gt=s(O,"P",{});var Bn=a(gt);dr=i(Bn,"This model inherits from "),Kt=s(Bn,"A",{href:!0});var vl=a(Kt);cr=i(vl,"FlaxPreTrainedModel"),vl.forEach(o),pr=i(Bn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bn.forEach(o),hr=c(O),_t=s(O,"P",{});var Wn=a(_t);ur=i(Wn,`This model is also a Flax Linen
`),Tt=s(Wn,"A",{href:!0,rel:!0});var yl=a(Tt);mr=i(yl,"flax.nn.Module"),yl.forEach(o),fr=i(Wn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Wn.forEach(o),gr=c(O),Lo=s(O,"P",{});var bl=a(Lo);_r=i(bl,"Finally, this model supports inherent JAX features such as:"),bl.forEach(o),Tr=c(O),H=s(O,"UL",{});var Ce=a(H);So=s(Ce,"LI",{});var Pl=a(So);vt=s(Pl,"A",{href:!0,rel:!0});var kl=a(vt);vr=i(kl,"Just-In-Time (JIT) compilation"),kl.forEach(o),Pl.forEach(o),yr=c(Ce),Oo=s(Ce,"LI",{});var wl=a(Oo);yt=s(wl,"A",{href:!0,rel:!0});var Nl=a(yt);br=i(Nl,"Automatic Differentiation"),Nl.forEach(o),wl.forEach(o),Pr=c(Ce),Do=s(Ce,"LI",{});var $l=a(Do);bt=s($l,"A",{href:!0,rel:!0});var Gl=a(bt);kr=i(Gl,"Vectorization"),Gl.forEach(o),$l.forEach(o),wr=c(Ce),Bo=s(Ce,"LI",{});var xl=a(Bo);Pt=s(xl,"A",{href:!0,rel:!0});var Ml=a(Pt);Nr=i(Ml,"Parallelization"),Ml.forEach(o),xl.forEach(o),Ce.forEach(o),$r=c(O),A=s(O,"DIV",{class:!0});var ee=a(A);T(kt.$$.fragment,ee),Gr=c(ee),me=s(ee,"P",{});var so=a(me);xr=i(so,"The "),Wo=s(so,"CODE",{});var Fl=a(Wo);Mr=i(Fl,"FlaxGPTNeoPreTrainedModel"),Fl.forEach(o),Fr=i(so,"forward method, overrides the "),Ho=s(so,"CODE",{});var El=a(Ho);Er=i(El,"__call__"),El.forEach(o),zr=i(so," special method."),so.forEach(o),Cr=c(ee),T(Fe.$$.fragment,ee),qr=c(ee),Uo=s(ee,"P",{});var zl=a(Uo);jr=i(zl,"Example:"),zl.forEach(o),Ar=c(ee),T(wt.$$.fragment,ee),ee.forEach(o),O.forEach(o),Nn=c(t),fe=s(t,"H2",{class:!0});var Hn=a(fe);Ee=s(Hn,"A",{id:!0,class:!0,href:!0});var Cl=a(Ee);Vo=s(Cl,"SPAN",{});var ql=a(Vo);T(Nt.$$.fragment,ql),ql.forEach(o),Cl.forEach(o),Ir=c(Hn),Jo=s(Hn,"SPAN",{});var jl=a(Jo);Lr=i(jl,"FlaxGPTNeoForCausalLM"),jl.forEach(o),Hn.forEach(o),$n=c(t),M=s(t,"DIV",{class:!0});var D=a(M);T($t.$$.fragment,D),Sr=c(D),Ro=s(D,"P",{});var Al=a(Ro);Or=i(Al,`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Al.forEach(o),Dr=c(D),Gt=s(D,"P",{});var Un=a(Gt);Br=i(Un,"This model inherits from "),Qt=s(Un,"A",{href:!0});var Il=a(Qt);Wr=i(Il,"FlaxPreTrainedModel"),Il.forEach(o),Hr=i(Un,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Un.forEach(o),Ur=c(D),xt=s(D,"P",{});var Vn=a(xt);Vr=i(Vn,`This model is also a Flax Linen
`),Mt=s(Vn,"A",{href:!0,rel:!0});var Ll=a(Mt);Jr=i(Ll,"flax.nn.Module"),Ll.forEach(o),Rr=i(Vn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Vn.forEach(o),Xr=c(D),Xo=s(D,"P",{});var Sl=a(Xo);Kr=i(Sl,"Finally, this model supports inherent JAX features such as:"),Sl.forEach(o),Qr=c(D),U=s(D,"UL",{});var qe=a(U);Ko=s(qe,"LI",{});var Ol=a(Ko);Ft=s(Ol,"A",{href:!0,rel:!0});var Dl=a(Ft);Yr=i(Dl,"Just-In-Time (JIT) compilation"),Dl.forEach(o),Ol.forEach(o),Zr=c(qe),Qo=s(qe,"LI",{});var Bl=a(Qo);Et=s(Bl,"A",{href:!0,rel:!0});var Wl=a(Et);ei=i(Wl,"Automatic Differentiation"),Wl.forEach(o),Bl.forEach(o),ti=c(qe),Yo=s(qe,"LI",{});var Hl=a(Yo);zt=s(Hl,"A",{href:!0,rel:!0});var Ul=a(zt);oi=i(Ul,"Vectorization"),Ul.forEach(o),Hl.forEach(o),ni=c(qe),Zo=s(qe,"LI",{});var Vl=a(Zo);Ct=s(Vl,"A",{href:!0,rel:!0});var Jl=a(Ct);si=i(Jl,"Parallelization"),Jl.forEach(o),Vl.forEach(o),qe.forEach(o),ai=c(D),I=s(D,"DIV",{class:!0});var te=a(I);T(qt.$$.fragment,te),ri=c(te),ge=s(te,"P",{});var ao=a(ge);ii=i(ao,"The "),en=s(ao,"CODE",{});var Rl=a(en);li=i(Rl,"FlaxGPTNeoPreTrainedModel"),Rl.forEach(o),di=i(ao,"forward method, overrides the "),tn=s(ao,"CODE",{});var Xl=a(tn);ci=i(Xl,"__call__"),Xl.forEach(o),pi=i(ao," special method."),ao.forEach(o),hi=c(te),T(ze.$$.fragment,te),ui=c(te),on=s(te,"P",{});var Kl=a(on);mi=i(Kl,"Example:"),Kl.forEach(o),fi=c(te),T(jt.$$.fragment,te),te.forEach(o),D.forEach(o),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(id)),l(k,"id","gpt-neo"),l(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(k,"href","#gpt-neo"),l(m,"class","relative group"),l(Te,"id","overview"),l(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Te,"href","#overview"),l(oe,"class","relative group"),l(Ie,"href","https://github.com/EleutherAI/gpt-neo"),l(Ie,"rel","nofollow"),l(Le,"href","https://pile.eleuther.ai/"),l(Le,"rel","nofollow"),l(Se,"href","https://huggingface.co/valhalla"),l(Se,"rel","nofollow"),l(ye,"id","generation"),l(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ye,"href","#generation"),l(ne,"class","relative group"),l(Pe,"id","transformers.GPTNeoConfig"),l(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Pe,"href","#transformers.GPTNeoConfig"),l(se,"class","relative group"),l(St,"href","/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(He,"href","https://huggingface.co/EleutherAI/gpt-neo-1.3B"),l(He,"rel","nofollow"),l(Ot,"href","/docs/transformers/pr_16345/en/main_classes/configuration#transformers.PretrainedConfig"),l(Dt,"href","/docs/transformers/pr_16345/en/main_classes/configuration#transformers.PretrainedConfig"),l(E,"class","docstring"),l(ke,"id","transformers.GPTNeoModel"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.GPTNeoModel"),l(ie,"class","relative group"),l(Bt,"href","/docs/transformers/pr_16345/en/main_classes/model#transformers.PreTrainedModel"),l(Ke,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ke,"rel","nofollow"),l(Wt,"href","/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(q,"class","docstring"),l(z,"class","docstring"),l(Ne,"id","transformers.GPTNeoForCausalLM"),l(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ne,"href","#transformers.GPTNeoForCausalLM"),l(de,"class","relative group"),l(Ht,"href","/docs/transformers/pr_16345/en/main_classes/model#transformers.PreTrainedModel"),l(nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(nt,"rel","nofollow"),l(Ut,"href","/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),l(j,"class","docstring"),l(C,"class","docstring"),l(Ge,"id","transformers.GPTNeoForSequenceClassification"),l(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ge,"href","#transformers.GPTNeoForSequenceClassification"),l(pe,"class","relative group"),l(Jt,"href","/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(Rt,"href","/docs/transformers/pr_16345/en/main_classes/model#transformers.PreTrainedModel"),l(ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(ct,"rel","nofollow"),l(Xt,"href","/docs/transformers/pr_16345/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(F,"class","docstring"),l(G,"class","docstring"),l(Me,"id","transformers.FlaxGPTNeoModel"),l(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Me,"href","#transformers.FlaxGPTNeoModel"),l(ue,"class","relative group"),l(Kt,"href","/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(Tt,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(Tt,"rel","nofollow"),l(vt,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(vt,"rel","nofollow"),l(yt,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(yt,"rel","nofollow"),l(bt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(bt,"rel","nofollow"),l(Pt,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(Pt,"rel","nofollow"),l(A,"class","docstring"),l(x,"class","docstring"),l(Ee,"id","transformers.FlaxGPTNeoForCausalLM"),l(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ee,"href","#transformers.FlaxGPTNeoForCausalLM"),l(fe,"class","relative group"),l(Qt,"href","/docs/transformers/pr_16345/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(Mt,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(Mt,"rel","nofollow"),l(Ft,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(Ft,"rel","nofollow"),l(Et,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(Et,"rel","nofollow"),l(zt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(zt,"rel","nofollow"),l(Ct,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(Ct,"rel","nofollow"),l(I,"class","docstring"),l(M,"class","docstring")},m(t,h){e(document.head,p),u(t,w,h),u(t,m,h),e(m,k),e(k,N),v(g,N,null),e(m,f),e(m,$),e($,Jn),u(t,ln,h),u(t,oe,h),e(oe,Te),e(Te,ro),v(Ae,ro,null),e(oe,Rn),e(oe,io),e(io,Xn),u(t,dn,h),u(t,J,h),e(J,Kn),e(J,Ie),e(Ie,Qn),e(J,Yn),e(J,Le),e(Le,Zn),e(J,es),u(t,cn,h),u(t,Lt,h),e(Lt,ts),u(t,pn,h),u(t,ve,h),e(ve,os),e(ve,Se),e(Se,ns),e(ve,ss),u(t,hn,h),u(t,ne,h),e(ne,ye),e(ye,lo),v(Oe,lo,null),e(ne,as),e(ne,co),e(co,rs),u(t,un,h),u(t,be,h),e(be,is),e(be,po),e(po,ls),e(be,ds),u(t,mn,h),v(De,t,h),u(t,fn,h),u(t,se,h),e(se,Pe),e(Pe,ho),v(Be,ho,null),e(se,cs),e(se,uo),e(uo,ps),u(t,gn,h),u(t,E,h),v(We,E,null),e(E,hs),e(E,ae),e(ae,us),e(ae,St),e(St,ms),e(ae,fs),e(ae,He),e(He,gs),e(ae,_s),e(E,Ts),e(E,re),e(re,vs),e(re,Ot),e(Ot,ys),e(re,bs),e(re,Dt),e(Dt,Ps),e(re,ks),e(E,ws),e(E,mo),e(mo,Ns),e(E,$s),v(Ue,E,null),u(t,_n,h),u(t,ie,h),e(ie,ke),e(ke,fo),v(Ve,fo,null),e(ie,Gs),e(ie,go),e(go,xs),u(t,Tn,h),u(t,z,h),v(Je,z,null),e(z,Ms),e(z,_o),e(_o,Fs),e(z,Es),e(z,Re),e(Re,zs),e(Re,Bt),e(Bt,Cs),e(Re,qs),e(z,js),e(z,Xe),e(Xe,As),e(Xe,Ke),e(Ke,Is),e(Xe,Ls),e(z,Ss),e(z,q),v(Qe,q,null),e(q,Os),e(q,le),e(le,Ds),e(le,Wt),e(Wt,Bs),e(le,Ws),e(le,To),e(To,Hs),e(le,Us),e(q,Vs),v(we,q,null),e(q,Js),e(q,vo),e(vo,Rs),e(q,Xs),v(Ye,q,null),u(t,vn,h),u(t,de,h),e(de,Ne),e(Ne,yo),v(Ze,yo,null),e(de,Ks),e(de,bo),e(bo,Qs),u(t,yn,h),u(t,C,h),v(et,C,null),e(C,Ys),e(C,Po),e(Po,Zs),e(C,ea),e(C,tt),e(tt,ta),e(tt,Ht),e(Ht,oa),e(tt,na),e(C,sa),e(C,ot),e(ot,aa),e(ot,nt),e(nt,ra),e(ot,ia),e(C,la),e(C,j),v(st,j,null),e(j,da),e(j,ce),e(ce,ca),e(ce,Ut),e(Ut,pa),e(ce,ha),e(ce,ko),e(ko,ua),e(ce,ma),e(j,fa),v($e,j,null),e(j,ga),e(j,wo),e(wo,_a),e(j,Ta),v(at,j,null),u(t,bn,h),u(t,pe,h),e(pe,Ge),e(Ge,No),v(rt,No,null),e(pe,va),e(pe,$o),e($o,ya),u(t,Pn,h),u(t,G,h),v(it,G,null),e(G,ba),e(G,Go),e(Go,Pa),e(G,ka),e(G,Vt),e(Vt,Jt),e(Jt,wa),e(Vt,Na),e(G,$a),e(G,W),e(W,Ga),e(W,xo),e(xo,xa),e(W,Ma),e(W,Mo),e(Mo,Fa),e(W,Ea),e(W,Fo),e(Fo,za),e(W,Ca),e(W,Eo),e(Eo,qa),e(W,ja),e(G,Aa),e(G,lt),e(lt,Ia),e(lt,Rt),e(Rt,La),e(lt,Sa),e(G,Oa),e(G,dt),e(dt,Da),e(dt,ct),e(ct,Ba),e(dt,Wa),e(G,Ha),e(G,F),v(pt,F,null),e(F,Ua),e(F,he),e(he,Va),e(he,Xt),e(Xt,Ja),e(he,Ra),e(he,zo),e(zo,Xa),e(he,Ka),e(F,Qa),v(xe,F,null),e(F,Ya),e(F,Co),e(Co,Za),e(F,er),v(ht,F,null),e(F,tr),e(F,qo),e(qo,or),e(F,nr),v(ut,F,null),u(t,kn,h),u(t,ue,h),e(ue,Me),e(Me,jo),v(mt,jo,null),e(ue,sr),e(ue,Ao),e(Ao,ar),u(t,wn,h),u(t,x,h),v(ft,x,null),e(x,rr),e(x,Io),e(Io,ir),e(x,lr),e(x,gt),e(gt,dr),e(gt,Kt),e(Kt,cr),e(gt,pr),e(x,hr),e(x,_t),e(_t,ur),e(_t,Tt),e(Tt,mr),e(_t,fr),e(x,gr),e(x,Lo),e(Lo,_r),e(x,Tr),e(x,H),e(H,So),e(So,vt),e(vt,vr),e(H,yr),e(H,Oo),e(Oo,yt),e(yt,br),e(H,Pr),e(H,Do),e(Do,bt),e(bt,kr),e(H,wr),e(H,Bo),e(Bo,Pt),e(Pt,Nr),e(x,$r),e(x,A),v(kt,A,null),e(A,Gr),e(A,me),e(me,xr),e(me,Wo),e(Wo,Mr),e(me,Fr),e(me,Ho),e(Ho,Er),e(me,zr),e(A,Cr),v(Fe,A,null),e(A,qr),e(A,Uo),e(Uo,jr),e(A,Ar),v(wt,A,null),u(t,Nn,h),u(t,fe,h),e(fe,Ee),e(Ee,Vo),v(Nt,Vo,null),e(fe,Ir),e(fe,Jo),e(Jo,Lr),u(t,$n,h),u(t,M,h),v($t,M,null),e(M,Sr),e(M,Ro),e(Ro,Or),e(M,Dr),e(M,Gt),e(Gt,Br),e(Gt,Qt),e(Qt,Wr),e(Gt,Hr),e(M,Ur),e(M,xt),e(xt,Vr),e(xt,Mt),e(Mt,Jr),e(xt,Rr),e(M,Xr),e(M,Xo),e(Xo,Kr),e(M,Qr),e(M,U),e(U,Ko),e(Ko,Ft),e(Ft,Yr),e(U,Zr),e(U,Qo),e(Qo,Et),e(Et,ei),e(U,ti),e(U,Yo),e(Yo,zt),e(zt,oi),e(U,ni),e(U,Zo),e(Zo,Ct),e(Ct,si),e(M,ai),e(M,I),v(qt,I,null),e(I,ri),e(I,ge),e(ge,ii),e(ge,en),e(en,li),e(ge,di),e(ge,tn),e(tn,ci),e(ge,pi),e(I,hi),v(ze,I,null),e(I,ui),e(I,on),e(on,mi),e(I,fi),v(jt,I,null),Gn=!0},p(t,[h]){const At={};h&2&&(At.$$scope={dirty:h,ctx:t}),we.$set(At);const nn={};h&2&&(nn.$$scope={dirty:h,ctx:t}),$e.$set(nn);const sn={};h&2&&(sn.$$scope={dirty:h,ctx:t}),xe.$set(sn);const an={};h&2&&(an.$$scope={dirty:h,ctx:t}),Fe.$set(an);const It={};h&2&&(It.$$scope={dirty:h,ctx:t}),ze.$set(It)},i(t){Gn||(y(g.$$.fragment,t),y(Ae.$$.fragment,t),y(Oe.$$.fragment,t),y(De.$$.fragment,t),y(Be.$$.fragment,t),y(We.$$.fragment,t),y(Ue.$$.fragment,t),y(Ve.$$.fragment,t),y(Je.$$.fragment,t),y(Qe.$$.fragment,t),y(we.$$.fragment,t),y(Ye.$$.fragment,t),y(Ze.$$.fragment,t),y(et.$$.fragment,t),y(st.$$.fragment,t),y($e.$$.fragment,t),y(at.$$.fragment,t),y(rt.$$.fragment,t),y(it.$$.fragment,t),y(pt.$$.fragment,t),y(xe.$$.fragment,t),y(ht.$$.fragment,t),y(ut.$$.fragment,t),y(mt.$$.fragment,t),y(ft.$$.fragment,t),y(kt.$$.fragment,t),y(Fe.$$.fragment,t),y(wt.$$.fragment,t),y(Nt.$$.fragment,t),y($t.$$.fragment,t),y(qt.$$.fragment,t),y(ze.$$.fragment,t),y(jt.$$.fragment,t),Gn=!0)},o(t){b(g.$$.fragment,t),b(Ae.$$.fragment,t),b(Oe.$$.fragment,t),b(De.$$.fragment,t),b(Be.$$.fragment,t),b(We.$$.fragment,t),b(Ue.$$.fragment,t),b(Ve.$$.fragment,t),b(Je.$$.fragment,t),b(Qe.$$.fragment,t),b(we.$$.fragment,t),b(Ye.$$.fragment,t),b(Ze.$$.fragment,t),b(et.$$.fragment,t),b(st.$$.fragment,t),b($e.$$.fragment,t),b(at.$$.fragment,t),b(rt.$$.fragment,t),b(it.$$.fragment,t),b(pt.$$.fragment,t),b(xe.$$.fragment,t),b(ht.$$.fragment,t),b(ut.$$.fragment,t),b(mt.$$.fragment,t),b(ft.$$.fragment,t),b(kt.$$.fragment,t),b(Fe.$$.fragment,t),b(wt.$$.fragment,t),b(Nt.$$.fragment,t),b($t.$$.fragment,t),b(qt.$$.fragment,t),b(ze.$$.fragment,t),b(jt.$$.fragment,t),Gn=!1},d(t){o(p),t&&o(w),t&&o(m),P(g),t&&o(ln),t&&o(oe),P(Ae),t&&o(dn),t&&o(J),t&&o(cn),t&&o(Lt),t&&o(pn),t&&o(ve),t&&o(hn),t&&o(ne),P(Oe),t&&o(un),t&&o(be),t&&o(mn),P(De,t),t&&o(fn),t&&o(se),P(Be),t&&o(gn),t&&o(E),P(We),P(Ue),t&&o(_n),t&&o(ie),P(Ve),t&&o(Tn),t&&o(z),P(Je),P(Qe),P(we),P(Ye),t&&o(vn),t&&o(de),P(Ze),t&&o(yn),t&&o(C),P(et),P(st),P($e),P(at),t&&o(bn),t&&o(pe),P(rt),t&&o(Pn),t&&o(G),P(it),P(pt),P(xe),P(ht),P(ut),t&&o(kn),t&&o(ue),P(mt),t&&o(wn),t&&o(x),P(ft),P(kt),P(Fe),P(wt),t&&o(Nn),t&&o(fe),P(Nt),t&&o($n),t&&o(M),P($t),P(qt),P(ze),P(jt)}}}const id={local:"gpt-neo",sections:[{local:"overview",sections:[{local:"generation",title:"Generation"}],title:"Overview"},{local:"transformers.GPTNeoConfig",title:"GPTNeoConfig"},{local:"transformers.GPTNeoModel",title:"GPTNeoModel"},{local:"transformers.GPTNeoForCausalLM",title:"GPTNeoForCausalLM"},{local:"transformers.GPTNeoForSequenceClassification",title:"GPTNeoForSequenceClassification"},{local:"transformers.FlaxGPTNeoModel",title:"FlaxGPTNeoModel"},{local:"transformers.FlaxGPTNeoForCausalLM",title:"FlaxGPTNeoForCausalLM"}],title:"GPT Neo"};function ld(B,p,w){let{fw:m}=p;return B.$$set=k=>{"fw"in k&&w(0,m=k.fw)},[m]}class md extends Ql{constructor(p){super();Yl(this,p,ld,rd,Zl,{fw:0})}}export{md as default,id as metadata};
