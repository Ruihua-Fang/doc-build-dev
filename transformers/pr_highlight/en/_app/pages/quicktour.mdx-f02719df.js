import{S as kp,i as yp,s as bp,e as l,k as c,w as b,t as o,M as Ep,c as i,d as s,m as d,a as p,x as E,h as n,b as $,F as t,g as u,y as A,q as j,o as T,B as x,v as Ap,L as qe}from"../chunks/vendor-6b77c823.js";import{T as Ss}from"../chunks/Tip-39098574.js";import{Y as wp}from"../chunks/Youtube-5c6e11e6.js";import{I as ht}from"../chunks/IconCopyLink-7a11ce68.js";import{C as U}from"../chunks/CodeBlock-3a8b25a8.js";import{D as jp}from"../chunks/DocNotebookDropdown-f2b55cd8.js";import{F as Ps,M as _e}from"../chunks/Markdown-9acf6d91.js";function Tp(F){let a,h;return{c(){a=l("p"),h=o(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(r){a=i(r,"P",{});var m=p(a);h=n(m,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),m.forEach(s)},m(r,m){u(r,a,m),t(a,h)},d(r){r&&s(a)}}}function xp(F){let a,h,r,m,_,v,q,C;return{c(){a=l("p"),h=o("For more details about the "),r=l("a"),m=o("pipeline()"),_=o(" and associated tasks, refer to the documentation "),v=l("a"),q=o("here"),C=o("."),this.h()},l(w){a=i(w,"P",{});var S=p(a);h=n(S,"For more details about the "),r=i(S,"A",{href:!0});var N=p(r);m=n(N,"pipeline()"),N.forEach(s),_=n(S," and associated tasks, refer to the documentation "),v=i(S,"A",{href:!0});var I=p(v);q=n(I,"here"),I.forEach(s),C=n(S,"."),S.forEach(s),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(v,"href","./main_classes/pipelines")},m(w,S){u(w,a,S),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C)},d(w){w&&s(a)}}}function zp(F){let a,h;return a=new U({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p:qe,i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function qp(F){let a,h;return a=new _e({props:{$$slots:{default:[zp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Fp(F){let a,h;return a=new U({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p:qe,i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Pp(F){let a,h;return a=new _e({props:{$$slots:{default:[Fp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Sp(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R;return L=new U({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),h=o("Use the "),r=l("a"),m=o("AutoModelForSequenceClassification"),_=o(" and "),v=l("a"),q=o("AutoTokenizer"),C=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),S=o("AutoClass"),N=o(" below):"),I=c(),b(L.$$.fragment),this.h()},l(y){a=i(y,"P",{});var M=p(a);h=n(M,"Use the "),r=i(M,"A",{href:!0});var g=p(r);m=n(g,"AutoModelForSequenceClassification"),g.forEach(s),_=n(M," and "),v=i(M,"A",{href:!0});var P=p(v);q=n(P,"AutoTokenizer"),P.forEach(s),C=n(M," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(M,"CODE",{});var W=p(w);S=n(W,"AutoClass"),W.forEach(s),N=n(M," below):"),M.forEach(s),I=d(y),E(L.$$.fragment,y),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(v,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer")},m(y,M){u(y,a,M),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C),t(a,w),t(w,S),t(a,N),u(y,I,M),A(L,y,M),R=!0},p:qe,i(y){R||(j(L.$$.fragment,y),R=!0)},o(y){T(L.$$.fragment,y),R=!1},d(y){y&&s(a),y&&s(I),x(L,y)}}}function Mp(F){let a,h;return a=new _e({props:{$$slots:{default:[Sp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Cp(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R;return L=new U({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),h=o("Use the "),r=l("a"),m=o("TFAutoModelForSequenceClassification"),_=o(" and "),v=l("a"),q=o("AutoTokenizer"),C=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),S=o("TFAutoClass"),N=o(" below):"),I=c(),b(L.$$.fragment),this.h()},l(y){a=i(y,"P",{});var M=p(a);h=n(M,"Use the "),r=i(M,"A",{href:!0});var g=p(r);m=n(g,"TFAutoModelForSequenceClassification"),g.forEach(s),_=n(M," and "),v=i(M,"A",{href:!0});var P=p(v);q=n(P,"AutoTokenizer"),P.forEach(s),C=n(M," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(M,"CODE",{});var W=p(w);S=n(W,"TFAutoClass"),W.forEach(s),N=n(M," below):"),M.forEach(s),I=d(y),E(L.$$.fragment,y),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),$(v,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer")},m(y,M){u(y,a,M),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C),t(a,w),t(w,S),t(a,N),u(y,I,M),A(L,y,M),R=!0},p:qe,i(y){R||(j(L.$$.fragment,y),R=!0)},o(y){T(L.$$.fragment,y),R=!1},d(y){y&&s(a),y&&s(I),x(L,y)}}}function Np(F){let a,h;return a=new _e({props:{$$slots:{default:[Cp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Ip(F){let a,h;return a=new U({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p:qe,i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Op(F){let a,h;return a=new _e({props:{$$slots:{default:[Ip]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Lp(F){let a,h;return a=new U({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p:qe,i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Dp(F){let a,h;return a=new _e({props:{$$slots:{default:[Lp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Rp(F){let a,h,r,m,_,v,q,C;return{c(){a=l("p"),h=o("See the "),r=l("a"),m=o("task summary"),_=o(" for which "),v=l("a"),q=o("AutoModel"),C=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var S=p(a);h=n(S,"See the "),r=i(S,"A",{href:!0});var N=p(r);m=n(N,"task summary"),N.forEach(s),_=n(S," for which "),v=i(S,"A",{href:!0});var I=p(v);q=n(I,"AutoModel"),I.forEach(s),C=n(S," class to use for which task."),S.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModel")},m(w,S){u(w,a,S),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C)},d(w){w&&s(a)}}}function Hp(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R,y,M,g,P,W,H,Q,G,se,J,Y,ee,K,V,me,re,de,oe,te,ne,$e,z,O,le;return M=new U({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),P=new Ss({props:{$$slots:{default:[Rp]},$$scope:{ctx:F}}}),ee=new U({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),O=new U({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){a=l("p"),h=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),m=o("AutoModel"),_=o(" like you would load an "),v=l("a"),q=o("AutoTokenizer"),C=o(". The only difference is selecting the correct "),w=l("a"),S=o("AutoModel"),N=o(" for the task. Since you are doing text - or sequence - classification, load "),I=l("a"),L=o("AutoModelForSequenceClassification"),R=o(":"),y=c(),b(M.$$.fragment),g=c(),b(P.$$.fragment),W=c(),H=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),G=l("code"),se=o("**"),J=o(":"),Y=c(),b(ee.$$.fragment),K=c(),V=l("p"),me=o("The model outputs the final activations in the "),re=l("code"),de=o("logits"),oe=o(" attribute. Apply the softmax function to the "),te=l("code"),ne=o("logits"),$e=o(" to retrieve the probabilities:"),z=c(),b(O.$$.fragment),this.h()},l(k){a=i(k,"P",{});var D=p(a);h=n(D,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(D,"A",{href:!0});var ie=p(r);m=n(ie,"AutoModel"),ie.forEach(s),_=n(D," like you would load an "),v=i(D,"A",{href:!0});var Fe=p(v);q=n(Fe,"AutoTokenizer"),Fe.forEach(s),C=n(D,". The only difference is selecting the correct "),w=i(D,"A",{href:!0});var ce=p(w);S=n(ce,"AutoModel"),ce.forEach(s),N=n(D," for the task. Since you are doing text - or sequence - classification, load "),I=i(D,"A",{href:!0});var ge=p(I);L=n(ge,"AutoModelForSequenceClassification"),ge.forEach(s),R=n(D,":"),D.forEach(s),y=d(k),E(M.$$.fragment,k),g=d(k),E(P.$$.fragment,k),W=d(k),H=i(k,"P",{});var pe=p(H);Q=n(pe,"Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),G=i(pe,"CODE",{});var Le=p(G);se=n(Le,"**"),Le.forEach(s),J=n(pe,":"),pe.forEach(s),Y=d(k),E(ee.$$.fragment,k),K=d(k),V=i(k,"P",{});var ve=p(V);me=n(ve,"The model outputs the final activations in the "),re=i(ve,"CODE",{});var Yt=p(re);de=n(Yt,"logits"),Yt.forEach(s),oe=n(ve," attribute. Apply the softmax function to the "),te=i(ve,"CODE",{});var mt=p(te);ne=n(mt,"logits"),mt.forEach(s),$e=n(ve," to retrieve the probabilities:"),ve.forEach(s),z=d(k),E(O.$$.fragment,k),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModel"),$(v,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModel"),$(I,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(k,D){u(k,a,D),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C),t(a,w),t(w,S),t(a,N),t(a,I),t(I,L),t(a,R),u(k,y,D),A(M,k,D),u(k,g,D),A(P,k,D),u(k,W,D),u(k,H,D),t(H,Q),t(H,G),t(G,se),t(H,J),u(k,Y,D),A(ee,k,D),u(k,K,D),u(k,V,D),t(V,me),t(V,re),t(re,de),t(V,oe),t(V,te),t(te,ne),t(V,$e),u(k,z,D),A(O,k,D),le=!0},p(k,D){const ie={};D&2&&(ie.$$scope={dirty:D,ctx:k}),P.$set(ie)},i(k){le||(j(M.$$.fragment,k),j(P.$$.fragment,k),j(ee.$$.fragment,k),j(O.$$.fragment,k),le=!0)},o(k){T(M.$$.fragment,k),T(P.$$.fragment,k),T(ee.$$.fragment,k),T(O.$$.fragment,k),le=!1},d(k){k&&s(a),k&&s(y),x(M,k),k&&s(g),x(P,k),k&&s(W),k&&s(H),k&&s(Y),x(ee,k),k&&s(K),k&&s(V),k&&s(z),x(O,k)}}}function Up(F){let a,h;return a=new _e({props:{$$slots:{default:[Hp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Wp(F){let a,h,r,m,_,v,q,C;return{c(){a=l("p"),h=o("See the "),r=l("a"),m=o("task summary"),_=o(" for which "),v=l("a"),q=o("AutoModel"),C=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var S=p(a);h=n(S,"See the "),r=i(S,"A",{href:!0});var N=p(r);m=n(N,"task summary"),N.forEach(s),_=n(S," for which "),v=i(S,"A",{href:!0});var I=p(v);q=n(I,"AutoModel"),I.forEach(s),C=n(S," class to use for which task."),S.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModel")},m(w,S){u(w,a,S),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C)},d(w){w&&s(a)}}}function Yp(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R,y,M,g,P,W,H,Q,G,se,J,Y,ee,K,V,me,re,de,oe,te,ne,$e;return M=new U({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),P=new Ss({props:{$$slots:{default:[Wp]},$$scope:{ctx:F}}}),se=new U({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ne=new U({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){a=l("p"),h=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),m=o("TFAutoModel"),_=o(" like you would load an "),v=l("a"),q=o("AutoTokenizer"),C=o(". The only difference is selecting the correct "),w=l("a"),S=o("TFAutoModel"),N=o(" for the task. Since you are doing text - or sequence - classification, load "),I=l("a"),L=o("TFAutoModelForSequenceClassification"),R=o(":"),y=c(),b(M.$$.fragment),g=c(),b(P.$$.fragment),W=c(),H=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),G=c(),b(se.$$.fragment),J=c(),Y=l("p"),ee=o("The model outputs the final activations in the "),K=l("code"),V=o("logits"),me=o(" attribute. Apply the softmax function to the "),re=l("code"),de=o("logits"),oe=o(" to retrieve the probabilities:"),te=c(),b(ne.$$.fragment),this.h()},l(z){a=i(z,"P",{});var O=p(a);h=n(O,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(O,"A",{href:!0});var le=p(r);m=n(le,"TFAutoModel"),le.forEach(s),_=n(O," like you would load an "),v=i(O,"A",{href:!0});var k=p(v);q=n(k,"AutoTokenizer"),k.forEach(s),C=n(O,". The only difference is selecting the correct "),w=i(O,"A",{href:!0});var D=p(w);S=n(D,"TFAutoModel"),D.forEach(s),N=n(O," for the task. Since you are doing text - or sequence - classification, load "),I=i(O,"A",{href:!0});var ie=p(I);L=n(ie,"TFAutoModelForSequenceClassification"),ie.forEach(s),R=n(O,":"),O.forEach(s),y=d(z),E(M.$$.fragment,z),g=d(z),E(P.$$.fragment,z),W=d(z),H=i(z,"P",{});var Fe=p(H);Q=n(Fe,"Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),Fe.forEach(s),G=d(z),E(se.$$.fragment,z),J=d(z),Y=i(z,"P",{});var ce=p(Y);ee=n(ce,"The model outputs the final activations in the "),K=i(ce,"CODE",{});var ge=p(K);V=n(ge,"logits"),ge.forEach(s),me=n(ce," attribute. Apply the softmax function to the "),re=i(ce,"CODE",{});var pe=p(re);de=n(pe,"logits"),pe.forEach(s),oe=n(ce," to retrieve the probabilities:"),ce.forEach(s),te=d(z),E(ne.$$.fragment,z),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.TFAutoModel"),$(v,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.TFAutoModel"),$(I,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(z,O){u(z,a,O),t(a,h),t(a,r),t(r,m),t(a,_),t(a,v),t(v,q),t(a,C),t(a,w),t(w,S),t(a,N),t(a,I),t(I,L),t(a,R),u(z,y,O),A(M,z,O),u(z,g,O),A(P,z,O),u(z,W,O),u(z,H,O),t(H,Q),u(z,G,O),A(se,z,O),u(z,J,O),u(z,Y,O),t(Y,ee),t(Y,K),t(K,V),t(Y,me),t(Y,re),t(re,de),t(Y,oe),u(z,te,O),A(ne,z,O),$e=!0},p(z,O){const le={};O&2&&(le.$$scope={dirty:O,ctx:z}),P.$set(le)},i(z){$e||(j(M.$$.fragment,z),j(P.$$.fragment,z),j(se.$$.fragment,z),j(ne.$$.fragment,z),$e=!0)},o(z){T(M.$$.fragment,z),T(P.$$.fragment,z),T(se.$$.fragment,z),T(ne.$$.fragment,z),$e=!1},d(z){z&&s(a),z&&s(y),x(M,z),z&&s(g),x(P,z),z&&s(W),z&&s(H),z&&s(G),x(se,z),z&&s(J),z&&s(Y),z&&s(te),x(ne,z)}}}function Bp(F){let a,h;return a=new _e({props:{$$slots:{default:[Yp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Gp(F){let a,h,r,m,_;return{c(){a=l("p"),h=o("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=l("em"),m=o("before"),_=o(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(v){a=i(v,"P",{});var q=p(a);h=n(q,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=i(q,"EM",{});var C=p(r);m=n(C,"before"),C.forEach(s),_=n(q,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),q.forEach(s)},m(v,q){u(v,a,q),t(a,h),t(a,r),t(r,m),t(a,_)},d(v){v&&s(a)}}}function Qp(F){let a,h,r,m,_;return{c(){a=l("p"),h=o(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=l("code"),m=o("None"),_=o(" are ignored.")},l(v){a=i(v,"P",{});var q=p(a);h=n(q,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=i(q,"CODE",{});var C=p(r);m=n(C,"None"),C.forEach(s),_=n(q," are ignored."),q.forEach(s)},m(v,q){u(v,a,q),t(a,h),t(a,r),t(r,m),t(a,_)},d(v){v&&s(a)}}}function Jp(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R,y,M;return q=new U({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),y=new U({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),h=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),m=o("PreTrainedModel.save_pretrained()"),_=o(":"),v=c(),b(q.$$.fragment),C=c(),w=l("p"),S=o("When you are ready to use the model again, reload it with "),N=l("a"),I=o("PreTrainedModel.from_pretrained()"),L=o(":"),R=c(),b(y.$$.fragment),this.h()},l(g){a=i(g,"P",{});var P=p(a);h=n(P,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(P,"A",{href:!0});var W=p(r);m=n(W,"PreTrainedModel.save_pretrained()"),W.forEach(s),_=n(P,":"),P.forEach(s),v=d(g),E(q.$$.fragment,g),C=d(g),w=i(g,"P",{});var H=p(w);S=n(H,"When you are ready to use the model again, reload it with "),N=i(H,"A",{href:!0});var Q=p(N);I=n(Q,"PreTrainedModel.from_pretrained()"),Q.forEach(s),L=n(H,":"),H.forEach(s),R=d(g),E(y.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),$(N,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(g,P){u(g,a,P),t(a,h),t(a,r),t(r,m),t(a,_),u(g,v,P),A(q,g,P),u(g,C,P),u(g,w,P),t(w,S),t(w,N),t(N,I),t(w,L),u(g,R,P),A(y,g,P),M=!0},p:qe,i(g){M||(j(q.$$.fragment,g),j(y.$$.fragment,g),M=!0)},o(g){T(q.$$.fragment,g),T(y.$$.fragment,g),M=!1},d(g){g&&s(a),g&&s(v),x(q,g),g&&s(C),g&&s(w),g&&s(R),x(y,g)}}}function Kp(F){let a,h;return a=new _e({props:{$$slots:{default:[Jp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Vp(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R,y,M;return q=new U({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),y=new U({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),h=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),m=o("TFPreTrainedModel.save_pretrained()"),_=o(":"),v=c(),b(q.$$.fragment),C=c(),w=l("p"),S=o("When you are ready to use the model again, reload it with "),N=l("a"),I=o("TFPreTrainedModel.from_pretrained()"),L=o(":"),R=c(),b(y.$$.fragment),this.h()},l(g){a=i(g,"P",{});var P=p(a);h=n(P,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(P,"A",{href:!0});var W=p(r);m=n(W,"TFPreTrainedModel.save_pretrained()"),W.forEach(s),_=n(P,":"),P.forEach(s),v=d(g),E(q.$$.fragment,g),C=d(g),w=i(g,"P",{});var H=p(w);S=n(H,"When you are ready to use the model again, reload it with "),N=i(H,"A",{href:!0});var Q=p(N);I=n(Q,"TFPreTrainedModel.from_pretrained()"),Q.forEach(s),L=n(H,":"),H.forEach(s),R=d(g),E(y.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),$(N,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained")},m(g,P){u(g,a,P),t(a,h),t(a,r),t(r,m),t(a,_),u(g,v,P),A(q,g,P),u(g,C,P),u(g,w,P),t(w,S),t(w,N),t(N,I),t(w,L),u(g,R,P),A(y,g,P),M=!0},p:qe,i(g){M||(j(q.$$.fragment,g),j(y.$$.fragment,g),M=!0)},o(g){T(q.$$.fragment,g),T(y.$$.fragment,g),M=!1},d(g){g&&s(a),g&&s(v),x(q,g),g&&s(C),g&&s(w),g&&s(R),x(y,g)}}}function Zp(F){let a,h;return a=new _e({props:{$$slots:{default:[Vp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function Xp(F){let a,h;return a=new U({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p:qe,i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function ef(F){let a,h;return a=new _e({props:{$$slots:{default:[Xp]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function tf(F){let a,h;return a=new U({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p:qe,i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function sf(F){let a,h;return a=new _e({props:{$$slots:{default:[tf]},$$scope:{ctx:F}}}),{c(){b(a.$$.fragment)},l(r){E(a.$$.fragment,r)},m(r,m){A(a,r,m),h=!0},p(r,m){const _={};m&2&&(_.$$scope={dirty:m,ctx:r}),a.$set(_)},i(r){h||(j(a.$$.fragment,r),h=!0)},o(r){T(a.$$.fragment,r),h=!1},d(r){x(a,r)}}}function af(F){let a,h,r,m,_,v,q,C,w,S,N,I,L,R,y,M,g,P,W,H,Q,G,se,J,Y,ee,K,V,me,re,de,oe,te,ne,$e,z,O,le,k,D,ie,Fe,ce,ge,pe,Le,ve,Yt,mt,B,Ms,Br,Gr,Cs,Qr,Jr,Ns,Kr,Vr,Is,Zr,Xr,Os,eo,to,Ls,so,ao,Ds,ro,oo,Rs,no,Aa,ct,Hs,lo,io,ja,we,Us,po,fo,Ws,uo,ho,Ys,mo,Ta,dt,Bs,co,$o,xa,De,Gs,_o,go,Qs,vo,za,Re,qa,Pe,He,Js,$t,wo,Ks,ko,Fa,Ue,yo,Bt,bo,Eo,Pa,Gt,Ao,Sa,We,Ma,Ye,jo,Qt,To,xo,Ca,_t,Na,ke,zo,gt,qo,Fo,Vs,Po,So,Ia,vt,Oa,Be,Mo,Jt,Co,No,La,wt,Da,ye,Io,Kt,Oo,Lo,kt,Do,Ro,Ra,yt,Ha,Ge,Ho,Vt,Uo,Wo,Ua,bt,Wa,be,Yo,Et,Bo,Go,At,Qo,Jo,Ya,jt,Ba,Zt,Ko,Ga,Tt,Qa,Qe,Vo,Xt,Zo,Xo,Ja,Se,Je,Zs,xt,en,Xs,tn,Ka,fe,sn,es,an,rn,zt,on,nn,ts,ln,pn,qt,fn,un,Va,Ft,Za,Ke,Xa,Ee,hn,ss,mn,cn,ea,dn,$n,er,Pt,tr,Ae,_n,as,gn,vn,rs,wn,kn,sr,Me,Ve,ta,St,yn,sa,bn,ar,Mt,rr,Z,En,os,An,jn,ns,Tn,xn,ls,zn,qn,is,Fn,Pn,aa,Sn,Mn,ps,Cn,Nn,or,je,In,ra,On,Ln,fs,Dn,Rn,nr,Ce,Ze,oa,Ct,Hn,na,Un,lr,Te,Wn,la,Yn,Bn,us,Gn,Qn,ir,Xe,Jn,hs,Kn,Vn,pr,Nt,fr,et,Zn,ia,Xn,el,ur,ms,tl,hr,It,mr,cs,sl,cr,tt,ds,$s,al,rl,ol,_s,gs,nl,ll,dr,st,il,vs,pl,fl,$r,at,_r,rt,ul,ws,hl,ml,gr,Ne,ot,pa,Ot,cl,fa,dl,vr,nt,wr,lt,kr,X,$l,Lt,ua,_l,gl,Dt,ha,vl,wl,ks,kl,yl,ma,bl,El,Rt,Al,jl,ys,Tl,xl,yr,it,br,Ie,pt,ca,Ht,zl,da,ql,Er,ft,Ar,xe,Fl,$a,Pl,Sl,_a,Ml,Cl,jr,ut,Tr;return v=new ht({}),N=new jp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),G=new Ss({props:{$$slots:{default:[Tp]},$$scope:{ctx:F}}}),K=new ht({}),O=new wp({props:{id:"tiZFewofSLM"}}),Re=new Ss({props:{$$slots:{default:[xp]},$$scope:{ctx:F}}}),$t=new ht({}),We=new Ps({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Pp],pytorch:[qp]},$$scope:{ctx:F}}}),_t=new U({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),vt=new U({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),wt=new U({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),yt=new U({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),bt=new U({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),jt=new U({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),Tt=new U({props:{code:`files = dataset["file"]
speech_recognizer(files[:4])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;file&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;STUFFERED INTO YOU HIS BELLY COUNSELLED HIM&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HO BERTIE ANY GOOD IN YOUR MIND&#x27;</span>}]`}}),xt=new ht({}),Ft=new U({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Ke=new Ps({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Np],pytorch:[Mp]},$$scope:{ctx:F}}}),Pt=new U({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),St=new ht({}),Mt=new wp({props:{id:"AhChOFRegn4"}}),Ct=new ht({}),Nt=new U({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),It=new U({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),at=new Ps({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Dp],pytorch:[Op]},$$scope:{ctx:F}}}),Ot=new ht({}),nt=new Ps({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Bp],pytorch:[Up]},$$scope:{ctx:F}}}),lt=new Ss({props:{$$slots:{default:[Gp]},$$scope:{ctx:F}}}),it=new Ss({props:{$$slots:{default:[Qp]},$$scope:{ctx:F}}}),Ht=new ht({}),ft=new Ps({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Zp],pytorch:[Kp]},$$scope:{ctx:F}}}),ut=new Ps({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[sf],pytorch:[ef]},$$scope:{ctx:F}}}),{c(){a=l("meta"),h=c(),r=l("h1"),m=l("a"),_=l("span"),b(v.$$.fragment),q=c(),C=l("span"),w=o("Quick tour"),S=c(),b(N.$$.fragment),I=c(),L=l("p"),R=o("Get up and running with \u{1F917} Transformers! Start using the "),y=l("a"),M=o("pipeline()"),g=o(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),P=l("a"),W=o("AutoClass"),H=o(" to solve your text, vision or audio task."),Q=c(),b(G.$$.fragment),se=c(),J=l("h2"),Y=l("a"),ee=l("span"),b(K.$$.fragment),V=c(),me=l("span"),re=o("Pipeline"),de=c(),oe=l("p"),te=l("a"),ne=o("pipeline()"),$e=o(" is the easiest way to use a pretrained model for a given task."),z=c(),b(O.$$.fragment),le=c(),k=l("p"),D=o("The "),ie=l("a"),Fe=o("pipeline()"),ce=o(" supports many common tasks out-of-the-box:"),ge=c(),pe=l("p"),Le=l("strong"),ve=o("Text"),Yt=o(":"),mt=c(),B=l("ul"),Ms=l("li"),Br=o("Sentiment analysis: classify the polarity of a given text."),Gr=c(),Cs=l("li"),Qr=o("Text generation (in English): generate text from a given input."),Jr=c(),Ns=l("li"),Kr=o("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Vr=c(),Is=l("li"),Zr=o("Question answering: extract the answer from the context, given some context and a question."),Xr=c(),Os=l("li"),eo=o("Fill-mask: fill in the blank given a text with masked words."),to=c(),Ls=l("li"),so=o("Summarization: generate a summary of a long sequence of text or document."),ao=c(),Ds=l("li"),ro=o("Translation: translate text into another language."),oo=c(),Rs=l("li"),no=o("Feature extraction: create a tensor representation of the text."),Aa=c(),ct=l("p"),Hs=l("strong"),lo=o("Image"),io=o(":"),ja=c(),we=l("ul"),Us=l("li"),po=o("Image classification: classify an image."),fo=c(),Ws=l("li"),uo=o("Image segmentation: classify every pixel in an image."),ho=c(),Ys=l("li"),mo=o("Object detection: detect objects within an image."),Ta=c(),dt=l("p"),Bs=l("strong"),co=o("Audio"),$o=o(":"),xa=c(),De=l("ul"),Gs=l("li"),_o=o("Audio classification: assign a label to a given segment of audio."),go=c(),Qs=l("li"),vo=o("Automatic speech recognition (ASR): transcribe audio data into text."),za=c(),b(Re.$$.fragment),qa=c(),Pe=l("h3"),He=l("a"),Js=l("span"),b($t.$$.fragment),wo=c(),Ks=l("span"),ko=o("Pipeline usage"),Fa=c(),Ue=l("p"),yo=o("In the following example, you will use the "),Bt=l("a"),bo=o("pipeline()"),Eo=o(" for sentiment analysis."),Pa=c(),Gt=l("p"),Ao=o("Install the following dependencies if you haven\u2019t already:"),Sa=c(),b(We.$$.fragment),Ma=c(),Ye=l("p"),jo=o("Import "),Qt=l("a"),To=o("pipeline()"),xo=o(" and specify the task you want to complete:"),Ca=c(),b(_t.$$.fragment),Na=c(),ke=l("p"),zo=o("The pipeline downloads and caches a default "),gt=l("a"),qo=o("pretrained model"),Fo=o(" and tokenizer for sentiment analysis. Now you can use the "),Vs=l("code"),Po=o("classifier"),So=o(" on your target text:"),Ia=c(),b(vt.$$.fragment),Oa=c(),Be=l("p"),Mo=o("For more than one sentence, pass a list of sentences to the "),Jt=l("a"),Co=o("pipeline()"),No=o(" which returns a list of dictionaries:"),La=c(),b(wt.$$.fragment),Da=c(),ye=l("p"),Io=o("The "),Kt=l("a"),Oo=o("pipeline()"),Lo=o(" can also iterate over an entire dataset. Start by installing the "),kt=l("a"),Do=o("\u{1F917} Datasets"),Ro=o(" library:"),Ra=c(),b(yt.$$.fragment),Ha=c(),Ge=l("p"),Ho=o("Create a "),Vt=l("a"),Uo=o("pipeline()"),Wo=o(" with the task you want to solve for and the model you want to use."),Ua=c(),b(bt.$$.fragment),Wa=c(),be=l("p"),Yo=o("Next, load a dataset (see the \u{1F917} Datasets "),Et=l("a"),Bo=o("Quick Start"),Go=o(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),At=l("a"),Qo=o("SUPERB"),Jo=o(" dataset:"),Ya=c(),b(jt.$$.fragment),Ba=c(),Zt=l("p"),Ko=o("You can pass a whole dataset pipeline:"),Ga=c(),b(Tt.$$.fragment),Qa=c(),Qe=l("p"),Vo=o("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Xt=l("a"),Zo=o("pipeline documentation"),Xo=o(" for more information."),Ja=c(),Se=l("h3"),Je=l("a"),Zs=l("span"),b(xt.$$.fragment),en=c(),Xs=l("span"),tn=o("Use another model and tokenizer in the pipeline"),Ka=c(),fe=l("p"),sn=o("The "),es=l("a"),an=o("pipeline()"),rn=o(" can accommodate any model from the "),zt=l("a"),on=o("Model Hub"),nn=o(", making it easy to adapt the "),ts=l("a"),ln=o("pipeline()"),pn=o(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),qt=l("a"),fn=o("BERT model"),un=o(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),Va=c(),b(Ft.$$.fragment),Za=c(),b(Ke.$$.fragment),Xa=c(),Ee=l("p"),hn=o("Then you can specify the model and tokenizer in the "),ss=l("a"),mn=o("pipeline()"),cn=o(", and apply the "),ea=l("code"),dn=o("classifier"),$n=o(" on your target text:"),er=c(),b(Pt.$$.fragment),tr=c(),Ae=l("p"),_n=o("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),as=l("a"),gn=o("fine-tuning tutorial"),vn=o(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),rs=l("a"),wn=o("here"),kn=o(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),sr=c(),Me=l("h2"),Ve=l("a"),ta=l("span"),b(St.$$.fragment),yn=c(),sa=l("span"),bn=o("AutoClass"),ar=c(),b(Mt.$$.fragment),rr=c(),Z=l("p"),En=o("Under the hood, the "),os=l("a"),An=o("AutoModelForSequenceClassification"),jn=o(" and "),ns=l("a"),Tn=o("AutoTokenizer"),xn=o(" classes work together to power the "),ls=l("a"),zn=o("pipeline()"),qn=o(". An "),is=l("a"),Fn=o("AutoClass"),Pn=o(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=l("code"),Sn=o("AutoClass"),Mn=o(" for your task and it\u2019s associated tokenizer with "),ps=l("a"),Cn=o("AutoTokenizer"),Nn=o("."),or=c(),je=l("p"),In=o("Let\u2019s return to our example and see how you can use the "),ra=l("code"),On=o("AutoClass"),Ln=o(" to replicate the results of the "),fs=l("a"),Dn=o("pipeline()"),Rn=o("."),nr=c(),Ce=l("h3"),Ze=l("a"),oa=l("span"),b(Ct.$$.fragment),Hn=c(),na=l("span"),Un=o("AutoTokenizer"),lr=c(),Te=l("p"),Wn=o("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=l("em"),Yn=o("tokens"),Bn=o(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),us=l("a"),Gn=o("here"),Qn=o("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),ir=c(),Xe=l("p"),Jn=o("Load a tokenizer with "),hs=l("a"),Kn=o("AutoTokenizer"),Vn=o(":"),pr=c(),b(Nt.$$.fragment),fr=c(),et=l("p"),Zn=o("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=l("em"),Xn=o("vocabulary"),el=o("."),ur=c(),ms=l("p"),tl=o("Pass your text to the tokenizer:"),hr=c(),b(It.$$.fragment),mr=c(),cs=l("p"),sl=o("The tokenizer will return a dictionary containing:"),cr=c(),tt=l("ul"),ds=l("li"),$s=l("a"),al=o("input_ids"),rl=o(": numerical representions of your tokens."),ol=c(),_s=l("li"),gs=l("a"),nl=o("atttention_mask"),ll=o(": indicates which tokens should be attended to."),dr=c(),st=l("p"),il=o("Just like the "),vs=l("a"),pl=o("pipeline()"),fl=o(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),$r=c(),b(at.$$.fragment),_r=c(),rt=l("p"),ul=o("Read the "),ws=l("a"),hl=o("preprocessing"),ml=o(" tutorial for more details about tokenization."),gr=c(),Ne=l("h3"),ot=l("a"),pa=l("span"),b(Ot.$$.fragment),cl=c(),fa=l("span"),dl=o("AutoModel"),vr=c(),b(nt.$$.fragment),wr=c(),b(lt.$$.fragment),kr=c(),X=l("p"),$l=o("Models are a standard "),Lt=l("a"),ua=l("code"),_l=o("torch.nn.Module"),gl=o(" or a "),Dt=l("a"),ha=l("code"),vl=o("tf.keras.Model"),wl=o(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ks=l("a"),kl=o("Trainer"),yl=o(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ma=l("code"),bl=o("fit"),El=o(" method from "),Rt=l("a"),Al=o("Keras"),jl=o(". Refer to the "),ys=l("a"),Tl=o("training tutorial"),xl=o(" for more details."),yr=c(),b(it.$$.fragment),br=c(),Ie=l("h3"),pt=l("a"),ca=l("span"),b(Ht.$$.fragment),zl=c(),da=l("span"),ql=o("Save a model"),Er=c(),b(ft.$$.fragment),Ar=c(),xe=l("p"),Fl=o("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=l("code"),Pl=o("from_pt"),Sl=o(" or "),_a=l("code"),Ml=o("from_tf"),Cl=o(" parameter can convert the model from one framework to the other:"),jr=c(),b(ut.$$.fragment),this.h()},l(e){const f=Ep('[data-svelte="svelte-1phssyn"]',document.head);a=i(f,"META",{name:!0,content:!0}),f.forEach(s),h=d(e),r=i(e,"H1",{class:!0});var Ut=p(r);m=i(Ut,"A",{id:!0,class:!0,href:!0});var ga=p(m);_=i(ga,"SPAN",{});var va=p(_);E(v.$$.fragment,va),va.forEach(s),ga.forEach(s),q=d(Ut),C=i(Ut,"SPAN",{});var wa=p(C);w=n(wa,"Quick tour"),wa.forEach(s),Ut.forEach(s),S=d(e),E(N.$$.fragment,e),I=d(e),L=i(e,"P",{});var Oe=p(L);R=n(Oe,"Get up and running with \u{1F917} Transformers! Start using the "),y=i(Oe,"A",{href:!0});var ka=p(y);M=n(ka,"pipeline()"),ka.forEach(s),g=n(Oe," for rapid inference, and quickly load a pretrained model and tokenizer with an "),P=i(Oe,"A",{href:!0});var ya=p(P);W=n(ya,"AutoClass"),ya.forEach(s),H=n(Oe," to solve your text, vision or audio task."),Oe.forEach(s),Q=d(e),E(G.$$.fragment,e),se=d(e),J=i(e,"H2",{class:!0});var Wt=p(J);Y=i(Wt,"A",{id:!0,class:!0,href:!0});var ba=p(Y);ee=i(ba,"SPAN",{});var Ea=p(ee);E(K.$$.fragment,Ea),Ea.forEach(s),ba.forEach(s),V=d(Wt),me=i(Wt,"SPAN",{});var Hl=p(me);re=n(Hl,"Pipeline"),Hl.forEach(s),Wt.forEach(s),de=d(e),oe=i(e,"P",{});var Nl=p(oe);te=i(Nl,"A",{href:!0});var Ul=p(te);ne=n(Ul,"pipeline()"),Ul.forEach(s),$e=n(Nl," is the easiest way to use a pretrained model for a given task."),Nl.forEach(s),z=d(e),E(O.$$.fragment,e),le=d(e),k=i(e,"P",{});var xr=p(k);D=n(xr,"The "),ie=i(xr,"A",{href:!0});var Wl=p(ie);Fe=n(Wl,"pipeline()"),Wl.forEach(s),ce=n(xr," supports many common tasks out-of-the-box:"),xr.forEach(s),ge=d(e),pe=i(e,"P",{});var Il=p(pe);Le=i(Il,"STRONG",{});var Yl=p(Le);ve=n(Yl,"Text"),Yl.forEach(s),Yt=n(Il,":"),Il.forEach(s),mt=d(e),B=i(e,"UL",{});var ae=p(B);Ms=i(ae,"LI",{});var Bl=p(Ms);Br=n(Bl,"Sentiment analysis: classify the polarity of a given text."),Bl.forEach(s),Gr=d(ae),Cs=i(ae,"LI",{});var Gl=p(Cs);Qr=n(Gl,"Text generation (in English): generate text from a given input."),Gl.forEach(s),Jr=d(ae),Ns=i(ae,"LI",{});var Ql=p(Ns);Kr=n(Ql,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Ql.forEach(s),Vr=d(ae),Is=i(ae,"LI",{});var Jl=p(Is);Zr=n(Jl,"Question answering: extract the answer from the context, given some context and a question."),Jl.forEach(s),Xr=d(ae),Os=i(ae,"LI",{});var Kl=p(Os);eo=n(Kl,"Fill-mask: fill in the blank given a text with masked words."),Kl.forEach(s),to=d(ae),Ls=i(ae,"LI",{});var Vl=p(Ls);so=n(Vl,"Summarization: generate a summary of a long sequence of text or document."),Vl.forEach(s),ao=d(ae),Ds=i(ae,"LI",{});var Zl=p(Ds);ro=n(Zl,"Translation: translate text into another language."),Zl.forEach(s),oo=d(ae),Rs=i(ae,"LI",{});var Xl=p(Rs);no=n(Xl,"Feature extraction: create a tensor representation of the text."),Xl.forEach(s),ae.forEach(s),Aa=d(e),ct=i(e,"P",{});var Ol=p(ct);Hs=i(Ol,"STRONG",{});var ei=p(Hs);lo=n(ei,"Image"),ei.forEach(s),io=n(Ol,":"),Ol.forEach(s),ja=d(e),we=i(e,"UL",{});var bs=p(we);Us=i(bs,"LI",{});var ti=p(Us);po=n(ti,"Image classification: classify an image."),ti.forEach(s),fo=d(bs),Ws=i(bs,"LI",{});var si=p(Ws);uo=n(si,"Image segmentation: classify every pixel in an image."),si.forEach(s),ho=d(bs),Ys=i(bs,"LI",{});var ai=p(Ys);mo=n(ai,"Object detection: detect objects within an image."),ai.forEach(s),bs.forEach(s),Ta=d(e),dt=i(e,"P",{});var Ll=p(dt);Bs=i(Ll,"STRONG",{});var ri=p(Bs);co=n(ri,"Audio"),ri.forEach(s),$o=n(Ll,":"),Ll.forEach(s),xa=d(e),De=i(e,"UL",{});var zr=p(De);Gs=i(zr,"LI",{});var oi=p(Gs);_o=n(oi,"Audio classification: assign a label to a given segment of audio."),oi.forEach(s),go=d(zr),Qs=i(zr,"LI",{});var ni=p(Qs);vo=n(ni,"Automatic speech recognition (ASR): transcribe audio data into text."),ni.forEach(s),zr.forEach(s),za=d(e),E(Re.$$.fragment,e),qa=d(e),Pe=i(e,"H3",{class:!0});var qr=p(Pe);He=i(qr,"A",{id:!0,class:!0,href:!0});var li=p(He);Js=i(li,"SPAN",{});var ii=p(Js);E($t.$$.fragment,ii),ii.forEach(s),li.forEach(s),wo=d(qr),Ks=i(qr,"SPAN",{});var pi=p(Ks);ko=n(pi,"Pipeline usage"),pi.forEach(s),qr.forEach(s),Fa=d(e),Ue=i(e,"P",{});var Fr=p(Ue);yo=n(Fr,"In the following example, you will use the "),Bt=i(Fr,"A",{href:!0});var fi=p(Bt);bo=n(fi,"pipeline()"),fi.forEach(s),Eo=n(Fr," for sentiment analysis."),Fr.forEach(s),Pa=d(e),Gt=i(e,"P",{});var ui=p(Gt);Ao=n(ui,"Install the following dependencies if you haven\u2019t already:"),ui.forEach(s),Sa=d(e),E(We.$$.fragment,e),Ma=d(e),Ye=i(e,"P",{});var Pr=p(Ye);jo=n(Pr,"Import "),Qt=i(Pr,"A",{href:!0});var hi=p(Qt);To=n(hi,"pipeline()"),hi.forEach(s),xo=n(Pr," and specify the task you want to complete:"),Pr.forEach(s),Ca=d(e),E(_t.$$.fragment,e),Na=d(e),ke=i(e,"P",{});var Es=p(ke);zo=n(Es,"The pipeline downloads and caches a default "),gt=i(Es,"A",{href:!0,rel:!0});var mi=p(gt);qo=n(mi,"pretrained model"),mi.forEach(s),Fo=n(Es," and tokenizer for sentiment analysis. Now you can use the "),Vs=i(Es,"CODE",{});var ci=p(Vs);Po=n(ci,"classifier"),ci.forEach(s),So=n(Es," on your target text:"),Es.forEach(s),Ia=d(e),E(vt.$$.fragment,e),Oa=d(e),Be=i(e,"P",{});var Sr=p(Be);Mo=n(Sr,"For more than one sentence, pass a list of sentences to the "),Jt=i(Sr,"A",{href:!0});var di=p(Jt);Co=n(di,"pipeline()"),di.forEach(s),No=n(Sr," which returns a list of dictionaries:"),Sr.forEach(s),La=d(e),E(wt.$$.fragment,e),Da=d(e),ye=i(e,"P",{});var As=p(ye);Io=n(As,"The "),Kt=i(As,"A",{href:!0});var $i=p(Kt);Oo=n($i,"pipeline()"),$i.forEach(s),Lo=n(As," can also iterate over an entire dataset. Start by installing the "),kt=i(As,"A",{href:!0,rel:!0});var _i=p(kt);Do=n(_i,"\u{1F917} Datasets"),_i.forEach(s),Ro=n(As," library:"),As.forEach(s),Ra=d(e),E(yt.$$.fragment,e),Ha=d(e),Ge=i(e,"P",{});var Mr=p(Ge);Ho=n(Mr,"Create a "),Vt=i(Mr,"A",{href:!0});var gi=p(Vt);Uo=n(gi,"pipeline()"),gi.forEach(s),Wo=n(Mr," with the task you want to solve for and the model you want to use."),Mr.forEach(s),Ua=d(e),E(bt.$$.fragment,e),Wa=d(e),be=i(e,"P",{});var js=p(be);Yo=n(js,"Next, load a dataset (see the \u{1F917} Datasets "),Et=i(js,"A",{href:!0,rel:!0});var vi=p(Et);Bo=n(vi,"Quick Start"),vi.forEach(s),Go=n(js," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),At=i(js,"A",{href:!0,rel:!0});var wi=p(At);Qo=n(wi,"SUPERB"),wi.forEach(s),Jo=n(js," dataset:"),js.forEach(s),Ya=d(e),E(jt.$$.fragment,e),Ba=d(e),Zt=i(e,"P",{});var ki=p(Zt);Ko=n(ki,"You can pass a whole dataset pipeline:"),ki.forEach(s),Ga=d(e),E(Tt.$$.fragment,e),Qa=d(e),Qe=i(e,"P",{});var Cr=p(Qe);Vo=n(Cr,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Xt=i(Cr,"A",{href:!0});var yi=p(Xt);Zo=n(yi,"pipeline documentation"),yi.forEach(s),Xo=n(Cr," for more information."),Cr.forEach(s),Ja=d(e),Se=i(e,"H3",{class:!0});var Nr=p(Se);Je=i(Nr,"A",{id:!0,class:!0,href:!0});var bi=p(Je);Zs=i(bi,"SPAN",{});var Ei=p(Zs);E(xt.$$.fragment,Ei),Ei.forEach(s),bi.forEach(s),en=d(Nr),Xs=i(Nr,"SPAN",{});var Ai=p(Xs);tn=n(Ai,"Use another model and tokenizer in the pipeline"),Ai.forEach(s),Nr.forEach(s),Ka=d(e),fe=i(e,"P",{});var ze=p(fe);sn=n(ze,"The "),es=i(ze,"A",{href:!0});var ji=p(es);an=n(ji,"pipeline()"),ji.forEach(s),rn=n(ze," can accommodate any model from the "),zt=i(ze,"A",{href:!0,rel:!0});var Ti=p(zt);on=n(Ti,"Model Hub"),Ti.forEach(s),nn=n(ze,", making it easy to adapt the "),ts=i(ze,"A",{href:!0});var xi=p(ts);ln=n(xi,"pipeline()"),xi.forEach(s),pn=n(ze," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),qt=i(ze,"A",{href:!0,rel:!0});var zi=p(qt);fn=n(zi,"BERT model"),zi.forEach(s),un=n(ze," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),ze.forEach(s),Va=d(e),E(Ft.$$.fragment,e),Za=d(e),E(Ke.$$.fragment,e),Xa=d(e),Ee=i(e,"P",{});var Ts=p(Ee);hn=n(Ts,"Then you can specify the model and tokenizer in the "),ss=i(Ts,"A",{href:!0});var qi=p(ss);mn=n(qi,"pipeline()"),qi.forEach(s),cn=n(Ts,", and apply the "),ea=i(Ts,"CODE",{});var Fi=p(ea);dn=n(Fi,"classifier"),Fi.forEach(s),$n=n(Ts," on your target text:"),Ts.forEach(s),er=d(e),E(Pt.$$.fragment,e),tr=d(e),Ae=i(e,"P",{});var xs=p(Ae);_n=n(xs,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),as=i(xs,"A",{href:!0});var Pi=p(as);gn=n(Pi,"fine-tuning tutorial"),Pi.forEach(s),vn=n(xs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),rs=i(xs,"A",{href:!0});var Si=p(rs);wn=n(Si,"here"),Si.forEach(s),kn=n(xs,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),xs.forEach(s),sr=d(e),Me=i(e,"H2",{class:!0});var Ir=p(Me);Ve=i(Ir,"A",{id:!0,class:!0,href:!0});var Mi=p(Ve);ta=i(Mi,"SPAN",{});var Ci=p(ta);E(St.$$.fragment,Ci),Ci.forEach(s),Mi.forEach(s),yn=d(Ir),sa=i(Ir,"SPAN",{});var Ni=p(sa);bn=n(Ni,"AutoClass"),Ni.forEach(s),Ir.forEach(s),ar=d(e),E(Mt.$$.fragment,e),rr=d(e),Z=i(e,"P",{});var ue=p(Z);En=n(ue,"Under the hood, the "),os=i(ue,"A",{href:!0});var Ii=p(os);An=n(Ii,"AutoModelForSequenceClassification"),Ii.forEach(s),jn=n(ue," and "),ns=i(ue,"A",{href:!0});var Oi=p(ns);Tn=n(Oi,"AutoTokenizer"),Oi.forEach(s),xn=n(ue," classes work together to power the "),ls=i(ue,"A",{href:!0});var Li=p(ls);zn=n(Li,"pipeline()"),Li.forEach(s),qn=n(ue,". An "),is=i(ue,"A",{href:!0});var Di=p(is);Fn=n(Di,"AutoClass"),Di.forEach(s),Pn=n(ue," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=i(ue,"CODE",{});var Ri=p(aa);Sn=n(Ri,"AutoClass"),Ri.forEach(s),Mn=n(ue," for your task and it\u2019s associated tokenizer with "),ps=i(ue,"A",{href:!0});var Hi=p(ps);Cn=n(Hi,"AutoTokenizer"),Hi.forEach(s),Nn=n(ue,"."),ue.forEach(s),or=d(e),je=i(e,"P",{});var zs=p(je);In=n(zs,"Let\u2019s return to our example and see how you can use the "),ra=i(zs,"CODE",{});var Ui=p(ra);On=n(Ui,"AutoClass"),Ui.forEach(s),Ln=n(zs," to replicate the results of the "),fs=i(zs,"A",{href:!0});var Wi=p(fs);Dn=n(Wi,"pipeline()"),Wi.forEach(s),Rn=n(zs,"."),zs.forEach(s),nr=d(e),Ce=i(e,"H3",{class:!0});var Or=p(Ce);Ze=i(Or,"A",{id:!0,class:!0,href:!0});var Yi=p(Ze);oa=i(Yi,"SPAN",{});var Bi=p(oa);E(Ct.$$.fragment,Bi),Bi.forEach(s),Yi.forEach(s),Hn=d(Or),na=i(Or,"SPAN",{});var Gi=p(na);Un=n(Gi,"AutoTokenizer"),Gi.forEach(s),Or.forEach(s),lr=d(e),Te=i(e,"P",{});var qs=p(Te);Wn=n(qs,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=i(qs,"EM",{});var Qi=p(la);Yn=n(Qi,"tokens"),Qi.forEach(s),Bn=n(qs,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),us=i(qs,"A",{href:!0});var Ji=p(us);Gn=n(Ji,"here"),Ji.forEach(s),Qn=n(qs,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),qs.forEach(s),ir=d(e),Xe=i(e,"P",{});var Lr=p(Xe);Jn=n(Lr,"Load a tokenizer with "),hs=i(Lr,"A",{href:!0});var Ki=p(hs);Kn=n(Ki,"AutoTokenizer"),Ki.forEach(s),Vn=n(Lr,":"),Lr.forEach(s),pr=d(e),E(Nt.$$.fragment,e),fr=d(e),et=i(e,"P",{});var Dr=p(et);Zn=n(Dr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=i(Dr,"EM",{});var Vi=p(ia);Xn=n(Vi,"vocabulary"),Vi.forEach(s),el=n(Dr,"."),Dr.forEach(s),ur=d(e),ms=i(e,"P",{});var Zi=p(ms);tl=n(Zi,"Pass your text to the tokenizer:"),Zi.forEach(s),hr=d(e),E(It.$$.fragment,e),mr=d(e),cs=i(e,"P",{});var Xi=p(cs);sl=n(Xi,"The tokenizer will return a dictionary containing:"),Xi.forEach(s),cr=d(e),tt=i(e,"UL",{});var Rr=p(tt);ds=i(Rr,"LI",{});var Dl=p(ds);$s=i(Dl,"A",{href:!0});var ep=p($s);al=n(ep,"input_ids"),ep.forEach(s),rl=n(Dl,": numerical representions of your tokens."),Dl.forEach(s),ol=d(Rr),_s=i(Rr,"LI",{});var Rl=p(_s);gs=i(Rl,"A",{href:!0});var tp=p(gs);nl=n(tp,"atttention_mask"),tp.forEach(s),ll=n(Rl,": indicates which tokens should be attended to."),Rl.forEach(s),Rr.forEach(s),dr=d(e),st=i(e,"P",{});var Hr=p(st);il=n(Hr,"Just like the "),vs=i(Hr,"A",{href:!0});var sp=p(vs);pl=n(sp,"pipeline()"),sp.forEach(s),fl=n(Hr,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Hr.forEach(s),$r=d(e),E(at.$$.fragment,e),_r=d(e),rt=i(e,"P",{});var Ur=p(rt);ul=n(Ur,"Read the "),ws=i(Ur,"A",{href:!0});var ap=p(ws);hl=n(ap,"preprocessing"),ap.forEach(s),ml=n(Ur," tutorial for more details about tokenization."),Ur.forEach(s),gr=d(e),Ne=i(e,"H3",{class:!0});var Wr=p(Ne);ot=i(Wr,"A",{id:!0,class:!0,href:!0});var rp=p(ot);pa=i(rp,"SPAN",{});var op=p(pa);E(Ot.$$.fragment,op),op.forEach(s),rp.forEach(s),cl=d(Wr),fa=i(Wr,"SPAN",{});var np=p(fa);dl=n(np,"AutoModel"),np.forEach(s),Wr.forEach(s),vr=d(e),E(nt.$$.fragment,e),wr=d(e),E(lt.$$.fragment,e),kr=d(e),X=i(e,"P",{});var he=p(X);$l=n(he,"Models are a standard "),Lt=i(he,"A",{href:!0,rel:!0});var lp=p(Lt);ua=i(lp,"CODE",{});var ip=p(ua);_l=n(ip,"torch.nn.Module"),ip.forEach(s),lp.forEach(s),gl=n(he," or a "),Dt=i(he,"A",{href:!0,rel:!0});var pp=p(Dt);ha=i(pp,"CODE",{});var fp=p(ha);vl=n(fp,"tf.keras.Model"),fp.forEach(s),pp.forEach(s),wl=n(he," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ks=i(he,"A",{href:!0});var up=p(ks);kl=n(up,"Trainer"),up.forEach(s),yl=n(he," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ma=i(he,"CODE",{});var hp=p(ma);bl=n(hp,"fit"),hp.forEach(s),El=n(he," method from "),Rt=i(he,"A",{href:!0,rel:!0});var mp=p(Rt);Al=n(mp,"Keras"),mp.forEach(s),jl=n(he,". Refer to the "),ys=i(he,"A",{href:!0});var cp=p(ys);Tl=n(cp,"training tutorial"),cp.forEach(s),xl=n(he," for more details."),he.forEach(s),yr=d(e),E(it.$$.fragment,e),br=d(e),Ie=i(e,"H3",{class:!0});var Yr=p(Ie);pt=i(Yr,"A",{id:!0,class:!0,href:!0});var dp=p(pt);ca=i(dp,"SPAN",{});var $p=p(ca);E(Ht.$$.fragment,$p),$p.forEach(s),dp.forEach(s),zl=d(Yr),da=i(Yr,"SPAN",{});var _p=p(da);ql=n(_p,"Save a model"),_p.forEach(s),Yr.forEach(s),Er=d(e),E(ft.$$.fragment,e),Ar=d(e),xe=i(e,"P",{});var Fs=p(xe);Fl=n(Fs,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=i(Fs,"CODE",{});var gp=p($a);Pl=n(gp,"from_pt"),gp.forEach(s),Sl=n(Fs," or "),_a=i(Fs,"CODE",{});var vp=p(_a);Ml=n(vp,"from_tf"),vp.forEach(s),Cl=n(Fs," parameter can convert the model from one framework to the other:"),Fs.forEach(s),jr=d(e),E(ut.$$.fragment,e),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(rf)),$(m,"id","quick-tour"),$(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(m,"href","#quick-tour"),$(r,"class","relative group"),$(y,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(P,"href","./model_doc/auto"),$(Y,"id","pipeline"),$(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Y,"href","#pipeline"),$(J,"class","relative group"),$(te,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(ie,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(He,"id","pipeline-usage"),$(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(He,"href","#pipeline-usage"),$(Pe,"class","relative group"),$(Bt,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(Qt,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(gt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),$(gt,"rel","nofollow"),$(Jt,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(Kt,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(kt,"href","https://huggingface.co/docs/datasets/"),$(kt,"rel","nofollow"),$(Vt,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(Et,"href","https://huggingface.co/docs/datasets/quickstart.html"),$(Et,"rel","nofollow"),$(At,"href","https://huggingface.co/datasets/superb"),$(At,"rel","nofollow"),$(Xt,"href","./main_classes/pipelines"),$(Je,"id","use-another-model-and-tokenizer-in-the-pipeline"),$(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Je,"href","#use-another-model-and-tokenizer-in-the-pipeline"),$(Se,"class","relative group"),$(es,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(zt,"href","https://huggingface.co/models"),$(zt,"rel","nofollow"),$(ts,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(qt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),$(qt,"rel","nofollow"),$(ss,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(as,"href","./training"),$(rs,"href","./model_sharing"),$(Ve,"id","autoclass"),$(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ve,"href","#autoclass"),$(Me,"class","relative group"),$(os,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(ns,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer"),$(ls,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(is,"href","./model_doc/auto"),$(ps,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer"),$(fs,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(Ze,"id","autotokenizer"),$(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ze,"href","#autotokenizer"),$(Ce,"class","relative group"),$(us,"href","./tokenizer_summary"),$(hs,"href","/docs/transformers/pr_highlight/en/model_doc/auto#transformers.AutoTokenizer"),$($s,"href","./glossary#input-ids"),$(gs,"href",".glossary#attention-mask"),$(vs,"href","/docs/transformers/pr_highlight/en/main_classes/pipelines#transformers.pipeline"),$(ws,"href","./preprocessing"),$(ot,"id","automodel"),$(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ot,"href","#automodel"),$(Ne,"class","relative group"),$(Lt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),$(Lt,"rel","nofollow"),$(Dt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),$(Dt,"rel","nofollow"),$(ks,"href","/docs/transformers/pr_highlight/en/main_classes/trainer#transformers.Trainer"),$(Rt,"href","https://keras.io/"),$(Rt,"rel","nofollow"),$(ys,"href","./training"),$(pt,"id","save-a-model"),$(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(pt,"href","#save-a-model"),$(Ie,"class","relative group")},m(e,f){t(document.head,a),u(e,h,f),u(e,r,f),t(r,m),t(m,_),A(v,_,null),t(r,q),t(r,C),t(C,w),u(e,S,f),A(N,e,f),u(e,I,f),u(e,L,f),t(L,R),t(L,y),t(y,M),t(L,g),t(L,P),t(P,W),t(L,H),u(e,Q,f),A(G,e,f),u(e,se,f),u(e,J,f),t(J,Y),t(Y,ee),A(K,ee,null),t(J,V),t(J,me),t(me,re),u(e,de,f),u(e,oe,f),t(oe,te),t(te,ne),t(oe,$e),u(e,z,f),A(O,e,f),u(e,le,f),u(e,k,f),t(k,D),t(k,ie),t(ie,Fe),t(k,ce),u(e,ge,f),u(e,pe,f),t(pe,Le),t(Le,ve),t(pe,Yt),u(e,mt,f),u(e,B,f),t(B,Ms),t(Ms,Br),t(B,Gr),t(B,Cs),t(Cs,Qr),t(B,Jr),t(B,Ns),t(Ns,Kr),t(B,Vr),t(B,Is),t(Is,Zr),t(B,Xr),t(B,Os),t(Os,eo),t(B,to),t(B,Ls),t(Ls,so),t(B,ao),t(B,Ds),t(Ds,ro),t(B,oo),t(B,Rs),t(Rs,no),u(e,Aa,f),u(e,ct,f),t(ct,Hs),t(Hs,lo),t(ct,io),u(e,ja,f),u(e,we,f),t(we,Us),t(Us,po),t(we,fo),t(we,Ws),t(Ws,uo),t(we,ho),t(we,Ys),t(Ys,mo),u(e,Ta,f),u(e,dt,f),t(dt,Bs),t(Bs,co),t(dt,$o),u(e,xa,f),u(e,De,f),t(De,Gs),t(Gs,_o),t(De,go),t(De,Qs),t(Qs,vo),u(e,za,f),A(Re,e,f),u(e,qa,f),u(e,Pe,f),t(Pe,He),t(He,Js),A($t,Js,null),t(Pe,wo),t(Pe,Ks),t(Ks,ko),u(e,Fa,f),u(e,Ue,f),t(Ue,yo),t(Ue,Bt),t(Bt,bo),t(Ue,Eo),u(e,Pa,f),u(e,Gt,f),t(Gt,Ao),u(e,Sa,f),A(We,e,f),u(e,Ma,f),u(e,Ye,f),t(Ye,jo),t(Ye,Qt),t(Qt,To),t(Ye,xo),u(e,Ca,f),A(_t,e,f),u(e,Na,f),u(e,ke,f),t(ke,zo),t(ke,gt),t(gt,qo),t(ke,Fo),t(ke,Vs),t(Vs,Po),t(ke,So),u(e,Ia,f),A(vt,e,f),u(e,Oa,f),u(e,Be,f),t(Be,Mo),t(Be,Jt),t(Jt,Co),t(Be,No),u(e,La,f),A(wt,e,f),u(e,Da,f),u(e,ye,f),t(ye,Io),t(ye,Kt),t(Kt,Oo),t(ye,Lo),t(ye,kt),t(kt,Do),t(ye,Ro),u(e,Ra,f),A(yt,e,f),u(e,Ha,f),u(e,Ge,f),t(Ge,Ho),t(Ge,Vt),t(Vt,Uo),t(Ge,Wo),u(e,Ua,f),A(bt,e,f),u(e,Wa,f),u(e,be,f),t(be,Yo),t(be,Et),t(Et,Bo),t(be,Go),t(be,At),t(At,Qo),t(be,Jo),u(e,Ya,f),A(jt,e,f),u(e,Ba,f),u(e,Zt,f),t(Zt,Ko),u(e,Ga,f),A(Tt,e,f),u(e,Qa,f),u(e,Qe,f),t(Qe,Vo),t(Qe,Xt),t(Xt,Zo),t(Qe,Xo),u(e,Ja,f),u(e,Se,f),t(Se,Je),t(Je,Zs),A(xt,Zs,null),t(Se,en),t(Se,Xs),t(Xs,tn),u(e,Ka,f),u(e,fe,f),t(fe,sn),t(fe,es),t(es,an),t(fe,rn),t(fe,zt),t(zt,on),t(fe,nn),t(fe,ts),t(ts,ln),t(fe,pn),t(fe,qt),t(qt,fn),t(fe,un),u(e,Va,f),A(Ft,e,f),u(e,Za,f),A(Ke,e,f),u(e,Xa,f),u(e,Ee,f),t(Ee,hn),t(Ee,ss),t(ss,mn),t(Ee,cn),t(Ee,ea),t(ea,dn),t(Ee,$n),u(e,er,f),A(Pt,e,f),u(e,tr,f),u(e,Ae,f),t(Ae,_n),t(Ae,as),t(as,gn),t(Ae,vn),t(Ae,rs),t(rs,wn),t(Ae,kn),u(e,sr,f),u(e,Me,f),t(Me,Ve),t(Ve,ta),A(St,ta,null),t(Me,yn),t(Me,sa),t(sa,bn),u(e,ar,f),A(Mt,e,f),u(e,rr,f),u(e,Z,f),t(Z,En),t(Z,os),t(os,An),t(Z,jn),t(Z,ns),t(ns,Tn),t(Z,xn),t(Z,ls),t(ls,zn),t(Z,qn),t(Z,is),t(is,Fn),t(Z,Pn),t(Z,aa),t(aa,Sn),t(Z,Mn),t(Z,ps),t(ps,Cn),t(Z,Nn),u(e,or,f),u(e,je,f),t(je,In),t(je,ra),t(ra,On),t(je,Ln),t(je,fs),t(fs,Dn),t(je,Rn),u(e,nr,f),u(e,Ce,f),t(Ce,Ze),t(Ze,oa),A(Ct,oa,null),t(Ce,Hn),t(Ce,na),t(na,Un),u(e,lr,f),u(e,Te,f),t(Te,Wn),t(Te,la),t(la,Yn),t(Te,Bn),t(Te,us),t(us,Gn),t(Te,Qn),u(e,ir,f),u(e,Xe,f),t(Xe,Jn),t(Xe,hs),t(hs,Kn),t(Xe,Vn),u(e,pr,f),A(Nt,e,f),u(e,fr,f),u(e,et,f),t(et,Zn),t(et,ia),t(ia,Xn),t(et,el),u(e,ur,f),u(e,ms,f),t(ms,tl),u(e,hr,f),A(It,e,f),u(e,mr,f),u(e,cs,f),t(cs,sl),u(e,cr,f),u(e,tt,f),t(tt,ds),t(ds,$s),t($s,al),t(ds,rl),t(tt,ol),t(tt,_s),t(_s,gs),t(gs,nl),t(_s,ll),u(e,dr,f),u(e,st,f),t(st,il),t(st,vs),t(vs,pl),t(st,fl),u(e,$r,f),A(at,e,f),u(e,_r,f),u(e,rt,f),t(rt,ul),t(rt,ws),t(ws,hl),t(rt,ml),u(e,gr,f),u(e,Ne,f),t(Ne,ot),t(ot,pa),A(Ot,pa,null),t(Ne,cl),t(Ne,fa),t(fa,dl),u(e,vr,f),A(nt,e,f),u(e,wr,f),A(lt,e,f),u(e,kr,f),u(e,X,f),t(X,$l),t(X,Lt),t(Lt,ua),t(ua,_l),t(X,gl),t(X,Dt),t(Dt,ha),t(ha,vl),t(X,wl),t(X,ks),t(ks,kl),t(X,yl),t(X,ma),t(ma,bl),t(X,El),t(X,Rt),t(Rt,Al),t(X,jl),t(X,ys),t(ys,Tl),t(X,xl),u(e,yr,f),A(it,e,f),u(e,br,f),u(e,Ie,f),t(Ie,pt),t(pt,ca),A(Ht,ca,null),t(Ie,zl),t(Ie,da),t(da,ql),u(e,Er,f),A(ft,e,f),u(e,Ar,f),u(e,xe,f),t(xe,Fl),t(xe,$a),t($a,Pl),t(xe,Sl),t(xe,_a),t(_a,Ml),t(xe,Cl),u(e,jr,f),A(ut,e,f),Tr=!0},p(e,[f]){const Ut={};f&2&&(Ut.$$scope={dirty:f,ctx:e}),G.$set(Ut);const ga={};f&2&&(ga.$$scope={dirty:f,ctx:e}),Re.$set(ga);const va={};f&2&&(va.$$scope={dirty:f,ctx:e}),We.$set(va);const wa={};f&2&&(wa.$$scope={dirty:f,ctx:e}),Ke.$set(wa);const Oe={};f&2&&(Oe.$$scope={dirty:f,ctx:e}),at.$set(Oe);const ka={};f&2&&(ka.$$scope={dirty:f,ctx:e}),nt.$set(ka);const ya={};f&2&&(ya.$$scope={dirty:f,ctx:e}),lt.$set(ya);const Wt={};f&2&&(Wt.$$scope={dirty:f,ctx:e}),it.$set(Wt);const ba={};f&2&&(ba.$$scope={dirty:f,ctx:e}),ft.$set(ba);const Ea={};f&2&&(Ea.$$scope={dirty:f,ctx:e}),ut.$set(Ea)},i(e){Tr||(j(v.$$.fragment,e),j(N.$$.fragment,e),j(G.$$.fragment,e),j(K.$$.fragment,e),j(O.$$.fragment,e),j(Re.$$.fragment,e),j($t.$$.fragment,e),j(We.$$.fragment,e),j(_t.$$.fragment,e),j(vt.$$.fragment,e),j(wt.$$.fragment,e),j(yt.$$.fragment,e),j(bt.$$.fragment,e),j(jt.$$.fragment,e),j(Tt.$$.fragment,e),j(xt.$$.fragment,e),j(Ft.$$.fragment,e),j(Ke.$$.fragment,e),j(Pt.$$.fragment,e),j(St.$$.fragment,e),j(Mt.$$.fragment,e),j(Ct.$$.fragment,e),j(Nt.$$.fragment,e),j(It.$$.fragment,e),j(at.$$.fragment,e),j(Ot.$$.fragment,e),j(nt.$$.fragment,e),j(lt.$$.fragment,e),j(it.$$.fragment,e),j(Ht.$$.fragment,e),j(ft.$$.fragment,e),j(ut.$$.fragment,e),Tr=!0)},o(e){T(v.$$.fragment,e),T(N.$$.fragment,e),T(G.$$.fragment,e),T(K.$$.fragment,e),T(O.$$.fragment,e),T(Re.$$.fragment,e),T($t.$$.fragment,e),T(We.$$.fragment,e),T(_t.$$.fragment,e),T(vt.$$.fragment,e),T(wt.$$.fragment,e),T(yt.$$.fragment,e),T(bt.$$.fragment,e),T(jt.$$.fragment,e),T(Tt.$$.fragment,e),T(xt.$$.fragment,e),T(Ft.$$.fragment,e),T(Ke.$$.fragment,e),T(Pt.$$.fragment,e),T(St.$$.fragment,e),T(Mt.$$.fragment,e),T(Ct.$$.fragment,e),T(Nt.$$.fragment,e),T(It.$$.fragment,e),T(at.$$.fragment,e),T(Ot.$$.fragment,e),T(nt.$$.fragment,e),T(lt.$$.fragment,e),T(it.$$.fragment,e),T(Ht.$$.fragment,e),T(ft.$$.fragment,e),T(ut.$$.fragment,e),Tr=!1},d(e){s(a),e&&s(h),e&&s(r),x(v),e&&s(S),x(N,e),e&&s(I),e&&s(L),e&&s(Q),x(G,e),e&&s(se),e&&s(J),x(K),e&&s(de),e&&s(oe),e&&s(z),x(O,e),e&&s(le),e&&s(k),e&&s(ge),e&&s(pe),e&&s(mt),e&&s(B),e&&s(Aa),e&&s(ct),e&&s(ja),e&&s(we),e&&s(Ta),e&&s(dt),e&&s(xa),e&&s(De),e&&s(za),x(Re,e),e&&s(qa),e&&s(Pe),x($t),e&&s(Fa),e&&s(Ue),e&&s(Pa),e&&s(Gt),e&&s(Sa),x(We,e),e&&s(Ma),e&&s(Ye),e&&s(Ca),x(_t,e),e&&s(Na),e&&s(ke),e&&s(Ia),x(vt,e),e&&s(Oa),e&&s(Be),e&&s(La),x(wt,e),e&&s(Da),e&&s(ye),e&&s(Ra),x(yt,e),e&&s(Ha),e&&s(Ge),e&&s(Ua),x(bt,e),e&&s(Wa),e&&s(be),e&&s(Ya),x(jt,e),e&&s(Ba),e&&s(Zt),e&&s(Ga),x(Tt,e),e&&s(Qa),e&&s(Qe),e&&s(Ja),e&&s(Se),x(xt),e&&s(Ka),e&&s(fe),e&&s(Va),x(Ft,e),e&&s(Za),x(Ke,e),e&&s(Xa),e&&s(Ee),e&&s(er),x(Pt,e),e&&s(tr),e&&s(Ae),e&&s(sr),e&&s(Me),x(St),e&&s(ar),x(Mt,e),e&&s(rr),e&&s(Z),e&&s(or),e&&s(je),e&&s(nr),e&&s(Ce),x(Ct),e&&s(lr),e&&s(Te),e&&s(ir),e&&s(Xe),e&&s(pr),x(Nt,e),e&&s(fr),e&&s(et),e&&s(ur),e&&s(ms),e&&s(hr),x(It,e),e&&s(mr),e&&s(cs),e&&s(cr),e&&s(tt),e&&s(dr),e&&s(st),e&&s($r),x(at,e),e&&s(_r),e&&s(rt),e&&s(gr),e&&s(Ne),x(Ot),e&&s(vr),x(nt,e),e&&s(wr),x(lt,e),e&&s(kr),e&&s(X),e&&s(yr),x(it,e),e&&s(br),e&&s(Ie),x(Ht),e&&s(Er),x(ft,e),e&&s(Ar),e&&s(xe),e&&s(jr),x(ut,e)}}}const rf={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function of(F){return Ap(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cf extends kp{constructor(a){super();yp(this,a,of,af,bp,{})}}export{cf as default,rf as metadata};
