import{S as w9,i as v9,s as b9,e as a,k as h,w as p,t as r,M as _9,c as i,d as o,m as f,a as s,x as u,h as n,b as m,N as E9,F as t,g as d,y as c,q as y,o as g,B as w,v as k9}from"../chunks/vendor-6b77c823.js";import{T as T9}from"../chunks/Tip-39098574.js";import{I as ue}from"../chunks/IconCopyLink-7a11ce68.js";import{C as E}from"../chunks/CodeBlock-3a8b25a8.js";function $9(Rh){let P,nt,B,H,xe,D,po;return{c(){P=a("p"),nt=r("In case you are using Windows, you should replace "),B=a("code"),H=r("RUN_SLOW=1"),xe=r(" with "),D=a("code"),po=r("SET RUN_SLOW=1")},l(le){P=i(le,"P",{});var de=s(P);nt=n(de,"In case you are using Windows, you should replace "),B=i(de,"CODE",{});var uo=s(B);H=n(uo,"RUN_SLOW=1"),uo.forEach(o),xe=n(de," with "),D=i(de,"CODE",{});var R=s(D);po=n(R,"SET RUN_SLOW=1"),R.forEach(o),de.forEach(o)},m(le,de){d(le,P,de),t(P,nt),t(P,B),t(B,H),t(P,xe),t(P,D),t(D,po)},d(le){le&&o(P)}}}function P9(Rh){let P,nt,B,H,xe,D,po,le,de,uo,R,dc,Za,hc,fc,Ka,mc,pc,Fh,at,uc,co,cc,yc,zh,Wr,gc,Hh,W,Qa,wc,vc,Va,bc,_c,ei,Ec,kc,he,Tc,ti,$c,Pc,oi,Ic,Ac,ri,Lc,Nc,Wh,it,Bc,yo,Mc,Oc,Gh,Gr,xc,Uh,je,st,ni,go,jc,ai,Cc,Yh,Ur,qc,Jh,lt,Sc,Yr,Dc,Rc,Xh,ce,ii,Fc,zc,si,Hc,Wc,wo,Gc,li,Uc,Yc,Zh,dt,Jc,di,Xc,Zc,Kh,Jr,Kc,Qh,Ce,ht,hi,vo,Qc,fi,Vc,Vh,G,ey,Xr,ty,oy,Zr,ry,ny,mi,ay,iy,ef,Kr,sy,tf,Qr,hk,of,v,ly,pi,dy,hy,ui,fy,my,Vr,py,uy,en,cy,yy,tn,gy,wy,on,vy,by,ci,_y,Ey,yi,ky,Ty,gi,$y,Py,wi,Iy,Ay,vi,Ly,Ny,bi,By,My,rn,Oy,xy,_i,jy,Cy,Ei,qy,Sy,rf,bo,nf,I,Dy,nn,Ry,Fy,ki,zy,Hy,Ti,Wy,Gy,an,Uy,Yy,sn,Jy,Xy,af,qe,ft,$i,_o,Zy,Pi,Ky,sf,ln,Qy,lf,O,Se,Vy,Ii,eg,tg,Eo,og,rg,ng,De,ag,Ai,ig,sg,Li,lg,dg,hg,Ni,fg,mg,Re,pg,Bi,ug,cg,Mi,yg,gg,wg,Oi,vg,df,Fe,mt,xi,ko,bg,ji,_g,hf,dn,Eg,ff,ze,pt,Ci,To,kg,qi,Tg,mf,hn,$g,pf,ut,$o,Po,Pg,Ig,Io,Ag,Lg,Ao,Lo,Ng,Bg,No,Mg,uf,fn,Og,cf,ye,fe,xg,Bo,jg,Cg,Mo,qg,Sg,Si,Dg,Rg,Fg,Di,zg,Hg,Ri,Wg,yf,mn,Gg,gf,pn,Ug,wf,b,Fi,zi,Hi,Yg,Jg,Wi,un,Gi,Xg,Zg,Ui,cn,Yi,Kg,Qg,Ji,yn,Xi,Vg,ew,Zi,gn,Ki,tw,ow,Qi,wn,Vi,rw,nw,es,vn,ts,aw,iw,os,bn,rs,sw,lw,ns,_n,as,dw,hw,is,En,ss,fw,mw,ls,kn,ds,pw,uw,hs,Tn,fs,cw,yw,ms,$n,ps,gw,ww,us,Pn,cs,vw,vf,x,bw,ys,_w,Ew,gs,kw,Tw,ws,$w,Pw,vs,Iw,Aw,bf,He,ct,bs,Oo,Lw,_s,Nw,_f,yt,Bw,Es,Mw,Ow,Ef,j,We,xw,ks,jw,Cw,In,qw,Sw,Dw,Ge,Rw,Ts,Fw,zw,$s,Hw,Ww,Gw,Ps,Uw,Yw,Ue,Jw,xo,Xw,Zw,Is,Kw,Qw,Vw,As,ev,kf,An,tv,Tf,Ye,gt,Ls,jo,ov,Ns,rv,$f,wt,Bs,Co,nv,qo,av,iv,sv,Ms,So,lv,Os,dv,hv,Pf,Do,If,Ro,xs,fv,Af,Fo,Lf,Ln,mv,Nf,zo,Bf,Ho,Je,pv,js,uv,cv,Wo,yv,gv,Mf,Go,Cs,wv,vv,Of,Uo,Yo,bv,qs,_v,Ev,xf,Jo,jf,vt,kv,Ss,Tv,$v,Cf,Xe,bt,Ds,Xo,Pv,Rs,Iv,qf,A,Av,Fs,Lv,Nv,zs,Bv,Mv,Hs,Ov,xv,Ws,jv,Cv,Gs,qv,Sv,Sf,Nn,Dv,Df,_t,Rv,Us,Fv,zv,Rf,L,Ys,Hv,Wv,Js,Gv,Uv,Xs,Yv,Jv,Zs,Xv,Zv,F,Kv,Ks,Qv,Vv,Qs,eb,tb,Vs,ob,rb,el,nb,ab,ib,Ze,sb,tl,lb,db,ol,hb,fb,Ff,Et,mb,rl,pb,ub,zf,Bn,cb,Hf,Mn,yb,Wf,kt,Zo,Ko,gb,wb,Qo,vb,bb,nl,_b,Gf,On,Eb,Uf,Tt,kb,al,Tb,$b,Yf,$t,Pb,il,Ib,Ab,Jf,Vo,Xf,xn,Lb,Zf,Pt,sl,Nb,Bb,Ke,Mb,ll,Ob,xb,dl,jb,Cb,Kf,jn,qb,Qf,It,Sb,hl,Db,Rb,Vf,U,fl,Fb,zb,ml,Hb,Wb,pl,Gb,Ub,ul,Yb,em,er,tr,Jb,Xb,tm,At,Zb,or,Kb,Qb,om,Cn,Vb,rm,qn,e_,nm,N,cl,t_,o_,yl,r_,n_,gl,a_,i_,wl,s_,l_,vl,d_,h_,bl,f_,am,Qe,m_,_l,p_,u_,El,c_,im,Sn,y_,sm,rr,lm,ge,g_,kl,w_,v_,Tl,b_,__,dm,C,z,E_,nr,k_,T_,$l,$_,P_,Pl,I_,A_,ar,L_,N_,B_,Il,M_,O_,T,x_,Al,j_,C_,Ll,q_,S_,Nl,D_,R_,Bl,F_,z_,Ml,H_,W_,Ol,G_,U_,xl,Y_,J_,jl,X_,Z_,Cl,K_,Q_,ql,V_,e1,t1,ir,o1,Sl,r1,n1,a1,me,i1,Dl,s1,l1,Rl,d1,h1,Fl,f1,m1,hm,Lt,p1,zl,u1,c1,fm,Ve,Nt,Hl,sr,y1,Wl,g1,mm,Dn,w1,pm,lr,um,Bt,v1,Rn,b1,_1,cm,Fn,E1,ym,Mt,zn,Gl,k1,T1,$1,Hn,Ul,P1,I1,gm,we,A1,Yl,L1,N1,dr,B1,M1,wm,Wn,Jl,O1,vm,ve,x1,Xl,j1,C1,Zl,q1,S1,bm,Gn,D1,_m,Un,Kl,R1,Em,hr,km,fr,Ql,F1,Tm,mr,$m,pr,Vl,z1,Pm,ur,Im,cr,ed,H1,Am,yr,Lm,et,td,od,W1,G1,rd,nd,U1,Nm,Yn,Y1,Bm,gr,Mm,Jn,J1,Om,Xn,X1,xm,Zn,Z1,jm,Kn,ad,K1,Cm,be,Q1,id,V1,e0,sd,t0,o0,qm,Y,r0,ld,n0,a0,dd,i0,s0,hd,l0,d0,Sm,M,fd,h0,f0,md,m0,p0,pd,u0,c0,ud,y0,g0,cd,w0,v0,Dm,wr,Rm,_e,b0,yd,_0,E0,gd,k0,T0,Fm,Qn,wd,$0,zm,J,P0,vd,I0,A0,bd,L0,N0,_d,B0,M0,Hm,Ot,Vn,O0,vr,x0,j0,ea,C0,br,q0,Wm,xt,S0,Ed,D0,R0,Gm,_r,Um,X,F0,kd,z0,H0,Td,W0,G0,$d,U0,Y0,Ym,Er,Jm,ta,J0,Xm,kr,Zm,oa,X0,Km,Tr,Qm,ra,Z0,Vm,$r,ep,Pr,K0,Pd,Q0,tp,Ir,op,Ee,V0,Id,e2,t2,Ad,o2,r2,rp,Ar,np,Lr,n2,Ld,a2,ap,Nr,ip,na,i2,sp,jt,s2,Nd,l2,d2,lp,Z,h2,Bd,f2,m2,Md,p2,u2,Od,c2,y2,dp,K,g2,xd,w2,v2,jd,b2,_2,Cd,E2,k2,hp,Br,fp,aa,qd,T2,mp,Ct,$2,ia,P2,I2,pp,Mr,up,q,A2,Sd,L2,N2,Dd,B2,M2,Rd,O2,x2,Fd,j2,C2,cp,Q,q2,zd,S2,D2,Hd,R2,F2,Wd,z2,H2,yp,V,tt,W2,Gd,G2,U2,Ud,Y2,J2,X2,Yd,Z2,K2,Jd,Q2,V2,ee,eE,Xd,tE,oE,Zd,rE,nE,Kd,aE,iE,Or,sE,gp,ke,lE,Qd,dE,hE,Vd,fE,mE,wp,qt,pE,eh,uE,cE,vp,sa,th,yE,bp,St,gE,oh,wE,vE,_p,xr,Ep,la,bE,kp,Dt,da,_E,rh,EE,kE,nh,TE,Tp,Rt,$E,ah,PE,IE,$p,jr,Pp,Ft,Ip,te,AE,ih,LE,NE,sh,BE,ME,lh,OE,xE,Ap,zt,Cr,jE,dh,CE,qE,SE,hh,DE,Lp,ha,fh,RE,Np,Ht,FE,mh,zE,HE,Bp,fa,WE,Mp,ma,GE,Op,qr,xp,Wt,UE,ph,YE,JE,jp,Sr,Cp,Gt,XE,uh,ZE,KE,qp,Te,QE,ch,VE,e3,yh,t3,o3,Sp,pa,gh,r3,Dp,oe,n3,wh,a3,i3,vh,s3,l3,bh,d3,h3,Rp,ua,_h,f3,Fp,re,m3,Eh,p3,u3,kh,c3,y3,Th,g3,w3,zp,$e,v3,$h,b3,_3,ca,E3,k3,Hp,ya,Ph,T3,Wp,Ut,$3,Ih,P3,I3,Gp,Dr,Up,ga,A3,Yp,Rr,Jp,wa,L3,Xp,va,N3,Zp,ba,B3,Kp,_a,Ah,M3,Qp,S,O3,Ea,x3,j3,Lh,C3,q3,Nh,S3,D3,Bh,R3,F3,Vp,Fr,eu,Yt,z3,Mh,H3,W3,tu,ka,Oh,G3,ou,Jt,U3,xh,Y3,J3,ru,Ta,jh,X3,nu,$a,Z3,au,ot,Xt,Ch,zr,K3,qh,Q3,iu,Pa,V3,su,Ia,Sh,ek,lu;return D=new ue({}),go=new ue({}),vo=new ue({}),bo=new E({props:{code:`model = BrandNewBertModel.from_pretrained("brandy/brand_new_bert")
model.config  # model has access to its config`,highlighted:`model = BrandNewBertModel.from_pretrained(<span class="hljs-string">&quot;brandy/brand_new_bert&quot;</span>)
model.config  <span class="hljs-comment"># model has access to its config</span>`}}),_o=new ue({}),ko=new ue({}),To=new ue({}),Oo=new ue({}),jo=new ue({}),Do=new E({props:{code:`git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/[your Github handle]/transformers.git
<span class="hljs-built_in">cd</span> transformers
git remote add upstream https://github.com/huggingface/transformers.git`}}),Fo=new E({props:{code:`python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"`,highlighted:`python -m venv .<span class="hljs-built_in">env</span>
<span class="hljs-built_in">source</span> .<span class="hljs-built_in">env</span>/bin/activate
pip install -e <span class="hljs-string">&quot;.[dev]&quot;</span>`}}),zo=new E({props:{code:"cd ..",highlighted:'<span class="hljs-built_in">cd</span> ..'}}),Jo=new E({props:{code:`git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git 
cd brand_new_bert
pip install -e .`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git 
<span class="hljs-built_in">cd</span> brand_new_bert
pip install -e .`}}),Xo=new ue({}),Vo=new E({props:{code:`model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids
original_output = model.predict(input_ids)`,highlighted:`model = BrandNewBertModel.load_pretrained_checkpoint(<span class="hljs-string">&quot;/path/to/checkpoint/&quot;</span>)
input_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]  <span class="hljs-comment"># vector of input ids</span>
original_output = model.predict(input_ids)`}}),rr=new E({props:{code:`[[
 [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],
 [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],
 [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],
 ...,
 [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],
 [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],
 [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],`,highlighted:`<span class="hljs-comment">[<span class="hljs-comment">[
 <span class="hljs-comment">[-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024]</span>,
 <span class="hljs-comment">[-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132]</span>,
 <span class="hljs-comment">[-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648]</span>,
 ...,
 <span class="hljs-comment">[-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288]</span>,
 <span class="hljs-comment">[-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191]</span>,
 <span class="hljs-comment">[-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]</span>]</span>]</span>,`}}),sr=new ue({}),lr=new E({props:{code:"cd transformers",highlighted:'<span class="hljs-built_in">cd</span> transformers'}}),hr=new E({props:{code:"git checkout -b add_brand_new_bert",highlighted:"git checkout -b add_brand_new_bert"}}),mr=new E({props:{code:`git add .
git commit`,highlighted:`git add .
git commit`}}),ur=new E({props:{code:`git fetch upstream
git rebase upstream/main`,highlighted:`git fetch upstream
git rebase upstream/main`}}),yr=new E({props:{code:"git push -u origin a-descriptive-name-for-my-changes",highlighted:"git push -u origin a-descriptive-name-for-my-changes"}}),gr=new E({props:{code:`git fetch upstream
git merge upstream/main`,highlighted:`git fetch upstream
git merge upstream/main`}}),wr=new E({props:{code:`from transformers import BrandNewBertModel, BrandNewBertConfig

model = BrandNewBertModel(BrandNewBertConfig())`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BrandNewBertModel, BrandNewBertConfig

model = BrandNewBertModel(BrandNewBertConfig())`}}),_r=new E({props:{code:`from torch import nn


class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(10, 10)
        self.intermediate = nn.Linear(10, 10)
        self.layer_norm = nn.LayerNorm(10)`,highlighted:`<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn


<span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleModel</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.dense = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
        self.intermediate = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
        self.layer_norm = nn.LayerNorm(<span class="hljs-number">10</span>)`}}),Er=new E({props:{code:`model = SimpleModel()

print(model)`,highlighted:`model = SimpleModel()

<span class="hljs-built_in">print</span>(model)`}}),kr=new E({props:{code:`SimpleModel(
  (dense): Linear(in_features=10, out_features=10, bias=True)
  (intermediate): Linear(in_features=10, out_features=10, bias=True)
  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
)`,highlighted:`SimpleModel(
  (dense): Linear(<span class="hljs-attribute">in_features</span>=10, <span class="hljs-attribute">out_features</span>=10, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)
  (intermediate): Linear(<span class="hljs-attribute">in_features</span>=10, <span class="hljs-attribute">out_features</span>=10, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)
  (layer_norm): LayerNorm((10,), <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">elementwise_affine</span>=<span class="hljs-literal">True</span>)
)`}}),Tr=new E({props:{code:"print(model.dense.weight.data)",highlighted:'<span class="hljs-built_in">print</span>(model.dense.weight.data)'}}),$r=new E({props:{code:`tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,
         -0.2077,  0.2157],
        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,
          0.2166, -0.0212],
        [-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,
         -0.1023, -0.0447],
        [-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,
         -0.1876, -0.2467],
        [ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,
          0.2577,  0.0402],
        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,
          0.2132,  0.1680],
        [ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,
          0.2707, -0.2509],
        [-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,
          0.1829, -0.1568],
        [-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,
          0.0333, -0.0536],
        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,
          0.2220,  0.2358]]).`,highlighted:`tensor([[<span class="hljs-string">-0</span>.0818,  0.2207, <span class="hljs-string">-0</span>.0749, <span class="hljs-string">-0</span>.0030,  0.0045, <span class="hljs-string">-0</span>.1569, <span class="hljs-string">-0</span>.1598,  0.0212,
         <span class="hljs-string">-0</span>.2077,  0.2157],
        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, <span class="hljs-string">-0</span>.2190,
          0.2166, <span class="hljs-string">-0</span>.0212],
        [<span class="hljs-string">-0</span>.2000,  0.1107, <span class="hljs-string">-0</span>.1999, <span class="hljs-string">-0</span>.3119,  0.1559,  0.0993,  0.1776, <span class="hljs-string">-0</span>.1950,
         <span class="hljs-string">-0</span>.1023, <span class="hljs-string">-0</span>.0447],
        [<span class="hljs-string">-0</span>.0888, <span class="hljs-string">-0</span>.1092,  0.2281,  0.0336,  0.1817, <span class="hljs-string">-0</span>.0115,  0.2096,  0.1415,
         <span class="hljs-string">-0</span>.1876, <span class="hljs-string">-0</span>.2467],
        [ 0.2208, <span class="hljs-string">-0</span>.2352, <span class="hljs-string">-0</span>.1426, <span class="hljs-string">-0</span>.2636, <span class="hljs-string">-0</span>.2889, <span class="hljs-string">-0</span>.2061, <span class="hljs-string">-0</span>.2849, <span class="hljs-string">-0</span>.0465,
          0.2577,  0.0402],
        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, <span class="hljs-string">-0</span>.0530,  0.1859, <span class="hljs-string">-0</span>.0604,
          0.2132,  0.1680],
        [ 0.1733, <span class="hljs-string">-0</span>.2407, <span class="hljs-string">-0</span>.1721,  0.1484,  0.0358, <span class="hljs-string">-0</span>.0633, <span class="hljs-string">-0</span>.0721, <span class="hljs-string">-0</span>.0090,
          0.2707, <span class="hljs-string">-0</span>.2509],
        [<span class="hljs-string">-0</span>.1173,  0.1561,  0.2945,  0.0595, <span class="hljs-string">-0</span>.1996,  0.2988, <span class="hljs-string">-0</span>.0802,  0.0407,
          0.1829, <span class="hljs-string">-0</span>.1568],
        [<span class="hljs-string">-0</span>.1164, <span class="hljs-string">-0</span>.2228, <span class="hljs-string">-0</span>.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,
          0.0333, <span class="hljs-string">-0</span>.0536],
        [<span class="hljs-string">-0</span>.1492, <span class="hljs-string">-0</span>.1616,  0.1057,  0.1950, <span class="hljs-string">-0</span>.2807, <span class="hljs-string">-0</span>.2710, <span class="hljs-string">-0</span>.1586,  0.0739,
          0.2220,  0.2358]]).`}}),Ir=new E({props:{code:`# retrieve matching layer weights, e.g. by
# recursive algorithm
layer_name = "dense"
pretrained_weight = array_of_dense_layer

model_pointer = getattr(model, "dense")

model_pointer.weight.data = torch.from_numpy(pretrained_weight)`,highlighted:`<span class="hljs-comment"># retrieve matching layer weights, e.g. by</span>
<span class="hljs-comment"># recursive algorithm</span>
layer_name = <span class="hljs-string">&quot;dense&quot;</span>
pretrained_weight = array_of_dense_layer

model_pointer = <span class="hljs-built_in">getattr</span>(model, <span class="hljs-string">&quot;dense&quot;</span>)

model_pointer.weight.data = torch.from_numpy(pretrained_weight)`}}),Ar=new E({props:{code:`assert (
    model_pointer.weight.shape == pretrained_weight.shape
), f"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched"`,highlighted:`<span class="hljs-keyword">assert</span> (
    model_pointer.weight.shape == pretrained_weight.shape
), <span class="hljs-string">f&quot;Pointer shape of random weight <span class="hljs-subst">{model_pointer.shape}</span> and array shape of checkpoint weight <span class="hljs-subst">{pretrained_weight.shape}</span> mismatched&quot;</span>`}}),Nr=new E({props:{code:'logger.info(f"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}")',highlighted:'logger.info(<span class="hljs-string">f&quot;Initialize PyTorch weight <span class="hljs-subst">{layer_name}</span> from <span class="hljs-subst">{pretrained_weight.name}</span>&quot;</span>)'}}),Br=new E({props:{code:'model.save_pretrained("/path/to/converted/checkpoint/folder")',highlighted:'model.save_pretrained(<span class="hljs-string">&quot;/path/to/converted/checkpoint/folder&quot;</span>)'}}),Mr=new E({props:{code:`model = BrandNewBertModel.from_pretrained("/path/to/converted/checkpoint/folder")
input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]
output = model(input_ids).last_hidden_states`,highlighted:`model = BrandNewBertModel.from_pretrained(<span class="hljs-string">&quot;/path/to/converted/checkpoint/folder&quot;</span>)
input_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">19</span>]
output = model(input_ids).last_hidden_states`}}),xr=new E({props:{code:"pytest tests/test_modeling_brand_new_bert.py",highlighted:"pytest tests/test_modeling_brand_new_bert.py"}}),jr=new E({props:{code:"RUN_SLOW=1 pytest -sv tests/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests",highlighted:"RUN_SLOW=1 pytest -sv tests/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests"}}),Ft=new T9({props:{$$slots:{default:[$9]},$$scope:{ctx:Rh}}}),qr=new E({props:{code:`input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."
model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = model.tokenize(input_str)`,highlighted:`input_str = <span class="hljs-string">&quot;This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.&quot;</span>
model = BrandNewBertModel.load_pretrained_checkpoint(<span class="hljs-string">&quot;/path/to/checkpoint/&quot;</span>)
input_ids = model.tokenize(input_str)`}}),Sr=new E({props:{code:`from transformers import BrandNewBertTokenizer

input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

tokenizer = BrandNewBertTokenizer.from_pretrained("/path/to/tokenizer/folder/")

input_ids = tokenizer(input_str).input_ids`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BrandNewBertTokenizer

input_str = <span class="hljs-string">&quot;This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.&quot;</span>

tokenizer = BrandNewBertTokenizer.from_pretrained(<span class="hljs-string">&quot;/path/to/tokenizer/folder/&quot;</span>)

input_ids = tokenizer(input_str).input_ids`}}),Dr=new E({props:{code:"make style",highlighted:"make style"}}),Rr=new E({props:{code:"make quality",highlighted:"make quality"}}),Fr=new E({props:{code:`brand_new_bert.push_to_hub(
    repo_path_or_name="brand_new_bert",
    # Uncomment the following line to push to an organization
    # organization="<ORGANIZATION>",
    commit_message="Add model",
    use_temp_dir=True,
)`,highlighted:`brand_new_bert.push_to_hub(
    repo_path_or_name=<span class="hljs-string">&quot;brand_new_bert&quot;</span>,
    <span class="hljs-comment"># Uncomment the following line to push to an organization</span>
    <span class="hljs-comment"># organization=&quot;&lt;ORGANIZATION&gt;&quot;,</span>
    commit_message=<span class="hljs-string">&quot;Add model&quot;</span>,
    use_temp_dir=<span class="hljs-literal">True</span>,
)`}}),zr=new ue({}),{c(){P=a("meta"),nt=h(),B=a("h1"),H=a("a"),xe=a("span"),p(D.$$.fragment),po=h(),le=a("span"),de=r("How to add a model to \u{1F917} Transformers?"),uo=h(),R=a("p"),dc=r(`Adding a new model is often difficult and requires an in-depth knowledge of the \u{1F917} Transformers library and ideally also
of the model\u2019s original repository. At Hugging Face, we are trying to empower the community more and more to add models
independently. Thus, for some new models that the community wants to be added to \u{1F917} Transformers, we create a customized
`),Za=a("em"),hc=r("call-for-model-addition"),fc=r(` that explains step-by-step how to add the requested model. With this
`),Ka=a("em"),mc=r("call-for-model-addition"),pc=r(`, we want to teach a motivated and experienced contributor of the community how to port a
model to \u{1F917} Transformers.`),Fh=h(),at=a("p"),uc=r(`If this sounds like something you would be interested in, feel free to check out the currently open
\u201Ccalls-for-model-addition\u201D `),co=a("a"),cc=r("here"),yc=r(`
and to contact us.`),zh=h(),Wr=a("p"),gc=r(`If selected, you will then work closely with one member of the Hugging Face team to integrate the model into \u{1F917}
Transformers. By doing so, you will both gain a theoretical and deep practical understanding of the proposed model. But
more importantly, you will have made a major open-source contribution to \u{1F917} Transformers. Along the way, you will:`),Hh=h(),W=a("ul"),Qa=a("li"),wc=r("get insights into open-source best practices"),vc=h(),Va=a("li"),bc=r("understand the design principles of one of the most popular NLP libraries"),_c=h(),ei=a("li"),Ec=r("learn how to do efficiently test large NLP models"),kc=h(),he=a("li"),Tc=r("learn how to integrate Python utilities like "),ti=a("code"),$c=r("black"),Pc=r(", "),oi=a("code"),Ic=r("isort"),Ac=r(", "),ri=a("code"),Lc=r("make fix-copies"),Nc=r(` into a library to always
ensure clean and readable code`),Wh=h(),it=a("p"),Bc=r(`We are also more than happy if you want to add a model that cannot be found in the \u201Ccalls-for-model-addition\u201D folder.
The following sections explain in detail how to add a new model. It might also be very helpful to check out already
added models to see if those resemble the model you would like to add `),yo=a("a"),Mc=r("here"),Oc=r("."),Gh=h(),Gr=a("p"),xc=r("To start, let\u2019s try to get a general overview of the Transformers library."),Uh=h(),je=a("h2"),st=a("a"),ni=a("span"),p(go.$$.fragment),jc=h(),ai=a("span"),Cc=r("General overview of \u{1F917} Transformers"),Yh=h(),Ur=a("p"),qc=r(`First, you should get a general overview of \u{1F917} Transformers. \u{1F917} Transformers is a very opinionated library, so there is a
chance that you don\u2019t agree with some of the library\u2019s philosophies or design choices. From our experience, however, we
found that the fundamental design choices and philosophies of the library are crucial to efficiently scale \u{1F917}
Transformers while keeping maintenance costs at a reasonable level.`),Jh=h(),lt=a("p"),Sc=r("A good first starting point to better understand the library is to read the "),Yr=a("a"),Dc=r("documentation of our philosophy"),Rc=r(". As a result of our way of working, there are some choices that we try to apply to all models:"),Xh=h(),ce=a("ul"),ii=a("li"),Fc=r("Composition is generally favored over-abstraction"),zc=h(),si=a("li"),Hc=r("Duplicating code is not always bad if it strongly improves the readability or accessibility of a model"),Wc=h(),wo=a("li"),Gc=r(`Model files are as self-contained as possible so that when you read the code of a specific model, you ideally only
have to look into the respective `),li=a("code"),Uc=r("modeling_....py"),Yc=r(" file."),Zh=h(),dt=a("p"),Jc=r("In our opinion, the library\u2019s code is not just a means to provide a product, "),di=a("em"),Xc=r("e.g."),Zc=r(` the ability to use BERT for
inference, but also as the very product that we want to improve. Hence, when adding a model, the user is not only the
person that will use your model, but also everybody that will read, try to understand, and possibly tweak your code.`),Kh=h(),Jr=a("p"),Kc=r("With this in mind, let\u2019s go a bit deeper into the general library design."),Qh=h(),Ce=a("h3"),ht=a("a"),hi=a("span"),p(vo.$$.fragment),Qc=h(),fi=a("span"),Vc=r("Overview of models"),Vh=h(),G=a("p"),ey=r(`To successfully add a model, it is important to understand the interaction between your model and its config,
`),Xr=a("a"),ty=r("PreTrainedModel"),oy=r(", and "),Zr=a("a"),ry=r("PretrainedConfig"),ny=r(`. For exemplary purposes, we will
call the model to be added to \u{1F917} Transformers `),mi=a("code"),ay=r("BrandNewBert"),iy=r("."),ef=h(),Kr=a("p"),sy=r("Let\u2019s take a look:"),tf=h(),Qr=a("img"),of=h(),v=a("p"),ly=r(`As you can see, we do make use of inheritance in \u{1F917} Transformers, but we keep the level of abstraction to an absolute
minimum. There are never more than two levels of abstraction for any model in the library. `),pi=a("code"),dy=r("BrandNewBertModel"),hy=r(`
inherits from `),ui=a("code"),fy=r("BrandNewBertPreTrainedModel"),my=r(" which in turn inherits from "),Vr=a("a"),py=r("PreTrainedModel"),uy=r(` and
that\u2019s it. As a general rule, we want to make sure that a new model only depends on
`),en=a("a"),cy=r("PreTrainedModel"),yy=r(`. The important functionalities that are automatically provided to every new
model are `),tn=a("a"),gy=r("from_pretrained()"),wy=r(` and
`),on=a("a"),vy=r("save_pretrained()"),by=r(`, which are used for serialization and deserialization. All of the
other important functionalities, such as `),ci=a("code"),_y=r("BrandNewBertModel.forward"),Ey=r(` should be completely defined in the new
`),yi=a("code"),ky=r("modeling_brand_new_bert.py"),Ty=r(` script. Next, we want to make sure that a model with a specific head layer, such as
`),gi=a("code"),$y=r("BrandNewBertForMaskedLM"),Py=r(" does not inherit from "),wi=a("code"),Iy=r("BrandNewBertModel"),Ay=r(", but rather uses "),vi=a("code"),Ly=r("BrandNewBertModel"),Ny=r(`
as a component that can be called in its forward pass to keep the level of abstraction low. Every new model requires a
configuration class, called `),bi=a("code"),By=r("BrandNewBertConfig"),My=r(`. This configuration is always stored as an attribute in
`),rn=a("a"),Oy=r("PreTrainedModel"),xy=r(", and thus can be accessed via the "),_i=a("code"),jy=r("config"),Cy=r(` attribute for all classes
inheriting from `),Ei=a("code"),qy=r("BrandNewBertPreTrainedModel"),Sy=r(":"),rf=h(),p(bo.$$.fragment),nf=h(),I=a("p"),Dy=r(`Similar to the model, the configuration inherits basic serialization and deserialization functionalities from
`),nn=a("a"),Ry=r("PretrainedConfig"),Fy=r(`. Note that the configuration and the model are always serialized into two
different formats - the model to a `),ki=a("em"),zy=r("pytorch_model.bin"),Hy=r(" file and the configuration to a "),Ti=a("em"),Wy=r("config.json"),Gy=r(` file. Calling
`),an=a("a"),Uy=r("save_pretrained()"),Yy=r(` will automatically call
`),sn=a("a"),Jy=r("save_pretrained()"),Xy=r(", so that both model and configuration are saved."),af=h(),qe=a("h3"),ft=a("a"),$i=a("span"),p(_o.$$.fragment),Zy=h(),Pi=a("span"),Ky=r("Code style"),sf=h(),ln=a("p"),Qy=r(`When coding your new model, keep in mind that Transformers is an opinionated library and we have a few quirks of our
own regarding how code should be written :-)`),lf=h(),O=a("ol"),Se=a("li"),Vy=r(`The forward pass of your model should be fully written in the modeling file while being fully independent of other
models in the library. If you want to reuse a block from another model, copy the code and paste it with a
`),Ii=a("code"),eg=r("# Copied from"),tg=r(" comment on top (see "),Eo=a("a"),og=r("here"),rg=r(`
for a good example).`),ng=h(),De=a("li"),ag=r(`The code should be fully understandable, even by a non-native English speaker. This means you should pick
descriptive variable names and avoid abbreviations. As an example, `),Ai=a("code"),ig=r("activation"),sg=r(" is preferred to "),Li=a("code"),lg=r("act"),dg=r(`.
One-letter variable names are strongly discouraged unless it\u2019s an index in a for loop.`),hg=h(),Ni=a("li"),fg=r("More generally we prefer longer explicit code to short magical one."),mg=h(),Re=a("li"),pg=r("Avoid subclassing "),Bi=a("code"),ug=r("nn.Sequential"),cg=r(" in PyTorch but subclass "),Mi=a("code"),yg=r("nn.Module"),gg=r(` and write the forward pass, so that anyone
using your code can quickly debug it by adding print statements or breaking points.`),wg=h(),Oi=a("li"),vg=r(`Your function signature should be type-annotated. For the rest, good variable names are way more readable and
understandable than type annotations.`),df=h(),Fe=a("h3"),mt=a("a"),xi=a("span"),p(ko.$$.fragment),bg=h(),ji=a("span"),_g=r("Overview of tokenizers"),hf=h(),dn=a("p"),Eg=r("Not quite ready yet :-( This section will be added soon!"),ff=h(),ze=a("h2"),pt=a("a"),Ci=a("span"),p(To.$$.fragment),kg=h(),qi=a("span"),Tg=r("Step-by-step recipe to add a model to \u{1F917} Transformers"),mf=h(),hn=a("p"),$g=r(`Everyone has different preferences of how to port a model so it can be very helpful for you to take a look at summaries
of how other contributors ported models to Hugging Face. Here is a list of community blog posts on how to port a model:`),pf=h(),ut=a("ol"),$o=a("li"),Po=a("a"),Pg=r("Porting GPT2 Model"),Ig=r(" by "),Io=a("a"),Ag=r("Thomas"),Lg=h(),Ao=a("li"),Lo=a("a"),Ng=r("Porting WMT19 MT Model"),Bg=r(" by "),No=a("a"),Mg=r("Stas"),uf=h(),fn=a("p"),Og=r("From experience, we can tell you that the most important things to keep in mind when adding a model are:"),cf=h(),ye=a("ul"),fe=a("li"),xg=r(`Don\u2019t reinvent the wheel! Most parts of the code you will add for the new \u{1F917} Transformers model already exist
somewhere in \u{1F917} Transformers. Take some time to find similar, already existing models and tokenizers you can copy
from. `),Bo=a("a"),jg=r("grep"),Cg=r(" and "),Mo=a("a"),qg=r("rg"),Sg=r(` are your
friends. Note that it might very well happen that your model\u2019s tokenizer is based on one model implementation, and
your model\u2019s modeling code on another one. `),Si=a("em"),Dg=r("E.g."),Rg=r(` FSMT\u2019s modeling code is based on BART, while FSMT\u2019s tokenizer code
is based on XLM.`),Fg=h(),Di=a("li"),zg=r(`It\u2019s more of an engineering challenge than a scientific challenge. You should spend more time on creating an
efficient debugging environment than trying to understand all theoretical aspects of the model in the paper.`),Hg=h(),Ri=a("li"),Wg=r(`Ask for help, when you\u2019re stuck! Models are the core component of \u{1F917} Transformers so that we at Hugging Face are more
than happy to help you at every step to add your model. Don\u2019t hesitate to ask if you notice you are not making
progress.`),yf=h(),mn=a("p"),Gg=r("In the following, we try to give you a general recipe that we found most useful when porting a model to \u{1F917} Transformers."),gf=h(),pn=a("p"),Ug=r(`The following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do
List:`),wf=h(),b=a("ul"),Fi=a("li"),zi=a("ol"),Hi=a("li"),Yg=r("\u2610 (Optional) Understood theoretical aspects"),Jg=h(),Wi=a("li"),un=a("ol"),Gi=a("li"),Xg=r("\u2610 Prepared transformers dev environment"),Zg=h(),Ui=a("li"),cn=a("ol"),Yi=a("li"),Kg=r("\u2610 Set up debugging environment of the original repository"),Qg=h(),Ji=a("li"),yn=a("ol"),Xi=a("li"),Vg=r("\u2610 Created script that successfully runs forward pass using original repository and checkpoint"),ew=h(),Zi=a("li"),gn=a("ol"),Ki=a("li"),tw=r("\u2610 Successfully added the model skeleton to Transformers"),ow=h(),Qi=a("li"),wn=a("ol"),Vi=a("li"),rw=r("\u2610 Successfully converted original checkpoint to Transformers checkpoint"),nw=h(),es=a("li"),vn=a("ol"),ts=a("li"),aw=r("\u2610 Successfully ran forward pass in Transformers that gives identical output to original checkpoint"),iw=h(),os=a("li"),bn=a("ol"),rs=a("li"),sw=r("\u2610 Finished model tests in Transformers"),lw=h(),ns=a("li"),_n=a("ol"),as=a("li"),dw=r("\u2610 Successfully added Tokenizer in Transformers"),hw=h(),is=a("li"),En=a("ol"),ss=a("li"),fw=r("\u2610 Run end-to-end integration tests"),mw=h(),ls=a("li"),kn=a("ol"),ds=a("li"),pw=r("\u2610 Finished docs"),uw=h(),hs=a("li"),Tn=a("ol"),fs=a("li"),cw=r("\u2610 Uploaded model weights to the hub"),yw=h(),ms=a("li"),$n=a("ol"),ps=a("li"),gw=r("\u2610 Submitted the pull request"),ww=h(),us=a("li"),Pn=a("ol"),cs=a("li"),vw=r("\u2610 (Optional) Added a demo notebook"),vf=h(),x=a("p"),bw=r("To begin with, we usually recommend to start by getting a good theoretical understanding of "),ys=a("code"),_w=r("BrandNewBert"),Ew=r(`. However,
if you prefer to understand the theoretical aspects of the model `),gs=a("em"),kw=r("on-the-job"),Tw=r(`, then it is totally fine to directly dive
into the `),ws=a("code"),$w=r("BrandNewBert"),Pw=r(`\u2019s code-base. This option might suit you better, if your engineering skills are better than
your theoretical skill, if you have trouble understanding `),vs=a("code"),Iw=r("BrandNewBert"),Aw=r(`\u2019s paper, or if you just enjoy programming
much more than reading scientific papers.`),bf=h(),He=a("h3"),ct=a("a"),bs=a("span"),p(Oo.$$.fragment),Lw=h(),_s=a("span"),Nw=r("1. (Optional) Theoretical aspects of BrandNewBert"),_f=h(),yt=a("p"),Bw=r("You should take some time to read "),Es=a("em"),Mw=r("BrandNewBert\u2019s"),Ow=r(` paper, if such descriptive work exists. There might be large
sections of the paper that are difficult to understand. If this is the case, this is fine - don\u2019t worry! The goal is
not to get a deep theoretical understanding of the paper, but to extract the necessary information required to
effectively re-implement the model in \u{1F917} Transformers. That being said, you don\u2019t have to spend too much time on the
theoretical aspects, but rather focus on the practical ones, namely:`),Ef=h(),j=a("ul"),We=a("li"),xw=r("What type of model is "),ks=a("em"),jw=r("brand_new_bert"),Cw=r(`? BERT-like encoder-only model? GPT2-like decoder-only model? BART-like
encoder-decoder model? Look at the `),In=a("a"),qw=r("model_summary"),Sw=r(" if you\u2019re not familiar with the differences between those."),Dw=h(),Ge=a("li"),Rw=r("What are the applications of "),Ts=a("em"),Fw=r("brand_new_bert"),zw=r("? Text classification? Text generation? Seq2Seq tasks, "),$s=a("em"),Hw=r("e.g.,"),Ww=r(`
summarization?`),Gw=h(),Ps=a("li"),Uw=r("What is the novel feature of the model making it different from BERT/GPT-2/BART?"),Yw=h(),Ue=a("li"),Jw=r("Which of the already existing "),xo=a("a"),Xw=r("\u{1F917} Transformers models"),Zw=r(` is most
similar to `),Is=a("em"),Kw=r("brand_new_bert"),Qw=r("?"),Vw=h(),As=a("li"),ev=r(`What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer? Is it the same tokenizer as used
for BERT or BART?`),kf=h(),An=a("p"),tv=r(`After you feel like you have gotten a good overview of the architecture of the model, you might want to write to the
Hugging Face team with any questions you might have. This might include questions regarding the model\u2019s architecture,
its attention layer, etc. We will be more than happy to help you.`),Tf=h(),Ye=a("h3"),gt=a("a"),Ls=a("span"),p(jo.$$.fragment),ov=h(),Ns=a("span"),rv=r("2. Next prepare your environment"),$f=h(),wt=a("ol"),Bs=a("li"),Co=a("p"),nv=r("Fork the "),qo=a("a"),av=r("repository"),iv=r(` by clicking on the \u2018Fork\u2019 button on the
repository\u2019s page. This creates a copy of the code under your GitHub user account.`),sv=h(),Ms=a("li"),So=a("p"),lv=r("Clone your "),Os=a("code"),dv=r("transformers"),hv=r(" fork to your local disk, and add the base repository as a remote:"),Pf=h(),p(Do.$$.fragment),If=h(),Ro=a("ol"),xs=a("li"),fv=r("Set up a development environment, for instance by running the following command:"),Af=h(),p(Fo.$$.fragment),Lf=h(),Ln=a("p"),mv=r("and return to the parent directory"),Nf=h(),p(zo.$$.fragment),Bf=h(),Ho=a("ol"),Je=a("li"),pv=r("We recommend adding the PyTorch version of "),js=a("em"),uv=r("brand_new_bert"),cv=r(` to Transformers. To install PyTorch, please follow the
instructions on `),Wo=a("a"),yv=r("https://pytorch.org/get-started/locally/"),gv=r("."),Mf=h(),Go=a("p"),Cs=a("strong"),wv=r("Note:"),vv=r(" You don\u2019t need to have CUDA installed. Making the new model work on CPU is sufficient."),Of=h(),Uo=a("ol"),Yo=a("li"),bv=r("To port "),qs=a("em"),_v=r("brand_new_bert"),Ev=r(", you will also need access to its original repository:"),xf=h(),p(Jo.$$.fragment),jf=h(),vt=a("p"),kv=r("Now you have set up a development environment to port "),Ss=a("em"),Tv=r("brand_new_bert"),$v=r(" to \u{1F917} Transformers."),Cf=h(),Xe=a("h3"),bt=a("a"),Ds=a("span"),p(Xo.$$.fragment),Pv=h(),Rs=a("span"),Iv=r("3.-4. Run a pretrained checkpoint using the original repository"),qf=h(),A=a("p"),Av=r("At first, you will work on the original "),Fs=a("em"),Lv=r("brand_new_bert"),Nv=r(` repository. Often, the original implementation is very
\u201Cresearchy\u201D. Meaning that documentation might be lacking and the code can be difficult to understand. But this should
be exactly your motivation to reimplement `),zs=a("em"),Bv=r("brand_new_bert"),Mv=r(". At Hugging Face, one of our main goals is to "),Hs=a("em"),Ov=r(`make people
stand on the shoulders of giants`),xv=r(` which translates here very well into taking a working model and rewriting it to make
it as `),Ws=a("strong"),jv=r("accessible, user-friendly, and beautiful"),Cv=r(` as possible. This is the number-one motivation to re-implement
models into \u{1F917} Transformers - trying to make complex new NLP technology accessible to `),Gs=a("strong"),qv=r("everybody"),Sv=r("."),Sf=h(),Nn=a("p"),Dv=r("You should start thereby by diving into the original repository."),Df=h(),_t=a("p"),Rv=r("Successfully running the official pretrained model in the original repository is often "),Us=a("strong"),Fv=r("the most difficult"),zv=r(` step.
From our experience, it is very important to spend some time getting familiar with the original code-base. You need to
figure out the following:`),Rf=h(),L=a("ul"),Ys=a("li"),Hv=r("Where to find the pretrained weights?"),Wv=h(),Js=a("li"),Gv=r("How to load the pretrained weights into the corresponding model?"),Uv=h(),Xs=a("li"),Yv=r("How to run the tokenizer independently from the model?"),Jv=h(),Zs=a("li"),Xv=r(`Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually,
you only have to reimplement those functions.`),Zv=h(),F=a("li"),Kv=r(`Be able to locate the important components of the model: Where is the model\u2019s class? Are there model sub-classes,
`),Ks=a("em"),Qv=r("e.g."),Vv=r(` EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers,
`),Qs=a("em"),eb=r("e.g."),tb=h(),Vs=a("em"),ob=r("self-attention"),rb=r(", "),el=a("em"),nb=r("cross-attention"),ab=r("\u2026?"),ib=h(),Ze=a("li"),sb=r("How can you debug the model in the original environment of the repo? Do you have to add "),tl=a("em"),lb=r("print"),db=r(` statements, can you
work with an interactive debugger like `),ol=a("em"),hb=r("ipdb"),fb=r(", or should you use an efficient IDE to debug the model, like PyCharm?"),Ff=h(),Et=a("p"),mb=r("It is very important that before you start the porting process, that you can "),rl=a("strong"),pb=r("efficiently"),ub=r(` debug code in the original
repository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or
even a pull request in the original repository. The maintainers of this repository are most likely very happy about
someone looking into their code!`),zf=h(),Bn=a("p"),cb=r(`At this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original
model. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to
dive into the original repository and also when starting to write the \u{1F917} Transformers implementation of the model. Only
at the very end, when the model has already been successfully ported to \u{1F917} Transformers, one should verify that the
model also works as expected on GPU.`),Hf=h(),Mn=a("p"),yb=r("In general, there are two possible debugging environments for running the original model"),Wf=h(),kt=a("ul"),Zo=a("li"),Ko=a("a"),gb=r("Jupyter notebooks"),wb=r(" / "),Qo=a("a"),vb=r("google colab"),bb=h(),nl=a("li"),_b=r("Local python scripts."),Gf=h(),On=a("p"),Eb=r(`Jupyter notebooks have the advantage that they allow for cell-by-cell execution which can be helpful to better split
logical components from one another and to have faster debugging cycles as intermediate results can be stored. Also,
notebooks are often easier to share with other contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupiter notebooks, we strongly recommend you to work with them.`),Uf=h(),Tt=a("p"),kb=r(`The obvious disadvantage of Jupyter notebooks is that if you are not used to working with them you will have to spend
some time adjusting to the new programming environment and that you might not be able to use your known debugging tools
anymore, like `),al=a("code"),Tb=r("ipdb"),$b=r("."),Yf=h(),$t=a("p"),Pb=r("For each code-base, a good first step is always to load a "),il=a("strong"),Ib=r("small"),Ab=r(` pretrained checkpoint and to be able to reproduce a
single forward pass using a dummy integer vector of input IDs as an input. Such a script could look like this (in
pseudocode):`),Jf=h(),p(Vo.$$.fragment),Xf=h(),xn=a("p"),Lb=r("Next, regarding the debugging strategy, there are generally a few from which to choose from:"),Zf=h(),Pt=a("ul"),sl=a("li"),Nb=r(`Decompose the original model into many small testable components and run a forward pass on each of those for
verification`),Bb=h(),Ke=a("li"),Mb=r("Decompose the original model only into the original "),ll=a("em"),Ob=r("tokenizer"),xb=r(" and the original "),dl=a("em"),jb=r("model"),Cb=r(`, run a forward pass on
those, and use intermediate print statements or breakpoints for verification`),Kf=h(),jn=a("p"),qb=r(`Again, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code
base.`),Qf=h(),It=a("p"),Sb=r("If the original code-base allows you to decompose the model into smaller sub-components, "),hl=a("em"),Db=r("e.g."),Rb=r(` if the original
code-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages
to taking the more difficult road in the beginning:`),Vf=h(),U=a("ul"),fl=a("li"),Fb=r(`at a later stage when comparing the original model to the Hugging Face implementation, you can verify automatically
for each component individually that the corresponding component of the \u{1F917} Transformers implementation matches instead
of relying on visual comparison via print statements`),zb=h(),ml=a("li"),Hb=r(`it can give you some rope to decompose the big problem of porting a model into smaller problems of just porting
individual components and thus structure your work better`),Wb=h(),pl=a("li"),Gb=r(`separating the model into logical meaningful components will help you to get a better overview of the model\u2019s design
and thus to better understand the model`),Ub=h(),ul=a("li"),Yb=r(`at a later stage those component-by-component tests help you to ensure that no regression occurs as you continue
changing your code`),em=h(),er=a("p"),tr=a("a"),Jb=r("Lysandre\u2019s"),Xb=r(` integration checks for ELECTRA
gives a nice example of how this can be done.`),tm=h(),At=a("p"),Zb=r(`However, if the original code-base is very complex or only allows intermediate components to be run in a compiled mode,
it might be too time-consuming or even impossible to separate the model into smaller testable sub-components. A good
example is `),or=a("a"),Kb=r("T5\u2019s MeshTensorFlow"),Qb=r(` library which is
very complex and does not offer a simple way to decompose the model into its sub-components. For such libraries, one
often relies on verifying print statements.`),om=h(),Cn=a("p"),Vb=r(`No matter which strategy you choose, the recommended procedure is often the same in that you should start to debug the
starting layers first and the ending layers last.`),rm=h(),qn=a("p"),e_=r(`It is recommended that you retrieve the output, either by print statements or sub-component functions, of the following
layers in the following order:`),nm=h(),N=a("ol"),cl=a("li"),t_=r("Retrieve the input IDs passed to the model"),o_=h(),yl=a("li"),r_=r("Retrieve the word embeddings"),n_=h(),gl=a("li"),a_=r("Retrieve the input of the first Transformer layer"),i_=h(),wl=a("li"),s_=r("Retrieve the output of the first Transformer layer"),l_=h(),vl=a("li"),d_=r("Retrieve the output of the following n - 1 Transformer layers"),h_=h(),bl=a("li"),f_=r("Retrieve the output of the whole BrandNewBert Model"),am=h(),Qe=a("p"),m_=r("Input IDs should thereby consists of an array of integers, "),_l=a("em"),p_=r("e.g."),u_=h(),El=a("code"),c_=r("input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]"),im=h(),Sn=a("p"),y_=r("The outputs of the following layers often consist of multi-dimensional float arrays and can look like this:"),sm=h(),p(rr.$$.fragment),lm=h(),ge=a("p"),g_=r(`We expect that every model added to \u{1F917} Transformers passes a couple of integration tests, meaning that the original
model and the reimplemented version in \u{1F917} Transformers have to give the exact same output up to a precision of 0.001!
Since it is normal that the exact same model written in different libraries can give a slightly different output
depending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives
nearly the same output, they have to be the almost identical. Therefore, you will certainly compare the intermediate
outputs of the \u{1F917} Transformers version multiple times against the intermediate outputs of the original implementation of
`),kl=a("em"),w_=r("brand_new_bert"),v_=r(" in which case an "),Tl=a("strong"),b_=r("efficient"),__=r(` debugging environment of the original repository is absolutely
important. Here is some advice is to make your debugging environment as efficient as possible.`),dm=h(),C=a("ul"),z=a("li"),E_=r(`Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should
probably take the time to write a longer script that decomposes the original model into smaller sub-components to
retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on
TensorFlow print operations like `),nr=a("a"),k_=r("tf.print"),T_=r(` to output
intermediate values. Is the original repository written in Jax? Then make sure that the model is `),$l=a("strong"),$_=r("not jitted"),P_=r(` when
running the forward pass, `),Pl=a("em"),I_=r("e.g."),A_=r(" check-out "),ar=a("a"),L_=r("this link"),N_=r("."),B_=h(),Il=a("li"),M_=r(`Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle
becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.
In case only very large checkpoints are available, it might make more sense to create a dummy model in the new
environment with randomly initialized weights and save those weights for comparison with the \u{1F917} Transformers version
of your model`),O_=h(),T=a("li"),x_=r(`Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want to
find the function in the original repository that `),Al=a("strong"),j_=r("only"),C_=r(" calls a single forward pass, "),Ll=a("em"),q_=r("i.e."),S_=r(` that is often called
`),Nl=a("code"),D_=r("predict"),R_=r(", "),Bl=a("code"),F_=r("evaluate"),z_=r(", "),Ml=a("code"),H_=r("forward"),W_=r(" or "),Ol=a("code"),G_=r("__call__"),U_=r(". You don\u2019t want to debug a function that calls "),xl=a("code"),Y_=r("forward"),J_=r(`
multiple times, `),jl=a("em"),X_=r("e.g."),Z_=r(" to generate text, like "),Cl=a("code"),K_=r("autoregressive_sample"),Q_=r(", "),ql=a("code"),V_=r("generate"),e1=r("."),t1=h(),ir=a("li"),o1=r("Try to separate the tokenization from the model\u2019s "),Sl=a("em"),r1=r("forward"),n1=r(` pass. If the original repository shows examples where
you have to input a string, then try to find out where in the forward call the string input is changed to input ids
and start from this point. This might mean that you have to possibly write a small script yourself or change the
original code so that you can directly input the ids instead of an input string.`),a1=h(),me=a("li"),i1=r("Make sure that the model in your debugging setup is "),Dl=a("strong"),s1=r("not"),l1=r(` in training mode, which often causes the model to yield
random outputs due to multiple dropout layers in the model. Make sure that the forward pass in your debugging
environment is `),Rl=a("strong"),d1=r("deterministic"),h1=r(" so that the dropout layers are not used. Or use "),Fl=a("em"),f1=r("transformers.file_utils.set_seed"),m1=r(`
if the old and new implementations are in the same framework.`),hm=h(),Lt=a("p"),p1=r("The following section gives you more specific details/tips on how you can do this for "),zl=a("em"),u1=r("brand_new_bert"),c1=r("."),fm=h(),Ve=a("h3"),Nt=a("a"),Hl=a("span"),p(sr.$$.fragment),y1=h(),Wl=a("span"),g1=r("5.-14. Port BrandNewBert to \u{1F917} Transformers"),mm=h(),Dn=a("p"),w1=r("Next, you can finally start adding new code to \u{1F917} Transformers. Go into the clone of your \u{1F917} Transformers\u2019 fork:"),pm=h(),p(lr.$$.fragment),um=h(),Bt=a("p"),v1=r(`In the special case that you are adding a model whose architecture exactly matches the model architecture of an
existing model you only have to add a conversion script as described in `),Rn=a("a"),b1=r("this section"),_1=r(`.
In this case, you can just re-use the whole model architecture of the already existing model.`),cm=h(),Fn=a("p"),E1=r("Otherwise, let\u2019s start generating a new model. You have two choices here:"),ym=h(),Mt=a("ul"),zn=a("li"),Gl=a("code"),k1=r("transformers-cli add-new-model-like"),T1=r(" to add a new model like an existing one"),$1=h(),Hn=a("li"),Ul=a("code"),P1=r("transformers-cli add-new-model"),I1=r(" to add a new model from our template (will look like BERT or Bart depending on the type of model you select)"),gm=h(),we=a("p"),A1=r("In both cases, you will be prompted with a questionnaire to fill the basic information of your model. The second command requires to install "),Yl=a("code"),L1=r("cookiecutter"),N1=r(", you can find more information on it "),dr=a("a"),B1=r("here"),M1=r("."),wm=h(),Wn=a("p"),Jl=a("strong"),O1=r("Open a Pull Request on the main huggingface/transformers repo"),vm=h(),ve=a("p"),x1=r(`Before starting to adapt the automatically generated code, now is the time to open a \u201CWork in progress (WIP)\u201D pull
request, `),Xl=a("em"),j1=r("e.g."),C1=r(" \u201C[WIP] Add "),Zl=a("em"),q1=r("brand_new_bert"),S1=r(`\u201D, in \u{1F917} Transformers so that you and the Hugging Face team can work
side-by-side on integrating the model into \u{1F917} Transformers.`),bm=h(),Gn=a("p"),D1=r("You should do the following:"),_m=h(),Un=a("ol"),Kl=a("li"),R1=r("Create a branch with a descriptive name from your main branch"),Em=h(),p(hr.$$.fragment),km=h(),fr=a("ol"),Ql=a("li"),F1=r("Commit the automatically generated code:"),Tm=h(),p(mr.$$.fragment),$m=h(),pr=a("ol"),Vl=a("li"),z1=r("Fetch and rebase to current main"),Pm=h(),p(ur.$$.fragment),Im=h(),cr=a("ol"),ed=a("li"),H1=r("Push the changes to your account using:"),Am=h(),p(yr.$$.fragment),Lm=h(),et=a("ol"),td=a("li"),od=a("p"),W1=r(`Once you are satisfied, go to the webpage of your fork on GitHub. Click on \u201CPull request\u201D. Make sure to add the
GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for
future changes.`),G1=h(),rd=a("li"),nd=a("p"),U1=r("Change the PR into a draft by clicking on \u201CConvert to draft\u201D on the right of the GitHub pull request web page."),Nm=h(),Yn=a("p"),Y1=r(`In the following, whenever you have done some progress, don\u2019t forget to commit your work and push it to your account so
that it shows in the pull request. Additionally, you should make sure to update your work with the current main from
time to time by doing:`),Bm=h(),p(gr.$$.fragment),Mm=h(),Jn=a("p"),J1=r(`In general, all questions you might have regarding the model or your implementation should be asked in your PR and
discussed/solved in the PR. This way, the Hugging Face team will always be notified when you are committing new code or
if you have a question. It is often very helpful to point the Hugging Face team to your added code so that the Hugging
Face team can efficiently understand your problem or question.`),Om=h(),Xn=a("p"),X1=r(`To do so, you can go to the \u201CFiles changed\u201D tab where you see all of your changes, go to a line regarding which you
want to ask a question, and click on the \u201C+\u201D symbol to add a comment. Whenever a question or problem has been solved,
you can click on the \u201CResolve\u201D button of the created comment.`),xm=h(),Zn=a("p"),Z1=r(`In the same way, the Hugging Face team will open comments when reviewing your code. We recommend asking most questions
on GitHub on your PR. For some very general questions that are not very useful for the public, feel free to ping the
Hugging Face team by Slack or email.`),jm=h(),Kn=a("p"),ad=a("strong"),K1=r("5. Adapt the generated models code for brand_new_bert"),Cm=h(),be=a("p"),Q1=r(`At first, we will focus only on the model itself and not care about the tokenizer. All the relevant code should be
found in the generated files `),id=a("code"),V1=r("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),e0=r(` and
`),sd=a("code"),t0=r("src/transformers/models/brand_new_bert/configuration_brand_new_bert.py"),o0=r("."),qm=h(),Y=a("p"),r0=r(`Now you can finally start coding :). The generated code in
`),ld=a("code"),n0=r("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),a0=r(` will either have the same architecture as BERT if
it\u2019s an encoder-only model or BART if it\u2019s an encoder-decoder model. At this point, you should remind yourself what
you\u2019ve learned in the beginning about the theoretical aspects of the model: `),dd=a("em"),i0=r(`How is the model different from BERT or
BART?`),s0=r("\u201D. Implement those changes which often means to change the "),hd=a("em"),l0=r("self-attention"),d0=r(` layer, the order of the normalization
layer, etc\u2026 Again, it is often useful to look at the similar architecture of already existing models in Transformers to
get a better feeling of how your model should be implemented.`),Sm=h(),M=a("p"),fd=a("strong"),h0=r("Note"),f0=r(` that at this point, you don\u2019t have to be very sure that your code is fully correct or clean. Rather, it is
advised to add a first `),md=a("em"),m0=r("unclean"),p0=r(`, copy-pasted version of the original code to
`),pd=a("code"),u0=r("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),c0=r(` until you feel like all the necessary code is
added. From our experience, it is much more efficient to quickly add a first version of the required code and
improve/correct the code iteratively with the conversion script as described in the next section. The only thing that
has to work at this point is that you can instantiate the \u{1F917} Transformers implementation of `),ud=a("em"),y0=r("brand_new_bert"),g0=r(", "),cd=a("em"),w0=r("i.e."),v0=r(` the
following command should work:`),Dm=h(),p(wr.$$.fragment),Rm=h(),_e=a("p"),b0=r("The above command will create a model according to the default parameters as defined in "),yd=a("code"),_0=r("BrandNewBertConfig()"),E0=r(` with
random weights, thus making sure that the `),gd=a("code"),k0=r("init()"),T0=r(" methods of all components works."),Fm=h(),Qn=a("p"),wd=a("strong"),$0=r("6. Write a conversion script"),zm=h(),J=a("p"),P0=r("Next, you should write a conversion script that lets you convert the checkpoint you used to debug "),vd=a("em"),I0=r("brand_new_bert"),A0=r(` in
the original repository to a checkpoint compatible with your just created \u{1F917} Transformers implementation of
`),bd=a("em"),L0=r("brand_new_bert"),N0=r(`. It is not advised to write the conversion script from scratch, but rather to look through already
existing conversion scripts in \u{1F917} Transformers for one that has been used to convert a similar model that was written in
the same framework as `),_d=a("em"),B0=r("brand_new_bert"),M0=r(`. Usually, it is enough to copy an already existing conversion script and
slightly adapt it for your use case. Don\u2019t hesitate to ask the Hugging Face team to point you to a similar already
existing conversion script for your model.`),Hm=h(),Ot=a("ul"),Vn=a("li"),O0=r("If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT\u2019s conversion script "),vr=a("a"),x0=r("here"),j0=h(),ea=a("li"),C0=r("If you are porting a model from PyTorch to PyTorch, a good starting point might be BART\u2019s conversion script "),br=a("a"),q0=r("here"),Wm=h(),xt=a("p"),S0=r(`In the following, we\u2019ll quickly explain how PyTorch models store layer weights and define layer names. In PyTorch, the
name of a layer is defined by the name of the class attribute you give the layer. Let\u2019s define a dummy model in
PyTorch, called `),Ed=a("code"),D0=r("SimpleModel"),R0=r(" as follows:"),Gm=h(),p(_r.$$.fragment),Um=h(),X=a("p"),F0=r("Now we can create an instance of this model definition which will fill all weights: "),kd=a("code"),z0=r("dense"),H0=r(", "),Td=a("code"),W0=r("intermediate"),G0=r(`,
`),$d=a("code"),U0=r("layer_norm"),Y0=r(" with random weights. We can print the model to see its architecture"),Ym=h(),p(Er.$$.fragment),Jm=h(),ta=a("p"),J0=r("This will print out the following:"),Xm=h(),p(kr.$$.fragment),Zm=h(),oa=a("p"),X0=r(`We can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight
values of a specific layer:`),Km=h(),p(Tr.$$.fragment),Qm=h(),ra=a("p"),Z0=r("to see that the weights were randomly initialized"),Vm=h(),p($r.$$.fragment),ep=h(),Pr=a("p"),K0=r(`In the conversion script, you should fill those randomly initialized weights with the exact weights of the
corresponding layer in the checkpoint. `),Pd=a("em"),Q0=r("E.g."),tp=h(),p(Ir.$$.fragment),op=h(),Ee=a("p"),V0=r(`While doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding
pretrained checkpoint weight exactly match in both `),Id=a("strong"),e2=r("shape and name"),t2=r(". To do so, it is "),Ad=a("strong"),o2=r("necessary"),r2=r(` to add assert
statements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:`),rp=h(),p(Ar.$$.fragment),np=h(),Lr=a("p"),n2=r("Besides, you should also print out the names of both weights to make sure they match, "),Ld=a("em"),a2=r("e.g."),ap=h(),p(Nr.$$.fragment),ip=h(),na=a("p"),i2=r(`If either the shape or the name doesn\u2019t match, you probably assigned the wrong checkpoint weight to a randomly
initialized layer of the \u{1F917} Transformers implementation.`),sp=h(),jt=a("p"),s2=r("An incorrect shape is most likely due to an incorrect setting of the config parameters in "),Nd=a("code"),l2=r("BrandNewBertConfig()"),d2=r(` that
do not exactly match those that were used for the checkpoint you want to convert. However, it could also be that
PyTorch\u2019s implementation of a layer requires the weight to be transposed beforehand.`),lp=h(),Z=a("p"),h2=r("Finally, you should also check that "),Bd=a("strong"),f2=r("all"),m2=r(` required weights are initialized and print out all checkpoint weights that
were not used for initialization to make sure the model is correctly converted. It is completely normal, that the
conversion trials fail with either a wrong shape statement or wrong name assignment. This is most likely because either
you used incorrect parameters in `),Md=a("code"),p2=r("BrandNewBertConfig()"),u2=r(`, have a wrong architecture in the \u{1F917} Transformers
implementation, you have a bug in the `),Od=a("code"),c2=r("init()"),y2=r(` functions of one of the components of the \u{1F917} Transformers
implementation or you need to transpose one of the checkpoint weights.`),dp=h(),K=a("p"),g2=r(`This step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the
Transformers model. Having correctly loaded the checkpoint into the \u{1F917} Transformers implementation, you can then save
the model under a folder of your choice `),xd=a("code"),w2=r("/path/to/converted/checkpoint/folder"),v2=r(` that should then contain both a
`),jd=a("code"),b2=r("pytorch_model.bin"),_2=r(" file and a "),Cd=a("code"),E2=r("config.json"),k2=r(" file:"),hp=h(),p(Br.$$.fragment),fp=h(),aa=a("p"),qd=a("strong"),T2=r("7. Implement the forward pass"),mp=h(),Ct=a("p"),$2=r(`Having managed to correctly load the pretrained weights into the \u{1F917} Transformers implementation, you should now make
sure that the forward pass is correctly implemented. In `),ia=a("a"),P2=r("Get familiar with the original repository"),I2=r(`, you have already created a script that runs a forward
pass of the model using the original repository. Now you should write an analogous script using the \u{1F917} Transformers
implementation instead of the original one. It should look as follows:`),pp=h(),p(Mr.$$.fragment),up=h(),q=a("p"),A2=r(`It is very likely that the \u{1F917} Transformers implementation and the original model implementation don\u2019t give the exact
same output the very first time or that the forward pass throws an error. Don\u2019t be disappointed - it\u2019s expected! First,
you should make sure that the forward pass doesn\u2019t throw any errors. It often happens that the wrong dimensions are
used leading to a `),Sd=a("em"),L2=r("Dimensionality mismatch"),N2=r(" error or that the wrong data type object is used, "),Dd=a("em"),B2=r("e.g."),M2=h(),Rd=a("code"),O2=r("torch.long"),x2=r(`
instead of `),Fd=a("code"),j2=r("torch.float32"),C2=r(`. Don\u2019t hesitate to ask the Hugging Face team for help, if you don\u2019t manage to solve
certain errors.`),cp=h(),Q=a("p"),q2=r(`The final part to make sure the \u{1F917} Transformers implementation works correctly is to ensure that the outputs are
equivalent to a precision of `),zd=a("code"),S2=r("1e-3"),D2=r(". First, you should ensure that the output shapes are identical, "),Hd=a("em"),R2=r("i.e."),F2=h(),Wd=a("code"),z2=r("outputs.shape"),H2=r(` should yield the same value for the script of the \u{1F917} Transformers implementation and the original
implementation. Next, you should make sure that the output values are identical as well. This one of the most difficult
parts of adding a new model. Common mistakes why the outputs are not identical are:`),yp=h(),V=a("ul"),tt=a("li"),W2=r("Some layers were not added, "),Gd=a("em"),G2=r("i.e."),U2=r(" an "),Ud=a("em"),Y2=r("activation"),J2=r(" layer was not added, or the residual connection was forgotten"),X2=h(),Yd=a("li"),Z2=r("The word embedding matrix was not tied"),K2=h(),Jd=a("li"),Q2=r("The wrong positional embeddings are used because the original implementation uses on offset"),V2=h(),ee=a("li"),eE=r("Dropout is applied during the forward pass. To fix this make sure "),Xd=a("em"),tE=r("model.training is False"),oE=r(` and that no dropout
layer is falsely activated during the forward pass, `),Zd=a("em"),rE=r("i.e."),nE=r(" pass "),Kd=a("em"),aE=r("self.training"),iE=r(" to "),Or=a("a"),sE=r("PyTorch\u2019s functional dropout"),gp=h(),ke=a("p"),lE=r(`The best way to fix the problem is usually to look at the forward pass of the original implementation and the \u{1F917}
Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out
intermediate outputs of both implementations of the forward pass to find the exact position in the network where the \u{1F917}
Transformers implementation shows a different output than the original implementation. First, make sure that the
hard-coded `),Qd=a("code"),dE=r("input_ids"),hE=r(` in both scripts are identical. Next, verify that the outputs of the first transformation of
the `),Vd=a("code"),fE=r("input_ids"),mE=r(` (usually the word embeddings) are identical. And then work your way up to the very last layer of the
network. At some point, you will notice a difference between the two implementations, which should point you to the bug
in the \u{1F917} Transformers implementation. From our experience, a simple and efficient way is to add many print statements
in both the original implementation and \u{1F917} Transformers implementation, at the same positions in the network
respectively, and to successively remove print statements showing the same values for intermediate presentations.`),wp=h(),qt=a("p"),pE=r(`When you\u2019re confident that both implementations yield the same output, verifying the outputs with
`),eh=a("code"),uE=r("torch.allclose(original_output, output, atol=1e-3)"),cE=r(`, you\u2019re done with the most difficult part! Congratulations - the
work left to be done should be a cakewalk \u{1F60A}.`),vp=h(),sa=a("p"),th=a("strong"),yE=r("8. Adding all necessary model tests"),bp=h(),St=a("p"),gE=r(`At this point, you have successfully added a new model. However, it is very much possible that the model does not yet
fully comply with the required design. To make sure, the implementation is fully compatible with \u{1F917} Transformers, all
common tests should pass. The Cookiecutter should have automatically added a test file for your model, probably under
the same `),oh=a("code"),wE=r("tests/test_modeling_brand_new_bert.py"),vE=r(". Run this test file to verify that all common tests pass:"),_p=h(),p(xr.$$.fragment),Ep=h(),la=a("p"),bE=r("Having fixed all common tests, it is now crucial to ensure that all the nice work you have done is well tested, so that"),kp=h(),Dt=a("ul"),da=a("li"),_E=r("a) The community can easily understand your work by looking at specific tests of "),rh=a("em"),EE=r("brand_new_bert"),kE=h(),nh=a("li"),TE=r("b) Future changes to your model will not break any important feature of the model."),Tp=h(),Rt=a("p"),$E=r(`At first, integration tests should be added. Those integration tests essentially do the same as the debugging scripts
you used earlier to implement the model to \u{1F917} Transformers. A template of those model tests is already added by the
Cookiecutter, called `),ah=a("code"),PE=r("BrandNewBertModelIntegrationTests"),IE=r(` and only has to be filled out by you. To ensure that those
tests are passing, run`),$p=h(),p(jr.$$.fragment),Pp=h(),p(Ft.$$.fragment),Ip=h(),te=a("p"),AE=r("Second, all features that are special to "),ih=a("em"),LE=r("brand_new_bert"),NE=r(` should be tested additionally in a separate test under
`),sh=a("code"),BE=r("BrandNewBertModelTester"),ME=r("/`"),lh=a("code"),OE=r("BrandNewBertModelTest"),xE=r(`. This part is often forgotten but is extremely useful in two
ways:`),Ap=h(),zt=a("ul"),Cr=a("li"),jE=r(`It helps to transfer the knowledge you have acquired during the model addition to the community by showing how the
special features of `),dh=a("em"),CE=r("brand_new_bert"),qE=r(" should work."),SE=h(),hh=a("li"),DE=r("Future contributors can quickly test changes to the model by running those special tests."),Lp=h(),ha=a("p"),fh=a("strong"),RE=r("9. Implement the tokenizer"),Np=h(),Ht=a("p"),FE=r("Next, we should add the tokenizer of "),mh=a("em"),zE=r("brand_new_bert"),HE=r(`. Usually, the tokenizer is equivalent or very similar to an
already existing tokenizer of \u{1F917} Transformers.`),Bp=h(),fa=a("p"),WE=r(`It is very important to find/extract the original tokenizer file and to manage to load this file into the \u{1F917}
Transformers\u2019 implementation of the tokenizer.`),Mp=h(),ma=a("p"),GE=r("To ensure that the tokenizer works correctly, it is recommended to first create a script in the original repository\nthat inputs a string and returns the `input_ids\u201C. It could look similar to this (in pseudo-code):"),Op=h(),p(qr.$$.fragment),xp=h(),Wt=a("p"),UE=r(`You might have to take a deeper look again into the original repository to find the correct tokenizer function or you
might even have to do changes to your clone of the original repository to only output the `),ph=a("code"),YE=r("input_ids"),JE=r(`. Having written
a functional tokenization script that uses the original repository, an analogous script for \u{1F917} Transformers should be
created. It should look similar to this:`),jp=h(),p(Sr.$$.fragment),Cp=h(),Gt=a("p"),XE=r("When both "),uh=a("code"),ZE=r("input_ids"),KE=r(" yield the same values, as a final step a tokenizer test file should also be added."),qp=h(),Te=a("p"),QE=r("Analogous to the modeling test files of "),ch=a("em"),VE=r("brand_new_bert"),e3=r(", the tokenization test files of "),yh=a("em"),t3=r("brand_new_bert"),o3=r(` should
contain a couple of hard-coded integration tests.`),Sp=h(),pa=a("p"),gh=a("strong"),r3=r("10. Run End-to-end integration tests"),Dp=h(),oe=a("p"),n3=r(`Having added the tokenizer, you should also add a couple of end-to-end integration tests using both the model and the
tokenizer to `),wh=a("code"),a3=r("tests/test_modeling_brand_new_bert.py"),i3=r(` in \u{1F917} Transformers. Such a test should show on a meaningful
text-to-text sample that the \u{1F917} Transformers implementation works as expected. A meaningful text-to-text sample can
include `),vh=a("em"),s3=r("e.g."),l3=r(` a source-to-target-translation pair, an article-to-summary pair, a question-to-answer pair, etc\u2026 If none
of the ported checkpoints has been fine-tuned on a downstream task it is enough to simply rely on the model tests. In a
final step to ensure that the model is fully functional, it is advised that you also run all tests on GPU. It can
happen that you forgot to add some `),bh=a("code"),d3=r(".to(self.device)"),h3=r(` statements to internal tensors of the model, which in such a
test would show in an error. In case you have no access to a GPU, the Hugging Face team can take care of running those
tests for you.`),Rp=h(),ua=a("p"),_h=a("strong"),f3=r("11. Add Docstring"),Fp=h(),re=a("p"),m3=r("Now, all the necessary functionality for "),Eh=a("em"),p3=r("brand_new_bert"),u3=r(` is added - you\u2019re almost done! The only thing left to add is
a nice docstring and a doc page. The Cookiecutter should have added a template file called
`),kh=a("code"),c3=r("docs/source/model_doc/brand_new_bert.rst"),y3=r(` that you should fill out. Users of your model will usually first look at
this page before using your model. Hence, the documentation must be understandable and concise. It is very useful for
the community to add some `),Th=a("em"),g3=r("Tips"),w3=r(` to show how the model should be used. Don\u2019t hesitate to ping the Hugging Face team
regarding the docstrings.`),zp=h(),$e=a("p"),v3=r("Next, make sure that the docstring added to "),$h=a("code"),b3=r("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),_3=r(` is
correct and included all necessary inputs and outputs. We have a detailed guide about writing documentation and our docstring format `),ca=a("a"),E3=r("here"),k3=r(`. It is always to good to remind oneself that documentation should
be treated at least as carefully as the code in \u{1F917} Transformers since the documentation is usually the first contact
point of the community with the model.`),Hp=h(),ya=a("p"),Ph=a("strong"),T3=r("Code refactor"),Wp=h(),Ut=a("p"),$3=r("Great, now you have added all the necessary code for "),Ih=a("em"),P3=r("brand_new_bert"),I3=r(`. At this point, you should correct some potential
incorrect code style by running:`),Gp=h(),p(Dr.$$.fragment),Up=h(),ga=a("p"),A3=r("and verify that your coding style passes the quality check:"),Yp=h(),p(Rr.$$.fragment),Jp=h(),wa=a("p"),L3=r(`There are a couple of other very strict design tests in \u{1F917} Transformers that might still be failing, which shows up in
the tests of your pull request. This is often because of some missing information in the docstring or some incorrect
naming. The Hugging Face team will surely help you if you\u2019re stuck here.`),Xp=h(),va=a("p"),N3=r(`Lastly, it is always a good idea to refactor one\u2019s code after having ensured that the code works correctly. With all
tests passing, now it\u2019s a good time to go over the added code again and do some refactoring.`),Zp=h(),ba=a("p"),B3=r("You have now finished the coding part, congratulation! \u{1F389} You are Awesome! \u{1F60E}"),Kp=h(),_a=a("p"),Ah=a("strong"),M3=r("12. Upload the models to the model hub"),Qp=h(),S=a("p"),O3=r(`In this final part, you should convert and upload all checkpoints to the model hub and add a model card for each
uploaded model checkpoint. You can get familiar with the hub functionalities by reading our `),Ea=a("a"),x3=r("Model sharing and uploading Page"),j3=r(`. You should work alongside the Hugging Face team here to decide on a fitting name for each
checkpoint and to get the required access rights to be able to upload the model under the author\u2019s organization of
`),Lh=a("em"),C3=r("brand_new_bert"),q3=r(". The "),Nh=a("code"),S3=r("push_to_hub"),D3=r(" method, present in all models in "),Bh=a("code"),R3=r("transformers"),F3=r(", is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:"),Vp=h(),p(Fr.$$.fragment),eu=h(),Yt=a("p"),z3=r(`It is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the
specific characteristics of this particular checkpoint, `),Mh=a("em"),H3=r("e.g."),W3=r(` On which dataset was the checkpoint
pretrained/fine-tuned on? On what down-stream task should the model be used? And also include some code on how to
correctly use the model.`),tu=h(),ka=a("p"),Oh=a("strong"),G3=r("13. (Optional) Add notebook"),ou=h(),Jt=a("p"),U3=r("It is very helpful to add a notebook that showcases in-detail how "),xh=a("em"),Y3=r("brand_new_bert"),J3=r(` can be used for inference and/or
fine-tuned on a downstream task. This is not mandatory to merge your PR, but very useful for the community.`),ru=h(),Ta=a("p"),jh=a("strong"),X3=r("14. Submit your finished PR"),nu=h(),$a=a("p"),Z3=r(`You\u2019re done programming now and can move to the last step, which is getting your PR merged into main. Usually, the
Hugging Face team should have helped you already at this point, but it is worth taking some time to give your finished
PR a nice description and eventually add comments to your code, if you want to point out certain design choices to your
reviewer.`),au=h(),ot=a("h3"),Xt=a("a"),Ch=a("span"),p(zr.$$.fragment),K3=h(),qh=a("span"),Q3=r("Share your work!!"),iu=h(),Pa=a("p"),V3=r(`Now, it\u2019s time to get some credit from the community for your work! Having completed a model addition is a major
contribution to Transformers and the whole NLP community. Your code and the ported pre-trained models will certainly be
used by hundreds and possibly even thousands of developers and researchers. You should be proud of your work and share
your achievement with the community.`),su=h(),Ia=a("p"),Sh=a("strong"),ek=r("You have made another model that is super easy to access for everyone in the community! \u{1F92F}"),this.h()},l(e){const l=_9('[data-svelte="svelte-1phssyn"]',document.head);P=i(l,"META",{name:!0,content:!0}),l.forEach(o),nt=f(e),B=i(e,"H1",{class:!0});var Hr=s(B);H=i(Hr,"A",{id:!0,class:!0,href:!0});var fk=s(H);xe=i(fk,"SPAN",{});var mk=s(xe);u(D.$$.fragment,mk),mk.forEach(o),fk.forEach(o),po=f(Hr),le=i(Hr,"SPAN",{});var pk=s(le);de=n(pk,"How to add a model to \u{1F917} Transformers?"),pk.forEach(o),Hr.forEach(o),uo=f(e),R=i(e,"P",{});var Aa=s(R);dc=n(Aa,`Adding a new model is often difficult and requires an in-depth knowledge of the \u{1F917} Transformers library and ideally also
of the model\u2019s original repository. At Hugging Face, we are trying to empower the community more and more to add models
independently. Thus, for some new models that the community wants to be added to \u{1F917} Transformers, we create a customized
`),Za=i(Aa,"EM",{});var uk=s(Za);hc=n(uk,"call-for-model-addition"),uk.forEach(o),fc=n(Aa,` that explains step-by-step how to add the requested model. With this
`),Ka=i(Aa,"EM",{});var ck=s(Ka);mc=n(ck,"call-for-model-addition"),ck.forEach(o),pc=n(Aa,`, we want to teach a motivated and experienced contributor of the community how to port a
model to \u{1F917} Transformers.`),Aa.forEach(o),Fh=f(e),at=i(e,"P",{});var du=s(at);uc=n(du,`If this sounds like something you would be interested in, feel free to check out the currently open
\u201Ccalls-for-model-addition\u201D `),co=i(du,"A",{href:!0,rel:!0});var yk=s(co);cc=n(yk,"here"),yk.forEach(o),yc=n(du,`
and to contact us.`),du.forEach(o),zh=f(e),Wr=i(e,"P",{});var gk=s(Wr);gc=n(gk,`If selected, you will then work closely with one member of the Hugging Face team to integrate the model into \u{1F917}
Transformers. By doing so, you will both gain a theoretical and deep practical understanding of the proposed model. But
more importantly, you will have made a major open-source contribution to \u{1F917} Transformers. Along the way, you will:`),gk.forEach(o),Hh=f(e),W=i(e,"UL",{});var Zt=s(W);Qa=i(Zt,"LI",{});var wk=s(Qa);wc=n(wk,"get insights into open-source best practices"),wk.forEach(o),vc=f(Zt),Va=i(Zt,"LI",{});var vk=s(Va);bc=n(vk,"understand the design principles of one of the most popular NLP libraries"),vk.forEach(o),_c=f(Zt),ei=i(Zt,"LI",{});var bk=s(ei);Ec=n(bk,"learn how to do efficiently test large NLP models"),bk.forEach(o),kc=f(Zt),he=i(Zt,"LI",{});var Kt=s(he);Tc=n(Kt,"learn how to integrate Python utilities like "),ti=i(Kt,"CODE",{});var _k=s(ti);$c=n(_k,"black"),_k.forEach(o),Pc=n(Kt,", "),oi=i(Kt,"CODE",{});var Ek=s(oi);Ic=n(Ek,"isort"),Ek.forEach(o),Ac=n(Kt,", "),ri=i(Kt,"CODE",{});var kk=s(ri);Lc=n(kk,"make fix-copies"),kk.forEach(o),Nc=n(Kt,` into a library to always
ensure clean and readable code`),Kt.forEach(o),Zt.forEach(o),Wh=f(e),it=i(e,"P",{});var hu=s(it);Bc=n(hu,`We are also more than happy if you want to add a model that cannot be found in the \u201Ccalls-for-model-addition\u201D folder.
The following sections explain in detail how to add a new model. It might also be very helpful to check out already
added models to see if those resemble the model you would like to add `),yo=i(hu,"A",{href:!0,rel:!0});var Tk=s(yo);Mc=n(Tk,"here"),Tk.forEach(o),Oc=n(hu,"."),hu.forEach(o),Gh=f(e),Gr=i(e,"P",{});var $k=s(Gr);xc=n($k,"To start, let\u2019s try to get a general overview of the Transformers library."),$k.forEach(o),Uh=f(e),je=i(e,"H2",{class:!0});var fu=s(je);st=i(fu,"A",{id:!0,class:!0,href:!0});var Pk=s(st);ni=i(Pk,"SPAN",{});var Ik=s(ni);u(go.$$.fragment,Ik),Ik.forEach(o),Pk.forEach(o),jc=f(fu),ai=i(fu,"SPAN",{});var Ak=s(ai);Cc=n(Ak,"General overview of \u{1F917} Transformers"),Ak.forEach(o),fu.forEach(o),Yh=f(e),Ur=i(e,"P",{});var Lk=s(Ur);qc=n(Lk,`First, you should get a general overview of \u{1F917} Transformers. \u{1F917} Transformers is a very opinionated library, so there is a
chance that you don\u2019t agree with some of the library\u2019s philosophies or design choices. From our experience, however, we
found that the fundamental design choices and philosophies of the library are crucial to efficiently scale \u{1F917}
Transformers while keeping maintenance costs at a reasonable level.`),Lk.forEach(o),Jh=f(e),lt=i(e,"P",{});var mu=s(lt);Sc=n(mu,"A good first starting point to better understand the library is to read the "),Yr=i(mu,"A",{href:!0});var Nk=s(Yr);Dc=n(Nk,"documentation of our philosophy"),Nk.forEach(o),Rc=n(mu,". As a result of our way of working, there are some choices that we try to apply to all models:"),mu.forEach(o),Xh=f(e),ce=i(e,"UL",{});var La=s(ce);ii=i(La,"LI",{});var Bk=s(ii);Fc=n(Bk,"Composition is generally favored over-abstraction"),Bk.forEach(o),zc=f(La),si=i(La,"LI",{});var Mk=s(si);Hc=n(Mk,"Duplicating code is not always bad if it strongly improves the readability or accessibility of a model"),Mk.forEach(o),Wc=f(La),wo=i(La,"LI",{});var pu=s(wo);Gc=n(pu,`Model files are as self-contained as possible so that when you read the code of a specific model, you ideally only
have to look into the respective `),li=i(pu,"CODE",{});var Ok=s(li);Uc=n(Ok,"modeling_....py"),Ok.forEach(o),Yc=n(pu," file."),pu.forEach(o),La.forEach(o),Zh=f(e),dt=i(e,"P",{});var uu=s(dt);Jc=n(uu,"In our opinion, the library\u2019s code is not just a means to provide a product, "),di=i(uu,"EM",{});var xk=s(di);Xc=n(xk,"e.g."),xk.forEach(o),Zc=n(uu,` the ability to use BERT for
inference, but also as the very product that we want to improve. Hence, when adding a model, the user is not only the
person that will use your model, but also everybody that will read, try to understand, and possibly tweak your code.`),uu.forEach(o),Kh=f(e),Jr=i(e,"P",{});var jk=s(Jr);Kc=n(jk,"With this in mind, let\u2019s go a bit deeper into the general library design."),jk.forEach(o),Qh=f(e),Ce=i(e,"H3",{class:!0});var cu=s(Ce);ht=i(cu,"A",{id:!0,class:!0,href:!0});var Ck=s(ht);hi=i(Ck,"SPAN",{});var qk=s(hi);u(vo.$$.fragment,qk),qk.forEach(o),Ck.forEach(o),Qc=f(cu),fi=i(cu,"SPAN",{});var Sk=s(fi);Vc=n(Sk,"Overview of models"),Sk.forEach(o),cu.forEach(o),Vh=f(e),G=i(e,"P",{});var Qt=s(G);ey=n(Qt,`To successfully add a model, it is important to understand the interaction between your model and its config,
`),Xr=i(Qt,"A",{href:!0});var Dk=s(Xr);ty=n(Dk,"PreTrainedModel"),Dk.forEach(o),oy=n(Qt,", and "),Zr=i(Qt,"A",{href:!0});var Rk=s(Zr);ry=n(Rk,"PretrainedConfig"),Rk.forEach(o),ny=n(Qt,`. For exemplary purposes, we will
call the model to be added to \u{1F917} Transformers `),mi=i(Qt,"CODE",{});var Fk=s(mi);ay=n(Fk,"BrandNewBert"),Fk.forEach(o),iy=n(Qt,"."),Qt.forEach(o),ef=f(e),Kr=i(e,"P",{});var zk=s(Kr);sy=n(zk,"Let\u2019s take a look:"),zk.forEach(o),tf=f(e),Qr=i(e,"IMG",{src:!0}),of=f(e),v=i(e,"P",{});var _=s(v);ly=n(_,`As you can see, we do make use of inheritance in \u{1F917} Transformers, but we keep the level of abstraction to an absolute
minimum. There are never more than two levels of abstraction for any model in the library. `),pi=i(_,"CODE",{});var Hk=s(pi);dy=n(Hk,"BrandNewBertModel"),Hk.forEach(o),hy=n(_,`
inherits from `),ui=i(_,"CODE",{});var Wk=s(ui);fy=n(Wk,"BrandNewBertPreTrainedModel"),Wk.forEach(o),my=n(_," which in turn inherits from "),Vr=i(_,"A",{href:!0});var Gk=s(Vr);py=n(Gk,"PreTrainedModel"),Gk.forEach(o),uy=n(_,` and
that\u2019s it. As a general rule, we want to make sure that a new model only depends on
`),en=i(_,"A",{href:!0});var Uk=s(en);cy=n(Uk,"PreTrainedModel"),Uk.forEach(o),yy=n(_,`. The important functionalities that are automatically provided to every new
model are `),tn=i(_,"A",{href:!0});var Yk=s(tn);gy=n(Yk,"from_pretrained()"),Yk.forEach(o),wy=n(_,` and
`),on=i(_,"A",{href:!0});var Jk=s(on);vy=n(Jk,"save_pretrained()"),Jk.forEach(o),by=n(_,`, which are used for serialization and deserialization. All of the
other important functionalities, such as `),ci=i(_,"CODE",{});var Xk=s(ci);_y=n(Xk,"BrandNewBertModel.forward"),Xk.forEach(o),Ey=n(_,` should be completely defined in the new
`),yi=i(_,"CODE",{});var Zk=s(yi);ky=n(Zk,"modeling_brand_new_bert.py"),Zk.forEach(o),Ty=n(_,` script. Next, we want to make sure that a model with a specific head layer, such as
`),gi=i(_,"CODE",{});var Kk=s(gi);$y=n(Kk,"BrandNewBertForMaskedLM"),Kk.forEach(o),Py=n(_," does not inherit from "),wi=i(_,"CODE",{});var Qk=s(wi);Iy=n(Qk,"BrandNewBertModel"),Qk.forEach(o),Ay=n(_,", but rather uses "),vi=i(_,"CODE",{});var Vk=s(vi);Ly=n(Vk,"BrandNewBertModel"),Vk.forEach(o),Ny=n(_,`
as a component that can be called in its forward pass to keep the level of abstraction low. Every new model requires a
configuration class, called `),bi=i(_,"CODE",{});var e4=s(bi);By=n(e4,"BrandNewBertConfig"),e4.forEach(o),My=n(_,`. This configuration is always stored as an attribute in
`),rn=i(_,"A",{href:!0});var t4=s(rn);Oy=n(t4,"PreTrainedModel"),t4.forEach(o),xy=n(_,", and thus can be accessed via the "),_i=i(_,"CODE",{});var o4=s(_i);jy=n(o4,"config"),o4.forEach(o),Cy=n(_,` attribute for all classes
inheriting from `),Ei=i(_,"CODE",{});var r4=s(Ei);qy=n(r4,"BrandNewBertPreTrainedModel"),r4.forEach(o),Sy=n(_,":"),_.forEach(o),rf=f(e),u(bo.$$.fragment,e),nf=f(e),I=i(e,"P",{});var ne=s(I);Dy=n(ne,`Similar to the model, the configuration inherits basic serialization and deserialization functionalities from
`),nn=i(ne,"A",{href:!0});var n4=s(nn);Ry=n(n4,"PretrainedConfig"),n4.forEach(o),Fy=n(ne,`. Note that the configuration and the model are always serialized into two
different formats - the model to a `),ki=i(ne,"EM",{});var a4=s(ki);zy=n(a4,"pytorch_model.bin"),a4.forEach(o),Hy=n(ne," file and the configuration to a "),Ti=i(ne,"EM",{});var i4=s(Ti);Wy=n(i4,"config.json"),i4.forEach(o),Gy=n(ne,` file. Calling
`),an=i(ne,"A",{href:!0});var s4=s(an);Uy=n(s4,"save_pretrained()"),s4.forEach(o),Yy=n(ne,` will automatically call
`),sn=i(ne,"A",{href:!0});var l4=s(sn);Jy=n(l4,"save_pretrained()"),l4.forEach(o),Xy=n(ne,", so that both model and configuration are saved."),ne.forEach(o),af=f(e),qe=i(e,"H3",{class:!0});var yu=s(qe);ft=i(yu,"A",{id:!0,class:!0,href:!0});var d4=s(ft);$i=i(d4,"SPAN",{});var h4=s($i);u(_o.$$.fragment,h4),h4.forEach(o),d4.forEach(o),Zy=f(yu),Pi=i(yu,"SPAN",{});var f4=s(Pi);Ky=n(f4,"Code style"),f4.forEach(o),yu.forEach(o),sf=f(e),ln=i(e,"P",{});var m4=s(ln);Qy=n(m4,`When coding your new model, keep in mind that Transformers is an opinionated library and we have a few quirks of our
own regarding how code should be written :-)`),m4.forEach(o),lf=f(e),O=i(e,"OL",{});var Pe=s(O);Se=i(Pe,"LI",{});var Na=s(Se);Vy=n(Na,`The forward pass of your model should be fully written in the modeling file while being fully independent of other
models in the library. If you want to reuse a block from another model, copy the code and paste it with a
`),Ii=i(Na,"CODE",{});var p4=s(Ii);eg=n(p4,"# Copied from"),p4.forEach(o),tg=n(Na," comment on top (see "),Eo=i(Na,"A",{href:!0,rel:!0});var u4=s(Eo);og=n(u4,"here"),u4.forEach(o),rg=n(Na,`
for a good example).`),Na.forEach(o),ng=f(Pe),De=i(Pe,"LI",{});var Ba=s(De);ag=n(Ba,`The code should be fully understandable, even by a non-native English speaker. This means you should pick
descriptive variable names and avoid abbreviations. As an example, `),Ai=i(Ba,"CODE",{});var c4=s(Ai);ig=n(c4,"activation"),c4.forEach(o),sg=n(Ba," is preferred to "),Li=i(Ba,"CODE",{});var y4=s(Li);lg=n(y4,"act"),y4.forEach(o),dg=n(Ba,`.
One-letter variable names are strongly discouraged unless it\u2019s an index in a for loop.`),Ba.forEach(o),hg=f(Pe),Ni=i(Pe,"LI",{});var g4=s(Ni);fg=n(g4,"More generally we prefer longer explicit code to short magical one."),g4.forEach(o),mg=f(Pe),Re=i(Pe,"LI",{});var Ma=s(Re);pg=n(Ma,"Avoid subclassing "),Bi=i(Ma,"CODE",{});var w4=s(Bi);ug=n(w4,"nn.Sequential"),w4.forEach(o),cg=n(Ma," in PyTorch but subclass "),Mi=i(Ma,"CODE",{});var v4=s(Mi);yg=n(v4,"nn.Module"),v4.forEach(o),gg=n(Ma,` and write the forward pass, so that anyone
using your code can quickly debug it by adding print statements or breaking points.`),Ma.forEach(o),wg=f(Pe),Oi=i(Pe,"LI",{});var b4=s(Oi);vg=n(b4,`Your function signature should be type-annotated. For the rest, good variable names are way more readable and
understandable than type annotations.`),b4.forEach(o),Pe.forEach(o),df=f(e),Fe=i(e,"H3",{class:!0});var gu=s(Fe);mt=i(gu,"A",{id:!0,class:!0,href:!0});var _4=s(mt);xi=i(_4,"SPAN",{});var E4=s(xi);u(ko.$$.fragment,E4),E4.forEach(o),_4.forEach(o),bg=f(gu),ji=i(gu,"SPAN",{});var k4=s(ji);_g=n(k4,"Overview of tokenizers"),k4.forEach(o),gu.forEach(o),hf=f(e),dn=i(e,"P",{});var T4=s(dn);Eg=n(T4,"Not quite ready yet :-( This section will be added soon!"),T4.forEach(o),ff=f(e),ze=i(e,"H2",{class:!0});var wu=s(ze);pt=i(wu,"A",{id:!0,class:!0,href:!0});var $4=s(pt);Ci=i($4,"SPAN",{});var P4=s(Ci);u(To.$$.fragment,P4),P4.forEach(o),$4.forEach(o),kg=f(wu),qi=i(wu,"SPAN",{});var I4=s(qi);Tg=n(I4,"Step-by-step recipe to add a model to \u{1F917} Transformers"),I4.forEach(o),wu.forEach(o),mf=f(e),hn=i(e,"P",{});var A4=s(hn);$g=n(A4,`Everyone has different preferences of how to port a model so it can be very helpful for you to take a look at summaries
of how other contributors ported models to Hugging Face. Here is a list of community blog posts on how to port a model:`),A4.forEach(o),pf=f(e),ut=i(e,"OL",{});var vu=s(ut);$o=i(vu,"LI",{});var bu=s($o);Po=i(bu,"A",{href:!0,rel:!0});var L4=s(Po);Pg=n(L4,"Porting GPT2 Model"),L4.forEach(o),Ig=n(bu," by "),Io=i(bu,"A",{href:!0,rel:!0});var N4=s(Io);Ag=n(N4,"Thomas"),N4.forEach(o),bu.forEach(o),Lg=f(vu),Ao=i(vu,"LI",{});var _u=s(Ao);Lo=i(_u,"A",{href:!0,rel:!0});var B4=s(Lo);Ng=n(B4,"Porting WMT19 MT Model"),B4.forEach(o),Bg=n(_u," by "),No=i(_u,"A",{href:!0,rel:!0});var M4=s(No);Mg=n(M4,"Stas"),M4.forEach(o),_u.forEach(o),vu.forEach(o),uf=f(e),fn=i(e,"P",{});var O4=s(fn);Og=n(O4,"From experience, we can tell you that the most important things to keep in mind when adding a model are:"),O4.forEach(o),cf=f(e),ye=i(e,"UL",{});var Oa=s(ye);fe=i(Oa,"LI",{});var Vt=s(fe);xg=n(Vt,`Don\u2019t reinvent the wheel! Most parts of the code you will add for the new \u{1F917} Transformers model already exist
somewhere in \u{1F917} Transformers. Take some time to find similar, already existing models and tokenizers you can copy
from. `),Bo=i(Vt,"A",{href:!0,rel:!0});var x4=s(Bo);jg=n(x4,"grep"),x4.forEach(o),Cg=n(Vt," and "),Mo=i(Vt,"A",{href:!0,rel:!0});var j4=s(Mo);qg=n(j4,"rg"),j4.forEach(o),Sg=n(Vt,` are your
friends. Note that it might very well happen that your model\u2019s tokenizer is based on one model implementation, and
your model\u2019s modeling code on another one. `),Si=i(Vt,"EM",{});var C4=s(Si);Dg=n(C4,"E.g."),C4.forEach(o),Rg=n(Vt,` FSMT\u2019s modeling code is based on BART, while FSMT\u2019s tokenizer code
is based on XLM.`),Vt.forEach(o),Fg=f(Oa),Di=i(Oa,"LI",{});var q4=s(Di);zg=n(q4,`It\u2019s more of an engineering challenge than a scientific challenge. You should spend more time on creating an
efficient debugging environment than trying to understand all theoretical aspects of the model in the paper.`),q4.forEach(o),Hg=f(Oa),Ri=i(Oa,"LI",{});var S4=s(Ri);Wg=n(S4,`Ask for help, when you\u2019re stuck! Models are the core component of \u{1F917} Transformers so that we at Hugging Face are more
than happy to help you at every step to add your model. Don\u2019t hesitate to ask if you notice you are not making
progress.`),S4.forEach(o),Oa.forEach(o),yf=f(e),mn=i(e,"P",{});var D4=s(mn);Gg=n(D4,"In the following, we try to give you a general recipe that we found most useful when porting a model to \u{1F917} Transformers."),D4.forEach(o),gf=f(e),pn=i(e,"P",{});var R4=s(pn);Ug=n(R4,`The following list is a summary of everything that has to be done to add a model and can be used by you as a To-Do
List:`),R4.forEach(o),wf=f(e),b=i(e,"UL",{});var k=s(b);Fi=i(k,"LI",{});var F4=s(Fi);zi=i(F4,"OL",{});var z4=s(zi);Hi=i(z4,"LI",{});var H4=s(Hi);Yg=n(H4,"\u2610 (Optional) Understood theoretical aspects"),H4.forEach(o),z4.forEach(o),F4.forEach(o),Jg=f(k),Wi=i(k,"LI",{});var W4=s(Wi);un=i(W4,"OL",{start:!0});var G4=s(un);Gi=i(G4,"LI",{});var U4=s(Gi);Xg=n(U4,"\u2610 Prepared transformers dev environment"),U4.forEach(o),G4.forEach(o),W4.forEach(o),Zg=f(k),Ui=i(k,"LI",{});var Y4=s(Ui);cn=i(Y4,"OL",{start:!0});var J4=s(cn);Yi=i(J4,"LI",{});var X4=s(Yi);Kg=n(X4,"\u2610 Set up debugging environment of the original repository"),X4.forEach(o),J4.forEach(o),Y4.forEach(o),Qg=f(k),Ji=i(k,"LI",{});var Z4=s(Ji);yn=i(Z4,"OL",{start:!0});var K4=s(yn);Xi=i(K4,"LI",{});var Q4=s(Xi);Vg=n(Q4,"\u2610 Created script that successfully runs forward pass using original repository and checkpoint"),Q4.forEach(o),K4.forEach(o),Z4.forEach(o),ew=f(k),Zi=i(k,"LI",{});var V4=s(Zi);gn=i(V4,"OL",{start:!0});var e5=s(gn);Ki=i(e5,"LI",{});var t5=s(Ki);tw=n(t5,"\u2610 Successfully added the model skeleton to Transformers"),t5.forEach(o),e5.forEach(o),V4.forEach(o),ow=f(k),Qi=i(k,"LI",{});var o5=s(Qi);wn=i(o5,"OL",{start:!0});var r5=s(wn);Vi=i(r5,"LI",{});var n5=s(Vi);rw=n(n5,"\u2610 Successfully converted original checkpoint to Transformers checkpoint"),n5.forEach(o),r5.forEach(o),o5.forEach(o),nw=f(k),es=i(k,"LI",{});var a5=s(es);vn=i(a5,"OL",{start:!0});var i5=s(vn);ts=i(i5,"LI",{});var s5=s(ts);aw=n(s5,"\u2610 Successfully ran forward pass in Transformers that gives identical output to original checkpoint"),s5.forEach(o),i5.forEach(o),a5.forEach(o),iw=f(k),os=i(k,"LI",{});var l5=s(os);bn=i(l5,"OL",{start:!0});var d5=s(bn);rs=i(d5,"LI",{});var h5=s(rs);sw=n(h5,"\u2610 Finished model tests in Transformers"),h5.forEach(o),d5.forEach(o),l5.forEach(o),lw=f(k),ns=i(k,"LI",{});var f5=s(ns);_n=i(f5,"OL",{start:!0});var m5=s(_n);as=i(m5,"LI",{});var p5=s(as);dw=n(p5,"\u2610 Successfully added Tokenizer in Transformers"),p5.forEach(o),m5.forEach(o),f5.forEach(o),hw=f(k),is=i(k,"LI",{});var u5=s(is);En=i(u5,"OL",{start:!0});var c5=s(En);ss=i(c5,"LI",{});var y5=s(ss);fw=n(y5,"\u2610 Run end-to-end integration tests"),y5.forEach(o),c5.forEach(o),u5.forEach(o),mw=f(k),ls=i(k,"LI",{});var g5=s(ls);kn=i(g5,"OL",{start:!0});var w5=s(kn);ds=i(w5,"LI",{});var v5=s(ds);pw=n(v5,"\u2610 Finished docs"),v5.forEach(o),w5.forEach(o),g5.forEach(o),uw=f(k),hs=i(k,"LI",{});var b5=s(hs);Tn=i(b5,"OL",{start:!0});var _5=s(Tn);fs=i(_5,"LI",{});var E5=s(fs);cw=n(E5,"\u2610 Uploaded model weights to the hub"),E5.forEach(o),_5.forEach(o),b5.forEach(o),yw=f(k),ms=i(k,"LI",{});var k5=s(ms);$n=i(k5,"OL",{start:!0});var T5=s($n);ps=i(T5,"LI",{});var $5=s(ps);gw=n($5,"\u2610 Submitted the pull request"),$5.forEach(o),T5.forEach(o),k5.forEach(o),ww=f(k),us=i(k,"LI",{});var P5=s(us);Pn=i(P5,"OL",{start:!0});var I5=s(Pn);cs=i(I5,"LI",{});var A5=s(cs);vw=n(A5,"\u2610 (Optional) Added a demo notebook"),A5.forEach(o),I5.forEach(o),P5.forEach(o),k.forEach(o),vf=f(e),x=i(e,"P",{});var Ie=s(x);bw=n(Ie,"To begin with, we usually recommend to start by getting a good theoretical understanding of "),ys=i(Ie,"CODE",{});var L5=s(ys);_w=n(L5,"BrandNewBert"),L5.forEach(o),Ew=n(Ie,`. However,
if you prefer to understand the theoretical aspects of the model `),gs=i(Ie,"EM",{});var N5=s(gs);kw=n(N5,"on-the-job"),N5.forEach(o),Tw=n(Ie,`, then it is totally fine to directly dive
into the `),ws=i(Ie,"CODE",{});var B5=s(ws);$w=n(B5,"BrandNewBert"),B5.forEach(o),Pw=n(Ie,`\u2019s code-base. This option might suit you better, if your engineering skills are better than
your theoretical skill, if you have trouble understanding `),vs=i(Ie,"CODE",{});var M5=s(vs);Iw=n(M5,"BrandNewBert"),M5.forEach(o),Aw=n(Ie,`\u2019s paper, or if you just enjoy programming
much more than reading scientific papers.`),Ie.forEach(o),bf=f(e),He=i(e,"H3",{class:!0});var Eu=s(He);ct=i(Eu,"A",{id:!0,class:!0,href:!0});var O5=s(ct);bs=i(O5,"SPAN",{});var x5=s(bs);u(Oo.$$.fragment,x5),x5.forEach(o),O5.forEach(o),Lw=f(Eu),_s=i(Eu,"SPAN",{});var j5=s(_s);Nw=n(j5,"1. (Optional) Theoretical aspects of BrandNewBert"),j5.forEach(o),Eu.forEach(o),_f=f(e),yt=i(e,"P",{});var ku=s(yt);Bw=n(ku,"You should take some time to read "),Es=i(ku,"EM",{});var C5=s(Es);Mw=n(C5,"BrandNewBert\u2019s"),C5.forEach(o),Ow=n(ku,` paper, if such descriptive work exists. There might be large
sections of the paper that are difficult to understand. If this is the case, this is fine - don\u2019t worry! The goal is
not to get a deep theoretical understanding of the paper, but to extract the necessary information required to
effectively re-implement the model in \u{1F917} Transformers. That being said, you don\u2019t have to spend too much time on the
theoretical aspects, but rather focus on the practical ones, namely:`),ku.forEach(o),Ef=f(e),j=i(e,"UL",{});var Ae=s(j);We=i(Ae,"LI",{});var xa=s(We);xw=n(xa,"What type of model is "),ks=i(xa,"EM",{});var q5=s(ks);jw=n(q5,"brand_new_bert"),q5.forEach(o),Cw=n(xa,`? BERT-like encoder-only model? GPT2-like decoder-only model? BART-like
encoder-decoder model? Look at the `),In=i(xa,"A",{href:!0});var S5=s(In);qw=n(S5,"model_summary"),S5.forEach(o),Sw=n(xa," if you\u2019re not familiar with the differences between those."),xa.forEach(o),Dw=f(Ae),Ge=i(Ae,"LI",{});var ja=s(Ge);Rw=n(ja,"What are the applications of "),Ts=i(ja,"EM",{});var D5=s(Ts);Fw=n(D5,"brand_new_bert"),D5.forEach(o),zw=n(ja,"? Text classification? Text generation? Seq2Seq tasks, "),$s=i(ja,"EM",{});var R5=s($s);Hw=n(R5,"e.g.,"),R5.forEach(o),Ww=n(ja,`
summarization?`),ja.forEach(o),Gw=f(Ae),Ps=i(Ae,"LI",{});var F5=s(Ps);Uw=n(F5,"What is the novel feature of the model making it different from BERT/GPT-2/BART?"),F5.forEach(o),Yw=f(Ae),Ue=i(Ae,"LI",{});var Ca=s(Ue);Jw=n(Ca,"Which of the already existing "),xo=i(Ca,"A",{href:!0,rel:!0});var z5=s(xo);Xw=n(z5,"\u{1F917} Transformers models"),z5.forEach(o),Zw=n(Ca,` is most
similar to `),Is=i(Ca,"EM",{});var H5=s(Is);Kw=n(H5,"brand_new_bert"),H5.forEach(o),Qw=n(Ca,"?"),Ca.forEach(o),Vw=f(Ae),As=i(Ae,"LI",{});var W5=s(As);ev=n(W5,`What type of tokenizer is used? A sentencepiece tokenizer? Word piece tokenizer? Is it the same tokenizer as used
for BERT or BART?`),W5.forEach(o),Ae.forEach(o),kf=f(e),An=i(e,"P",{});var G5=s(An);tv=n(G5,`After you feel like you have gotten a good overview of the architecture of the model, you might want to write to the
Hugging Face team with any questions you might have. This might include questions regarding the model\u2019s architecture,
its attention layer, etc. We will be more than happy to help you.`),G5.forEach(o),Tf=f(e),Ye=i(e,"H3",{class:!0});var Tu=s(Ye);gt=i(Tu,"A",{id:!0,class:!0,href:!0});var U5=s(gt);Ls=i(U5,"SPAN",{});var Y5=s(Ls);u(jo.$$.fragment,Y5),Y5.forEach(o),U5.forEach(o),ov=f(Tu),Ns=i(Tu,"SPAN",{});var J5=s(Ns);rv=n(J5,"2. Next prepare your environment"),J5.forEach(o),Tu.forEach(o),$f=f(e),wt=i(e,"OL",{});var $u=s(wt);Bs=i($u,"LI",{});var X5=s(Bs);Co=i(X5,"P",{});var Pu=s(Co);nv=n(Pu,"Fork the "),qo=i(Pu,"A",{href:!0,rel:!0});var Z5=s(qo);av=n(Z5,"repository"),Z5.forEach(o),iv=n(Pu,` by clicking on the \u2018Fork\u2019 button on the
repository\u2019s page. This creates a copy of the code under your GitHub user account.`),Pu.forEach(o),X5.forEach(o),sv=f($u),Ms=i($u,"LI",{});var K5=s(Ms);So=i(K5,"P",{});var Iu=s(So);lv=n(Iu,"Clone your "),Os=i(Iu,"CODE",{});var Q5=s(Os);dv=n(Q5,"transformers"),Q5.forEach(o),hv=n(Iu," fork to your local disk, and add the base repository as a remote:"),Iu.forEach(o),K5.forEach(o),$u.forEach(o),Pf=f(e),u(Do.$$.fragment,e),If=f(e),Ro=i(e,"OL",{start:!0});var V5=s(Ro);xs=i(V5,"LI",{});var e6=s(xs);fv=n(e6,"Set up a development environment, for instance by running the following command:"),e6.forEach(o),V5.forEach(o),Af=f(e),u(Fo.$$.fragment,e),Lf=f(e),Ln=i(e,"P",{});var t6=s(Ln);mv=n(t6,"and return to the parent directory"),t6.forEach(o),Nf=f(e),u(zo.$$.fragment,e),Bf=f(e),Ho=i(e,"OL",{start:!0});var o6=s(Ho);Je=i(o6,"LI",{});var qa=s(Je);pv=n(qa,"We recommend adding the PyTorch version of "),js=i(qa,"EM",{});var r6=s(js);uv=n(r6,"brand_new_bert"),r6.forEach(o),cv=n(qa,` to Transformers. To install PyTorch, please follow the
instructions on `),Wo=i(qa,"A",{href:!0,rel:!0});var n6=s(Wo);yv=n(n6,"https://pytorch.org/get-started/locally/"),n6.forEach(o),gv=n(qa,"."),qa.forEach(o),o6.forEach(o),Mf=f(e),Go=i(e,"P",{});var tk=s(Go);Cs=i(tk,"STRONG",{});var a6=s(Cs);wv=n(a6,"Note:"),a6.forEach(o),vv=n(tk," You don\u2019t need to have CUDA installed. Making the new model work on CPU is sufficient."),tk.forEach(o),Of=f(e),Uo=i(e,"OL",{start:!0});var i6=s(Uo);Yo=i(i6,"LI",{});var Au=s(Yo);bv=n(Au,"To port "),qs=i(Au,"EM",{});var s6=s(qs);_v=n(s6,"brand_new_bert"),s6.forEach(o),Ev=n(Au,", you will also need access to its original repository:"),Au.forEach(o),i6.forEach(o),xf=f(e),u(Jo.$$.fragment,e),jf=f(e),vt=i(e,"P",{});var Lu=s(vt);kv=n(Lu,"Now you have set up a development environment to port "),Ss=i(Lu,"EM",{});var l6=s(Ss);Tv=n(l6,"brand_new_bert"),l6.forEach(o),$v=n(Lu," to \u{1F917} Transformers."),Lu.forEach(o),Cf=f(e),Xe=i(e,"H3",{class:!0});var Nu=s(Xe);bt=i(Nu,"A",{id:!0,class:!0,href:!0});var d6=s(bt);Ds=i(d6,"SPAN",{});var h6=s(Ds);u(Xo.$$.fragment,h6),h6.forEach(o),d6.forEach(o),Pv=f(Nu),Rs=i(Nu,"SPAN",{});var f6=s(Rs);Iv=n(f6,"3.-4. Run a pretrained checkpoint using the original repository"),f6.forEach(o),Nu.forEach(o),qf=f(e),A=i(e,"P",{});var ae=s(A);Av=n(ae,"At first, you will work on the original "),Fs=i(ae,"EM",{});var m6=s(Fs);Lv=n(m6,"brand_new_bert"),m6.forEach(o),Nv=n(ae,` repository. Often, the original implementation is very
\u201Cresearchy\u201D. Meaning that documentation might be lacking and the code can be difficult to understand. But this should
be exactly your motivation to reimplement `),zs=i(ae,"EM",{});var p6=s(zs);Bv=n(p6,"brand_new_bert"),p6.forEach(o),Mv=n(ae,". At Hugging Face, one of our main goals is to "),Hs=i(ae,"EM",{});var u6=s(Hs);Ov=n(u6,`make people
stand on the shoulders of giants`),u6.forEach(o),xv=n(ae,` which translates here very well into taking a working model and rewriting it to make
it as `),Ws=i(ae,"STRONG",{});var c6=s(Ws);jv=n(c6,"accessible, user-friendly, and beautiful"),c6.forEach(o),Cv=n(ae,` as possible. This is the number-one motivation to re-implement
models into \u{1F917} Transformers - trying to make complex new NLP technology accessible to `),Gs=i(ae,"STRONG",{});var y6=s(Gs);qv=n(y6,"everybody"),y6.forEach(o),Sv=n(ae,"."),ae.forEach(o),Sf=f(e),Nn=i(e,"P",{});var g6=s(Nn);Dv=n(g6,"You should start thereby by diving into the original repository."),g6.forEach(o),Df=f(e),_t=i(e,"P",{});var Bu=s(_t);Rv=n(Bu,"Successfully running the official pretrained model in the original repository is often "),Us=i(Bu,"STRONG",{});var w6=s(Us);Fv=n(w6,"the most difficult"),w6.forEach(o),zv=n(Bu,` step.
From our experience, it is very important to spend some time getting familiar with the original code-base. You need to
figure out the following:`),Bu.forEach(o),Rf=f(e),L=i(e,"UL",{});var ie=s(L);Ys=i(ie,"LI",{});var v6=s(Ys);Hv=n(v6,"Where to find the pretrained weights?"),v6.forEach(o),Wv=f(ie),Js=i(ie,"LI",{});var b6=s(Js);Gv=n(b6,"How to load the pretrained weights into the corresponding model?"),b6.forEach(o),Uv=f(ie),Xs=i(ie,"LI",{});var _6=s(Xs);Yv=n(_6,"How to run the tokenizer independently from the model?"),_6.forEach(o),Jv=f(ie),Zs=i(ie,"LI",{});var E6=s(Zs);Xv=n(E6,`Trace one forward pass so that you know which classes and functions are required for a simple forward pass. Usually,
you only have to reimplement those functions.`),E6.forEach(o),Zv=f(ie),F=i(ie,"LI",{});var Le=s(F);Kv=n(Le,`Be able to locate the important components of the model: Where is the model\u2019s class? Are there model sub-classes,
`),Ks=i(Le,"EM",{});var k6=s(Ks);Qv=n(k6,"e.g."),k6.forEach(o),Vv=n(Le,` EncoderModel, DecoderModel? Where is the self-attention layer? Are there multiple different attention layers,
`),Qs=i(Le,"EM",{});var T6=s(Qs);eb=n(T6,"e.g."),T6.forEach(o),tb=f(Le),Vs=i(Le,"EM",{});var $6=s(Vs);ob=n($6,"self-attention"),$6.forEach(o),rb=n(Le,", "),el=i(Le,"EM",{});var P6=s(el);nb=n(P6,"cross-attention"),P6.forEach(o),ab=n(Le,"\u2026?"),Le.forEach(o),ib=f(ie),Ze=i(ie,"LI",{});var Sa=s(Ze);sb=n(Sa,"How can you debug the model in the original environment of the repo? Do you have to add "),tl=i(Sa,"EM",{});var I6=s(tl);lb=n(I6,"print"),I6.forEach(o),db=n(Sa,` statements, can you
work with an interactive debugger like `),ol=i(Sa,"EM",{});var A6=s(ol);hb=n(A6,"ipdb"),A6.forEach(o),fb=n(Sa,", or should you use an efficient IDE to debug the model, like PyCharm?"),Sa.forEach(o),ie.forEach(o),Ff=f(e),Et=i(e,"P",{});var Mu=s(Et);mb=n(Mu,"It is very important that before you start the porting process, that you can "),rl=i(Mu,"STRONG",{});var L6=s(rl);pb=n(L6,"efficiently"),L6.forEach(o),ub=n(Mu,` debug code in the original
repository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or
even a pull request in the original repository. The maintainers of this repository are most likely very happy about
someone looking into their code!`),Mu.forEach(o),zf=f(e),Bn=i(e,"P",{});var N6=s(Bn);cb=n(N6,`At this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original
model. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to
dive into the original repository and also when starting to write the \u{1F917} Transformers implementation of the model. Only
at the very end, when the model has already been successfully ported to \u{1F917} Transformers, one should verify that the
model also works as expected on GPU.`),N6.forEach(o),Hf=f(e),Mn=i(e,"P",{});var B6=s(Mn);yb=n(B6,"In general, there are two possible debugging environments for running the original model"),B6.forEach(o),Wf=f(e),kt=i(e,"UL",{});var Ou=s(kt);Zo=i(Ou,"LI",{});var xu=s(Zo);Ko=i(xu,"A",{href:!0,rel:!0});var M6=s(Ko);gb=n(M6,"Jupyter notebooks"),M6.forEach(o),wb=n(xu," / "),Qo=i(xu,"A",{href:!0,rel:!0});var O6=s(Qo);vb=n(O6,"google colab"),O6.forEach(o),xu.forEach(o),bb=f(Ou),nl=i(Ou,"LI",{});var x6=s(nl);_b=n(x6,"Local python scripts."),x6.forEach(o),Ou.forEach(o),Gf=f(e),On=i(e,"P",{});var j6=s(On);Eb=n(j6,`Jupyter notebooks have the advantage that they allow for cell-by-cell execution which can be helpful to better split
logical components from one another and to have faster debugging cycles as intermediate results can be stored. Also,
notebooks are often easier to share with other contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupiter notebooks, we strongly recommend you to work with them.`),j6.forEach(o),Uf=f(e),Tt=i(e,"P",{});var ju=s(Tt);kb=n(ju,`The obvious disadvantage of Jupyter notebooks is that if you are not used to working with them you will have to spend
some time adjusting to the new programming environment and that you might not be able to use your known debugging tools
anymore, like `),al=i(ju,"CODE",{});var C6=s(al);Tb=n(C6,"ipdb"),C6.forEach(o),$b=n(ju,"."),ju.forEach(o),Yf=f(e),$t=i(e,"P",{});var Cu=s($t);Pb=n(Cu,"For each code-base, a good first step is always to load a "),il=i(Cu,"STRONG",{});var q6=s(il);Ib=n(q6,"small"),q6.forEach(o),Ab=n(Cu,` pretrained checkpoint and to be able to reproduce a
single forward pass using a dummy integer vector of input IDs as an input. Such a script could look like this (in
pseudocode):`),Cu.forEach(o),Jf=f(e),u(Vo.$$.fragment,e),Xf=f(e),xn=i(e,"P",{});var S6=s(xn);Lb=n(S6,"Next, regarding the debugging strategy, there are generally a few from which to choose from:"),S6.forEach(o),Zf=f(e),Pt=i(e,"UL",{});var qu=s(Pt);sl=i(qu,"LI",{});var D6=s(sl);Nb=n(D6,`Decompose the original model into many small testable components and run a forward pass on each of those for
verification`),D6.forEach(o),Bb=f(qu),Ke=i(qu,"LI",{});var Da=s(Ke);Mb=n(Da,"Decompose the original model only into the original "),ll=i(Da,"EM",{});var R6=s(ll);Ob=n(R6,"tokenizer"),R6.forEach(o),xb=n(Da," and the original "),dl=i(Da,"EM",{});var F6=s(dl);jb=n(F6,"model"),F6.forEach(o),Cb=n(Da,`, run a forward pass on
those, and use intermediate print statements or breakpoints for verification`),Da.forEach(o),qu.forEach(o),Kf=f(e),jn=i(e,"P",{});var z6=s(jn);qb=n(z6,`Again, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code
base.`),z6.forEach(o),Qf=f(e),It=i(e,"P",{});var Su=s(It);Sb=n(Su,"If the original code-base allows you to decompose the model into smaller sub-components, "),hl=i(Su,"EM",{});var H6=s(hl);Db=n(H6,"e.g."),H6.forEach(o),Rb=n(Su,` if the original
code-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages
to taking the more difficult road in the beginning:`),Su.forEach(o),Vf=f(e),U=i(e,"UL",{});var eo=s(U);fl=i(eo,"LI",{});var W6=s(fl);Fb=n(W6,`at a later stage when comparing the original model to the Hugging Face implementation, you can verify automatically
for each component individually that the corresponding component of the \u{1F917} Transformers implementation matches instead
of relying on visual comparison via print statements`),W6.forEach(o),zb=f(eo),ml=i(eo,"LI",{});var G6=s(ml);Hb=n(G6,`it can give you some rope to decompose the big problem of porting a model into smaller problems of just porting
individual components and thus structure your work better`),G6.forEach(o),Wb=f(eo),pl=i(eo,"LI",{});var U6=s(pl);Gb=n(U6,`separating the model into logical meaningful components will help you to get a better overview of the model\u2019s design
and thus to better understand the model`),U6.forEach(o),Ub=f(eo),ul=i(eo,"LI",{});var Y6=s(ul);Yb=n(Y6,`at a later stage those component-by-component tests help you to ensure that no regression occurs as you continue
changing your code`),Y6.forEach(o),eo.forEach(o),em=f(e),er=i(e,"P",{});var ok=s(er);tr=i(ok,"A",{href:!0,rel:!0});var J6=s(tr);Jb=n(J6,"Lysandre\u2019s"),J6.forEach(o),Xb=n(ok,` integration checks for ELECTRA
gives a nice example of how this can be done.`),ok.forEach(o),tm=f(e),At=i(e,"P",{});var Du=s(At);Zb=n(Du,`However, if the original code-base is very complex or only allows intermediate components to be run in a compiled mode,
it might be too time-consuming or even impossible to separate the model into smaller testable sub-components. A good
example is `),or=i(Du,"A",{href:!0,rel:!0});var X6=s(or);Kb=n(X6,"T5\u2019s MeshTensorFlow"),X6.forEach(o),Qb=n(Du,` library which is
very complex and does not offer a simple way to decompose the model into its sub-components. For such libraries, one
often relies on verifying print statements.`),Du.forEach(o),om=f(e),Cn=i(e,"P",{});var Z6=s(Cn);Vb=n(Z6,`No matter which strategy you choose, the recommended procedure is often the same in that you should start to debug the
starting layers first and the ending layers last.`),Z6.forEach(o),rm=f(e),qn=i(e,"P",{});var K6=s(qn);e_=n(K6,`It is recommended that you retrieve the output, either by print statements or sub-component functions, of the following
layers in the following order:`),K6.forEach(o),nm=f(e),N=i(e,"OL",{});var se=s(N);cl=i(se,"LI",{});var Q6=s(cl);t_=n(Q6,"Retrieve the input IDs passed to the model"),Q6.forEach(o),o_=f(se),yl=i(se,"LI",{});var V6=s(yl);r_=n(V6,"Retrieve the word embeddings"),V6.forEach(o),n_=f(se),gl=i(se,"LI",{});var e7=s(gl);a_=n(e7,"Retrieve the input of the first Transformer layer"),e7.forEach(o),i_=f(se),wl=i(se,"LI",{});var t7=s(wl);s_=n(t7,"Retrieve the output of the first Transformer layer"),t7.forEach(o),l_=f(se),vl=i(se,"LI",{});var o7=s(vl);d_=n(o7,"Retrieve the output of the following n - 1 Transformer layers"),o7.forEach(o),h_=f(se),bl=i(se,"LI",{});var r7=s(bl);f_=n(r7,"Retrieve the output of the whole BrandNewBert Model"),r7.forEach(o),se.forEach(o),am=f(e),Qe=i(e,"P",{});var Dh=s(Qe);m_=n(Dh,"Input IDs should thereby consists of an array of integers, "),_l=i(Dh,"EM",{});var n7=s(_l);p_=n(n7,"e.g."),n7.forEach(o),u_=f(Dh),El=i(Dh,"CODE",{});var a7=s(El);c_=n(a7,"input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]"),a7.forEach(o),Dh.forEach(o),im=f(e),Sn=i(e,"P",{});var i7=s(Sn);y_=n(i7,"The outputs of the following layers often consist of multi-dimensional float arrays and can look like this:"),i7.forEach(o),sm=f(e),u(rr.$$.fragment,e),lm=f(e),ge=i(e,"P",{});var Ra=s(ge);g_=n(Ra,`We expect that every model added to \u{1F917} Transformers passes a couple of integration tests, meaning that the original
model and the reimplemented version in \u{1F917} Transformers have to give the exact same output up to a precision of 0.001!
Since it is normal that the exact same model written in different libraries can give a slightly different output
depending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives
nearly the same output, they have to be the almost identical. Therefore, you will certainly compare the intermediate
outputs of the \u{1F917} Transformers version multiple times against the intermediate outputs of the original implementation of
`),kl=i(Ra,"EM",{});var s7=s(kl);w_=n(s7,"brand_new_bert"),s7.forEach(o),v_=n(Ra," in which case an "),Tl=i(Ra,"STRONG",{});var l7=s(Tl);b_=n(l7,"efficient"),l7.forEach(o),__=n(Ra,` debugging environment of the original repository is absolutely
important. Here is some advice is to make your debugging environment as efficient as possible.`),Ra.forEach(o),dm=f(e),C=i(e,"UL",{});var Ne=s(C);z=i(Ne,"LI",{});var Be=s(z);E_=n(Be,`Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should
probably take the time to write a longer script that decomposes the original model into smaller sub-components to
retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on
TensorFlow print operations like `),nr=i(Be,"A",{href:!0,rel:!0});var d7=s(nr);k_=n(d7,"tf.print"),d7.forEach(o),T_=n(Be,` to output
intermediate values. Is the original repository written in Jax? Then make sure that the model is `),$l=i(Be,"STRONG",{});var h7=s($l);$_=n(h7,"not jitted"),h7.forEach(o),P_=n(Be,` when
running the forward pass, `),Pl=i(Be,"EM",{});var f7=s(Pl);I_=n(f7,"e.g."),f7.forEach(o),A_=n(Be," check-out "),ar=i(Be,"A",{href:!0,rel:!0});var m7=s(ar);L_=n(m7,"this link"),m7.forEach(o),N_=n(Be,"."),Be.forEach(o),B_=f(Ne),Il=i(Ne,"LI",{});var p7=s(Il);M_=n(p7,`Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle
becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.
In case only very large checkpoints are available, it might make more sense to create a dummy model in the new
environment with randomly initialized weights and save those weights for comparison with the \u{1F917} Transformers version
of your model`),p7.forEach(o),O_=f(Ne),T=i(Ne,"LI",{});var $=s(T);x_=n($,`Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want to
find the function in the original repository that `),Al=i($,"STRONG",{});var u7=s(Al);j_=n(u7,"only"),u7.forEach(o),C_=n($," calls a single forward pass, "),Ll=i($,"EM",{});var c7=s(Ll);q_=n(c7,"i.e."),c7.forEach(o),S_=n($,` that is often called
`),Nl=i($,"CODE",{});var y7=s(Nl);D_=n(y7,"predict"),y7.forEach(o),R_=n($,", "),Bl=i($,"CODE",{});var g7=s(Bl);F_=n(g7,"evaluate"),g7.forEach(o),z_=n($,", "),Ml=i($,"CODE",{});var w7=s(Ml);H_=n(w7,"forward"),w7.forEach(o),W_=n($," or "),Ol=i($,"CODE",{});var v7=s(Ol);G_=n(v7,"__call__"),v7.forEach(o),U_=n($,". You don\u2019t want to debug a function that calls "),xl=i($,"CODE",{});var b7=s(xl);Y_=n(b7,"forward"),b7.forEach(o),J_=n($,`
multiple times, `),jl=i($,"EM",{});var _7=s(jl);X_=n(_7,"e.g."),_7.forEach(o),Z_=n($," to generate text, like "),Cl=i($,"CODE",{});var E7=s(Cl);K_=n(E7,"autoregressive_sample"),E7.forEach(o),Q_=n($,", "),ql=i($,"CODE",{});var k7=s(ql);V_=n(k7,"generate"),k7.forEach(o),e1=n($,"."),$.forEach(o),t1=f(Ne),ir=i(Ne,"LI",{});var Ru=s(ir);o1=n(Ru,"Try to separate the tokenization from the model\u2019s "),Sl=i(Ru,"EM",{});var T7=s(Sl);r1=n(T7,"forward"),T7.forEach(o),n1=n(Ru,` pass. If the original repository shows examples where
you have to input a string, then try to find out where in the forward call the string input is changed to input ids
and start from this point. This might mean that you have to possibly write a small script yourself or change the
original code so that you can directly input the ids instead of an input string.`),Ru.forEach(o),a1=f(Ne),me=i(Ne,"LI",{});var to=s(me);i1=n(to,"Make sure that the model in your debugging setup is "),Dl=i(to,"STRONG",{});var $7=s(Dl);s1=n($7,"not"),$7.forEach(o),l1=n(to,` in training mode, which often causes the model to yield
random outputs due to multiple dropout layers in the model. Make sure that the forward pass in your debugging
environment is `),Rl=i(to,"STRONG",{});var P7=s(Rl);d1=n(P7,"deterministic"),P7.forEach(o),h1=n(to," so that the dropout layers are not used. Or use "),Fl=i(to,"EM",{});var I7=s(Fl);f1=n(I7,"transformers.file_utils.set_seed"),I7.forEach(o),m1=n(to,`
if the old and new implementations are in the same framework.`),to.forEach(o),Ne.forEach(o),hm=f(e),Lt=i(e,"P",{});var Fu=s(Lt);p1=n(Fu,"The following section gives you more specific details/tips on how you can do this for "),zl=i(Fu,"EM",{});var A7=s(zl);u1=n(A7,"brand_new_bert"),A7.forEach(o),c1=n(Fu,"."),Fu.forEach(o),fm=f(e),Ve=i(e,"H3",{class:!0});var zu=s(Ve);Nt=i(zu,"A",{id:!0,class:!0,href:!0});var L7=s(Nt);Hl=i(L7,"SPAN",{});var N7=s(Hl);u(sr.$$.fragment,N7),N7.forEach(o),L7.forEach(o),y1=f(zu),Wl=i(zu,"SPAN",{});var B7=s(Wl);g1=n(B7,"5.-14. Port BrandNewBert to \u{1F917} Transformers"),B7.forEach(o),zu.forEach(o),mm=f(e),Dn=i(e,"P",{});var M7=s(Dn);w1=n(M7,"Next, you can finally start adding new code to \u{1F917} Transformers. Go into the clone of your \u{1F917} Transformers\u2019 fork:"),M7.forEach(o),pm=f(e),u(lr.$$.fragment,e),um=f(e),Bt=i(e,"P",{});var Hu=s(Bt);v1=n(Hu,`In the special case that you are adding a model whose architecture exactly matches the model architecture of an
existing model you only have to add a conversion script as described in `),Rn=i(Hu,"A",{href:!0});var O7=s(Rn);b1=n(O7,"this section"),O7.forEach(o),_1=n(Hu,`.
In this case, you can just re-use the whole model architecture of the already existing model.`),Hu.forEach(o),cm=f(e),Fn=i(e,"P",{});var x7=s(Fn);E1=n(x7,"Otherwise, let\u2019s start generating a new model. You have two choices here:"),x7.forEach(o),ym=f(e),Mt=i(e,"UL",{});var Wu=s(Mt);zn=i(Wu,"LI",{});var rk=s(zn);Gl=i(rk,"CODE",{});var j7=s(Gl);k1=n(j7,"transformers-cli add-new-model-like"),j7.forEach(o),T1=n(rk," to add a new model like an existing one"),rk.forEach(o),$1=f(Wu),Hn=i(Wu,"LI",{});var nk=s(Hn);Ul=i(nk,"CODE",{});var C7=s(Ul);P1=n(C7,"transformers-cli add-new-model"),C7.forEach(o),I1=n(nk," to add a new model from our template (will look like BERT or Bart depending on the type of model you select)"),nk.forEach(o),Wu.forEach(o),gm=f(e),we=i(e,"P",{});var Fa=s(we);A1=n(Fa,"In both cases, you will be prompted with a questionnaire to fill the basic information of your model. The second command requires to install "),Yl=i(Fa,"CODE",{});var q7=s(Yl);L1=n(q7,"cookiecutter"),q7.forEach(o),N1=n(Fa,", you can find more information on it "),dr=i(Fa,"A",{href:!0,rel:!0});var S7=s(dr);B1=n(S7,"here"),S7.forEach(o),M1=n(Fa,"."),Fa.forEach(o),wm=f(e),Wn=i(e,"P",{});var D7=s(Wn);Jl=i(D7,"STRONG",{});var R7=s(Jl);O1=n(R7,"Open a Pull Request on the main huggingface/transformers repo"),R7.forEach(o),D7.forEach(o),vm=f(e),ve=i(e,"P",{});var za=s(ve);x1=n(za,`Before starting to adapt the automatically generated code, now is the time to open a \u201CWork in progress (WIP)\u201D pull
request, `),Xl=i(za,"EM",{});var F7=s(Xl);j1=n(F7,"e.g."),F7.forEach(o),C1=n(za," \u201C[WIP] Add "),Zl=i(za,"EM",{});var z7=s(Zl);q1=n(z7,"brand_new_bert"),z7.forEach(o),S1=n(za,`\u201D, in \u{1F917} Transformers so that you and the Hugging Face team can work
side-by-side on integrating the model into \u{1F917} Transformers.`),za.forEach(o),bm=f(e),Gn=i(e,"P",{});var H7=s(Gn);D1=n(H7,"You should do the following:"),H7.forEach(o),_m=f(e),Un=i(e,"OL",{});var W7=s(Un);Kl=i(W7,"LI",{});var G7=s(Kl);R1=n(G7,"Create a branch with a descriptive name from your main branch"),G7.forEach(o),W7.forEach(o),Em=f(e),u(hr.$$.fragment,e),km=f(e),fr=i(e,"OL",{start:!0});var U7=s(fr);Ql=i(U7,"LI",{});var Y7=s(Ql);F1=n(Y7,"Commit the automatically generated code:"),Y7.forEach(o),U7.forEach(o),Tm=f(e),u(mr.$$.fragment,e),$m=f(e),pr=i(e,"OL",{start:!0});var J7=s(pr);Vl=i(J7,"LI",{});var X7=s(Vl);z1=n(X7,"Fetch and rebase to current main"),X7.forEach(o),J7.forEach(o),Pm=f(e),u(ur.$$.fragment,e),Im=f(e),cr=i(e,"OL",{start:!0});var Z7=s(cr);ed=i(Z7,"LI",{});var K7=s(ed);H1=n(K7,"Push the changes to your account using:"),K7.forEach(o),Z7.forEach(o),Am=f(e),u(yr.$$.fragment,e),Lm=f(e),et=i(e,"OL",{start:!0});var Gu=s(et);td=i(Gu,"LI",{});var Q7=s(td);od=i(Q7,"P",{});var V7=s(od);W1=n(V7,`Once you are satisfied, go to the webpage of your fork on GitHub. Click on \u201CPull request\u201D. Make sure to add the
GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for
future changes.`),V7.forEach(o),Q7.forEach(o),G1=f(Gu),rd=i(Gu,"LI",{});var eT=s(rd);nd=i(eT,"P",{});var tT=s(nd);U1=n(tT,"Change the PR into a draft by clicking on \u201CConvert to draft\u201D on the right of the GitHub pull request web page."),tT.forEach(o),eT.forEach(o),Gu.forEach(o),Nm=f(e),Yn=i(e,"P",{});var oT=s(Yn);Y1=n(oT,`In the following, whenever you have done some progress, don\u2019t forget to commit your work and push it to your account so
that it shows in the pull request. Additionally, you should make sure to update your work with the current main from
time to time by doing:`),oT.forEach(o),Bm=f(e),u(gr.$$.fragment,e),Mm=f(e),Jn=i(e,"P",{});var rT=s(Jn);J1=n(rT,`In general, all questions you might have regarding the model or your implementation should be asked in your PR and
discussed/solved in the PR. This way, the Hugging Face team will always be notified when you are committing new code or
if you have a question. It is often very helpful to point the Hugging Face team to your added code so that the Hugging
Face team can efficiently understand your problem or question.`),rT.forEach(o),Om=f(e),Xn=i(e,"P",{});var nT=s(Xn);X1=n(nT,`To do so, you can go to the \u201CFiles changed\u201D tab where you see all of your changes, go to a line regarding which you
want to ask a question, and click on the \u201C+\u201D symbol to add a comment. Whenever a question or problem has been solved,
you can click on the \u201CResolve\u201D button of the created comment.`),nT.forEach(o),xm=f(e),Zn=i(e,"P",{});var aT=s(Zn);Z1=n(aT,`In the same way, the Hugging Face team will open comments when reviewing your code. We recommend asking most questions
on GitHub on your PR. For some very general questions that are not very useful for the public, feel free to ping the
Hugging Face team by Slack or email.`),aT.forEach(o),jm=f(e),Kn=i(e,"P",{});var iT=s(Kn);ad=i(iT,"STRONG",{});var sT=s(ad);K1=n(sT,"5. Adapt the generated models code for brand_new_bert"),sT.forEach(o),iT.forEach(o),Cm=f(e),be=i(e,"P",{});var Ha=s(be);Q1=n(Ha,`At first, we will focus only on the model itself and not care about the tokenizer. All the relevant code should be
found in the generated files `),id=i(Ha,"CODE",{});var lT=s(id);V1=n(lT,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),lT.forEach(o),e0=n(Ha,` and
`),sd=i(Ha,"CODE",{});var dT=s(sd);t0=n(dT,"src/transformers/models/brand_new_bert/configuration_brand_new_bert.py"),dT.forEach(o),o0=n(Ha,"."),Ha.forEach(o),qm=f(e),Y=i(e,"P",{});var oo=s(Y);r0=n(oo,`Now you can finally start coding :). The generated code in
`),ld=i(oo,"CODE",{});var hT=s(ld);n0=n(hT,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),hT.forEach(o),a0=n(oo,` will either have the same architecture as BERT if
it\u2019s an encoder-only model or BART if it\u2019s an encoder-decoder model. At this point, you should remind yourself what
you\u2019ve learned in the beginning about the theoretical aspects of the model: `),dd=i(oo,"EM",{});var fT=s(dd);i0=n(fT,`How is the model different from BERT or
BART?`),fT.forEach(o),s0=n(oo,"\u201D. Implement those changes which often means to change the "),hd=i(oo,"EM",{});var mT=s(hd);l0=n(mT,"self-attention"),mT.forEach(o),d0=n(oo,` layer, the order of the normalization
layer, etc\u2026 Again, it is often useful to look at the similar architecture of already existing models in Transformers to
get a better feeling of how your model should be implemented.`),oo.forEach(o),Sm=f(e),M=i(e,"P",{});var pe=s(M);fd=i(pe,"STRONG",{});var pT=s(fd);h0=n(pT,"Note"),pT.forEach(o),f0=n(pe,` that at this point, you don\u2019t have to be very sure that your code is fully correct or clean. Rather, it is
advised to add a first `),md=i(pe,"EM",{});var uT=s(md);m0=n(uT,"unclean"),uT.forEach(o),p0=n(pe,`, copy-pasted version of the original code to
`),pd=i(pe,"CODE",{});var cT=s(pd);u0=n(cT,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),cT.forEach(o),c0=n(pe,` until you feel like all the necessary code is
added. From our experience, it is much more efficient to quickly add a first version of the required code and
improve/correct the code iteratively with the conversion script as described in the next section. The only thing that
has to work at this point is that you can instantiate the \u{1F917} Transformers implementation of `),ud=i(pe,"EM",{});var yT=s(ud);y0=n(yT,"brand_new_bert"),yT.forEach(o),g0=n(pe,", "),cd=i(pe,"EM",{});var gT=s(cd);w0=n(gT,"i.e."),gT.forEach(o),v0=n(pe,` the
following command should work:`),pe.forEach(o),Dm=f(e),u(wr.$$.fragment,e),Rm=f(e),_e=i(e,"P",{});var Wa=s(_e);b0=n(Wa,"The above command will create a model according to the default parameters as defined in "),yd=i(Wa,"CODE",{});var wT=s(yd);_0=n(wT,"BrandNewBertConfig()"),wT.forEach(o),E0=n(Wa,` with
random weights, thus making sure that the `),gd=i(Wa,"CODE",{});var vT=s(gd);k0=n(vT,"init()"),vT.forEach(o),T0=n(Wa," methods of all components works."),Wa.forEach(o),Fm=f(e),Qn=i(e,"P",{});var bT=s(Qn);wd=i(bT,"STRONG",{});var _T=s(wd);$0=n(_T,"6. Write a conversion script"),_T.forEach(o),bT.forEach(o),zm=f(e),J=i(e,"P",{});var ro=s(J);P0=n(ro,"Next, you should write a conversion script that lets you convert the checkpoint you used to debug "),vd=i(ro,"EM",{});var ET=s(vd);I0=n(ET,"brand_new_bert"),ET.forEach(o),A0=n(ro,` in
the original repository to a checkpoint compatible with your just created \u{1F917} Transformers implementation of
`),bd=i(ro,"EM",{});var kT=s(bd);L0=n(kT,"brand_new_bert"),kT.forEach(o),N0=n(ro,`. It is not advised to write the conversion script from scratch, but rather to look through already
existing conversion scripts in \u{1F917} Transformers for one that has been used to convert a similar model that was written in
the same framework as `),_d=i(ro,"EM",{});var TT=s(_d);B0=n(TT,"brand_new_bert"),TT.forEach(o),M0=n(ro,`. Usually, it is enough to copy an already existing conversion script and
slightly adapt it for your use case. Don\u2019t hesitate to ask the Hugging Face team to point you to a similar already
existing conversion script for your model.`),ro.forEach(o),Hm=f(e),Ot=i(e,"UL",{});var Uu=s(Ot);Vn=i(Uu,"LI",{});var ak=s(Vn);O0=n(ak,"If you are porting a model from TensorFlow to PyTorch, a good starting point might be BERT\u2019s conversion script "),vr=i(ak,"A",{href:!0,rel:!0});var $T=s(vr);x0=n($T,"here"),$T.forEach(o),ak.forEach(o),j0=f(Uu),ea=i(Uu,"LI",{});var ik=s(ea);C0=n(ik,"If you are porting a model from PyTorch to PyTorch, a good starting point might be BART\u2019s conversion script "),br=i(ik,"A",{href:!0,rel:!0});var PT=s(br);q0=n(PT,"here"),PT.forEach(o),ik.forEach(o),Uu.forEach(o),Wm=f(e),xt=i(e,"P",{});var Yu=s(xt);S0=n(Yu,`In the following, we\u2019ll quickly explain how PyTorch models store layer weights and define layer names. In PyTorch, the
name of a layer is defined by the name of the class attribute you give the layer. Let\u2019s define a dummy model in
PyTorch, called `),Ed=i(Yu,"CODE",{});var IT=s(Ed);D0=n(IT,"SimpleModel"),IT.forEach(o),R0=n(Yu," as follows:"),Yu.forEach(o),Gm=f(e),u(_r.$$.fragment,e),Um=f(e),X=i(e,"P",{});var no=s(X);F0=n(no,"Now we can create an instance of this model definition which will fill all weights: "),kd=i(no,"CODE",{});var AT=s(kd);z0=n(AT,"dense"),AT.forEach(o),H0=n(no,", "),Td=i(no,"CODE",{});var LT=s(Td);W0=n(LT,"intermediate"),LT.forEach(o),G0=n(no,`,
`),$d=i(no,"CODE",{});var NT=s($d);U0=n(NT,"layer_norm"),NT.forEach(o),Y0=n(no," with random weights. We can print the model to see its architecture"),no.forEach(o),Ym=f(e),u(Er.$$.fragment,e),Jm=f(e),ta=i(e,"P",{});var BT=s(ta);J0=n(BT,"This will print out the following:"),BT.forEach(o),Xm=f(e),u(kr.$$.fragment,e),Zm=f(e),oa=i(e,"P",{});var MT=s(oa);X0=n(MT,`We can see that the layer names are defined by the name of the class attribute in PyTorch. You can print out the weight
values of a specific layer:`),MT.forEach(o),Km=f(e),u(Tr.$$.fragment,e),Qm=f(e),ra=i(e,"P",{});var OT=s(ra);Z0=n(OT,"to see that the weights were randomly initialized"),OT.forEach(o),Vm=f(e),u($r.$$.fragment,e),ep=f(e),Pr=i(e,"P",{});var sk=s(Pr);K0=n(sk,`In the conversion script, you should fill those randomly initialized weights with the exact weights of the
corresponding layer in the checkpoint. `),Pd=i(sk,"EM",{});var xT=s(Pd);Q0=n(xT,"E.g."),xT.forEach(o),sk.forEach(o),tp=f(e),u(Ir.$$.fragment,e),op=f(e),Ee=i(e,"P",{});var Ga=s(Ee);V0=n(Ga,`While doing so, you must verify that each randomly initialized weight of your PyTorch model and its corresponding
pretrained checkpoint weight exactly match in both `),Id=i(Ga,"STRONG",{});var jT=s(Id);e2=n(jT,"shape and name"),jT.forEach(o),t2=n(Ga,". To do so, it is "),Ad=i(Ga,"STRONG",{});var CT=s(Ad);o2=n(CT,"necessary"),CT.forEach(o),r2=n(Ga,` to add assert
statements for the shape and print out the names of the checkpoints weights. E.g. you should add statements like:`),Ga.forEach(o),rp=f(e),u(Ar.$$.fragment,e),np=f(e),Lr=i(e,"P",{});var lk=s(Lr);n2=n(lk,"Besides, you should also print out the names of both weights to make sure they match, "),Ld=i(lk,"EM",{});var qT=s(Ld);a2=n(qT,"e.g."),qT.forEach(o),lk.forEach(o),ap=f(e),u(Nr.$$.fragment,e),ip=f(e),na=i(e,"P",{});var ST=s(na);i2=n(ST,`If either the shape or the name doesn\u2019t match, you probably assigned the wrong checkpoint weight to a randomly
initialized layer of the \u{1F917} Transformers implementation.`),ST.forEach(o),sp=f(e),jt=i(e,"P",{});var Ju=s(jt);s2=n(Ju,"An incorrect shape is most likely due to an incorrect setting of the config parameters in "),Nd=i(Ju,"CODE",{});var DT=s(Nd);l2=n(DT,"BrandNewBertConfig()"),DT.forEach(o),d2=n(Ju,` that
do not exactly match those that were used for the checkpoint you want to convert. However, it could also be that
PyTorch\u2019s implementation of a layer requires the weight to be transposed beforehand.`),Ju.forEach(o),lp=f(e),Z=i(e,"P",{});var ao=s(Z);h2=n(ao,"Finally, you should also check that "),Bd=i(ao,"STRONG",{});var RT=s(Bd);f2=n(RT,"all"),RT.forEach(o),m2=n(ao,` required weights are initialized and print out all checkpoint weights that
were not used for initialization to make sure the model is correctly converted. It is completely normal, that the
conversion trials fail with either a wrong shape statement or wrong name assignment. This is most likely because either
you used incorrect parameters in `),Md=i(ao,"CODE",{});var FT=s(Md);p2=n(FT,"BrandNewBertConfig()"),FT.forEach(o),u2=n(ao,`, have a wrong architecture in the \u{1F917} Transformers
implementation, you have a bug in the `),Od=i(ao,"CODE",{});var zT=s(Od);c2=n(zT,"init()"),zT.forEach(o),y2=n(ao,` functions of one of the components of the \u{1F917} Transformers
implementation or you need to transpose one of the checkpoint weights.`),ao.forEach(o),dp=f(e),K=i(e,"P",{});var io=s(K);g2=n(io,`This step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the
Transformers model. Having correctly loaded the checkpoint into the \u{1F917} Transformers implementation, you can then save
the model under a folder of your choice `),xd=i(io,"CODE",{});var HT=s(xd);w2=n(HT,"/path/to/converted/checkpoint/folder"),HT.forEach(o),v2=n(io,` that should then contain both a
`),jd=i(io,"CODE",{});var WT=s(jd);b2=n(WT,"pytorch_model.bin"),WT.forEach(o),_2=n(io," file and a "),Cd=i(io,"CODE",{});var GT=s(Cd);E2=n(GT,"config.json"),GT.forEach(o),k2=n(io," file:"),io.forEach(o),hp=f(e),u(Br.$$.fragment,e),fp=f(e),aa=i(e,"P",{});var UT=s(aa);qd=i(UT,"STRONG",{});var YT=s(qd);T2=n(YT,"7. Implement the forward pass"),YT.forEach(o),UT.forEach(o),mp=f(e),Ct=i(e,"P",{});var Xu=s(Ct);$2=n(Xu,`Having managed to correctly load the pretrained weights into the \u{1F917} Transformers implementation, you should now make
sure that the forward pass is correctly implemented. In `),ia=i(Xu,"A",{href:!0});var JT=s(ia);P2=n(JT,"Get familiar with the original repository"),JT.forEach(o),I2=n(Xu,`, you have already created a script that runs a forward
pass of the model using the original repository. Now you should write an analogous script using the \u{1F917} Transformers
implementation instead of the original one. It should look as follows:`),Xu.forEach(o),pp=f(e),u(Mr.$$.fragment,e),up=f(e),q=i(e,"P",{});var Me=s(q);A2=n(Me,`It is very likely that the \u{1F917} Transformers implementation and the original model implementation don\u2019t give the exact
same output the very first time or that the forward pass throws an error. Don\u2019t be disappointed - it\u2019s expected! First,
you should make sure that the forward pass doesn\u2019t throw any errors. It often happens that the wrong dimensions are
used leading to a `),Sd=i(Me,"EM",{});var XT=s(Sd);L2=n(XT,"Dimensionality mismatch"),XT.forEach(o),N2=n(Me," error or that the wrong data type object is used, "),Dd=i(Me,"EM",{});var ZT=s(Dd);B2=n(ZT,"e.g."),ZT.forEach(o),M2=f(Me),Rd=i(Me,"CODE",{});var KT=s(Rd);O2=n(KT,"torch.long"),KT.forEach(o),x2=n(Me,`
instead of `),Fd=i(Me,"CODE",{});var QT=s(Fd);j2=n(QT,"torch.float32"),QT.forEach(o),C2=n(Me,`. Don\u2019t hesitate to ask the Hugging Face team for help, if you don\u2019t manage to solve
certain errors.`),Me.forEach(o),cp=f(e),Q=i(e,"P",{});var so=s(Q);q2=n(so,`The final part to make sure the \u{1F917} Transformers implementation works correctly is to ensure that the outputs are
equivalent to a precision of `),zd=i(so,"CODE",{});var VT=s(zd);S2=n(VT,"1e-3"),VT.forEach(o),D2=n(so,". First, you should ensure that the output shapes are identical, "),Hd=i(so,"EM",{});var e8=s(Hd);R2=n(e8,"i.e."),e8.forEach(o),F2=f(so),Wd=i(so,"CODE",{});var t8=s(Wd);z2=n(t8,"outputs.shape"),t8.forEach(o),H2=n(so,` should yield the same value for the script of the \u{1F917} Transformers implementation and the original
implementation. Next, you should make sure that the output values are identical as well. This one of the most difficult
parts of adding a new model. Common mistakes why the outputs are not identical are:`),so.forEach(o),yp=f(e),V=i(e,"UL",{});var lo=s(V);tt=i(lo,"LI",{});var Ua=s(tt);W2=n(Ua,"Some layers were not added, "),Gd=i(Ua,"EM",{});var o8=s(Gd);G2=n(o8,"i.e."),o8.forEach(o),U2=n(Ua," an "),Ud=i(Ua,"EM",{});var r8=s(Ud);Y2=n(r8,"activation"),r8.forEach(o),J2=n(Ua," layer was not added, or the residual connection was forgotten"),Ua.forEach(o),X2=f(lo),Yd=i(lo,"LI",{});var n8=s(Yd);Z2=n(n8,"The word embedding matrix was not tied"),n8.forEach(o),K2=f(lo),Jd=i(lo,"LI",{});var a8=s(Jd);Q2=n(a8,"The wrong positional embeddings are used because the original implementation uses on offset"),a8.forEach(o),V2=f(lo),ee=i(lo,"LI",{});var rt=s(ee);eE=n(rt,"Dropout is applied during the forward pass. To fix this make sure "),Xd=i(rt,"EM",{});var i8=s(Xd);tE=n(i8,"model.training is False"),i8.forEach(o),oE=n(rt,` and that no dropout
layer is falsely activated during the forward pass, `),Zd=i(rt,"EM",{});var s8=s(Zd);rE=n(s8,"i.e."),s8.forEach(o),nE=n(rt," pass "),Kd=i(rt,"EM",{});var l8=s(Kd);aE=n(l8,"self.training"),l8.forEach(o),iE=n(rt," to "),Or=i(rt,"A",{href:!0,rel:!0});var d8=s(Or);sE=n(d8,"PyTorch\u2019s functional dropout"),d8.forEach(o),rt.forEach(o),lo.forEach(o),gp=f(e),ke=i(e,"P",{});var Ya=s(ke);lE=n(Ya,`The best way to fix the problem is usually to look at the forward pass of the original implementation and the \u{1F917}
Transformers implementation side-by-side and check if there are any differences. Ideally, you should debug/print out
intermediate outputs of both implementations of the forward pass to find the exact position in the network where the \u{1F917}
Transformers implementation shows a different output than the original implementation. First, make sure that the
hard-coded `),Qd=i(Ya,"CODE",{});var h8=s(Qd);dE=n(h8,"input_ids"),h8.forEach(o),hE=n(Ya,` in both scripts are identical. Next, verify that the outputs of the first transformation of
the `),Vd=i(Ya,"CODE",{});var f8=s(Vd);fE=n(f8,"input_ids"),f8.forEach(o),mE=n(Ya,` (usually the word embeddings) are identical. And then work your way up to the very last layer of the
network. At some point, you will notice a difference between the two implementations, which should point you to the bug
in the \u{1F917} Transformers implementation. From our experience, a simple and efficient way is to add many print statements
in both the original implementation and \u{1F917} Transformers implementation, at the same positions in the network
respectively, and to successively remove print statements showing the same values for intermediate presentations.`),Ya.forEach(o),wp=f(e),qt=i(e,"P",{});var Zu=s(qt);pE=n(Zu,`When you\u2019re confident that both implementations yield the same output, verifying the outputs with
`),eh=i(Zu,"CODE",{});var m8=s(eh);uE=n(m8,"torch.allclose(original_output, output, atol=1e-3)"),m8.forEach(o),cE=n(Zu,`, you\u2019re done with the most difficult part! Congratulations - the
work left to be done should be a cakewalk \u{1F60A}.`),Zu.forEach(o),vp=f(e),sa=i(e,"P",{});var p8=s(sa);th=i(p8,"STRONG",{});var u8=s(th);yE=n(u8,"8. Adding all necessary model tests"),u8.forEach(o),p8.forEach(o),bp=f(e),St=i(e,"P",{});var Ku=s(St);gE=n(Ku,`At this point, you have successfully added a new model. However, it is very much possible that the model does not yet
fully comply with the required design. To make sure, the implementation is fully compatible with \u{1F917} Transformers, all
common tests should pass. The Cookiecutter should have automatically added a test file for your model, probably under
the same `),oh=i(Ku,"CODE",{});var c8=s(oh);wE=n(c8,"tests/test_modeling_brand_new_bert.py"),c8.forEach(o),vE=n(Ku,". Run this test file to verify that all common tests pass:"),Ku.forEach(o),_p=f(e),u(xr.$$.fragment,e),Ep=f(e),la=i(e,"P",{});var y8=s(la);bE=n(y8,"Having fixed all common tests, it is now crucial to ensure that all the nice work you have done is well tested, so that"),y8.forEach(o),kp=f(e),Dt=i(e,"UL",{});var Qu=s(Dt);da=i(Qu,"LI",{});var dk=s(da);_E=n(dk,"a) The community can easily understand your work by looking at specific tests of "),rh=i(dk,"EM",{});var g8=s(rh);EE=n(g8,"brand_new_bert"),g8.forEach(o),dk.forEach(o),kE=f(Qu),nh=i(Qu,"LI",{});var w8=s(nh);TE=n(w8,"b) Future changes to your model will not break any important feature of the model."),w8.forEach(o),Qu.forEach(o),Tp=f(e),Rt=i(e,"P",{});var Vu=s(Rt);$E=n(Vu,`At first, integration tests should be added. Those integration tests essentially do the same as the debugging scripts
you used earlier to implement the model to \u{1F917} Transformers. A template of those model tests is already added by the
Cookiecutter, called `),ah=i(Vu,"CODE",{});var v8=s(ah);PE=n(v8,"BrandNewBertModelIntegrationTests"),v8.forEach(o),IE=n(Vu,` and only has to be filled out by you. To ensure that those
tests are passing, run`),Vu.forEach(o),$p=f(e),u(jr.$$.fragment,e),Pp=f(e),u(Ft.$$.fragment,e),Ip=f(e),te=i(e,"P",{});var ho=s(te);AE=n(ho,"Second, all features that are special to "),ih=i(ho,"EM",{});var b8=s(ih);LE=n(b8,"brand_new_bert"),b8.forEach(o),NE=n(ho,` should be tested additionally in a separate test under
`),sh=i(ho,"CODE",{});var _8=s(sh);BE=n(_8,"BrandNewBertModelTester"),_8.forEach(o),ME=n(ho,"/`"),lh=i(ho,"CODE",{});var E8=s(lh);OE=n(E8,"BrandNewBertModelTest"),E8.forEach(o),xE=n(ho,`. This part is often forgotten but is extremely useful in two
ways:`),ho.forEach(o),Ap=f(e),zt=i(e,"UL",{});var ec=s(zt);Cr=i(ec,"LI",{});var tc=s(Cr);jE=n(tc,`It helps to transfer the knowledge you have acquired during the model addition to the community by showing how the
special features of `),dh=i(tc,"EM",{});var k8=s(dh);CE=n(k8,"brand_new_bert"),k8.forEach(o),qE=n(tc," should work."),tc.forEach(o),SE=f(ec),hh=i(ec,"LI",{});var T8=s(hh);DE=n(T8,"Future contributors can quickly test changes to the model by running those special tests."),T8.forEach(o),ec.forEach(o),Lp=f(e),ha=i(e,"P",{});var $8=s(ha);fh=i($8,"STRONG",{});var P8=s(fh);RE=n(P8,"9. Implement the tokenizer"),P8.forEach(o),$8.forEach(o),Np=f(e),Ht=i(e,"P",{});var oc=s(Ht);FE=n(oc,"Next, we should add the tokenizer of "),mh=i(oc,"EM",{});var I8=s(mh);zE=n(I8,"brand_new_bert"),I8.forEach(o),HE=n(oc,`. Usually, the tokenizer is equivalent or very similar to an
already existing tokenizer of \u{1F917} Transformers.`),oc.forEach(o),Bp=f(e),fa=i(e,"P",{});var A8=s(fa);WE=n(A8,`It is very important to find/extract the original tokenizer file and to manage to load this file into the \u{1F917}
Transformers\u2019 implementation of the tokenizer.`),A8.forEach(o),Mp=f(e),ma=i(e,"P",{});var L8=s(ma);GE=n(L8,"To ensure that the tokenizer works correctly, it is recommended to first create a script in the original repository\nthat inputs a string and returns the `input_ids\u201C. It could look similar to this (in pseudo-code):"),L8.forEach(o),Op=f(e),u(qr.$$.fragment,e),xp=f(e),Wt=i(e,"P",{});var rc=s(Wt);UE=n(rc,`You might have to take a deeper look again into the original repository to find the correct tokenizer function or you
might even have to do changes to your clone of the original repository to only output the `),ph=i(rc,"CODE",{});var N8=s(ph);YE=n(N8,"input_ids"),N8.forEach(o),JE=n(rc,`. Having written
a functional tokenization script that uses the original repository, an analogous script for \u{1F917} Transformers should be
created. It should look similar to this:`),rc.forEach(o),jp=f(e),u(Sr.$$.fragment,e),Cp=f(e),Gt=i(e,"P",{});var nc=s(Gt);XE=n(nc,"When both "),uh=i(nc,"CODE",{});var B8=s(uh);ZE=n(B8,"input_ids"),B8.forEach(o),KE=n(nc," yield the same values, as a final step a tokenizer test file should also be added."),nc.forEach(o),qp=f(e),Te=i(e,"P",{});var Ja=s(Te);QE=n(Ja,"Analogous to the modeling test files of "),ch=i(Ja,"EM",{});var M8=s(ch);VE=n(M8,"brand_new_bert"),M8.forEach(o),e3=n(Ja,", the tokenization test files of "),yh=i(Ja,"EM",{});var O8=s(yh);t3=n(O8,"brand_new_bert"),O8.forEach(o),o3=n(Ja,` should
contain a couple of hard-coded integration tests.`),Ja.forEach(o),Sp=f(e),pa=i(e,"P",{});var x8=s(pa);gh=i(x8,"STRONG",{});var j8=s(gh);r3=n(j8,"10. Run End-to-end integration tests"),j8.forEach(o),x8.forEach(o),Dp=f(e),oe=i(e,"P",{});var fo=s(oe);n3=n(fo,`Having added the tokenizer, you should also add a couple of end-to-end integration tests using both the model and the
tokenizer to `),wh=i(fo,"CODE",{});var C8=s(wh);a3=n(C8,"tests/test_modeling_brand_new_bert.py"),C8.forEach(o),i3=n(fo,` in \u{1F917} Transformers. Such a test should show on a meaningful
text-to-text sample that the \u{1F917} Transformers implementation works as expected. A meaningful text-to-text sample can
include `),vh=i(fo,"EM",{});var q8=s(vh);s3=n(q8,"e.g."),q8.forEach(o),l3=n(fo,` a source-to-target-translation pair, an article-to-summary pair, a question-to-answer pair, etc\u2026 If none
of the ported checkpoints has been fine-tuned on a downstream task it is enough to simply rely on the model tests. In a
final step to ensure that the model is fully functional, it is advised that you also run all tests on GPU. It can
happen that you forgot to add some `),bh=i(fo,"CODE",{});var S8=s(bh);d3=n(S8,".to(self.device)"),S8.forEach(o),h3=n(fo,` statements to internal tensors of the model, which in such a
test would show in an error. In case you have no access to a GPU, the Hugging Face team can take care of running those
tests for you.`),fo.forEach(o),Rp=f(e),ua=i(e,"P",{});var D8=s(ua);_h=i(D8,"STRONG",{});var R8=s(_h);f3=n(R8,"11. Add Docstring"),R8.forEach(o),D8.forEach(o),Fp=f(e),re=i(e,"P",{});var mo=s(re);m3=n(mo,"Now, all the necessary functionality for "),Eh=i(mo,"EM",{});var F8=s(Eh);p3=n(F8,"brand_new_bert"),F8.forEach(o),u3=n(mo,` is added - you\u2019re almost done! The only thing left to add is
a nice docstring and a doc page. The Cookiecutter should have added a template file called
`),kh=i(mo,"CODE",{});var z8=s(kh);c3=n(z8,"docs/source/model_doc/brand_new_bert.rst"),z8.forEach(o),y3=n(mo,` that you should fill out. Users of your model will usually first look at
this page before using your model. Hence, the documentation must be understandable and concise. It is very useful for
the community to add some `),Th=i(mo,"EM",{});var H8=s(Th);g3=n(H8,"Tips"),H8.forEach(o),w3=n(mo,` to show how the model should be used. Don\u2019t hesitate to ping the Hugging Face team
regarding the docstrings.`),mo.forEach(o),zp=f(e),$e=i(e,"P",{});var Xa=s($e);v3=n(Xa,"Next, make sure that the docstring added to "),$h=i(Xa,"CODE",{});var W8=s($h);b3=n(W8,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),W8.forEach(o),_3=n(Xa,` is
correct and included all necessary inputs and outputs. We have a detailed guide about writing documentation and our docstring format `),ca=i(Xa,"A",{href:!0});var G8=s(ca);E3=n(G8,"here"),G8.forEach(o),k3=n(Xa,`. It is always to good to remind oneself that documentation should
be treated at least as carefully as the code in \u{1F917} Transformers since the documentation is usually the first contact
point of the community with the model.`),Xa.forEach(o),Hp=f(e),ya=i(e,"P",{});var U8=s(ya);Ph=i(U8,"STRONG",{});var Y8=s(Ph);T3=n(Y8,"Code refactor"),Y8.forEach(o),U8.forEach(o),Wp=f(e),Ut=i(e,"P",{});var ac=s(Ut);$3=n(ac,"Great, now you have added all the necessary code for "),Ih=i(ac,"EM",{});var J8=s(Ih);P3=n(J8,"brand_new_bert"),J8.forEach(o),I3=n(ac,`. At this point, you should correct some potential
incorrect code style by running:`),ac.forEach(o),Gp=f(e),u(Dr.$$.fragment,e),Up=f(e),ga=i(e,"P",{});var X8=s(ga);A3=n(X8,"and verify that your coding style passes the quality check:"),X8.forEach(o),Yp=f(e),u(Rr.$$.fragment,e),Jp=f(e),wa=i(e,"P",{});var Z8=s(wa);L3=n(Z8,`There are a couple of other very strict design tests in \u{1F917} Transformers that might still be failing, which shows up in
the tests of your pull request. This is often because of some missing information in the docstring or some incorrect
naming. The Hugging Face team will surely help you if you\u2019re stuck here.`),Z8.forEach(o),Xp=f(e),va=i(e,"P",{});var K8=s(va);N3=n(K8,`Lastly, it is always a good idea to refactor one\u2019s code after having ensured that the code works correctly. With all
tests passing, now it\u2019s a good time to go over the added code again and do some refactoring.`),K8.forEach(o),Zp=f(e),ba=i(e,"P",{});var Q8=s(ba);B3=n(Q8,"You have now finished the coding part, congratulation! \u{1F389} You are Awesome! \u{1F60E}"),Q8.forEach(o),Kp=f(e),_a=i(e,"P",{});var V8=s(_a);Ah=i(V8,"STRONG",{});var e9=s(Ah);M3=n(e9,"12. Upload the models to the model hub"),e9.forEach(o),V8.forEach(o),Qp=f(e),S=i(e,"P",{});var Oe=s(S);O3=n(Oe,`In this final part, you should convert and upload all checkpoints to the model hub and add a model card for each
uploaded model checkpoint. You can get familiar with the hub functionalities by reading our `),Ea=i(Oe,"A",{href:!0});var t9=s(Ea);x3=n(t9,"Model sharing and uploading Page"),t9.forEach(o),j3=n(Oe,`. You should work alongside the Hugging Face team here to decide on a fitting name for each
checkpoint and to get the required access rights to be able to upload the model under the author\u2019s organization of
`),Lh=i(Oe,"EM",{});var o9=s(Lh);C3=n(o9,"brand_new_bert"),o9.forEach(o),q3=n(Oe,". The "),Nh=i(Oe,"CODE",{});var r9=s(Nh);S3=n(r9,"push_to_hub"),r9.forEach(o),D3=n(Oe," method, present in all models in "),Bh=i(Oe,"CODE",{});var n9=s(Bh);R3=n(n9,"transformers"),n9.forEach(o),F3=n(Oe,", is a quick and efficient way to push your checkpoint to the hub. A little snippet is pasted below:"),Oe.forEach(o),Vp=f(e),u(Fr.$$.fragment,e),eu=f(e),Yt=i(e,"P",{});var ic=s(Yt);z3=n(ic,`It is worth spending some time to create fitting model cards for each checkpoint. The model cards should highlight the
specific characteristics of this particular checkpoint, `),Mh=i(ic,"EM",{});var a9=s(Mh);H3=n(a9,"e.g."),a9.forEach(o),W3=n(ic,` On which dataset was the checkpoint
pretrained/fine-tuned on? On what down-stream task should the model be used? And also include some code on how to
correctly use the model.`),ic.forEach(o),tu=f(e),ka=i(e,"P",{});var i9=s(ka);Oh=i(i9,"STRONG",{});var s9=s(Oh);G3=n(s9,"13. (Optional) Add notebook"),s9.forEach(o),i9.forEach(o),ou=f(e),Jt=i(e,"P",{});var sc=s(Jt);U3=n(sc,"It is very helpful to add a notebook that showcases in-detail how "),xh=i(sc,"EM",{});var l9=s(xh);Y3=n(l9,"brand_new_bert"),l9.forEach(o),J3=n(sc,` can be used for inference and/or
fine-tuned on a downstream task. This is not mandatory to merge your PR, but very useful for the community.`),sc.forEach(o),ru=f(e),Ta=i(e,"P",{});var d9=s(Ta);jh=i(d9,"STRONG",{});var h9=s(jh);X3=n(h9,"14. Submit your finished PR"),h9.forEach(o),d9.forEach(o),nu=f(e),$a=i(e,"P",{});var f9=s($a);Z3=n(f9,`You\u2019re done programming now and can move to the last step, which is getting your PR merged into main. Usually, the
Hugging Face team should have helped you already at this point, but it is worth taking some time to give your finished
PR a nice description and eventually add comments to your code, if you want to point out certain design choices to your
reviewer.`),f9.forEach(o),au=f(e),ot=i(e,"H3",{class:!0});var lc=s(ot);Xt=i(lc,"A",{id:!0,class:!0,href:!0});var m9=s(Xt);Ch=i(m9,"SPAN",{});var p9=s(Ch);u(zr.$$.fragment,p9),p9.forEach(o),m9.forEach(o),K3=f(lc),qh=i(lc,"SPAN",{});var u9=s(qh);Q3=n(u9,"Share your work!!"),u9.forEach(o),lc.forEach(o),iu=f(e),Pa=i(e,"P",{});var c9=s(Pa);V3=n(c9,`Now, it\u2019s time to get some credit from the community for your work! Having completed a model addition is a major
contribution to Transformers and the whole NLP community. Your code and the ported pre-trained models will certainly be
used by hundreds and possibly even thousands of developers and researchers. You should be proud of your work and share
your achievement with the community.`),c9.forEach(o),su=f(e),Ia=i(e,"P",{});var y9=s(Ia);Sh=i(y9,"STRONG",{});var g9=s(Sh);ek=n(g9,"You have made another model that is super easy to access for everyone in the community! \u{1F92F}"),g9.forEach(o),y9.forEach(o),this.h()},h(){m(P,"name","hf:doc:metadata"),m(P,"content",JSON.stringify(I9)),m(H,"id","how-to-add-a-model-to-transformers"),m(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(H,"href","#how-to-add-a-model-to-transformers"),m(B,"class","relative group"),m(co,"href","https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model/open_model_proposals/README.md"),m(co,"rel","nofollow"),m(yo,"href","https://github.com/huggingface/transformers/pulls?q=is%3Apr+label%3A%22PR+for+Model+Addition%22+is%3Aclosed"),m(yo,"rel","nofollow"),m(st,"id","general-overview-of-transformers"),m(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(st,"href","#general-overview-of-transformers"),m(je,"class","relative group"),m(Yr,"href","philosophy"),m(ht,"id","overview-of-models"),m(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ht,"href","#overview-of-models"),m(Ce,"class","relative group"),m(Xr,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),m(Zr,"href","/docs/transformers/pr_highlight/en/main_classes/configuration#transformers.PretrainedConfig"),E9(Qr.src,hk="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png")||m(Qr,"src",hk),m(Vr,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),m(en,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),m(tn,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(on,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(rn,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),m(nn,"href","/docs/transformers/pr_highlight/en/main_classes/configuration#transformers.PretrainedConfig"),m(an,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(sn,"href","/docs/transformers/pr_highlight/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained"),m(ft,"id","code-style"),m(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ft,"href","#code-style"),m(qe,"class","relative group"),m(Eo,"href","https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160"),m(Eo,"rel","nofollow"),m(mt,"id","overview-of-tokenizers"),m(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(mt,"href","#overview-of-tokenizers"),m(Fe,"class","relative group"),m(pt,"id","stepbystep-recipe-to-add-a-model-to-transformers"),m(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pt,"href","#stepbystep-recipe-to-add-a-model-to-transformers"),m(ze,"class","relative group"),m(Po,"href","https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28"),m(Po,"rel","nofollow"),m(Io,"href","https://huggingface.co/thomwolf"),m(Io,"rel","nofollow"),m(Lo,"href","https://huggingface.co/blog/porting-fsmt"),m(Lo,"rel","nofollow"),m(No,"href","https://huggingface.co/stas"),m(No,"rel","nofollow"),m(Bo,"href","https://www.gnu.org/software/grep/"),m(Bo,"rel","nofollow"),m(Mo,"href","https://github.com/BurntSushi/ripgrep"),m(Mo,"rel","nofollow"),m(un,"start","2"),m(cn,"start","3"),m(yn,"start","4"),m(gn,"start","5"),m(wn,"start","6"),m(vn,"start","7"),m(bn,"start","8"),m(_n,"start","9"),m(En,"start","10"),m(kn,"start","11"),m(Tn,"start","12"),m($n,"start","13"),m(Pn,"start","14"),m(ct,"id","1-optional-theoretical-aspects-of-brandnewbert"),m(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ct,"href","#1-optional-theoretical-aspects-of-brandnewbert"),m(He,"class","relative group"),m(In,"href","model_summary"),m(xo,"href","https://huggingface.co/transformers/#contents"),m(xo,"rel","nofollow"),m(gt,"id","2-next-prepare-your-environment"),m(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(gt,"href","#2-next-prepare-your-environment"),m(Ye,"class","relative group"),m(qo,"href","https://github.com/huggingface/transformers"),m(qo,"rel","nofollow"),m(Ro,"start","3"),m(Wo,"href","https://pytorch.org/get-started/locally/"),m(Wo,"rel","nofollow"),m(Ho,"start","4"),m(Uo,"start","5"),m(bt,"id","34-run-a-pretrained-checkpoint-using-the-original-repository"),m(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(bt,"href","#34-run-a-pretrained-checkpoint-using-the-original-repository"),m(Xe,"class","relative group"),m(Ko,"href","https://jupyter.org/"),m(Ko,"rel","nofollow"),m(Qo,"href","https://colab.research.google.com/notebooks/intro.ipynb"),m(Qo,"rel","nofollow"),m(tr,"href","https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed"),m(tr,"rel","nofollow"),m(or,"href","https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow"),m(or,"rel","nofollow"),m(nr,"href","https://www.tensorflow.org/api_docs/python/tf/print"),m(nr,"rel","nofollow"),m(ar,"href","https://github.com/google/jax/issues/196"),m(ar,"rel","nofollow"),m(Nt,"id","514-port-brandnewbert-to-transformers"),m(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Nt,"href","#514-port-brandnewbert-to-transformers"),m(Ve,"class","relative group"),m(Rn,"href","#write-a-conversion-script"),m(dr,"href","https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model"),m(dr,"rel","nofollow"),m(fr,"start","2"),m(pr,"start","3"),m(cr,"start","4"),m(et,"start","5"),m(vr,"href","https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91"),m(vr,"rel","nofollow"),m(br,"href","https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py"),m(br,"rel","nofollow"),m(ia,"href","#run-a-pretrained-checkpoint-using-the-original-repository"),m(Or,"href","https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout"),m(Or,"rel","nofollow"),m(ca,"href","writing-documentation"),m(Ea,"href","model_sharing"),m(Xt,"id","share-your-work"),m(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Xt,"href","#share-your-work"),m(ot,"class","relative group")},m(e,l){t(document.head,P),d(e,nt,l),d(e,B,l),t(B,H),t(H,xe),c(D,xe,null),t(B,po),t(B,le),t(le,de),d(e,uo,l),d(e,R,l),t(R,dc),t(R,Za),t(Za,hc),t(R,fc),t(R,Ka),t(Ka,mc),t(R,pc),d(e,Fh,l),d(e,at,l),t(at,uc),t(at,co),t(co,cc),t(at,yc),d(e,zh,l),d(e,Wr,l),t(Wr,gc),d(e,Hh,l),d(e,W,l),t(W,Qa),t(Qa,wc),t(W,vc),t(W,Va),t(Va,bc),t(W,_c),t(W,ei),t(ei,Ec),t(W,kc),t(W,he),t(he,Tc),t(he,ti),t(ti,$c),t(he,Pc),t(he,oi),t(oi,Ic),t(he,Ac),t(he,ri),t(ri,Lc),t(he,Nc),d(e,Wh,l),d(e,it,l),t(it,Bc),t(it,yo),t(yo,Mc),t(it,Oc),d(e,Gh,l),d(e,Gr,l),t(Gr,xc),d(e,Uh,l),d(e,je,l),t(je,st),t(st,ni),c(go,ni,null),t(je,jc),t(je,ai),t(ai,Cc),d(e,Yh,l),d(e,Ur,l),t(Ur,qc),d(e,Jh,l),d(e,lt,l),t(lt,Sc),t(lt,Yr),t(Yr,Dc),t(lt,Rc),d(e,Xh,l),d(e,ce,l),t(ce,ii),t(ii,Fc),t(ce,zc),t(ce,si),t(si,Hc),t(ce,Wc),t(ce,wo),t(wo,Gc),t(wo,li),t(li,Uc),t(wo,Yc),d(e,Zh,l),d(e,dt,l),t(dt,Jc),t(dt,di),t(di,Xc),t(dt,Zc),d(e,Kh,l),d(e,Jr,l),t(Jr,Kc),d(e,Qh,l),d(e,Ce,l),t(Ce,ht),t(ht,hi),c(vo,hi,null),t(Ce,Qc),t(Ce,fi),t(fi,Vc),d(e,Vh,l),d(e,G,l),t(G,ey),t(G,Xr),t(Xr,ty),t(G,oy),t(G,Zr),t(Zr,ry),t(G,ny),t(G,mi),t(mi,ay),t(G,iy),d(e,ef,l),d(e,Kr,l),t(Kr,sy),d(e,tf,l),d(e,Qr,l),d(e,of,l),d(e,v,l),t(v,ly),t(v,pi),t(pi,dy),t(v,hy),t(v,ui),t(ui,fy),t(v,my),t(v,Vr),t(Vr,py),t(v,uy),t(v,en),t(en,cy),t(v,yy),t(v,tn),t(tn,gy),t(v,wy),t(v,on),t(on,vy),t(v,by),t(v,ci),t(ci,_y),t(v,Ey),t(v,yi),t(yi,ky),t(v,Ty),t(v,gi),t(gi,$y),t(v,Py),t(v,wi),t(wi,Iy),t(v,Ay),t(v,vi),t(vi,Ly),t(v,Ny),t(v,bi),t(bi,By),t(v,My),t(v,rn),t(rn,Oy),t(v,xy),t(v,_i),t(_i,jy),t(v,Cy),t(v,Ei),t(Ei,qy),t(v,Sy),d(e,rf,l),c(bo,e,l),d(e,nf,l),d(e,I,l),t(I,Dy),t(I,nn),t(nn,Ry),t(I,Fy),t(I,ki),t(ki,zy),t(I,Hy),t(I,Ti),t(Ti,Wy),t(I,Gy),t(I,an),t(an,Uy),t(I,Yy),t(I,sn),t(sn,Jy),t(I,Xy),d(e,af,l),d(e,qe,l),t(qe,ft),t(ft,$i),c(_o,$i,null),t(qe,Zy),t(qe,Pi),t(Pi,Ky),d(e,sf,l),d(e,ln,l),t(ln,Qy),d(e,lf,l),d(e,O,l),t(O,Se),t(Se,Vy),t(Se,Ii),t(Ii,eg),t(Se,tg),t(Se,Eo),t(Eo,og),t(Se,rg),t(O,ng),t(O,De),t(De,ag),t(De,Ai),t(Ai,ig),t(De,sg),t(De,Li),t(Li,lg),t(De,dg),t(O,hg),t(O,Ni),t(Ni,fg),t(O,mg),t(O,Re),t(Re,pg),t(Re,Bi),t(Bi,ug),t(Re,cg),t(Re,Mi),t(Mi,yg),t(Re,gg),t(O,wg),t(O,Oi),t(Oi,vg),d(e,df,l),d(e,Fe,l),t(Fe,mt),t(mt,xi),c(ko,xi,null),t(Fe,bg),t(Fe,ji),t(ji,_g),d(e,hf,l),d(e,dn,l),t(dn,Eg),d(e,ff,l),d(e,ze,l),t(ze,pt),t(pt,Ci),c(To,Ci,null),t(ze,kg),t(ze,qi),t(qi,Tg),d(e,mf,l),d(e,hn,l),t(hn,$g),d(e,pf,l),d(e,ut,l),t(ut,$o),t($o,Po),t(Po,Pg),t($o,Ig),t($o,Io),t(Io,Ag),t(ut,Lg),t(ut,Ao),t(Ao,Lo),t(Lo,Ng),t(Ao,Bg),t(Ao,No),t(No,Mg),d(e,uf,l),d(e,fn,l),t(fn,Og),d(e,cf,l),d(e,ye,l),t(ye,fe),t(fe,xg),t(fe,Bo),t(Bo,jg),t(fe,Cg),t(fe,Mo),t(Mo,qg),t(fe,Sg),t(fe,Si),t(Si,Dg),t(fe,Rg),t(ye,Fg),t(ye,Di),t(Di,zg),t(ye,Hg),t(ye,Ri),t(Ri,Wg),d(e,yf,l),d(e,mn,l),t(mn,Gg),d(e,gf,l),d(e,pn,l),t(pn,Ug),d(e,wf,l),d(e,b,l),t(b,Fi),t(Fi,zi),t(zi,Hi),t(Hi,Yg),t(b,Jg),t(b,Wi),t(Wi,un),t(un,Gi),t(Gi,Xg),t(b,Zg),t(b,Ui),t(Ui,cn),t(cn,Yi),t(Yi,Kg),t(b,Qg),t(b,Ji),t(Ji,yn),t(yn,Xi),t(Xi,Vg),t(b,ew),t(b,Zi),t(Zi,gn),t(gn,Ki),t(Ki,tw),t(b,ow),t(b,Qi),t(Qi,wn),t(wn,Vi),t(Vi,rw),t(b,nw),t(b,es),t(es,vn),t(vn,ts),t(ts,aw),t(b,iw),t(b,os),t(os,bn),t(bn,rs),t(rs,sw),t(b,lw),t(b,ns),t(ns,_n),t(_n,as),t(as,dw),t(b,hw),t(b,is),t(is,En),t(En,ss),t(ss,fw),t(b,mw),t(b,ls),t(ls,kn),t(kn,ds),t(ds,pw),t(b,uw),t(b,hs),t(hs,Tn),t(Tn,fs),t(fs,cw),t(b,yw),t(b,ms),t(ms,$n),t($n,ps),t(ps,gw),t(b,ww),t(b,us),t(us,Pn),t(Pn,cs),t(cs,vw),d(e,vf,l),d(e,x,l),t(x,bw),t(x,ys),t(ys,_w),t(x,Ew),t(x,gs),t(gs,kw),t(x,Tw),t(x,ws),t(ws,$w),t(x,Pw),t(x,vs),t(vs,Iw),t(x,Aw),d(e,bf,l),d(e,He,l),t(He,ct),t(ct,bs),c(Oo,bs,null),t(He,Lw),t(He,_s),t(_s,Nw),d(e,_f,l),d(e,yt,l),t(yt,Bw),t(yt,Es),t(Es,Mw),t(yt,Ow),d(e,Ef,l),d(e,j,l),t(j,We),t(We,xw),t(We,ks),t(ks,jw),t(We,Cw),t(We,In),t(In,qw),t(We,Sw),t(j,Dw),t(j,Ge),t(Ge,Rw),t(Ge,Ts),t(Ts,Fw),t(Ge,zw),t(Ge,$s),t($s,Hw),t(Ge,Ww),t(j,Gw),t(j,Ps),t(Ps,Uw),t(j,Yw),t(j,Ue),t(Ue,Jw),t(Ue,xo),t(xo,Xw),t(Ue,Zw),t(Ue,Is),t(Is,Kw),t(Ue,Qw),t(j,Vw),t(j,As),t(As,ev),d(e,kf,l),d(e,An,l),t(An,tv),d(e,Tf,l),d(e,Ye,l),t(Ye,gt),t(gt,Ls),c(jo,Ls,null),t(Ye,ov),t(Ye,Ns),t(Ns,rv),d(e,$f,l),d(e,wt,l),t(wt,Bs),t(Bs,Co),t(Co,nv),t(Co,qo),t(qo,av),t(Co,iv),t(wt,sv),t(wt,Ms),t(Ms,So),t(So,lv),t(So,Os),t(Os,dv),t(So,hv),d(e,Pf,l),c(Do,e,l),d(e,If,l),d(e,Ro,l),t(Ro,xs),t(xs,fv),d(e,Af,l),c(Fo,e,l),d(e,Lf,l),d(e,Ln,l),t(Ln,mv),d(e,Nf,l),c(zo,e,l),d(e,Bf,l),d(e,Ho,l),t(Ho,Je),t(Je,pv),t(Je,js),t(js,uv),t(Je,cv),t(Je,Wo),t(Wo,yv),t(Je,gv),d(e,Mf,l),d(e,Go,l),t(Go,Cs),t(Cs,wv),t(Go,vv),d(e,Of,l),d(e,Uo,l),t(Uo,Yo),t(Yo,bv),t(Yo,qs),t(qs,_v),t(Yo,Ev),d(e,xf,l),c(Jo,e,l),d(e,jf,l),d(e,vt,l),t(vt,kv),t(vt,Ss),t(Ss,Tv),t(vt,$v),d(e,Cf,l),d(e,Xe,l),t(Xe,bt),t(bt,Ds),c(Xo,Ds,null),t(Xe,Pv),t(Xe,Rs),t(Rs,Iv),d(e,qf,l),d(e,A,l),t(A,Av),t(A,Fs),t(Fs,Lv),t(A,Nv),t(A,zs),t(zs,Bv),t(A,Mv),t(A,Hs),t(Hs,Ov),t(A,xv),t(A,Ws),t(Ws,jv),t(A,Cv),t(A,Gs),t(Gs,qv),t(A,Sv),d(e,Sf,l),d(e,Nn,l),t(Nn,Dv),d(e,Df,l),d(e,_t,l),t(_t,Rv),t(_t,Us),t(Us,Fv),t(_t,zv),d(e,Rf,l),d(e,L,l),t(L,Ys),t(Ys,Hv),t(L,Wv),t(L,Js),t(Js,Gv),t(L,Uv),t(L,Xs),t(Xs,Yv),t(L,Jv),t(L,Zs),t(Zs,Xv),t(L,Zv),t(L,F),t(F,Kv),t(F,Ks),t(Ks,Qv),t(F,Vv),t(F,Qs),t(Qs,eb),t(F,tb),t(F,Vs),t(Vs,ob),t(F,rb),t(F,el),t(el,nb),t(F,ab),t(L,ib),t(L,Ze),t(Ze,sb),t(Ze,tl),t(tl,lb),t(Ze,db),t(Ze,ol),t(ol,hb),t(Ze,fb),d(e,Ff,l),d(e,Et,l),t(Et,mb),t(Et,rl),t(rl,pb),t(Et,ub),d(e,zf,l),d(e,Bn,l),t(Bn,cb),d(e,Hf,l),d(e,Mn,l),t(Mn,yb),d(e,Wf,l),d(e,kt,l),t(kt,Zo),t(Zo,Ko),t(Ko,gb),t(Zo,wb),t(Zo,Qo),t(Qo,vb),t(kt,bb),t(kt,nl),t(nl,_b),d(e,Gf,l),d(e,On,l),t(On,Eb),d(e,Uf,l),d(e,Tt,l),t(Tt,kb),t(Tt,al),t(al,Tb),t(Tt,$b),d(e,Yf,l),d(e,$t,l),t($t,Pb),t($t,il),t(il,Ib),t($t,Ab),d(e,Jf,l),c(Vo,e,l),d(e,Xf,l),d(e,xn,l),t(xn,Lb),d(e,Zf,l),d(e,Pt,l),t(Pt,sl),t(sl,Nb),t(Pt,Bb),t(Pt,Ke),t(Ke,Mb),t(Ke,ll),t(ll,Ob),t(Ke,xb),t(Ke,dl),t(dl,jb),t(Ke,Cb),d(e,Kf,l),d(e,jn,l),t(jn,qb),d(e,Qf,l),d(e,It,l),t(It,Sb),t(It,hl),t(hl,Db),t(It,Rb),d(e,Vf,l),d(e,U,l),t(U,fl),t(fl,Fb),t(U,zb),t(U,ml),t(ml,Hb),t(U,Wb),t(U,pl),t(pl,Gb),t(U,Ub),t(U,ul),t(ul,Yb),d(e,em,l),d(e,er,l),t(er,tr),t(tr,Jb),t(er,Xb),d(e,tm,l),d(e,At,l),t(At,Zb),t(At,or),t(or,Kb),t(At,Qb),d(e,om,l),d(e,Cn,l),t(Cn,Vb),d(e,rm,l),d(e,qn,l),t(qn,e_),d(e,nm,l),d(e,N,l),t(N,cl),t(cl,t_),t(N,o_),t(N,yl),t(yl,r_),t(N,n_),t(N,gl),t(gl,a_),t(N,i_),t(N,wl),t(wl,s_),t(N,l_),t(N,vl),t(vl,d_),t(N,h_),t(N,bl),t(bl,f_),d(e,am,l),d(e,Qe,l),t(Qe,m_),t(Qe,_l),t(_l,p_),t(Qe,u_),t(Qe,El),t(El,c_),d(e,im,l),d(e,Sn,l),t(Sn,y_),d(e,sm,l),c(rr,e,l),d(e,lm,l),d(e,ge,l),t(ge,g_),t(ge,kl),t(kl,w_),t(ge,v_),t(ge,Tl),t(Tl,b_),t(ge,__),d(e,dm,l),d(e,C,l),t(C,z),t(z,E_),t(z,nr),t(nr,k_),t(z,T_),t(z,$l),t($l,$_),t(z,P_),t(z,Pl),t(Pl,I_),t(z,A_),t(z,ar),t(ar,L_),t(z,N_),t(C,B_),t(C,Il),t(Il,M_),t(C,O_),t(C,T),t(T,x_),t(T,Al),t(Al,j_),t(T,C_),t(T,Ll),t(Ll,q_),t(T,S_),t(T,Nl),t(Nl,D_),t(T,R_),t(T,Bl),t(Bl,F_),t(T,z_),t(T,Ml),t(Ml,H_),t(T,W_),t(T,Ol),t(Ol,G_),t(T,U_),t(T,xl),t(xl,Y_),t(T,J_),t(T,jl),t(jl,X_),t(T,Z_),t(T,Cl),t(Cl,K_),t(T,Q_),t(T,ql),t(ql,V_),t(T,e1),t(C,t1),t(C,ir),t(ir,o1),t(ir,Sl),t(Sl,r1),t(ir,n1),t(C,a1),t(C,me),t(me,i1),t(me,Dl),t(Dl,s1),t(me,l1),t(me,Rl),t(Rl,d1),t(me,h1),t(me,Fl),t(Fl,f1),t(me,m1),d(e,hm,l),d(e,Lt,l),t(Lt,p1),t(Lt,zl),t(zl,u1),t(Lt,c1),d(e,fm,l),d(e,Ve,l),t(Ve,Nt),t(Nt,Hl),c(sr,Hl,null),t(Ve,y1),t(Ve,Wl),t(Wl,g1),d(e,mm,l),d(e,Dn,l),t(Dn,w1),d(e,pm,l),c(lr,e,l),d(e,um,l),d(e,Bt,l),t(Bt,v1),t(Bt,Rn),t(Rn,b1),t(Bt,_1),d(e,cm,l),d(e,Fn,l),t(Fn,E1),d(e,ym,l),d(e,Mt,l),t(Mt,zn),t(zn,Gl),t(Gl,k1),t(zn,T1),t(Mt,$1),t(Mt,Hn),t(Hn,Ul),t(Ul,P1),t(Hn,I1),d(e,gm,l),d(e,we,l),t(we,A1),t(we,Yl),t(Yl,L1),t(we,N1),t(we,dr),t(dr,B1),t(we,M1),d(e,wm,l),d(e,Wn,l),t(Wn,Jl),t(Jl,O1),d(e,vm,l),d(e,ve,l),t(ve,x1),t(ve,Xl),t(Xl,j1),t(ve,C1),t(ve,Zl),t(Zl,q1),t(ve,S1),d(e,bm,l),d(e,Gn,l),t(Gn,D1),d(e,_m,l),d(e,Un,l),t(Un,Kl),t(Kl,R1),d(e,Em,l),c(hr,e,l),d(e,km,l),d(e,fr,l),t(fr,Ql),t(Ql,F1),d(e,Tm,l),c(mr,e,l),d(e,$m,l),d(e,pr,l),t(pr,Vl),t(Vl,z1),d(e,Pm,l),c(ur,e,l),d(e,Im,l),d(e,cr,l),t(cr,ed),t(ed,H1),d(e,Am,l),c(yr,e,l),d(e,Lm,l),d(e,et,l),t(et,td),t(td,od),t(od,W1),t(et,G1),t(et,rd),t(rd,nd),t(nd,U1),d(e,Nm,l),d(e,Yn,l),t(Yn,Y1),d(e,Bm,l),c(gr,e,l),d(e,Mm,l),d(e,Jn,l),t(Jn,J1),d(e,Om,l),d(e,Xn,l),t(Xn,X1),d(e,xm,l),d(e,Zn,l),t(Zn,Z1),d(e,jm,l),d(e,Kn,l),t(Kn,ad),t(ad,K1),d(e,Cm,l),d(e,be,l),t(be,Q1),t(be,id),t(id,V1),t(be,e0),t(be,sd),t(sd,t0),t(be,o0),d(e,qm,l),d(e,Y,l),t(Y,r0),t(Y,ld),t(ld,n0),t(Y,a0),t(Y,dd),t(dd,i0),t(Y,s0),t(Y,hd),t(hd,l0),t(Y,d0),d(e,Sm,l),d(e,M,l),t(M,fd),t(fd,h0),t(M,f0),t(M,md),t(md,m0),t(M,p0),t(M,pd),t(pd,u0),t(M,c0),t(M,ud),t(ud,y0),t(M,g0),t(M,cd),t(cd,w0),t(M,v0),d(e,Dm,l),c(wr,e,l),d(e,Rm,l),d(e,_e,l),t(_e,b0),t(_e,yd),t(yd,_0),t(_e,E0),t(_e,gd),t(gd,k0),t(_e,T0),d(e,Fm,l),d(e,Qn,l),t(Qn,wd),t(wd,$0),d(e,zm,l),d(e,J,l),t(J,P0),t(J,vd),t(vd,I0),t(J,A0),t(J,bd),t(bd,L0),t(J,N0),t(J,_d),t(_d,B0),t(J,M0),d(e,Hm,l),d(e,Ot,l),t(Ot,Vn),t(Vn,O0),t(Vn,vr),t(vr,x0),t(Ot,j0),t(Ot,ea),t(ea,C0),t(ea,br),t(br,q0),d(e,Wm,l),d(e,xt,l),t(xt,S0),t(xt,Ed),t(Ed,D0),t(xt,R0),d(e,Gm,l),c(_r,e,l),d(e,Um,l),d(e,X,l),t(X,F0),t(X,kd),t(kd,z0),t(X,H0),t(X,Td),t(Td,W0),t(X,G0),t(X,$d),t($d,U0),t(X,Y0),d(e,Ym,l),c(Er,e,l),d(e,Jm,l),d(e,ta,l),t(ta,J0),d(e,Xm,l),c(kr,e,l),d(e,Zm,l),d(e,oa,l),t(oa,X0),d(e,Km,l),c(Tr,e,l),d(e,Qm,l),d(e,ra,l),t(ra,Z0),d(e,Vm,l),c($r,e,l),d(e,ep,l),d(e,Pr,l),t(Pr,K0),t(Pr,Pd),t(Pd,Q0),d(e,tp,l),c(Ir,e,l),d(e,op,l),d(e,Ee,l),t(Ee,V0),t(Ee,Id),t(Id,e2),t(Ee,t2),t(Ee,Ad),t(Ad,o2),t(Ee,r2),d(e,rp,l),c(Ar,e,l),d(e,np,l),d(e,Lr,l),t(Lr,n2),t(Lr,Ld),t(Ld,a2),d(e,ap,l),c(Nr,e,l),d(e,ip,l),d(e,na,l),t(na,i2),d(e,sp,l),d(e,jt,l),t(jt,s2),t(jt,Nd),t(Nd,l2),t(jt,d2),d(e,lp,l),d(e,Z,l),t(Z,h2),t(Z,Bd),t(Bd,f2),t(Z,m2),t(Z,Md),t(Md,p2),t(Z,u2),t(Z,Od),t(Od,c2),t(Z,y2),d(e,dp,l),d(e,K,l),t(K,g2),t(K,xd),t(xd,w2),t(K,v2),t(K,jd),t(jd,b2),t(K,_2),t(K,Cd),t(Cd,E2),t(K,k2),d(e,hp,l),c(Br,e,l),d(e,fp,l),d(e,aa,l),t(aa,qd),t(qd,T2),d(e,mp,l),d(e,Ct,l),t(Ct,$2),t(Ct,ia),t(ia,P2),t(Ct,I2),d(e,pp,l),c(Mr,e,l),d(e,up,l),d(e,q,l),t(q,A2),t(q,Sd),t(Sd,L2),t(q,N2),t(q,Dd),t(Dd,B2),t(q,M2),t(q,Rd),t(Rd,O2),t(q,x2),t(q,Fd),t(Fd,j2),t(q,C2),d(e,cp,l),d(e,Q,l),t(Q,q2),t(Q,zd),t(zd,S2),t(Q,D2),t(Q,Hd),t(Hd,R2),t(Q,F2),t(Q,Wd),t(Wd,z2),t(Q,H2),d(e,yp,l),d(e,V,l),t(V,tt),t(tt,W2),t(tt,Gd),t(Gd,G2),t(tt,U2),t(tt,Ud),t(Ud,Y2),t(tt,J2),t(V,X2),t(V,Yd),t(Yd,Z2),t(V,K2),t(V,Jd),t(Jd,Q2),t(V,V2),t(V,ee),t(ee,eE),t(ee,Xd),t(Xd,tE),t(ee,oE),t(ee,Zd),t(Zd,rE),t(ee,nE),t(ee,Kd),t(Kd,aE),t(ee,iE),t(ee,Or),t(Or,sE),d(e,gp,l),d(e,ke,l),t(ke,lE),t(ke,Qd),t(Qd,dE),t(ke,hE),t(ke,Vd),t(Vd,fE),t(ke,mE),d(e,wp,l),d(e,qt,l),t(qt,pE),t(qt,eh),t(eh,uE),t(qt,cE),d(e,vp,l),d(e,sa,l),t(sa,th),t(th,yE),d(e,bp,l),d(e,St,l),t(St,gE),t(St,oh),t(oh,wE),t(St,vE),d(e,_p,l),c(xr,e,l),d(e,Ep,l),d(e,la,l),t(la,bE),d(e,kp,l),d(e,Dt,l),t(Dt,da),t(da,_E),t(da,rh),t(rh,EE),t(Dt,kE),t(Dt,nh),t(nh,TE),d(e,Tp,l),d(e,Rt,l),t(Rt,$E),t(Rt,ah),t(ah,PE),t(Rt,IE),d(e,$p,l),c(jr,e,l),d(e,Pp,l),c(Ft,e,l),d(e,Ip,l),d(e,te,l),t(te,AE),t(te,ih),t(ih,LE),t(te,NE),t(te,sh),t(sh,BE),t(te,ME),t(te,lh),t(lh,OE),t(te,xE),d(e,Ap,l),d(e,zt,l),t(zt,Cr),t(Cr,jE),t(Cr,dh),t(dh,CE),t(Cr,qE),t(zt,SE),t(zt,hh),t(hh,DE),d(e,Lp,l),d(e,ha,l),t(ha,fh),t(fh,RE),d(e,Np,l),d(e,Ht,l),t(Ht,FE),t(Ht,mh),t(mh,zE),t(Ht,HE),d(e,Bp,l),d(e,fa,l),t(fa,WE),d(e,Mp,l),d(e,ma,l),t(ma,GE),d(e,Op,l),c(qr,e,l),d(e,xp,l),d(e,Wt,l),t(Wt,UE),t(Wt,ph),t(ph,YE),t(Wt,JE),d(e,jp,l),c(Sr,e,l),d(e,Cp,l),d(e,Gt,l),t(Gt,XE),t(Gt,uh),t(uh,ZE),t(Gt,KE),d(e,qp,l),d(e,Te,l),t(Te,QE),t(Te,ch),t(ch,VE),t(Te,e3),t(Te,yh),t(yh,t3),t(Te,o3),d(e,Sp,l),d(e,pa,l),t(pa,gh),t(gh,r3),d(e,Dp,l),d(e,oe,l),t(oe,n3),t(oe,wh),t(wh,a3),t(oe,i3),t(oe,vh),t(vh,s3),t(oe,l3),t(oe,bh),t(bh,d3),t(oe,h3),d(e,Rp,l),d(e,ua,l),t(ua,_h),t(_h,f3),d(e,Fp,l),d(e,re,l),t(re,m3),t(re,Eh),t(Eh,p3),t(re,u3),t(re,kh),t(kh,c3),t(re,y3),t(re,Th),t(Th,g3),t(re,w3),d(e,zp,l),d(e,$e,l),t($e,v3),t($e,$h),t($h,b3),t($e,_3),t($e,ca),t(ca,E3),t($e,k3),d(e,Hp,l),d(e,ya,l),t(ya,Ph),t(Ph,T3),d(e,Wp,l),d(e,Ut,l),t(Ut,$3),t(Ut,Ih),t(Ih,P3),t(Ut,I3),d(e,Gp,l),c(Dr,e,l),d(e,Up,l),d(e,ga,l),t(ga,A3),d(e,Yp,l),c(Rr,e,l),d(e,Jp,l),d(e,wa,l),t(wa,L3),d(e,Xp,l),d(e,va,l),t(va,N3),d(e,Zp,l),d(e,ba,l),t(ba,B3),d(e,Kp,l),d(e,_a,l),t(_a,Ah),t(Ah,M3),d(e,Qp,l),d(e,S,l),t(S,O3),t(S,Ea),t(Ea,x3),t(S,j3),t(S,Lh),t(Lh,C3),t(S,q3),t(S,Nh),t(Nh,S3),t(S,D3),t(S,Bh),t(Bh,R3),t(S,F3),d(e,Vp,l),c(Fr,e,l),d(e,eu,l),d(e,Yt,l),t(Yt,z3),t(Yt,Mh),t(Mh,H3),t(Yt,W3),d(e,tu,l),d(e,ka,l),t(ka,Oh),t(Oh,G3),d(e,ou,l),d(e,Jt,l),t(Jt,U3),t(Jt,xh),t(xh,Y3),t(Jt,J3),d(e,ru,l),d(e,Ta,l),t(Ta,jh),t(jh,X3),d(e,nu,l),d(e,$a,l),t($a,Z3),d(e,au,l),d(e,ot,l),t(ot,Xt),t(Xt,Ch),c(zr,Ch,null),t(ot,K3),t(ot,qh),t(qh,Q3),d(e,iu,l),d(e,Pa,l),t(Pa,V3),d(e,su,l),d(e,Ia,l),t(Ia,Sh),t(Sh,ek),lu=!0},p(e,[l]){const Hr={};l&2&&(Hr.$$scope={dirty:l,ctx:e}),Ft.$set(Hr)},i(e){lu||(y(D.$$.fragment,e),y(go.$$.fragment,e),y(vo.$$.fragment,e),y(bo.$$.fragment,e),y(_o.$$.fragment,e),y(ko.$$.fragment,e),y(To.$$.fragment,e),y(Oo.$$.fragment,e),y(jo.$$.fragment,e),y(Do.$$.fragment,e),y(Fo.$$.fragment,e),y(zo.$$.fragment,e),y(Jo.$$.fragment,e),y(Xo.$$.fragment,e),y(Vo.$$.fragment,e),y(rr.$$.fragment,e),y(sr.$$.fragment,e),y(lr.$$.fragment,e),y(hr.$$.fragment,e),y(mr.$$.fragment,e),y(ur.$$.fragment,e),y(yr.$$.fragment,e),y(gr.$$.fragment,e),y(wr.$$.fragment,e),y(_r.$$.fragment,e),y(Er.$$.fragment,e),y(kr.$$.fragment,e),y(Tr.$$.fragment,e),y($r.$$.fragment,e),y(Ir.$$.fragment,e),y(Ar.$$.fragment,e),y(Nr.$$.fragment,e),y(Br.$$.fragment,e),y(Mr.$$.fragment,e),y(xr.$$.fragment,e),y(jr.$$.fragment,e),y(Ft.$$.fragment,e),y(qr.$$.fragment,e),y(Sr.$$.fragment,e),y(Dr.$$.fragment,e),y(Rr.$$.fragment,e),y(Fr.$$.fragment,e),y(zr.$$.fragment,e),lu=!0)},o(e){g(D.$$.fragment,e),g(go.$$.fragment,e),g(vo.$$.fragment,e),g(bo.$$.fragment,e),g(_o.$$.fragment,e),g(ko.$$.fragment,e),g(To.$$.fragment,e),g(Oo.$$.fragment,e),g(jo.$$.fragment,e),g(Do.$$.fragment,e),g(Fo.$$.fragment,e),g(zo.$$.fragment,e),g(Jo.$$.fragment,e),g(Xo.$$.fragment,e),g(Vo.$$.fragment,e),g(rr.$$.fragment,e),g(sr.$$.fragment,e),g(lr.$$.fragment,e),g(hr.$$.fragment,e),g(mr.$$.fragment,e),g(ur.$$.fragment,e),g(yr.$$.fragment,e),g(gr.$$.fragment,e),g(wr.$$.fragment,e),g(_r.$$.fragment,e),g(Er.$$.fragment,e),g(kr.$$.fragment,e),g(Tr.$$.fragment,e),g($r.$$.fragment,e),g(Ir.$$.fragment,e),g(Ar.$$.fragment,e),g(Nr.$$.fragment,e),g(Br.$$.fragment,e),g(Mr.$$.fragment,e),g(xr.$$.fragment,e),g(jr.$$.fragment,e),g(Ft.$$.fragment,e),g(qr.$$.fragment,e),g(Sr.$$.fragment,e),g(Dr.$$.fragment,e),g(Rr.$$.fragment,e),g(Fr.$$.fragment,e),g(zr.$$.fragment,e),lu=!1},d(e){o(P),e&&o(nt),e&&o(B),w(D),e&&o(uo),e&&o(R),e&&o(Fh),e&&o(at),e&&o(zh),e&&o(Wr),e&&o(Hh),e&&o(W),e&&o(Wh),e&&o(it),e&&o(Gh),e&&o(Gr),e&&o(Uh),e&&o(je),w(go),e&&o(Yh),e&&o(Ur),e&&o(Jh),e&&o(lt),e&&o(Xh),e&&o(ce),e&&o(Zh),e&&o(dt),e&&o(Kh),e&&o(Jr),e&&o(Qh),e&&o(Ce),w(vo),e&&o(Vh),e&&o(G),e&&o(ef),e&&o(Kr),e&&o(tf),e&&o(Qr),e&&o(of),e&&o(v),e&&o(rf),w(bo,e),e&&o(nf),e&&o(I),e&&o(af),e&&o(qe),w(_o),e&&o(sf),e&&o(ln),e&&o(lf),e&&o(O),e&&o(df),e&&o(Fe),w(ko),e&&o(hf),e&&o(dn),e&&o(ff),e&&o(ze),w(To),e&&o(mf),e&&o(hn),e&&o(pf),e&&o(ut),e&&o(uf),e&&o(fn),e&&o(cf),e&&o(ye),e&&o(yf),e&&o(mn),e&&o(gf),e&&o(pn),e&&o(wf),e&&o(b),e&&o(vf),e&&o(x),e&&o(bf),e&&o(He),w(Oo),e&&o(_f),e&&o(yt),e&&o(Ef),e&&o(j),e&&o(kf),e&&o(An),e&&o(Tf),e&&o(Ye),w(jo),e&&o($f),e&&o(wt),e&&o(Pf),w(Do,e),e&&o(If),e&&o(Ro),e&&o(Af),w(Fo,e),e&&o(Lf),e&&o(Ln),e&&o(Nf),w(zo,e),e&&o(Bf),e&&o(Ho),e&&o(Mf),e&&o(Go),e&&o(Of),e&&o(Uo),e&&o(xf),w(Jo,e),e&&o(jf),e&&o(vt),e&&o(Cf),e&&o(Xe),w(Xo),e&&o(qf),e&&o(A),e&&o(Sf),e&&o(Nn),e&&o(Df),e&&o(_t),e&&o(Rf),e&&o(L),e&&o(Ff),e&&o(Et),e&&o(zf),e&&o(Bn),e&&o(Hf),e&&o(Mn),e&&o(Wf),e&&o(kt),e&&o(Gf),e&&o(On),e&&o(Uf),e&&o(Tt),e&&o(Yf),e&&o($t),e&&o(Jf),w(Vo,e),e&&o(Xf),e&&o(xn),e&&o(Zf),e&&o(Pt),e&&o(Kf),e&&o(jn),e&&o(Qf),e&&o(It),e&&o(Vf),e&&o(U),e&&o(em),e&&o(er),e&&o(tm),e&&o(At),e&&o(om),e&&o(Cn),e&&o(rm),e&&o(qn),e&&o(nm),e&&o(N),e&&o(am),e&&o(Qe),e&&o(im),e&&o(Sn),e&&o(sm),w(rr,e),e&&o(lm),e&&o(ge),e&&o(dm),e&&o(C),e&&o(hm),e&&o(Lt),e&&o(fm),e&&o(Ve),w(sr),e&&o(mm),e&&o(Dn),e&&o(pm),w(lr,e),e&&o(um),e&&o(Bt),e&&o(cm),e&&o(Fn),e&&o(ym),e&&o(Mt),e&&o(gm),e&&o(we),e&&o(wm),e&&o(Wn),e&&o(vm),e&&o(ve),e&&o(bm),e&&o(Gn),e&&o(_m),e&&o(Un),e&&o(Em),w(hr,e),e&&o(km),e&&o(fr),e&&o(Tm),w(mr,e),e&&o($m),e&&o(pr),e&&o(Pm),w(ur,e),e&&o(Im),e&&o(cr),e&&o(Am),w(yr,e),e&&o(Lm),e&&o(et),e&&o(Nm),e&&o(Yn),e&&o(Bm),w(gr,e),e&&o(Mm),e&&o(Jn),e&&o(Om),e&&o(Xn),e&&o(xm),e&&o(Zn),e&&o(jm),e&&o(Kn),e&&o(Cm),e&&o(be),e&&o(qm),e&&o(Y),e&&o(Sm),e&&o(M),e&&o(Dm),w(wr,e),e&&o(Rm),e&&o(_e),e&&o(Fm),e&&o(Qn),e&&o(zm),e&&o(J),e&&o(Hm),e&&o(Ot),e&&o(Wm),e&&o(xt),e&&o(Gm),w(_r,e),e&&o(Um),e&&o(X),e&&o(Ym),w(Er,e),e&&o(Jm),e&&o(ta),e&&o(Xm),w(kr,e),e&&o(Zm),e&&o(oa),e&&o(Km),w(Tr,e),e&&o(Qm),e&&o(ra),e&&o(Vm),w($r,e),e&&o(ep),e&&o(Pr),e&&o(tp),w(Ir,e),e&&o(op),e&&o(Ee),e&&o(rp),w(Ar,e),e&&o(np),e&&o(Lr),e&&o(ap),w(Nr,e),e&&o(ip),e&&o(na),e&&o(sp),e&&o(jt),e&&o(lp),e&&o(Z),e&&o(dp),e&&o(K),e&&o(hp),w(Br,e),e&&o(fp),e&&o(aa),e&&o(mp),e&&o(Ct),e&&o(pp),w(Mr,e),e&&o(up),e&&o(q),e&&o(cp),e&&o(Q),e&&o(yp),e&&o(V),e&&o(gp),e&&o(ke),e&&o(wp),e&&o(qt),e&&o(vp),e&&o(sa),e&&o(bp),e&&o(St),e&&o(_p),w(xr,e),e&&o(Ep),e&&o(la),e&&o(kp),e&&o(Dt),e&&o(Tp),e&&o(Rt),e&&o($p),w(jr,e),e&&o(Pp),w(Ft,e),e&&o(Ip),e&&o(te),e&&o(Ap),e&&o(zt),e&&o(Lp),e&&o(ha),e&&o(Np),e&&o(Ht),e&&o(Bp),e&&o(fa),e&&o(Mp),e&&o(ma),e&&o(Op),w(qr,e),e&&o(xp),e&&o(Wt),e&&o(jp),w(Sr,e),e&&o(Cp),e&&o(Gt),e&&o(qp),e&&o(Te),e&&o(Sp),e&&o(pa),e&&o(Dp),e&&o(oe),e&&o(Rp),e&&o(ua),e&&o(Fp),e&&o(re),e&&o(zp),e&&o($e),e&&o(Hp),e&&o(ya),e&&o(Wp),e&&o(Ut),e&&o(Gp),w(Dr,e),e&&o(Up),e&&o(ga),e&&o(Yp),w(Rr,e),e&&o(Jp),e&&o(wa),e&&o(Xp),e&&o(va),e&&o(Zp),e&&o(ba),e&&o(Kp),e&&o(_a),e&&o(Qp),e&&o(S),e&&o(Vp),w(Fr,e),e&&o(eu),e&&o(Yt),e&&o(tu),e&&o(ka),e&&o(ou),e&&o(Jt),e&&o(ru),e&&o(Ta),e&&o(nu),e&&o($a),e&&o(au),e&&o(ot),w(zr),e&&o(iu),e&&o(Pa),e&&o(su),e&&o(Ia)}}}const I9={local:"how-to-add-a-model-to-transformers",sections:[{local:"general-overview-of-transformers",sections:[{local:"overview-of-models",title:"Overview of models"},{local:"code-style",title:"Code style"},{local:"overview-of-tokenizers",title:"Overview of tokenizers"}],title:"General overview of \u{1F917} Transformers"},{local:"stepbystep-recipe-to-add-a-model-to-transformers",sections:[{local:"1-optional-theoretical-aspects-of-brandnewbert",title:"1. (Optional) Theoretical aspects of BrandNewBert"},{local:"2-next-prepare-your-environment",title:"2. Next prepare your environment"},{local:"34-run-a-pretrained-checkpoint-using-the-original-repository",title:"3.-4. Run a pretrained checkpoint using the original repository"},{local:"514-port-brandnewbert-to-transformers",title:"5.-14. Port BrandNewBert to \u{1F917} Transformers"},{local:"share-your-work",title:"Share your work!!"}],title:"Step-by-step recipe to add a model to \u{1F917} Transformers"}],title:"How to add a model to \u{1F917} Transformers?"};function A9(Rh){return k9(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class O9 extends w9{constructor(P){super();v9(this,P,A9,P9,b9,{})}}export{O9 as default,I9 as metadata};
