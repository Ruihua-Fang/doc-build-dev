import{S as lu,i as du,s as cu,e as n,k as l,w as k,t as a,M as pu,c as r,d as t,m as d,a as s,x as $,h as i,b as p,F as e,g as f,y as P,q as w,o as R,B as E,v as hu,L as In}from"../../chunks/vendor-6b77c823.js";import{T as yt}from"../../chunks/Tip-39098574.js";import{D as U}from"../../chunks/Docstring-1088f2fb.js";import{C as Sn}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ge}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Ln}from"../../chunks/ExampleCodeBlock-5212b321.js";function fu(F){let h,T;return h=new Sn({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),{c(){k(h.$$.fragment)},l(g){$(h.$$.fragment,g)},m(g,u){P(h,g,u),T=!0},p:In,i(g){T||(w(h.$$.fragment,g),T=!0)},o(g){R(h.$$.fragment,g),T=!1},d(g){E(h,g)}}}function uu(F){let h,T,g,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var x=s(g);u=i(x,"Module"),x.forEach(t),b=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(c,_){f(c,h,_),e(h,T),e(h,g),e(g,u),e(h,b)},d(c){c&&t(h)}}}function mu(F){let h,T,g,u,b;return u=new Sn({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),g=l(),k(u.$$.fragment)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Examples:"),_.forEach(t),g=d(c),$(u.$$.fragment,c)},m(c,_){f(c,h,_),e(h,T),f(c,g,_),P(u,c,_),b=!0},p:In,i(c){b||(w(u.$$.fragment,c),b=!0)},o(c){R(u.$$.fragment,c),b=!1},d(c){c&&t(h),c&&t(g),E(u,c)}}}function gu(F){let h,T,g,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var x=s(g);u=i(x,"Module"),x.forEach(t),b=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(c,_){f(c,h,_),e(h,T),e(h,g),e(g,u),e(h,b)},d(c){c&&t(h)}}}function _u(F){let h,T,g,u,b;return u=new Sn({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),g=l(),k(u.$$.fragment)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Examples:"),_.forEach(t),g=d(c),$(u.$$.fragment,c)},m(c,_){f(c,h,_),e(h,T),f(c,g,_),P(u,c,_),b=!0},p:In,i(c){b||(w(u.$$.fragment,c),b=!0)},o(c){R(u.$$.fragment,c),b=!1},d(c){c&&t(h),c&&t(g),E(u,c)}}}function vu(F){let h,T,g,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var x=s(g);u=i(x,"Module"),x.forEach(t),b=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(c,_){f(c,h,_),e(h,T),e(h,g),e(g,u),e(h,b)},d(c){c&&t(h)}}}function bu(F){let h,T,g,u,b;return u=new Sn({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=n("p"),T=a("Examples:"),g=l(),k(u.$$.fragment)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Examples:"),_.forEach(t),g=d(c),$(u.$$.fragment,c)},m(c,_){f(c,h,_),e(h,T),f(c,g,_),P(u,c,_),b=!0},p:In,i(c){b||(w(u.$$.fragment,c),b=!0)},o(c){R(u.$$.fragment,c),b=!1},d(c){c&&t(h),c&&t(g),E(u,c)}}}function Tu(F){let h,T,g,u,b,c,_,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Re,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),g=l(),u=n("ul"),b=n("li"),c=a("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=l(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Re=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(v){h=r(v,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),g=d(v),u=r(v,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);c=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),_=d(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=d(v),D=r(v,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),I=r(C,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(C,"CODE",{});var Ne=s(S);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),H=d(v),L=r(v,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=d(v),q=r(v,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),B=r(Y,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=d(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Re=d(O),Q=r(O,"LI",{});var W=s(Q);K=i(W,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(W,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),W.forEach(t),O.forEach(t)},m(v,y){f(v,h,y),e(h,T),f(v,g,y),f(v,u,y),e(u,b),e(b,c),e(u,_),e(u,x),e(x,_e),f(v,Z,y),f(v,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),f(v,H,y),f(v,L,y),e(L,te),f(v,oe,y),f(v,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,B),e(B,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Re),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(v){v&&t(h),v&&t(g),v&&t(u),v&&t(Z),v&&t(D),v&&t(H),v&&t(L),v&&t(oe),v&&t(q)}}}function ku(F){let h,T,g,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var x=s(g);u=i(x,"Module"),x.forEach(t),b=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(c,_){f(c,h,_),e(h,T),e(h,g),e(g,u),e(h,b)},d(c){c&&t(h)}}}function $u(F){let h,T,g,u,b;return u=new Sn({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),g=l(),k(u.$$.fragment)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Examples:"),_.forEach(t),g=d(c),$(u.$$.fragment,c)},m(c,_){f(c,h,_),e(h,T),f(c,g,_),P(u,c,_),b=!0},p:In,i(c){b||(w(u.$$.fragment,c),b=!0)},o(c){R(u.$$.fragment,c),b=!1},d(c){c&&t(h),c&&t(g),E(u,c)}}}function Pu(F){let h,T,g,u,b,c,_,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Re,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),g=l(),u=n("ul"),b=n("li"),c=a("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=l(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Re=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(v){h=r(v,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),g=d(v),u=r(v,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);c=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),_=d(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=d(v),D=r(v,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),I=r(C,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(C,"CODE",{});var Ne=s(S);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),H=d(v),L=r(v,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=d(v),q=r(v,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),B=r(Y,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=d(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Re=d(O),Q=r(O,"LI",{});var W=s(Q);K=i(W,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(W,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),W.forEach(t),O.forEach(t)},m(v,y){f(v,h,y),e(h,T),f(v,g,y),f(v,u,y),e(u,b),e(b,c),e(u,_),e(u,x),e(x,_e),f(v,Z,y),f(v,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),f(v,H,y),f(v,L,y),e(L,te),f(v,oe,y),f(v,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,B),e(B,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Re),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(v){v&&t(h),v&&t(g),v&&t(u),v&&t(Z),v&&t(D),v&&t(H),v&&t(L),v&&t(oe),v&&t(q)}}}function wu(F){let h,T,g,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var x=s(g);u=i(x,"Module"),x.forEach(t),b=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(c,_){f(c,h,_),e(h,T),e(h,g),e(g,u),e(h,b)},d(c){c&&t(h)}}}function Ru(F){let h,T,g,u,b;return u=new Sn({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),g=l(),k(u.$$.fragment)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Examples:"),_.forEach(t),g=d(c),$(u.$$.fragment,c)},m(c,_){f(c,h,_),e(h,T),f(c,g,_),P(u,c,_),b=!0},p:In,i(c){b||(w(u.$$.fragment,c),b=!0)},o(c){R(u.$$.fragment,c),b=!1},d(c){c&&t(h),c&&t(g),E(u,c)}}}function Eu(F){let h,T,g,u,b,c,_,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Re,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),g=l(),u=n("ul"),b=n("li"),c=a("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=l(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Re=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(v){h=r(v,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),g=d(v),u=r(v,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);c=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),_=d(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=d(v),D=r(v,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),I=r(C,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(C,"CODE",{});var Ne=s(S);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),H=d(v),L=r(v,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=d(v),q=r(v,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),B=r(Y,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=d(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Re=d(O),Q=r(O,"LI",{});var W=s(Q);K=i(W,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(W,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),W.forEach(t),O.forEach(t)},m(v,y){f(v,h,y),e(h,T),f(v,g,y),f(v,u,y),e(u,b),e(b,c),e(u,_),e(u,x),e(x,_e),f(v,Z,y),f(v,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),f(v,H,y),f(v,L,y),e(L,te),f(v,oe,y),f(v,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,B),e(B,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Re),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(v){v&&t(h),v&&t(g),v&&t(u),v&&t(Z),v&&t(D),v&&t(H),v&&t(L),v&&t(oe),v&&t(q)}}}function Du(F){let h,T,g,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(_,"CODE",{});var x=s(g);u=i(x,"Module"),x.forEach(t),b=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(c,_){f(c,h,_),e(h,T),e(h,g),e(g,u),e(h,b)},d(c){c&&t(h)}}}function yu(F){let h,T,g,u,b;return u=new Sn({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=n("p"),T=a("Examples:"),g=l(),k(u.$$.fragment)},l(c){h=r(c,"P",{});var _=s(h);T=i(_,"Examples:"),_.forEach(t),g=d(c),$(u.$$.fragment,c)},m(c,_){f(c,h,_),e(h,T),f(c,g,_),P(u,c,_),b=!0},p:In,i(c){b||(w(u.$$.fragment,c),b=!0)},o(c){R(u.$$.fragment,c),b=!1},d(c){c&&t(h),c&&t(g),E(u,c)}}}function xu(F){let h,T,g,u,b,c,_,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Re,Q,K,N,se,v,y,G,Oe,ze,C,me,Ne,ae,O,Y,je,xe,X,Me,Qe,W,Le,Hn,pi,hi,Bn,fi,ui,Wn,mi,gi,_i,Po,vi,Un,bi,Ti,Ks,st,xt,Ar,wo,ki,Or,$i,Ys,qe,Ro,Pi,Nr,wi,Ri,zt,Vn,Ei,Di,Kn,yi,xi,zi,Eo,qi,Yn,Fi,Ci,Xs,at,qt,jr,Do,Ai,Mr,Oi,Js,Fe,yo,Ni,xo,ji,Qr,Mi,Qi,Li,Ft,Xn,Ii,Si,Jn,Hi,Bi,Wi,zo,Ui,Gn,Vi,Ki,Gs,it,Ct,Lr,qo,Yi,Ir,Xi,Zs,Ce,Fo,Ji,Sr,Gi,Zi,At,Zn,el,tl,er,ol,nl,rl,Co,sl,tr,al,il,ea,lt,Ot,Hr,Ao,ll,Br,dl,ta,Ae,Oo,cl,No,pl,Wr,hl,fl,ul,Nt,or,ml,gl,nr,_l,vl,bl,jo,Tl,rr,kl,$l,oa,dt,jt,Ur,Mo,Pl,Vr,wl,na,ie,Qo,Rl,Kr,El,Dl,et,sr,yl,xl,ar,zl,ql,ir,Fl,Cl,Al,Lo,Ol,lr,Nl,jl,Ml,Ge,Ql,Yr,Ll,Il,Xr,Sl,Hl,Jr,Bl,Wl,Ul,Mt,ra,ct,Qt,Gr,Io,Vl,Zr,Kl,sa,le,So,Yl,Ho,Xl,es,Jl,Gl,Zl,tt,dr,ed,td,cr,od,nd,pr,rd,sd,ad,Bo,id,hr,ld,dd,cd,Ze,pd,ts,hd,fd,os,ud,md,ns,gd,_d,vd,rs,bd,aa,pt,Lt,ss,Wo,Td,as,kd,ia,ht,Uo,$d,Vo,Pd,fr,wd,Rd,la,ft,Ko,Ed,Yo,Dd,ur,yd,xd,da,ut,Xo,zd,Jo,qd,mr,Fd,Cd,ca,mt,It,is,Go,Ad,ls,Od,pa,Ee,Zo,Nd,ds,jd,Md,en,Qd,gr,Ld,Id,Sd,tn,Hd,on,Bd,Wd,Ud,Ie,nn,Vd,gt,Kd,_r,Yd,Xd,cs,Jd,Gd,Zd,St,ec,Ht,ha,_t,Bt,ps,rn,tc,hs,oc,fa,De,sn,nc,fs,rc,sc,an,ac,vr,ic,lc,dc,ln,cc,dn,pc,hc,fc,Se,cn,uc,vt,mc,br,gc,_c,us,vc,bc,Tc,Wt,kc,Ut,ua,bt,Vt,ms,pn,$c,gs,Pc,ma,ye,hn,wc,_s,Rc,Ec,fn,Dc,Tr,yc,xc,zc,un,qc,mn,Fc,Cc,Ac,He,gn,Oc,Tt,Nc,kr,jc,Mc,vs,Qc,Lc,Ic,Kt,Sc,Yt,ga,kt,Xt,bs,_n,Hc,Ts,Bc,_a,de,vn,Wc,ks,Uc,Vc,bn,Kc,$r,Yc,Xc,Jc,Tn,Gc,kn,Zc,ep,tp,Jt,op,Be,$n,np,$t,rp,Pr,sp,ap,$s,ip,lp,dp,Gt,cp,Zt,va,Pt,eo,Ps,Pn,pp,ws,hp,ba,ce,wn,fp,Rs,up,mp,Rn,gp,wr,_p,vp,bp,En,Tp,Dn,kp,$p,Pp,to,wp,We,yn,Rp,wt,Ep,Rr,Dp,yp,Es,xp,zp,qp,oo,Fp,no,Ta,Rt,ro,Ds,xn,Cp,ys,Ap,ka,pe,zn,Op,xs,Np,jp,qn,Mp,Er,Qp,Lp,Ip,Fn,Sp,Cn,Hp,Bp,Wp,so,Up,Ue,An,Vp,Et,Kp,Dr,Yp,Xp,zs,Jp,Gp,Zp,ao,eh,io,$a;return c=new ge({}),ee=new ge({}),v=new ge({}),me=new U({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/configuration_dpr.py#L33"}}),wo=new ge({}),Ro=new U({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/tokenization_dpr.py#L89"}}),Do=new ge({}),yo=new U({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/tokenization_dpr_fast.py#L90"}}),qo=new ge({}),Fo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/tokenization_dpr.py#L105"}}),Ao=new ge({}),Oo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/tokenization_dpr_fast.py#L107"}}),Mo=new ge({}),Qo=new U({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/tokenization_dpr.py#L369",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Mt=new Ln({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[fu]},$$scope:{ctx:F}}}),Io=new ge({}),So=new U({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/pr_highlight/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/tokenization_dpr_fast.py#L370",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Wo=new ge({}),Uo=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L62"}}),Ko=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L90"}}),Xo=new U({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L118"}}),Go=new ge({}),Zo=new U({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L445"}}),nn=new U({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L453",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),St=new yt({props:{$$slots:{default:[uu]},$$scope:{ctx:F}}}),Ht=new Ln({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[mu]},$$scope:{ctx:F}}}),rn=new ge({}),sn=new U({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L526"}}),cn=new U({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L534",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Wt=new yt({props:{$$slots:{default:[gu]},$$scope:{ctx:F}}}),Ut=new Ln({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[_u]},$$scope:{ctx:F}}}),pn=new ge({}),hn=new U({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L607"}}),gn=new U({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_highlight/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_dpr.py#L615",returnDescription:`
<p>A <a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new yt({props:{$$slots:{default:[vu]},$$scope:{ctx:F}}}),Yt=new Ln({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[bu]},$$scope:{ctx:F}}}),_n=new ge({}),vn=new U({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_tf_dpr.py#L538"}}),Jt=new yt({props:{$$slots:{default:[Tu]},$$scope:{ctx:F}}}),$n=new U({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_tf_dpr.py#L550",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),Gt=new yt({props:{$$slots:{default:[ku]},$$scope:{ctx:F}}}),Zt=new Ln({props:{anchor:"transformers.TFDPRContextEncoder.call.example",$$slots:{default:[$u]},$$scope:{ctx:F}}}),Pn=new ge({}),wn=new U({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_tf_dpr.py#L626"}}),to=new yt({props:{$$slots:{default:[Pu]},$$scope:{ctx:F}}}),yn=new U({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_tf_dpr.py#L638",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),oo=new yt({props:{$$slots:{default:[wu]},$$scope:{ctx:F}}}),no=new Ln({props:{anchor:"transformers.TFDPRQuestionEncoder.call.example",$$slots:{default:[Ru]},$$scope:{ctx:F}}}),xn=new ge({}),zn=new U({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_tf_dpr.py#L713"}}),so=new yt({props:{$$slots:{default:[Eu]},$$scope:{ctx:F}}}),An=new U({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_highlight/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/pr_highlight/src/transformers/models/dpr/modeling_tf_dpr.py#L725",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ao=new yt({props:{$$slots:{default:[Du]},$$scope:{ctx:F}}}),io=new Ln({props:{anchor:"transformers.TFDPRReader.call.example",$$slots:{default:[yu]},$$scope:{ctx:F}}}),{c(){h=n("meta"),T=l(),g=n("h1"),u=n("a"),b=n("span"),k(c.$$.fragment),_=l(),x=n("span"),_e=a("DPR"),Z=l(),D=n("h2"),J=n("a"),I=n("span"),k(ee.$$.fragment),ve=l(),S=n("span"),be=a("Overview"),he=l(),H=n("p"),L=a(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=n("a"),oe=a("Dense Passage Retrieval for Open-Domain Question Answering"),q=a(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),A=l(),ne=n("p"),V=a("The abstract from the paper is the following:"),fe=l(),re=n("p"),B=n("em"),Te=a(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ue=l(),z=n("p"),ke=a("This model was contributed by "),j=n("a"),$e=a("lhoestq"),Pe=a(". The original code can be found "),M=n("a"),we=a("here"),Re=a("."),Q=l(),K=n("h2"),N=n("a"),se=n("span"),k(v.$$.fragment),y=l(),G=n("span"),Oe=a("DPRConfig"),ze=l(),C=n("div"),k(me.$$.fragment),Ne=l(),ae=n("p"),O=n("a"),Y=a("DPRConfig"),je=a(" is the configuration class to store the configuration of a "),xe=n("em"),X=a("DPRModel"),Me=a("."),Qe=l(),W=n("p"),Le=a("This is the configuration class to store the configuration of a "),Hn=n("a"),pi=a("DPRContextEncoder"),hi=a(", "),Bn=n("a"),fi=a("DPRQuestionEncoder"),ui=a(`, or a
`),Wn=n("a"),mi=a("DPRReader"),gi=a(". It is used to instantiate the components of the DPR model."),_i=l(),Po=n("p"),vi=a("This class is a subclass of "),Un=n("a"),bi=a("BertConfig"),Ti=a(". Please check the superclass for the documentation of all kwargs."),Ks=l(),st=n("h2"),xt=n("a"),Ar=n("span"),k(wo.$$.fragment),ki=l(),Or=n("span"),$i=a("DPRContextEncoderTokenizer"),Ys=l(),qe=n("div"),k(Ro.$$.fragment),Pi=l(),Nr=n("p"),wi=a("Construct a DPRContextEncoder tokenizer."),Ri=l(),zt=n("p"),Vn=n("a"),Ei=a("DPRContextEncoderTokenizer"),Di=a(" is identical to "),Kn=n("a"),yi=a("BertTokenizer"),xi=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),zi=l(),Eo=n("p"),qi=a("Refer to superclass "),Yn=n("a"),Fi=a("BertTokenizer"),Ci=a(" for usage examples and documentation concerning parameters."),Xs=l(),at=n("h2"),qt=n("a"),jr=n("span"),k(Do.$$.fragment),Ai=l(),Mr=n("span"),Oi=a("DPRContextEncoderTokenizerFast"),Js=l(),Fe=n("div"),k(yo.$$.fragment),Ni=l(),xo=n("p"),ji=a("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Qr=n("em"),Mi=a("tokenizers"),Qi=a(" library)."),Li=l(),Ft=n("p"),Xn=n("a"),Ii=a("DPRContextEncoderTokenizerFast"),Si=a(" is identical to "),Jn=n("a"),Hi=a("BertTokenizerFast"),Bi=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Wi=l(),zo=n("p"),Ui=a("Refer to superclass "),Gn=n("a"),Vi=a("BertTokenizerFast"),Ki=a(" for usage examples and documentation concerning parameters."),Gs=l(),it=n("h2"),Ct=n("a"),Lr=n("span"),k(qo.$$.fragment),Yi=l(),Ir=n("span"),Xi=a("DPRQuestionEncoderTokenizer"),Zs=l(),Ce=n("div"),k(Fo.$$.fragment),Ji=l(),Sr=n("p"),Gi=a("Constructs a DPRQuestionEncoder tokenizer."),Zi=l(),At=n("p"),Zn=n("a"),el=a("DPRQuestionEncoderTokenizer"),tl=a(" is identical to "),er=n("a"),ol=a("BertTokenizer"),nl=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),rl=l(),Co=n("p"),sl=a("Refer to superclass "),tr=n("a"),al=a("BertTokenizer"),il=a(" for usage examples and documentation concerning parameters."),ea=l(),lt=n("h2"),Ot=n("a"),Hr=n("span"),k(Ao.$$.fragment),ll=l(),Br=n("span"),dl=a("DPRQuestionEncoderTokenizerFast"),ta=l(),Ae=n("div"),k(Oo.$$.fragment),cl=l(),No=n("p"),pl=a("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Wr=n("em"),hl=a("tokenizers"),fl=a(" library)."),ul=l(),Nt=n("p"),or=n("a"),ml=a("DPRQuestionEncoderTokenizerFast"),gl=a(" is identical to "),nr=n("a"),_l=a("BertTokenizerFast"),vl=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),bl=l(),jo=n("p"),Tl=a("Refer to superclass "),rr=n("a"),kl=a("BertTokenizerFast"),$l=a(" for usage examples and documentation concerning parameters."),oa=l(),dt=n("h2"),jt=n("a"),Ur=n("span"),k(Mo.$$.fragment),Pl=l(),Vr=n("span"),wl=a("DPRReaderTokenizer"),na=l(),ie=n("div"),k(Qo.$$.fragment),Rl=l(),Kr=n("p"),El=a("Construct a DPRReader tokenizer."),Dl=l(),et=n("p"),sr=n("a"),yl=a("DPRReaderTokenizer"),xl=a(" is almost identical to "),ar=n("a"),zl=a("BertTokenizer"),ql=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),ir=n("a"),Fl=a("DPRReader"),Cl=a(" model."),Al=l(),Lo=n("p"),Ol=a("Refer to superclass "),lr=n("a"),Nl=a("BertTokenizer"),jl=a(" for usage examples and documentation concerning parameters."),Ml=l(),Ge=n("p"),Ql=a("Return a dictionary with the token ids of the input strings and other information to give to "),Yr=n("code"),Ll=a(".decode_best_spans"),Il=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Xr=n("code"),Sl=a("input_ids"),Hl=a(" is a matrix of size "),Jr=n("code"),Bl=a("(n_passages, sequence_length)"),Wl=a(`
with the format:`),Ul=l(),k(Mt.$$.fragment),ra=l(),ct=n("h2"),Qt=n("a"),Gr=n("span"),k(Io.$$.fragment),Vl=l(),Zr=n("span"),Kl=a("DPRReaderTokenizerFast"),sa=l(),le=n("div"),k(So.$$.fragment),Yl=l(),Ho=n("p"),Xl=a("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),es=n("em"),Jl=a("tokenizers"),Gl=a(" library)."),Zl=l(),tt=n("p"),dr=n("a"),ed=a("DPRReaderTokenizerFast"),td=a(" is almost identical to "),cr=n("a"),od=a("BertTokenizerFast"),nd=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),pr=n("a"),rd=a("DPRReader"),sd=a(" model."),ad=l(),Bo=n("p"),id=a("Refer to superclass "),hr=n("a"),ld=a("BertTokenizerFast"),dd=a(" for usage examples and documentation concerning parameters."),cd=l(),Ze=n("p"),pd=a("Return a dictionary with the token ids of the input strings and other information to give to "),ts=n("code"),hd=a(".decode_best_spans"),fd=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),os=n("code"),ud=a("input_ids"),md=a(" is a matrix of size "),ns=n("code"),gd=a("(n_passages, sequence_length)"),_d=a(`
with the format:`),vd=l(),rs=n("p"),bd=a("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),aa=l(),pt=n("h2"),Lt=n("a"),ss=n("span"),k(Wo.$$.fragment),Td=l(),as=n("span"),kd=a("DPR specific outputs"),ia=l(),ht=n("div"),k(Uo.$$.fragment),$d=l(),Vo=n("p"),Pd=a("Class for outputs of "),fr=n("a"),wd=a("DPRQuestionEncoder"),Rd=a("."),la=l(),ft=n("div"),k(Ko.$$.fragment),Ed=l(),Yo=n("p"),Dd=a("Class for outputs of "),ur=n("a"),yd=a("DPRQuestionEncoder"),xd=a("."),da=l(),ut=n("div"),k(Xo.$$.fragment),zd=l(),Jo=n("p"),qd=a("Class for outputs of "),mr=n("a"),Fd=a("DPRQuestionEncoder"),Cd=a("."),ca=l(),mt=n("h2"),It=n("a"),is=n("span"),k(Go.$$.fragment),Ad=l(),ls=n("span"),Od=a("DPRContextEncoder"),pa=l(),Ee=n("div"),k(Zo.$$.fragment),Nd=l(),ds=n("p"),jd=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Md=l(),en=n("p"),Qd=a("This model inherits from "),gr=n("a"),Ld=a("PreTrainedModel"),Id=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sd=l(),tn=n("p"),Hd=a("This model is also a PyTorch "),on=n("a"),Bd=a("torch.nn.Module"),Wd=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ud=l(),Ie=n("div"),k(nn.$$.fragment),Vd=l(),gt=n("p"),Kd=a("The "),_r=n("a"),Yd=a("DPRContextEncoder"),Xd=a(" forward method, overrides the "),cs=n("code"),Jd=a("__call__"),Gd=a(" special method."),Zd=l(),k(St.$$.fragment),ec=l(),k(Ht.$$.fragment),ha=l(),_t=n("h2"),Bt=n("a"),ps=n("span"),k(rn.$$.fragment),tc=l(),hs=n("span"),oc=a("DPRQuestionEncoder"),fa=l(),De=n("div"),k(sn.$$.fragment),nc=l(),fs=n("p"),rc=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),sc=l(),an=n("p"),ac=a("This model inherits from "),vr=n("a"),ic=a("PreTrainedModel"),lc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dc=l(),ln=n("p"),cc=a("This model is also a PyTorch "),dn=n("a"),pc=a("torch.nn.Module"),hc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),fc=l(),Se=n("div"),k(cn.$$.fragment),uc=l(),vt=n("p"),mc=a("The "),br=n("a"),gc=a("DPRQuestionEncoder"),_c=a(" forward method, overrides the "),us=n("code"),vc=a("__call__"),bc=a(" special method."),Tc=l(),k(Wt.$$.fragment),kc=l(),k(Ut.$$.fragment),ua=l(),bt=n("h2"),Vt=n("a"),ms=n("span"),k(pn.$$.fragment),$c=l(),gs=n("span"),Pc=a("DPRReader"),ma=l(),ye=n("div"),k(hn.$$.fragment),wc=l(),_s=n("p"),Rc=a("The bare DPRReader transformer outputting span predictions."),Ec=l(),fn=n("p"),Dc=a("This model inherits from "),Tr=n("a"),yc=a("PreTrainedModel"),xc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zc=l(),un=n("p"),qc=a("This model is also a PyTorch "),mn=n("a"),Fc=a("torch.nn.Module"),Cc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ac=l(),He=n("div"),k(gn.$$.fragment),Oc=l(),Tt=n("p"),Nc=a("The "),kr=n("a"),jc=a("DPRReader"),Mc=a(" forward method, overrides the "),vs=n("code"),Qc=a("__call__"),Lc=a(" special method."),Ic=l(),k(Kt.$$.fragment),Sc=l(),k(Yt.$$.fragment),ga=l(),kt=n("h2"),Xt=n("a"),bs=n("span"),k(_n.$$.fragment),Hc=l(),Ts=n("span"),Bc=a("TFDPRContextEncoder"),_a=l(),de=n("div"),k(vn.$$.fragment),Wc=l(),ks=n("p"),Uc=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Vc=l(),bn=n("p"),Kc=a("This model inherits from "),$r=n("a"),Yc=a("TFPreTrainedModel"),Xc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jc=l(),Tn=n("p"),Gc=a("This model is also a Tensorflow "),kn=n("a"),Zc=a("tf.keras.Model"),ep=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),tp=l(),k(Jt.$$.fragment),op=l(),Be=n("div"),k($n.$$.fragment),np=l(),$t=n("p"),rp=a("The "),Pr=n("a"),sp=a("TFDPRContextEncoder"),ap=a(" forward method, overrides the "),$s=n("code"),ip=a("__call__"),lp=a(" special method."),dp=l(),k(Gt.$$.fragment),cp=l(),k(Zt.$$.fragment),va=l(),Pt=n("h2"),eo=n("a"),Ps=n("span"),k(Pn.$$.fragment),pp=l(),ws=n("span"),hp=a("TFDPRQuestionEncoder"),ba=l(),ce=n("div"),k(wn.$$.fragment),fp=l(),Rs=n("p"),up=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),mp=l(),Rn=n("p"),gp=a("This model inherits from "),wr=n("a"),_p=a("TFPreTrainedModel"),vp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),bp=l(),En=n("p"),Tp=a("This model is also a Tensorflow "),Dn=n("a"),kp=a("tf.keras.Model"),$p=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Pp=l(),k(to.$$.fragment),wp=l(),We=n("div"),k(yn.$$.fragment),Rp=l(),wt=n("p"),Ep=a("The "),Rr=n("a"),Dp=a("TFDPRQuestionEncoder"),yp=a(" forward method, overrides the "),Es=n("code"),xp=a("__call__"),zp=a(" special method."),qp=l(),k(oo.$$.fragment),Fp=l(),k(no.$$.fragment),Ta=l(),Rt=n("h2"),ro=n("a"),Ds=n("span"),k(xn.$$.fragment),Cp=l(),ys=n("span"),Ap=a("TFDPRReader"),ka=l(),pe=n("div"),k(zn.$$.fragment),Op=l(),xs=n("p"),Np=a("The bare DPRReader transformer outputting span predictions."),jp=l(),qn=n("p"),Mp=a("This model inherits from "),Er=n("a"),Qp=a("TFPreTrainedModel"),Lp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ip=l(),Fn=n("p"),Sp=a("This model is also a Tensorflow "),Cn=n("a"),Hp=a("tf.keras.Model"),Bp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Wp=l(),k(so.$$.fragment),Up=l(),Ue=n("div"),k(An.$$.fragment),Vp=l(),Et=n("p"),Kp=a("The "),Dr=n("a"),Yp=a("TFDPRReader"),Xp=a(" forward method, overrides the "),zs=n("code"),Jp=a("__call__"),Gp=a(" special method."),Zp=l(),k(ao.$$.fragment),eh=l(),k(io.$$.fragment),this.h()},l(o){const m=pu('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(t),T=d(o),g=r(o,"H1",{class:!0});var On=s(g);u=r(On,"A",{id:!0,class:!0,href:!0});var qs=s(u);b=r(qs,"SPAN",{});var Fs=s(b);$(c.$$.fragment,Fs),Fs.forEach(t),qs.forEach(t),_=d(On),x=r(On,"SPAN",{});var Cs=s(x);_e=i(Cs,"DPR"),Cs.forEach(t),On.forEach(t),Z=d(o),D=r(o,"H2",{class:!0});var Nn=s(D);J=r(Nn,"A",{id:!0,class:!0,href:!0});var As=s(J);I=r(As,"SPAN",{});var Os=s(I);$(ee.$$.fragment,Os),Os.forEach(t),As.forEach(t),ve=d(Nn),S=r(Nn,"SPAN",{});var Ns=s(S);be=i(Ns,"Overview"),Ns.forEach(t),Nn.forEach(t),he=d(o),H=r(o,"P",{});var jn=s(H);L=i(jn,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=r(jn,"A",{href:!0,rel:!0});var js=s(te);oe=i(js,"Dense Passage Retrieval for Open-Domain Question Answering"),js.forEach(t),q=i(jn,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),jn.forEach(t),A=d(o),ne=r(o,"P",{});var Ms=s(ne);V=i(Ms,"The abstract from the paper is the following:"),Ms.forEach(t),fe=d(o),re=r(o,"P",{});var Qs=s(re);B=r(Qs,"EM",{});var Ls=s(B);Te=i(Ls,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),Ls.forEach(t),Qs.forEach(t),ue=d(o),z=r(o,"P",{});var Dt=s(z);ke=i(Dt,"This model was contributed by "),j=r(Dt,"A",{href:!0,rel:!0});var Is=s(j);$e=i(Is,"lhoestq"),Is.forEach(t),Pe=i(Dt,". The original code can be found "),M=r(Dt,"A",{href:!0,rel:!0});var Ss=s(M);we=i(Ss,"here"),Ss.forEach(t),Re=i(Dt,"."),Dt.forEach(t),Q=d(o),K=r(o,"H2",{class:!0});var Pa=s(K);N=r(Pa,"A",{id:!0,class:!0,href:!0});var th=s(N);se=r(th,"SPAN",{});var oh=s(se);$(v.$$.fragment,oh),oh.forEach(t),th.forEach(t),y=d(Pa),G=r(Pa,"SPAN",{});var nh=s(G);Oe=i(nh,"DPRConfig"),nh.forEach(t),Pa.forEach(t),ze=d(o),C=r(o,"DIV",{class:!0});var lo=s(C);$(me.$$.fragment,lo),Ne=d(lo),ae=r(lo,"P",{});var Hs=s(ae);O=r(Hs,"A",{href:!0});var rh=s(O);Y=i(rh,"DPRConfig"),rh.forEach(t),je=i(Hs," is the configuration class to store the configuration of a "),xe=r(Hs,"EM",{});var sh=s(xe);X=i(sh,"DPRModel"),sh.forEach(t),Me=i(Hs,"."),Hs.forEach(t),Qe=d(lo),W=r(lo,"P",{});var co=s(W);Le=i(co,"This is the configuration class to store the configuration of a "),Hn=r(co,"A",{href:!0});var ah=s(Hn);pi=i(ah,"DPRContextEncoder"),ah.forEach(t),hi=i(co,", "),Bn=r(co,"A",{href:!0});var ih=s(Bn);fi=i(ih,"DPRQuestionEncoder"),ih.forEach(t),ui=i(co,`, or a
`),Wn=r(co,"A",{href:!0});var lh=s(Wn);mi=i(lh,"DPRReader"),lh.forEach(t),gi=i(co,". It is used to instantiate the components of the DPR model."),co.forEach(t),_i=d(lo),Po=r(lo,"P",{});var wa=s(Po);vi=i(wa,"This class is a subclass of "),Un=r(wa,"A",{href:!0});var dh=s(Un);bi=i(dh,"BertConfig"),dh.forEach(t),Ti=i(wa,". Please check the superclass for the documentation of all kwargs."),wa.forEach(t),lo.forEach(t),Ks=d(o),st=r(o,"H2",{class:!0});var Ra=s(st);xt=r(Ra,"A",{id:!0,class:!0,href:!0});var ch=s(xt);Ar=r(ch,"SPAN",{});var ph=s(Ar);$(wo.$$.fragment,ph),ph.forEach(t),ch.forEach(t),ki=d(Ra),Or=r(Ra,"SPAN",{});var hh=s(Or);$i=i(hh,"DPRContextEncoderTokenizer"),hh.forEach(t),Ra.forEach(t),Ys=d(o),qe=r(o,"DIV",{class:!0});var po=s(qe);$(Ro.$$.fragment,po),Pi=d(po),Nr=r(po,"P",{});var fh=s(Nr);wi=i(fh,"Construct a DPRContextEncoder tokenizer."),fh.forEach(t),Ri=d(po),zt=r(po,"P",{});var Bs=s(zt);Vn=r(Bs,"A",{href:!0});var uh=s(Vn);Ei=i(uh,"DPRContextEncoderTokenizer"),uh.forEach(t),Di=i(Bs," is identical to "),Kn=r(Bs,"A",{href:!0});var mh=s(Kn);yi=i(mh,"BertTokenizer"),mh.forEach(t),xi=i(Bs,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Bs.forEach(t),zi=d(po),Eo=r(po,"P",{});var Ea=s(Eo);qi=i(Ea,"Refer to superclass "),Yn=r(Ea,"A",{href:!0});var gh=s(Yn);Fi=i(gh,"BertTokenizer"),gh.forEach(t),Ci=i(Ea," for usage examples and documentation concerning parameters."),Ea.forEach(t),po.forEach(t),Xs=d(o),at=r(o,"H2",{class:!0});var Da=s(at);qt=r(Da,"A",{id:!0,class:!0,href:!0});var _h=s(qt);jr=r(_h,"SPAN",{});var vh=s(jr);$(Do.$$.fragment,vh),vh.forEach(t),_h.forEach(t),Ai=d(Da),Mr=r(Da,"SPAN",{});var bh=s(Mr);Oi=i(bh,"DPRContextEncoderTokenizerFast"),bh.forEach(t),Da.forEach(t),Js=d(o),Fe=r(o,"DIV",{class:!0});var ho=s(Fe);$(yo.$$.fragment,ho),Ni=d(ho),xo=r(ho,"P",{});var ya=s(xo);ji=i(ya,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Qr=r(ya,"EM",{});var Th=s(Qr);Mi=i(Th,"tokenizers"),Th.forEach(t),Qi=i(ya," library)."),ya.forEach(t),Li=d(ho),Ft=r(ho,"P",{});var Ws=s(Ft);Xn=r(Ws,"A",{href:!0});var kh=s(Xn);Ii=i(kh,"DPRContextEncoderTokenizerFast"),kh.forEach(t),Si=i(Ws," is identical to "),Jn=r(Ws,"A",{href:!0});var $h=s(Jn);Hi=i($h,"BertTokenizerFast"),$h.forEach(t),Bi=i(Ws,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ws.forEach(t),Wi=d(ho),zo=r(ho,"P",{});var xa=s(zo);Ui=i(xa,"Refer to superclass "),Gn=r(xa,"A",{href:!0});var Ph=s(Gn);Vi=i(Ph,"BertTokenizerFast"),Ph.forEach(t),Ki=i(xa," for usage examples and documentation concerning parameters."),xa.forEach(t),ho.forEach(t),Gs=d(o),it=r(o,"H2",{class:!0});var za=s(it);Ct=r(za,"A",{id:!0,class:!0,href:!0});var wh=s(Ct);Lr=r(wh,"SPAN",{});var Rh=s(Lr);$(qo.$$.fragment,Rh),Rh.forEach(t),wh.forEach(t),Yi=d(za),Ir=r(za,"SPAN",{});var Eh=s(Ir);Xi=i(Eh,"DPRQuestionEncoderTokenizer"),Eh.forEach(t),za.forEach(t),Zs=d(o),Ce=r(o,"DIV",{class:!0});var fo=s(Ce);$(Fo.$$.fragment,fo),Ji=d(fo),Sr=r(fo,"P",{});var Dh=s(Sr);Gi=i(Dh,"Constructs a DPRQuestionEncoder tokenizer."),Dh.forEach(t),Zi=d(fo),At=r(fo,"P",{});var Us=s(At);Zn=r(Us,"A",{href:!0});var yh=s(Zn);el=i(yh,"DPRQuestionEncoderTokenizer"),yh.forEach(t),tl=i(Us," is identical to "),er=r(Us,"A",{href:!0});var xh=s(er);ol=i(xh,"BertTokenizer"),xh.forEach(t),nl=i(Us,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Us.forEach(t),rl=d(fo),Co=r(fo,"P",{});var qa=s(Co);sl=i(qa,"Refer to superclass "),tr=r(qa,"A",{href:!0});var zh=s(tr);al=i(zh,"BertTokenizer"),zh.forEach(t),il=i(qa," for usage examples and documentation concerning parameters."),qa.forEach(t),fo.forEach(t),ea=d(o),lt=r(o,"H2",{class:!0});var Fa=s(lt);Ot=r(Fa,"A",{id:!0,class:!0,href:!0});var qh=s(Ot);Hr=r(qh,"SPAN",{});var Fh=s(Hr);$(Ao.$$.fragment,Fh),Fh.forEach(t),qh.forEach(t),ll=d(Fa),Br=r(Fa,"SPAN",{});var Ch=s(Br);dl=i(Ch,"DPRQuestionEncoderTokenizerFast"),Ch.forEach(t),Fa.forEach(t),ta=d(o),Ae=r(o,"DIV",{class:!0});var uo=s(Ae);$(Oo.$$.fragment,uo),cl=d(uo),No=r(uo,"P",{});var Ca=s(No);pl=i(Ca,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Wr=r(Ca,"EM",{});var Ah=s(Wr);hl=i(Ah,"tokenizers"),Ah.forEach(t),fl=i(Ca," library)."),Ca.forEach(t),ul=d(uo),Nt=r(uo,"P",{});var Vs=s(Nt);or=r(Vs,"A",{href:!0});var Oh=s(or);ml=i(Oh,"DPRQuestionEncoderTokenizerFast"),Oh.forEach(t),gl=i(Vs," is identical to "),nr=r(Vs,"A",{href:!0});var Nh=s(nr);_l=i(Nh,"BertTokenizerFast"),Nh.forEach(t),vl=i(Vs,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Vs.forEach(t),bl=d(uo),jo=r(uo,"P",{});var Aa=s(jo);Tl=i(Aa,"Refer to superclass "),rr=r(Aa,"A",{href:!0});var jh=s(rr);kl=i(jh,"BertTokenizerFast"),jh.forEach(t),$l=i(Aa," for usage examples and documentation concerning parameters."),Aa.forEach(t),uo.forEach(t),oa=d(o),dt=r(o,"H2",{class:!0});var Oa=s(dt);jt=r(Oa,"A",{id:!0,class:!0,href:!0});var Mh=s(jt);Ur=r(Mh,"SPAN",{});var Qh=s(Ur);$(Mo.$$.fragment,Qh),Qh.forEach(t),Mh.forEach(t),Pl=d(Oa),Vr=r(Oa,"SPAN",{});var Lh=s(Vr);wl=i(Lh,"DPRReaderTokenizer"),Lh.forEach(t),Oa.forEach(t),na=d(o),ie=r(o,"DIV",{class:!0});var Ve=s(ie);$(Qo.$$.fragment,Ve),Rl=d(Ve),Kr=r(Ve,"P",{});var Ih=s(Kr);El=i(Ih,"Construct a DPRReader tokenizer."),Ih.forEach(t),Dl=d(Ve),et=r(Ve,"P",{});var Mn=s(et);sr=r(Mn,"A",{href:!0});var Sh=s(sr);yl=i(Sh,"DPRReaderTokenizer"),Sh.forEach(t),xl=i(Mn," is almost identical to "),ar=r(Mn,"A",{href:!0});var Hh=s(ar);zl=i(Hh,"BertTokenizer"),Hh.forEach(t),ql=i(Mn,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),ir=r(Mn,"A",{href:!0});var Bh=s(ir);Fl=i(Bh,"DPRReader"),Bh.forEach(t),Cl=i(Mn," model."),Mn.forEach(t),Al=d(Ve),Lo=r(Ve,"P",{});var Na=s(Lo);Ol=i(Na,"Refer to superclass "),lr=r(Na,"A",{href:!0});var Wh=s(lr);Nl=i(Wh,"BertTokenizer"),Wh.forEach(t),jl=i(Na," for usage examples and documentation concerning parameters."),Na.forEach(t),Ml=d(Ve),Ge=r(Ve,"P",{});var mo=s(Ge);Ql=i(mo,"Return a dictionary with the token ids of the input strings and other information to give to "),Yr=r(mo,"CODE",{});var Uh=s(Yr);Ll=i(Uh,".decode_best_spans"),Uh.forEach(t),Il=i(mo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Xr=r(mo,"CODE",{});var Vh=s(Xr);Sl=i(Vh,"input_ids"),Vh.forEach(t),Hl=i(mo," is a matrix of size "),Jr=r(mo,"CODE",{});var Kh=s(Jr);Bl=i(Kh,"(n_passages, sequence_length)"),Kh.forEach(t),Wl=i(mo,`
with the format:`),mo.forEach(t),Ul=d(Ve),$(Mt.$$.fragment,Ve),Ve.forEach(t),ra=d(o),ct=r(o,"H2",{class:!0});var ja=s(ct);Qt=r(ja,"A",{id:!0,class:!0,href:!0});var Yh=s(Qt);Gr=r(Yh,"SPAN",{});var Xh=s(Gr);$(Io.$$.fragment,Xh),Xh.forEach(t),Yh.forEach(t),Vl=d(ja),Zr=r(ja,"SPAN",{});var Jh=s(Zr);Kl=i(Jh,"DPRReaderTokenizerFast"),Jh.forEach(t),ja.forEach(t),sa=d(o),le=r(o,"DIV",{class:!0});var Ke=s(le);$(So.$$.fragment,Ke),Yl=d(Ke),Ho=r(Ke,"P",{});var Ma=s(Ho);Xl=i(Ma,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),es=r(Ma,"EM",{});var Gh=s(es);Jl=i(Gh,"tokenizers"),Gh.forEach(t),Gl=i(Ma," library)."),Ma.forEach(t),Zl=d(Ke),tt=r(Ke,"P",{});var Qn=s(tt);dr=r(Qn,"A",{href:!0});var Zh=s(dr);ed=i(Zh,"DPRReaderTokenizerFast"),Zh.forEach(t),td=i(Qn," is almost identical to "),cr=r(Qn,"A",{href:!0});var ef=s(cr);od=i(ef,"BertTokenizerFast"),ef.forEach(t),nd=i(Qn,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),pr=r(Qn,"A",{href:!0});var tf=s(pr);rd=i(tf,"DPRReader"),tf.forEach(t),sd=i(Qn," model."),Qn.forEach(t),ad=d(Ke),Bo=r(Ke,"P",{});var Qa=s(Bo);id=i(Qa,"Refer to superclass "),hr=r(Qa,"A",{href:!0});var of=s(hr);ld=i(of,"BertTokenizerFast"),of.forEach(t),dd=i(Qa," for usage examples and documentation concerning parameters."),Qa.forEach(t),cd=d(Ke),Ze=r(Ke,"P",{});var go=s(Ze);pd=i(go,"Return a dictionary with the token ids of the input strings and other information to give to "),ts=r(go,"CODE",{});var nf=s(ts);hd=i(nf,".decode_best_spans"),nf.forEach(t),fd=i(go,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),os=r(go,"CODE",{});var rf=s(os);ud=i(rf,"input_ids"),rf.forEach(t),md=i(go," is a matrix of size "),ns=r(go,"CODE",{});var sf=s(ns);gd=i(sf,"(n_passages, sequence_length)"),sf.forEach(t),_d=i(go,`
with the format:`),go.forEach(t),vd=d(Ke),rs=r(Ke,"P",{});var af=s(rs);bd=i(af,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),af.forEach(t),Ke.forEach(t),aa=d(o),pt=r(o,"H2",{class:!0});var La=s(pt);Lt=r(La,"A",{id:!0,class:!0,href:!0});var lf=s(Lt);ss=r(lf,"SPAN",{});var df=s(ss);$(Wo.$$.fragment,df),df.forEach(t),lf.forEach(t),Td=d(La),as=r(La,"SPAN",{});var cf=s(as);kd=i(cf,"DPR specific outputs"),cf.forEach(t),La.forEach(t),ia=d(o),ht=r(o,"DIV",{class:!0});var Ia=s(ht);$(Uo.$$.fragment,Ia),$d=d(Ia),Vo=r(Ia,"P",{});var Sa=s(Vo);Pd=i(Sa,"Class for outputs of "),fr=r(Sa,"A",{href:!0});var pf=s(fr);wd=i(pf,"DPRQuestionEncoder"),pf.forEach(t),Rd=i(Sa,"."),Sa.forEach(t),Ia.forEach(t),la=d(o),ft=r(o,"DIV",{class:!0});var Ha=s(ft);$(Ko.$$.fragment,Ha),Ed=d(Ha),Yo=r(Ha,"P",{});var Ba=s(Yo);Dd=i(Ba,"Class for outputs of "),ur=r(Ba,"A",{href:!0});var hf=s(ur);yd=i(hf,"DPRQuestionEncoder"),hf.forEach(t),xd=i(Ba,"."),Ba.forEach(t),Ha.forEach(t),da=d(o),ut=r(o,"DIV",{class:!0});var Wa=s(ut);$(Xo.$$.fragment,Wa),zd=d(Wa),Jo=r(Wa,"P",{});var Ua=s(Jo);qd=i(Ua,"Class for outputs of "),mr=r(Ua,"A",{href:!0});var ff=s(mr);Fd=i(ff,"DPRQuestionEncoder"),ff.forEach(t),Cd=i(Ua,"."),Ua.forEach(t),Wa.forEach(t),ca=d(o),mt=r(o,"H2",{class:!0});var Va=s(mt);It=r(Va,"A",{id:!0,class:!0,href:!0});var uf=s(It);is=r(uf,"SPAN",{});var mf=s(is);$(Go.$$.fragment,mf),mf.forEach(t),uf.forEach(t),Ad=d(Va),ls=r(Va,"SPAN",{});var gf=s(ls);Od=i(gf,"DPRContextEncoder"),gf.forEach(t),Va.forEach(t),pa=d(o),Ee=r(o,"DIV",{class:!0});var ot=s(Ee);$(Zo.$$.fragment,ot),Nd=d(ot),ds=r(ot,"P",{});var _f=s(ds);jd=i(_f,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),_f.forEach(t),Md=d(ot),en=r(ot,"P",{});var Ka=s(en);Qd=i(Ka,"This model inherits from "),gr=r(Ka,"A",{href:!0});var vf=s(gr);Ld=i(vf,"PreTrainedModel"),vf.forEach(t),Id=i(Ka,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ka.forEach(t),Sd=d(ot),tn=r(ot,"P",{});var Ya=s(tn);Hd=i(Ya,"This model is also a PyTorch "),on=r(Ya,"A",{href:!0,rel:!0});var bf=s(on);Bd=i(bf,"torch.nn.Module"),bf.forEach(t),Wd=i(Ya,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ya.forEach(t),Ud=d(ot),Ie=r(ot,"DIV",{class:!0});var _o=s(Ie);$(nn.$$.fragment,_o),Vd=d(_o),gt=r(_o,"P",{});var yr=s(gt);Kd=i(yr,"The "),_r=r(yr,"A",{href:!0});var Tf=s(_r);Yd=i(Tf,"DPRContextEncoder"),Tf.forEach(t),Xd=i(yr," forward method, overrides the "),cs=r(yr,"CODE",{});var kf=s(cs);Jd=i(kf,"__call__"),kf.forEach(t),Gd=i(yr," special method."),yr.forEach(t),Zd=d(_o),$(St.$$.fragment,_o),ec=d(_o),$(Ht.$$.fragment,_o),_o.forEach(t),ot.forEach(t),ha=d(o),_t=r(o,"H2",{class:!0});var Xa=s(_t);Bt=r(Xa,"A",{id:!0,class:!0,href:!0});var $f=s(Bt);ps=r($f,"SPAN",{});var Pf=s(ps);$(rn.$$.fragment,Pf),Pf.forEach(t),$f.forEach(t),tc=d(Xa),hs=r(Xa,"SPAN",{});var wf=s(hs);oc=i(wf,"DPRQuestionEncoder"),wf.forEach(t),Xa.forEach(t),fa=d(o),De=r(o,"DIV",{class:!0});var nt=s(De);$(sn.$$.fragment,nt),nc=d(nt),fs=r(nt,"P",{});var Rf=s(fs);rc=i(Rf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Rf.forEach(t),sc=d(nt),an=r(nt,"P",{});var Ja=s(an);ac=i(Ja,"This model inherits from "),vr=r(Ja,"A",{href:!0});var Ef=s(vr);ic=i(Ef,"PreTrainedModel"),Ef.forEach(t),lc=i(Ja,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ja.forEach(t),dc=d(nt),ln=r(nt,"P",{});var Ga=s(ln);cc=i(Ga,"This model is also a PyTorch "),dn=r(Ga,"A",{href:!0,rel:!0});var Df=s(dn);pc=i(Df,"torch.nn.Module"),Df.forEach(t),hc=i(Ga,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ga.forEach(t),fc=d(nt),Se=r(nt,"DIV",{class:!0});var vo=s(Se);$(cn.$$.fragment,vo),uc=d(vo),vt=r(vo,"P",{});var xr=s(vt);mc=i(xr,"The "),br=r(xr,"A",{href:!0});var yf=s(br);gc=i(yf,"DPRQuestionEncoder"),yf.forEach(t),_c=i(xr," forward method, overrides the "),us=r(xr,"CODE",{});var xf=s(us);vc=i(xf,"__call__"),xf.forEach(t),bc=i(xr," special method."),xr.forEach(t),Tc=d(vo),$(Wt.$$.fragment,vo),kc=d(vo),$(Ut.$$.fragment,vo),vo.forEach(t),nt.forEach(t),ua=d(o),bt=r(o,"H2",{class:!0});var Za=s(bt);Vt=r(Za,"A",{id:!0,class:!0,href:!0});var zf=s(Vt);ms=r(zf,"SPAN",{});var qf=s(ms);$(pn.$$.fragment,qf),qf.forEach(t),zf.forEach(t),$c=d(Za),gs=r(Za,"SPAN",{});var Ff=s(gs);Pc=i(Ff,"DPRReader"),Ff.forEach(t),Za.forEach(t),ma=d(o),ye=r(o,"DIV",{class:!0});var rt=s(ye);$(hn.$$.fragment,rt),wc=d(rt),_s=r(rt,"P",{});var Cf=s(_s);Rc=i(Cf,"The bare DPRReader transformer outputting span predictions."),Cf.forEach(t),Ec=d(rt),fn=r(rt,"P",{});var ei=s(fn);Dc=i(ei,"This model inherits from "),Tr=r(ei,"A",{href:!0});var Af=s(Tr);yc=i(Af,"PreTrainedModel"),Af.forEach(t),xc=i(ei,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ei.forEach(t),zc=d(rt),un=r(rt,"P",{});var ti=s(un);qc=i(ti,"This model is also a PyTorch "),mn=r(ti,"A",{href:!0,rel:!0});var Of=s(mn);Fc=i(Of,"torch.nn.Module"),Of.forEach(t),Cc=i(ti,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ti.forEach(t),Ac=d(rt),He=r(rt,"DIV",{class:!0});var bo=s(He);$(gn.$$.fragment,bo),Oc=d(bo),Tt=r(bo,"P",{});var zr=s(Tt);Nc=i(zr,"The "),kr=r(zr,"A",{href:!0});var Nf=s(kr);jc=i(Nf,"DPRReader"),Nf.forEach(t),Mc=i(zr," forward method, overrides the "),vs=r(zr,"CODE",{});var jf=s(vs);Qc=i(jf,"__call__"),jf.forEach(t),Lc=i(zr," special method."),zr.forEach(t),Ic=d(bo),$(Kt.$$.fragment,bo),Sc=d(bo),$(Yt.$$.fragment,bo),bo.forEach(t),rt.forEach(t),ga=d(o),kt=r(o,"H2",{class:!0});var oi=s(kt);Xt=r(oi,"A",{id:!0,class:!0,href:!0});var Mf=s(Xt);bs=r(Mf,"SPAN",{});var Qf=s(bs);$(_n.$$.fragment,Qf),Qf.forEach(t),Mf.forEach(t),Hc=d(oi),Ts=r(oi,"SPAN",{});var Lf=s(Ts);Bc=i(Lf,"TFDPRContextEncoder"),Lf.forEach(t),oi.forEach(t),_a=d(o),de=r(o,"DIV",{class:!0});var Ye=s(de);$(vn.$$.fragment,Ye),Wc=d(Ye),ks=r(Ye,"P",{});var If=s(ks);Uc=i(If,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),If.forEach(t),Vc=d(Ye),bn=r(Ye,"P",{});var ni=s(bn);Kc=i(ni,"This model inherits from "),$r=r(ni,"A",{href:!0});var Sf=s($r);Yc=i(Sf,"TFPreTrainedModel"),Sf.forEach(t),Xc=i(ni,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ni.forEach(t),Jc=d(Ye),Tn=r(Ye,"P",{});var ri=s(Tn);Gc=i(ri,"This model is also a Tensorflow "),kn=r(ri,"A",{href:!0,rel:!0});var Hf=s(kn);Zc=i(Hf,"tf.keras.Model"),Hf.forEach(t),ep=i(ri,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ri.forEach(t),tp=d(Ye),$(Jt.$$.fragment,Ye),op=d(Ye),Be=r(Ye,"DIV",{class:!0});var To=s(Be);$($n.$$.fragment,To),np=d(To),$t=r(To,"P",{});var qr=s($t);rp=i(qr,"The "),Pr=r(qr,"A",{href:!0});var Bf=s(Pr);sp=i(Bf,"TFDPRContextEncoder"),Bf.forEach(t),ap=i(qr," forward method, overrides the "),$s=r(qr,"CODE",{});var Wf=s($s);ip=i(Wf,"__call__"),Wf.forEach(t),lp=i(qr," special method."),qr.forEach(t),dp=d(To),$(Gt.$$.fragment,To),cp=d(To),$(Zt.$$.fragment,To),To.forEach(t),Ye.forEach(t),va=d(o),Pt=r(o,"H2",{class:!0});var si=s(Pt);eo=r(si,"A",{id:!0,class:!0,href:!0});var Uf=s(eo);Ps=r(Uf,"SPAN",{});var Vf=s(Ps);$(Pn.$$.fragment,Vf),Vf.forEach(t),Uf.forEach(t),pp=d(si),ws=r(si,"SPAN",{});var Kf=s(ws);hp=i(Kf,"TFDPRQuestionEncoder"),Kf.forEach(t),si.forEach(t),ba=d(o),ce=r(o,"DIV",{class:!0});var Xe=s(ce);$(wn.$$.fragment,Xe),fp=d(Xe),Rs=r(Xe,"P",{});var Yf=s(Rs);up=i(Yf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Yf.forEach(t),mp=d(Xe),Rn=r(Xe,"P",{});var ai=s(Rn);gp=i(ai,"This model inherits from "),wr=r(ai,"A",{href:!0});var Xf=s(wr);_p=i(Xf,"TFPreTrainedModel"),Xf.forEach(t),vp=i(ai,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ai.forEach(t),bp=d(Xe),En=r(Xe,"P",{});var ii=s(En);Tp=i(ii,"This model is also a Tensorflow "),Dn=r(ii,"A",{href:!0,rel:!0});var Jf=s(Dn);kp=i(Jf,"tf.keras.Model"),Jf.forEach(t),$p=i(ii,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ii.forEach(t),Pp=d(Xe),$(to.$$.fragment,Xe),wp=d(Xe),We=r(Xe,"DIV",{class:!0});var ko=s(We);$(yn.$$.fragment,ko),Rp=d(ko),wt=r(ko,"P",{});var Fr=s(wt);Ep=i(Fr,"The "),Rr=r(Fr,"A",{href:!0});var Gf=s(Rr);Dp=i(Gf,"TFDPRQuestionEncoder"),Gf.forEach(t),yp=i(Fr," forward method, overrides the "),Es=r(Fr,"CODE",{});var Zf=s(Es);xp=i(Zf,"__call__"),Zf.forEach(t),zp=i(Fr," special method."),Fr.forEach(t),qp=d(ko),$(oo.$$.fragment,ko),Fp=d(ko),$(no.$$.fragment,ko),ko.forEach(t),Xe.forEach(t),Ta=d(o),Rt=r(o,"H2",{class:!0});var li=s(Rt);ro=r(li,"A",{id:!0,class:!0,href:!0});var eu=s(ro);Ds=r(eu,"SPAN",{});var tu=s(Ds);$(xn.$$.fragment,tu),tu.forEach(t),eu.forEach(t),Cp=d(li),ys=r(li,"SPAN",{});var ou=s(ys);Ap=i(ou,"TFDPRReader"),ou.forEach(t),li.forEach(t),ka=d(o),pe=r(o,"DIV",{class:!0});var Je=s(pe);$(zn.$$.fragment,Je),Op=d(Je),xs=r(Je,"P",{});var nu=s(xs);Np=i(nu,"The bare DPRReader transformer outputting span predictions."),nu.forEach(t),jp=d(Je),qn=r(Je,"P",{});var di=s(qn);Mp=i(di,"This model inherits from "),Er=r(di,"A",{href:!0});var ru=s(Er);Qp=i(ru,"TFPreTrainedModel"),ru.forEach(t),Lp=i(di,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),di.forEach(t),Ip=d(Je),Fn=r(Je,"P",{});var ci=s(Fn);Sp=i(ci,"This model is also a Tensorflow "),Cn=r(ci,"A",{href:!0,rel:!0});var su=s(Cn);Hp=i(su,"tf.keras.Model"),su.forEach(t),Bp=i(ci,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ci.forEach(t),Wp=d(Je),$(so.$$.fragment,Je),Up=d(Je),Ue=r(Je,"DIV",{class:!0});var $o=s(Ue);$(An.$$.fragment,$o),Vp=d($o),Et=r($o,"P",{});var Cr=s(Et);Kp=i(Cr,"The "),Dr=r(Cr,"A",{href:!0});var au=s(Dr);Yp=i(au,"TFDPRReader"),au.forEach(t),Xp=i(Cr," forward method, overrides the "),zs=r(Cr,"CODE",{});var iu=s(zs);Jp=i(iu,"__call__"),iu.forEach(t),Gp=i(Cr," special method."),Cr.forEach(t),Zp=d($o),$(ao.$$.fragment,$o),eh=d($o),$(io.$$.fragment,$o),$o.forEach(t),Je.forEach(t),this.h()},h(){p(h,"name","hf:doc:metadata"),p(h,"content",JSON.stringify(zu)),p(u,"id","dpr"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#dpr"),p(g,"class","relative group"),p(J,"id","overview"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#overview"),p(D,"class","relative group"),p(te,"href","https://arxiv.org/abs/2004.04906"),p(te,"rel","nofollow"),p(j,"href","https://huggingface.co/lhoestq"),p(j,"rel","nofollow"),p(M,"href","https://github.com/facebookresearch/DPR"),p(M,"rel","nofollow"),p(N,"id","transformers.DPRConfig"),p(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(N,"href","#transformers.DPRConfig"),p(K,"class","relative group"),p(O,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRConfig"),p(Hn,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRContextEncoder"),p(Bn,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Wn,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReader"),p(Un,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertConfig"),p(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(xt,"id","transformers.DPRContextEncoderTokenizer"),p(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(xt,"href","#transformers.DPRContextEncoderTokenizer"),p(st,"class","relative group"),p(Vn,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),p(Kn,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizer"),p(Yn,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizer"),p(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(qt,"id","transformers.DPRContextEncoderTokenizerFast"),p(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qt,"href","#transformers.DPRContextEncoderTokenizerFast"),p(at,"class","relative group"),p(Xn,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),p(Jn,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizerFast"),p(Gn,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizerFast"),p(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ct,"id","transformers.DPRQuestionEncoderTokenizer"),p(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ct,"href","#transformers.DPRQuestionEncoderTokenizer"),p(it,"class","relative group"),p(Zn,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),p(er,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizer"),p(tr,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizer"),p(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ot,"id","transformers.DPRQuestionEncoderTokenizerFast"),p(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ot,"href","#transformers.DPRQuestionEncoderTokenizerFast"),p(lt,"class","relative group"),p(or,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),p(nr,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizerFast"),p(rr,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizerFast"),p(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(jt,"id","transformers.DPRReaderTokenizer"),p(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(jt,"href","#transformers.DPRReaderTokenizer"),p(dt,"class","relative group"),p(sr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReaderTokenizer"),p(ar,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizer"),p(ir,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReader"),p(lr,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizer"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Qt,"id","transformers.DPRReaderTokenizerFast"),p(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Qt,"href","#transformers.DPRReaderTokenizerFast"),p(ct,"class","relative group"),p(dr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),p(cr,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizerFast"),p(pr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReader"),p(hr,"href","/docs/transformers/pr_highlight/en/model_doc/bert#transformers.BertTokenizerFast"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Lt,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Lt,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(pt,"class","relative group"),p(fr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ur,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(mr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(It,"id","transformers.DPRContextEncoder"),p(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(It,"href","#transformers.DPRContextEncoder"),p(mt,"class","relative group"),p(gr,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),p(on,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(on,"rel","nofollow"),p(_r,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRContextEncoder"),p(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Bt,"id","transformers.DPRQuestionEncoder"),p(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Bt,"href","#transformers.DPRQuestionEncoder"),p(_t,"class","relative group"),p(vr,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),p(dn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(dn,"rel","nofollow"),p(br,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Vt,"id","transformers.DPRReader"),p(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Vt,"href","#transformers.DPRReader"),p(bt,"class","relative group"),p(Tr,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel"),p(mn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(mn,"rel","nofollow"),p(kr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.DPRReader"),p(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Xt,"id","transformers.TFDPRContextEncoder"),p(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Xt,"href","#transformers.TFDPRContextEncoder"),p(kt,"class","relative group"),p($r,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel"),p(kn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(kn,"rel","nofollow"),p(Pr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.TFDPRContextEncoder"),p(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(eo,"id","transformers.TFDPRQuestionEncoder"),p(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(eo,"href","#transformers.TFDPRQuestionEncoder"),p(Pt,"class","relative group"),p(wr,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel"),p(Dn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Dn,"rel","nofollow"),p(Rr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),p(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ro,"id","transformers.TFDPRReader"),p(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ro,"href","#transformers.TFDPRReader"),p(Rt,"class","relative group"),p(Er,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.TFPreTrainedModel"),p(Cn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Cn,"rel","nofollow"),p(Dr,"href","/docs/transformers/pr_highlight/en/model_doc/dpr#transformers.TFDPRReader"),p(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),f(o,T,m),f(o,g,m),e(g,u),e(u,b),P(c,b,null),e(g,_),e(g,x),e(x,_e),f(o,Z,m),f(o,D,m),e(D,J),e(J,I),P(ee,I,null),e(D,ve),e(D,S),e(S,be),f(o,he,m),f(o,H,m),e(H,L),e(H,te),e(te,oe),e(H,q),f(o,A,m),f(o,ne,m),e(ne,V),f(o,fe,m),f(o,re,m),e(re,B),e(B,Te),f(o,ue,m),f(o,z,m),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(z,Re),f(o,Q,m),f(o,K,m),e(K,N),e(N,se),P(v,se,null),e(K,y),e(K,G),e(G,Oe),f(o,ze,m),f(o,C,m),P(me,C,null),e(C,Ne),e(C,ae),e(ae,O),e(O,Y),e(ae,je),e(ae,xe),e(xe,X),e(ae,Me),e(C,Qe),e(C,W),e(W,Le),e(W,Hn),e(Hn,pi),e(W,hi),e(W,Bn),e(Bn,fi),e(W,ui),e(W,Wn),e(Wn,mi),e(W,gi),e(C,_i),e(C,Po),e(Po,vi),e(Po,Un),e(Un,bi),e(Po,Ti),f(o,Ks,m),f(o,st,m),e(st,xt),e(xt,Ar),P(wo,Ar,null),e(st,ki),e(st,Or),e(Or,$i),f(o,Ys,m),f(o,qe,m),P(Ro,qe,null),e(qe,Pi),e(qe,Nr),e(Nr,wi),e(qe,Ri),e(qe,zt),e(zt,Vn),e(Vn,Ei),e(zt,Di),e(zt,Kn),e(Kn,yi),e(zt,xi),e(qe,zi),e(qe,Eo),e(Eo,qi),e(Eo,Yn),e(Yn,Fi),e(Eo,Ci),f(o,Xs,m),f(o,at,m),e(at,qt),e(qt,jr),P(Do,jr,null),e(at,Ai),e(at,Mr),e(Mr,Oi),f(o,Js,m),f(o,Fe,m),P(yo,Fe,null),e(Fe,Ni),e(Fe,xo),e(xo,ji),e(xo,Qr),e(Qr,Mi),e(xo,Qi),e(Fe,Li),e(Fe,Ft),e(Ft,Xn),e(Xn,Ii),e(Ft,Si),e(Ft,Jn),e(Jn,Hi),e(Ft,Bi),e(Fe,Wi),e(Fe,zo),e(zo,Ui),e(zo,Gn),e(Gn,Vi),e(zo,Ki),f(o,Gs,m),f(o,it,m),e(it,Ct),e(Ct,Lr),P(qo,Lr,null),e(it,Yi),e(it,Ir),e(Ir,Xi),f(o,Zs,m),f(o,Ce,m),P(Fo,Ce,null),e(Ce,Ji),e(Ce,Sr),e(Sr,Gi),e(Ce,Zi),e(Ce,At),e(At,Zn),e(Zn,el),e(At,tl),e(At,er),e(er,ol),e(At,nl),e(Ce,rl),e(Ce,Co),e(Co,sl),e(Co,tr),e(tr,al),e(Co,il),f(o,ea,m),f(o,lt,m),e(lt,Ot),e(Ot,Hr),P(Ao,Hr,null),e(lt,ll),e(lt,Br),e(Br,dl),f(o,ta,m),f(o,Ae,m),P(Oo,Ae,null),e(Ae,cl),e(Ae,No),e(No,pl),e(No,Wr),e(Wr,hl),e(No,fl),e(Ae,ul),e(Ae,Nt),e(Nt,or),e(or,ml),e(Nt,gl),e(Nt,nr),e(nr,_l),e(Nt,vl),e(Ae,bl),e(Ae,jo),e(jo,Tl),e(jo,rr),e(rr,kl),e(jo,$l),f(o,oa,m),f(o,dt,m),e(dt,jt),e(jt,Ur),P(Mo,Ur,null),e(dt,Pl),e(dt,Vr),e(Vr,wl),f(o,na,m),f(o,ie,m),P(Qo,ie,null),e(ie,Rl),e(ie,Kr),e(Kr,El),e(ie,Dl),e(ie,et),e(et,sr),e(sr,yl),e(et,xl),e(et,ar),e(ar,zl),e(et,ql),e(et,ir),e(ir,Fl),e(et,Cl),e(ie,Al),e(ie,Lo),e(Lo,Ol),e(Lo,lr),e(lr,Nl),e(Lo,jl),e(ie,Ml),e(ie,Ge),e(Ge,Ql),e(Ge,Yr),e(Yr,Ll),e(Ge,Il),e(Ge,Xr),e(Xr,Sl),e(Ge,Hl),e(Ge,Jr),e(Jr,Bl),e(Ge,Wl),e(ie,Ul),P(Mt,ie,null),f(o,ra,m),f(o,ct,m),e(ct,Qt),e(Qt,Gr),P(Io,Gr,null),e(ct,Vl),e(ct,Zr),e(Zr,Kl),f(o,sa,m),f(o,le,m),P(So,le,null),e(le,Yl),e(le,Ho),e(Ho,Xl),e(Ho,es),e(es,Jl),e(Ho,Gl),e(le,Zl),e(le,tt),e(tt,dr),e(dr,ed),e(tt,td),e(tt,cr),e(cr,od),e(tt,nd),e(tt,pr),e(pr,rd),e(tt,sd),e(le,ad),e(le,Bo),e(Bo,id),e(Bo,hr),e(hr,ld),e(Bo,dd),e(le,cd),e(le,Ze),e(Ze,pd),e(Ze,ts),e(ts,hd),e(Ze,fd),e(Ze,os),e(os,ud),e(Ze,md),e(Ze,ns),e(ns,gd),e(Ze,_d),e(le,vd),e(le,rs),e(rs,bd),f(o,aa,m),f(o,pt,m),e(pt,Lt),e(Lt,ss),P(Wo,ss,null),e(pt,Td),e(pt,as),e(as,kd),f(o,ia,m),f(o,ht,m),P(Uo,ht,null),e(ht,$d),e(ht,Vo),e(Vo,Pd),e(Vo,fr),e(fr,wd),e(Vo,Rd),f(o,la,m),f(o,ft,m),P(Ko,ft,null),e(ft,Ed),e(ft,Yo),e(Yo,Dd),e(Yo,ur),e(ur,yd),e(Yo,xd),f(o,da,m),f(o,ut,m),P(Xo,ut,null),e(ut,zd),e(ut,Jo),e(Jo,qd),e(Jo,mr),e(mr,Fd),e(Jo,Cd),f(o,ca,m),f(o,mt,m),e(mt,It),e(It,is),P(Go,is,null),e(mt,Ad),e(mt,ls),e(ls,Od),f(o,pa,m),f(o,Ee,m),P(Zo,Ee,null),e(Ee,Nd),e(Ee,ds),e(ds,jd),e(Ee,Md),e(Ee,en),e(en,Qd),e(en,gr),e(gr,Ld),e(en,Id),e(Ee,Sd),e(Ee,tn),e(tn,Hd),e(tn,on),e(on,Bd),e(tn,Wd),e(Ee,Ud),e(Ee,Ie),P(nn,Ie,null),e(Ie,Vd),e(Ie,gt),e(gt,Kd),e(gt,_r),e(_r,Yd),e(gt,Xd),e(gt,cs),e(cs,Jd),e(gt,Gd),e(Ie,Zd),P(St,Ie,null),e(Ie,ec),P(Ht,Ie,null),f(o,ha,m),f(o,_t,m),e(_t,Bt),e(Bt,ps),P(rn,ps,null),e(_t,tc),e(_t,hs),e(hs,oc),f(o,fa,m),f(o,De,m),P(sn,De,null),e(De,nc),e(De,fs),e(fs,rc),e(De,sc),e(De,an),e(an,ac),e(an,vr),e(vr,ic),e(an,lc),e(De,dc),e(De,ln),e(ln,cc),e(ln,dn),e(dn,pc),e(ln,hc),e(De,fc),e(De,Se),P(cn,Se,null),e(Se,uc),e(Se,vt),e(vt,mc),e(vt,br),e(br,gc),e(vt,_c),e(vt,us),e(us,vc),e(vt,bc),e(Se,Tc),P(Wt,Se,null),e(Se,kc),P(Ut,Se,null),f(o,ua,m),f(o,bt,m),e(bt,Vt),e(Vt,ms),P(pn,ms,null),e(bt,$c),e(bt,gs),e(gs,Pc),f(o,ma,m),f(o,ye,m),P(hn,ye,null),e(ye,wc),e(ye,_s),e(_s,Rc),e(ye,Ec),e(ye,fn),e(fn,Dc),e(fn,Tr),e(Tr,yc),e(fn,xc),e(ye,zc),e(ye,un),e(un,qc),e(un,mn),e(mn,Fc),e(un,Cc),e(ye,Ac),e(ye,He),P(gn,He,null),e(He,Oc),e(He,Tt),e(Tt,Nc),e(Tt,kr),e(kr,jc),e(Tt,Mc),e(Tt,vs),e(vs,Qc),e(Tt,Lc),e(He,Ic),P(Kt,He,null),e(He,Sc),P(Yt,He,null),f(o,ga,m),f(o,kt,m),e(kt,Xt),e(Xt,bs),P(_n,bs,null),e(kt,Hc),e(kt,Ts),e(Ts,Bc),f(o,_a,m),f(o,de,m),P(vn,de,null),e(de,Wc),e(de,ks),e(ks,Uc),e(de,Vc),e(de,bn),e(bn,Kc),e(bn,$r),e($r,Yc),e(bn,Xc),e(de,Jc),e(de,Tn),e(Tn,Gc),e(Tn,kn),e(kn,Zc),e(Tn,ep),e(de,tp),P(Jt,de,null),e(de,op),e(de,Be),P($n,Be,null),e(Be,np),e(Be,$t),e($t,rp),e($t,Pr),e(Pr,sp),e($t,ap),e($t,$s),e($s,ip),e($t,lp),e(Be,dp),P(Gt,Be,null),e(Be,cp),P(Zt,Be,null),f(o,va,m),f(o,Pt,m),e(Pt,eo),e(eo,Ps),P(Pn,Ps,null),e(Pt,pp),e(Pt,ws),e(ws,hp),f(o,ba,m),f(o,ce,m),P(wn,ce,null),e(ce,fp),e(ce,Rs),e(Rs,up),e(ce,mp),e(ce,Rn),e(Rn,gp),e(Rn,wr),e(wr,_p),e(Rn,vp),e(ce,bp),e(ce,En),e(En,Tp),e(En,Dn),e(Dn,kp),e(En,$p),e(ce,Pp),P(to,ce,null),e(ce,wp),e(ce,We),P(yn,We,null),e(We,Rp),e(We,wt),e(wt,Ep),e(wt,Rr),e(Rr,Dp),e(wt,yp),e(wt,Es),e(Es,xp),e(wt,zp),e(We,qp),P(oo,We,null),e(We,Fp),P(no,We,null),f(o,Ta,m),f(o,Rt,m),e(Rt,ro),e(ro,Ds),P(xn,Ds,null),e(Rt,Cp),e(Rt,ys),e(ys,Ap),f(o,ka,m),f(o,pe,m),P(zn,pe,null),e(pe,Op),e(pe,xs),e(xs,Np),e(pe,jp),e(pe,qn),e(qn,Mp),e(qn,Er),e(Er,Qp),e(qn,Lp),e(pe,Ip),e(pe,Fn),e(Fn,Sp),e(Fn,Cn),e(Cn,Hp),e(Fn,Bp),e(pe,Wp),P(so,pe,null),e(pe,Up),e(pe,Ue),P(An,Ue,null),e(Ue,Vp),e(Ue,Et),e(Et,Kp),e(Et,Dr),e(Dr,Yp),e(Et,Xp),e(Et,zs),e(zs,Jp),e(Et,Gp),e(Ue,Zp),P(ao,Ue,null),e(Ue,eh),P(io,Ue,null),$a=!0},p(o,[m]){const On={};m&2&&(On.$$scope={dirty:m,ctx:o}),Mt.$set(On);const qs={};m&2&&(qs.$$scope={dirty:m,ctx:o}),St.$set(qs);const Fs={};m&2&&(Fs.$$scope={dirty:m,ctx:o}),Ht.$set(Fs);const Cs={};m&2&&(Cs.$$scope={dirty:m,ctx:o}),Wt.$set(Cs);const Nn={};m&2&&(Nn.$$scope={dirty:m,ctx:o}),Ut.$set(Nn);const As={};m&2&&(As.$$scope={dirty:m,ctx:o}),Kt.$set(As);const Os={};m&2&&(Os.$$scope={dirty:m,ctx:o}),Yt.$set(Os);const Ns={};m&2&&(Ns.$$scope={dirty:m,ctx:o}),Jt.$set(Ns);const jn={};m&2&&(jn.$$scope={dirty:m,ctx:o}),Gt.$set(jn);const js={};m&2&&(js.$$scope={dirty:m,ctx:o}),Zt.$set(js);const Ms={};m&2&&(Ms.$$scope={dirty:m,ctx:o}),to.$set(Ms);const Qs={};m&2&&(Qs.$$scope={dirty:m,ctx:o}),oo.$set(Qs);const Ls={};m&2&&(Ls.$$scope={dirty:m,ctx:o}),no.$set(Ls);const Dt={};m&2&&(Dt.$$scope={dirty:m,ctx:o}),so.$set(Dt);const Is={};m&2&&(Is.$$scope={dirty:m,ctx:o}),ao.$set(Is);const Ss={};m&2&&(Ss.$$scope={dirty:m,ctx:o}),io.$set(Ss)},i(o){$a||(w(c.$$.fragment,o),w(ee.$$.fragment,o),w(v.$$.fragment,o),w(me.$$.fragment,o),w(wo.$$.fragment,o),w(Ro.$$.fragment,o),w(Do.$$.fragment,o),w(yo.$$.fragment,o),w(qo.$$.fragment,o),w(Fo.$$.fragment,o),w(Ao.$$.fragment,o),w(Oo.$$.fragment,o),w(Mo.$$.fragment,o),w(Qo.$$.fragment,o),w(Mt.$$.fragment,o),w(Io.$$.fragment,o),w(So.$$.fragment,o),w(Wo.$$.fragment,o),w(Uo.$$.fragment,o),w(Ko.$$.fragment,o),w(Xo.$$.fragment,o),w(Go.$$.fragment,o),w(Zo.$$.fragment,o),w(nn.$$.fragment,o),w(St.$$.fragment,o),w(Ht.$$.fragment,o),w(rn.$$.fragment,o),w(sn.$$.fragment,o),w(cn.$$.fragment,o),w(Wt.$$.fragment,o),w(Ut.$$.fragment,o),w(pn.$$.fragment,o),w(hn.$$.fragment,o),w(gn.$$.fragment,o),w(Kt.$$.fragment,o),w(Yt.$$.fragment,o),w(_n.$$.fragment,o),w(vn.$$.fragment,o),w(Jt.$$.fragment,o),w($n.$$.fragment,o),w(Gt.$$.fragment,o),w(Zt.$$.fragment,o),w(Pn.$$.fragment,o),w(wn.$$.fragment,o),w(to.$$.fragment,o),w(yn.$$.fragment,o),w(oo.$$.fragment,o),w(no.$$.fragment,o),w(xn.$$.fragment,o),w(zn.$$.fragment,o),w(so.$$.fragment,o),w(An.$$.fragment,o),w(ao.$$.fragment,o),w(io.$$.fragment,o),$a=!0)},o(o){R(c.$$.fragment,o),R(ee.$$.fragment,o),R(v.$$.fragment,o),R(me.$$.fragment,o),R(wo.$$.fragment,o),R(Ro.$$.fragment,o),R(Do.$$.fragment,o),R(yo.$$.fragment,o),R(qo.$$.fragment,o),R(Fo.$$.fragment,o),R(Ao.$$.fragment,o),R(Oo.$$.fragment,o),R(Mo.$$.fragment,o),R(Qo.$$.fragment,o),R(Mt.$$.fragment,o),R(Io.$$.fragment,o),R(So.$$.fragment,o),R(Wo.$$.fragment,o),R(Uo.$$.fragment,o),R(Ko.$$.fragment,o),R(Xo.$$.fragment,o),R(Go.$$.fragment,o),R(Zo.$$.fragment,o),R(nn.$$.fragment,o),R(St.$$.fragment,o),R(Ht.$$.fragment,o),R(rn.$$.fragment,o),R(sn.$$.fragment,o),R(cn.$$.fragment,o),R(Wt.$$.fragment,o),R(Ut.$$.fragment,o),R(pn.$$.fragment,o),R(hn.$$.fragment,o),R(gn.$$.fragment,o),R(Kt.$$.fragment,o),R(Yt.$$.fragment,o),R(_n.$$.fragment,o),R(vn.$$.fragment,o),R(Jt.$$.fragment,o),R($n.$$.fragment,o),R(Gt.$$.fragment,o),R(Zt.$$.fragment,o),R(Pn.$$.fragment,o),R(wn.$$.fragment,o),R(to.$$.fragment,o),R(yn.$$.fragment,o),R(oo.$$.fragment,o),R(no.$$.fragment,o),R(xn.$$.fragment,o),R(zn.$$.fragment,o),R(so.$$.fragment,o),R(An.$$.fragment,o),R(ao.$$.fragment,o),R(io.$$.fragment,o),$a=!1},d(o){t(h),o&&t(T),o&&t(g),E(c),o&&t(Z),o&&t(D),E(ee),o&&t(he),o&&t(H),o&&t(A),o&&t(ne),o&&t(fe),o&&t(re),o&&t(ue),o&&t(z),o&&t(Q),o&&t(K),E(v),o&&t(ze),o&&t(C),E(me),o&&t(Ks),o&&t(st),E(wo),o&&t(Ys),o&&t(qe),E(Ro),o&&t(Xs),o&&t(at),E(Do),o&&t(Js),o&&t(Fe),E(yo),o&&t(Gs),o&&t(it),E(qo),o&&t(Zs),o&&t(Ce),E(Fo),o&&t(ea),o&&t(lt),E(Ao),o&&t(ta),o&&t(Ae),E(Oo),o&&t(oa),o&&t(dt),E(Mo),o&&t(na),o&&t(ie),E(Qo),E(Mt),o&&t(ra),o&&t(ct),E(Io),o&&t(sa),o&&t(le),E(So),o&&t(aa),o&&t(pt),E(Wo),o&&t(ia),o&&t(ht),E(Uo),o&&t(la),o&&t(ft),E(Ko),o&&t(da),o&&t(ut),E(Xo),o&&t(ca),o&&t(mt),E(Go),o&&t(pa),o&&t(Ee),E(Zo),E(nn),E(St),E(Ht),o&&t(ha),o&&t(_t),E(rn),o&&t(fa),o&&t(De),E(sn),E(cn),E(Wt),E(Ut),o&&t(ua),o&&t(bt),E(pn),o&&t(ma),o&&t(ye),E(hn),E(gn),E(Kt),E(Yt),o&&t(ga),o&&t(kt),E(_n),o&&t(_a),o&&t(de),E(vn),E(Jt),E($n),E(Gt),E(Zt),o&&t(va),o&&t(Pt),E(Pn),o&&t(ba),o&&t(ce),E(wn),E(to),E(yn),E(oo),E(no),o&&t(Ta),o&&t(Rt),E(xn),o&&t(ka),o&&t(pe),E(zn),E(so),E(An),E(ao),E(io)}}}const zu={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function qu(F){return hu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mu extends lu{constructor(h){super();du(this,h,qu,xu,cu,{})}}export{Mu as default,zu as metadata};
