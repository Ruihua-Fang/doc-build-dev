import{S as ji,i as Mi,s as Ni,e as r,k as h,w as u,t as n,M as Hi,c as l,d as o,m as p,a,x as v,h as s,b as m,N as Di,F as t,g as f,y as _,q as $,o as g,B as w,v as qi}from"../chunks/vendor-6b77c823.js";import{T as bl}from"../chunks/Tip-39098574.js";import{I as $e}from"../chunks/IconCopyLink-7a11ce68.js";import{C as k}from"../chunks/CodeBlock-3a8b25a8.js";function Li(N){let d,P,c,b,T;return{c(){d=r("p"),P=n("You must keep the "),c=r("code"),b=n("transformers"),T=n(" folder if you want to keep using the library.")},l(y){d=l(y,"P",{});var E=a(d);P=s(E,"You must keep the "),c=l(E,"CODE",{});var A=a(c);b=s(A,"transformers"),A.forEach(o),T=s(E," folder if you want to keep using the library."),E.forEach(o)},m(y,E){f(y,d,E),t(d,P),t(d,c),t(c,b),t(d,T)},d(y){y&&o(d)}}}function zi(N){let d,P,c,b,T,y,E,A,F,S,x;return{c(){d=r("p"),P=n("\u{1F917} Transformers will use the shell environment variables "),c=r("code"),b=n("PYTORCH_TRANSFORMERS_CACHE"),T=n(" or "),y=r("code"),E=n("PYTORCH_PRETRAINED_BERT_CACHE"),A=n(" if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable "),F=r("code"),S=n("TRANSFORMERS_CACHE"),x=n(".")},l(R){d=l(R,"P",{});var O=a(d);P=s(O,"\u{1F917} Transformers will use the shell environment variables "),c=l(O,"CODE",{});var U=a(c);b=s(U,"PYTORCH_TRANSFORMERS_CACHE"),U.forEach(o),T=s(O," or "),y=l(O,"CODE",{});var Et=a(y);E=s(Et,"PYTORCH_PRETRAINED_BERT_CACHE"),Et.forEach(o),A=s(O," if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable "),F=l(O,"CODE",{});var ge=a(F);S=s(ge,"TRANSFORMERS_CACHE"),ge.forEach(o),x=s(O,"."),O.forEach(o)},m(R,O){f(R,d,O),t(d,P),t(d,c),t(c,b),t(d,T),t(d,y),t(y,E),t(d,A),t(d,F),t(F,S),t(d,x)},d(R){R&&o(d)}}}function Ui(N){let d,P,c,b,T,y,E,A;return{c(){d=r("p"),P=n("Add "),c=r("a"),b=n("\u{1F917} Datasets"),T=n(" to your offline training workflow by setting the environment variable "),y=r("code"),E=n("HF_DATASETS_OFFLINE=1"),A=n("."),this.h()},l(F){d=l(F,"P",{});var S=a(d);P=s(S,"Add "),c=l(S,"A",{href:!0,rel:!0});var x=a(c);b=s(x,"\u{1F917} Datasets"),x.forEach(o),T=s(S," to your offline training workflow by setting the environment variable "),y=l(S,"CODE",{});var R=a(y);E=s(R,"HF_DATASETS_OFFLINE=1"),R.forEach(o),A=s(S,"."),S.forEach(o),this.h()},h(){m(c,"href","https://huggingface.co/docs/datasets/"),m(c,"rel","nofollow")},m(F,S){f(F,d,S),t(d,P),t(d,c),t(c,b),t(d,T),t(d,y),t(y,E),t(d,A)},d(F){F&&o(d)}}}function Yi(N){let d,P,c,b,T;return{c(){d=r("p"),P=n("See the "),c=r("a"),b=n("How to download files from the Hub"),T=n(" section for more details on downloading files stored on the Hub."),this.h()},l(y){d=l(y,"P",{});var E=a(d);P=s(E,"See the "),c=l(E,"A",{href:!0,rel:!0});var A=a(c);b=s(A,"How to download files from the Hub"),A.forEach(o),T=s(E," section for more details on downloading files stored on the Hub."),E.forEach(o),this.h()},h(){m(c,"href","https://huggingface.co/docs/hub/how-to-downstream"),m(c,"rel","nofollow")},m(y,E){f(y,d,E),t(d,P),t(d,c),t(c,b),t(d,T)},d(y){y&&o(d)}}}function Bi(N){let d,P,c,b,T,y,E,A,F,S,x,R,O,U,Et,ge,H,bt,we,Tl,kl,Pl,Tt,ye,Al,Cl,Sl,kt,Ee,Fl,Ol,Go,Y,te,ao,be,Il,no,xl,Vo,D,Rl,Te,jl,Ml,ke,Nl,Hl,Xo,Pt,Dl,Jo,Pe,Ko,At,ql,Qo,Ae,Zo,Ct,Ll,er,Ce,tr,St,zl,or,Se,rr,Ft,Ul,lr,Fe,ar,Ot,Yl,nr,Oe,sr,It,Bl,ir,Ie,fr,xt,Wl,hr,xe,pr,B,oe,so,Re,Gl,io,Vl,mr,Rt,Xl,dr,je,cr,C,Jl,fo,Kl,Ql,ho,Zl,ea,po,ta,oa,mo,ra,la,co,aa,na,Me,sa,ia,ur,jt,fa,vr,Ne,_r,W,re,uo,He,ha,vo,pa,$r,Mt,ma,gr,le,De,da,_o,ca,ua,va,$o,_a,wr,Nt,$a,yr,qe,Er,q,ga,go,wa,ya,wo,Ea,ba,br,ae,Tr,Ht,Ta,kr,Le,Pr,ne,ka,yo,Pa,Aa,Ar,G,se,Eo,ze,Ca,bo,Sa,Cr,ie,Fa,To,Oa,Ia,Sr,Ue,Fr,V,fe,ko,Ye,xa,Po,Ra,Or,j,ja,Ao,Ma,Na,Co,Ha,Da,So,qa,La,Ir,L,Be,za,Fo,Ua,Ya,Ba,X,Wa,Oo,Ga,Va,Io,Xa,Ja,Ka,J,Qa,xo,Za,en,Ro,tn,on,xr,he,Rr,K,pe,jo,We,rn,Mo,ln,jr,me,an,No,nn,sn,Mr,de,Nr,Dt,fn,Hr,Ge,Dr,qt,hn,qr,Ve,Lr,Lt,pn,zr,Q,ce,Ho,Xe,mn,Do,dn,Ur,zt,cn,Yr,z,Je,Ke,un,Qe,vn,_n,$n,qo,Ut,is,gn,Ze,Z,wn,Yt,yn,En,Bt,bn,Tn,kn,ee,et,tt,Pn,Wt,An,Cn,Sn,ot,Fn,rt,lt,On,Gt,In,xn,Rn,at,jn,nt,st,Mn,Vt,Nn,Hn,Dn,it,qn,ft,ht,Ln,pt,zn,Un,Yn,mt,dt,ct,Bn,Lo,Wn,Gn,Vn,ut,Xn,vt,M,Jn,_t,zo,Kn,Qn,Uo,Zn,es,$t,ts,os,rs,gt,Br,Xt,ls,Wr,wt,Gr,ue,Vr;return y=new $e({}),be=new $e({}),Pe=new k({props:{code:"python -m venv .env",highlighted:'python -m venv .<span class="hljs-built_in">env</span>'}}),Ae=new k({props:{code:"source .env/bin/activate",highlighted:'<span class="hljs-built_in">source</span> .<span class="hljs-built_in">env</span>/bin/activate'}}),Ce=new k({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),Se=new k({props:{code:"pip install transformers[torch]",highlighted:"pip install transformers[torch]"}}),Fe=new k({props:{code:"pip install transformers[tf-cpu]",highlighted:"pip install transformers[tf-cpu]"}}),Oe=new k({props:{code:"pip install transformers[flax]",highlighted:"pip install transformers[flax]"}}),Ie=new k({props:{code:`python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"`,highlighted:'python -c <span class="hljs-string">&quot;from transformers import pipeline; print(pipeline(&#x27;sentiment-analysis&#x27;)(&#x27;we love you&#x27;))&quot;</span>'}}),xe=new k({props:{code:"[{'label': 'POSITIVE', 'score': 0.9998704791069031}]",highlighted:'[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: 0.9998704791069031}]'}}),Re=new $e({}),je=new k({props:{code:"pip install git+https://github.com/huggingface/transformers",highlighted:"pip install git+https://github.com/huggingface/transformers"}}),Ne=new k({props:{code:`python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"`,highlighted:'python -c <span class="hljs-string">&quot;from transformers import pipeline; print(pipeline(&#x27;sentiment-analysis&#x27;)(&#x27;I love you&#x27;))&quot;</span>'}}),He=new $e({}),qe=new k({props:{code:`git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/huggingface/transformers.git
<span class="hljs-built_in">cd</span> transformers
pip install -e .`}}),ae=new bl({props:{warning:!0,$$slots:{default:[Li]},$$scope:{ctx:N}}}),Le=new k({props:{code:`cd ~/transformers/
git pull`,highlighted:`<span class="hljs-built_in">cd</span> ~/transformers/
git pull`}}),ze=new $e({}),Ue=new k({props:{code:"conda install -c huggingface transformers",highlighted:"conda install -c huggingface transformers"}}),Ye=new $e({}),he=new bl({props:{$$slots:{default:[zi]},$$scope:{ctx:N}}}),We=new $e({}),de=new bl({props:{$$slots:{default:[Ui]},$$scope:{ctx:N}}}),Ge=new k({props:{code:"python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...",highlighted:"python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ..."}}),Ve=new k({props:{code:`HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...`,highlighted:`HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...`}}),Xe=new $e({}),ot=new k({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigscience/T0_3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;bigscience/T0_3B&quot;</span>)`}}),at=new k({props:{code:`tokenizer.save_pretrained("./your/path/bigscience_t0")
model.save_pretrained("./your/path/bigscience_t0")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)`}}),it=new k({props:{code:`tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
model = AutoModel.from_pretrained("./your/path/bigscience_t0")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)`}}),ut=new k({props:{code:"python -m pip install huggingface_hub",highlighted:"python -m pip install huggingface_hub"}}),gt=new k({props:{code:`from huggingface_hub import hf_hub_download

hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download

<span class="hljs-meta">&gt;&gt;&gt; </span>hf_hub_download(repo_id=<span class="hljs-string">&quot;bigscience/T0_3B&quot;</span>, filename=<span class="hljs-string">&quot;config.json&quot;</span>, cache_dir=<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)`}}),wt=new k({props:{code:`from transformers import AutoConfig

config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0/config.json&quot;</span>)`}}),ue=new bl({props:{$$slots:{default:[Yi]},$$scope:{ctx:N}}}),{c(){d=r("meta"),P=h(),c=r("h1"),b=r("a"),T=r("span"),u(y.$$.fragment),E=h(),A=r("span"),F=n("Installation"),S=h(),x=r("p"),R=n("Install \u{1F917} Transformers for whichever deep learning library you\u2019re working with, setup your cache, and optionally configure \u{1F917} Transformers to run offline."),O=h(),U=r("p"),Et=n("\u{1F917} Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:"),ge=h(),H=r("ul"),bt=r("li"),we=r("a"),Tl=n("PyTorch"),kl=n(" installation instructions."),Pl=h(),Tt=r("li"),ye=r("a"),Al=n("TensorFlow 2.0"),Cl=n(" installation instructions."),Sl=h(),kt=r("li"),Ee=r("a"),Fl=n("Flax"),Ol=n(" installation instructions."),Go=h(),Y=r("h2"),te=r("a"),ao=r("span"),u(be.$$.fragment),Il=h(),no=r("span"),xl=n("Install with pip"),Vo=h(),D=r("p"),Rl=n("You should install \u{1F917} Transformers in a "),Te=r("a"),jl=n("virtual environment"),Ml=n(". If you\u2019re unfamiliar with Python virtual environments, take a look at this "),ke=r("a"),Nl=n("guide"),Hl=n(". A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies."),Xo=h(),Pt=r("p"),Dl=n("Start by creating a virtual environment in your project directory:"),Jo=h(),u(Pe.$$.fragment),Ko=h(),At=r("p"),ql=n("Activate the virtual environment:"),Qo=h(),u(Ae.$$.fragment),Zo=h(),Ct=r("p"),Ll=n("Now you\u2019re ready to install \u{1F917} Transformers with the following command:"),er=h(),u(Ce.$$.fragment),tr=h(),St=r("p"),zl=n("For CPU-support only, you can conveniently install \u{1F917} Transformers and a deep learning library in one line. For example, install \u{1F917} Transformers and PyTorch with:"),or=h(),u(Se.$$.fragment),rr=h(),Ft=r("p"),Ul=n("\u{1F917} Transformers and TensorFlow 2.0:"),lr=h(),u(Fe.$$.fragment),ar=h(),Ot=r("p"),Yl=n("\u{1F917} Transformers and Flax:"),nr=h(),u(Oe.$$.fragment),sr=h(),It=r("p"),Bl=n("Finally, check if \u{1F917} Transformers has been properly installed by running the following command. It will download a pretrained model:"),ir=h(),u(Ie.$$.fragment),fr=h(),xt=r("p"),Wl=n("Then print out the label and score:"),hr=h(),u(xe.$$.fragment),pr=h(),B=r("h2"),oe=r("a"),so=r("span"),u(Re.$$.fragment),Gl=h(),io=r("span"),Vl=n("Install from source"),mr=h(),Rt=r("p"),Xl=n("Install \u{1F917} Transformers from source with the following command:"),dr=h(),u(je.$$.fragment),cr=h(),C=r("p"),Jl=n("This command installs the bleeding edge "),fo=r("code"),Kl=n("main"),Ql=n(" version rather than the latest "),ho=r("code"),Zl=n("stable"),ea=n(" version. The "),po=r("code"),ta=n("main"),oa=n(" version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn\u2019t been rolled out yet. However, this means the "),mo=r("code"),ra=n("main"),la=n(" version may not always be stable. We strive to keep the "),co=r("code"),aa=n("main"),na=n(" version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an "),Me=r("a"),sa=n("Issue"),ia=n(" so we can fix it even sooner!"),ur=h(),jt=r("p"),fa=n("Check if \u{1F917} Transformers has been properly installed by running the following command:"),vr=h(),u(Ne.$$.fragment),_r=h(),W=r("h2"),re=r("a"),uo=r("span"),u(He.$$.fragment),ha=h(),vo=r("span"),pa=n("Editable install"),$r=h(),Mt=r("p"),ma=n("You will need an editable install if you\u2019d like to:"),gr=h(),le=r("ul"),De=r("li"),da=n("Use the "),_o=r("code"),ca=n("main"),ua=n(" version of the source code."),va=h(),$o=r("li"),_a=n("Contribute to \u{1F917} Transformers and need to test changes in the code."),wr=h(),Nt=r("p"),$a=n("Clone the repository and install \u{1F917} Transformers with the following commands:"),yr=h(),u(qe.$$.fragment),Er=h(),q=r("p"),ga=n("These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in "),go=r("code"),wa=n("~/anaconda3/envs/main/lib/python3.7/site-packages/"),ya=n(", Python will also search the folder you cloned to: "),wo=r("code"),Ea=n("~/transformers/"),ba=n("."),br=h(),u(ae.$$.fragment),Tr=h(),Ht=r("p"),Ta=n("Now you can easily update your clone to the latest version of \u{1F917} Transformers with the following command:"),kr=h(),u(Le.$$.fragment),Pr=h(),ne=r("p"),ka=n("Your Python environment will find the "),yo=r("code"),Pa=n("main"),Aa=n(" version of \u{1F917} Transformers on the next run."),Ar=h(),G=r("h2"),se=r("a"),Eo=r("span"),u(ze.$$.fragment),Ca=h(),bo=r("span"),Sa=n("Install with conda"),Cr=h(),ie=r("p"),Fa=n("Install from the conda channel "),To=r("code"),Oa=n("huggingface"),Ia=n(":"),Sr=h(),u(Ue.$$.fragment),Fr=h(),V=r("h2"),fe=r("a"),ko=r("span"),u(Ye.$$.fragment),xa=h(),Po=r("span"),Ra=n("Cache setup"),Or=h(),j=r("p"),ja=n("Pretrained models are downloaded and locally cached at: "),Ao=r("code"),Ma=n("~/.cache/huggingface/transformers/"),Na=n(". This is the default directory given by the shell environment variable "),Co=r("code"),Ha=n("TRANSFORMERS_CACHE"),Da=n(". On Windows, the default directory is given by "),So=r("code"),qa=n("C:\\Users\\username\\.cache\\huggingface\\transformers"),La=n(". You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:"),Ir=h(),L=r("ol"),Be=r("li"),za=n("Shell environment variable (default): "),Fo=r("code"),Ua=n("TRANSFORMERS_CACHE"),Ya=n("."),Ba=h(),X=r("li"),Wa=n("Shell environment variable: "),Oo=r("code"),Ga=n("HF_HOME"),Va=n(" + "),Io=r("code"),Xa=n("transformers/"),Ja=n("."),Ka=h(),J=r("li"),Qa=n("Shell environment variable: "),xo=r("code"),Za=n("XDG_CACHE_HOME"),en=n(" + "),Ro=r("code"),tn=n("/huggingface/transformers"),on=n("."),xr=h(),u(he.$$.fragment),Rr=h(),K=r("h2"),pe=r("a"),jo=r("span"),u(We.$$.fragment),rn=h(),Mo=r("span"),ln=n("Offline mode"),jr=h(),me=r("p"),an=n("\u{1F917} Transformers is able to run in a firewalled or offline environment by only using local files. Set the environment variable "),No=r("code"),nn=n("TRANSFORMERS_OFFLINE=1"),sn=n(" to enable this behavior."),Mr=h(),u(de.$$.fragment),Nr=h(),Dt=r("p"),fn=n("For example, you would typically run a program on a normal network firewalled to external instances with the following command:"),Hr=h(),u(Ge.$$.fragment),Dr=h(),qt=r("p"),hn=n("Run this same program in an offline instance with:"),qr=h(),u(Ve.$$.fragment),Lr=h(),Lt=r("p"),pn=n("The script should now run without hanging or waiting to timeout because it knows it should only look for local files."),zr=h(),Q=r("h3"),ce=r("a"),Ho=r("span"),u(Xe.$$.fragment),mn=h(),Do=r("span"),dn=n("Fetch models and tokenizers to use offline"),Ur=h(),zt=r("p"),cn=n("Another option for using \u{1F917} Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:"),Yr=h(),z=r("ul"),Je=r("li"),Ke=r("p"),un=n("Download a file through the user interface on the "),Qe=r("a"),vn=n("Model Hub"),_n=n(" by clicking on the \u2193 icon."),$n=h(),qo=r("p"),Ut=r("img"),gn=h(),Ze=r("li"),Z=r("p"),wn=n("Use the "),Yt=r("a"),yn=n("PreTrainedModel.from_pretrained()"),En=n(" and "),Bt=r("a"),bn=n("PreTrainedModel.save_pretrained()"),Tn=n(" workflow:"),kn=h(),ee=r("ol"),et=r("li"),tt=r("p"),Pn=n("Download your files ahead of time with "),Wt=r("a"),An=n("PreTrainedModel.from_pretrained()"),Cn=n(":"),Sn=h(),u(ot.$$.fragment),Fn=h(),rt=r("li"),lt=r("p"),On=n("Save your files to a specified directory with "),Gt=r("a"),In=n("PreTrainedModel.save_pretrained()"),xn=n(":"),Rn=h(),u(at.$$.fragment),jn=h(),nt=r("li"),st=r("p"),Mn=n("Now when you\u2019re offline, reload your files with "),Vt=r("a"),Nn=n("PreTrainedModel.from_pretrained()"),Hn=n(" from the specified directory:"),Dn=h(),u(it.$$.fragment),qn=h(),ft=r("li"),ht=r("p"),Ln=n("Programmatically download files with the "),pt=r("a"),zn=n("huggingface_hub"),Un=n(" library:"),Yn=h(),mt=r("ol"),dt=r("li"),ct=r("p"),Bn=n("Install the "),Lo=r("code"),Wn=n("huggingface_hub"),Gn=n(" library in your virtual environment:"),Vn=h(),u(ut.$$.fragment),Xn=h(),vt=r("li"),M=r("p"),Jn=n("Use the "),_t=r("a"),zo=r("code"),Kn=n("hf_hub_download"),Qn=n(" function to download a file to a specific path. For example, the following command downloads the "),Uo=r("code"),Zn=n("config.json"),es=n(" file from the "),$t=r("a"),ts=n("T0"),os=n(" model to your desired path:"),rs=h(),u(gt.$$.fragment),Br=h(),Xt=r("p"),ls=n("Once your file is downloaded and locally cached, specify it\u2019s local path to load and use it:"),Wr=h(),u(wt.$$.fragment),Gr=h(),u(ue.$$.fragment),this.h()},l(e){const i=Hi('[data-svelte="svelte-1phssyn"]',document.head);d=l(i,"META",{name:!0,content:!0}),i.forEach(o),P=p(e),c=l(e,"H1",{class:!0});var yt=a(c);b=l(yt,"A",{id:!0,class:!0,href:!0});var Yo=a(b);T=l(Yo,"SPAN",{});var Bo=a(T);v(y.$$.fragment,Bo),Bo.forEach(o),Yo.forEach(o),E=p(yt),A=l(yt,"SPAN",{});var Wo=a(A);F=s(Wo,"Installation"),Wo.forEach(o),yt.forEach(o),S=p(e),x=l(e,"P",{});var fs=a(x);R=s(fs,"Install \u{1F917} Transformers for whichever deep learning library you\u2019re working with, setup your cache, and optionally configure \u{1F917} Transformers to run offline."),fs.forEach(o),O=p(e),U=l(e,"P",{});var hs=a(U);Et=s(hs,"\u{1F917} Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:"),hs.forEach(o),ge=p(e),H=l(e,"UL",{});var Jt=a(H);bt=l(Jt,"LI",{});var as=a(bt);we=l(as,"A",{href:!0,rel:!0});var ps=a(we);Tl=s(ps,"PyTorch"),ps.forEach(o),kl=s(as," installation instructions."),as.forEach(o),Pl=p(Jt),Tt=l(Jt,"LI",{});var ns=a(Tt);ye=l(ns,"A",{href:!0,rel:!0});var ms=a(ye);Al=s(ms,"TensorFlow 2.0"),ms.forEach(o),Cl=s(ns," installation instructions."),ns.forEach(o),Sl=p(Jt),kt=l(Jt,"LI",{});var ss=a(kt);Ee=l(ss,"A",{href:!0,rel:!0});var ds=a(Ee);Fl=s(ds,"Flax"),ds.forEach(o),Ol=s(ss," installation instructions."),ss.forEach(o),Jt.forEach(o),Go=p(e),Y=l(e,"H2",{class:!0});var Xr=a(Y);te=l(Xr,"A",{id:!0,class:!0,href:!0});var cs=a(te);ao=l(cs,"SPAN",{});var us=a(ao);v(be.$$.fragment,us),us.forEach(o),cs.forEach(o),Il=p(Xr),no=l(Xr,"SPAN",{});var vs=a(no);xl=s(vs,"Install with pip"),vs.forEach(o),Xr.forEach(o),Vo=p(e),D=l(e,"P",{});var Kt=a(D);Rl=s(Kt,"You should install \u{1F917} Transformers in a "),Te=l(Kt,"A",{href:!0,rel:!0});var _s=a(Te);jl=s(_s,"virtual environment"),_s.forEach(o),Ml=s(Kt,". If you\u2019re unfamiliar with Python virtual environments, take a look at this "),ke=l(Kt,"A",{href:!0,rel:!0});var $s=a(ke);Nl=s($s,"guide"),$s.forEach(o),Hl=s(Kt,". A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies."),Kt.forEach(o),Xo=p(e),Pt=l(e,"P",{});var gs=a(Pt);Dl=s(gs,"Start by creating a virtual environment in your project directory:"),gs.forEach(o),Jo=p(e),v(Pe.$$.fragment,e),Ko=p(e),At=l(e,"P",{});var ws=a(At);ql=s(ws,"Activate the virtual environment:"),ws.forEach(o),Qo=p(e),v(Ae.$$.fragment,e),Zo=p(e),Ct=l(e,"P",{});var ys=a(Ct);Ll=s(ys,"Now you\u2019re ready to install \u{1F917} Transformers with the following command:"),ys.forEach(o),er=p(e),v(Ce.$$.fragment,e),tr=p(e),St=l(e,"P",{});var Es=a(St);zl=s(Es,"For CPU-support only, you can conveniently install \u{1F917} Transformers and a deep learning library in one line. For example, install \u{1F917} Transformers and PyTorch with:"),Es.forEach(o),or=p(e),v(Se.$$.fragment,e),rr=p(e),Ft=l(e,"P",{});var bs=a(Ft);Ul=s(bs,"\u{1F917} Transformers and TensorFlow 2.0:"),bs.forEach(o),lr=p(e),v(Fe.$$.fragment,e),ar=p(e),Ot=l(e,"P",{});var Ts=a(Ot);Yl=s(Ts,"\u{1F917} Transformers and Flax:"),Ts.forEach(o),nr=p(e),v(Oe.$$.fragment,e),sr=p(e),It=l(e,"P",{});var ks=a(It);Bl=s(ks,"Finally, check if \u{1F917} Transformers has been properly installed by running the following command. It will download a pretrained model:"),ks.forEach(o),ir=p(e),v(Ie.$$.fragment,e),fr=p(e),xt=l(e,"P",{});var Ps=a(xt);Wl=s(Ps,"Then print out the label and score:"),Ps.forEach(o),hr=p(e),v(xe.$$.fragment,e),pr=p(e),B=l(e,"H2",{class:!0});var Jr=a(B);oe=l(Jr,"A",{id:!0,class:!0,href:!0});var As=a(oe);so=l(As,"SPAN",{});var Cs=a(so);v(Re.$$.fragment,Cs),Cs.forEach(o),As.forEach(o),Gl=p(Jr),io=l(Jr,"SPAN",{});var Ss=a(io);Vl=s(Ss,"Install from source"),Ss.forEach(o),Jr.forEach(o),mr=p(e),Rt=l(e,"P",{});var Fs=a(Rt);Xl=s(Fs,"Install \u{1F917} Transformers from source with the following command:"),Fs.forEach(o),dr=p(e),v(je.$$.fragment,e),cr=p(e),C=l(e,"P",{});var I=a(C);Jl=s(I,"This command installs the bleeding edge "),fo=l(I,"CODE",{});var Os=a(fo);Kl=s(Os,"main"),Os.forEach(o),Ql=s(I," version rather than the latest "),ho=l(I,"CODE",{});var Is=a(ho);Zl=s(Is,"stable"),Is.forEach(o),ea=s(I," version. The "),po=l(I,"CODE",{});var xs=a(po);ta=s(xs,"main"),xs.forEach(o),oa=s(I," version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn\u2019t been rolled out yet. However, this means the "),mo=l(I,"CODE",{});var Rs=a(mo);ra=s(Rs,"main"),Rs.forEach(o),la=s(I," version may not always be stable. We strive to keep the "),co=l(I,"CODE",{});var js=a(co);aa=s(js,"main"),js.forEach(o),na=s(I," version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an "),Me=l(I,"A",{href:!0,rel:!0});var Ms=a(Me);sa=s(Ms,"Issue"),Ms.forEach(o),ia=s(I," so we can fix it even sooner!"),I.forEach(o),ur=p(e),jt=l(e,"P",{});var Ns=a(jt);fa=s(Ns,"Check if \u{1F917} Transformers has been properly installed by running the following command:"),Ns.forEach(o),vr=p(e),v(Ne.$$.fragment,e),_r=p(e),W=l(e,"H2",{class:!0});var Kr=a(W);re=l(Kr,"A",{id:!0,class:!0,href:!0});var Hs=a(re);uo=l(Hs,"SPAN",{});var Ds=a(uo);v(He.$$.fragment,Ds),Ds.forEach(o),Hs.forEach(o),ha=p(Kr),vo=l(Kr,"SPAN",{});var qs=a(vo);pa=s(qs,"Editable install"),qs.forEach(o),Kr.forEach(o),$r=p(e),Mt=l(e,"P",{});var Ls=a(Mt);ma=s(Ls,"You will need an editable install if you\u2019d like to:"),Ls.forEach(o),gr=p(e),le=l(e,"UL",{});var Qr=a(le);De=l(Qr,"LI",{});var Zr=a(De);da=s(Zr,"Use the "),_o=l(Zr,"CODE",{});var zs=a(_o);ca=s(zs,"main"),zs.forEach(o),ua=s(Zr," version of the source code."),Zr.forEach(o),va=p(Qr),$o=l(Qr,"LI",{});var Us=a($o);_a=s(Us,"Contribute to \u{1F917} Transformers and need to test changes in the code."),Us.forEach(o),Qr.forEach(o),wr=p(e),Nt=l(e,"P",{});var Ys=a(Nt);$a=s(Ys,"Clone the repository and install \u{1F917} Transformers with the following commands:"),Ys.forEach(o),yr=p(e),v(qe.$$.fragment,e),Er=p(e),q=l(e,"P",{});var Qt=a(q);ga=s(Qt,"These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in "),go=l(Qt,"CODE",{});var Bs=a(go);wa=s(Bs,"~/anaconda3/envs/main/lib/python3.7/site-packages/"),Bs.forEach(o),ya=s(Qt,", Python will also search the folder you cloned to: "),wo=l(Qt,"CODE",{});var Ws=a(wo);Ea=s(Ws,"~/transformers/"),Ws.forEach(o),ba=s(Qt,"."),Qt.forEach(o),br=p(e),v(ae.$$.fragment,e),Tr=p(e),Ht=l(e,"P",{});var Gs=a(Ht);Ta=s(Gs,"Now you can easily update your clone to the latest version of \u{1F917} Transformers with the following command:"),Gs.forEach(o),kr=p(e),v(Le.$$.fragment,e),Pr=p(e),ne=l(e,"P",{});var el=a(ne);ka=s(el,"Your Python environment will find the "),yo=l(el,"CODE",{});var Vs=a(yo);Pa=s(Vs,"main"),Vs.forEach(o),Aa=s(el," version of \u{1F917} Transformers on the next run."),el.forEach(o),Ar=p(e),G=l(e,"H2",{class:!0});var tl=a(G);se=l(tl,"A",{id:!0,class:!0,href:!0});var Xs=a(se);Eo=l(Xs,"SPAN",{});var Js=a(Eo);v(ze.$$.fragment,Js),Js.forEach(o),Xs.forEach(o),Ca=p(tl),bo=l(tl,"SPAN",{});var Ks=a(bo);Sa=s(Ks,"Install with conda"),Ks.forEach(o),tl.forEach(o),Cr=p(e),ie=l(e,"P",{});var ol=a(ie);Fa=s(ol,"Install from the conda channel "),To=l(ol,"CODE",{});var Qs=a(To);Oa=s(Qs,"huggingface"),Qs.forEach(o),Ia=s(ol,":"),ol.forEach(o),Sr=p(e),v(Ue.$$.fragment,e),Fr=p(e),V=l(e,"H2",{class:!0});var rl=a(V);fe=l(rl,"A",{id:!0,class:!0,href:!0});var Zs=a(fe);ko=l(Zs,"SPAN",{});var ei=a(ko);v(Ye.$$.fragment,ei),ei.forEach(o),Zs.forEach(o),xa=p(rl),Po=l(rl,"SPAN",{});var ti=a(Po);Ra=s(ti,"Cache setup"),ti.forEach(o),rl.forEach(o),Or=p(e),j=l(e,"P",{});var ve=a(j);ja=s(ve,"Pretrained models are downloaded and locally cached at: "),Ao=l(ve,"CODE",{});var oi=a(Ao);Ma=s(oi,"~/.cache/huggingface/transformers/"),oi.forEach(o),Na=s(ve,". This is the default directory given by the shell environment variable "),Co=l(ve,"CODE",{});var ri=a(Co);Ha=s(ri,"TRANSFORMERS_CACHE"),ri.forEach(o),Da=s(ve,". On Windows, the default directory is given by "),So=l(ve,"CODE",{});var li=a(So);qa=s(li,"C:\\Users\\username\\.cache\\huggingface\\transformers"),li.forEach(o),La=s(ve,". You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:"),ve.forEach(o),Ir=p(e),L=l(e,"OL",{});var Zt=a(L);Be=l(Zt,"LI",{});var ll=a(Be);za=s(ll,"Shell environment variable (default): "),Fo=l(ll,"CODE",{});var ai=a(Fo);Ua=s(ai,"TRANSFORMERS_CACHE"),ai.forEach(o),Ya=s(ll,"."),ll.forEach(o),Ba=p(Zt),X=l(Zt,"LI",{});var eo=a(X);Wa=s(eo,"Shell environment variable: "),Oo=l(eo,"CODE",{});var ni=a(Oo);Ga=s(ni,"HF_HOME"),ni.forEach(o),Va=s(eo," + "),Io=l(eo,"CODE",{});var si=a(Io);Xa=s(si,"transformers/"),si.forEach(o),Ja=s(eo,"."),eo.forEach(o),Ka=p(Zt),J=l(Zt,"LI",{});var to=a(J);Qa=s(to,"Shell environment variable: "),xo=l(to,"CODE",{});var ii=a(xo);Za=s(ii,"XDG_CACHE_HOME"),ii.forEach(o),en=s(to," + "),Ro=l(to,"CODE",{});var fi=a(Ro);tn=s(fi,"/huggingface/transformers"),fi.forEach(o),on=s(to,"."),to.forEach(o),Zt.forEach(o),xr=p(e),v(he.$$.fragment,e),Rr=p(e),K=l(e,"H2",{class:!0});var al=a(K);pe=l(al,"A",{id:!0,class:!0,href:!0});var hi=a(pe);jo=l(hi,"SPAN",{});var pi=a(jo);v(We.$$.fragment,pi),pi.forEach(o),hi.forEach(o),rn=p(al),Mo=l(al,"SPAN",{});var mi=a(Mo);ln=s(mi,"Offline mode"),mi.forEach(o),al.forEach(o),jr=p(e),me=l(e,"P",{});var nl=a(me);an=s(nl,"\u{1F917} Transformers is able to run in a firewalled or offline environment by only using local files. Set the environment variable "),No=l(nl,"CODE",{});var di=a(No);nn=s(di,"TRANSFORMERS_OFFLINE=1"),di.forEach(o),sn=s(nl," to enable this behavior."),nl.forEach(o),Mr=p(e),v(de.$$.fragment,e),Nr=p(e),Dt=l(e,"P",{});var ci=a(Dt);fn=s(ci,"For example, you would typically run a program on a normal network firewalled to external instances with the following command:"),ci.forEach(o),Hr=p(e),v(Ge.$$.fragment,e),Dr=p(e),qt=l(e,"P",{});var ui=a(qt);hn=s(ui,"Run this same program in an offline instance with:"),ui.forEach(o),qr=p(e),v(Ve.$$.fragment,e),Lr=p(e),Lt=l(e,"P",{});var vi=a(Lt);pn=s(vi,"The script should now run without hanging or waiting to timeout because it knows it should only look for local files."),vi.forEach(o),zr=p(e),Q=l(e,"H3",{class:!0});var sl=a(Q);ce=l(sl,"A",{id:!0,class:!0,href:!0});var _i=a(ce);Ho=l(_i,"SPAN",{});var $i=a(Ho);v(Xe.$$.fragment,$i),$i.forEach(o),_i.forEach(o),mn=p(sl),Do=l(sl,"SPAN",{});var gi=a(Do);dn=s(gi,"Fetch models and tokenizers to use offline"),gi.forEach(o),sl.forEach(o),Ur=p(e),zt=l(e,"P",{});var wi=a(zt);cn=s(wi,"Another option for using \u{1F917} Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:"),wi.forEach(o),Yr=p(e),z=l(e,"UL",{});var oo=a(z);Je=l(oo,"LI",{});var il=a(Je);Ke=l(il,"P",{});var fl=a(Ke);un=s(fl,"Download a file through the user interface on the "),Qe=l(fl,"A",{href:!0,rel:!0});var yi=a(Qe);vn=s(yi,"Model Hub"),yi.forEach(o),_n=s(fl," by clicking on the \u2193 icon."),fl.forEach(o),$n=p(il),qo=l(il,"P",{});var Ei=a(qo);Ut=l(Ei,"IMG",{src:!0,alt:!0}),Ei.forEach(o),il.forEach(o),gn=p(oo),Ze=l(oo,"LI",{});var hl=a(Ze);Z=l(hl,"P",{});var ro=a(Z);wn=s(ro,"Use the "),Yt=l(ro,"A",{href:!0});var bi=a(Yt);yn=s(bi,"PreTrainedModel.from_pretrained()"),bi.forEach(o),En=s(ro," and "),Bt=l(ro,"A",{href:!0});var Ti=a(Bt);bn=s(Ti,"PreTrainedModel.save_pretrained()"),Ti.forEach(o),Tn=s(ro," workflow:"),ro.forEach(o),kn=p(hl),ee=l(hl,"OL",{});var lo=a(ee);et=l(lo,"LI",{});var pl=a(et);tt=l(pl,"P",{});var ml=a(tt);Pn=s(ml,"Download your files ahead of time with "),Wt=l(ml,"A",{href:!0});var ki=a(Wt);An=s(ki,"PreTrainedModel.from_pretrained()"),ki.forEach(o),Cn=s(ml,":"),ml.forEach(o),Sn=p(pl),v(ot.$$.fragment,pl),pl.forEach(o),Fn=p(lo),rt=l(lo,"LI",{});var dl=a(rt);lt=l(dl,"P",{});var cl=a(lt);On=s(cl,"Save your files to a specified directory with "),Gt=l(cl,"A",{href:!0});var Pi=a(Gt);In=s(Pi,"PreTrainedModel.save_pretrained()"),Pi.forEach(o),xn=s(cl,":"),cl.forEach(o),Rn=p(dl),v(at.$$.fragment,dl),dl.forEach(o),jn=p(lo),nt=l(lo,"LI",{});var ul=a(nt);st=l(ul,"P",{});var vl=a(st);Mn=s(vl,"Now when you\u2019re offline, reload your files with "),Vt=l(vl,"A",{href:!0});var Ai=a(Vt);Nn=s(Ai,"PreTrainedModel.from_pretrained()"),Ai.forEach(o),Hn=s(vl," from the specified directory:"),vl.forEach(o),Dn=p(ul),v(it.$$.fragment,ul),ul.forEach(o),lo.forEach(o),hl.forEach(o),qn=p(oo),ft=l(oo,"LI",{});var _l=a(ft);ht=l(_l,"P",{});var $l=a(ht);Ln=s($l,"Programmatically download files with the "),pt=l($l,"A",{href:!0,rel:!0});var Ci=a(pt);zn=s(Ci,"huggingface_hub"),Ci.forEach(o),Un=s($l," library:"),$l.forEach(o),Yn=p(_l),mt=l(_l,"OL",{});var gl=a(mt);dt=l(gl,"LI",{});var wl=a(dt);ct=l(wl,"P",{});var yl=a(ct);Bn=s(yl,"Install the "),Lo=l(yl,"CODE",{});var Si=a(Lo);Wn=s(Si,"huggingface_hub"),Si.forEach(o),Gn=s(yl," library in your virtual environment:"),yl.forEach(o),Vn=p(wl),v(ut.$$.fragment,wl),wl.forEach(o),Xn=p(gl),vt=l(gl,"LI",{});var El=a(vt);M=l(El,"P",{});var _e=a(M);Jn=s(_e,"Use the "),_t=l(_e,"A",{href:!0,rel:!0});var Fi=a(_t);zo=l(Fi,"CODE",{});var Oi=a(zo);Kn=s(Oi,"hf_hub_download"),Oi.forEach(o),Fi.forEach(o),Qn=s(_e," function to download a file to a specific path. For example, the following command downloads the "),Uo=l(_e,"CODE",{});var Ii=a(Uo);Zn=s(Ii,"config.json"),Ii.forEach(o),es=s(_e," file from the "),$t=l(_e,"A",{href:!0,rel:!0});var xi=a($t);ts=s(xi,"T0"),xi.forEach(o),os=s(_e," model to your desired path:"),_e.forEach(o),rs=p(El),v(gt.$$.fragment,El),El.forEach(o),gl.forEach(o),_l.forEach(o),oo.forEach(o),Br=p(e),Xt=l(e,"P",{});var Ri=a(Xt);ls=s(Ri,"Once your file is downloaded and locally cached, specify it\u2019s local path to load and use it:"),Ri.forEach(o),Wr=p(e),v(wt.$$.fragment,e),Gr=p(e),v(ue.$$.fragment,e),this.h()},h(){m(d,"name","hf:doc:metadata"),m(d,"content",JSON.stringify(Wi)),m(b,"id","installation"),m(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(b,"href","#installation"),m(c,"class","relative group"),m(we,"href","https://pytorch.org/get-started/locally/"),m(we,"rel","nofollow"),m(ye,"href","https://www.tensorflow.org/install/pip"),m(ye,"rel","nofollow"),m(Ee,"href","https://flax.readthedocs.io/en/latest/"),m(Ee,"rel","nofollow"),m(te,"id","install-with-pip"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#install-with-pip"),m(Y,"class","relative group"),m(Te,"href","https://docs.python.org/3/library/venv.html"),m(Te,"rel","nofollow"),m(ke,"href","https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/"),m(ke,"rel","nofollow"),m(oe,"id","install-from-source"),m(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(oe,"href","#install-from-source"),m(B,"class","relative group"),m(Me,"href","https://github.com/huggingface/transformers/issues"),m(Me,"rel","nofollow"),m(re,"id","editable-install"),m(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(re,"href","#editable-install"),m(W,"class","relative group"),m(se,"id","install-with-conda"),m(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(se,"href","#install-with-conda"),m(G,"class","relative group"),m(fe,"id","cache-setup"),m(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fe,"href","#cache-setup"),m(V,"class","relative group"),m(pe,"id","offline-mode"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#offline-mode"),m(K,"class","relative group"),m(ce,"id","fetch-models-and-tokenizers-to-use-offline"),m(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ce,"href","#fetch-models-and-tokenizers-to-use-offline"),m(Q,"class","relative group"),m(Qe,"href","https://huggingface.co/models"),m(Qe,"rel","nofollow"),Di(Ut.src,is="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png")||m(Ut,"src",is),m(Ut,"alt","download-icon"),m(Yt,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(Bt,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(Wt,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(Gt,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(Vt,"href","/docs/transformers/pr_highlight/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(pt,"href","https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub"),m(pt,"rel","nofollow"),m(_t,"href","https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub"),m(_t,"rel","nofollow"),m($t,"href","https://huggingface.co/bigscience/T0_3B"),m($t,"rel","nofollow")},m(e,i){t(document.head,d),f(e,P,i),f(e,c,i),t(c,b),t(b,T),_(y,T,null),t(c,E),t(c,A),t(A,F),f(e,S,i),f(e,x,i),t(x,R),f(e,O,i),f(e,U,i),t(U,Et),f(e,ge,i),f(e,H,i),t(H,bt),t(bt,we),t(we,Tl),t(bt,kl),t(H,Pl),t(H,Tt),t(Tt,ye),t(ye,Al),t(Tt,Cl),t(H,Sl),t(H,kt),t(kt,Ee),t(Ee,Fl),t(kt,Ol),f(e,Go,i),f(e,Y,i),t(Y,te),t(te,ao),_(be,ao,null),t(Y,Il),t(Y,no),t(no,xl),f(e,Vo,i),f(e,D,i),t(D,Rl),t(D,Te),t(Te,jl),t(D,Ml),t(D,ke),t(ke,Nl),t(D,Hl),f(e,Xo,i),f(e,Pt,i),t(Pt,Dl),f(e,Jo,i),_(Pe,e,i),f(e,Ko,i),f(e,At,i),t(At,ql),f(e,Qo,i),_(Ae,e,i),f(e,Zo,i),f(e,Ct,i),t(Ct,Ll),f(e,er,i),_(Ce,e,i),f(e,tr,i),f(e,St,i),t(St,zl),f(e,or,i),_(Se,e,i),f(e,rr,i),f(e,Ft,i),t(Ft,Ul),f(e,lr,i),_(Fe,e,i),f(e,ar,i),f(e,Ot,i),t(Ot,Yl),f(e,nr,i),_(Oe,e,i),f(e,sr,i),f(e,It,i),t(It,Bl),f(e,ir,i),_(Ie,e,i),f(e,fr,i),f(e,xt,i),t(xt,Wl),f(e,hr,i),_(xe,e,i),f(e,pr,i),f(e,B,i),t(B,oe),t(oe,so),_(Re,so,null),t(B,Gl),t(B,io),t(io,Vl),f(e,mr,i),f(e,Rt,i),t(Rt,Xl),f(e,dr,i),_(je,e,i),f(e,cr,i),f(e,C,i),t(C,Jl),t(C,fo),t(fo,Kl),t(C,Ql),t(C,ho),t(ho,Zl),t(C,ea),t(C,po),t(po,ta),t(C,oa),t(C,mo),t(mo,ra),t(C,la),t(C,co),t(co,aa),t(C,na),t(C,Me),t(Me,sa),t(C,ia),f(e,ur,i),f(e,jt,i),t(jt,fa),f(e,vr,i),_(Ne,e,i),f(e,_r,i),f(e,W,i),t(W,re),t(re,uo),_(He,uo,null),t(W,ha),t(W,vo),t(vo,pa),f(e,$r,i),f(e,Mt,i),t(Mt,ma),f(e,gr,i),f(e,le,i),t(le,De),t(De,da),t(De,_o),t(_o,ca),t(De,ua),t(le,va),t(le,$o),t($o,_a),f(e,wr,i),f(e,Nt,i),t(Nt,$a),f(e,yr,i),_(qe,e,i),f(e,Er,i),f(e,q,i),t(q,ga),t(q,go),t(go,wa),t(q,ya),t(q,wo),t(wo,Ea),t(q,ba),f(e,br,i),_(ae,e,i),f(e,Tr,i),f(e,Ht,i),t(Ht,Ta),f(e,kr,i),_(Le,e,i),f(e,Pr,i),f(e,ne,i),t(ne,ka),t(ne,yo),t(yo,Pa),t(ne,Aa),f(e,Ar,i),f(e,G,i),t(G,se),t(se,Eo),_(ze,Eo,null),t(G,Ca),t(G,bo),t(bo,Sa),f(e,Cr,i),f(e,ie,i),t(ie,Fa),t(ie,To),t(To,Oa),t(ie,Ia),f(e,Sr,i),_(Ue,e,i),f(e,Fr,i),f(e,V,i),t(V,fe),t(fe,ko),_(Ye,ko,null),t(V,xa),t(V,Po),t(Po,Ra),f(e,Or,i),f(e,j,i),t(j,ja),t(j,Ao),t(Ao,Ma),t(j,Na),t(j,Co),t(Co,Ha),t(j,Da),t(j,So),t(So,qa),t(j,La),f(e,Ir,i),f(e,L,i),t(L,Be),t(Be,za),t(Be,Fo),t(Fo,Ua),t(Be,Ya),t(L,Ba),t(L,X),t(X,Wa),t(X,Oo),t(Oo,Ga),t(X,Va),t(X,Io),t(Io,Xa),t(X,Ja),t(L,Ka),t(L,J),t(J,Qa),t(J,xo),t(xo,Za),t(J,en),t(J,Ro),t(Ro,tn),t(J,on),f(e,xr,i),_(he,e,i),f(e,Rr,i),f(e,K,i),t(K,pe),t(pe,jo),_(We,jo,null),t(K,rn),t(K,Mo),t(Mo,ln),f(e,jr,i),f(e,me,i),t(me,an),t(me,No),t(No,nn),t(me,sn),f(e,Mr,i),_(de,e,i),f(e,Nr,i),f(e,Dt,i),t(Dt,fn),f(e,Hr,i),_(Ge,e,i),f(e,Dr,i),f(e,qt,i),t(qt,hn),f(e,qr,i),_(Ve,e,i),f(e,Lr,i),f(e,Lt,i),t(Lt,pn),f(e,zr,i),f(e,Q,i),t(Q,ce),t(ce,Ho),_(Xe,Ho,null),t(Q,mn),t(Q,Do),t(Do,dn),f(e,Ur,i),f(e,zt,i),t(zt,cn),f(e,Yr,i),f(e,z,i),t(z,Je),t(Je,Ke),t(Ke,un),t(Ke,Qe),t(Qe,vn),t(Ke,_n),t(Je,$n),t(Je,qo),t(qo,Ut),t(z,gn),t(z,Ze),t(Ze,Z),t(Z,wn),t(Z,Yt),t(Yt,yn),t(Z,En),t(Z,Bt),t(Bt,bn),t(Z,Tn),t(Ze,kn),t(Ze,ee),t(ee,et),t(et,tt),t(tt,Pn),t(tt,Wt),t(Wt,An),t(tt,Cn),t(et,Sn),_(ot,et,null),t(ee,Fn),t(ee,rt),t(rt,lt),t(lt,On),t(lt,Gt),t(Gt,In),t(lt,xn),t(rt,Rn),_(at,rt,null),t(ee,jn),t(ee,nt),t(nt,st),t(st,Mn),t(st,Vt),t(Vt,Nn),t(st,Hn),t(nt,Dn),_(it,nt,null),t(z,qn),t(z,ft),t(ft,ht),t(ht,Ln),t(ht,pt),t(pt,zn),t(ht,Un),t(ft,Yn),t(ft,mt),t(mt,dt),t(dt,ct),t(ct,Bn),t(ct,Lo),t(Lo,Wn),t(ct,Gn),t(dt,Vn),_(ut,dt,null),t(mt,Xn),t(mt,vt),t(vt,M),t(M,Jn),t(M,_t),t(_t,zo),t(zo,Kn),t(M,Qn),t(M,Uo),t(Uo,Zn),t(M,es),t(M,$t),t($t,ts),t(M,os),t(vt,rs),_(gt,vt,null),f(e,Br,i),f(e,Xt,i),t(Xt,ls),f(e,Wr,i),_(wt,e,i),f(e,Gr,i),_(ue,e,i),Vr=!0},p(e,[i]){const yt={};i&2&&(yt.$$scope={dirty:i,ctx:e}),ae.$set(yt);const Yo={};i&2&&(Yo.$$scope={dirty:i,ctx:e}),he.$set(Yo);const Bo={};i&2&&(Bo.$$scope={dirty:i,ctx:e}),de.$set(Bo);const Wo={};i&2&&(Wo.$$scope={dirty:i,ctx:e}),ue.$set(Wo)},i(e){Vr||($(y.$$.fragment,e),$(be.$$.fragment,e),$(Pe.$$.fragment,e),$(Ae.$$.fragment,e),$(Ce.$$.fragment,e),$(Se.$$.fragment,e),$(Fe.$$.fragment,e),$(Oe.$$.fragment,e),$(Ie.$$.fragment,e),$(xe.$$.fragment,e),$(Re.$$.fragment,e),$(je.$$.fragment,e),$(Ne.$$.fragment,e),$(He.$$.fragment,e),$(qe.$$.fragment,e),$(ae.$$.fragment,e),$(Le.$$.fragment,e),$(ze.$$.fragment,e),$(Ue.$$.fragment,e),$(Ye.$$.fragment,e),$(he.$$.fragment,e),$(We.$$.fragment,e),$(de.$$.fragment,e),$(Ge.$$.fragment,e),$(Ve.$$.fragment,e),$(Xe.$$.fragment,e),$(ot.$$.fragment,e),$(at.$$.fragment,e),$(it.$$.fragment,e),$(ut.$$.fragment,e),$(gt.$$.fragment,e),$(wt.$$.fragment,e),$(ue.$$.fragment,e),Vr=!0)},o(e){g(y.$$.fragment,e),g(be.$$.fragment,e),g(Pe.$$.fragment,e),g(Ae.$$.fragment,e),g(Ce.$$.fragment,e),g(Se.$$.fragment,e),g(Fe.$$.fragment,e),g(Oe.$$.fragment,e),g(Ie.$$.fragment,e),g(xe.$$.fragment,e),g(Re.$$.fragment,e),g(je.$$.fragment,e),g(Ne.$$.fragment,e),g(He.$$.fragment,e),g(qe.$$.fragment,e),g(ae.$$.fragment,e),g(Le.$$.fragment,e),g(ze.$$.fragment,e),g(Ue.$$.fragment,e),g(Ye.$$.fragment,e),g(he.$$.fragment,e),g(We.$$.fragment,e),g(de.$$.fragment,e),g(Ge.$$.fragment,e),g(Ve.$$.fragment,e),g(Xe.$$.fragment,e),g(ot.$$.fragment,e),g(at.$$.fragment,e),g(it.$$.fragment,e),g(ut.$$.fragment,e),g(gt.$$.fragment,e),g(wt.$$.fragment,e),g(ue.$$.fragment,e),Vr=!1},d(e){o(d),e&&o(P),e&&o(c),w(y),e&&o(S),e&&o(x),e&&o(O),e&&o(U),e&&o(ge),e&&o(H),e&&o(Go),e&&o(Y),w(be),e&&o(Vo),e&&o(D),e&&o(Xo),e&&o(Pt),e&&o(Jo),w(Pe,e),e&&o(Ko),e&&o(At),e&&o(Qo),w(Ae,e),e&&o(Zo),e&&o(Ct),e&&o(er),w(Ce,e),e&&o(tr),e&&o(St),e&&o(or),w(Se,e),e&&o(rr),e&&o(Ft),e&&o(lr),w(Fe,e),e&&o(ar),e&&o(Ot),e&&o(nr),w(Oe,e),e&&o(sr),e&&o(It),e&&o(ir),w(Ie,e),e&&o(fr),e&&o(xt),e&&o(hr),w(xe,e),e&&o(pr),e&&o(B),w(Re),e&&o(mr),e&&o(Rt),e&&o(dr),w(je,e),e&&o(cr),e&&o(C),e&&o(ur),e&&o(jt),e&&o(vr),w(Ne,e),e&&o(_r),e&&o(W),w(He),e&&o($r),e&&o(Mt),e&&o(gr),e&&o(le),e&&o(wr),e&&o(Nt),e&&o(yr),w(qe,e),e&&o(Er),e&&o(q),e&&o(br),w(ae,e),e&&o(Tr),e&&o(Ht),e&&o(kr),w(Le,e),e&&o(Pr),e&&o(ne),e&&o(Ar),e&&o(G),w(ze),e&&o(Cr),e&&o(ie),e&&o(Sr),w(Ue,e),e&&o(Fr),e&&o(V),w(Ye),e&&o(Or),e&&o(j),e&&o(Ir),e&&o(L),e&&o(xr),w(he,e),e&&o(Rr),e&&o(K),w(We),e&&o(jr),e&&o(me),e&&o(Mr),w(de,e),e&&o(Nr),e&&o(Dt),e&&o(Hr),w(Ge,e),e&&o(Dr),e&&o(qt),e&&o(qr),w(Ve,e),e&&o(Lr),e&&o(Lt),e&&o(zr),e&&o(Q),w(Xe),e&&o(Ur),e&&o(zt),e&&o(Yr),e&&o(z),w(ot),w(at),w(it),w(ut),w(gt),e&&o(Br),e&&o(Xt),e&&o(Wr),w(wt,e),e&&o(Gr),w(ue,e)}}}const Wi={local:"installation",sections:[{local:"install-with-pip",title:"Install with pip"},{local:"install-from-source",title:"Install from source"},{local:"editable-install",title:"Editable install"},{local:"install-with-conda",title:"Install with conda"},{local:"cache-setup",title:"Cache setup"},{local:"offline-mode",sections:[{local:"fetch-models-and-tokenizers-to-use-offline",title:"Fetch models and tokenizers to use offline"}],title:"Offline mode"}],title:"Installation"};function Gi(N){return qi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qi extends ji{constructor(d){super();Mi(this,d,Gi,Bi,Ni,{})}}export{Qi as default,Wi as metadata};
