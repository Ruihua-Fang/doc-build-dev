---
local: pipelines-for-inference
sections:
- local: pipeline-usage
  sections:
  - local: choose-a-model-and-tokenizer
    title: Choose a model and tokenizer
  title: Pipeline usage
- local: audio-pipeline
  title: Audio pipeline
- local: vision-pipeline
  title: Vision pipeline
title: Pipelines for inference
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import ColabDropdown from "./ColabDropdown.svelte";
import IconCopyLink from "./IconCopyLink.svelte";
export let fw: "pt" | "tf"
</script>
<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="pipelines-for-inference">Pipelines for inference</h1>

The [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) makes it simple to use any model from the [Model Hub](https://huggingface.co/models) for inference. You can use the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) for a variety of tasks such as text generation, image segmentation and audio classification. Even if you don't have experience with a specific modality or understand the code powering the models, you can still use them with the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline)! This tutorial will teach you to:

* Use a [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) for inference.
* Use a specific tokenizer or model.
* Use a [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) for audio and vision tasks.

<Tip>

See [here](main_classes/pipelines) for a complete list of tasks supported by [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline).

</Tip>

<h2 id="pipeline-usage">Pipeline usage</h2>

While each task has an associated [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline), it is simpler to use the general [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) abstraction which contains all the specific task pipelines. The [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) automatically loads a default model and tokenizer capable of completing your task. 

1. Start by creating a [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) with your desired inference task:

```py
>>> from transformers import pipeline

>>> generator = pipeline(task="text-generation")
```

2. Pass your input text to the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline):

```py
>>> generator("Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone")
[&amp;lcub;'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Seven for the Iron-priests at the door to the east, and thirteen for the Lord Kings at the end of the mountain'}]
```

If you have more than one input, pass your input as a list:

```py
>>> generator([
    "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone",
    "Nine for Mortal Men, doomed to die, One for the Dark Lord on his dark throne"
])
```

Any additional parameters to your task can also be included in the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline). For example, `text-generation` has a [generation_utils.GenerationMixin.generate()](/docs/transformers/pr_15165/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate) method with several parameters for controlling the output. Here, you can set the `num_return_sequences` parameter to return more than one output:

```py
>>> generator(
    "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone",
    num_return_sequences=2,
)
```

<h3 id="choose-a-model-and-tokenizer">Choose a model and tokenizer</h3>

The [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) accepts any model from the [Model Hub](https://huggingface.co/models). You can use with the tags on the Model Hub to filter for a model you'd like to use for your task. Then you can load a model and tokenizer with the appropriate `AutoModelFor` and [`AutoTokenizer'] class:

```py
>>> from transformers import AutoTokenizer, AutoModelForCausalLM

>>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
>>> model = AutoModelForCausalLM.from_pretrained("distilgpt2")
```

1. Create a [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) for your task, and specify the model and tokenizer you've loaded:

```py
>>> from transformers import pipeline

>>> generator = pipeline(task="text-generation", model=model, tokenizer=tokenizer)
```

2. Pass your input text to the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline):

```py
>>> generator("Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone")
[&amp;lcub;'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Seven for the Dragon-lords (for them to rule in a world ruled by their rulers, and all who live within the realm'}]
```

<h2 id="audio-pipeline">Audio pipeline</h2>

The flexibility of the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) API means it can also be extended to audio tasks.

For example, let's classify the emotion from a short clip of John F. Kennedy's famous ["We choose to go to the Moon"](https://en.wikipedia.org/wiki/We_choose_to_go_to_the_Moon) speech. Find an [audio classification](https://huggingface.co/models?pipeline_tag=audio-classification) model on the Model Hub for emotion recognition and load it in the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline):

```py
>>> from transformers import pipeline

>>> audio_classifier = pipeline(task="audio-classification", model="superb/wav2vec2-base-superb-er")
```

Pass the audio file to the [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline):

```py
>>> audio_classifier("jfk_moon_speech.wav")
[&amp;lcub;'label': 'hap', 'score': 0.47071996331214905},
 &amp;lcub;'label': 'ang', 'score': 0.3070950210094452},
 &amp;lcub;'label': 'neu', 'score': 0.15184970200061798},
 &amp;lcub;'label': 'sad', 'score': 0.07033531367778778}]
```

<h2 id="vision-pipeline">Vision pipeline</h2>

Finally, using a [pipeline()](/docs/transformers/pr_15165/en/main_classes/pipelines#transformers.pipeline) for vision tasks is nearly identical.

Specify your vision task and pass your image to the classifier. For example, if you're interested in classifying the species of cat shown below:

![pipeline-cat-chonk](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg)

Pass your input as a link or a local path to the image:

```py
>>> from transformers import pipeline

>>> vision_classifier = pipeline(task="image-classification")
>>> vision_classifier(images="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg")
[&amp;lcub;'label': 'lynx, catamount', 'score': 0.4403027892112732},
 &amp;lcub;'label': 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',
  'score': 0.03433405980467796},
 &amp;lcub;'label': 'snow leopard, ounce, Panthera uncia',
  'score': 0.032148055732250214},
 &amp;lcub;'label': 'Egyptian cat', 'score': 0.02353910356760025},
 &amp;lcub;'label': 'tiger cat', 'score': 0.023034192621707916}]
```
