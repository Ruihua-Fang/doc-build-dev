---
local: preprocess
sections:
- local: nlp
  sections:
  - local: tokenize
    title: Tokenize
  - local: pad
    title: Pad
  - local: truncation
    title: Truncation
  - local: build-tensors
    title: Build tensors
  title: NLP
- local: audio
  sections:
  - local: resample
    title: Resample
  - local: feature-extractor
    title: Feature extractor
  title: Audio
- local: vision
  sections:
  - local: feature-extractor
    title: Feature extractor
  - local: data-augmentation
    title: Data augmentation
  title: Vision
- local: multimodal
  sections:
  - local: processor
    title: Processor
  title: Multimodal
- local: everything-you-always-wanted-to-know-about-padding-and-truncation
  title: Everything you always wanted to know about padding and truncation
title: Preprocess
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import ColabDropdown from "./ColabDropdown.svelte";
import IconCopyLink from "./IconCopyLink.svelte";
export let fw: "pt" | "tf"
</script>
<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="preprocess">Preprocess</h1>

<ColabDropdown hydrate-props={{
  classNames: "absolute z-10 right-0 top-0",
  options:[
    {label: "Mixed", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/preprocessing.ipynb"},
    {label: "PyTorch", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/preprocessing.ipynb"},
    {label: "TensorFlow", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/preprocessing.ipynb"},
]}} />

Before you can use your data in a model, you need to process the data into a format the model can understand. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:

* Preprocess textual data with a tokenizer.
* Preprocess image or audio data with a feature extractor.
* Preprocess data for a multimodal task with a processor.

<h2 id="nlp">NLP</h2>

<Youtube id="Yffk5aydLzg"/>

The main tool for processing textual data is a [tokenizer](main_classes/tokenizer). A tokenizer begins by splitting text into _tokens_ according to a set of rules. The _tokens_ are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer.

<Tip>

If you plan on using a pretrained model, it's important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the _vocab_) during pretraining.

</Tip>

You can quickly get started by loading a pretrained tokenizer with the [AutoTokenizer](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoTokenizer) class. This downloads the _vocab_ used during pretraining or fine-tuning a model.

<h3 id="tokenize">Tokenize</h3>

Load a pretrained tokenizer with the [AutoTokenizer.from_pretrained()](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) method:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

Then you can pass your sentence to the tokenizer:

```py
>>> encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
>>> print(encoded_input)
&amp;lcub;'input_ids': [101, 2079, 2025, 19960, 10362, 1999, 1996, 3821, 1997, 16657, 1010, 2005, 2027, 2024, 11259, 1998, 4248, 2000, 4963, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

The tokenizer returns a dictionary with three important itmes:

* [input_ids](glossary#input-ids) are the indices corresponding to each token in the sentence.
* [attention_mask](glossary#attention-mask) indicates whether a token should be attended to or not.
* [token_type_ids](glossary#token-type-ids) identifies which sequence a token belongs to when there is more than one sequence.

You can decode the `input_ids` to return the original input:

```py
>>> tokenizer.decode(encoded_input["input_ids"])
'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]'
```

As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer will automatically add them for you.

If you have several sentences you want to process, you can do this efficiently by sending the sentences as a list to the tokenizer:

```py
>>> batch_sentences = ["But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?"]
>>> encoded_inputs = tokenizer(batch_sentences)
>>> print(encoded_inputs)
&amp;lcub;'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1]]}
```

<h3 id="pad">Pad</h3>

This brings us to an important topic. When you process a batch of sentences, they aren't always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special _padding token_ to sentences with fewer tokens.

Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence:

```py
>>> batch_sentences = ["But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?"]
>>> encoded_input = tokenizer(batch_sentences, padding=True)
>>> print(encoded_input)
&amp;lcub;'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

Notice the tokenizer padded the first and third sentences with a `0` because they are shorter!

<h3 id="truncation">Truncation</h3>

On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length.

Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model:

```py
>>> batch_sentences = ["But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?"]
>>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
>>> print(encoded_input)
&amp;lcub;'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0], 
               [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102], 
               [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 
 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], 
                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}
```

<h3 id="build-tensors">Build tensors</h3>

Finally, you want the tokenizer to return the actual tensors that are fed to the model.

Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:

```py
>>> batch_sentences = batch_sentences = ["But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?"]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')
>>> print(encoded_input)
&amp;lcub;'input_ids': tensor([[  101,   153,  7719, 21490,  1122,  1114,  9582,  1623,   102],
                      [  101,  5226,  1122,  9649,  1199,  2610,  1236,   102,     0]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],
                           [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 1, 0]])}
===PT-TF-SPLIT===
>>> batch_sentences = batch_sentences = ["But what about second breakfast?", "Don't think he knows about second breakfast, Pip.", "What about elevensies?"]
>>> encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="tf")
>>> print(encoded_input)
&amp;lcub;'input_ids': &amp;lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[  101,   153,  7719, 21490,  1122,  1114,  9582,  1623,   102],
       [  101,  5226,  1122,  9649,  1199,  2610,  1236,   102,     0]],
      dtype=int32)>, 
 'token_type_ids': &amp;lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[0, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 
 'attention_mask': &amp;lt;tf.Tensor: shape=(2, 9), dtype=int32, numpy=
array([[1, 1, 1, 1, 1, 1, 1, 1, 1],
       [1, 1, 1, 1, 1, 1, 1, 1, 0]], dtype=int32)>}
```

<h2 id="audio">Audio</h2>

Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A [feature extractor](main_classes/feature_extractor) will help you with this. Before you begin, install 🤗 Datasets to load an audio dataset to experiment with.

```bash
pip install datasets
```

Load the keyword spotting task from the [SUPERB](https://huggingface.co/datasets/superb) benchmark (see the 🤗 [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub.html) for more information):

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset("superb", "ks")
```

Access the first element of the `audio` column to take a look at the input. Calling the `audio` column will automatically load and resample the audio file:

```py
>>> dataset["train"][0]["audio"]
&amp;lcub;'array': array([ 0.        ,  0.        ,  0.        , ..., -0.00592041,
        -0.00405884, -0.00253296], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/05734a36d88019a09725c20cc024e1c4e7982e37d7d55c0c1ca1742ea1cdd47f/_background_noise_/doing_the_dishes.wav',
 'sampling_rate': 16000}
```

This returns three items:

* `array` is the speech signal, or the input format the model expects.
* `path` points to the location of the audio file.
* `sampling_rate` refers to how many data points in the speech signal are measured per second.

<h3 id="resample">Resample</h3>

For this tutorial, you will use the [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data's sampling rate isn't the same, then you will need to resample your audio data. For example, if your audio data has a sampling rate of 48kHz, then you will need to downsample to 16kHz.

1. Use 🤗 Datasets `cast_column` method to downsample the sampling rate to 16kHz:

```py
>>> dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))
```

2. Now when you load the audio file, it will be automatically resampled to 16kHz:

```py
>>> dataset["train"][0]["audio"]
```

<h3 id="feature-extractor">Feature extractor</h3>

The next step is to load a feature extractor to normalize the input.

Load the feature extractor with the [AutoFeatureExtractor.from_pretrained()](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained) method:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")
```

Pass the audio `array` to the feature extractor. We also recommend you add the `sampling_rate` argument in the feature extractor in order to better debug any silent errors that may occur.

```py
>>> audio_input = [dataset["train"][0]["audio"]["array"]]
>>> feature_extractor(audio_input, sampling_rate=16000)
&amp;lcub;'input_values': [array([ 0.00045439,  0.00045439,  0.00045439, ..., -0.1578519 , -0.10807519, -0.06727459], dtype=float32)]}
```

Just like the tokenizer, you can also apply a padding or truncation strategy to handle short and long sequences in a batch:

```py
>>> feature_extractor(audio_input, sampling_rate=16000, padding=True, max_length=160000, truncation=True)
```

<h2 id="vision">Vision</h2>

A feature extractor can also be used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors for the model.

Let's load the [food101](https://huggingface.co/datasets/food101) dataset for this tutorial. Use 🤗 Datasets `split` parameter to load only a small sample of the training split since the dataset is quite large:

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("food101", split='train[:100]')
```

Next, take a look at the image with 🤗 Datasets `Image`(https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image) feature:

```py
>>> dataset[0]["image"]
```

![vision-preprocess-tutorial.png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png)

<h3 id="feature-extractor">Feature extractor</h3>

Load the feature extractor with the [AutoFeatureExtractor.from_pretrained()](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained) method:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")
```

<h3 id="data-augmentation">Data augmentation</h3>

For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you'd like, but in this tutorial, you will use torchvision's `transforms`(https://pytorch.org/vision/stable/transforms.html) module.

1. Normalize the image and use `Compose` to chain some transforms together:

```py
>>> from torchvision.transforms import CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor

>>> normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
>>> _transforms = Compose(
        [
            RandomResizedCrop(feature_extractor.size),
            RandomHorizontalFlip(),
            ToTensor(),
            normalize,
        ]
    )
```

2. The model accepts `pixel_values`(model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values) as it's input which is generated by the feature extractor. Create a function that generates the `pixel_values` from the transforms you want to apply:

```py
>>> def transforms(examples):
...    examples['pixel_values'] = [_transforms(image.convert("RGB")) for image in examples['image']]
...    return examples
```

3. Then use 🤗 Datasets `set_transform`(https://huggingface.co/docs/datasets/process.html#format-transform) to apply the transforms on-the-fly:

```py
>>> dataset.set_transform(transforms)
```

4. Now when you access the image, you will notice the feature extractor has added the model input `pixel_values`:

```py
>>> dataset[0]["image"]
&amp;lcub;'image': &amp;lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F1A7B0630D0>,
 'label': 6,
 'pixel_values': tensor([[[ 0.0353,  0.0745,  0.1216,  ..., -0.9922, -0.9922, -0.9922],
          [-0.0196,  0.0667,  0.1294,  ..., -0.9765, -0.9843, -0.9922],
          [ 0.0196,  0.0824,  0.1137,  ..., -0.9765, -0.9686, -0.8667],
          ...,
          [ 0.0275,  0.0745,  0.0510,  ..., -0.1137, -0.1216, -0.0824],
          [ 0.0667,  0.0824,  0.0667,  ..., -0.0588, -0.0745, -0.0980],
          [ 0.0353,  0.0353,  0.0431,  ..., -0.0039, -0.0039, -0.0588]],
 
         [[ 0.2078,  0.2471,  0.2863,  ..., -0.9451, -0.9373, -0.9451],
          [ 0.1608,  0.2471,  0.3098,  ..., -0.9373, -0.9451, -0.9373],
          [ 0.2078,  0.2706,  0.3020,  ..., -0.9608, -0.9373, -0.8275],
          ...,
          [-0.0353,  0.0118, -0.0039,  ..., -0.2392, -0.2471, -0.2078],
          [ 0.0196,  0.0353,  0.0196,  ..., -0.1843, -0.2000, -0.2235],
          [-0.0118, -0.0039, -0.0039,  ..., -0.0980, -0.0980, -0.1529]],
 
         [[ 0.3961,  0.4431,  0.4980,  ..., -0.9216, -0.9137, -0.9216],
          [ 0.3569,  0.4510,  0.5216,  ..., -0.9059, -0.9137, -0.9137],
          [ 0.4118,  0.4745,  0.5216,  ..., -0.9137, -0.8902, -0.7804],
          ...,
          [-0.2314, -0.1922, -0.2078,  ..., -0.4196, -0.4275, -0.3882],
          [-0.1843, -0.1686, -0.2000,  ..., -0.3647, -0.3804, -0.4039],
          [-0.1922, -0.1922, -0.1922,  ..., -0.2941, -0.2863, -0.3412]]])}
```

<h2 id="multimodal">Multimodal</h2>

Finally, for multimodal tasks you will use a combination of everything you've learned so far. In this multimodal preprocessing tutorial, you will focus on a automatic speech recognition (ASR) task. This means you will need a:

* Feature extractor to preprocess the audio data.
* Tokenizer to process the text.

Let's use the `TIMIT`(https://huggingface.co/datasets/timit_asr) dataset:

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("timit_asr", split="test")
```

For this tutorial, you are mainly interested in the `audio` and `text` column. Remove all the other columns:

```py
>>> dataset = dataset.map(remove_columns=["file", "phonetic_detail", "word_detail", "dialect_region", "sentence_type", "speaker_id", "id"])
```

Now let's take a look at the `audio` and `text` columns:

```py
>>> dataset[0]["audio"]
&amp;lcub;'array': array([ 2.4414062e-04, -3.0517578e-05,  3.0517578e-05, ...,
         6.1035156e-05,  9.1552734e-05,  6.1035156e-05], dtype=float32),
 'path': '/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TEST/DR4/MGMM0/SX139.WAV',
 'sampling_rate': 16000}

>>> dataset[0]["text"]
'The bungalow was pleasantly situated near the shore.'
```

<h3 id="processor">Processor</h3>

A processor combines the feature extractor and tokenizer. Load a processor with the [`AutoProcessor.from_pretrained] method:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")
```

1. Create a function to process the audio data to `input_values`, and to tokenize the text to `labels`. These are your inputs to the model:

```py
>>> def prepare_dataset(example):
...    audio = example["audio"]
...
...    example["input_values"] = processor(audio["array"], sampling_rate=16000)
...    
...    with processor.as_target_processor():
...            example["labels"] = processor(example["text"]).input_ids
...    return example
```

2. Apply `prepare_dataset` to your example, and you will see the processor has added `input_values` and `labels`:

```py
>>> prepare_dataset(dataset[0])
```

Awesome, you should now be able to preprocess data for any modality! In the next tutorial, you will learn how to fine-tune a model on the preprocessed data.

<h2 id="everything-you-always-wanted-to-know-about-padding-and-truncation">Everything you always wanted to know about padding and truncation</h2>

We have seen the commands that will work for most cases (pad your batch to the length of the maximum sentence and
truncate to the maximum length the model can accept). However, the API supports more strategies if you need them. The
three arguments you need to know for this are `padding`, `truncation` and `max_length`.

- `padding` controls the padding. It can be a boolean or a string which should be:

  - `True` or `'longest'` to pad to the longest sequence in the batch (doing no padding if you only provide
    a single sequence).
  - `'max_length'` to pad to a length specified by the `max_length` argument or the maximum length accepted
    by the model if no `max_length` is provided (`max_length=None`). If you only provide a single sequence,
    padding will still be applied to it.
  - `False` or `'do_not_pad'` to not pad the sequences. As we have seen before, this is the default
    behavior.

- `truncation` controls the truncation. It can be a boolean or a string which should be:

  - `True` or `'longest_first'` truncate to a maximum length specified by the `max_length` argument or
    the maximum length accepted by the model if no `max_length` is provided (`max_length=None`). This will
    truncate token by token, removing a token from the longest sequence in the pair until the proper length is
    reached.
  - `'only_second'` truncate to a maximum length specified by the `max_length` argument or the maximum
    length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate
    the second sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.
  - `'only_first'` truncate to a maximum length specified by the `max_length` argument or the maximum
    length accepted by the model if no `max_length` is provided (`max_length=None`). This will only truncate
    the first sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.
  - `False` or `'do_not_truncate'` to not truncate the sequences. As we have seen before, this is the
    default behavior.

- `max_length` to control the length of the padding/truncation. It can be an integer or `None`, in which case
  it will default to the maximum length the model can accept. If the model has no specific maximum input length,
  truncation/padding to `max_length` is deactivated.

Here is a table summarizing the recommend way to setup padding and truncation. If you use pair of inputs sequence in
any of the following examples, you can replace `truncation=True` by a `STRATEGY` selected in
`['only_first', 'only_second', 'longest_first']`, i.e. `truncation='only_second'` or `truncation= 'longest_first'` to control how both sequence in the pair are truncated as detailed before.

| Truncation                           | Padding                           | Instruction                                                                                 |
|--------------------------------------|-----------------------------------|---------------------------------------------------------------------------------------------|
| no truncation                        | no padding                        | `tokenizer(batch_sentences)`                                                           |
|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True)` or                                          |
|                                      |                                   | `tokenizer(batch_sentences, padding='longest')`                                        |
|                                      | padding to max model input length | `tokenizer(batch_sentences, padding='max_length')`                                     |
|                                      | padding to specific length        | `tokenizer(batch_sentences, padding='max_length', max_length=42)`                      |
| truncation to max model input length | no padding                        | `tokenizer(batch_sentences, truncation=True)` or                                       |
|                                      |                                   | `tokenizer(batch_sentences, truncation=STRATEGY)`                                      |
|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True, truncation=True)` or                         |
|                                      |                                   | `tokenizer(batch_sentences, padding=True, truncation=STRATEGY)`                        |
|                                      | padding to max model input length | `tokenizer(batch_sentences, padding='max_length', truncation=True)` or                 |
|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)`                |
|                                      | padding to specific length        | Not possible                                                                                |
| truncation to specific length        | no padding                        | `tokenizer(batch_sentences, truncation=True, max_length=42)` or                        |
|                                      |                                   | `tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)`                       |
|                                      | padding to max sequence in batch  | `tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)` or          |
|                                      |                                   | `tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)`         |
|                                      | padding to max model input length | Not possible                                                                                |
|                                      | padding to specific length        | `tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)` or  |
|                                      |                                   | `tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)` |
