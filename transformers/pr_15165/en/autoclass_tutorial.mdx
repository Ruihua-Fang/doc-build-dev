---
local: load-a-pretrained-instance-with-an-autoclass
sections:
- local: autotokenizer
  title: '`AutoTokenizer`'
- local: autofeatureextractor
  title: '`AutoFeatureExtractor`'
- local: autoprocessor
  title: '`AutoProcessor`'
- local: automodel
  title: '`AutoModel`'
title: Load a pretrained instance with an AutoClass
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import ColabDropdown from "./ColabDropdown.svelte";
import IconCopyLink from "./IconCopyLink.svelte";
export let fw: "pt" | "tf"
</script>
<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="load-a-pretrained-instance-with-an-autoclass">Load a pretrained instance with an AutoClass</h1>

With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of ðŸ¤— Transformers core philosophy to make the library easy, simple and flexible to use, the `AutoClasses` automatically infers and loads the correct architecture given a checkpoint. The `from_pretrained` method lets you quickly load a pretrained model for any architecture so you don't have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint (as long as it was trained for a similar task) even if the architecture is different.

<Tip>

Remember that the architecture refers to the skeleton of the model, and checkpoints are the weights for a given architecture. For example, [BERT](https://huggingface.co/bert-base-uncased) is an architecture while `bert-base-uncased` is a checkpoint. Model is a general term that can mean either architecture or checkpoint.

</Tip>

In this tutorial, you will:

* Load a pretrained tokenizer.
* Load a pretrained feature extractor.
* Load a pretrained processor.
* Load a pretrained model.

<h2 id="autotokenizer">`AutoTokenizer`</h2>

Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.

Load a tokenizer with the [AutoTokenizer.from_pretrained()](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained) method:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

Then you can tokenize your input as shown below:

```py
>>> sequence = "In a hole in the ground there lived a hobbit."
>>> print(tokenizer(sequence))
&amp;lcub;'input_ids': [101, 1999, 1037, 4920, 1999, 1996, 2598, 2045, 2973, 1037, 7570, 10322, 4183, 1012, 102], 
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

<h2 id="autofeatureextractor">`AutoFeatureExtractor`</h2>

For audio and vision tasks, you need a feature extractor to process the audio signal or image to a model's input format.

Load a feature extractor with the [AutoFeatureExtractor.from_pretrained()](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained) method:

```py
>>> from transformers import AutoFeatureExtractor

>>> feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")
```

<h2 id="autoprocessor">`AutoProcessor`</h2>

Multimodal tasks require a processor that combines two types of preprocessing tools. For example, the [LayoutLMV2](model_doc/layoutlmv2) model requires a feature extractor to handle images and a tokenizer to handle text; the processor combines both of them.

Load a processor with the [AutoProcessor.from_pretrained()](/docs/transformers/pr_15165/en/model_doc/auto#transformers.AutoProcessor.from_pretrained) method:

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")
```

<h2 id="automodel">`AutoModel`</h2>

Finally, the `AutoModelFor` classes lets you load a pretrained model for a given task (see [here](model_doc/auto) for a complete list of available tasks). For example, if you want to load a model for sequence classification you should use `AutoModelForSequenceClassification.from_pretrained()`:

```py
>>> from transformers import AutoModelForSequenceClassification

>>> model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
===PT-TF-SPLIT===
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
```

You can easily reuse the same checkpoint to load an architecture for a different task:

```py
>>> from transformers import AutoModelForTokenClassification

>>> model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")
===PT-TF-SPLIT===
>>> from transformers import TFAutoModelForTokenClassification

>>> model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")
```
