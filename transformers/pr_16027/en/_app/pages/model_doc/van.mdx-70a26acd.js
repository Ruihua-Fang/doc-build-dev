import{S as ko,i as Fo,s as Po,e as o,k as d,w,t as r,M as Io,c as n,d as a,m as h,a as s,x as b,h as i,b as l,F as t,g as f,y as $,q as y,o as V,B as x}from"../../chunks/vendor-4833417e.js";import{T as Mo}from"../../chunks/Tip-fffd6df1.js";import{D as et}from"../../chunks/Docstring-4f315ed9.js";import{C as Ka}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as tt}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function qo(Y){let p,E,m,u,j;return{c(){p=o("p"),E=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=o("code"),u=r("Module"),j=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=n(g,"P",{});var _=s(p);E=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n(_,"CODE",{});var M=s(m);u=i(M,"Module"),M.forEach(a),j=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(g,_){f(g,p,_),t(p,E),t(p,m),t(m,u),t(p,j)},d(g){g&&a(p)}}}function No(Y){let p,E,m,u,j;return{c(){p=o("p"),E=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=o("code"),u=r("Module"),j=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){p=n(g,"P",{});var _=s(p);E=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n(_,"CODE",{});var M=s(m);u=i(M,"Module"),M.forEach(a),j=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(g,_){f(g,p,_),t(p,E),t(p,m),t(m,u),t(p,j)},d(g){g&&a(p)}}}function zo(Y){let p,E,m,u,j,g,_,M,Ct,at,N,U,De,ee,Tt,Le,jt,ot,K,At,te,Mt,kt,nt,Ve,Ft,st,xe,Pt,rt,R,It,ae,qt,Nt,it,Ee,zt,lt,Ce,oe,Dt,Se,Lt,St,ct,F,Ot,ne,Ht,Wt,se,Ut,Kt,dt,z,Z,Oe,re,Rt,He,Zt,ht,v,ie,Gt,D,Bt,Te,Xt,Jt,le,Qt,Yt,ea,L,ta,je,aa,oa,Ae,na,sa,ra,We,ia,la,ce,ft,S,G,Ue,de,ca,Ke,da,pt,k,he,ha,fe,fa,pe,pa,ma,ua,C,me,ga,O,_a,Me,va,wa,Re,ba,$a,ya,B,Va,Ze,xa,Ea,ue,mt,H,X,Ge,ge,Ca,Be,Ta,ut,A,_e,ja,Xe,Aa,Ma,ve,ka,we,Fa,Pa,Ia,T,be,qa,W,Na,ke,za,Da,Je,La,Sa,Oa,J,Ha,Qe,Wa,Ua,$e,gt;return g=new tt({}),ee=new tt({}),re=new tt({}),ie=new et({props:{name:"class transformers.VanConfig",anchor:"transformers.VanConfig",parameters:[{name:"image_size",val:" = 224"},{name:"num_channels",val:" = 3"},{name:"patch_sizes",val:" = [7, 3, 3, 3]"},{name:"strides",val:" = [4, 2, 2, 2]"},{name:"hidden_sizes",val:" = [64, 128, 320, 512]"},{name:"depths",val:" = [3, 3, 12, 3]"},{name:"mlp_expansions",val:" = [8, 8, 4, 4]"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"layer_scale_init_value",val:" = 0.01"},{name:"drop_path_rate",val:" = 0.0"},{name:"dropout_rate",val:" = 0.0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/configuration_van.py#L28",parametersDescription:[{anchor:"transformers.VanConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The image size the network is train on.`,name:"image_size"},{anchor:"transformers.VanConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VanConfig.patch_sizes",description:`<strong>patch_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [7, 3, 3, 3]) &#x2014;
Patch size to use in each stage&#x2019;s embedding layer.`,name:"patch_sizes"},{anchor:"transformers.VanConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [4, 2, 2, 2]) &#x2014;
Stride size to use in each stage&#x2019;s embedding layer allowing to downsample the input.`,name:"strides"},{anchor:"transformers.VanConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [64, 128, 320, 512]) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.VanConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [3, 3, 12, 3]) &#x2014;
Depth (number of blocks) for each stage.`,name:"depths"},{anchor:"transformers.VanConfig.mlp_expansions",description:`<strong>mlp_expansions</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [8, 8, 4, 4]) &#x2014;
The expansion factor for mlp layer at each stage.`,name:"mlp_expansions"},{anchor:"transformers.VanConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VanConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VanConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VanConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-2) &#x2014;
The initial value for the layer scale.`,name:"layer_scale_init_value"},{anchor:"transformers.VanConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The drop rate for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.VanConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The drop rate for dropout.`,name:"dropout_rate"}]}}),ce=new Ka({props:{code:`from transformers import VanModel, VanConfig

# Initializing a Van van-base style configuration
configuration = VanConfig()
# Initializing a model from the van-base style configuration
model = VanModel(configuration)
# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VanModel, VanConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Van van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VanConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),de=new tt({}),he=new et({props:{name:"class transformers.VanModel",anchor:"transformers.VanModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L460",parametersDescription:[{anchor:"transformers.VanModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16027/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),me=new et({props:{name:"forward",anchor:"transformers.VanModel.forward",parameters:[{name:"pixel_values",val:""},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L470",parametersDescription:[{anchor:"transformers.VanModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16027/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code>for details.`,name:"pixel_values"},{anchor:"transformers.VanModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16027/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.van.modeling_van.VanModelOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) \u2014 Last hidden states (final feature map) of the last stage of the model.</li>
<li><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.dim[-1])</code>) \u2014 Global average pooling of the last feature map followed by a layernorm.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called feature maps) of the model at the output of each stage.</li>
</ul>
`}}),B=new Mo({props:{$$slots:{default:[qo]},$$scope:{ctx:Y}}}),ue=new Ka({props:{code:`from transformers import AutoFeatureExtractor, VanModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("van-base")
model = VanModel.from_pretrained("van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`}}),ge=new tt({}),_e=new et({props:{name:"class transformers.VanForImageClassification",anchor:"transformers.VanForImageClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L511",parametersDescription:[{anchor:"transformers.VanForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16027/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),be=new et({props:{name:"forward",anchor:"transformers.VanForImageClassification.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"labels",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L548",parametersDescription:[{anchor:"transformers.VanForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16027/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code>for details.`,name:"pixel_values"},{anchor:"transformers.VanForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16027/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.VanForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>transformers.models.van.modeling_van.VanClassifierOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called feature maps) of the model at the output of each stage.</li>
</ul>
`}}),J=new Mo({props:{$$slots:{default:[No]},$$scope:{ctx:Y}}}),$e=new Ka({props:{code:`from transformers import AutoFeatureExtractor, VanForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("van-base")
model = VanForImageClassification.from_pretrained("van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanForImageClassification.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`}}),{c(){p=o("meta"),E=d(),m=o("h1"),u=o("a"),j=o("span"),w(g.$$.fragment),_=d(),M=o("span"),Ct=r("van"),at=d(),N=o("h2"),U=o("a"),De=o("span"),w(ee.$$.fragment),Tt=d(),Le=o("span"),jt=r("Overview"),ot=d(),K=o("p"),At=r("The van model was proposed in "),te=o("a"),Mt=r("Visual Attention Network"),kt=r(" by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),nt=d(),Ve=o("p"),Ft=r("This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernels convolution layers. The later uses a dilated convolution to capture distant correlations."),st=d(),xe=o("p"),Pt=r("The abstract from the paper is the following:"),rt=d(),R=o("p"),It=r("While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),ae=o("a"),qt=r("this https URL"),Nt=r("."),it=d(),Ee=o("p"),zt=r("Tips:"),lt=d(),Ce=o("ul"),oe=o("li"),Dt=r("Van doesn not have an embedding layer, thus the "),Se=o("code"),Lt=r("hidden_states"),St=r(" will have a lenght equal to the number of stages."),ct=d(),F=o("p"),Ot=r("This model was contributed by "),ne=o("a"),Ht=r("Francesco"),Wt=r(". The original code can be found "),se=o("a"),Ut=r("here"),Kt=r("."),dt=d(),z=o("h2"),Z=o("a"),Oe=o("span"),w(re.$$.fragment),Rt=d(),He=o("span"),Zt=r("VanConfig"),ht=d(),v=o("div"),w(ie.$$.fragment),Gt=d(),D=o("p"),Bt=r("This is the configuration class to store the configuration of a "),Te=o("a"),Xt=r("VanModel"),Jt=r(`. It is used to instantiate a Van model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the ConvNeXT `),le=o("a"),Qt=r("van-base"),Yt=r(`
architecture.`),ea=d(),L=o("p"),ta=r("Configuration objects inherit from "),je=o("a"),aa=r("PretrainedConfig"),oa=r(` and can be used to control the model outputs. Read the
documentation from `),Ae=o("a"),na=r("PretrainedConfig"),sa=r(" for more information."),ra=d(),We=o("p"),ia=r("Example:"),la=d(),w(ce.$$.fragment),ft=d(),S=o("h2"),G=o("a"),Ue=o("span"),w(de.$$.fragment),ca=d(),Ke=o("span"),da=r("VanModel"),pt=d(),k=o("div"),w(he.$$.fragment),ha=d(),fe=o("p"),fa=r(`The bare Van model outputting raw features without any specific head on top. Note, van does not have an embedding layer.
This model is a PyTorch `),pe=o("a"),pa=r("torch.nn.Module"),ma=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ua=d(),C=o("div"),w(me.$$.fragment),ga=d(),O=o("p"),_a=r("The "),Me=o("a"),va=r("VanModel"),wa=r(" forward method, overrides the "),Re=o("code"),ba=r("__call__"),$a=r(" special method."),ya=d(),w(B.$$.fragment),Va=d(),Ze=o("p"),xa=r("Example:"),Ea=d(),w(ue.$$.fragment),mt=d(),H=o("h2"),X=o("a"),Ge=o("span"),w(ge.$$.fragment),Ca=d(),Be=o("span"),Ta=r("VanForImageClassification"),ut=d(),A=o("div"),w(_e.$$.fragment),ja=d(),Xe=o("p"),Aa=r(`Van Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Ma=d(),ve=o("p"),ka=r("This model is a PyTorch "),we=o("a"),Fa=r("torch.nn.Module"),Pa=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ia=d(),T=o("div"),w(be.$$.fragment),qa=d(),W=o("p"),Na=r("The "),ke=o("a"),za=r("VanForImageClassification"),Da=r(" forward method, overrides the "),Je=o("code"),La=r("__call__"),Sa=r(" special method."),Oa=d(),w(J.$$.fragment),Ha=d(),Qe=o("p"),Wa=r("Example:"),Ua=d(),w($e.$$.fragment),this.h()},l(e){const c=Io('[data-svelte="svelte-1phssyn"]',document.head);p=n(c,"META",{name:!0,content:!0}),c.forEach(a),E=h(e),m=n(e,"H1",{class:!0});var ye=s(m);u=n(ye,"A",{id:!0,class:!0,href:!0});var Ye=s(u);j=n(Ye,"SPAN",{});var Ra=s(j);b(g.$$.fragment,Ra),Ra.forEach(a),Ye.forEach(a),_=h(ye),M=n(ye,"SPAN",{});var Za=s(M);Ct=i(Za,"van"),Za.forEach(a),ye.forEach(a),at=h(e),N=n(e,"H2",{class:!0});var _t=s(N);U=n(_t,"A",{id:!0,class:!0,href:!0});var Ga=s(U);De=n(Ga,"SPAN",{});var Ba=s(De);b(ee.$$.fragment,Ba),Ba.forEach(a),Ga.forEach(a),Tt=h(_t),Le=n(_t,"SPAN",{});var Xa=s(Le);jt=i(Xa,"Overview"),Xa.forEach(a),_t.forEach(a),ot=h(e),K=n(e,"P",{});var vt=s(K);At=i(vt,"The van model was proposed in "),te=n(vt,"A",{href:!0,rel:!0});var Ja=s(te);Mt=i(Ja,"Visual Attention Network"),Ja.forEach(a),kt=i(vt," by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),vt.forEach(a),nt=h(e),Ve=n(e,"P",{});var Qa=s(Ve);Ft=i(Qa,"This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernels convolution layers. The later uses a dilated convolution to capture distant correlations."),Qa.forEach(a),st=h(e),xe=n(e,"P",{});var Ya=s(xe);Pt=i(Ya,"The abstract from the paper is the following:"),Ya.forEach(a),rt=h(e),R=n(e,"P",{});var wt=s(R);It=i(wt,"While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),ae=n(wt,"A",{href:!0,rel:!0});var eo=s(ae);qt=i(eo,"this https URL"),eo.forEach(a),Nt=i(wt,"."),wt.forEach(a),it=h(e),Ee=n(e,"P",{});var to=s(Ee);zt=i(to,"Tips:"),to.forEach(a),lt=h(e),Ce=n(e,"UL",{});var ao=s(Ce);oe=n(ao,"LI",{});var bt=s(oe);Dt=i(bt,"Van doesn not have an embedding layer, thus the "),Se=n(bt,"CODE",{});var oo=s(Se);Lt=i(oo,"hidden_states"),oo.forEach(a),St=i(bt," will have a lenght equal to the number of stages."),bt.forEach(a),ao.forEach(a),ct=h(e),F=n(e,"P",{});var Fe=s(F);Ot=i(Fe,"This model was contributed by "),ne=n(Fe,"A",{href:!0,rel:!0});var no=s(ne);Ht=i(no,"Francesco"),no.forEach(a),Wt=i(Fe,". The original code can be found "),se=n(Fe,"A",{href:!0,rel:!0});var so=s(se);Ut=i(so,"here"),so.forEach(a),Kt=i(Fe,"."),Fe.forEach(a),dt=h(e),z=n(e,"H2",{class:!0});var $t=s(z);Z=n($t,"A",{id:!0,class:!0,href:!0});var ro=s(Z);Oe=n(ro,"SPAN",{});var io=s(Oe);b(re.$$.fragment,io),io.forEach(a),ro.forEach(a),Rt=h($t),He=n($t,"SPAN",{});var lo=s(He);Zt=i(lo,"VanConfig"),lo.forEach(a),$t.forEach(a),ht=h(e),v=n(e,"DIV",{class:!0});var P=s(v);b(ie.$$.fragment,P),Gt=h(P),D=n(P,"P",{});var Pe=s(D);Bt=i(Pe,"This is the configuration class to store the configuration of a "),Te=n(Pe,"A",{href:!0});var co=s(Te);Xt=i(co,"VanModel"),co.forEach(a),Jt=i(Pe,`. It is used to instantiate a Van model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the ConvNeXT `),le=n(Pe,"A",{href:!0,rel:!0});var ho=s(le);Qt=i(ho,"van-base"),ho.forEach(a),Yt=i(Pe,`
architecture.`),Pe.forEach(a),ea=h(P),L=n(P,"P",{});var Ie=s(L);ta=i(Ie,"Configuration objects inherit from "),je=n(Ie,"A",{href:!0});var fo=s(je);aa=i(fo,"PretrainedConfig"),fo.forEach(a),oa=i(Ie,` and can be used to control the model outputs. Read the
documentation from `),Ae=n(Ie,"A",{href:!0});var po=s(Ae);na=i(po,"PretrainedConfig"),po.forEach(a),sa=i(Ie," for more information."),Ie.forEach(a),ra=h(P),We=n(P,"P",{});var mo=s(We);ia=i(mo,"Example:"),mo.forEach(a),la=h(P),b(ce.$$.fragment,P),P.forEach(a),ft=h(e),S=n(e,"H2",{class:!0});var yt=s(S);G=n(yt,"A",{id:!0,class:!0,href:!0});var uo=s(G);Ue=n(uo,"SPAN",{});var go=s(Ue);b(de.$$.fragment,go),go.forEach(a),uo.forEach(a),ca=h(yt),Ke=n(yt,"SPAN",{});var _o=s(Ke);da=i(_o,"VanModel"),_o.forEach(a),yt.forEach(a),pt=h(e),k=n(e,"DIV",{class:!0});var qe=s(k);b(he.$$.fragment,qe),ha=h(qe),fe=n(qe,"P",{});var Vt=s(fe);fa=i(Vt,`The bare Van model outputting raw features without any specific head on top. Note, van does not have an embedding layer.
This model is a PyTorch `),pe=n(Vt,"A",{href:!0,rel:!0});var vo=s(pe);pa=i(vo,"torch.nn.Module"),vo.forEach(a),ma=i(Vt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Vt.forEach(a),ua=h(qe),C=n(qe,"DIV",{class:!0});var I=s(C);b(me.$$.fragment,I),ga=h(I),O=n(I,"P",{});var Ne=s(O);_a=i(Ne,"The "),Me=n(Ne,"A",{href:!0});var wo=s(Me);va=i(wo,"VanModel"),wo.forEach(a),wa=i(Ne," forward method, overrides the "),Re=n(Ne,"CODE",{});var bo=s(Re);ba=i(bo,"__call__"),bo.forEach(a),$a=i(Ne," special method."),Ne.forEach(a),ya=h(I),b(B.$$.fragment,I),Va=h(I),Ze=n(I,"P",{});var $o=s(Ze);xa=i($o,"Example:"),$o.forEach(a),Ea=h(I),b(ue.$$.fragment,I),I.forEach(a),qe.forEach(a),mt=h(e),H=n(e,"H2",{class:!0});var xt=s(H);X=n(xt,"A",{id:!0,class:!0,href:!0});var yo=s(X);Ge=n(yo,"SPAN",{});var Vo=s(Ge);b(ge.$$.fragment,Vo),Vo.forEach(a),yo.forEach(a),Ca=h(xt),Be=n(xt,"SPAN",{});var xo=s(Be);Ta=i(xo,"VanForImageClassification"),xo.forEach(a),xt.forEach(a),ut=h(e),A=n(e,"DIV",{class:!0});var Q=s(A);b(_e.$$.fragment,Q),ja=h(Q),Xe=n(Q,"P",{});var Eo=s(Xe);Aa=i(Eo,`Van Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Eo.forEach(a),Ma=h(Q),ve=n(Q,"P",{});var Et=s(ve);ka=i(Et,"This model is a PyTorch "),we=n(Et,"A",{href:!0,rel:!0});var Co=s(we);Fa=i(Co,"torch.nn.Module"),Co.forEach(a),Pa=i(Et,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Et.forEach(a),Ia=h(Q),T=n(Q,"DIV",{class:!0});var q=s(T);b(be.$$.fragment,q),qa=h(q),W=n(q,"P",{});var ze=s(W);Na=i(ze,"The "),ke=n(ze,"A",{href:!0});var To=s(ke);za=i(To,"VanForImageClassification"),To.forEach(a),Da=i(ze," forward method, overrides the "),Je=n(ze,"CODE",{});var jo=s(Je);La=i(jo,"__call__"),jo.forEach(a),Sa=i(ze," special method."),ze.forEach(a),Oa=h(q),b(J.$$.fragment,q),Ha=h(q),Qe=n(q,"P",{});var Ao=s(Qe);Wa=i(Ao,"Example:"),Ao.forEach(a),Ua=h(q),b($e.$$.fragment,q),q.forEach(a),Q.forEach(a),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(Do)),l(u,"id","van"),l(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(u,"href","#van"),l(m,"class","relative group"),l(U,"id","overview"),l(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(U,"href","#overview"),l(N,"class","relative group"),l(te,"href","https://arxiv.org/abs/2202.09741"),l(te,"rel","nofollow"),l(ae,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),l(ae,"rel","nofollow"),l(ne,"href","https://huggingface.co/Francesco"),l(ne,"rel","nofollow"),l(se,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),l(se,"rel","nofollow"),l(Z,"id","transformers.VanConfig"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#transformers.VanConfig"),l(z,"class","relative group"),l(Te,"href","/docs/transformers/pr_16027/en/model_doc/van#transformers.VanModel"),l(le,"href","https://huggingface.co/van-base"),l(le,"rel","nofollow"),l(je,"href","/docs/transformers/pr_16027/en/main_classes/configuration#transformers.PretrainedConfig"),l(Ae,"href","/docs/transformers/pr_16027/en/main_classes/configuration#transformers.PretrainedConfig"),l(v,"class","docstring"),l(G,"id","transformers.VanModel"),l(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(G,"href","#transformers.VanModel"),l(S,"class","relative group"),l(pe,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(pe,"rel","nofollow"),l(Me,"href","/docs/transformers/pr_16027/en/model_doc/van#transformers.VanModel"),l(C,"class","docstring"),l(k,"class","docstring"),l(X,"id","transformers.VanForImageClassification"),l(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(X,"href","#transformers.VanForImageClassification"),l(H,"class","relative group"),l(we,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(we,"rel","nofollow"),l(ke,"href","/docs/transformers/pr_16027/en/model_doc/van#transformers.VanForImageClassification"),l(T,"class","docstring"),l(A,"class","docstring")},m(e,c){t(document.head,p),f(e,E,c),f(e,m,c),t(m,u),t(u,j),$(g,j,null),t(m,_),t(m,M),t(M,Ct),f(e,at,c),f(e,N,c),t(N,U),t(U,De),$(ee,De,null),t(N,Tt),t(N,Le),t(Le,jt),f(e,ot,c),f(e,K,c),t(K,At),t(K,te),t(te,Mt),t(K,kt),f(e,nt,c),f(e,Ve,c),t(Ve,Ft),f(e,st,c),f(e,xe,c),t(xe,Pt),f(e,rt,c),f(e,R,c),t(R,It),t(R,ae),t(ae,qt),t(R,Nt),f(e,it,c),f(e,Ee,c),t(Ee,zt),f(e,lt,c),f(e,Ce,c),t(Ce,oe),t(oe,Dt),t(oe,Se),t(Se,Lt),t(oe,St),f(e,ct,c),f(e,F,c),t(F,Ot),t(F,ne),t(ne,Ht),t(F,Wt),t(F,se),t(se,Ut),t(F,Kt),f(e,dt,c),f(e,z,c),t(z,Z),t(Z,Oe),$(re,Oe,null),t(z,Rt),t(z,He),t(He,Zt),f(e,ht,c),f(e,v,c),$(ie,v,null),t(v,Gt),t(v,D),t(D,Bt),t(D,Te),t(Te,Xt),t(D,Jt),t(D,le),t(le,Qt),t(D,Yt),t(v,ea),t(v,L),t(L,ta),t(L,je),t(je,aa),t(L,oa),t(L,Ae),t(Ae,na),t(L,sa),t(v,ra),t(v,We),t(We,ia),t(v,la),$(ce,v,null),f(e,ft,c),f(e,S,c),t(S,G),t(G,Ue),$(de,Ue,null),t(S,ca),t(S,Ke),t(Ke,da),f(e,pt,c),f(e,k,c),$(he,k,null),t(k,ha),t(k,fe),t(fe,fa),t(fe,pe),t(pe,pa),t(fe,ma),t(k,ua),t(k,C),$(me,C,null),t(C,ga),t(C,O),t(O,_a),t(O,Me),t(Me,va),t(O,wa),t(O,Re),t(Re,ba),t(O,$a),t(C,ya),$(B,C,null),t(C,Va),t(C,Ze),t(Ze,xa),t(C,Ea),$(ue,C,null),f(e,mt,c),f(e,H,c),t(H,X),t(X,Ge),$(ge,Ge,null),t(H,Ca),t(H,Be),t(Be,Ta),f(e,ut,c),f(e,A,c),$(_e,A,null),t(A,ja),t(A,Xe),t(Xe,Aa),t(A,Ma),t(A,ve),t(ve,ka),t(ve,we),t(we,Fa),t(ve,Pa),t(A,Ia),t(A,T),$(be,T,null),t(T,qa),t(T,W),t(W,Na),t(W,ke),t(ke,za),t(W,Da),t(W,Je),t(Je,La),t(W,Sa),t(T,Oa),$(J,T,null),t(T,Ha),t(T,Qe),t(Qe,Wa),t(T,Ua),$($e,T,null),gt=!0},p(e,[c]){const ye={};c&2&&(ye.$$scope={dirty:c,ctx:e}),B.$set(ye);const Ye={};c&2&&(Ye.$$scope={dirty:c,ctx:e}),J.$set(Ye)},i(e){gt||(y(g.$$.fragment,e),y(ee.$$.fragment,e),y(re.$$.fragment,e),y(ie.$$.fragment,e),y(ce.$$.fragment,e),y(de.$$.fragment,e),y(he.$$.fragment,e),y(me.$$.fragment,e),y(B.$$.fragment,e),y(ue.$$.fragment,e),y(ge.$$.fragment,e),y(_e.$$.fragment,e),y(be.$$.fragment,e),y(J.$$.fragment,e),y($e.$$.fragment,e),gt=!0)},o(e){V(g.$$.fragment,e),V(ee.$$.fragment,e),V(re.$$.fragment,e),V(ie.$$.fragment,e),V(ce.$$.fragment,e),V(de.$$.fragment,e),V(he.$$.fragment,e),V(me.$$.fragment,e),V(B.$$.fragment,e),V(ue.$$.fragment,e),V(ge.$$.fragment,e),V(_e.$$.fragment,e),V(be.$$.fragment,e),V(J.$$.fragment,e),V($e.$$.fragment,e),gt=!1},d(e){a(p),e&&a(E),e&&a(m),x(g),e&&a(at),e&&a(N),x(ee),e&&a(ot),e&&a(K),e&&a(nt),e&&a(Ve),e&&a(st),e&&a(xe),e&&a(rt),e&&a(R),e&&a(it),e&&a(Ee),e&&a(lt),e&&a(Ce),e&&a(ct),e&&a(F),e&&a(dt),e&&a(z),x(re),e&&a(ht),e&&a(v),x(ie),x(ce),e&&a(ft),e&&a(S),x(de),e&&a(pt),e&&a(k),x(he),x(me),x(B),x(ue),e&&a(mt),e&&a(H),x(ge),e&&a(ut),e&&a(A),x(_e),x(be),x(J),x($e)}}}const Do={local:"van",sections:[{local:"overview",title:"Overview"},{local:"transformers.VanConfig",title:"VanConfig"},{local:"transformers.VanModel",title:"VanModel"},{local:"transformers.VanForImageClassification",title:"VanForImageClassification"}],title:"van"};function Lo(Y,p,E){let{fw:m}=p;return Y.$$set=u=>{"fw"in u&&E(0,m=u.fw)},[m]}class Ro extends ko{constructor(p){super();Fo(this,p,Lo,zo,Po,{fw:0})}}export{Ro as default,Do as metadata};
