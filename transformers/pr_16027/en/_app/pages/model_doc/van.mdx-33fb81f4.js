import{S as No,i as Po,s as Io,e as o,k as d,w as b,t as r,M as qo,c as n,d as a,m as h,a as s,x as w,h as i,b as l,F as t,g as p,y as $,q as y,o as V,B as x}from"../../chunks/vendor-4833417e.js";import{T as Fo}from"../../chunks/Tip-fffd6df1.js";import{D as tt}from"../../chunks/Docstring-4f315ed9.js";import{C as Ra}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as at}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function zo(X){let f,E,m,u,T;return{c(){f=o("p"),E=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=o("code"),u=r("Module"),T=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){f=n(g,"P",{});var _=s(f);E=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n(_,"CODE",{});var M=s(m);u=i(M,"Module"),M.forEach(a),T=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(g,_){p(g,f,_),t(f,E),t(f,m),t(m,u),t(f,T)},d(g){g&&a(f)}}}function Do(X){let f,E,m,u,T;return{c(){f=o("p"),E=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=o("code"),u=r("Module"),T=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(g){f=n(g,"P",{});var _=s(f);E=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n(_,"CODE",{});var M=s(m);u=i(M,"Module"),M.forEach(a),T=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(a)},m(g,_){p(g,f,_),t(f,E),t(f,m),t(m,u),t(f,T)},d(g){g&&a(f)}}}function Lo(X){let f,E,m,u,T,g,_,M,At,ot,q,U,Le,Y,Tt,Se,jt,nt,K,Mt,ee,kt,Ft,st,Ve,Nt,rt,xe,Pt,it,Ee,te,It,ae,qt,zt,lt,Ce,Dt,ct,Ae,oe,Lt,Oe,St,Ot,dt,F,Ht,ne,Wt,Ut,se,Kt,Rt,ht,z,R,He,re,Zt,We,Gt,pt,v,ie,Bt,D,Jt,Te,Qt,Xt,le,Yt,ea,ta,L,aa,je,oa,na,Me,sa,ra,ia,Ue,la,ca,ce,ft,S,Z,Ke,de,da,Re,ha,mt,k,he,pa,pe,fa,fe,ma,ua,ga,C,me,_a,O,va,ke,ba,wa,Ze,$a,ya,Va,G,xa,Ge,Ea,Ca,ue,ut,H,B,Be,ge,Aa,Je,Ta,gt,j,_e,ja,Qe,Ma,ka,ve,Fa,be,Na,Pa,Ia,A,we,qa,W,za,Fe,Da,La,Xe,Sa,Oa,Ha,J,Wa,Ye,Ua,Ka,$e,_t;return g=new at({}),Y=new at({}),re=new at({}),ie=new tt({props:{name:"class transformers.VanConfig",anchor:"transformers.VanConfig",parameters:[{name:"image_size",val:" = 224"},{name:"num_channels",val:" = 3"},{name:"patch_sizes",val:" = [7, 3, 3, 3]"},{name:"strides",val:" = [4, 2, 2, 2]"},{name:"hidden_sizes",val:" = [64, 128, 320, 512]"},{name:"depths",val:" = [3, 3, 12, 3]"},{name:"mlp_expansions",val:" = [8, 8, 4, 4]"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"layer_scale_init_value",val:" = 0.01"},{name:"drop_path_rate",val:" = 0.0"},{name:"dropout_rate",val:" = 0.0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/configuration_van.py#L28",parametersDescription:[{anchor:"transformers.VanConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.VanConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VanConfig.patch_sizes",description:`<strong>patch_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[7, 3, 3, 3]</code>) &#x2014;
Patch size to use in each stage&#x2019;s embedding layer.`,name:"patch_sizes"},{anchor:"transformers.VanConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 2, 2]</code>) &#x2014;
Stride size to use in each stage&#x2019;s embedding layer to downsample the input.`,name:"strides"},{anchor:"transformers.VanConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[64, 128, 320, 512]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.VanConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 12, 3]</code>) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.VanConfig.mlp_expansions",description:`<strong>mlp_expansions</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[8, 8, 4, 4]</code>) &#x2014;
The expansion factor for mlp layer at each stage.`,name:"mlp_expansions"},{anchor:"transformers.VanConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each layer. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VanConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VanConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VanConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-2) &#x2014;
The initial value for layer scaling.`,name:"layer_scale_init_value"},{anchor:"transformers.VanConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.VanConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for dropout.`,name:"dropout_rate"}]}}),ce=new Ra({props:{code:`from transformers import VanModel, VanConfig

# Initializing a VAN van-base style configuration
configuration = VanConfig()
# Initializing a model from the van-base style configuration
model = VanModel(configuration)
# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VanModel, VanConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VAN van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VanConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),de=new at({}),he=new tt({props:{name:"class transformers.VanModel",anchor:"transformers.VanModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L452",parametersDescription:[{anchor:"transformers.VanModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16027/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),me=new tt({props:{name:"forward",anchor:"transformers.VanModel.forward",parameters:[{name:"pixel_values",val:""},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L462",parametersDescription:[{anchor:"transformers.VanModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16027/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code>for details.`,name:"pixel_values"},{anchor:"transformers.VanModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16027/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.van.modeling_van.VanModelOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) \u2014 Last hidden states (final feature map) of the last stage of the model.</li>
<li><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.hidden_sizes[-1])</code>) \u2014 Global average pooling of the last feature map followed by a layernorm.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><code>transformers.models.van.modeling_van.VanModelOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new Fo({props:{$$slots:{default:[zo]},$$scope:{ctx:X}}}),ue=new Ra({props:{code:`from transformers import AutoFeatureExtractor, VanModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("van-base")
model = VanModel.from_pretrained("van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`}}),ge=new at({}),_e=new tt({props:{name:"class transformers.VanForImageClassification",anchor:"transformers.VanForImageClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L503",parametersDescription:[{anchor:"transformers.VanForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_16027/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),we=new tt({props:{name:"forward",anchor:"transformers.VanForImageClassification.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"labels",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_16027/src/transformers/models/van/modeling_van.py#L540",parametersDescription:[{anchor:"transformers.VanForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/pr_16027/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code>for details.`,name:"pixel_values"},{anchor:"transformers.VanForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_16027/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.VanForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>transformers.models.van.modeling_van.VanClassifierOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_16027/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><code>transformers.models.van.modeling_van.VanClassifierOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),J=new Fo({props:{$$slots:{default:[Do]},$$scope:{ctx:X}}}),$e=new Ra({props:{code:`from transformers import AutoFeatureExtractor, VanForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("van-base")
model = VanForImageClassification.from_pretrained("van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanForImageClassification.from_pretrained(<span class="hljs-string">&quot;van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`}}),{c(){f=o("meta"),E=d(),m=o("h1"),u=o("a"),T=o("span"),b(g.$$.fragment),_=d(),M=o("span"),At=r("VAN"),ot=d(),q=o("h2"),U=o("a"),Le=o("span"),b(Y.$$.fragment),Tt=d(),Se=o("span"),jt=r("Overview"),nt=d(),K=o("p"),Mt=r("The VAN model was proposed in "),ee=o("a"),kt=r("Visual Attention Network"),Ft=r(" by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),st=d(),Ve=o("p"),Nt=r("This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations."),rt=d(),xe=o("p"),Pt=r("The abstract from the paper is the following:"),it=d(),Ee=o("p"),te=o("em"),It=r("While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),ae=o("a"),qt=r("this https URL"),zt=r("."),lt=d(),Ce=o("p"),Dt=r("Tips:"),ct=d(),Ae=o("ul"),oe=o("li"),Lt=r("VAN does not have an embedding layer, thus the "),Oe=o("code"),St=r("hidden_states"),Ot=r(" will have a length equal to the number of stages."),dt=d(),F=o("p"),Ht=r("This model was contributed by "),ne=o("a"),Wt=r("Francesco"),Ut=r(". The original code can be found "),se=o("a"),Kt=r("here"),Rt=r("."),ht=d(),z=o("h2"),R=o("a"),He=o("span"),b(re.$$.fragment),Zt=d(),We=o("span"),Gt=r("VanConfig"),pt=d(),v=o("div"),b(ie.$$.fragment),Bt=d(),D=o("p"),Jt=r("This is the configuration class to store the configuration of a "),Te=o("a"),Qt=r("VanModel"),Xt=r(`. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN `),le=o("a"),Yt=r("van-base"),ea=r(`
architecture.`),ta=d(),L=o("p"),aa=r("Configuration objects inherit from "),je=o("a"),oa=r("PretrainedConfig"),na=r(` and can be used to control the model outputs. Read the
documentation from `),Me=o("a"),sa=r("PretrainedConfig"),ra=r(" for more information."),ia=d(),Ue=o("p"),la=r("Example:"),ca=d(),b(ce.$$.fragment),ft=d(),S=o("h2"),Z=o("a"),Ke=o("span"),b(de.$$.fragment),da=d(),Re=o("span"),ha=r("VanModel"),mt=d(),k=o("div"),b(he.$$.fragment),pa=d(),pe=o("p"),fa=r(`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch `),fe=o("a"),ma=r("torch.nn.Module"),ua=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ga=d(),C=o("div"),b(me.$$.fragment),_a=d(),O=o("p"),va=r("The "),ke=o("a"),ba=r("VanModel"),wa=r(" forward method, overrides the "),Ze=o("code"),$a=r("__call__"),ya=r(" special method."),Va=d(),b(G.$$.fragment),xa=d(),Ge=o("p"),Ea=r("Example:"),Ca=d(),b(ue.$$.fragment),ut=d(),H=o("h2"),B=o("a"),Be=o("span"),b(ge.$$.fragment),Aa=d(),Je=o("span"),Ta=r("VanForImageClassification"),gt=d(),j=o("div"),b(_e.$$.fragment),ja=d(),Qe=o("p"),Ma=r(`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),ka=d(),ve=o("p"),Fa=r("This model is a PyTorch "),be=o("a"),Na=r("torch.nn.Module"),Pa=r(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ia=d(),A=o("div"),b(we.$$.fragment),qa=d(),W=o("p"),za=r("The "),Fe=o("a"),Da=r("VanForImageClassification"),La=r(" forward method, overrides the "),Xe=o("code"),Sa=r("__call__"),Oa=r(" special method."),Ha=d(),b(J.$$.fragment),Wa=d(),Ye=o("p"),Ua=r("Example:"),Ka=d(),b($e.$$.fragment),this.h()},l(e){const c=qo('[data-svelte="svelte-1phssyn"]',document.head);f=n(c,"META",{name:!0,content:!0}),c.forEach(a),E=h(e),m=n(e,"H1",{class:!0});var ye=s(m);u=n(ye,"A",{id:!0,class:!0,href:!0});var et=s(u);T=n(et,"SPAN",{});var Za=s(T);w(g.$$.fragment,Za),Za.forEach(a),et.forEach(a),_=h(ye),M=n(ye,"SPAN",{});var Ga=s(M);At=i(Ga,"VAN"),Ga.forEach(a),ye.forEach(a),ot=h(e),q=n(e,"H2",{class:!0});var vt=s(q);U=n(vt,"A",{id:!0,class:!0,href:!0});var Ba=s(U);Le=n(Ba,"SPAN",{});var Ja=s(Le);w(Y.$$.fragment,Ja),Ja.forEach(a),Ba.forEach(a),Tt=h(vt),Se=n(vt,"SPAN",{});var Qa=s(Se);jt=i(Qa,"Overview"),Qa.forEach(a),vt.forEach(a),nt=h(e),K=n(e,"P",{});var bt=s(K);Mt=i(bt,"The VAN model was proposed in "),ee=n(bt,"A",{href:!0,rel:!0});var Xa=s(ee);kt=i(Xa,"Visual Attention Network"),Xa.forEach(a),Ft=i(bt," by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),bt.forEach(a),st=h(e),Ve=n(e,"P",{});var Ya=s(Ve);Nt=i(Ya,"This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations."),Ya.forEach(a),rt=h(e),xe=n(e,"P",{});var eo=s(xe);Pt=i(eo,"The abstract from the paper is the following:"),eo.forEach(a),it=h(e),Ee=n(e,"P",{});var to=s(Ee);te=n(to,"EM",{});var wt=s(te);It=i(wt,"While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),ae=n(wt,"A",{href:!0,rel:!0});var ao=s(ae);qt=i(ao,"this https URL"),ao.forEach(a),zt=i(wt,"."),wt.forEach(a),to.forEach(a),lt=h(e),Ce=n(e,"P",{});var oo=s(Ce);Dt=i(oo,"Tips:"),oo.forEach(a),ct=h(e),Ae=n(e,"UL",{});var no=s(Ae);oe=n(no,"LI",{});var $t=s(oe);Lt=i($t,"VAN does not have an embedding layer, thus the "),Oe=n($t,"CODE",{});var so=s(Oe);St=i(so,"hidden_states"),so.forEach(a),Ot=i($t," will have a length equal to the number of stages."),$t.forEach(a),no.forEach(a),dt=h(e),F=n(e,"P",{});var Ne=s(F);Ht=i(Ne,"This model was contributed by "),ne=n(Ne,"A",{href:!0,rel:!0});var ro=s(ne);Wt=i(ro,"Francesco"),ro.forEach(a),Ut=i(Ne,". The original code can be found "),se=n(Ne,"A",{href:!0,rel:!0});var io=s(se);Kt=i(io,"here"),io.forEach(a),Rt=i(Ne,"."),Ne.forEach(a),ht=h(e),z=n(e,"H2",{class:!0});var yt=s(z);R=n(yt,"A",{id:!0,class:!0,href:!0});var lo=s(R);He=n(lo,"SPAN",{});var co=s(He);w(re.$$.fragment,co),co.forEach(a),lo.forEach(a),Zt=h(yt),We=n(yt,"SPAN",{});var ho=s(We);Gt=i(ho,"VanConfig"),ho.forEach(a),yt.forEach(a),pt=h(e),v=n(e,"DIV",{class:!0});var N=s(v);w(ie.$$.fragment,N),Bt=h(N),D=n(N,"P",{});var Pe=s(D);Jt=i(Pe,"This is the configuration class to store the configuration of a "),Te=n(Pe,"A",{href:!0});var po=s(Te);Qt=i(po,"VanModel"),po.forEach(a),Xt=i(Pe,`. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN `),le=n(Pe,"A",{href:!0,rel:!0});var fo=s(le);Yt=i(fo,"van-base"),fo.forEach(a),ea=i(Pe,`
architecture.`),Pe.forEach(a),ta=h(N),L=n(N,"P",{});var Ie=s(L);aa=i(Ie,"Configuration objects inherit from "),je=n(Ie,"A",{href:!0});var mo=s(je);oa=i(mo,"PretrainedConfig"),mo.forEach(a),na=i(Ie,` and can be used to control the model outputs. Read the
documentation from `),Me=n(Ie,"A",{href:!0});var uo=s(Me);sa=i(uo,"PretrainedConfig"),uo.forEach(a),ra=i(Ie," for more information."),Ie.forEach(a),ia=h(N),Ue=n(N,"P",{});var go=s(Ue);la=i(go,"Example:"),go.forEach(a),ca=h(N),w(ce.$$.fragment,N),N.forEach(a),ft=h(e),S=n(e,"H2",{class:!0});var Vt=s(S);Z=n(Vt,"A",{id:!0,class:!0,href:!0});var _o=s(Z);Ke=n(_o,"SPAN",{});var vo=s(Ke);w(de.$$.fragment,vo),vo.forEach(a),_o.forEach(a),da=h(Vt),Re=n(Vt,"SPAN",{});var bo=s(Re);ha=i(bo,"VanModel"),bo.forEach(a),Vt.forEach(a),mt=h(e),k=n(e,"DIV",{class:!0});var qe=s(k);w(he.$$.fragment,qe),pa=h(qe),pe=n(qe,"P",{});var xt=s(pe);fa=i(xt,`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch `),fe=n(xt,"A",{href:!0,rel:!0});var wo=s(fe);ma=i(wo,"torch.nn.Module"),wo.forEach(a),ua=i(xt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),xt.forEach(a),ga=h(qe),C=n(qe,"DIV",{class:!0});var P=s(C);w(me.$$.fragment,P),_a=h(P),O=n(P,"P",{});var ze=s(O);va=i(ze,"The "),ke=n(ze,"A",{href:!0});var $o=s(ke);ba=i($o,"VanModel"),$o.forEach(a),wa=i(ze," forward method, overrides the "),Ze=n(ze,"CODE",{});var yo=s(Ze);$a=i(yo,"__call__"),yo.forEach(a),ya=i(ze," special method."),ze.forEach(a),Va=h(P),w(G.$$.fragment,P),xa=h(P),Ge=n(P,"P",{});var Vo=s(Ge);Ea=i(Vo,"Example:"),Vo.forEach(a),Ca=h(P),w(ue.$$.fragment,P),P.forEach(a),qe.forEach(a),ut=h(e),H=n(e,"H2",{class:!0});var Et=s(H);B=n(Et,"A",{id:!0,class:!0,href:!0});var xo=s(B);Be=n(xo,"SPAN",{});var Eo=s(Be);w(ge.$$.fragment,Eo),Eo.forEach(a),xo.forEach(a),Aa=h(Et),Je=n(Et,"SPAN",{});var Co=s(Je);Ta=i(Co,"VanForImageClassification"),Co.forEach(a),Et.forEach(a),gt=h(e),j=n(e,"DIV",{class:!0});var Q=s(j);w(_e.$$.fragment,Q),ja=h(Q),Qe=n(Q,"P",{});var Ao=s(Qe);Ma=i(Ao,`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Ao.forEach(a),ka=h(Q),ve=n(Q,"P",{});var Ct=s(ve);Fa=i(Ct,"This model is a PyTorch "),be=n(Ct,"A",{href:!0,rel:!0});var To=s(be);Na=i(To,"torch.nn.Module"),To.forEach(a),Pa=i(Ct,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ct.forEach(a),Ia=h(Q),A=n(Q,"DIV",{class:!0});var I=s(A);w(we.$$.fragment,I),qa=h(I),W=n(I,"P",{});var De=s(W);za=i(De,"The "),Fe=n(De,"A",{href:!0});var jo=s(Fe);Da=i(jo,"VanForImageClassification"),jo.forEach(a),La=i(De," forward method, overrides the "),Xe=n(De,"CODE",{});var Mo=s(Xe);Sa=i(Mo,"__call__"),Mo.forEach(a),Oa=i(De," special method."),De.forEach(a),Ha=h(I),w(J.$$.fragment,I),Wa=h(I),Ye=n(I,"P",{});var ko=s(Ye);Ua=i(ko,"Example:"),ko.forEach(a),Ka=h(I),w($e.$$.fragment,I),I.forEach(a),Q.forEach(a),this.h()},h(){l(f,"name","hf:doc:metadata"),l(f,"content",JSON.stringify(So)),l(u,"id","van"),l(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(u,"href","#van"),l(m,"class","relative group"),l(U,"id","overview"),l(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(U,"href","#overview"),l(q,"class","relative group"),l(ee,"href","https://arxiv.org/abs/2202.09741"),l(ee,"rel","nofollow"),l(ae,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),l(ae,"rel","nofollow"),l(ne,"href","https://huggingface.co/Francesco"),l(ne,"rel","nofollow"),l(se,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),l(se,"rel","nofollow"),l(R,"id","transformers.VanConfig"),l(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(R,"href","#transformers.VanConfig"),l(z,"class","relative group"),l(Te,"href","/docs/transformers/pr_16027/en/model_doc/van#transformers.VanModel"),l(le,"href","https://huggingface.co/van-base"),l(le,"rel","nofollow"),l(je,"href","/docs/transformers/pr_16027/en/main_classes/configuration#transformers.PretrainedConfig"),l(Me,"href","/docs/transformers/pr_16027/en/main_classes/configuration#transformers.PretrainedConfig"),l(v,"class","docstring"),l(Z,"id","transformers.VanModel"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#transformers.VanModel"),l(S,"class","relative group"),l(fe,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(fe,"rel","nofollow"),l(ke,"href","/docs/transformers/pr_16027/en/model_doc/van#transformers.VanModel"),l(C,"class","docstring"),l(k,"class","docstring"),l(B,"id","transformers.VanForImageClassification"),l(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(B,"href","#transformers.VanForImageClassification"),l(H,"class","relative group"),l(be,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(be,"rel","nofollow"),l(Fe,"href","/docs/transformers/pr_16027/en/model_doc/van#transformers.VanForImageClassification"),l(A,"class","docstring"),l(j,"class","docstring")},m(e,c){t(document.head,f),p(e,E,c),p(e,m,c),t(m,u),t(u,T),$(g,T,null),t(m,_),t(m,M),t(M,At),p(e,ot,c),p(e,q,c),t(q,U),t(U,Le),$(Y,Le,null),t(q,Tt),t(q,Se),t(Se,jt),p(e,nt,c),p(e,K,c),t(K,Mt),t(K,ee),t(ee,kt),t(K,Ft),p(e,st,c),p(e,Ve,c),t(Ve,Nt),p(e,rt,c),p(e,xe,c),t(xe,Pt),p(e,it,c),p(e,Ee,c),t(Ee,te),t(te,It),t(te,ae),t(ae,qt),t(te,zt),p(e,lt,c),p(e,Ce,c),t(Ce,Dt),p(e,ct,c),p(e,Ae,c),t(Ae,oe),t(oe,Lt),t(oe,Oe),t(Oe,St),t(oe,Ot),p(e,dt,c),p(e,F,c),t(F,Ht),t(F,ne),t(ne,Wt),t(F,Ut),t(F,se),t(se,Kt),t(F,Rt),p(e,ht,c),p(e,z,c),t(z,R),t(R,He),$(re,He,null),t(z,Zt),t(z,We),t(We,Gt),p(e,pt,c),p(e,v,c),$(ie,v,null),t(v,Bt),t(v,D),t(D,Jt),t(D,Te),t(Te,Qt),t(D,Xt),t(D,le),t(le,Yt),t(D,ea),t(v,ta),t(v,L),t(L,aa),t(L,je),t(je,oa),t(L,na),t(L,Me),t(Me,sa),t(L,ra),t(v,ia),t(v,Ue),t(Ue,la),t(v,ca),$(ce,v,null),p(e,ft,c),p(e,S,c),t(S,Z),t(Z,Ke),$(de,Ke,null),t(S,da),t(S,Re),t(Re,ha),p(e,mt,c),p(e,k,c),$(he,k,null),t(k,pa),t(k,pe),t(pe,fa),t(pe,fe),t(fe,ma),t(pe,ua),t(k,ga),t(k,C),$(me,C,null),t(C,_a),t(C,O),t(O,va),t(O,ke),t(ke,ba),t(O,wa),t(O,Ze),t(Ze,$a),t(O,ya),t(C,Va),$(G,C,null),t(C,xa),t(C,Ge),t(Ge,Ea),t(C,Ca),$(ue,C,null),p(e,ut,c),p(e,H,c),t(H,B),t(B,Be),$(ge,Be,null),t(H,Aa),t(H,Je),t(Je,Ta),p(e,gt,c),p(e,j,c),$(_e,j,null),t(j,ja),t(j,Qe),t(Qe,Ma),t(j,ka),t(j,ve),t(ve,Fa),t(ve,be),t(be,Na),t(ve,Pa),t(j,Ia),t(j,A),$(we,A,null),t(A,qa),t(A,W),t(W,za),t(W,Fe),t(Fe,Da),t(W,La),t(W,Xe),t(Xe,Sa),t(W,Oa),t(A,Ha),$(J,A,null),t(A,Wa),t(A,Ye),t(Ye,Ua),t(A,Ka),$($e,A,null),_t=!0},p(e,[c]){const ye={};c&2&&(ye.$$scope={dirty:c,ctx:e}),G.$set(ye);const et={};c&2&&(et.$$scope={dirty:c,ctx:e}),J.$set(et)},i(e){_t||(y(g.$$.fragment,e),y(Y.$$.fragment,e),y(re.$$.fragment,e),y(ie.$$.fragment,e),y(ce.$$.fragment,e),y(de.$$.fragment,e),y(he.$$.fragment,e),y(me.$$.fragment,e),y(G.$$.fragment,e),y(ue.$$.fragment,e),y(ge.$$.fragment,e),y(_e.$$.fragment,e),y(we.$$.fragment,e),y(J.$$.fragment,e),y($e.$$.fragment,e),_t=!0)},o(e){V(g.$$.fragment,e),V(Y.$$.fragment,e),V(re.$$.fragment,e),V(ie.$$.fragment,e),V(ce.$$.fragment,e),V(de.$$.fragment,e),V(he.$$.fragment,e),V(me.$$.fragment,e),V(G.$$.fragment,e),V(ue.$$.fragment,e),V(ge.$$.fragment,e),V(_e.$$.fragment,e),V(we.$$.fragment,e),V(J.$$.fragment,e),V($e.$$.fragment,e),_t=!1},d(e){a(f),e&&a(E),e&&a(m),x(g),e&&a(ot),e&&a(q),x(Y),e&&a(nt),e&&a(K),e&&a(st),e&&a(Ve),e&&a(rt),e&&a(xe),e&&a(it),e&&a(Ee),e&&a(lt),e&&a(Ce),e&&a(ct),e&&a(Ae),e&&a(dt),e&&a(F),e&&a(ht),e&&a(z),x(re),e&&a(pt),e&&a(v),x(ie),x(ce),e&&a(ft),e&&a(S),x(de),e&&a(mt),e&&a(k),x(he),x(me),x(G),x(ue),e&&a(ut),e&&a(H),x(ge),e&&a(gt),e&&a(j),x(_e),x(we),x(J),x($e)}}}const So={local:"van",sections:[{local:"overview",title:"Overview"},{local:"transformers.VanConfig",title:"VanConfig"},{local:"transformers.VanModel",title:"VanModel"},{local:"transformers.VanForImageClassification",title:"VanForImageClassification"}],title:"VAN"};function Oo(X,f,E){let{fw:m}=f;return X.$$set=u=>{"fw"in u&&E(0,m=u.fw)},[m]}class Go extends No{constructor(f){super();Po(this,f,Oo,Lo,Io,{fw:0})}}export{Go as default,So as metadata};
