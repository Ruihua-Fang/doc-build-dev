import{S as S5t,i as P5t,s as $5t,e as a,k as l,w as f,t as o,M as I5t,c as n,d as t,m as i,a as s,x as m,h as r,b as d,F as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-4833417e.js";import{T as $Lr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-7b52c3d4.js";import{C as w}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as z}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function D5t(Ai){let J,Pe,ie,ge,lo,fe,Te,Xo,Li,Ef,sa,Bi,xi,B4,yf,Le,io,ki,In,x4,Dn,jn,k4,Ri,Nn,R4,Si,wf,$a;return{c(){J=a("p"),Pe=o("If your "),ie=a("code"),ge=o("NewModelConfig"),lo=o(" is a subclass of "),fe=a("code"),Te=o("PretrainedConfig"),Xo=o(`, make sure its
`),Li=a("code"),Ef=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),Bi=a("code"),xi=o('"new-model"'),B4=o(")."),yf=l(),Le=a("p"),io=o("Likewise, if your "),ki=a("code"),In=o("NewModel"),x4=o(" is a subclass of "),Dn=a("a"),jn=o("PreTrainedModel"),k4=o(`, make sure its
`),Ri=a("code"),Nn=o("config_class"),R4=o(` attribute is set to the same class you use when registering the model (here
`),Si=a("code"),wf=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var he=s(J);Pe=r(he,"If your "),ie=n(he,"CODE",{});var h8=s(ie);ge=r(h8,"NewModelConfig"),h8.forEach(t),lo=r(he," is a subclass of "),fe=n(he,"CODE",{});var Pi=s(fe);Te=r(Pi,"PretrainedConfig"),Pi.forEach(t),Xo=r(he,`, make sure its
`),Li=n(he,"CODE",{});var p8=s(Li);Ef=r(p8,"model_type"),p8.forEach(t),sa=r(he," attribute is set to the same key you use when registering the config (here "),Bi=n(he,"CODE",{});var _8=s(Bi);xi=r(_8,'"new-model"'),_8.forEach(t),B4=r(he,")."),he.forEach(t),yf=i(co),Le=n(co,"P",{});var Vo=s(Le);io=r(Vo,"Likewise, if your "),ki=n(Vo,"CODE",{});var Ia=s(ki);In=r(Ia,"NewModel"),Ia.forEach(t),x4=r(Vo," is a subclass of "),Dn=n(Vo,"A",{href:!0});var u8=s(Dn);jn=r(u8,"PreTrainedModel"),u8.forEach(t),k4=r(Vo,`, make sure its
`),Ri=n(Vo,"CODE",{});var Af=s(Ri);Nn=r(Af,"config_class"),Af.forEach(t),R4=r(Vo,` attribute is set to the same class you use when registering the model (here
`),Si=n(Vo,"CODE",{});var b8=s(Si);wf=r(b8,"NewModelConfig"),b8.forEach(t),$a=r(Vo,")."),Vo.forEach(t),this.h()},h(){d(Dn,"href","/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel")},m(co,he){b(co,J,he),e(J,Pe),e(J,ie),e(ie,ge),e(J,lo),e(J,fe),e(fe,Te),e(J,Xo),e(J,Li),e(Li,Ef),e(J,sa),e(J,Bi),e(Bi,xi),e(J,B4),b(co,yf,he),b(co,Le,he),e(Le,io),e(Le,ki),e(ki,In),e(Le,x4),e(Le,Dn),e(Dn,jn),e(Le,k4),e(Le,Ri),e(Ri,Nn),e(Le,R4),e(Le,Si),e(Si,wf),e(Le,$a)},d(co){co&&t(J),co&&t(yf),co&&t(Le)}}}function j5t(Ai){let J,Pe,ie,ge,lo;return{c(){J=a("p"),Pe=o("Passing "),ie=a("code"),ge=o("use_auth_token=True"),lo=o(" is required when you want to use a private model.")},l(fe){J=n(fe,"P",{});var Te=s(J);Pe=r(Te,"Passing "),ie=n(Te,"CODE",{});var Xo=s(ie);ge=r(Xo,"use_auth_token=True"),Xo.forEach(t),lo=r(Te," is required when you want to use a private model."),Te.forEach(t)},m(fe,Te){b(fe,J,Te),e(J,Pe),e(J,ie),e(ie,ge),e(J,lo)},d(fe){fe&&t(J)}}}function N5t(Ai){let J,Pe,ie,ge,lo;return{c(){J=a("p"),Pe=o("Passing "),ie=a("code"),ge=o("use_auth_token=True"),lo=o(" is required when you want to use a private model.")},l(fe){J=n(fe,"P",{});var Te=s(J);Pe=r(Te,"Passing "),ie=n(Te,"CODE",{});var Xo=s(ie);ge=r(Xo,"use_auth_token=True"),Xo.forEach(t),lo=r(Te," is required when you want to use a private model."),Te.forEach(t)},m(fe,Te){b(fe,J,Te),e(J,Pe),e(J,ie),e(ie,ge),e(J,lo)},d(fe){fe&&t(J)}}}function q5t(Ai){let J,Pe,ie,ge,lo,fe,Te,Xo,Li,Ef,sa,Bi,xi,B4,yf,Le,io,ki,In,x4,Dn,jn,k4,Ri,Nn,R4,Si,wf,$a,co,he,h8,Pi,p8,_8,Vo,Ia,u8,Af,b8,dSe,Z7e,$i,Lf,AW,S4,cSe,LW,fSe,eBe,qn,mSe,BW,gSe,hSe,xW,pSe,_Se,oBe,P4,rBe,v8,uSe,tBe,Bf,aBe,Ii,xf,kW,$4,bSe,RW,vSe,nBe,zo,I4,TSe,D4,FSe,T8,CSe,MSe,ESe,j4,ySe,SW,wSe,ASe,LSe,fo,N4,BSe,PW,xSe,kSe,Di,RSe,$W,SSe,PSe,IW,$Se,ISe,DSe,v,kf,DW,jSe,NSe,F8,qSe,GSe,OSe,Rf,jW,XSe,VSe,C8,zSe,WSe,QSe,Sf,NW,HSe,USe,M8,JSe,YSe,KSe,Pf,qW,ZSe,ePe,E8,oPe,rPe,tPe,$f,GW,aPe,nPe,y8,sPe,lPe,iPe,If,OW,dPe,cPe,w8,fPe,mPe,gPe,Df,XW,hPe,pPe,A8,_Pe,uPe,bPe,jf,VW,vPe,TPe,L8,FPe,CPe,MPe,Nf,zW,EPe,yPe,B8,wPe,APe,LPe,qf,WW,BPe,xPe,x8,kPe,RPe,SPe,Gf,QW,PPe,$Pe,k8,IPe,DPe,jPe,Of,HW,NPe,qPe,R8,GPe,OPe,XPe,Xf,UW,VPe,zPe,S8,WPe,QPe,HPe,Vf,JW,UPe,JPe,P8,YPe,KPe,ZPe,zf,YW,e$e,o$e,$8,r$e,t$e,a$e,Wf,KW,n$e,s$e,I8,l$e,i$e,d$e,Qf,ZW,c$e,f$e,D8,m$e,g$e,h$e,Hf,eQ,p$e,_$e,j8,u$e,b$e,v$e,Uf,oQ,T$e,F$e,N8,C$e,M$e,E$e,Jf,rQ,y$e,w$e,q8,A$e,L$e,B$e,Yf,tQ,x$e,k$e,G8,R$e,S$e,P$e,Kf,aQ,$$e,I$e,O8,D$e,j$e,N$e,Zf,nQ,q$e,G$e,X8,O$e,X$e,V$e,em,sQ,z$e,W$e,V8,Q$e,H$e,U$e,om,lQ,J$e,Y$e,z8,K$e,Z$e,eIe,rm,iQ,oIe,rIe,W8,tIe,aIe,nIe,tm,dQ,sIe,lIe,Q8,iIe,dIe,cIe,am,cQ,fIe,mIe,H8,gIe,hIe,pIe,nm,fQ,_Ie,uIe,U8,bIe,vIe,TIe,sm,mQ,FIe,CIe,J8,MIe,EIe,yIe,lm,gQ,wIe,AIe,Y8,LIe,BIe,xIe,im,hQ,kIe,RIe,K8,SIe,PIe,$Ie,dm,pQ,IIe,DIe,Z8,jIe,NIe,qIe,cm,_Q,GIe,OIe,e7,XIe,VIe,zIe,fm,uQ,WIe,QIe,o7,HIe,UIe,JIe,mm,bQ,YIe,KIe,r7,ZIe,eDe,oDe,gm,vQ,rDe,tDe,t7,aDe,nDe,sDe,hm,TQ,lDe,iDe,a7,dDe,cDe,fDe,pm,FQ,mDe,gDe,n7,hDe,pDe,_De,_m,CQ,uDe,bDe,s7,vDe,TDe,FDe,um,MQ,CDe,MDe,l7,EDe,yDe,wDe,bm,EQ,ADe,LDe,i7,BDe,xDe,kDe,vm,yQ,RDe,SDe,d7,PDe,$De,IDe,Tm,wQ,DDe,jDe,c7,NDe,qDe,GDe,Fm,AQ,ODe,XDe,f7,VDe,zDe,WDe,Cm,LQ,QDe,HDe,m7,UDe,JDe,YDe,Mm,BQ,KDe,ZDe,g7,eje,oje,rje,Em,xQ,tje,aje,h7,nje,sje,lje,ym,kQ,ije,dje,p7,cje,fje,mje,wm,RQ,gje,hje,_7,pje,_je,uje,Am,SQ,bje,vje,u7,Tje,Fje,Cje,Lm,PQ,Mje,Eje,b7,yje,wje,Aje,Bm,$Q,Lje,Bje,v7,xje,kje,Rje,xm,IQ,Sje,Pje,T7,$je,Ije,Dje,km,DQ,jje,Nje,F7,qje,Gje,Oje,Rm,jQ,Xje,Vje,C7,zje,Wje,Qje,Sm,NQ,Hje,Uje,M7,Jje,Yje,Kje,Pm,qQ,Zje,eNe,E7,oNe,rNe,tNe,$m,GQ,aNe,nNe,y7,sNe,lNe,iNe,Im,OQ,dNe,cNe,w7,fNe,mNe,gNe,Dm,XQ,hNe,pNe,A7,_Ne,uNe,bNe,jm,VQ,vNe,TNe,L7,FNe,CNe,MNe,Nm,zQ,ENe,yNe,B7,wNe,ANe,LNe,qm,WQ,BNe,xNe,x7,kNe,RNe,SNe,Gm,QQ,PNe,$Ne,k7,INe,DNe,jNe,Om,HQ,NNe,qNe,R7,GNe,ONe,XNe,Xm,UQ,VNe,zNe,S7,WNe,QNe,HNe,Vm,JQ,UNe,JNe,P7,YNe,KNe,ZNe,zm,YQ,eqe,oqe,$7,rqe,tqe,aqe,Wm,KQ,nqe,sqe,I7,lqe,iqe,dqe,Qm,ZQ,cqe,fqe,D7,mqe,gqe,hqe,Hm,eH,pqe,_qe,j7,uqe,bqe,vqe,Um,oH,Tqe,Fqe,N7,Cqe,Mqe,Eqe,Jm,rH,yqe,wqe,q7,Aqe,Lqe,Bqe,Ym,tH,xqe,kqe,G7,Rqe,Sqe,Pqe,Km,aH,$qe,Iqe,O7,Dqe,jqe,Nqe,Zm,nH,qqe,Gqe,X7,Oqe,Xqe,Vqe,eg,sH,zqe,Wqe,V7,Qqe,Hqe,Uqe,og,lH,Jqe,Yqe,z7,Kqe,Zqe,eGe,rg,iH,oGe,rGe,W7,tGe,aGe,nGe,tg,dH,sGe,lGe,Q7,iGe,dGe,cGe,ag,cH,fGe,mGe,H7,gGe,hGe,pGe,ng,fH,_Ge,uGe,U7,bGe,vGe,TGe,sg,mH,FGe,CGe,J7,MGe,EGe,yGe,lg,gH,wGe,AGe,Y7,LGe,BGe,xGe,ig,hH,kGe,RGe,K7,SGe,PGe,$Ge,dg,pH,IGe,DGe,Z7,jGe,NGe,qGe,cg,_H,GGe,OGe,eB,XGe,VGe,zGe,fg,uH,WGe,QGe,oB,HGe,UGe,JGe,mg,bH,YGe,KGe,rB,ZGe,eOe,oOe,gg,vH,rOe,tOe,tB,aOe,nOe,sOe,hg,TH,lOe,iOe,aB,dOe,cOe,fOe,pg,FH,mOe,gOe,nB,hOe,pOe,_Oe,_g,CH,uOe,bOe,sB,vOe,TOe,FOe,MH,COe,MOe,q4,EOe,ug,G4,yOe,EH,wOe,sBe,ji,bg,yH,O4,AOe,wH,LOe,lBe,Wo,X4,BOe,V4,xOe,lB,kOe,ROe,SOe,z4,POe,AH,$Oe,IOe,DOe,mo,W4,jOe,LH,NOe,qOe,Da,GOe,BH,OOe,XOe,xH,VOe,zOe,kH,WOe,QOe,HOe,M,Gn,RH,UOe,JOe,iB,YOe,KOe,dB,ZOe,eXe,oXe,On,SH,rXe,tXe,cB,aXe,nXe,fB,sXe,lXe,iXe,Xn,PH,dXe,cXe,mB,fXe,mXe,gB,gXe,hXe,pXe,vg,$H,_Xe,uXe,hB,bXe,vXe,TXe,Vn,IH,FXe,CXe,pB,MXe,EXe,_B,yXe,wXe,AXe,Tg,DH,LXe,BXe,uB,xXe,kXe,RXe,Fg,jH,SXe,PXe,bB,$Xe,IXe,DXe,Cg,NH,jXe,NXe,vB,qXe,GXe,OXe,zn,qH,XXe,VXe,TB,zXe,WXe,FB,QXe,HXe,UXe,Wn,GH,JXe,YXe,CB,KXe,ZXe,MB,eVe,oVe,rVe,Qn,OH,tVe,aVe,EB,nVe,sVe,yB,lVe,iVe,dVe,Mg,XH,cVe,fVe,wB,mVe,gVe,hVe,Eg,VH,pVe,_Ve,AB,uVe,bVe,vVe,Hn,zH,TVe,FVe,LB,CVe,MVe,BB,EVe,yVe,wVe,yg,WH,AVe,LVe,xB,BVe,xVe,kVe,Un,QH,RVe,SVe,kB,PVe,$Ve,RB,IVe,DVe,jVe,Jn,HH,NVe,qVe,SB,GVe,OVe,PB,XVe,VVe,zVe,Yn,UH,WVe,QVe,$B,HVe,UVe,JH,JVe,YVe,KVe,wg,YH,ZVe,eze,IB,oze,rze,tze,Kn,KH,aze,nze,DB,sze,lze,jB,ize,dze,cze,Ag,ZH,fze,mze,NB,gze,hze,pze,Zn,eU,_ze,uze,qB,bze,vze,GB,Tze,Fze,Cze,es,oU,Mze,Eze,OB,yze,wze,XB,Aze,Lze,Bze,os,rU,xze,kze,VB,Rze,Sze,zB,Pze,$ze,Ize,Lg,tU,Dze,jze,WB,Nze,qze,Gze,rs,aU,Oze,Xze,QB,Vze,zze,HB,Wze,Qze,Hze,Bg,nU,Uze,Jze,UB,Yze,Kze,Zze,ts,sU,eWe,oWe,JB,rWe,tWe,YB,aWe,nWe,sWe,as,lU,lWe,iWe,KB,dWe,cWe,ZB,fWe,mWe,gWe,ns,iU,hWe,pWe,ex,_We,uWe,ox,bWe,vWe,TWe,ss,dU,FWe,CWe,rx,MWe,EWe,tx,yWe,wWe,AWe,xg,cU,LWe,BWe,ax,xWe,kWe,RWe,ls,fU,SWe,PWe,nx,$We,IWe,sx,DWe,jWe,NWe,is,mU,qWe,GWe,lx,OWe,XWe,ix,VWe,zWe,WWe,ds,gU,QWe,HWe,dx,UWe,JWe,cx,YWe,KWe,ZWe,cs,hU,eQe,oQe,fx,rQe,tQe,mx,aQe,nQe,sQe,fs,pU,lQe,iQe,gx,dQe,cQe,hx,fQe,mQe,gQe,ms,_U,hQe,pQe,px,_Qe,uQe,_x,bQe,vQe,TQe,kg,uU,FQe,CQe,ux,MQe,EQe,yQe,gs,bU,wQe,AQe,bx,LQe,BQe,vx,xQe,kQe,RQe,Rg,vU,SQe,PQe,Tx,$Qe,IQe,DQe,Sg,TU,jQe,NQe,Fx,qQe,GQe,OQe,hs,FU,XQe,VQe,Cx,zQe,WQe,Mx,QQe,HQe,UQe,ps,CU,JQe,YQe,Ex,KQe,ZQe,yx,eHe,oHe,rHe,Pg,MU,tHe,aHe,wx,nHe,sHe,lHe,_s,EU,iHe,dHe,Ax,cHe,fHe,Lx,mHe,gHe,hHe,us,yU,pHe,_He,Bx,uHe,bHe,xx,vHe,THe,FHe,bs,wU,CHe,MHe,kx,EHe,yHe,Rx,wHe,AHe,LHe,vs,AU,BHe,xHe,Sx,kHe,RHe,Px,SHe,PHe,$He,Ts,LU,IHe,DHe,$x,jHe,NHe,Ix,qHe,GHe,OHe,$g,BU,XHe,VHe,Dx,zHe,WHe,QHe,Ig,xU,HHe,UHe,jx,JHe,YHe,KHe,Dg,kU,ZHe,eUe,Nx,oUe,rUe,tUe,jg,RU,aUe,nUe,qx,sUe,lUe,iUe,Fs,SU,dUe,cUe,Gx,fUe,mUe,Ox,gUe,hUe,pUe,Ng,PU,_Ue,uUe,Xx,bUe,vUe,TUe,Cs,$U,FUe,CUe,Vx,MUe,EUe,zx,yUe,wUe,AUe,Ms,IU,LUe,BUe,Wx,xUe,kUe,Qx,RUe,SUe,PUe,Es,DU,$Ue,IUe,Hx,DUe,jUe,Ux,NUe,qUe,GUe,ys,jU,OUe,XUe,Jx,VUe,zUe,Yx,WUe,QUe,HUe,ws,NU,UUe,JUe,Kx,YUe,KUe,Zx,ZUe,eJe,oJe,qg,qU,rJe,tJe,ek,aJe,nJe,sJe,Gg,GU,lJe,iJe,ok,dJe,cJe,fJe,As,OU,mJe,gJe,rk,hJe,pJe,tk,_Je,uJe,bJe,Ls,XU,vJe,TJe,ak,FJe,CJe,nk,MJe,EJe,yJe,Bs,VU,wJe,AJe,sk,LJe,BJe,lk,xJe,kJe,RJe,Og,zU,SJe,PJe,ik,$Je,IJe,DJe,Xg,WU,jJe,NJe,dk,qJe,GJe,OJe,Vg,QU,XJe,VJe,ck,zJe,WJe,QJe,zg,HU,HJe,UJe,fk,JJe,YJe,KJe,xs,UU,ZJe,eYe,mk,oYe,rYe,gk,tYe,aYe,nYe,Wg,JU,sYe,lYe,hk,iYe,dYe,cYe,Qg,YU,fYe,mYe,pk,gYe,hYe,pYe,ks,KU,_Ye,uYe,_k,bYe,vYe,uk,TYe,FYe,CYe,Rs,ZU,MYe,EYe,bk,yYe,wYe,vk,AYe,LYe,BYe,eJ,xYe,kYe,Q4,RYe,Hg,H4,SYe,oJ,PYe,iBe,Ni,Ug,rJ,U4,$Ye,tJ,IYe,dBe,Qo,J4,DYe,Y4,jYe,Tk,NYe,qYe,GYe,K4,OYe,aJ,XYe,VYe,zYe,$e,Z4,WYe,nJ,QYe,HYe,ja,UYe,sJ,JYe,YYe,lJ,KYe,ZYe,iJ,eKe,oKe,rKe,se,Jg,dJ,tKe,aKe,Fk,nKe,sKe,lKe,Yg,cJ,iKe,dKe,Ck,cKe,fKe,mKe,Kg,fJ,gKe,hKe,Mk,pKe,_Ke,uKe,Zg,mJ,bKe,vKe,Ek,TKe,FKe,CKe,eh,gJ,MKe,EKe,yk,yKe,wKe,AKe,oh,hJ,LKe,BKe,wk,xKe,kKe,RKe,rh,pJ,SKe,PKe,Ak,$Ke,IKe,DKe,th,_J,jKe,NKe,Lk,qKe,GKe,OKe,ah,uJ,XKe,VKe,Bk,zKe,WKe,QKe,nh,bJ,HKe,UKe,xk,JKe,YKe,KKe,sh,vJ,ZKe,eZe,kk,oZe,rZe,tZe,lh,TJ,aZe,nZe,Rk,sZe,lZe,iZe,ih,FJ,dZe,cZe,Sk,fZe,mZe,gZe,dh,CJ,hZe,pZe,Pk,_Ze,uZe,bZe,ch,MJ,vZe,TZe,$k,FZe,CZe,MZe,fh,EZe,EJ,yZe,wZe,eE,AZe,mh,oE,LZe,yJ,BZe,cBe,qi,gh,wJ,rE,xZe,AJ,kZe,fBe,Ho,tE,RZe,aE,SZe,Ik,PZe,$Ze,IZe,nE,DZe,LJ,jZe,NZe,qZe,Ie,sE,GZe,BJ,OZe,XZe,Gi,VZe,xJ,zZe,WZe,kJ,QZe,HZe,UZe,Be,hh,RJ,JZe,YZe,Dk,KZe,ZZe,eeo,ph,SJ,oeo,reo,jk,teo,aeo,neo,_h,PJ,seo,leo,Nk,ieo,deo,ceo,uh,$J,feo,meo,qk,geo,heo,peo,bh,IJ,_eo,ueo,Gk,beo,veo,Teo,vh,DJ,Feo,Ceo,Ok,Meo,Eeo,yeo,Th,jJ,weo,Aeo,Xk,Leo,Beo,xeo,Fh,NJ,keo,Reo,Vk,Seo,Peo,$eo,Ch,Ieo,qJ,Deo,jeo,lE,Neo,Mh,iE,qeo,GJ,Geo,mBe,Oi,Eh,OJ,dE,Oeo,XJ,Xeo,gBe,Uo,cE,Veo,Xi,zeo,VJ,Weo,Qeo,zJ,Heo,Ueo,Jeo,fE,Yeo,WJ,Keo,Zeo,eoo,Or,mE,ooo,QJ,roo,too,Vi,aoo,HJ,noo,soo,UJ,loo,ioo,doo,JJ,coo,foo,gE,moo,De,hE,goo,YJ,hoo,poo,Na,_oo,KJ,uoo,boo,ZJ,voo,Too,eY,Foo,Coo,Moo,F,yh,oY,Eoo,yoo,zk,woo,Aoo,Loo,wh,rY,Boo,xoo,Wk,koo,Roo,Soo,Ah,tY,Poo,$oo,Qk,Ioo,Doo,joo,Lh,aY,Noo,qoo,Hk,Goo,Ooo,Xoo,Bh,nY,Voo,zoo,Uk,Woo,Qoo,Hoo,xh,sY,Uoo,Joo,Jk,Yoo,Koo,Zoo,kh,lY,ero,oro,Yk,rro,tro,aro,Rh,iY,nro,sro,Kk,lro,iro,dro,Sh,dY,cro,fro,Zk,mro,gro,hro,Ph,cY,pro,_ro,eR,uro,bro,vro,$h,fY,Tro,Fro,oR,Cro,Mro,Ero,Ih,mY,yro,wro,rR,Aro,Lro,Bro,Dh,gY,xro,kro,tR,Rro,Sro,Pro,jh,hY,$ro,Iro,aR,Dro,jro,Nro,Nh,pY,qro,Gro,nR,Oro,Xro,Vro,qh,_Y,zro,Wro,sR,Qro,Hro,Uro,Gh,uY,Jro,Yro,lR,Kro,Zro,eto,Oh,bY,oto,rto,iR,tto,ato,nto,Xh,vY,sto,lto,dR,ito,dto,cto,Vh,TY,fto,mto,cR,gto,hto,pto,zh,FY,_to,uto,fR,bto,vto,Tto,Wh,CY,Fto,Cto,mR,Mto,Eto,yto,Qh,MY,wto,Ato,gR,Lto,Bto,xto,Hh,EY,kto,Rto,hR,Sto,Pto,$to,Uh,yY,Ito,Dto,pR,jto,Nto,qto,Jh,wY,Gto,Oto,_R,Xto,Vto,zto,Yh,AY,Wto,Qto,uR,Hto,Uto,Jto,Ss,LY,Yto,Kto,bR,Zto,eao,vR,oao,rao,tao,Kh,BY,aao,nao,TR,sao,lao,iao,Zh,xY,dao,cao,FR,fao,mao,gao,ep,kY,hao,pao,CR,_ao,uao,bao,op,RY,vao,Tao,MR,Fao,Cao,Mao,rp,SY,Eao,yao,ER,wao,Aao,Lao,tp,PY,Bao,xao,yR,kao,Rao,Sao,ap,$Y,Pao,$ao,wR,Iao,Dao,jao,np,IY,Nao,qao,AR,Gao,Oao,Xao,sp,DY,Vao,zao,LR,Wao,Qao,Hao,lp,jY,Uao,Jao,BR,Yao,Kao,Zao,ip,NY,eno,ono,xR,rno,tno,ano,dp,qY,nno,sno,kR,lno,ino,dno,cp,GY,cno,fno,RR,mno,gno,hno,fp,OY,pno,_no,SR,uno,bno,vno,mp,XY,Tno,Fno,PR,Cno,Mno,Eno,gp,VY,yno,wno,$R,Ano,Lno,Bno,hp,zY,xno,kno,IR,Rno,Sno,Pno,pp,WY,$no,Ino,DR,Dno,jno,Nno,_p,QY,qno,Gno,jR,Ono,Xno,Vno,up,HY,zno,Wno,NR,Qno,Hno,Uno,bp,UY,Jno,Yno,qR,Kno,Zno,eso,vp,JY,oso,rso,GR,tso,aso,nso,Tp,YY,sso,lso,OR,iso,dso,cso,Fp,KY,fso,mso,XR,gso,hso,pso,Cp,ZY,_so,uso,VR,bso,vso,Tso,Mp,eK,Fso,Cso,zR,Mso,Eso,yso,Ep,oK,wso,Aso,WR,Lso,Bso,xso,yp,rK,kso,Rso,QR,Sso,Pso,$so,wp,tK,Iso,Dso,HR,jso,Nso,qso,Ap,aK,Gso,Oso,UR,Xso,Vso,zso,Lp,nK,Wso,Qso,JR,Hso,Uso,Jso,Bp,sK,Yso,Kso,YR,Zso,elo,olo,xp,lK,rlo,tlo,KR,alo,nlo,slo,kp,iK,llo,ilo,ZR,dlo,clo,flo,Rp,dK,mlo,glo,eS,hlo,plo,_lo,Sp,cK,ulo,blo,oS,vlo,Tlo,Flo,Pp,fK,Clo,Mlo,rS,Elo,ylo,wlo,$p,mK,Alo,Llo,tS,Blo,xlo,klo,Ip,gK,Rlo,Slo,aS,Plo,$lo,Ilo,Dp,hK,Dlo,jlo,nS,Nlo,qlo,Glo,jp,pK,Olo,Xlo,sS,Vlo,zlo,Wlo,Np,_K,Qlo,Hlo,lS,Ulo,Jlo,Ylo,qp,uK,Klo,Zlo,iS,eio,oio,rio,Gp,bK,tio,aio,dS,nio,sio,lio,Op,vK,iio,dio,cS,cio,fio,mio,Xp,TK,gio,hio,fS,pio,_io,uio,Vp,FK,bio,vio,mS,Tio,Fio,Cio,zp,CK,Mio,Eio,gS,yio,wio,Aio,Wp,MK,Lio,Bio,hS,xio,kio,Rio,Qp,EK,Sio,Pio,pS,$io,Iio,Dio,Hp,yK,jio,Nio,_S,qio,Gio,Oio,Up,wK,Xio,Vio,uS,zio,Wio,Qio,Jp,AK,Hio,Uio,bS,Jio,Yio,Kio,Yp,LK,Zio,edo,vS,odo,rdo,tdo,Kp,BK,ado,ndo,TS,sdo,ldo,ido,Zp,xK,ddo,cdo,FS,fdo,mdo,gdo,e_,kK,hdo,pdo,CS,_do,udo,bdo,o_,RK,vdo,Tdo,MS,Fdo,Cdo,Mdo,r_,SK,Edo,ydo,ES,wdo,Ado,Ldo,t_,Bdo,PK,xdo,kdo,$K,Rdo,Sdo,IK,Pdo,$do,pE,hBe,zi,a_,DK,_E,Ido,jK,Ddo,pBe,Jo,uE,jdo,Wi,Ndo,NK,qdo,Gdo,qK,Odo,Xdo,Vdo,bE,zdo,GK,Wdo,Qdo,Hdo,Xr,vE,Udo,OK,Jdo,Ydo,Qi,Kdo,XK,Zdo,eco,VK,oco,rco,tco,zK,aco,nco,TE,sco,je,FE,lco,WK,ico,dco,qa,cco,QK,fco,mco,HK,gco,hco,UK,pco,_co,uco,k,n_,JK,bco,vco,yS,Tco,Fco,Cco,s_,YK,Mco,Eco,wS,yco,wco,Aco,l_,KK,Lco,Bco,AS,xco,kco,Rco,i_,ZK,Sco,Pco,LS,$co,Ico,Dco,d_,eZ,jco,Nco,BS,qco,Gco,Oco,c_,oZ,Xco,Vco,xS,zco,Wco,Qco,f_,rZ,Hco,Uco,kS,Jco,Yco,Kco,m_,tZ,Zco,efo,RS,ofo,rfo,tfo,g_,aZ,afo,nfo,SS,sfo,lfo,ifo,h_,nZ,dfo,cfo,PS,ffo,mfo,gfo,p_,sZ,hfo,pfo,$S,_fo,ufo,bfo,__,lZ,vfo,Tfo,IS,Ffo,Cfo,Mfo,u_,iZ,Efo,yfo,DS,wfo,Afo,Lfo,b_,dZ,Bfo,xfo,jS,kfo,Rfo,Sfo,v_,cZ,Pfo,$fo,NS,Ifo,Dfo,jfo,T_,fZ,Nfo,qfo,qS,Gfo,Ofo,Xfo,F_,mZ,Vfo,zfo,GS,Wfo,Qfo,Hfo,C_,gZ,Ufo,Jfo,OS,Yfo,Kfo,Zfo,M_,hZ,emo,omo,XS,rmo,tmo,amo,E_,pZ,nmo,smo,VS,lmo,imo,dmo,y_,_Z,cmo,fmo,zS,mmo,gmo,hmo,w_,uZ,pmo,_mo,WS,umo,bmo,vmo,A_,bZ,Tmo,Fmo,QS,Cmo,Mmo,Emo,L_,vZ,ymo,wmo,HS,Amo,Lmo,Bmo,B_,TZ,xmo,kmo,US,Rmo,Smo,Pmo,x_,FZ,$mo,Imo,JS,Dmo,jmo,Nmo,k_,CZ,qmo,Gmo,YS,Omo,Xmo,Vmo,R_,MZ,zmo,Wmo,KS,Qmo,Hmo,Umo,S_,EZ,Jmo,Ymo,ZS,Kmo,Zmo,ego,P_,yZ,ogo,rgo,eP,tgo,ago,ngo,$_,wZ,sgo,lgo,oP,igo,dgo,cgo,I_,AZ,fgo,mgo,rP,ggo,hgo,pgo,D_,LZ,_go,ugo,tP,bgo,vgo,Tgo,j_,BZ,Fgo,Cgo,aP,Mgo,Ego,ygo,N_,xZ,wgo,Ago,nP,Lgo,Bgo,xgo,q_,kZ,kgo,Rgo,sP,Sgo,Pgo,$go,G_,RZ,Igo,Dgo,lP,jgo,Ngo,qgo,O_,SZ,Ggo,Ogo,iP,Xgo,Vgo,zgo,X_,PZ,Wgo,Qgo,dP,Hgo,Ugo,Jgo,V_,Ygo,$Z,Kgo,Zgo,IZ,eho,oho,DZ,rho,tho,CE,_Be,Hi,z_,jZ,ME,aho,NZ,nho,uBe,Yo,EE,sho,Ui,lho,qZ,iho,dho,GZ,cho,fho,mho,yE,gho,OZ,hho,pho,_ho,Vr,wE,uho,XZ,bho,vho,Ji,Tho,VZ,Fho,Cho,zZ,Mho,Eho,yho,WZ,who,Aho,AE,Lho,Ne,LE,Bho,QZ,xho,kho,Ga,Rho,HZ,Sho,Pho,UZ,$ho,Iho,JZ,Dho,jho,Nho,$,W_,YZ,qho,Gho,cP,Oho,Xho,Vho,Q_,KZ,zho,Who,fP,Qho,Hho,Uho,H_,ZZ,Jho,Yho,mP,Kho,Zho,epo,U_,eee,opo,rpo,gP,tpo,apo,npo,J_,oee,spo,lpo,hP,ipo,dpo,cpo,Y_,ree,fpo,mpo,pP,gpo,hpo,ppo,K_,tee,_po,upo,_P,bpo,vpo,Tpo,Z_,aee,Fpo,Cpo,uP,Mpo,Epo,ypo,eu,nee,wpo,Apo,bP,Lpo,Bpo,xpo,ou,see,kpo,Rpo,vP,Spo,Ppo,$po,ru,lee,Ipo,Dpo,TP,jpo,Npo,qpo,tu,iee,Gpo,Opo,FP,Xpo,Vpo,zpo,au,dee,Wpo,Qpo,CP,Hpo,Upo,Jpo,nu,cee,Ypo,Kpo,MP,Zpo,e_o,o_o,su,fee,r_o,t_o,EP,a_o,n_o,s_o,lu,mee,l_o,i_o,yP,d_o,c_o,f_o,iu,gee,m_o,g_o,wP,h_o,p_o,__o,du,hee,u_o,b_o,AP,v_o,T_o,F_o,cu,pee,C_o,M_o,LP,E_o,y_o,w_o,fu,_ee,A_o,L_o,BP,B_o,x_o,k_o,mu,uee,R_o,S_o,xP,P_o,$_o,I_o,gu,bee,D_o,j_o,kP,N_o,q_o,G_o,hu,vee,O_o,X_o,RP,V_o,z_o,W_o,pu,Tee,Q_o,H_o,SP,U_o,J_o,Y_o,_u,Fee,K_o,Z_o,PP,euo,ouo,ruo,uu,Cee,tuo,auo,$P,nuo,suo,luo,bu,Mee,iuo,duo,IP,cuo,fuo,muo,vu,Eee,guo,huo,DP,puo,_uo,uuo,Tu,yee,buo,vuo,jP,Tuo,Fuo,Cuo,Fu,wee,Muo,Euo,NP,yuo,wuo,Auo,Cu,Aee,Luo,Buo,qP,xuo,kuo,Ruo,Mu,Lee,Suo,Puo,GP,$uo,Iuo,Duo,Eu,Bee,juo,Nuo,OP,quo,Guo,Ouo,yu,xee,Xuo,Vuo,XP,zuo,Wuo,Quo,wu,kee,Huo,Uuo,VP,Juo,Yuo,Kuo,Au,Zuo,Ree,e0o,o0o,See,r0o,t0o,Pee,a0o,n0o,BE,bBe,Yi,Lu,$ee,xE,s0o,Iee,l0o,vBe,Ko,kE,i0o,Ki,d0o,Dee,c0o,f0o,jee,m0o,g0o,h0o,RE,p0o,Nee,_0o,u0o,b0o,zr,SE,v0o,qee,T0o,F0o,Zi,C0o,Gee,M0o,E0o,Oee,y0o,w0o,A0o,Xee,L0o,B0o,PE,x0o,qe,$E,k0o,Vee,R0o,S0o,Oa,P0o,zee,$0o,I0o,Wee,D0o,j0o,Qee,N0o,q0o,G0o,I,Bu,Hee,O0o,X0o,zP,V0o,z0o,W0o,xu,Uee,Q0o,H0o,WP,U0o,J0o,Y0o,ku,Jee,K0o,Z0o,QP,e1o,o1o,r1o,Ru,Yee,t1o,a1o,HP,n1o,s1o,l1o,Su,Kee,i1o,d1o,UP,c1o,f1o,m1o,Pu,Zee,g1o,h1o,JP,p1o,_1o,u1o,$u,eoe,b1o,v1o,YP,T1o,F1o,C1o,Iu,ooe,M1o,E1o,KP,y1o,w1o,A1o,Du,roe,L1o,B1o,ZP,x1o,k1o,R1o,ju,toe,S1o,P1o,e$,$1o,I1o,D1o,Nu,aoe,j1o,N1o,o$,q1o,G1o,O1o,qu,noe,X1o,V1o,r$,z1o,W1o,Q1o,Gu,soe,H1o,U1o,t$,J1o,Y1o,K1o,Ou,loe,Z1o,ebo,a$,obo,rbo,tbo,Xu,ioe,abo,nbo,n$,sbo,lbo,ibo,Vu,doe,dbo,cbo,s$,fbo,mbo,gbo,zu,coe,hbo,pbo,l$,_bo,ubo,bbo,Wu,foe,vbo,Tbo,i$,Fbo,Cbo,Mbo,Qu,moe,Ebo,ybo,d$,wbo,Abo,Lbo,Hu,goe,Bbo,xbo,c$,kbo,Rbo,Sbo,Uu,hoe,Pbo,$bo,f$,Ibo,Dbo,jbo,Ju,poe,Nbo,qbo,m$,Gbo,Obo,Xbo,Yu,_oe,Vbo,zbo,g$,Wbo,Qbo,Hbo,Ku,uoe,Ubo,Jbo,h$,Ybo,Kbo,Zbo,Zu,boe,e5o,o5o,p$,r5o,t5o,a5o,e0,voe,n5o,s5o,_$,l5o,i5o,d5o,o0,Toe,c5o,f5o,u$,m5o,g5o,h5o,r0,Foe,p5o,_5o,b$,u5o,b5o,v5o,t0,Coe,T5o,F5o,v$,C5o,M5o,E5o,a0,Moe,y5o,w5o,T$,A5o,L5o,B5o,n0,Eoe,x5o,k5o,yoe,R5o,S5o,P5o,s0,woe,$5o,I5o,F$,D5o,j5o,N5o,l0,Aoe,q5o,G5o,C$,O5o,X5o,V5o,i0,Loe,z5o,W5o,M$,Q5o,H5o,U5o,d0,Boe,J5o,Y5o,E$,K5o,Z5o,e2o,c0,o2o,xoe,r2o,t2o,koe,a2o,n2o,Roe,s2o,l2o,IE,TBe,ed,f0,Soe,DE,i2o,Poe,d2o,FBe,Zo,jE,c2o,od,f2o,$oe,m2o,g2o,Ioe,h2o,p2o,_2o,NE,u2o,Doe,b2o,v2o,T2o,Wr,qE,F2o,joe,C2o,M2o,rd,E2o,Noe,y2o,w2o,qoe,A2o,L2o,B2o,Goe,x2o,k2o,GE,R2o,Ge,OE,S2o,Ooe,P2o,$2o,Xa,I2o,Xoe,D2o,j2o,Voe,N2o,q2o,zoe,G2o,O2o,X2o,ae,m0,Woe,V2o,z2o,y$,W2o,Q2o,H2o,g0,Qoe,U2o,J2o,w$,Y2o,K2o,Z2o,h0,Hoe,evo,ovo,A$,rvo,tvo,avo,p0,Uoe,nvo,svo,L$,lvo,ivo,dvo,_0,Joe,cvo,fvo,B$,mvo,gvo,hvo,u0,Yoe,pvo,_vo,x$,uvo,bvo,vvo,b0,Koe,Tvo,Fvo,k$,Cvo,Mvo,Evo,v0,Zoe,yvo,wvo,R$,Avo,Lvo,Bvo,T0,ere,xvo,kvo,S$,Rvo,Svo,Pvo,F0,ore,$vo,Ivo,P$,Dvo,jvo,Nvo,C0,rre,qvo,Gvo,$$,Ovo,Xvo,Vvo,M0,tre,zvo,Wvo,I$,Qvo,Hvo,Uvo,E0,are,Jvo,Yvo,D$,Kvo,Zvo,eTo,y0,nre,oTo,rTo,j$,tTo,aTo,nTo,w0,sre,sTo,lTo,N$,iTo,dTo,cTo,A0,lre,fTo,mTo,q$,gTo,hTo,pTo,L0,_To,ire,uTo,bTo,dre,vTo,TTo,cre,FTo,CTo,XE,CBe,td,B0,fre,VE,MTo,mre,ETo,MBe,er,zE,yTo,ad,wTo,gre,ATo,LTo,hre,BTo,xTo,kTo,WE,RTo,pre,STo,PTo,$To,Qr,QE,ITo,_re,DTo,jTo,nd,NTo,ure,qTo,GTo,bre,OTo,XTo,VTo,vre,zTo,WTo,HE,QTo,Oe,UE,HTo,Tre,UTo,JTo,Va,YTo,Fre,KTo,ZTo,Cre,eFo,oFo,Mre,rFo,tFo,aFo,A,x0,Ere,nFo,sFo,G$,lFo,iFo,dFo,k0,yre,cFo,fFo,O$,mFo,gFo,hFo,R0,wre,pFo,_Fo,X$,uFo,bFo,vFo,S0,Are,TFo,FFo,V$,CFo,MFo,EFo,P0,Lre,yFo,wFo,z$,AFo,LFo,BFo,$0,Bre,xFo,kFo,W$,RFo,SFo,PFo,I0,xre,$Fo,IFo,Q$,DFo,jFo,NFo,D0,kre,qFo,GFo,H$,OFo,XFo,VFo,j0,Rre,zFo,WFo,U$,QFo,HFo,UFo,N0,Sre,JFo,YFo,J$,KFo,ZFo,e9o,q0,Pre,o9o,r9o,Y$,t9o,a9o,n9o,G0,$re,s9o,l9o,K$,i9o,d9o,c9o,O0,Ire,f9o,m9o,Z$,g9o,h9o,p9o,X0,Dre,_9o,u9o,eI,b9o,v9o,T9o,V0,jre,F9o,C9o,oI,M9o,E9o,y9o,z0,Nre,w9o,A9o,rI,L9o,B9o,x9o,W0,qre,k9o,R9o,tI,S9o,P9o,$9o,Q0,Gre,I9o,D9o,aI,j9o,N9o,q9o,H0,Ore,G9o,O9o,nI,X9o,V9o,z9o,U0,Xre,W9o,Q9o,sI,H9o,U9o,J9o,J0,Vre,Y9o,K9o,lI,Z9o,eCo,oCo,Y0,zre,rCo,tCo,iI,aCo,nCo,sCo,K0,Wre,lCo,iCo,dI,dCo,cCo,fCo,Z0,Qre,mCo,gCo,cI,hCo,pCo,_Co,e1,Hre,uCo,bCo,fI,vCo,TCo,FCo,o1,Ure,CCo,MCo,mI,ECo,yCo,wCo,r1,Jre,ACo,LCo,gI,BCo,xCo,kCo,t1,Yre,RCo,SCo,hI,PCo,$Co,ICo,a1,Kre,DCo,jCo,pI,NCo,qCo,GCo,n1,Zre,OCo,XCo,_I,VCo,zCo,WCo,s1,ete,QCo,HCo,uI,UCo,JCo,YCo,l1,ote,KCo,ZCo,bI,eMo,oMo,rMo,i1,rte,tMo,aMo,vI,nMo,sMo,lMo,d1,tte,iMo,dMo,TI,cMo,fMo,mMo,c1,ate,gMo,hMo,FI,pMo,_Mo,uMo,f1,nte,bMo,vMo,CI,TMo,FMo,CMo,m1,ste,MMo,EMo,MI,yMo,wMo,AMo,g1,lte,LMo,BMo,EI,xMo,kMo,RMo,h1,ite,SMo,PMo,yI,$Mo,IMo,DMo,p1,dte,jMo,NMo,wI,qMo,GMo,OMo,_1,cte,XMo,VMo,AI,zMo,WMo,QMo,u1,fte,HMo,UMo,LI,JMo,YMo,KMo,b1,mte,ZMo,e4o,BI,o4o,r4o,t4o,v1,gte,a4o,n4o,xI,s4o,l4o,i4o,T1,hte,d4o,c4o,kI,f4o,m4o,g4o,F1,pte,h4o,p4o,RI,_4o,u4o,b4o,C1,v4o,_te,T4o,F4o,ute,C4o,M4o,bte,E4o,y4o,JE,EBe,sd,M1,vte,YE,w4o,Tte,A4o,yBe,or,KE,L4o,ld,B4o,Fte,x4o,k4o,Cte,R4o,S4o,P4o,ZE,$4o,Mte,I4o,D4o,j4o,Hr,e3,N4o,Ete,q4o,G4o,id,O4o,yte,X4o,V4o,wte,z4o,W4o,Q4o,Ate,H4o,U4o,o3,J4o,Xe,r3,Y4o,Lte,K4o,Z4o,za,eEo,Bte,oEo,rEo,xte,tEo,aEo,kte,nEo,sEo,lEo,G,E1,Rte,iEo,dEo,SI,cEo,fEo,mEo,y1,Ste,gEo,hEo,PI,pEo,_Eo,uEo,w1,Pte,bEo,vEo,$I,TEo,FEo,CEo,A1,$te,MEo,EEo,II,yEo,wEo,AEo,L1,Ite,LEo,BEo,DI,xEo,kEo,REo,B1,Dte,SEo,PEo,jI,$Eo,IEo,DEo,x1,jte,jEo,NEo,NI,qEo,GEo,OEo,k1,Nte,XEo,VEo,qI,zEo,WEo,QEo,R1,qte,HEo,UEo,GI,JEo,YEo,KEo,S1,Gte,ZEo,e3o,OI,o3o,r3o,t3o,P1,Ote,a3o,n3o,XI,s3o,l3o,i3o,$1,Xte,d3o,c3o,VI,f3o,m3o,g3o,I1,Vte,h3o,p3o,zI,_3o,u3o,b3o,D1,zte,v3o,T3o,WI,F3o,C3o,M3o,j1,Wte,E3o,y3o,QI,w3o,A3o,L3o,N1,Qte,B3o,x3o,HI,k3o,R3o,S3o,q1,Hte,P3o,$3o,UI,I3o,D3o,j3o,G1,Ute,N3o,q3o,JI,G3o,O3o,X3o,O1,Jte,V3o,z3o,YI,W3o,Q3o,H3o,X1,Yte,U3o,J3o,KI,Y3o,K3o,Z3o,V1,Kte,eyo,oyo,ZI,ryo,tyo,ayo,z1,Zte,nyo,syo,eD,lyo,iyo,dyo,W1,eae,cyo,fyo,oD,myo,gyo,hyo,Q1,oae,pyo,_yo,rD,uyo,byo,vyo,H1,rae,Tyo,Fyo,tD,Cyo,Myo,Eyo,U1,tae,yyo,wyo,aD,Ayo,Lyo,Byo,J1,aae,xyo,kyo,nD,Ryo,Syo,Pyo,Y1,nae,$yo,Iyo,sD,Dyo,jyo,Nyo,K1,qyo,sae,Gyo,Oyo,lae,Xyo,Vyo,iae,zyo,Wyo,t3,wBe,dd,Z1,dae,a3,Qyo,cae,Hyo,ABe,rr,n3,Uyo,cd,Jyo,fae,Yyo,Kyo,mae,Zyo,ewo,owo,s3,rwo,gae,two,awo,nwo,Ur,l3,swo,hae,lwo,iwo,fd,dwo,pae,cwo,fwo,_ae,mwo,gwo,hwo,uae,pwo,_wo,i3,uwo,Ve,d3,bwo,bae,vwo,Two,Wa,Fwo,vae,Cwo,Mwo,Tae,Ewo,ywo,Fae,wwo,Awo,Lwo,na,eb,Cae,Bwo,xwo,lD,kwo,Rwo,Swo,ob,Mae,Pwo,$wo,iD,Iwo,Dwo,jwo,rb,Eae,Nwo,qwo,dD,Gwo,Owo,Xwo,tb,yae,Vwo,zwo,cD,Wwo,Qwo,Hwo,ab,wae,Uwo,Jwo,fD,Ywo,Kwo,Zwo,nb,e6o,Aae,o6o,r6o,Lae,t6o,a6o,Bae,n6o,s6o,c3,LBe,md,sb,xae,f3,l6o,kae,i6o,BBe,tr,m3,d6o,gd,c6o,Rae,f6o,m6o,Sae,g6o,h6o,p6o,g3,_6o,Pae,u6o,b6o,v6o,Jr,h3,T6o,$ae,F6o,C6o,hd,M6o,Iae,E6o,y6o,Dae,w6o,A6o,L6o,jae,B6o,x6o,p3,k6o,ze,_3,R6o,Nae,S6o,P6o,Qa,$6o,qae,I6o,D6o,Gae,j6o,N6o,Oae,q6o,G6o,O6o,N,lb,Xae,X6o,V6o,mD,z6o,W6o,Q6o,ib,Vae,H6o,U6o,gD,J6o,Y6o,K6o,db,zae,Z6o,eAo,hD,oAo,rAo,tAo,cb,Wae,aAo,nAo,pD,sAo,lAo,iAo,fb,Qae,dAo,cAo,_D,fAo,mAo,gAo,mb,Hae,hAo,pAo,uD,_Ao,uAo,bAo,gb,Uae,vAo,TAo,bD,FAo,CAo,MAo,hb,Jae,EAo,yAo,vD,wAo,AAo,LAo,pb,Yae,BAo,xAo,TD,kAo,RAo,SAo,_b,Kae,PAo,$Ao,FD,IAo,DAo,jAo,ub,Zae,NAo,qAo,CD,GAo,OAo,XAo,bb,ene,VAo,zAo,MD,WAo,QAo,HAo,vb,one,UAo,JAo,ED,YAo,KAo,ZAo,Tb,rne,eLo,oLo,yD,rLo,tLo,aLo,Fb,tne,nLo,sLo,wD,lLo,iLo,dLo,Cb,ane,cLo,fLo,AD,mLo,gLo,hLo,Mb,nne,pLo,_Lo,LD,uLo,bLo,vLo,Eb,sne,TLo,FLo,BD,CLo,MLo,ELo,yb,lne,yLo,wLo,xD,ALo,LLo,BLo,wb,ine,xLo,kLo,kD,RLo,SLo,PLo,Ab,dne,$Lo,ILo,RD,DLo,jLo,NLo,Lb,cne,qLo,GLo,SD,OLo,XLo,VLo,Bb,fne,zLo,WLo,PD,QLo,HLo,ULo,xb,mne,JLo,YLo,$D,KLo,ZLo,e8o,kb,gne,o8o,r8o,ID,t8o,a8o,n8o,Rb,hne,s8o,l8o,DD,i8o,d8o,c8o,Sb,pne,f8o,m8o,jD,g8o,h8o,p8o,Pb,_ne,_8o,u8o,ND,b8o,v8o,T8o,$b,une,F8o,C8o,qD,M8o,E8o,y8o,Ib,bne,w8o,A8o,GD,L8o,B8o,x8o,Db,vne,k8o,R8o,OD,S8o,P8o,$8o,jb,Tne,I8o,D8o,XD,j8o,N8o,q8o,Nb,Fne,G8o,O8o,VD,X8o,V8o,z8o,qb,W8o,Cne,Q8o,H8o,Mne,U8o,J8o,Ene,Y8o,K8o,u3,xBe,pd,Gb,yne,b3,Z8o,wne,e7o,kBe,ar,v3,o7o,_d,r7o,Ane,t7o,a7o,Lne,n7o,s7o,l7o,T3,i7o,Bne,d7o,c7o,f7o,Yr,F3,m7o,xne,g7o,h7o,ud,p7o,kne,_7o,u7o,Rne,b7o,v7o,T7o,Sne,F7o,C7o,C3,M7o,We,M3,E7o,Pne,y7o,w7o,Ha,A7o,$ne,L7o,B7o,Ine,x7o,k7o,Dne,R7o,S7o,P7o,R,Ob,jne,$7o,I7o,zD,D7o,j7o,N7o,Xb,Nne,q7o,G7o,WD,O7o,X7o,V7o,Vb,qne,z7o,W7o,QD,Q7o,H7o,U7o,zb,Gne,J7o,Y7o,HD,K7o,Z7o,eBo,Wb,One,oBo,rBo,UD,tBo,aBo,nBo,Qb,Xne,sBo,lBo,JD,iBo,dBo,cBo,Hb,Vne,fBo,mBo,YD,gBo,hBo,pBo,Ub,zne,_Bo,uBo,KD,bBo,vBo,TBo,Jb,Wne,FBo,CBo,ZD,MBo,EBo,yBo,Yb,Qne,wBo,ABo,ej,LBo,BBo,xBo,Kb,Hne,kBo,RBo,oj,SBo,PBo,$Bo,Zb,Une,IBo,DBo,rj,jBo,NBo,qBo,e5,Jne,GBo,OBo,tj,XBo,VBo,zBo,o5,Yne,WBo,QBo,aj,HBo,UBo,JBo,r5,Kne,YBo,KBo,nj,ZBo,exo,oxo,t5,Zne,rxo,txo,sj,axo,nxo,sxo,a5,ese,lxo,ixo,lj,dxo,cxo,fxo,n5,ose,mxo,gxo,ij,hxo,pxo,_xo,s5,rse,uxo,bxo,dj,vxo,Txo,Fxo,l5,tse,Cxo,Mxo,cj,Exo,yxo,wxo,i5,ase,Axo,Lxo,fj,Bxo,xxo,kxo,d5,nse,Rxo,Sxo,mj,Pxo,$xo,Ixo,c5,sse,Dxo,jxo,gj,Nxo,qxo,Gxo,f5,lse,Oxo,Xxo,hj,Vxo,zxo,Wxo,m5,ise,Qxo,Hxo,pj,Uxo,Jxo,Yxo,g5,dse,Kxo,Zxo,_j,eko,oko,rko,h5,cse,tko,ako,uj,nko,sko,lko,p5,fse,iko,dko,bj,cko,fko,mko,_5,mse,gko,hko,vj,pko,_ko,uko,u5,gse,bko,vko,Tj,Tko,Fko,Cko,b5,hse,Mko,Eko,Fj,yko,wko,Ako,v5,pse,Lko,Bko,Cj,xko,kko,Rko,T5,_se,Sko,Pko,Mj,$ko,Iko,Dko,F5,use,jko,Nko,Ej,qko,Gko,Oko,C5,bse,Xko,Vko,yj,zko,Wko,Qko,M5,vse,Hko,Uko,wj,Jko,Yko,Kko,E5,Tse,Zko,eRo,Aj,oRo,rRo,tRo,y5,Fse,aRo,nRo,Lj,sRo,lRo,iRo,w5,Cse,dRo,cRo,Bj,fRo,mRo,gRo,A5,hRo,Mse,pRo,_Ro,Ese,uRo,bRo,yse,vRo,TRo,E3,RBe,bd,L5,wse,y3,FRo,Ase,CRo,SBe,nr,w3,MRo,vd,ERo,Lse,yRo,wRo,Bse,ARo,LRo,BRo,A3,xRo,xse,kRo,RRo,SRo,Kr,L3,PRo,kse,$Ro,IRo,Td,DRo,Rse,jRo,NRo,Sse,qRo,GRo,ORo,Pse,XRo,VRo,B3,zRo,Qe,x3,WRo,$se,QRo,HRo,Ua,URo,Ise,JRo,YRo,Dse,KRo,ZRo,jse,eSo,oSo,rSo,Nse,B5,qse,tSo,aSo,xj,nSo,sSo,lSo,x5,iSo,Gse,dSo,cSo,Ose,fSo,mSo,Xse,gSo,hSo,k3,PBe,Fd,k5,Vse,R3,pSo,zse,_So,$Be,sr,S3,uSo,Cd,bSo,Wse,vSo,TSo,Qse,FSo,CSo,MSo,P3,ESo,Hse,ySo,wSo,ASo,Zr,$3,LSo,Use,BSo,xSo,Md,kSo,Jse,RSo,SSo,Yse,PSo,$So,ISo,Kse,DSo,jSo,I3,NSo,He,D3,qSo,Zse,GSo,OSo,Ja,XSo,ele,VSo,zSo,ole,WSo,QSo,rle,HSo,USo,JSo,Fe,R5,tle,YSo,KSo,kj,ZSo,ePo,oPo,S5,ale,rPo,tPo,Rj,aPo,nPo,sPo,Ps,nle,lPo,iPo,Sj,dPo,cPo,Pj,fPo,mPo,gPo,P5,sle,hPo,pPo,$j,_Po,uPo,bPo,la,lle,vPo,TPo,Ij,FPo,CPo,Dj,MPo,EPo,jj,yPo,wPo,APo,$5,ile,LPo,BPo,Nj,xPo,kPo,RPo,I5,dle,SPo,PPo,qj,$Po,IPo,DPo,D5,cle,jPo,NPo,Gj,qPo,GPo,OPo,j5,fle,XPo,VPo,Oj,zPo,WPo,QPo,N5,HPo,mle,UPo,JPo,gle,YPo,KPo,hle,ZPo,e$o,j3,IBe,Ed,q5,ple,N3,o$o,_le,r$o,DBe,lr,q3,t$o,yd,a$o,ule,n$o,s$o,ble,l$o,i$o,d$o,G3,c$o,vle,f$o,m$o,g$o,et,O3,h$o,Tle,p$o,_$o,wd,u$o,Fle,b$o,v$o,Cle,T$o,F$o,C$o,Mle,M$o,E$o,X3,y$o,Ue,V3,w$o,Ele,A$o,L$o,Ya,B$o,yle,x$o,k$o,wle,R$o,S$o,Ale,P$o,$$o,I$o,Lle,G5,Ble,D$o,j$o,Xj,N$o,q$o,G$o,O5,O$o,xle,X$o,V$o,kle,z$o,W$o,Rle,Q$o,H$o,z3,jBe,Ad,X5,Sle,W3,U$o,Ple,J$o,NBe,ir,Q3,Y$o,Ld,K$o,$le,Z$o,eIo,Ile,oIo,rIo,tIo,H3,aIo,Dle,nIo,sIo,lIo,ot,U3,iIo,jle,dIo,cIo,Bd,fIo,Nle,mIo,gIo,qle,hIo,pIo,_Io,Gle,uIo,bIo,J3,vIo,Je,Y3,TIo,Ole,FIo,CIo,Ka,MIo,Xle,EIo,yIo,Vle,wIo,AIo,zle,LIo,BIo,xIo,xe,V5,Wle,kIo,RIo,Vj,SIo,PIo,$Io,z5,Qle,IIo,DIo,zj,jIo,NIo,qIo,W5,Hle,GIo,OIo,Wj,XIo,VIo,zIo,Q5,Ule,WIo,QIo,Qj,HIo,UIo,JIo,H5,Jle,YIo,KIo,Hj,ZIo,eDo,oDo,U5,Yle,rDo,tDo,Uj,aDo,nDo,sDo,J5,Kle,lDo,iDo,Jj,dDo,cDo,fDo,Y5,Zle,mDo,gDo,Yj,hDo,pDo,_Do,K5,uDo,eie,bDo,vDo,oie,TDo,FDo,rie,CDo,MDo,K3,qBe,xd,Z5,tie,Z3,EDo,aie,yDo,GBe,dr,ey,wDo,kd,ADo,nie,LDo,BDo,sie,xDo,kDo,RDo,oy,SDo,lie,PDo,$Do,IDo,rt,ry,DDo,iie,jDo,NDo,Rd,qDo,die,GDo,ODo,cie,XDo,VDo,zDo,fie,WDo,QDo,ty,HDo,Ye,ay,UDo,mie,JDo,YDo,Za,KDo,gie,ZDo,ejo,hie,ojo,rjo,pie,tjo,ajo,njo,en,e2,_ie,sjo,ljo,Kj,ijo,djo,cjo,o2,uie,fjo,mjo,Zj,gjo,hjo,pjo,r2,bie,_jo,ujo,eN,bjo,vjo,Tjo,t2,vie,Fjo,Cjo,oN,Mjo,Ejo,yjo,a2,wjo,Tie,Ajo,Ljo,Fie,Bjo,xjo,Cie,kjo,Rjo,ny,OBe,Sd,n2,Mie,sy,Sjo,Eie,Pjo,XBe,cr,ly,$jo,Pd,Ijo,yie,Djo,jjo,wie,Njo,qjo,Gjo,iy,Ojo,Aie,Xjo,Vjo,zjo,tt,dy,Wjo,Lie,Qjo,Hjo,$d,Ujo,Bie,Jjo,Yjo,xie,Kjo,Zjo,eNo,kie,oNo,rNo,cy,tNo,Ke,fy,aNo,Rie,nNo,sNo,on,lNo,Sie,iNo,dNo,Pie,cNo,fNo,$ie,mNo,gNo,hNo,ke,s2,Iie,pNo,_No,rN,uNo,bNo,vNo,l2,Die,TNo,FNo,tN,CNo,MNo,ENo,i2,jie,yNo,wNo,aN,ANo,LNo,BNo,d2,Nie,xNo,kNo,nN,RNo,SNo,PNo,c2,qie,$No,INo,sN,DNo,jNo,NNo,f2,Gie,qNo,GNo,lN,ONo,XNo,VNo,m2,Oie,zNo,WNo,iN,QNo,HNo,UNo,g2,Xie,JNo,YNo,dN,KNo,ZNo,eqo,h2,oqo,Vie,rqo,tqo,zie,aqo,nqo,Wie,sqo,lqo,my,VBe,Id,p2,Qie,gy,iqo,Hie,dqo,zBe,fr,hy,cqo,Dd,fqo,Uie,mqo,gqo,Jie,hqo,pqo,_qo,py,uqo,Yie,bqo,vqo,Tqo,at,_y,Fqo,Kie,Cqo,Mqo,jd,Eqo,Zie,yqo,wqo,ede,Aqo,Lqo,Bqo,ode,xqo,kqo,uy,Rqo,Ze,by,Sqo,rde,Pqo,$qo,rn,Iqo,tde,Dqo,jqo,ade,Nqo,qqo,nde,Gqo,Oqo,Xqo,vy,_2,sde,Vqo,zqo,cN,Wqo,Qqo,Hqo,u2,lde,Uqo,Jqo,fN,Yqo,Kqo,Zqo,b2,eGo,ide,oGo,rGo,dde,tGo,aGo,cde,nGo,sGo,Ty,WBe,Nd,v2,fde,Fy,lGo,mde,iGo,QBe,mr,Cy,dGo,qd,cGo,gde,fGo,mGo,hde,gGo,hGo,pGo,My,_Go,pde,uGo,bGo,vGo,nt,Ey,TGo,_de,FGo,CGo,Gd,MGo,ude,EGo,yGo,bde,wGo,AGo,LGo,vde,BGo,xGo,yy,kGo,eo,wy,RGo,Tde,SGo,PGo,tn,$Go,Fde,IGo,DGo,Cde,jGo,NGo,Mde,qGo,GGo,OGo,an,T2,Ede,XGo,VGo,mN,zGo,WGo,QGo,F2,yde,HGo,UGo,gN,JGo,YGo,KGo,C2,wde,ZGo,eOo,hN,oOo,rOo,tOo,M2,Ade,aOo,nOo,pN,sOo,lOo,iOo,E2,dOo,Lde,cOo,fOo,Bde,mOo,gOo,xde,hOo,pOo,Ay,HBe,Od,y2,kde,Ly,_Oo,Rde,uOo,UBe,gr,By,bOo,Xd,vOo,Sde,TOo,FOo,Pde,COo,MOo,EOo,xy,yOo,$de,wOo,AOo,LOo,st,ky,BOo,Ide,xOo,kOo,Vd,ROo,Dde,SOo,POo,jde,$Oo,IOo,DOo,Nde,jOo,NOo,Ry,qOo,oo,Sy,GOo,qde,OOo,XOo,nn,VOo,Gde,zOo,WOo,Ode,QOo,HOo,Xde,UOo,JOo,YOo,zd,w2,Vde,KOo,ZOo,_N,eXo,oXo,rXo,A2,zde,tXo,aXo,uN,nXo,sXo,lXo,L2,Wde,iXo,dXo,bN,cXo,fXo,mXo,B2,gXo,Qde,hXo,pXo,Hde,_Xo,uXo,Ude,bXo,vXo,Py,JBe,Wd,x2,Jde,$y,TXo,Yde,FXo,YBe,hr,Iy,CXo,Qd,MXo,Kde,EXo,yXo,Zde,wXo,AXo,LXo,Dy,BXo,ece,xXo,kXo,RXo,lt,jy,SXo,oce,PXo,$Xo,Hd,IXo,rce,DXo,jXo,tce,NXo,qXo,GXo,ace,OXo,XXo,Ny,VXo,ro,qy,zXo,nce,WXo,QXo,sn,HXo,sce,UXo,JXo,lce,YXo,KXo,ice,ZXo,eVo,oVo,dce,k2,cce,rVo,tVo,vN,aVo,nVo,sVo,R2,lVo,fce,iVo,dVo,mce,cVo,fVo,gce,mVo,gVo,Gy,KBe,Ud,S2,hce,Oy,hVo,pce,pVo,ZBe,pr,Xy,_Vo,Jd,uVo,_ce,bVo,vVo,uce,TVo,FVo,CVo,Vy,MVo,bce,EVo,yVo,wVo,it,zy,AVo,vce,LVo,BVo,Yd,xVo,Tce,kVo,RVo,Fce,SVo,PVo,$Vo,Cce,IVo,DVo,Wy,jVo,to,Qy,NVo,Mce,qVo,GVo,ln,OVo,Ece,XVo,VVo,yce,zVo,WVo,wce,QVo,HVo,UVo,Ace,P2,Lce,JVo,YVo,TN,KVo,ZVo,ezo,$2,ozo,Bce,rzo,tzo,xce,azo,nzo,kce,szo,lzo,Hy,exe,Kd,I2,Rce,Uy,izo,Sce,dzo,oxe,_r,Jy,czo,Zd,fzo,Pce,mzo,gzo,$ce,hzo,pzo,_zo,Yy,uzo,Ice,bzo,vzo,Tzo,dt,Ky,Fzo,Dce,Czo,Mzo,ec,Ezo,jce,yzo,wzo,Nce,Azo,Lzo,Bzo,qce,xzo,kzo,Zy,Rzo,ao,ew,Szo,Gce,Pzo,$zo,dn,Izo,Oce,Dzo,jzo,Xce,Nzo,qzo,Vce,Gzo,Ozo,Xzo,ow,D2,zce,Vzo,zzo,FN,Wzo,Qzo,Hzo,j2,Wce,Uzo,Jzo,CN,Yzo,Kzo,Zzo,N2,eWo,Qce,oWo,rWo,Hce,tWo,aWo,Uce,nWo,sWo,rw,rxe,oc,q2,Jce,tw,lWo,Yce,iWo,txe,ur,aw,dWo,rc,cWo,Kce,fWo,mWo,Zce,gWo,hWo,pWo,nw,_Wo,efe,uWo,bWo,vWo,ct,sw,TWo,ofe,FWo,CWo,tc,MWo,rfe,EWo,yWo,tfe,wWo,AWo,LWo,afe,BWo,xWo,lw,kWo,go,iw,RWo,nfe,SWo,PWo,cn,$Wo,sfe,IWo,DWo,lfe,jWo,NWo,ife,qWo,GWo,OWo,B,G2,dfe,XWo,VWo,MN,zWo,WWo,QWo,O2,cfe,HWo,UWo,EN,JWo,YWo,KWo,X2,ffe,ZWo,eQo,yN,oQo,rQo,tQo,V2,mfe,aQo,nQo,wN,sQo,lQo,iQo,z2,gfe,dQo,cQo,AN,fQo,mQo,gQo,W2,hfe,hQo,pQo,LN,_Qo,uQo,bQo,Q2,pfe,vQo,TQo,BN,FQo,CQo,MQo,H2,_fe,EQo,yQo,xN,wQo,AQo,LQo,U2,ufe,BQo,xQo,kN,kQo,RQo,SQo,J2,bfe,PQo,$Qo,RN,IQo,DQo,jQo,Y2,vfe,NQo,qQo,SN,GQo,OQo,XQo,K2,Tfe,VQo,zQo,PN,WQo,QQo,HQo,Z2,Ffe,UQo,JQo,$N,YQo,KQo,ZQo,ev,Cfe,eHo,oHo,IN,rHo,tHo,aHo,ov,Mfe,nHo,sHo,DN,lHo,iHo,dHo,rv,Efe,cHo,fHo,jN,mHo,gHo,hHo,$s,yfe,pHo,_Ho,NN,uHo,bHo,qN,vHo,THo,FHo,tv,wfe,CHo,MHo,GN,EHo,yHo,wHo,av,Afe,AHo,LHo,ON,BHo,xHo,kHo,nv,Lfe,RHo,SHo,XN,PHo,$Ho,IHo,sv,Bfe,DHo,jHo,VN,NHo,qHo,GHo,lv,xfe,OHo,XHo,zN,VHo,zHo,WHo,iv,kfe,QHo,HHo,WN,UHo,JHo,YHo,dv,Rfe,KHo,ZHo,QN,eUo,oUo,rUo,cv,Sfe,tUo,aUo,HN,nUo,sUo,lUo,fv,Pfe,iUo,dUo,UN,cUo,fUo,mUo,mv,$fe,gUo,hUo,JN,pUo,_Uo,uUo,gv,Ife,bUo,vUo,YN,TUo,FUo,CUo,hv,Dfe,MUo,EUo,KN,yUo,wUo,AUo,pv,jfe,LUo,BUo,ZN,xUo,kUo,RUo,_v,Nfe,SUo,PUo,eq,$Uo,IUo,DUo,uv,qfe,jUo,NUo,oq,qUo,GUo,OUo,bv,Gfe,XUo,VUo,rq,zUo,WUo,QUo,vv,Ofe,HUo,UUo,tq,JUo,YUo,KUo,Tv,Xfe,ZUo,eJo,aq,oJo,rJo,tJo,Fv,Vfe,aJo,nJo,nq,sJo,lJo,iJo,Cv,zfe,dJo,cJo,sq,fJo,mJo,gJo,Mv,Wfe,hJo,pJo,lq,_Jo,uJo,bJo,Ev,Qfe,vJo,TJo,iq,FJo,CJo,MJo,yv,Hfe,EJo,yJo,dq,wJo,AJo,LJo,wv,Ufe,BJo,xJo,cq,kJo,RJo,SJo,Av,Jfe,PJo,$Jo,fq,IJo,DJo,jJo,Yfe,NJo,qJo,dw,axe,ac,Lv,Kfe,cw,GJo,Zfe,OJo,nxe,br,fw,XJo,nc,VJo,eme,zJo,WJo,ome,QJo,HJo,UJo,mw,JJo,rme,YJo,KJo,ZJo,ft,gw,eYo,tme,oYo,rYo,sc,tYo,ame,aYo,nYo,nme,sYo,lYo,iYo,sme,dYo,cYo,hw,fYo,ho,pw,mYo,lme,gYo,hYo,fn,pYo,ime,_Yo,uYo,dme,bYo,vYo,cme,TYo,FYo,CYo,H,Bv,fme,MYo,EYo,mq,yYo,wYo,AYo,xv,mme,LYo,BYo,gq,xYo,kYo,RYo,kv,gme,SYo,PYo,hq,$Yo,IYo,DYo,Rv,hme,jYo,NYo,pq,qYo,GYo,OYo,Sv,pme,XYo,VYo,_q,zYo,WYo,QYo,Pv,_me,HYo,UYo,uq,JYo,YYo,KYo,$v,ume,ZYo,eKo,bq,oKo,rKo,tKo,Iv,bme,aKo,nKo,vq,sKo,lKo,iKo,Dv,vme,dKo,cKo,Tq,fKo,mKo,gKo,jv,Tme,hKo,pKo,Fq,_Ko,uKo,bKo,Nv,Fme,vKo,TKo,Cq,FKo,CKo,MKo,qv,Cme,EKo,yKo,Mq,wKo,AKo,LKo,Gv,Mme,BKo,xKo,Eq,kKo,RKo,SKo,Ov,Eme,PKo,$Ko,yq,IKo,DKo,jKo,Xv,yme,NKo,qKo,wq,GKo,OKo,XKo,Vv,wme,VKo,zKo,Aq,WKo,QKo,HKo,zv,Ame,UKo,JKo,Lq,YKo,KKo,ZKo,Wv,Lme,eZo,oZo,Bq,rZo,tZo,aZo,Qv,Bme,nZo,sZo,xq,lZo,iZo,dZo,Hv,xme,cZo,fZo,kq,mZo,gZo,hZo,Uv,kme,pZo,_Zo,Rq,uZo,bZo,vZo,Jv,Rme,TZo,FZo,Sq,CZo,MZo,EZo,Sme,yZo,wZo,_w,sxe,lc,Yv,Pme,uw,AZo,$me,LZo,lxe,vr,bw,BZo,ic,xZo,Ime,kZo,RZo,Dme,SZo,PZo,$Zo,vw,IZo,jme,DZo,jZo,NZo,mt,Tw,qZo,Nme,GZo,OZo,dc,XZo,qme,VZo,zZo,Gme,WZo,QZo,HZo,Ome,UZo,JZo,Fw,YZo,po,Cw,KZo,Xme,ZZo,eer,mn,oer,Vme,rer,ter,zme,aer,ner,Wme,ser,ler,ier,pe,Kv,Qme,der,cer,Pq,fer,mer,ger,Zv,Hme,her,per,$q,_er,uer,ber,eT,Ume,ver,Ter,Iq,Fer,Cer,Mer,oT,Jme,Eer,yer,Dq,wer,Aer,Ler,rT,Yme,Ber,xer,jq,ker,Rer,Ser,tT,Kme,Per,$er,Nq,Ier,Der,jer,aT,Zme,Ner,qer,qq,Ger,Oer,Xer,nT,ege,Ver,zer,Gq,Wer,Qer,Her,sT,oge,Uer,Jer,Oq,Yer,Ker,Zer,lT,rge,eor,oor,Xq,ror,tor,aor,tge,nor,sor,Mw,ixe,cc,iT,age,Ew,lor,nge,ior,dxe,Tr,yw,dor,fc,cor,sge,mor,gor,lge,hor,por,_or,ww,uor,ige,bor,vor,Tor,gt,Aw,For,dge,Cor,Mor,mc,Eor,cge,yor,wor,fge,Aor,Lor,Bor,mge,xor,kor,Lw,Ror,_o,Bw,Sor,gge,Por,$or,gn,Ior,hge,Dor,jor,pge,Nor,qor,_ge,Gor,Oor,Xor,xw,dT,uge,Vor,zor,Vq,Wor,Qor,Hor,cT,bge,Uor,Jor,zq,Yor,Kor,Zor,vge,err,orr,kw,cxe,gc,fT,Tge,Rw,rrr,Fge,trr,fxe,Fr,Sw,arr,hc,nrr,Cge,srr,lrr,Mge,irr,drr,crr,Pw,frr,Ege,mrr,grr,hrr,ht,$w,prr,yge,_rr,urr,pc,brr,wge,vrr,Trr,Age,Frr,Crr,Mrr,Lge,Err,yrr,Iw,wrr,uo,Dw,Arr,Bge,Lrr,Brr,hn,xrr,xge,krr,Rrr,kge,Srr,Prr,Rge,$rr,Irr,Drr,Y,mT,Sge,jrr,Nrr,Wq,qrr,Grr,Orr,gT,Pge,Xrr,Vrr,Qq,zrr,Wrr,Qrr,hT,$ge,Hrr,Urr,Hq,Jrr,Yrr,Krr,pT,Ige,Zrr,etr,Uq,otr,rtr,ttr,_T,Dge,atr,ntr,Jq,str,ltr,itr,uT,jge,dtr,ctr,Yq,ftr,mtr,gtr,bT,Nge,htr,ptr,Kq,_tr,utr,btr,vT,qge,vtr,Ttr,Zq,Ftr,Ctr,Mtr,TT,Gge,Etr,ytr,eG,wtr,Atr,Ltr,FT,Oge,Btr,xtr,oG,ktr,Rtr,Str,CT,Xge,Ptr,$tr,rG,Itr,Dtr,jtr,MT,Vge,Ntr,qtr,tG,Gtr,Otr,Xtr,ET,zge,Vtr,ztr,aG,Wtr,Qtr,Htr,yT,Wge,Utr,Jtr,nG,Ytr,Ktr,Ztr,wT,Qge,ear,oar,sG,rar,tar,aar,AT,Hge,nar,sar,lG,lar,iar,dar,LT,Uge,car,far,iG,mar,gar,har,BT,Jge,par,_ar,dG,uar,bar,Tar,xT,Yge,Far,Car,cG,Mar,Ear,yar,kT,Kge,war,Aar,fG,Lar,Bar,xar,Zge,kar,Rar,jw,mxe,_c,RT,ehe,Nw,Sar,ohe,Par,gxe,Cr,qw,$ar,uc,Iar,rhe,Dar,jar,the,Nar,qar,Gar,Gw,Oar,ahe,Xar,Var,zar,pt,Ow,War,nhe,Qar,Har,bc,Uar,she,Jar,Yar,lhe,Kar,Zar,enr,ihe,onr,rnr,Xw,tnr,bo,Vw,anr,dhe,nnr,snr,pn,lnr,che,inr,dnr,fhe,cnr,fnr,mhe,mnr,gnr,hnr,_e,ST,ghe,pnr,_nr,mG,unr,bnr,vnr,PT,hhe,Tnr,Fnr,gG,Cnr,Mnr,Enr,$T,phe,ynr,wnr,hG,Anr,Lnr,Bnr,IT,_he,xnr,knr,pG,Rnr,Snr,Pnr,DT,uhe,$nr,Inr,_G,Dnr,jnr,Nnr,jT,bhe,qnr,Gnr,uG,Onr,Xnr,Vnr,NT,vhe,znr,Wnr,bG,Qnr,Hnr,Unr,qT,The,Jnr,Ynr,vG,Knr,Znr,esr,GT,Fhe,osr,rsr,TG,tsr,asr,nsr,OT,Che,ssr,lsr,FG,isr,dsr,csr,Mhe,fsr,msr,zw,hxe,vc,XT,Ehe,Ww,gsr,yhe,hsr,pxe,Mr,Qw,psr,Tc,_sr,whe,usr,bsr,Ahe,vsr,Tsr,Fsr,Hw,Csr,Lhe,Msr,Esr,ysr,_t,Uw,wsr,Bhe,Asr,Lsr,Fc,Bsr,xhe,xsr,ksr,khe,Rsr,Ssr,Psr,Rhe,$sr,Isr,Jw,Dsr,vo,Yw,jsr,She,Nsr,qsr,_n,Gsr,Phe,Osr,Xsr,$he,Vsr,zsr,Ihe,Wsr,Qsr,Hsr,X,VT,Dhe,Usr,Jsr,CG,Ysr,Ksr,Zsr,zT,jhe,elr,olr,MG,rlr,tlr,alr,WT,Nhe,nlr,slr,EG,llr,ilr,dlr,QT,qhe,clr,flr,yG,mlr,glr,hlr,HT,Ghe,plr,_lr,wG,ulr,blr,vlr,UT,Ohe,Tlr,Flr,AG,Clr,Mlr,Elr,JT,Xhe,ylr,wlr,LG,Alr,Llr,Blr,YT,Vhe,xlr,klr,BG,Rlr,Slr,Plr,KT,zhe,$lr,Ilr,xG,Dlr,jlr,Nlr,ZT,Whe,qlr,Glr,kG,Olr,Xlr,Vlr,eF,Qhe,zlr,Wlr,RG,Qlr,Hlr,Ulr,oF,Hhe,Jlr,Ylr,SG,Klr,Zlr,eir,rF,Uhe,oir,rir,PG,tir,air,nir,tF,Jhe,sir,lir,$G,iir,dir,cir,aF,Yhe,fir,mir,IG,gir,hir,pir,nF,Khe,_ir,uir,DG,bir,vir,Tir,sF,Zhe,Fir,Cir,jG,Mir,Eir,yir,lF,epe,wir,Air,NG,Lir,Bir,xir,iF,ope,kir,Rir,qG,Sir,Pir,$ir,dF,rpe,Iir,Dir,GG,jir,Nir,qir,cF,tpe,Gir,Oir,OG,Xir,Vir,zir,fF,ape,Wir,Qir,XG,Hir,Uir,Jir,mF,npe,Yir,Kir,VG,Zir,edr,odr,gF,spe,rdr,tdr,zG,adr,ndr,sdr,hF,lpe,ldr,idr,WG,ddr,cdr,fdr,ipe,mdr,gdr,Kw,_xe,Cc,pF,dpe,Zw,hdr,cpe,pdr,uxe,Er,e6,_dr,Mc,udr,fpe,bdr,vdr,mpe,Tdr,Fdr,Cdr,o6,Mdr,gpe,Edr,ydr,wdr,ut,r6,Adr,hpe,Ldr,Bdr,Ec,xdr,ppe,kdr,Rdr,_pe,Sdr,Pdr,$dr,upe,Idr,Ddr,t6,jdr,To,a6,Ndr,bpe,qdr,Gdr,un,Odr,vpe,Xdr,Vdr,Tpe,zdr,Wdr,Fpe,Qdr,Hdr,Udr,te,_F,Cpe,Jdr,Ydr,QG,Kdr,Zdr,ecr,uF,Mpe,ocr,rcr,HG,tcr,acr,ncr,bF,Epe,scr,lcr,UG,icr,dcr,ccr,vF,ype,fcr,mcr,JG,gcr,hcr,pcr,TF,wpe,_cr,ucr,YG,bcr,vcr,Tcr,FF,Ape,Fcr,Ccr,KG,Mcr,Ecr,ycr,CF,Lpe,wcr,Acr,ZG,Lcr,Bcr,xcr,MF,Bpe,kcr,Rcr,eO,Scr,Pcr,$cr,EF,xpe,Icr,Dcr,oO,jcr,Ncr,qcr,yF,kpe,Gcr,Ocr,rO,Xcr,Vcr,zcr,wF,Rpe,Wcr,Qcr,tO,Hcr,Ucr,Jcr,AF,Spe,Ycr,Kcr,aO,Zcr,efr,ofr,LF,Ppe,rfr,tfr,nO,afr,nfr,sfr,BF,$pe,lfr,ifr,sO,dfr,cfr,ffr,xF,Ipe,mfr,gfr,lO,hfr,pfr,_fr,kF,Dpe,ufr,bfr,iO,vfr,Tfr,Ffr,RF,jpe,Cfr,Mfr,dO,Efr,yfr,wfr,Npe,Afr,Lfr,n6,bxe,yc,SF,qpe,s6,Bfr,Gpe,xfr,vxe,yr,l6,kfr,wc,Rfr,Ope,Sfr,Pfr,Xpe,$fr,Ifr,Dfr,i6,jfr,Vpe,Nfr,qfr,Gfr,bt,d6,Ofr,zpe,Xfr,Vfr,Ac,zfr,Wpe,Wfr,Qfr,Qpe,Hfr,Ufr,Jfr,Hpe,Yfr,Kfr,c6,Zfr,Fo,f6,emr,Upe,omr,rmr,bn,tmr,Jpe,amr,nmr,Ype,smr,lmr,Kpe,imr,dmr,cmr,Zpe,PF,e_e,fmr,mmr,cO,gmr,hmr,pmr,o_e,_mr,umr,m6,Txe,Lc,$F,r_e,g6,bmr,t_e,vmr,Fxe,wr,h6,Tmr,Bc,Fmr,a_e,Cmr,Mmr,n_e,Emr,ymr,wmr,p6,Amr,s_e,Lmr,Bmr,xmr,vt,_6,kmr,l_e,Rmr,Smr,xc,Pmr,i_e,$mr,Imr,d_e,Dmr,jmr,Nmr,c_e,qmr,Gmr,u6,Omr,Co,b6,Xmr,f_e,Vmr,zmr,vn,Wmr,m_e,Qmr,Hmr,g_e,Umr,Jmr,h_e,Ymr,Kmr,Zmr,K,IF,p_e,egr,ogr,fO,rgr,tgr,agr,DF,__e,ngr,sgr,mO,lgr,igr,dgr,jF,u_e,cgr,fgr,gO,mgr,ggr,hgr,NF,b_e,pgr,_gr,hO,ugr,bgr,vgr,qF,v_e,Tgr,Fgr,pO,Cgr,Mgr,Egr,GF,T_e,ygr,wgr,_O,Agr,Lgr,Bgr,OF,F_e,xgr,kgr,uO,Rgr,Sgr,Pgr,XF,C_e,$gr,Igr,bO,Dgr,jgr,Ngr,VF,M_e,qgr,Ggr,vO,Ogr,Xgr,Vgr,zF,E_e,zgr,Wgr,TO,Qgr,Hgr,Ugr,WF,y_e,Jgr,Ygr,FO,Kgr,Zgr,ehr,QF,w_e,ohr,rhr,CO,thr,ahr,nhr,HF,A_e,shr,lhr,MO,ihr,dhr,chr,UF,L_e,fhr,mhr,EO,ghr,hhr,phr,JF,B_e,_hr,uhr,yO,bhr,vhr,Thr,YF,x_e,Fhr,Chr,wO,Mhr,Ehr,yhr,KF,k_e,whr,Ahr,AO,Lhr,Bhr,xhr,ZF,R_e,khr,Rhr,LO,Shr,Phr,$hr,e9,S_e,Ihr,Dhr,BO,jhr,Nhr,qhr,o9,P_e,Ghr,Ohr,xO,Xhr,Vhr,zhr,$_e,Whr,Qhr,v6,Cxe,kc,r9,I_e,T6,Hhr,D_e,Uhr,Mxe,Ar,F6,Jhr,Rc,Yhr,j_e,Khr,Zhr,N_e,epr,opr,rpr,C6,tpr,q_e,apr,npr,spr,Tt,M6,lpr,G_e,ipr,dpr,Sc,cpr,O_e,fpr,mpr,X_e,gpr,hpr,ppr,V_e,_pr,upr,E6,bpr,Mo,y6,vpr,z_e,Tpr,Fpr,Tn,Cpr,W_e,Mpr,Epr,Q_e,ypr,wpr,H_e,Apr,Lpr,Bpr,Z,t9,U_e,xpr,kpr,kO,Rpr,Spr,Ppr,a9,J_e,$pr,Ipr,RO,Dpr,jpr,Npr,n9,Y_e,qpr,Gpr,SO,Opr,Xpr,Vpr,s9,K_e,zpr,Wpr,PO,Qpr,Hpr,Upr,l9,Z_e,Jpr,Ypr,$O,Kpr,Zpr,e_r,i9,eue,o_r,r_r,IO,t_r,a_r,n_r,d9,oue,s_r,l_r,DO,i_r,d_r,c_r,c9,rue,f_r,m_r,jO,g_r,h_r,p_r,f9,tue,__r,u_r,NO,b_r,v_r,T_r,m9,aue,F_r,C_r,qO,M_r,E_r,y_r,g9,nue,w_r,A_r,GO,L_r,B_r,x_r,h9,sue,k_r,R_r,OO,S_r,P_r,$_r,p9,lue,I_r,D_r,XO,j_r,N_r,q_r,_9,iue,G_r,O_r,VO,X_r,V_r,z_r,u9,due,W_r,Q_r,zO,H_r,U_r,J_r,b9,cue,Y_r,K_r,WO,Z_r,eur,our,v9,fue,rur,tur,QO,aur,nur,sur,T9,mue,lur,iur,HO,dur,cur,fur,F9,gue,mur,gur,UO,hur,pur,_ur,hue,uur,bur,w6,Exe,Pc,C9,pue,A6,vur,_ue,Tur,yxe,Lr,L6,Fur,$c,Cur,uue,Mur,Eur,bue,yur,wur,Aur,B6,Lur,vue,Bur,xur,kur,Ft,x6,Rur,Tue,Sur,Pur,Ic,$ur,Fue,Iur,Dur,Cue,jur,Nur,qur,Mue,Gur,Our,k6,Xur,Eo,R6,Vur,Eue,zur,Wur,Fn,Qur,yue,Hur,Uur,wue,Jur,Yur,Aue,Kur,Zur,e0r,Lue,M9,Bue,o0r,r0r,JO,t0r,a0r,n0r,xue,s0r,l0r,S6,wxe,Dc,E9,kue,P6,i0r,Rue,d0r,Axe,Br,$6,c0r,jc,f0r,Sue,m0r,g0r,Pue,h0r,p0r,_0r,I6,u0r,$ue,b0r,v0r,T0r,Ct,D6,F0r,Iue,C0r,M0r,Nc,E0r,Due,y0r,w0r,jue,A0r,L0r,B0r,Nue,x0r,k0r,j6,R0r,yo,N6,S0r,que,P0r,$0r,Cn,I0r,Gue,D0r,j0r,Oue,N0r,q0r,Xue,G0r,O0r,X0r,Vue,y9,zue,V0r,z0r,YO,W0r,Q0r,H0r,Wue,U0r,J0r,q6,Lxe,qc,w9,Que,G6,Y0r,Hue,K0r,Bxe,xr,O6,Z0r,Gc,e1r,Uue,o1r,r1r,Jue,t1r,a1r,n1r,X6,s1r,Yue,l1r,i1r,d1r,Mt,V6,c1r,Kue,f1r,m1r,Oc,g1r,Zue,h1r,p1r,e0e,_1r,u1r,b1r,o0e,v1r,T1r,z6,F1r,wo,W6,C1r,r0e,M1r,E1r,Mn,y1r,t0e,w1r,A1r,a0e,L1r,B1r,n0e,x1r,k1r,R1r,V,A9,s0e,S1r,P1r,KO,$1r,I1r,D1r,L9,l0e,j1r,N1r,ZO,q1r,G1r,O1r,B9,i0e,X1r,V1r,eX,z1r,W1r,Q1r,x9,d0e,H1r,U1r,oX,J1r,Y1r,K1r,k9,c0e,Z1r,ebr,rX,obr,rbr,tbr,R9,f0e,abr,nbr,tX,sbr,lbr,ibr,S9,m0e,dbr,cbr,aX,fbr,mbr,gbr,P9,g0e,hbr,pbr,nX,_br,ubr,bbr,$9,h0e,vbr,Tbr,sX,Fbr,Cbr,Mbr,I9,p0e,Ebr,ybr,lX,wbr,Abr,Lbr,D9,_0e,Bbr,xbr,iX,kbr,Rbr,Sbr,j9,u0e,Pbr,$br,dX,Ibr,Dbr,jbr,N9,b0e,Nbr,qbr,cX,Gbr,Obr,Xbr,q9,v0e,Vbr,zbr,fX,Wbr,Qbr,Hbr,G9,T0e,Ubr,Jbr,mX,Ybr,Kbr,Zbr,O9,F0e,e5r,o5r,gX,r5r,t5r,a5r,X9,C0e,n5r,s5r,hX,l5r,i5r,d5r,V9,M0e,c5r,f5r,pX,m5r,g5r,h5r,z9,E0e,p5r,_5r,_X,u5r,b5r,v5r,W9,y0e,T5r,F5r,uX,C5r,M5r,E5r,Q9,w0e,y5r,w5r,bX,A5r,L5r,B5r,H9,A0e,x5r,k5r,vX,R5r,S5r,P5r,U9,L0e,$5r,I5r,TX,D5r,j5r,N5r,J9,B0e,q5r,G5r,FX,O5r,X5r,V5r,Y9,x0e,z5r,W5r,k0e,Q5r,H5r,U5r,R0e,J5r,Y5r,Q6,xxe,Xc,K9,S0e,H6,K5r,P0e,Z5r,kxe,kr,U6,e2r,Vc,o2r,$0e,r2r,t2r,I0e,a2r,n2r,s2r,J6,l2r,D0e,i2r,d2r,c2r,Et,Y6,f2r,j0e,m2r,g2r,zc,h2r,N0e,p2r,_2r,q0e,u2r,b2r,v2r,G0e,T2r,F2r,K6,C2r,Ao,Z6,M2r,O0e,E2r,y2r,En,w2r,X0e,A2r,L2r,V0e,B2r,x2r,z0e,k2r,R2r,S2r,yn,Z9,W0e,P2r,$2r,CX,I2r,D2r,j2r,eC,Q0e,N2r,q2r,MX,G2r,O2r,X2r,oC,H0e,V2r,z2r,EX,W2r,Q2r,H2r,rC,U0e,U2r,J2r,yX,Y2r,K2r,Z2r,J0e,evr,ovr,eA,Rxe,Wc,tC,Y0e,oA,rvr,K0e,tvr,Sxe,Rr,rA,avr,Qc,nvr,Z0e,svr,lvr,e1e,ivr,dvr,cvr,tA,fvr,o1e,mvr,gvr,hvr,yt,aA,pvr,r1e,_vr,uvr,Hc,bvr,t1e,vvr,Tvr,a1e,Fvr,Cvr,Mvr,n1e,Evr,yvr,nA,wvr,Lo,sA,Avr,s1e,Lvr,Bvr,wn,xvr,l1e,kvr,Rvr,i1e,Svr,Pvr,d1e,$vr,Ivr,Dvr,ce,aC,c1e,jvr,Nvr,wX,qvr,Gvr,Ovr,nC,f1e,Xvr,Vvr,AX,zvr,Wvr,Qvr,sC,m1e,Hvr,Uvr,LX,Jvr,Yvr,Kvr,lC,g1e,Zvr,eTr,BX,oTr,rTr,tTr,iC,h1e,aTr,nTr,xX,sTr,lTr,iTr,dC,p1e,dTr,cTr,kX,fTr,mTr,gTr,cC,_1e,hTr,pTr,RX,_Tr,uTr,bTr,fC,u1e,vTr,TTr,SX,FTr,CTr,MTr,mC,b1e,ETr,yTr,PX,wTr,ATr,LTr,gC,v1e,BTr,xTr,$X,kTr,RTr,STr,hC,T1e,PTr,$Tr,IX,ITr,DTr,jTr,pC,F1e,NTr,qTr,C1e,GTr,OTr,XTr,M1e,VTr,zTr,lA,Pxe,Uc,_C,E1e,iA,WTr,y1e,QTr,$xe,Sr,dA,HTr,Jc,UTr,w1e,JTr,YTr,A1e,KTr,ZTr,eFr,cA,oFr,L1e,rFr,tFr,aFr,wt,fA,nFr,B1e,sFr,lFr,Yc,iFr,x1e,dFr,cFr,k1e,fFr,mFr,gFr,R1e,hFr,pFr,mA,_Fr,Bo,gA,uFr,S1e,bFr,vFr,An,TFr,P1e,FFr,CFr,$1e,MFr,EFr,I1e,yFr,wFr,AFr,ue,uC,D1e,LFr,BFr,DX,xFr,kFr,RFr,bC,j1e,SFr,PFr,jX,$Fr,IFr,DFr,vC,N1e,jFr,NFr,NX,qFr,GFr,OFr,TC,q1e,XFr,VFr,qX,zFr,WFr,QFr,FC,G1e,HFr,UFr,GX,JFr,YFr,KFr,CC,O1e,ZFr,e9r,OX,o9r,r9r,t9r,MC,X1e,a9r,n9r,XX,s9r,l9r,i9r,EC,V1e,d9r,c9r,VX,f9r,m9r,g9r,yC,z1e,h9r,p9r,zX,_9r,u9r,b9r,wC,W1e,v9r,T9r,Q1e,F9r,C9r,M9r,H1e,E9r,y9r,hA,Ixe,Kc,AC,U1e,pA,w9r,J1e,A9r,Dxe,Pr,_A,L9r,Zc,B9r,Y1e,x9r,k9r,K1e,R9r,S9r,P9r,uA,$9r,Z1e,I9r,D9r,j9r,At,bA,N9r,ebe,q9r,G9r,ef,O9r,obe,X9r,V9r,rbe,z9r,W9r,Q9r,tbe,H9r,U9r,vA,J9r,xo,TA,Y9r,abe,K9r,Z9r,Ln,eCr,nbe,oCr,rCr,sbe,tCr,aCr,lbe,nCr,sCr,lCr,Ce,LC,ibe,iCr,dCr,WX,cCr,fCr,mCr,BC,dbe,gCr,hCr,QX,pCr,_Cr,uCr,xC,cbe,bCr,vCr,HX,TCr,FCr,CCr,kC,fbe,MCr,ECr,UX,yCr,wCr,ACr,RC,mbe,LCr,BCr,JX,xCr,kCr,RCr,SC,gbe,SCr,PCr,YX,$Cr,ICr,DCr,PC,hbe,jCr,NCr,KX,qCr,GCr,OCr,$C,pbe,XCr,VCr,ZX,zCr,WCr,QCr,IC,_be,HCr,UCr,eV,JCr,YCr,KCr,ube,ZCr,eMr,FA,jxe,of,DC,bbe,CA,oMr,vbe,rMr,Nxe,$r,MA,tMr,rf,aMr,Tbe,nMr,sMr,Fbe,lMr,iMr,dMr,EA,cMr,Cbe,fMr,mMr,gMr,Lt,yA,hMr,Mbe,pMr,_Mr,tf,uMr,Ebe,bMr,vMr,ybe,TMr,FMr,CMr,wbe,MMr,EMr,wA,yMr,ko,AA,wMr,Abe,AMr,LMr,Bn,BMr,Lbe,xMr,kMr,Bbe,RMr,SMr,xbe,PMr,$Mr,IMr,be,jC,kbe,DMr,jMr,oV,NMr,qMr,GMr,NC,Rbe,OMr,XMr,rV,VMr,zMr,WMr,qC,Sbe,QMr,HMr,tV,UMr,JMr,YMr,GC,Pbe,KMr,ZMr,aV,e4r,o4r,r4r,OC,$be,t4r,a4r,nV,n4r,s4r,l4r,XC,Ibe,i4r,d4r,sV,c4r,f4r,m4r,VC,Dbe,g4r,h4r,lV,p4r,_4r,u4r,zC,jbe,b4r,v4r,iV,T4r,F4r,C4r,WC,Nbe,M4r,E4r,dV,y4r,w4r,A4r,QC,qbe,L4r,B4r,Gbe,x4r,k4r,R4r,Obe,S4r,P4r,LA,qxe,af,HC,Xbe,BA,$4r,Vbe,I4r,Gxe,Ir,xA,D4r,nf,j4r,zbe,N4r,q4r,Wbe,G4r,O4r,X4r,kA,V4r,Qbe,z4r,W4r,Q4r,Bt,RA,H4r,Hbe,U4r,J4r,sf,Y4r,Ube,K4r,Z4r,Jbe,eEr,oEr,rEr,Ybe,tEr,aEr,SA,nEr,Ro,PA,sEr,Kbe,lEr,iEr,xn,dEr,Zbe,cEr,fEr,e5e,mEr,gEr,o5e,hEr,pEr,_Er,ve,UC,r5e,uEr,bEr,cV,vEr,TEr,FEr,JC,t5e,CEr,MEr,fV,EEr,yEr,wEr,YC,a5e,AEr,LEr,mV,BEr,xEr,kEr,KC,n5e,REr,SEr,gV,PEr,$Er,IEr,ZC,s5e,DEr,jEr,hV,NEr,qEr,GEr,eM,l5e,OEr,XEr,pV,VEr,zEr,WEr,oM,i5e,QEr,HEr,_V,UEr,JEr,YEr,rM,d5e,KEr,ZEr,uV,e3r,o3r,r3r,tM,c5e,t3r,a3r,bV,n3r,s3r,l3r,aM,f5e,i3r,d3r,m5e,c3r,f3r,m3r,g5e,g3r,h3r,$A,Oxe,lf,nM,h5e,IA,p3r,p5e,_3r,Xxe,Dr,DA,u3r,df,b3r,_5e,v3r,T3r,u5e,F3r,C3r,M3r,jA,E3r,b5e,y3r,w3r,A3r,xt,NA,L3r,v5e,B3r,x3r,cf,k3r,T5e,R3r,S3r,F5e,P3r,$3r,I3r,C5e,D3r,j3r,qA,N3r,So,GA,q3r,M5e,G3r,O3r,kn,X3r,E5e,V3r,z3r,y5e,W3r,Q3r,w5e,H3r,U3r,J3r,Re,sM,A5e,Y3r,K3r,vV,Z3r,eyr,oyr,lM,L5e,ryr,tyr,TV,ayr,nyr,syr,iM,B5e,lyr,iyr,FV,dyr,cyr,fyr,dM,x5e,myr,gyr,CV,hyr,pyr,_yr,cM,k5e,uyr,byr,MV,vyr,Tyr,Fyr,fM,R5e,Cyr,Myr,EV,Eyr,yyr,wyr,mM,S5e,Ayr,Lyr,yV,Byr,xyr,kyr,gM,P5e,Ryr,Syr,$5e,Pyr,$yr,Iyr,I5e,Dyr,jyr,OA,Vxe,ff,hM,D5e,XA,Nyr,j5e,qyr,zxe,jr,VA,Gyr,mf,Oyr,N5e,Xyr,Vyr,q5e,zyr,Wyr,Qyr,zA,Hyr,G5e,Uyr,Jyr,Yyr,kt,WA,Kyr,O5e,Zyr,ewr,gf,owr,X5e,rwr,twr,V5e,awr,nwr,swr,z5e,lwr,iwr,QA,dwr,Po,HA,cwr,W5e,fwr,mwr,Rn,gwr,Q5e,hwr,pwr,H5e,_wr,uwr,U5e,bwr,vwr,Twr,Se,pM,J5e,Fwr,Cwr,wV,Mwr,Ewr,ywr,_M,Y5e,wwr,Awr,AV,Lwr,Bwr,xwr,uM,K5e,kwr,Rwr,LV,Swr,Pwr,$wr,bM,Z5e,Iwr,Dwr,BV,jwr,Nwr,qwr,vM,e2e,Gwr,Owr,xV,Xwr,Vwr,zwr,TM,o2e,Wwr,Qwr,kV,Hwr,Uwr,Jwr,FM,r2e,Ywr,Kwr,RV,Zwr,e6r,o6r,CM,t2e,r6r,t6r,a2e,a6r,n6r,s6r,n2e,l6r,i6r,UA,Wxe,hf,MM,s2e,JA,d6r,l2e,c6r,Qxe,Nr,YA,f6r,pf,m6r,i2e,g6r,h6r,d2e,p6r,_6r,u6r,KA,b6r,c2e,v6r,T6r,F6r,Rt,ZA,C6r,f2e,M6r,E6r,_f,y6r,m2e,w6r,A6r,g2e,L6r,B6r,x6r,h2e,k6r,R6r,eL,S6r,$o,oL,P6r,p2e,$6r,I6r,Sn,D6r,_2e,j6r,N6r,u2e,q6r,G6r,b2e,O6r,X6r,V6r,v2e,EM,T2e,z6r,W6r,SV,Q6r,H6r,U6r,F2e,J6r,Y6r,rL,Hxe,uf,yM,C2e,tL,K6r,M2e,Z6r,Uxe,qr,aL,eAr,bf,oAr,E2e,rAr,tAr,y2e,aAr,nAr,sAr,nL,lAr,w2e,iAr,dAr,cAr,St,sL,fAr,A2e,mAr,gAr,vf,hAr,L2e,pAr,_Ar,B2e,uAr,bAr,vAr,x2e,TAr,FAr,lL,CAr,Io,iL,MAr,k2e,EAr,yAr,Pn,wAr,R2e,AAr,LAr,S2e,BAr,xAr,P2e,kAr,RAr,SAr,dL,wM,$2e,PAr,$Ar,PV,IAr,DAr,jAr,AM,I2e,NAr,qAr,$V,GAr,OAr,XAr,D2e,VAr,zAr,cL,Jxe,Tf,LM,j2e,fL,WAr,N2e,QAr,Yxe,Gr,mL,HAr,Ff,UAr,q2e,JAr,YAr,G2e,KAr,ZAr,eLr,gL,oLr,O2e,rLr,tLr,aLr,Pt,hL,nLr,X2e,sLr,lLr,Cf,iLr,V2e,dLr,cLr,z2e,fLr,mLr,gLr,W2e,hLr,pLr,pL,_Lr,Do,_L,uLr,Q2e,bLr,vLr,$n,TLr,H2e,FLr,CLr,U2e,MLr,ELr,J2e,yLr,wLr,ALr,Y2e,BM,K2e,LLr,BLr,IV,xLr,kLr,RLr,Z2e,SLr,PLr,uL,Kxe;return fe=new z({}),$a=new w({props:{codee:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),S4=new z({}),P4=new w({props:{codee:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bf=new $Lr({props:{warning:"&lcub;true}",$$slots:{default:[D5t]},$$scope:{ctx:Ai}}}),$4=new z({}),I4=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/configuration_auto.py#L526"}}),N4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/configuration_auto.py#L549",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),q4=new w({props:{codee:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),G4=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/configuration_auto.py#L671",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),O4=new z({}),X4=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/tokenization_auto.py#L351"}}),W4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15900/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),Q4=new w({props:{codee:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),H4=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),U4=new z({}),J4=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),Z4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15900/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),fh=new $Lr({props:{$$slots:{default:[j5t]},$$scope:{ctx:Ai}}}),eE=new w({props:{codee:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),oE=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),rE=new z({}),tE=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/processing_auto.py#L71"}}),sE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Ch=new $Lr({props:{$$slots:{default:[N5t]},$$scope:{ctx:Ai}}}),lE=new w({props:{codee:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),iE=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),dE=new z({}),cE=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L687"}}),mE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),gE=new w({props:{codee:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),hE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pE=new w({props:{codee:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_E=new z({}),uE=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L694"}}),vE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),TE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),FE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ME=new z({}),EE=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L709"}}),wE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),AE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),LE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),BE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xE=new z({}),kE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L716"}}),SE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),PE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),$E=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),IE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DE=new z({}),jE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L723"}}),qE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),GE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),OE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VE=new z({}),zE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L732"}}),QE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),HE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),UE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JE=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YE=new z({}),KE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L766"}}),e3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),o3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),r3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),t3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),a3=new z({}),n3=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L773"}}),l3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),i3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),d3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),c3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),f3=new z({}),m3=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L759"}}),h3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),p3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),_3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),u3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),b3=new z({}),v3=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L741"}}),F3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),C3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),M3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),E3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),y3=new z({}),w3=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L748"}}),L3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),B3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),x3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),k3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),R3=new z({}),S3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L782"}}),$3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),I3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),D3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),j3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),N3=new z({}),q3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L812"}}),O3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),X3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),V3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),z3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),W3=new z({}),Q3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L819"}}),U3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),J3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),Y3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),K3=new w({props:{codee:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Z3=new z({}),ey=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L842"}}),ry=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),ty=new w({props:{codee:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),ay=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ny=new w({props:{codee:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sy=new z({}),ly=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L826"}}),dy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),cy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),fy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),my=new w({props:{codee:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gy=new z({}),hy=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L833"}}),_y=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),uy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),by=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ty=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Fy=new z({}),Cy=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L851"}}),Ey=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),yy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),wy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ay=new w({props:{codee:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ly=new z({}),By=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L858"}}),ky=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Ry=new w({props:{codee:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),Sy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Py=new w({props:{codee:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$y=new z({}),Iy=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L805"}}),jy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Ny=new w({props:{codee:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),qy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Gy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Oy=new z({}),Xy=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L789"}}),zy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Wy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),Qy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Hy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Uy=new z({}),Jy=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L796"}}),Ky=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),Zy=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),ew=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rw=new w({props:{codee:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tw=new z({}),aw=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L373"}}),sw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),lw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),iw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cw=new z({}),fw=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L380"}}),gw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),hw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),pw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_w=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uw=new z({}),bw=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L395"}}),Tw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Fw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Cw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Mw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ew=new z({}),yw=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L402"}}),Aw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Lw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),Bw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),kw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Rw=new z({}),Sw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L416"}}),$w=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),Iw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),Dw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Nw=new z({}),qw=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L423"}}),Ow=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),Xw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),Vw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ww=new z({}),Qw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),Uw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Jw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Yw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Kw=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Zw=new z({}),e6=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L468"}}),r6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),t6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),a6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),n6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),s6=new z({}),l6=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L448"}}),d6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),c6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),f6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),g6=new z({}),h6=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L459"}}),_6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),u6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),b6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),v6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),T6=new z({}),F6=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),M6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),E6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),y6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),w6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),A6=new z({}),L6=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L409"}}),x6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),k6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),R6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),S6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),P6=new z({}),$6=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L484"}}),D6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),j6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),N6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),q6=new w({props:{codee:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),G6=new z({}),O6=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L236"}}),V6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaModel</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),z6=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),W6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Q6=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),H6=new z({}),U6=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L250"}}),Y6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),K6=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),Z6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),eA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),oA=new z({}),rA=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L243"}}),aA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaForMaskedLM</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),nA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),sA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),lA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),iA=new z({}),dA=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),fA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaForMaskedLM</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),mA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),gA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),hA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),pA=new z({}),_A=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L264"}}),bA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),vA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),TA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),FA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),CA=new z({}),MA=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),yA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaForSequenceClassification</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),wA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),AA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),LA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),BA=new z({}),xA=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),RA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaForQuestionAnswering</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),SA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),PA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$A=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),IA=new z({}),DA=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),NA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaForTokenClassification</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),qA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),GA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),OA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),XA=new z({}),VA=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),WA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <code>FlaxXLMRobertaForMultipleChoice</code>(XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),QA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),HA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),UA=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),JA=new z({}),YA=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L305"}}),ZA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),eL=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),oL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rL=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),tL=new z({}),aL=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L314"}}),sL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),lL=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),iL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cL=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fL=new z({}),mL=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L323"}}),hL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),pL=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),_L=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uL=new w({props:{codee:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Pe=l(),ie=a("h1"),ge=a("a"),lo=a("span"),f(fe.$$.fragment),Te=l(),Xo=a("span"),Li=o("Auto Classes"),Ef=l(),sa=a("p"),Bi=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),xi=a("code"),B4=o("from_pretrained()"),yf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Le=l(),io=a("p"),ki=o("Instantiating one of "),In=a("a"),x4=o("AutoConfig"),Dn=o(", "),jn=a("a"),k4=o("AutoModel"),Ri=o(`, and
`),Nn=a("a"),R4=o("AutoTokenizer"),Si=o(" will directly create a class of the relevant architecture. For instance"),wf=l(),f($a.$$.fragment),co=l(),he=a("p"),h8=o("will create a model that is an instance of "),Pi=a("a"),p8=o("BertModel"),_8=o("."),Vo=l(),Ia=a("p"),u8=o("There is one class of "),Af=a("code"),b8=o("AutoModel"),dSe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),Z7e=l(),$i=a("h2"),Lf=a("a"),AW=a("span"),f(S4.$$.fragment),cSe=l(),LW=a("span"),fSe=o("Extending the Auto Classes"),eBe=l(),qn=a("p"),mSe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),BW=a("code"),gSe=o("NewModel"),hSe=o(", make sure you have a "),xW=a("code"),pSe=o("NewModelConfig"),_Se=o(` then you can add those to the auto
classes like this:`),oBe=l(),f(P4.$$.fragment),rBe=l(),v8=a("p"),uSe=o("You will then be able to use the auto classes like you would usually do!"),tBe=l(),f(Bf.$$.fragment),aBe=l(),Ii=a("h2"),xf=a("a"),kW=a("span"),f($4.$$.fragment),bSe=l(),RW=a("span"),vSe=o("AutoConfig"),nBe=l(),zo=a("div"),f(I4.$$.fragment),TSe=l(),D4=a("p"),FSe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),T8=a("a"),CSe=o("from_pretrained()"),MSe=o(" class method."),ESe=l(),j4=a("p"),ySe=o("This class cannot be instantiated directly using "),SW=a("code"),wSe=o("__init__()"),ASe=o(" (throws an error)."),LSe=l(),fo=a("div"),f(N4.$$.fragment),BSe=l(),PW=a("p"),xSe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),kSe=l(),Di=a("p"),RSe=o("The configuration class to instantiate is selected based on the "),$W=a("code"),SSe=o("model_type"),PSe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),IW=a("code"),$Se=o("pretrained_model_name_or_path"),ISe=o(":"),DSe=l(),v=a("ul"),kf=a("li"),DW=a("strong"),jSe=o("albert"),NSe=o(" \u2014 "),F8=a("a"),qSe=o("AlbertConfig"),GSe=o(" (ALBERT model)"),OSe=l(),Rf=a("li"),jW=a("strong"),XSe=o("bart"),VSe=o(" \u2014 "),C8=a("a"),zSe=o("BartConfig"),WSe=o(" (BART model)"),QSe=l(),Sf=a("li"),NW=a("strong"),HSe=o("beit"),USe=o(" \u2014 "),M8=a("a"),JSe=o("BeitConfig"),YSe=o(" (BEiT model)"),KSe=l(),Pf=a("li"),qW=a("strong"),ZSe=o("bert"),ePe=o(" \u2014 "),E8=a("a"),oPe=o("BertConfig"),rPe=o(" (BERT model)"),tPe=l(),$f=a("li"),GW=a("strong"),aPe=o("bert-generation"),nPe=o(" \u2014 "),y8=a("a"),sPe=o("BertGenerationConfig"),lPe=o(" (Bert Generation model)"),iPe=l(),If=a("li"),OW=a("strong"),dPe=o("big_bird"),cPe=o(" \u2014 "),w8=a("a"),fPe=o("BigBirdConfig"),mPe=o(" (BigBird model)"),gPe=l(),Df=a("li"),XW=a("strong"),hPe=o("bigbird_pegasus"),pPe=o(" \u2014 "),A8=a("a"),_Pe=o("BigBirdPegasusConfig"),uPe=o(" (BigBirdPegasus model)"),bPe=l(),jf=a("li"),VW=a("strong"),vPe=o("blenderbot"),TPe=o(" \u2014 "),L8=a("a"),FPe=o("BlenderbotConfig"),CPe=o(" (Blenderbot model)"),MPe=l(),Nf=a("li"),zW=a("strong"),EPe=o("blenderbot-small"),yPe=o(" \u2014 "),B8=a("a"),wPe=o("BlenderbotSmallConfig"),APe=o(" (BlenderbotSmall model)"),LPe=l(),qf=a("li"),WW=a("strong"),BPe=o("camembert"),xPe=o(" \u2014 "),x8=a("a"),kPe=o("CamembertConfig"),RPe=o(" (CamemBERT model)"),SPe=l(),Gf=a("li"),QW=a("strong"),PPe=o("canine"),$Pe=o(" \u2014 "),k8=a("a"),IPe=o("CanineConfig"),DPe=o(" (Canine model)"),jPe=l(),Of=a("li"),HW=a("strong"),NPe=o("clip"),qPe=o(" \u2014 "),R8=a("a"),GPe=o("CLIPConfig"),OPe=o(" (CLIP model)"),XPe=l(),Xf=a("li"),UW=a("strong"),VPe=o("convbert"),zPe=o(" \u2014 "),S8=a("a"),WPe=o("ConvBertConfig"),QPe=o(" (ConvBERT model)"),HPe=l(),Vf=a("li"),JW=a("strong"),UPe=o("convnext"),JPe=o(" \u2014 "),P8=a("a"),YPe=o("ConvNextConfig"),KPe=o(" (ConvNext model)"),ZPe=l(),zf=a("li"),YW=a("strong"),e$e=o("ctrl"),o$e=o(" \u2014 "),$8=a("a"),r$e=o("CTRLConfig"),t$e=o(" (CTRL model)"),a$e=l(),Wf=a("li"),KW=a("strong"),n$e=o("data2vec-audio"),s$e=o(" \u2014 "),I8=a("a"),l$e=o("Data2VecAudioConfig"),i$e=o(" (Data2VecAudio model)"),d$e=l(),Qf=a("li"),ZW=a("strong"),c$e=o("data2vec-text"),f$e=o(" \u2014 "),D8=a("a"),m$e=o("Data2VecTextConfig"),g$e=o(" (Data2VecText model)"),h$e=l(),Hf=a("li"),eQ=a("strong"),p$e=o("deberta"),_$e=o(" \u2014 "),j8=a("a"),u$e=o("DebertaConfig"),b$e=o(" (DeBERTa model)"),v$e=l(),Uf=a("li"),oQ=a("strong"),T$e=o("deberta-v2"),F$e=o(" \u2014 "),N8=a("a"),C$e=o("DebertaV2Config"),M$e=o(" (DeBERTa-v2 model)"),E$e=l(),Jf=a("li"),rQ=a("strong"),y$e=o("deit"),w$e=o(" \u2014 "),q8=a("a"),A$e=o("DeiTConfig"),L$e=o(" (DeiT model)"),B$e=l(),Yf=a("li"),tQ=a("strong"),x$e=o("detr"),k$e=o(" \u2014 "),G8=a("a"),R$e=o("DetrConfig"),S$e=o(" (DETR model)"),P$e=l(),Kf=a("li"),aQ=a("strong"),$$e=o("distilbert"),I$e=o(" \u2014 "),O8=a("a"),D$e=o("DistilBertConfig"),j$e=o(" (DistilBERT model)"),N$e=l(),Zf=a("li"),nQ=a("strong"),q$e=o("dpr"),G$e=o(" \u2014 "),X8=a("a"),O$e=o("DPRConfig"),X$e=o(" (DPR model)"),V$e=l(),em=a("li"),sQ=a("strong"),z$e=o("electra"),W$e=o(" \u2014 "),V8=a("a"),Q$e=o("ElectraConfig"),H$e=o(" (ELECTRA model)"),U$e=l(),om=a("li"),lQ=a("strong"),J$e=o("encoder-decoder"),Y$e=o(" \u2014 "),z8=a("a"),K$e=o("EncoderDecoderConfig"),Z$e=o(" (Encoder decoder model)"),eIe=l(),rm=a("li"),iQ=a("strong"),oIe=o("flaubert"),rIe=o(" \u2014 "),W8=a("a"),tIe=o("FlaubertConfig"),aIe=o(" (FlauBERT model)"),nIe=l(),tm=a("li"),dQ=a("strong"),sIe=o("fnet"),lIe=o(" \u2014 "),Q8=a("a"),iIe=o("FNetConfig"),dIe=o(" (FNet model)"),cIe=l(),am=a("li"),cQ=a("strong"),fIe=o("fsmt"),mIe=o(" \u2014 "),H8=a("a"),gIe=o("FSMTConfig"),hIe=o(" (FairSeq Machine-Translation model)"),pIe=l(),nm=a("li"),fQ=a("strong"),_Ie=o("funnel"),uIe=o(" \u2014 "),U8=a("a"),bIe=o("FunnelConfig"),vIe=o(" (Funnel Transformer model)"),TIe=l(),sm=a("li"),mQ=a("strong"),FIe=o("gpt2"),CIe=o(" \u2014 "),J8=a("a"),MIe=o("GPT2Config"),EIe=o(" (OpenAI GPT-2 model)"),yIe=l(),lm=a("li"),gQ=a("strong"),wIe=o("gpt_neo"),AIe=o(" \u2014 "),Y8=a("a"),LIe=o("GPTNeoConfig"),BIe=o(" (GPT Neo model)"),xIe=l(),im=a("li"),hQ=a("strong"),kIe=o("gptj"),RIe=o(" \u2014 "),K8=a("a"),SIe=o("GPTJConfig"),PIe=o(" (GPT-J model)"),$Ie=l(),dm=a("li"),pQ=a("strong"),IIe=o("hubert"),DIe=o(" \u2014 "),Z8=a("a"),jIe=o("HubertConfig"),NIe=o(" (Hubert model)"),qIe=l(),cm=a("li"),_Q=a("strong"),GIe=o("ibert"),OIe=o(" \u2014 "),e7=a("a"),XIe=o("IBertConfig"),VIe=o(" (I-BERT model)"),zIe=l(),fm=a("li"),uQ=a("strong"),WIe=o("imagegpt"),QIe=o(" \u2014 "),o7=a("a"),HIe=o("ImageGPTConfig"),UIe=o(" (ImageGPT model)"),JIe=l(),mm=a("li"),bQ=a("strong"),YIe=o("layoutlm"),KIe=o(" \u2014 "),r7=a("a"),ZIe=o("LayoutLMConfig"),eDe=o(" (LayoutLM model)"),oDe=l(),gm=a("li"),vQ=a("strong"),rDe=o("layoutlmv2"),tDe=o(" \u2014 "),t7=a("a"),aDe=o("LayoutLMv2Config"),nDe=o(" (LayoutLMv2 model)"),sDe=l(),hm=a("li"),TQ=a("strong"),lDe=o("led"),iDe=o(" \u2014 "),a7=a("a"),dDe=o("LEDConfig"),cDe=o(" (LED model)"),fDe=l(),pm=a("li"),FQ=a("strong"),mDe=o("longformer"),gDe=o(" \u2014 "),n7=a("a"),hDe=o("LongformerConfig"),pDe=o(" (Longformer model)"),_De=l(),_m=a("li"),CQ=a("strong"),uDe=o("luke"),bDe=o(" \u2014 "),s7=a("a"),vDe=o("LukeConfig"),TDe=o(" (LUKE model)"),FDe=l(),um=a("li"),MQ=a("strong"),CDe=o("lxmert"),MDe=o(" \u2014 "),l7=a("a"),EDe=o("LxmertConfig"),yDe=o(" (LXMERT model)"),wDe=l(),bm=a("li"),EQ=a("strong"),ADe=o("m2m_100"),LDe=o(" \u2014 "),i7=a("a"),BDe=o("M2M100Config"),xDe=o(" (M2M100 model)"),kDe=l(),vm=a("li"),yQ=a("strong"),RDe=o("marian"),SDe=o(" \u2014 "),d7=a("a"),PDe=o("MarianConfig"),$De=o(" (Marian model)"),IDe=l(),Tm=a("li"),wQ=a("strong"),DDe=o("maskformer"),jDe=o(" \u2014 "),c7=a("a"),NDe=o("MaskFormerConfig"),qDe=o(" (MaskFormer model)"),GDe=l(),Fm=a("li"),AQ=a("strong"),ODe=o("mbart"),XDe=o(" \u2014 "),f7=a("a"),VDe=o("MBartConfig"),zDe=o(" (mBART model)"),WDe=l(),Cm=a("li"),LQ=a("strong"),QDe=o("megatron-bert"),HDe=o(" \u2014 "),m7=a("a"),UDe=o("MegatronBertConfig"),JDe=o(" (MegatronBert model)"),YDe=l(),Mm=a("li"),BQ=a("strong"),KDe=o("mobilebert"),ZDe=o(" \u2014 "),g7=a("a"),eje=o("MobileBertConfig"),oje=o(" (MobileBERT model)"),rje=l(),Em=a("li"),xQ=a("strong"),tje=o("mpnet"),aje=o(" \u2014 "),h7=a("a"),nje=o("MPNetConfig"),sje=o(" (MPNet model)"),lje=l(),ym=a("li"),kQ=a("strong"),ije=o("mt5"),dje=o(" \u2014 "),p7=a("a"),cje=o("MT5Config"),fje=o(" (mT5 model)"),mje=l(),wm=a("li"),RQ=a("strong"),gje=o("nystromformer"),hje=o(" \u2014 "),_7=a("a"),pje=o("NystromformerConfig"),_je=o(" (Nystromformer model)"),uje=l(),Am=a("li"),SQ=a("strong"),bje=o("openai-gpt"),vje=o(" \u2014 "),u7=a("a"),Tje=o("OpenAIGPTConfig"),Fje=o(" (OpenAI GPT model)"),Cje=l(),Lm=a("li"),PQ=a("strong"),Mje=o("pegasus"),Eje=o(" \u2014 "),b7=a("a"),yje=o("PegasusConfig"),wje=o(" (Pegasus model)"),Aje=l(),Bm=a("li"),$Q=a("strong"),Lje=o("perceiver"),Bje=o(" \u2014 "),v7=a("a"),xje=o("PerceiverConfig"),kje=o(" (Perceiver model)"),Rje=l(),xm=a("li"),IQ=a("strong"),Sje=o("plbart"),Pje=o(" \u2014 "),T7=a("a"),$je=o("PLBartConfig"),Ije=o(" (PLBart model)"),Dje=l(),km=a("li"),DQ=a("strong"),jje=o("poolformer"),Nje=o(" \u2014 "),F7=a("a"),qje=o("PoolFormerConfig"),Gje=o(" (PoolFormer model)"),Oje=l(),Rm=a("li"),jQ=a("strong"),Xje=o("prophetnet"),Vje=o(" \u2014 "),C7=a("a"),zje=o("ProphetNetConfig"),Wje=o(" (ProphetNet model)"),Qje=l(),Sm=a("li"),NQ=a("strong"),Hje=o("qdqbert"),Uje=o(" \u2014 "),M7=a("a"),Jje=o("QDQBertConfig"),Yje=o(" (QDQBert model)"),Kje=l(),Pm=a("li"),qQ=a("strong"),Zje=o("rag"),eNe=o(" \u2014 "),E7=a("a"),oNe=o("RagConfig"),rNe=o(" (RAG model)"),tNe=l(),$m=a("li"),GQ=a("strong"),aNe=o("realm"),nNe=o(" \u2014 "),y7=a("a"),sNe=o("RealmConfig"),lNe=o(" (Realm model)"),iNe=l(),Im=a("li"),OQ=a("strong"),dNe=o("reformer"),cNe=o(" \u2014 "),w7=a("a"),fNe=o("ReformerConfig"),mNe=o(" (Reformer model)"),gNe=l(),Dm=a("li"),XQ=a("strong"),hNe=o("rembert"),pNe=o(" \u2014 "),A7=a("a"),_Ne=o("RemBertConfig"),uNe=o(" (RemBERT model)"),bNe=l(),jm=a("li"),VQ=a("strong"),vNe=o("retribert"),TNe=o(" \u2014 "),L7=a("a"),FNe=o("RetriBertConfig"),CNe=o(" (RetriBERT model)"),MNe=l(),Nm=a("li"),zQ=a("strong"),ENe=o("roberta"),yNe=o(" \u2014 "),B7=a("a"),wNe=o("RobertaConfig"),ANe=o(" (RoBERTa model)"),LNe=l(),qm=a("li"),WQ=a("strong"),BNe=o("roformer"),xNe=o(" \u2014 "),x7=a("a"),kNe=o("RoFormerConfig"),RNe=o(" (RoFormer model)"),SNe=l(),Gm=a("li"),QQ=a("strong"),PNe=o("segformer"),$Ne=o(" \u2014 "),k7=a("a"),INe=o("SegformerConfig"),DNe=o(" (SegFormer model)"),jNe=l(),Om=a("li"),HQ=a("strong"),NNe=o("sew"),qNe=o(" \u2014 "),R7=a("a"),GNe=o("SEWConfig"),ONe=o(" (SEW model)"),XNe=l(),Xm=a("li"),UQ=a("strong"),VNe=o("sew-d"),zNe=o(" \u2014 "),S7=a("a"),WNe=o("SEWDConfig"),QNe=o(" (SEW-D model)"),HNe=l(),Vm=a("li"),JQ=a("strong"),UNe=o("speech-encoder-decoder"),JNe=o(" \u2014 "),P7=a("a"),YNe=o("SpeechEncoderDecoderConfig"),KNe=o(" (Speech Encoder decoder model)"),ZNe=l(),zm=a("li"),YQ=a("strong"),eqe=o("speech_to_text"),oqe=o(" \u2014 "),$7=a("a"),rqe=o("Speech2TextConfig"),tqe=o(" (Speech2Text model)"),aqe=l(),Wm=a("li"),KQ=a("strong"),nqe=o("speech_to_text_2"),sqe=o(" \u2014 "),I7=a("a"),lqe=o("Speech2Text2Config"),iqe=o(" (Speech2Text2 model)"),dqe=l(),Qm=a("li"),ZQ=a("strong"),cqe=o("splinter"),fqe=o(" \u2014 "),D7=a("a"),mqe=o("SplinterConfig"),gqe=o(" (Splinter model)"),hqe=l(),Hm=a("li"),eH=a("strong"),pqe=o("squeezebert"),_qe=o(" \u2014 "),j7=a("a"),uqe=o("SqueezeBertConfig"),bqe=o(" (SqueezeBERT model)"),vqe=l(),Um=a("li"),oH=a("strong"),Tqe=o("swin"),Fqe=o(" \u2014 "),N7=a("a"),Cqe=o("SwinConfig"),Mqe=o(" (Swin model)"),Eqe=l(),Jm=a("li"),rH=a("strong"),yqe=o("t5"),wqe=o(" \u2014 "),q7=a("a"),Aqe=o("T5Config"),Lqe=o(" (T5 model)"),Bqe=l(),Ym=a("li"),tH=a("strong"),xqe=o("tapas"),kqe=o(" \u2014 "),G7=a("a"),Rqe=o("TapasConfig"),Sqe=o(" (TAPAS model)"),Pqe=l(),Km=a("li"),aH=a("strong"),$qe=o("transfo-xl"),Iqe=o(" \u2014 "),O7=a("a"),Dqe=o("TransfoXLConfig"),jqe=o(" (Transformer-XL model)"),Nqe=l(),Zm=a("li"),nH=a("strong"),qqe=o("trocr"),Gqe=o(" \u2014 "),X7=a("a"),Oqe=o("TrOCRConfig"),Xqe=o(" (TrOCR model)"),Vqe=l(),eg=a("li"),sH=a("strong"),zqe=o("unispeech"),Wqe=o(" \u2014 "),V7=a("a"),Qqe=o("UniSpeechConfig"),Hqe=o(" (UniSpeech model)"),Uqe=l(),og=a("li"),lH=a("strong"),Jqe=o("unispeech-sat"),Yqe=o(" \u2014 "),z7=a("a"),Kqe=o("UniSpeechSatConfig"),Zqe=o(" (UniSpeechSat model)"),eGe=l(),rg=a("li"),iH=a("strong"),oGe=o("vilt"),rGe=o(" \u2014 "),W7=a("a"),tGe=o("ViltConfig"),aGe=o(" (ViLT model)"),nGe=l(),tg=a("li"),dH=a("strong"),sGe=o("vision-encoder-decoder"),lGe=o(" \u2014 "),Q7=a("a"),iGe=o("VisionEncoderDecoderConfig"),dGe=o(" (Vision Encoder decoder model)"),cGe=l(),ag=a("li"),cH=a("strong"),fGe=o("vision-text-dual-encoder"),mGe=o(" \u2014 "),H7=a("a"),gGe=o("VisionTextDualEncoderConfig"),hGe=o(" (VisionTextDualEncoder model)"),pGe=l(),ng=a("li"),fH=a("strong"),_Ge=o("visual_bert"),uGe=o(" \u2014 "),U7=a("a"),bGe=o("VisualBertConfig"),vGe=o(" (VisualBert model)"),TGe=l(),sg=a("li"),mH=a("strong"),FGe=o("vit"),CGe=o(" \u2014 "),J7=a("a"),MGe=o("ViTConfig"),EGe=o(" (ViT model)"),yGe=l(),lg=a("li"),gH=a("strong"),wGe=o("vit_mae"),AGe=o(" \u2014 "),Y7=a("a"),LGe=o("ViTMAEConfig"),BGe=o(" (ViTMAE model)"),xGe=l(),ig=a("li"),hH=a("strong"),kGe=o("wav2vec2"),RGe=o(" \u2014 "),K7=a("a"),SGe=o("Wav2Vec2Config"),PGe=o(" (Wav2Vec2 model)"),$Ge=l(),dg=a("li"),pH=a("strong"),IGe=o("wavlm"),DGe=o(" \u2014 "),Z7=a("a"),jGe=o("WavLMConfig"),NGe=o(" (WavLM model)"),qGe=l(),cg=a("li"),_H=a("strong"),GGe=o("xglm"),OGe=o(" \u2014 "),eB=a("a"),XGe=o("XGLMConfig"),VGe=o(" (XGLM model)"),zGe=l(),fg=a("li"),uH=a("strong"),WGe=o("xlm"),QGe=o(" \u2014 "),oB=a("a"),HGe=o("XLMConfig"),UGe=o(" (XLM model)"),JGe=l(),mg=a("li"),bH=a("strong"),YGe=o("xlm-prophetnet"),KGe=o(" \u2014 "),rB=a("a"),ZGe=o("XLMProphetNetConfig"),eOe=o(" (XLMProphetNet model)"),oOe=l(),gg=a("li"),vH=a("strong"),rOe=o("xlm-roberta"),tOe=o(" \u2014 "),tB=a("a"),aOe=o("XLMRobertaConfig"),nOe=o(" (XLM-RoBERTa model)"),sOe=l(),hg=a("li"),TH=a("strong"),lOe=o("xlm-roberta-xl"),iOe=o(" \u2014 "),aB=a("a"),dOe=o("XLMRobertaXLConfig"),cOe=o(" (XLM-RoBERTa-XL model)"),fOe=l(),pg=a("li"),FH=a("strong"),mOe=o("xlnet"),gOe=o(" \u2014 "),nB=a("a"),hOe=o("XLNetConfig"),pOe=o(" (XLNet model)"),_Oe=l(),_g=a("li"),CH=a("strong"),uOe=o("yoso"),bOe=o(" \u2014 "),sB=a("a"),vOe=o("YosoConfig"),TOe=o(" (YOSO model)"),FOe=l(),MH=a("p"),COe=o("Examples:"),MOe=l(),f(q4.$$.fragment),EOe=l(),ug=a("div"),f(G4.$$.fragment),yOe=l(),EH=a("p"),wOe=o("Register a new configuration for this class."),sBe=l(),ji=a("h2"),bg=a("a"),yH=a("span"),f(O4.$$.fragment),AOe=l(),wH=a("span"),LOe=o("AutoTokenizer"),lBe=l(),Wo=a("div"),f(X4.$$.fragment),BOe=l(),V4=a("p"),xOe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),lB=a("a"),kOe=o("AutoTokenizer.from_pretrained()"),ROe=o(" class method."),SOe=l(),z4=a("p"),POe=o("This class cannot be instantiated directly using "),AH=a("code"),$Oe=o("__init__()"),IOe=o(" (throws an error)."),DOe=l(),mo=a("div"),f(W4.$$.fragment),jOe=l(),LH=a("p"),NOe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),qOe=l(),Da=a("p"),GOe=o("The tokenizer class to instantiate is selected based on the "),BH=a("code"),OOe=o("model_type"),XOe=o(` property of the config object (either
passed as an argument or loaded from `),xH=a("code"),VOe=o("pretrained_model_name_or_path"),zOe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kH=a("code"),WOe=o("pretrained_model_name_or_path"),QOe=o(":"),HOe=l(),M=a("ul"),Gn=a("li"),RH=a("strong"),UOe=o("albert"),JOe=o(" \u2014 "),iB=a("a"),YOe=o("AlbertTokenizer"),KOe=o(" or "),dB=a("a"),ZOe=o("AlbertTokenizerFast"),eXe=o(" (ALBERT model)"),oXe=l(),On=a("li"),SH=a("strong"),rXe=o("bart"),tXe=o(" \u2014 "),cB=a("a"),aXe=o("BartTokenizer"),nXe=o(" or "),fB=a("a"),sXe=o("BartTokenizerFast"),lXe=o(" (BART model)"),iXe=l(),Xn=a("li"),PH=a("strong"),dXe=o("barthez"),cXe=o(" \u2014 "),mB=a("a"),fXe=o("BarthezTokenizer"),mXe=o(" or "),gB=a("a"),gXe=o("BarthezTokenizerFast"),hXe=o(" (BARThez model)"),pXe=l(),vg=a("li"),$H=a("strong"),_Xe=o("bartpho"),uXe=o(" \u2014 "),hB=a("a"),bXe=o("BartphoTokenizer"),vXe=o(" (BARTpho model)"),TXe=l(),Vn=a("li"),IH=a("strong"),FXe=o("bert"),CXe=o(" \u2014 "),pB=a("a"),MXe=o("BertTokenizer"),EXe=o(" or "),_B=a("a"),yXe=o("BertTokenizerFast"),wXe=o(" (BERT model)"),AXe=l(),Tg=a("li"),DH=a("strong"),LXe=o("bert-generation"),BXe=o(" \u2014 "),uB=a("a"),xXe=o("BertGenerationTokenizer"),kXe=o(" (Bert Generation model)"),RXe=l(),Fg=a("li"),jH=a("strong"),SXe=o("bert-japanese"),PXe=o(" \u2014 "),bB=a("a"),$Xe=o("BertJapaneseTokenizer"),IXe=o(" (BertJapanese model)"),DXe=l(),Cg=a("li"),NH=a("strong"),jXe=o("bertweet"),NXe=o(" \u2014 "),vB=a("a"),qXe=o("BertweetTokenizer"),GXe=o(" (Bertweet model)"),OXe=l(),zn=a("li"),qH=a("strong"),XXe=o("big_bird"),VXe=o(" \u2014 "),TB=a("a"),zXe=o("BigBirdTokenizer"),WXe=o(" or "),FB=a("a"),QXe=o("BigBirdTokenizerFast"),HXe=o(" (BigBird model)"),UXe=l(),Wn=a("li"),GH=a("strong"),JXe=o("bigbird_pegasus"),YXe=o(" \u2014 "),CB=a("a"),KXe=o("PegasusTokenizer"),ZXe=o(" or "),MB=a("a"),eVe=o("PegasusTokenizerFast"),oVe=o(" (BigBirdPegasus model)"),rVe=l(),Qn=a("li"),OH=a("strong"),tVe=o("blenderbot"),aVe=o(" \u2014 "),EB=a("a"),nVe=o("BlenderbotTokenizer"),sVe=o(" or "),yB=a("a"),lVe=o("BlenderbotTokenizerFast"),iVe=o(" (Blenderbot model)"),dVe=l(),Mg=a("li"),XH=a("strong"),cVe=o("blenderbot-small"),fVe=o(" \u2014 "),wB=a("a"),mVe=o("BlenderbotSmallTokenizer"),gVe=o(" (BlenderbotSmall model)"),hVe=l(),Eg=a("li"),VH=a("strong"),pVe=o("byt5"),_Ve=o(" \u2014 "),AB=a("a"),uVe=o("ByT5Tokenizer"),bVe=o(" (ByT5 model)"),vVe=l(),Hn=a("li"),zH=a("strong"),TVe=o("camembert"),FVe=o(" \u2014 "),LB=a("a"),CVe=o("CamembertTokenizer"),MVe=o(" or "),BB=a("a"),EVe=o("CamembertTokenizerFast"),yVe=o(" (CamemBERT model)"),wVe=l(),yg=a("li"),WH=a("strong"),AVe=o("canine"),LVe=o(" \u2014 "),xB=a("a"),BVe=o("CanineTokenizer"),xVe=o(" (Canine model)"),kVe=l(),Un=a("li"),QH=a("strong"),RVe=o("clip"),SVe=o(" \u2014 "),kB=a("a"),PVe=o("CLIPTokenizer"),$Ve=o(" or "),RB=a("a"),IVe=o("CLIPTokenizerFast"),DVe=o(" (CLIP model)"),jVe=l(),Jn=a("li"),HH=a("strong"),NVe=o("convbert"),qVe=o(" \u2014 "),SB=a("a"),GVe=o("ConvBertTokenizer"),OVe=o(" or "),PB=a("a"),XVe=o("ConvBertTokenizerFast"),VVe=o(" (ConvBERT model)"),zVe=l(),Yn=a("li"),UH=a("strong"),WVe=o("cpm"),QVe=o(" \u2014 "),$B=a("a"),HVe=o("CpmTokenizer"),UVe=o(" or "),JH=a("code"),JVe=o("CpmTokenizerFast"),YVe=o(" (CPM model)"),KVe=l(),wg=a("li"),YH=a("strong"),ZVe=o("ctrl"),eze=o(" \u2014 "),IB=a("a"),oze=o("CTRLTokenizer"),rze=o(" (CTRL model)"),tze=l(),Kn=a("li"),KH=a("strong"),aze=o("deberta"),nze=o(" \u2014 "),DB=a("a"),sze=o("DebertaTokenizer"),lze=o(" or "),jB=a("a"),ize=o("DebertaTokenizerFast"),dze=o(" (DeBERTa model)"),cze=l(),Ag=a("li"),ZH=a("strong"),fze=o("deberta-v2"),mze=o(" \u2014 "),NB=a("a"),gze=o("DebertaV2Tokenizer"),hze=o(" (DeBERTa-v2 model)"),pze=l(),Zn=a("li"),eU=a("strong"),_ze=o("distilbert"),uze=o(" \u2014 "),qB=a("a"),bze=o("DistilBertTokenizer"),vze=o(" or "),GB=a("a"),Tze=o("DistilBertTokenizerFast"),Fze=o(" (DistilBERT model)"),Cze=l(),es=a("li"),oU=a("strong"),Mze=o("dpr"),Eze=o(" \u2014 "),OB=a("a"),yze=o("DPRQuestionEncoderTokenizer"),wze=o(" or "),XB=a("a"),Aze=o("DPRQuestionEncoderTokenizerFast"),Lze=o(" (DPR model)"),Bze=l(),os=a("li"),rU=a("strong"),xze=o("electra"),kze=o(" \u2014 "),VB=a("a"),Rze=o("ElectraTokenizer"),Sze=o(" or "),zB=a("a"),Pze=o("ElectraTokenizerFast"),$ze=o(" (ELECTRA model)"),Ize=l(),Lg=a("li"),tU=a("strong"),Dze=o("flaubert"),jze=o(" \u2014 "),WB=a("a"),Nze=o("FlaubertTokenizer"),qze=o(" (FlauBERT model)"),Gze=l(),rs=a("li"),aU=a("strong"),Oze=o("fnet"),Xze=o(" \u2014 "),QB=a("a"),Vze=o("FNetTokenizer"),zze=o(" or "),HB=a("a"),Wze=o("FNetTokenizerFast"),Qze=o(" (FNet model)"),Hze=l(),Bg=a("li"),nU=a("strong"),Uze=o("fsmt"),Jze=o(" \u2014 "),UB=a("a"),Yze=o("FSMTTokenizer"),Kze=o(" (FairSeq Machine-Translation model)"),Zze=l(),ts=a("li"),sU=a("strong"),eWe=o("funnel"),oWe=o(" \u2014 "),JB=a("a"),rWe=o("FunnelTokenizer"),tWe=o(" or "),YB=a("a"),aWe=o("FunnelTokenizerFast"),nWe=o(" (Funnel Transformer model)"),sWe=l(),as=a("li"),lU=a("strong"),lWe=o("gpt2"),iWe=o(" \u2014 "),KB=a("a"),dWe=o("GPT2Tokenizer"),cWe=o(" or "),ZB=a("a"),fWe=o("GPT2TokenizerFast"),mWe=o(" (OpenAI GPT-2 model)"),gWe=l(),ns=a("li"),iU=a("strong"),hWe=o("gpt_neo"),pWe=o(" \u2014 "),ex=a("a"),_We=o("GPT2Tokenizer"),uWe=o(" or "),ox=a("a"),bWe=o("GPT2TokenizerFast"),vWe=o(" (GPT Neo model)"),TWe=l(),ss=a("li"),dU=a("strong"),FWe=o("herbert"),CWe=o(" \u2014 "),rx=a("a"),MWe=o("HerbertTokenizer"),EWe=o(" or "),tx=a("a"),yWe=o("HerbertTokenizerFast"),wWe=o(" (HerBERT model)"),AWe=l(),xg=a("li"),cU=a("strong"),LWe=o("hubert"),BWe=o(" \u2014 "),ax=a("a"),xWe=o("Wav2Vec2CTCTokenizer"),kWe=o(" (Hubert model)"),RWe=l(),ls=a("li"),fU=a("strong"),SWe=o("ibert"),PWe=o(" \u2014 "),nx=a("a"),$We=o("RobertaTokenizer"),IWe=o(" or "),sx=a("a"),DWe=o("RobertaTokenizerFast"),jWe=o(" (I-BERT model)"),NWe=l(),is=a("li"),mU=a("strong"),qWe=o("layoutlm"),GWe=o(" \u2014 "),lx=a("a"),OWe=o("LayoutLMTokenizer"),XWe=o(" or "),ix=a("a"),VWe=o("LayoutLMTokenizerFast"),zWe=o(" (LayoutLM model)"),WWe=l(),ds=a("li"),gU=a("strong"),QWe=o("layoutlmv2"),HWe=o(" \u2014 "),dx=a("a"),UWe=o("LayoutLMv2Tokenizer"),JWe=o(" or "),cx=a("a"),YWe=o("LayoutLMv2TokenizerFast"),KWe=o(" (LayoutLMv2 model)"),ZWe=l(),cs=a("li"),hU=a("strong"),eQe=o("layoutxlm"),oQe=o(" \u2014 "),fx=a("a"),rQe=o("LayoutXLMTokenizer"),tQe=o(" or "),mx=a("a"),aQe=o("LayoutXLMTokenizerFast"),nQe=o(" (LayoutXLM model)"),sQe=l(),fs=a("li"),pU=a("strong"),lQe=o("led"),iQe=o(" \u2014 "),gx=a("a"),dQe=o("LEDTokenizer"),cQe=o(" or "),hx=a("a"),fQe=o("LEDTokenizerFast"),mQe=o(" (LED model)"),gQe=l(),ms=a("li"),_U=a("strong"),hQe=o("longformer"),pQe=o(" \u2014 "),px=a("a"),_Qe=o("LongformerTokenizer"),uQe=o(" or "),_x=a("a"),bQe=o("LongformerTokenizerFast"),vQe=o(" (Longformer model)"),TQe=l(),kg=a("li"),uU=a("strong"),FQe=o("luke"),CQe=o(" \u2014 "),ux=a("a"),MQe=o("LukeTokenizer"),EQe=o(" (LUKE model)"),yQe=l(),gs=a("li"),bU=a("strong"),wQe=o("lxmert"),AQe=o(" \u2014 "),bx=a("a"),LQe=o("LxmertTokenizer"),BQe=o(" or "),vx=a("a"),xQe=o("LxmertTokenizerFast"),kQe=o(" (LXMERT model)"),RQe=l(),Rg=a("li"),vU=a("strong"),SQe=o("m2m_100"),PQe=o(" \u2014 "),Tx=a("a"),$Qe=o("M2M100Tokenizer"),IQe=o(" (M2M100 model)"),DQe=l(),Sg=a("li"),TU=a("strong"),jQe=o("marian"),NQe=o(" \u2014 "),Fx=a("a"),qQe=o("MarianTokenizer"),GQe=o(" (Marian model)"),OQe=l(),hs=a("li"),FU=a("strong"),XQe=o("mbart"),VQe=o(" \u2014 "),Cx=a("a"),zQe=o("MBartTokenizer"),WQe=o(" or "),Mx=a("a"),QQe=o("MBartTokenizerFast"),HQe=o(" (mBART model)"),UQe=l(),ps=a("li"),CU=a("strong"),JQe=o("mbart50"),YQe=o(" \u2014 "),Ex=a("a"),KQe=o("MBart50Tokenizer"),ZQe=o(" or "),yx=a("a"),eHe=o("MBart50TokenizerFast"),oHe=o(" (mBART-50 model)"),rHe=l(),Pg=a("li"),MU=a("strong"),tHe=o("mluke"),aHe=o(" \u2014 "),wx=a("a"),nHe=o("MLukeTokenizer"),sHe=o(" (mLUKE model)"),lHe=l(),_s=a("li"),EU=a("strong"),iHe=o("mobilebert"),dHe=o(" \u2014 "),Ax=a("a"),cHe=o("MobileBertTokenizer"),fHe=o(" or "),Lx=a("a"),mHe=o("MobileBertTokenizerFast"),gHe=o(" (MobileBERT model)"),hHe=l(),us=a("li"),yU=a("strong"),pHe=o("mpnet"),_He=o(" \u2014 "),Bx=a("a"),uHe=o("MPNetTokenizer"),bHe=o(" or "),xx=a("a"),vHe=o("MPNetTokenizerFast"),THe=o(" (MPNet model)"),FHe=l(),bs=a("li"),wU=a("strong"),CHe=o("mt5"),MHe=o(" \u2014 "),kx=a("a"),EHe=o("MT5Tokenizer"),yHe=o(" or "),Rx=a("a"),wHe=o("MT5TokenizerFast"),AHe=o(" (mT5 model)"),LHe=l(),vs=a("li"),AU=a("strong"),BHe=o("openai-gpt"),xHe=o(" \u2014 "),Sx=a("a"),kHe=o("OpenAIGPTTokenizer"),RHe=o(" or "),Px=a("a"),SHe=o("OpenAIGPTTokenizerFast"),PHe=o(" (OpenAI GPT model)"),$He=l(),Ts=a("li"),LU=a("strong"),IHe=o("pegasus"),DHe=o(" \u2014 "),$x=a("a"),jHe=o("PegasusTokenizer"),NHe=o(" or "),Ix=a("a"),qHe=o("PegasusTokenizerFast"),GHe=o(" (Pegasus model)"),OHe=l(),$g=a("li"),BU=a("strong"),XHe=o("perceiver"),VHe=o(" \u2014 "),Dx=a("a"),zHe=o("PerceiverTokenizer"),WHe=o(" (Perceiver model)"),QHe=l(),Ig=a("li"),xU=a("strong"),HHe=o("phobert"),UHe=o(" \u2014 "),jx=a("a"),JHe=o("PhobertTokenizer"),YHe=o(" (PhoBERT model)"),KHe=l(),Dg=a("li"),kU=a("strong"),ZHe=o("plbart"),eUe=o(" \u2014 "),Nx=a("a"),oUe=o("PLBartTokenizer"),rUe=o(" (PLBart model)"),tUe=l(),jg=a("li"),RU=a("strong"),aUe=o("prophetnet"),nUe=o(" \u2014 "),qx=a("a"),sUe=o("ProphetNetTokenizer"),lUe=o(" (ProphetNet model)"),iUe=l(),Fs=a("li"),SU=a("strong"),dUe=o("qdqbert"),cUe=o(" \u2014 "),Gx=a("a"),fUe=o("BertTokenizer"),mUe=o(" or "),Ox=a("a"),gUe=o("BertTokenizerFast"),hUe=o(" (QDQBert model)"),pUe=l(),Ng=a("li"),PU=a("strong"),_Ue=o("rag"),uUe=o(" \u2014 "),Xx=a("a"),bUe=o("RagTokenizer"),vUe=o(" (RAG model)"),TUe=l(),Cs=a("li"),$U=a("strong"),FUe=o("reformer"),CUe=o(" \u2014 "),Vx=a("a"),MUe=o("ReformerTokenizer"),EUe=o(" or "),zx=a("a"),yUe=o("ReformerTokenizerFast"),wUe=o(" (Reformer model)"),AUe=l(),Ms=a("li"),IU=a("strong"),LUe=o("rembert"),BUe=o(" \u2014 "),Wx=a("a"),xUe=o("RemBertTokenizer"),kUe=o(" or "),Qx=a("a"),RUe=o("RemBertTokenizerFast"),SUe=o(" (RemBERT model)"),PUe=l(),Es=a("li"),DU=a("strong"),$Ue=o("retribert"),IUe=o(" \u2014 "),Hx=a("a"),DUe=o("RetriBertTokenizer"),jUe=o(" or "),Ux=a("a"),NUe=o("RetriBertTokenizerFast"),qUe=o(" (RetriBERT model)"),GUe=l(),ys=a("li"),jU=a("strong"),OUe=o("roberta"),XUe=o(" \u2014 "),Jx=a("a"),VUe=o("RobertaTokenizer"),zUe=o(" or "),Yx=a("a"),WUe=o("RobertaTokenizerFast"),QUe=o(" (RoBERTa model)"),HUe=l(),ws=a("li"),NU=a("strong"),UUe=o("roformer"),JUe=o(" \u2014 "),Kx=a("a"),YUe=o("RoFormerTokenizer"),KUe=o(" or "),Zx=a("a"),ZUe=o("RoFormerTokenizerFast"),eJe=o(" (RoFormer model)"),oJe=l(),qg=a("li"),qU=a("strong"),rJe=o("speech_to_text"),tJe=o(" \u2014 "),ek=a("a"),aJe=o("Speech2TextTokenizer"),nJe=o(" (Speech2Text model)"),sJe=l(),Gg=a("li"),GU=a("strong"),lJe=o("speech_to_text_2"),iJe=o(" \u2014 "),ok=a("a"),dJe=o("Speech2Text2Tokenizer"),cJe=o(" (Speech2Text2 model)"),fJe=l(),As=a("li"),OU=a("strong"),mJe=o("splinter"),gJe=o(" \u2014 "),rk=a("a"),hJe=o("SplinterTokenizer"),pJe=o(" or "),tk=a("a"),_Je=o("SplinterTokenizerFast"),uJe=o(" (Splinter model)"),bJe=l(),Ls=a("li"),XU=a("strong"),vJe=o("squeezebert"),TJe=o(" \u2014 "),ak=a("a"),FJe=o("SqueezeBertTokenizer"),CJe=o(" or "),nk=a("a"),MJe=o("SqueezeBertTokenizerFast"),EJe=o(" (SqueezeBERT model)"),yJe=l(),Bs=a("li"),VU=a("strong"),wJe=o("t5"),AJe=o(" \u2014 "),sk=a("a"),LJe=o("T5Tokenizer"),BJe=o(" or "),lk=a("a"),xJe=o("T5TokenizerFast"),kJe=o(" (T5 model)"),RJe=l(),Og=a("li"),zU=a("strong"),SJe=o("tapas"),PJe=o(" \u2014 "),ik=a("a"),$Je=o("TapasTokenizer"),IJe=o(" (TAPAS model)"),DJe=l(),Xg=a("li"),WU=a("strong"),jJe=o("transfo-xl"),NJe=o(" \u2014 "),dk=a("a"),qJe=o("TransfoXLTokenizer"),GJe=o(" (Transformer-XL model)"),OJe=l(),Vg=a("li"),QU=a("strong"),XJe=o("wav2vec2"),VJe=o(" \u2014 "),ck=a("a"),zJe=o("Wav2Vec2CTCTokenizer"),WJe=o(" (Wav2Vec2 model)"),QJe=l(),zg=a("li"),HU=a("strong"),HJe=o("wav2vec2_phoneme"),UJe=o(" \u2014 "),fk=a("a"),JJe=o("Wav2Vec2PhonemeCTCTokenizer"),YJe=o(" (Wav2Vec2Phoneme model)"),KJe=l(),xs=a("li"),UU=a("strong"),ZJe=o("xglm"),eYe=o(" \u2014 "),mk=a("a"),oYe=o("XGLMTokenizer"),rYe=o(" or "),gk=a("a"),tYe=o("XGLMTokenizerFast"),aYe=o(" (XGLM model)"),nYe=l(),Wg=a("li"),JU=a("strong"),sYe=o("xlm"),lYe=o(" \u2014 "),hk=a("a"),iYe=o("XLMTokenizer"),dYe=o(" (XLM model)"),cYe=l(),Qg=a("li"),YU=a("strong"),fYe=o("xlm-prophetnet"),mYe=o(" \u2014 "),pk=a("a"),gYe=o("XLMProphetNetTokenizer"),hYe=o(" (XLMProphetNet model)"),pYe=l(),ks=a("li"),KU=a("strong"),_Ye=o("xlm-roberta"),uYe=o(" \u2014 "),_k=a("a"),bYe=o("XLMRobertaTokenizer"),vYe=o(" or "),uk=a("a"),TYe=o("XLMRobertaTokenizerFast"),FYe=o(" (XLM-RoBERTa model)"),CYe=l(),Rs=a("li"),ZU=a("strong"),MYe=o("xlnet"),EYe=o(" \u2014 "),bk=a("a"),yYe=o("XLNetTokenizer"),wYe=o(" or "),vk=a("a"),AYe=o("XLNetTokenizerFast"),LYe=o(" (XLNet model)"),BYe=l(),eJ=a("p"),xYe=o("Examples:"),kYe=l(),f(Q4.$$.fragment),RYe=l(),Hg=a("div"),f(H4.$$.fragment),SYe=l(),oJ=a("p"),PYe=o("Register a new tokenizer in this mapping."),iBe=l(),Ni=a("h2"),Ug=a("a"),rJ=a("span"),f(U4.$$.fragment),$Ye=l(),tJ=a("span"),IYe=o("AutoFeatureExtractor"),dBe=l(),Qo=a("div"),f(J4.$$.fragment),DYe=l(),Y4=a("p"),jYe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Tk=a("a"),NYe=o("AutoFeatureExtractor.from_pretrained()"),qYe=o(" class method."),GYe=l(),K4=a("p"),OYe=o("This class cannot be instantiated directly using "),aJ=a("code"),XYe=o("__init__()"),VYe=o(" (throws an error)."),zYe=l(),$e=a("div"),f(Z4.$$.fragment),WYe=l(),nJ=a("p"),QYe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),HYe=l(),ja=a("p"),UYe=o("The feature extractor class to instantiate is selected based on the "),sJ=a("code"),JYe=o("model_type"),YYe=o(` property of the config object
(either passed as an argument or loaded from `),lJ=a("code"),KYe=o("pretrained_model_name_or_path"),ZYe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),iJ=a("code"),eKe=o("pretrained_model_name_or_path"),oKe=o(":"),rKe=l(),se=a("ul"),Jg=a("li"),dJ=a("strong"),tKe=o("beit"),aKe=o(" \u2014 "),Fk=a("a"),nKe=o("BeitFeatureExtractor"),sKe=o(" (BEiT model)"),lKe=l(),Yg=a("li"),cJ=a("strong"),iKe=o("clip"),dKe=o(" \u2014 "),Ck=a("a"),cKe=o("CLIPFeatureExtractor"),fKe=o(" (CLIP model)"),mKe=l(),Kg=a("li"),fJ=a("strong"),gKe=o("convnext"),hKe=o(" \u2014 "),Mk=a("a"),pKe=o("ConvNextFeatureExtractor"),_Ke=o(" (ConvNext model)"),uKe=l(),Zg=a("li"),mJ=a("strong"),bKe=o("deit"),vKe=o(" \u2014 "),Ek=a("a"),TKe=o("DeiTFeatureExtractor"),FKe=o(" (DeiT model)"),CKe=l(),eh=a("li"),gJ=a("strong"),MKe=o("detr"),EKe=o(" \u2014 "),yk=a("a"),yKe=o("DetrFeatureExtractor"),wKe=o(" (DETR model)"),AKe=l(),oh=a("li"),hJ=a("strong"),LKe=o("hubert"),BKe=o(" \u2014 "),wk=a("a"),xKe=o("Wav2Vec2FeatureExtractor"),kKe=o(" (Hubert model)"),RKe=l(),rh=a("li"),pJ=a("strong"),SKe=o("layoutlmv2"),PKe=o(" \u2014 "),Ak=a("a"),$Ke=o("LayoutLMv2FeatureExtractor"),IKe=o(" (LayoutLMv2 model)"),DKe=l(),th=a("li"),_J=a("strong"),jKe=o("perceiver"),NKe=o(" \u2014 "),Lk=a("a"),qKe=o("PerceiverFeatureExtractor"),GKe=o(" (Perceiver model)"),OKe=l(),ah=a("li"),uJ=a("strong"),XKe=o("poolformer"),VKe=o(" \u2014 "),Bk=a("a"),zKe=o("PoolFormerFeatureExtractor"),WKe=o(" (PoolFormer model)"),QKe=l(),nh=a("li"),bJ=a("strong"),HKe=o("segformer"),UKe=o(" \u2014 "),xk=a("a"),JKe=o("SegformerFeatureExtractor"),YKe=o(" (SegFormer model)"),KKe=l(),sh=a("li"),vJ=a("strong"),ZKe=o("speech_to_text"),eZe=o(" \u2014 "),kk=a("a"),oZe=o("Speech2TextFeatureExtractor"),rZe=o(" (Speech2Text model)"),tZe=l(),lh=a("li"),TJ=a("strong"),aZe=o("swin"),nZe=o(" \u2014 "),Rk=a("a"),sZe=o("ViTFeatureExtractor"),lZe=o(" (Swin model)"),iZe=l(),ih=a("li"),FJ=a("strong"),dZe=o("vit"),cZe=o(" \u2014 "),Sk=a("a"),fZe=o("ViTFeatureExtractor"),mZe=o(" (ViT model)"),gZe=l(),dh=a("li"),CJ=a("strong"),hZe=o("vit_mae"),pZe=o(" \u2014 "),Pk=a("a"),_Ze=o("ViTFeatureExtractor"),uZe=o(" (ViTMAE model)"),bZe=l(),ch=a("li"),MJ=a("strong"),vZe=o("wav2vec2"),TZe=o(" \u2014 "),$k=a("a"),FZe=o("Wav2Vec2FeatureExtractor"),CZe=o(" (Wav2Vec2 model)"),MZe=l(),f(fh.$$.fragment),EZe=l(),EJ=a("p"),yZe=o("Examples:"),wZe=l(),f(eE.$$.fragment),AZe=l(),mh=a("div"),f(oE.$$.fragment),LZe=l(),yJ=a("p"),BZe=o("Register a new feature extractor for this class."),cBe=l(),qi=a("h2"),gh=a("a"),wJ=a("span"),f(rE.$$.fragment),xZe=l(),AJ=a("span"),kZe=o("AutoProcessor"),fBe=l(),Ho=a("div"),f(tE.$$.fragment),RZe=l(),aE=a("p"),SZe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),Ik=a("a"),PZe=o("AutoProcessor.from_pretrained()"),$Ze=o(" class method."),IZe=l(),nE=a("p"),DZe=o("This class cannot be instantiated directly using "),LJ=a("code"),jZe=o("__init__()"),NZe=o(" (throws an error)."),qZe=l(),Ie=a("div"),f(sE.$$.fragment),GZe=l(),BJ=a("p"),OZe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),XZe=l(),Gi=a("p"),VZe=o("The processor class to instantiate is selected based on the "),xJ=a("code"),zZe=o("model_type"),WZe=o(` property of the config object (either
passed as an argument or loaded from `),kJ=a("code"),QZe=o("pretrained_model_name_or_path"),HZe=o(" if possible):"),UZe=l(),Be=a("ul"),hh=a("li"),RJ=a("strong"),JZe=o("clip"),YZe=o(" \u2014 "),Dk=a("a"),KZe=o("CLIPProcessor"),ZZe=o(" (CLIP model)"),eeo=l(),ph=a("li"),SJ=a("strong"),oeo=o("layoutlmv2"),reo=o(" \u2014 "),jk=a("a"),teo=o("LayoutLMv2Processor"),aeo=o(" (LayoutLMv2 model)"),neo=l(),_h=a("li"),PJ=a("strong"),seo=o("layoutxlm"),leo=o(" \u2014 "),Nk=a("a"),ieo=o("LayoutXLMProcessor"),deo=o(" (LayoutXLM model)"),ceo=l(),uh=a("li"),$J=a("strong"),feo=o("speech_to_text"),meo=o(" \u2014 "),qk=a("a"),geo=o("Speech2TextProcessor"),heo=o(" (Speech2Text model)"),peo=l(),bh=a("li"),IJ=a("strong"),_eo=o("speech_to_text_2"),ueo=o(" \u2014 "),Gk=a("a"),beo=o("Speech2Text2Processor"),veo=o(" (Speech2Text2 model)"),Teo=l(),vh=a("li"),DJ=a("strong"),Feo=o("trocr"),Ceo=o(" \u2014 "),Ok=a("a"),Meo=o("TrOCRProcessor"),Eeo=o(" (TrOCR model)"),yeo=l(),Th=a("li"),jJ=a("strong"),weo=o("vision-text-dual-encoder"),Aeo=o(" \u2014 "),Xk=a("a"),Leo=o("VisionTextDualEncoderProcessor"),Beo=o(" (VisionTextDualEncoder model)"),xeo=l(),Fh=a("li"),NJ=a("strong"),keo=o("wav2vec2"),Reo=o(" \u2014 "),Vk=a("a"),Seo=o("Wav2Vec2Processor"),Peo=o(" (Wav2Vec2 model)"),$eo=l(),f(Ch.$$.fragment),Ieo=l(),qJ=a("p"),Deo=o("Examples:"),jeo=l(),f(lE.$$.fragment),Neo=l(),Mh=a("div"),f(iE.$$.fragment),qeo=l(),GJ=a("p"),Geo=o("Register a new processor for this class."),mBe=l(),Oi=a("h2"),Eh=a("a"),OJ=a("span"),f(dE.$$.fragment),Oeo=l(),XJ=a("span"),Xeo=o("AutoModel"),gBe=l(),Uo=a("div"),f(cE.$$.fragment),Veo=l(),Xi=a("p"),zeo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),VJ=a("code"),Weo=o("from_pretrained()"),Qeo=o("class method or the "),zJ=a("code"),Heo=o("from_config()"),Ueo=o(`class
method.`),Jeo=l(),fE=a("p"),Yeo=o("This class cannot be instantiated directly using "),WJ=a("code"),Keo=o("__init__()"),Zeo=o(" (throws an error)."),eoo=l(),Or=a("div"),f(mE.$$.fragment),ooo=l(),QJ=a("p"),roo=o("Instantiates one of the base model classes of the library from a configuration."),too=l(),Vi=a("p"),aoo=o(`Note:
Loading a model from its configuration file does `),HJ=a("strong"),noo=o("not"),soo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),UJ=a("code"),loo=o("from_pretrained()"),ioo=o("to load the model weights."),doo=l(),JJ=a("p"),coo=o("Examples:"),foo=l(),f(gE.$$.fragment),moo=l(),De=a("div"),f(hE.$$.fragment),goo=l(),YJ=a("p"),hoo=o("Instantiate one of the base model classes of the library from a pretrained model."),poo=l(),Na=a("p"),_oo=o("The model class to instantiate is selected based on the "),KJ=a("code"),uoo=o("model_type"),boo=o(` property of the config object (either
passed as an argument or loaded from `),ZJ=a("code"),voo=o("pretrained_model_name_or_path"),Too=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eY=a("code"),Foo=o("pretrained_model_name_or_path"),Coo=o(":"),Moo=l(),F=a("ul"),yh=a("li"),oY=a("strong"),Eoo=o("albert"),yoo=o(" \u2014 "),zk=a("a"),woo=o("AlbertModel"),Aoo=o(" (ALBERT model)"),Loo=l(),wh=a("li"),rY=a("strong"),Boo=o("bart"),xoo=o(" \u2014 "),Wk=a("a"),koo=o("BartModel"),Roo=o(" (BART model)"),Soo=l(),Ah=a("li"),tY=a("strong"),Poo=o("beit"),$oo=o(" \u2014 "),Qk=a("a"),Ioo=o("BeitModel"),Doo=o(" (BEiT model)"),joo=l(),Lh=a("li"),aY=a("strong"),Noo=o("bert"),qoo=o(" \u2014 "),Hk=a("a"),Goo=o("BertModel"),Ooo=o(" (BERT model)"),Xoo=l(),Bh=a("li"),nY=a("strong"),Voo=o("bert-generation"),zoo=o(" \u2014 "),Uk=a("a"),Woo=o("BertGenerationEncoder"),Qoo=o(" (Bert Generation model)"),Hoo=l(),xh=a("li"),sY=a("strong"),Uoo=o("big_bird"),Joo=o(" \u2014 "),Jk=a("a"),Yoo=o("BigBirdModel"),Koo=o(" (BigBird model)"),Zoo=l(),kh=a("li"),lY=a("strong"),ero=o("bigbird_pegasus"),oro=o(" \u2014 "),Yk=a("a"),rro=o("BigBirdPegasusModel"),tro=o(" (BigBirdPegasus model)"),aro=l(),Rh=a("li"),iY=a("strong"),nro=o("blenderbot"),sro=o(" \u2014 "),Kk=a("a"),lro=o("BlenderbotModel"),iro=o(" (Blenderbot model)"),dro=l(),Sh=a("li"),dY=a("strong"),cro=o("blenderbot-small"),fro=o(" \u2014 "),Zk=a("a"),mro=o("BlenderbotSmallModel"),gro=o(" (BlenderbotSmall model)"),hro=l(),Ph=a("li"),cY=a("strong"),pro=o("camembert"),_ro=o(" \u2014 "),eR=a("a"),uro=o("CamembertModel"),bro=o(" (CamemBERT model)"),vro=l(),$h=a("li"),fY=a("strong"),Tro=o("canine"),Fro=o(" \u2014 "),oR=a("a"),Cro=o("CanineModel"),Mro=o(" (Canine model)"),Ero=l(),Ih=a("li"),mY=a("strong"),yro=o("clip"),wro=o(" \u2014 "),rR=a("a"),Aro=o("CLIPModel"),Lro=o(" (CLIP model)"),Bro=l(),Dh=a("li"),gY=a("strong"),xro=o("convbert"),kro=o(" \u2014 "),tR=a("a"),Rro=o("ConvBertModel"),Sro=o(" (ConvBERT model)"),Pro=l(),jh=a("li"),hY=a("strong"),$ro=o("convnext"),Iro=o(" \u2014 "),aR=a("a"),Dro=o("ConvNextModel"),jro=o(" (ConvNext model)"),Nro=l(),Nh=a("li"),pY=a("strong"),qro=o("ctrl"),Gro=o(" \u2014 "),nR=a("a"),Oro=o("CTRLModel"),Xro=o(" (CTRL model)"),Vro=l(),qh=a("li"),_Y=a("strong"),zro=o("data2vec-audio"),Wro=o(" \u2014 "),sR=a("a"),Qro=o("Data2VecAudioModel"),Hro=o(" (Data2VecAudio model)"),Uro=l(),Gh=a("li"),uY=a("strong"),Jro=o("data2vec-text"),Yro=o(" \u2014 "),lR=a("a"),Kro=o("Data2VecTextModel"),Zro=o(" (Data2VecText model)"),eto=l(),Oh=a("li"),bY=a("strong"),oto=o("deberta"),rto=o(" \u2014 "),iR=a("a"),tto=o("DebertaModel"),ato=o(" (DeBERTa model)"),nto=l(),Xh=a("li"),vY=a("strong"),sto=o("deberta-v2"),lto=o(" \u2014 "),dR=a("a"),ito=o("DebertaV2Model"),dto=o(" (DeBERTa-v2 model)"),cto=l(),Vh=a("li"),TY=a("strong"),fto=o("deit"),mto=o(" \u2014 "),cR=a("a"),gto=o("DeiTModel"),hto=o(" (DeiT model)"),pto=l(),zh=a("li"),FY=a("strong"),_to=o("detr"),uto=o(" \u2014 "),fR=a("a"),bto=o("DetrModel"),vto=o(" (DETR model)"),Tto=l(),Wh=a("li"),CY=a("strong"),Fto=o("distilbert"),Cto=o(" \u2014 "),mR=a("a"),Mto=o("DistilBertModel"),Eto=o(" (DistilBERT model)"),yto=l(),Qh=a("li"),MY=a("strong"),wto=o("dpr"),Ato=o(" \u2014 "),gR=a("a"),Lto=o("DPRQuestionEncoder"),Bto=o(" (DPR model)"),xto=l(),Hh=a("li"),EY=a("strong"),kto=o("electra"),Rto=o(" \u2014 "),hR=a("a"),Sto=o("ElectraModel"),Pto=o(" (ELECTRA model)"),$to=l(),Uh=a("li"),yY=a("strong"),Ito=o("flaubert"),Dto=o(" \u2014 "),pR=a("a"),jto=o("FlaubertModel"),Nto=o(" (FlauBERT model)"),qto=l(),Jh=a("li"),wY=a("strong"),Gto=o("fnet"),Oto=o(" \u2014 "),_R=a("a"),Xto=o("FNetModel"),Vto=o(" (FNet model)"),zto=l(),Yh=a("li"),AY=a("strong"),Wto=o("fsmt"),Qto=o(" \u2014 "),uR=a("a"),Hto=o("FSMTModel"),Uto=o(" (FairSeq Machine-Translation model)"),Jto=l(),Ss=a("li"),LY=a("strong"),Yto=o("funnel"),Kto=o(" \u2014 "),bR=a("a"),Zto=o("FunnelModel"),eao=o(" or "),vR=a("a"),oao=o("FunnelBaseModel"),rao=o(" (Funnel Transformer model)"),tao=l(),Kh=a("li"),BY=a("strong"),aao=o("gpt2"),nao=o(" \u2014 "),TR=a("a"),sao=o("GPT2Model"),lao=o(" (OpenAI GPT-2 model)"),iao=l(),Zh=a("li"),xY=a("strong"),dao=o("gpt_neo"),cao=o(" \u2014 "),FR=a("a"),fao=o("GPTNeoModel"),mao=o(" (GPT Neo model)"),gao=l(),ep=a("li"),kY=a("strong"),hao=o("gptj"),pao=o(" \u2014 "),CR=a("a"),_ao=o("GPTJModel"),uao=o(" (GPT-J model)"),bao=l(),op=a("li"),RY=a("strong"),vao=o("hubert"),Tao=o(" \u2014 "),MR=a("a"),Fao=o("HubertModel"),Cao=o(" (Hubert model)"),Mao=l(),rp=a("li"),SY=a("strong"),Eao=o("ibert"),yao=o(" \u2014 "),ER=a("a"),wao=o("IBertModel"),Aao=o(" (I-BERT model)"),Lao=l(),tp=a("li"),PY=a("strong"),Bao=o("imagegpt"),xao=o(" \u2014 "),yR=a("a"),kao=o("ImageGPTModel"),Rao=o(" (ImageGPT model)"),Sao=l(),ap=a("li"),$Y=a("strong"),Pao=o("layoutlm"),$ao=o(" \u2014 "),wR=a("a"),Iao=o("LayoutLMModel"),Dao=o(" (LayoutLM model)"),jao=l(),np=a("li"),IY=a("strong"),Nao=o("layoutlmv2"),qao=o(" \u2014 "),AR=a("a"),Gao=o("LayoutLMv2Model"),Oao=o(" (LayoutLMv2 model)"),Xao=l(),sp=a("li"),DY=a("strong"),Vao=o("led"),zao=o(" \u2014 "),LR=a("a"),Wao=o("LEDModel"),Qao=o(" (LED model)"),Hao=l(),lp=a("li"),jY=a("strong"),Uao=o("longformer"),Jao=o(" \u2014 "),BR=a("a"),Yao=o("LongformerModel"),Kao=o(" (Longformer model)"),Zao=l(),ip=a("li"),NY=a("strong"),eno=o("luke"),ono=o(" \u2014 "),xR=a("a"),rno=o("LukeModel"),tno=o(" (LUKE model)"),ano=l(),dp=a("li"),qY=a("strong"),nno=o("lxmert"),sno=o(" \u2014 "),kR=a("a"),lno=o("LxmertModel"),ino=o(" (LXMERT model)"),dno=l(),cp=a("li"),GY=a("strong"),cno=o("m2m_100"),fno=o(" \u2014 "),RR=a("a"),mno=o("M2M100Model"),gno=o(" (M2M100 model)"),hno=l(),fp=a("li"),OY=a("strong"),pno=o("marian"),_no=o(" \u2014 "),SR=a("a"),uno=o("MarianModel"),bno=o(" (Marian model)"),vno=l(),mp=a("li"),XY=a("strong"),Tno=o("maskformer"),Fno=o(" \u2014 "),PR=a("a"),Cno=o("MaskFormerModel"),Mno=o(" (MaskFormer model)"),Eno=l(),gp=a("li"),VY=a("strong"),yno=o("mbart"),wno=o(" \u2014 "),$R=a("a"),Ano=o("MBartModel"),Lno=o(" (mBART model)"),Bno=l(),hp=a("li"),zY=a("strong"),xno=o("megatron-bert"),kno=o(" \u2014 "),IR=a("a"),Rno=o("MegatronBertModel"),Sno=o(" (MegatronBert model)"),Pno=l(),pp=a("li"),WY=a("strong"),$no=o("mobilebert"),Ino=o(" \u2014 "),DR=a("a"),Dno=o("MobileBertModel"),jno=o(" (MobileBERT model)"),Nno=l(),_p=a("li"),QY=a("strong"),qno=o("mpnet"),Gno=o(" \u2014 "),jR=a("a"),Ono=o("MPNetModel"),Xno=o(" (MPNet model)"),Vno=l(),up=a("li"),HY=a("strong"),zno=o("mt5"),Wno=o(" \u2014 "),NR=a("a"),Qno=o("MT5Model"),Hno=o(" (mT5 model)"),Uno=l(),bp=a("li"),UY=a("strong"),Jno=o("nystromformer"),Yno=o(" \u2014 "),qR=a("a"),Kno=o("NystromformerModel"),Zno=o(" (Nystromformer model)"),eso=l(),vp=a("li"),JY=a("strong"),oso=o("openai-gpt"),rso=o(" \u2014 "),GR=a("a"),tso=o("OpenAIGPTModel"),aso=o(" (OpenAI GPT model)"),nso=l(),Tp=a("li"),YY=a("strong"),sso=o("pegasus"),lso=o(" \u2014 "),OR=a("a"),iso=o("PegasusModel"),dso=o(" (Pegasus model)"),cso=l(),Fp=a("li"),KY=a("strong"),fso=o("perceiver"),mso=o(" \u2014 "),XR=a("a"),gso=o("PerceiverModel"),hso=o(" (Perceiver model)"),pso=l(),Cp=a("li"),ZY=a("strong"),_so=o("plbart"),uso=o(" \u2014 "),VR=a("a"),bso=o("PLBartModel"),vso=o(" (PLBart model)"),Tso=l(),Mp=a("li"),eK=a("strong"),Fso=o("poolformer"),Cso=o(" \u2014 "),zR=a("a"),Mso=o("PoolFormerModel"),Eso=o(" (PoolFormer model)"),yso=l(),Ep=a("li"),oK=a("strong"),wso=o("prophetnet"),Aso=o(" \u2014 "),WR=a("a"),Lso=o("ProphetNetModel"),Bso=o(" (ProphetNet model)"),xso=l(),yp=a("li"),rK=a("strong"),kso=o("qdqbert"),Rso=o(" \u2014 "),QR=a("a"),Sso=o("QDQBertModel"),Pso=o(" (QDQBert model)"),$so=l(),wp=a("li"),tK=a("strong"),Iso=o("reformer"),Dso=o(" \u2014 "),HR=a("a"),jso=o("ReformerModel"),Nso=o(" (Reformer model)"),qso=l(),Ap=a("li"),aK=a("strong"),Gso=o("rembert"),Oso=o(" \u2014 "),UR=a("a"),Xso=o("RemBertModel"),Vso=o(" (RemBERT model)"),zso=l(),Lp=a("li"),nK=a("strong"),Wso=o("retribert"),Qso=o(" \u2014 "),JR=a("a"),Hso=o("RetriBertModel"),Uso=o(" (RetriBERT model)"),Jso=l(),Bp=a("li"),sK=a("strong"),Yso=o("roberta"),Kso=o(" \u2014 "),YR=a("a"),Zso=o("RobertaModel"),elo=o(" (RoBERTa model)"),olo=l(),xp=a("li"),lK=a("strong"),rlo=o("roformer"),tlo=o(" \u2014 "),KR=a("a"),alo=o("RoFormerModel"),nlo=o(" (RoFormer model)"),slo=l(),kp=a("li"),iK=a("strong"),llo=o("segformer"),ilo=o(" \u2014 "),ZR=a("a"),dlo=o("SegformerModel"),clo=o(" (SegFormer model)"),flo=l(),Rp=a("li"),dK=a("strong"),mlo=o("sew"),glo=o(" \u2014 "),eS=a("a"),hlo=o("SEWModel"),plo=o(" (SEW model)"),_lo=l(),Sp=a("li"),cK=a("strong"),ulo=o("sew-d"),blo=o(" \u2014 "),oS=a("a"),vlo=o("SEWDModel"),Tlo=o(" (SEW-D model)"),Flo=l(),Pp=a("li"),fK=a("strong"),Clo=o("speech_to_text"),Mlo=o(" \u2014 "),rS=a("a"),Elo=o("Speech2TextModel"),ylo=o(" (Speech2Text model)"),wlo=l(),$p=a("li"),mK=a("strong"),Alo=o("splinter"),Llo=o(" \u2014 "),tS=a("a"),Blo=o("SplinterModel"),xlo=o(" (Splinter model)"),klo=l(),Ip=a("li"),gK=a("strong"),Rlo=o("squeezebert"),Slo=o(" \u2014 "),aS=a("a"),Plo=o("SqueezeBertModel"),$lo=o(" (SqueezeBERT model)"),Ilo=l(),Dp=a("li"),hK=a("strong"),Dlo=o("swin"),jlo=o(" \u2014 "),nS=a("a"),Nlo=o("SwinModel"),qlo=o(" (Swin model)"),Glo=l(),jp=a("li"),pK=a("strong"),Olo=o("t5"),Xlo=o(" \u2014 "),sS=a("a"),Vlo=o("T5Model"),zlo=o(" (T5 model)"),Wlo=l(),Np=a("li"),_K=a("strong"),Qlo=o("tapas"),Hlo=o(" \u2014 "),lS=a("a"),Ulo=o("TapasModel"),Jlo=o(" (TAPAS model)"),Ylo=l(),qp=a("li"),uK=a("strong"),Klo=o("transfo-xl"),Zlo=o(" \u2014 "),iS=a("a"),eio=o("TransfoXLModel"),oio=o(" (Transformer-XL model)"),rio=l(),Gp=a("li"),bK=a("strong"),tio=o("unispeech"),aio=o(" \u2014 "),dS=a("a"),nio=o("UniSpeechModel"),sio=o(" (UniSpeech model)"),lio=l(),Op=a("li"),vK=a("strong"),iio=o("unispeech-sat"),dio=o(" \u2014 "),cS=a("a"),cio=o("UniSpeechSatModel"),fio=o(" (UniSpeechSat model)"),mio=l(),Xp=a("li"),TK=a("strong"),gio=o("vilt"),hio=o(" \u2014 "),fS=a("a"),pio=o("ViltModel"),_io=o(" (ViLT model)"),uio=l(),Vp=a("li"),FK=a("strong"),bio=o("vision-text-dual-encoder"),vio=o(" \u2014 "),mS=a("a"),Tio=o("VisionTextDualEncoderModel"),Fio=o(" (VisionTextDualEncoder model)"),Cio=l(),zp=a("li"),CK=a("strong"),Mio=o("visual_bert"),Eio=o(" \u2014 "),gS=a("a"),yio=o("VisualBertModel"),wio=o(" (VisualBert model)"),Aio=l(),Wp=a("li"),MK=a("strong"),Lio=o("vit"),Bio=o(" \u2014 "),hS=a("a"),xio=o("ViTModel"),kio=o(" (ViT model)"),Rio=l(),Qp=a("li"),EK=a("strong"),Sio=o("vit_mae"),Pio=o(" \u2014 "),pS=a("a"),$io=o("ViTMAEModel"),Iio=o(" (ViTMAE model)"),Dio=l(),Hp=a("li"),yK=a("strong"),jio=o("wav2vec2"),Nio=o(" \u2014 "),_S=a("a"),qio=o("Wav2Vec2Model"),Gio=o(" (Wav2Vec2 model)"),Oio=l(),Up=a("li"),wK=a("strong"),Xio=o("wavlm"),Vio=o(" \u2014 "),uS=a("a"),zio=o("WavLMModel"),Wio=o(" (WavLM model)"),Qio=l(),Jp=a("li"),AK=a("strong"),Hio=o("xglm"),Uio=o(" \u2014 "),bS=a("a"),Jio=o("XGLMModel"),Yio=o(" (XGLM model)"),Kio=l(),Yp=a("li"),LK=a("strong"),Zio=o("xlm"),edo=o(" \u2014 "),vS=a("a"),odo=o("XLMModel"),rdo=o(" (XLM model)"),tdo=l(),Kp=a("li"),BK=a("strong"),ado=o("xlm-prophetnet"),ndo=o(" \u2014 "),TS=a("a"),sdo=o("XLMProphetNetModel"),ldo=o(" (XLMProphetNet model)"),ido=l(),Zp=a("li"),xK=a("strong"),ddo=o("xlm-roberta"),cdo=o(" \u2014 "),FS=a("a"),fdo=o("XLMRobertaModel"),mdo=o(" (XLM-RoBERTa model)"),gdo=l(),e_=a("li"),kK=a("strong"),hdo=o("xlm-roberta-xl"),pdo=o(" \u2014 "),CS=a("a"),_do=o("XLMRobertaXLModel"),udo=o(" (XLM-RoBERTa-XL model)"),bdo=l(),o_=a("li"),RK=a("strong"),vdo=o("xlnet"),Tdo=o(" \u2014 "),MS=a("a"),Fdo=o("XLNetModel"),Cdo=o(" (XLNet model)"),Mdo=l(),r_=a("li"),SK=a("strong"),Edo=o("yoso"),ydo=o(" \u2014 "),ES=a("a"),wdo=o("YosoModel"),Ado=o(" (YOSO model)"),Ldo=l(),t_=a("p"),Bdo=o("The model is set in evaluation mode by default using "),PK=a("code"),xdo=o("model.eval()"),kdo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$K=a("code"),Rdo=o("model.train()"),Sdo=l(),IK=a("p"),Pdo=o("Examples:"),$do=l(),f(pE.$$.fragment),hBe=l(),zi=a("h2"),a_=a("a"),DK=a("span"),f(_E.$$.fragment),Ido=l(),jK=a("span"),Ddo=o("AutoModelForPreTraining"),pBe=l(),Jo=a("div"),f(uE.$$.fragment),jdo=l(),Wi=a("p"),Ndo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),NK=a("code"),qdo=o("from_pretrained()"),Gdo=o("class method or the "),qK=a("code"),Odo=o("from_config()"),Xdo=o(`class
method.`),Vdo=l(),bE=a("p"),zdo=o("This class cannot be instantiated directly using "),GK=a("code"),Wdo=o("__init__()"),Qdo=o(" (throws an error)."),Hdo=l(),Xr=a("div"),f(vE.$$.fragment),Udo=l(),OK=a("p"),Jdo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Ydo=l(),Qi=a("p"),Kdo=o(`Note:
Loading a model from its configuration file does `),XK=a("strong"),Zdo=o("not"),eco=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),VK=a("code"),oco=o("from_pretrained()"),rco=o("to load the model weights."),tco=l(),zK=a("p"),aco=o("Examples:"),nco=l(),f(TE.$$.fragment),sco=l(),je=a("div"),f(FE.$$.fragment),lco=l(),WK=a("p"),ico=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),dco=l(),qa=a("p"),cco=o("The model class to instantiate is selected based on the "),QK=a("code"),fco=o("model_type"),mco=o(` property of the config object (either
passed as an argument or loaded from `),HK=a("code"),gco=o("pretrained_model_name_or_path"),hco=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),UK=a("code"),pco=o("pretrained_model_name_or_path"),_co=o(":"),uco=l(),k=a("ul"),n_=a("li"),JK=a("strong"),bco=o("albert"),vco=o(" \u2014 "),yS=a("a"),Tco=o("AlbertForPreTraining"),Fco=o(" (ALBERT model)"),Cco=l(),s_=a("li"),YK=a("strong"),Mco=o("bart"),Eco=o(" \u2014 "),wS=a("a"),yco=o("BartForConditionalGeneration"),wco=o(" (BART model)"),Aco=l(),l_=a("li"),KK=a("strong"),Lco=o("bert"),Bco=o(" \u2014 "),AS=a("a"),xco=o("BertForPreTraining"),kco=o(" (BERT model)"),Rco=l(),i_=a("li"),ZK=a("strong"),Sco=o("big_bird"),Pco=o(" \u2014 "),LS=a("a"),$co=o("BigBirdForPreTraining"),Ico=o(" (BigBird model)"),Dco=l(),d_=a("li"),eZ=a("strong"),jco=o("camembert"),Nco=o(" \u2014 "),BS=a("a"),qco=o("CamembertForMaskedLM"),Gco=o(" (CamemBERT model)"),Oco=l(),c_=a("li"),oZ=a("strong"),Xco=o("ctrl"),Vco=o(" \u2014 "),xS=a("a"),zco=o("CTRLLMHeadModel"),Wco=o(" (CTRL model)"),Qco=l(),f_=a("li"),rZ=a("strong"),Hco=o("data2vec-text"),Uco=o(" \u2014 "),kS=a("a"),Jco=o("Data2VecTextForMaskedLM"),Yco=o(" (Data2VecText model)"),Kco=l(),m_=a("li"),tZ=a("strong"),Zco=o("deberta"),efo=o(" \u2014 "),RS=a("a"),ofo=o("DebertaForMaskedLM"),rfo=o(" (DeBERTa model)"),tfo=l(),g_=a("li"),aZ=a("strong"),afo=o("deberta-v2"),nfo=o(" \u2014 "),SS=a("a"),sfo=o("DebertaV2ForMaskedLM"),lfo=o(" (DeBERTa-v2 model)"),ifo=l(),h_=a("li"),nZ=a("strong"),dfo=o("distilbert"),cfo=o(" \u2014 "),PS=a("a"),ffo=o("DistilBertForMaskedLM"),mfo=o(" (DistilBERT model)"),gfo=l(),p_=a("li"),sZ=a("strong"),hfo=o("electra"),pfo=o(" \u2014 "),$S=a("a"),_fo=o("ElectraForPreTraining"),ufo=o(" (ELECTRA model)"),bfo=l(),__=a("li"),lZ=a("strong"),vfo=o("flaubert"),Tfo=o(" \u2014 "),IS=a("a"),Ffo=o("FlaubertWithLMHeadModel"),Cfo=o(" (FlauBERT model)"),Mfo=l(),u_=a("li"),iZ=a("strong"),Efo=o("fnet"),yfo=o(" \u2014 "),DS=a("a"),wfo=o("FNetForPreTraining"),Afo=o(" (FNet model)"),Lfo=l(),b_=a("li"),dZ=a("strong"),Bfo=o("fsmt"),xfo=o(" \u2014 "),jS=a("a"),kfo=o("FSMTForConditionalGeneration"),Rfo=o(" (FairSeq Machine-Translation model)"),Sfo=l(),v_=a("li"),cZ=a("strong"),Pfo=o("funnel"),$fo=o(" \u2014 "),NS=a("a"),Ifo=o("FunnelForPreTraining"),Dfo=o(" (Funnel Transformer model)"),jfo=l(),T_=a("li"),fZ=a("strong"),Nfo=o("gpt2"),qfo=o(" \u2014 "),qS=a("a"),Gfo=o("GPT2LMHeadModel"),Ofo=o(" (OpenAI GPT-2 model)"),Xfo=l(),F_=a("li"),mZ=a("strong"),Vfo=o("ibert"),zfo=o(" \u2014 "),GS=a("a"),Wfo=o("IBertForMaskedLM"),Qfo=o(" (I-BERT model)"),Hfo=l(),C_=a("li"),gZ=a("strong"),Ufo=o("layoutlm"),Jfo=o(" \u2014 "),OS=a("a"),Yfo=o("LayoutLMForMaskedLM"),Kfo=o(" (LayoutLM model)"),Zfo=l(),M_=a("li"),hZ=a("strong"),emo=o("longformer"),omo=o(" \u2014 "),XS=a("a"),rmo=o("LongformerForMaskedLM"),tmo=o(" (Longformer model)"),amo=l(),E_=a("li"),pZ=a("strong"),nmo=o("lxmert"),smo=o(" \u2014 "),VS=a("a"),lmo=o("LxmertForPreTraining"),imo=o(" (LXMERT model)"),dmo=l(),y_=a("li"),_Z=a("strong"),cmo=o("megatron-bert"),fmo=o(" \u2014 "),zS=a("a"),mmo=o("MegatronBertForPreTraining"),gmo=o(" (MegatronBert model)"),hmo=l(),w_=a("li"),uZ=a("strong"),pmo=o("mobilebert"),_mo=o(" \u2014 "),WS=a("a"),umo=o("MobileBertForPreTraining"),bmo=o(" (MobileBERT model)"),vmo=l(),A_=a("li"),bZ=a("strong"),Tmo=o("mpnet"),Fmo=o(" \u2014 "),QS=a("a"),Cmo=o("MPNetForMaskedLM"),Mmo=o(" (MPNet model)"),Emo=l(),L_=a("li"),vZ=a("strong"),ymo=o("openai-gpt"),wmo=o(" \u2014 "),HS=a("a"),Amo=o("OpenAIGPTLMHeadModel"),Lmo=o(" (OpenAI GPT model)"),Bmo=l(),B_=a("li"),TZ=a("strong"),xmo=o("retribert"),kmo=o(" \u2014 "),US=a("a"),Rmo=o("RetriBertModel"),Smo=o(" (RetriBERT model)"),Pmo=l(),x_=a("li"),FZ=a("strong"),$mo=o("roberta"),Imo=o(" \u2014 "),JS=a("a"),Dmo=o("RobertaForMaskedLM"),jmo=o(" (RoBERTa model)"),Nmo=l(),k_=a("li"),CZ=a("strong"),qmo=o("squeezebert"),Gmo=o(" \u2014 "),YS=a("a"),Omo=o("SqueezeBertForMaskedLM"),Xmo=o(" (SqueezeBERT model)"),Vmo=l(),R_=a("li"),MZ=a("strong"),zmo=o("t5"),Wmo=o(" \u2014 "),KS=a("a"),Qmo=o("T5ForConditionalGeneration"),Hmo=o(" (T5 model)"),Umo=l(),S_=a("li"),EZ=a("strong"),Jmo=o("tapas"),Ymo=o(" \u2014 "),ZS=a("a"),Kmo=o("TapasForMaskedLM"),Zmo=o(" (TAPAS model)"),ego=l(),P_=a("li"),yZ=a("strong"),ogo=o("transfo-xl"),rgo=o(" \u2014 "),eP=a("a"),tgo=o("TransfoXLLMHeadModel"),ago=o(" (Transformer-XL model)"),ngo=l(),$_=a("li"),wZ=a("strong"),sgo=o("unispeech"),lgo=o(" \u2014 "),oP=a("a"),igo=o("UniSpeechForPreTraining"),dgo=o(" (UniSpeech model)"),cgo=l(),I_=a("li"),AZ=a("strong"),fgo=o("unispeech-sat"),mgo=o(" \u2014 "),rP=a("a"),ggo=o("UniSpeechSatForPreTraining"),hgo=o(" (UniSpeechSat model)"),pgo=l(),D_=a("li"),LZ=a("strong"),_go=o("visual_bert"),ugo=o(" \u2014 "),tP=a("a"),bgo=o("VisualBertForPreTraining"),vgo=o(" (VisualBert model)"),Tgo=l(),j_=a("li"),BZ=a("strong"),Fgo=o("vit_mae"),Cgo=o(" \u2014 "),aP=a("a"),Mgo=o("ViTMAEForPreTraining"),Ego=o(" (ViTMAE model)"),ygo=l(),N_=a("li"),xZ=a("strong"),wgo=o("wav2vec2"),Ago=o(" \u2014 "),nP=a("a"),Lgo=o("Wav2Vec2ForPreTraining"),Bgo=o(" (Wav2Vec2 model)"),xgo=l(),q_=a("li"),kZ=a("strong"),kgo=o("xlm"),Rgo=o(" \u2014 "),sP=a("a"),Sgo=o("XLMWithLMHeadModel"),Pgo=o(" (XLM model)"),$go=l(),G_=a("li"),RZ=a("strong"),Igo=o("xlm-roberta"),Dgo=o(" \u2014 "),lP=a("a"),jgo=o("XLMRobertaForMaskedLM"),Ngo=o(" (XLM-RoBERTa model)"),qgo=l(),O_=a("li"),SZ=a("strong"),Ggo=o("xlm-roberta-xl"),Ogo=o(" \u2014 "),iP=a("a"),Xgo=o("XLMRobertaXLForMaskedLM"),Vgo=o(" (XLM-RoBERTa-XL model)"),zgo=l(),X_=a("li"),PZ=a("strong"),Wgo=o("xlnet"),Qgo=o(" \u2014 "),dP=a("a"),Hgo=o("XLNetLMHeadModel"),Ugo=o(" (XLNet model)"),Jgo=l(),V_=a("p"),Ygo=o("The model is set in evaluation mode by default using "),$Z=a("code"),Kgo=o("model.eval()"),Zgo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),IZ=a("code"),eho=o("model.train()"),oho=l(),DZ=a("p"),rho=o("Examples:"),tho=l(),f(CE.$$.fragment),_Be=l(),Hi=a("h2"),z_=a("a"),jZ=a("span"),f(ME.$$.fragment),aho=l(),NZ=a("span"),nho=o("AutoModelForCausalLM"),uBe=l(),Yo=a("div"),f(EE.$$.fragment),sho=l(),Ui=a("p"),lho=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),qZ=a("code"),iho=o("from_pretrained()"),dho=o("class method or the "),GZ=a("code"),cho=o("from_config()"),fho=o(`class
method.`),mho=l(),yE=a("p"),gho=o("This class cannot be instantiated directly using "),OZ=a("code"),hho=o("__init__()"),pho=o(" (throws an error)."),_ho=l(),Vr=a("div"),f(wE.$$.fragment),uho=l(),XZ=a("p"),bho=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),vho=l(),Ji=a("p"),Tho=o(`Note:
Loading a model from its configuration file does `),VZ=a("strong"),Fho=o("not"),Cho=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zZ=a("code"),Mho=o("from_pretrained()"),Eho=o("to load the model weights."),yho=l(),WZ=a("p"),who=o("Examples:"),Aho=l(),f(AE.$$.fragment),Lho=l(),Ne=a("div"),f(LE.$$.fragment),Bho=l(),QZ=a("p"),xho=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),kho=l(),Ga=a("p"),Rho=o("The model class to instantiate is selected based on the "),HZ=a("code"),Sho=o("model_type"),Pho=o(` property of the config object (either
passed as an argument or loaded from `),UZ=a("code"),$ho=o("pretrained_model_name_or_path"),Iho=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),JZ=a("code"),Dho=o("pretrained_model_name_or_path"),jho=o(":"),Nho=l(),$=a("ul"),W_=a("li"),YZ=a("strong"),qho=o("bart"),Gho=o(" \u2014 "),cP=a("a"),Oho=o("BartForCausalLM"),Xho=o(" (BART model)"),Vho=l(),Q_=a("li"),KZ=a("strong"),zho=o("bert"),Who=o(" \u2014 "),fP=a("a"),Qho=o("BertLMHeadModel"),Hho=o(" (BERT model)"),Uho=l(),H_=a("li"),ZZ=a("strong"),Jho=o("bert-generation"),Yho=o(" \u2014 "),mP=a("a"),Kho=o("BertGenerationDecoder"),Zho=o(" (Bert Generation model)"),epo=l(),U_=a("li"),eee=a("strong"),opo=o("big_bird"),rpo=o(" \u2014 "),gP=a("a"),tpo=o("BigBirdForCausalLM"),apo=o(" (BigBird model)"),npo=l(),J_=a("li"),oee=a("strong"),spo=o("bigbird_pegasus"),lpo=o(" \u2014 "),hP=a("a"),ipo=o("BigBirdPegasusForCausalLM"),dpo=o(" (BigBirdPegasus model)"),cpo=l(),Y_=a("li"),ree=a("strong"),fpo=o("blenderbot"),mpo=o(" \u2014 "),pP=a("a"),gpo=o("BlenderbotForCausalLM"),hpo=o(" (Blenderbot model)"),ppo=l(),K_=a("li"),tee=a("strong"),_po=o("blenderbot-small"),upo=o(" \u2014 "),_P=a("a"),bpo=o("BlenderbotSmallForCausalLM"),vpo=o(" (BlenderbotSmall model)"),Tpo=l(),Z_=a("li"),aee=a("strong"),Fpo=o("camembert"),Cpo=o(" \u2014 "),uP=a("a"),Mpo=o("CamembertForCausalLM"),Epo=o(" (CamemBERT model)"),ypo=l(),eu=a("li"),nee=a("strong"),wpo=o("ctrl"),Apo=o(" \u2014 "),bP=a("a"),Lpo=o("CTRLLMHeadModel"),Bpo=o(" (CTRL model)"),xpo=l(),ou=a("li"),see=a("strong"),kpo=o("data2vec-text"),Rpo=o(" \u2014 "),vP=a("a"),Spo=o("Data2VecTextForCausalLM"),Ppo=o(" (Data2VecText model)"),$po=l(),ru=a("li"),lee=a("strong"),Ipo=o("electra"),Dpo=o(" \u2014 "),TP=a("a"),jpo=o("ElectraForCausalLM"),Npo=o(" (ELECTRA model)"),qpo=l(),tu=a("li"),iee=a("strong"),Gpo=o("gpt2"),Opo=o(" \u2014 "),FP=a("a"),Xpo=o("GPT2LMHeadModel"),Vpo=o(" (OpenAI GPT-2 model)"),zpo=l(),au=a("li"),dee=a("strong"),Wpo=o("gpt_neo"),Qpo=o(" \u2014 "),CP=a("a"),Hpo=o("GPTNeoForCausalLM"),Upo=o(" (GPT Neo model)"),Jpo=l(),nu=a("li"),cee=a("strong"),Ypo=o("gptj"),Kpo=o(" \u2014 "),MP=a("a"),Zpo=o("GPTJForCausalLM"),e_o=o(" (GPT-J model)"),o_o=l(),su=a("li"),fee=a("strong"),r_o=o("marian"),t_o=o(" \u2014 "),EP=a("a"),a_o=o("MarianForCausalLM"),n_o=o(" (Marian model)"),s_o=l(),lu=a("li"),mee=a("strong"),l_o=o("mbart"),i_o=o(" \u2014 "),yP=a("a"),d_o=o("MBartForCausalLM"),c_o=o(" (mBART model)"),f_o=l(),iu=a("li"),gee=a("strong"),m_o=o("megatron-bert"),g_o=o(" \u2014 "),wP=a("a"),h_o=o("MegatronBertForCausalLM"),p_o=o(" (MegatronBert model)"),__o=l(),du=a("li"),hee=a("strong"),u_o=o("openai-gpt"),b_o=o(" \u2014 "),AP=a("a"),v_o=o("OpenAIGPTLMHeadModel"),T_o=o(" (OpenAI GPT model)"),F_o=l(),cu=a("li"),pee=a("strong"),C_o=o("pegasus"),M_o=o(" \u2014 "),LP=a("a"),E_o=o("PegasusForCausalLM"),y_o=o(" (Pegasus model)"),w_o=l(),fu=a("li"),_ee=a("strong"),A_o=o("plbart"),L_o=o(" \u2014 "),BP=a("a"),B_o=o("PLBartForCausalLM"),x_o=o(" (PLBart model)"),k_o=l(),mu=a("li"),uee=a("strong"),R_o=o("prophetnet"),S_o=o(" \u2014 "),xP=a("a"),P_o=o("ProphetNetForCausalLM"),$_o=o(" (ProphetNet model)"),I_o=l(),gu=a("li"),bee=a("strong"),D_o=o("qdqbert"),j_o=o(" \u2014 "),kP=a("a"),N_o=o("QDQBertLMHeadModel"),q_o=o(" (QDQBert model)"),G_o=l(),hu=a("li"),vee=a("strong"),O_o=o("reformer"),X_o=o(" \u2014 "),RP=a("a"),V_o=o("ReformerModelWithLMHead"),z_o=o(" (Reformer model)"),W_o=l(),pu=a("li"),Tee=a("strong"),Q_o=o("rembert"),H_o=o(" \u2014 "),SP=a("a"),U_o=o("RemBertForCausalLM"),J_o=o(" (RemBERT model)"),Y_o=l(),_u=a("li"),Fee=a("strong"),K_o=o("roberta"),Z_o=o(" \u2014 "),PP=a("a"),euo=o("RobertaForCausalLM"),ouo=o(" (RoBERTa model)"),ruo=l(),uu=a("li"),Cee=a("strong"),tuo=o("roformer"),auo=o(" \u2014 "),$P=a("a"),nuo=o("RoFormerForCausalLM"),suo=o(" (RoFormer model)"),luo=l(),bu=a("li"),Mee=a("strong"),iuo=o("speech_to_text_2"),duo=o(" \u2014 "),IP=a("a"),cuo=o("Speech2Text2ForCausalLM"),fuo=o(" (Speech2Text2 model)"),muo=l(),vu=a("li"),Eee=a("strong"),guo=o("transfo-xl"),huo=o(" \u2014 "),DP=a("a"),puo=o("TransfoXLLMHeadModel"),_uo=o(" (Transformer-XL model)"),uuo=l(),Tu=a("li"),yee=a("strong"),buo=o("trocr"),vuo=o(" \u2014 "),jP=a("a"),Tuo=o("TrOCRForCausalLM"),Fuo=o(" (TrOCR model)"),Cuo=l(),Fu=a("li"),wee=a("strong"),Muo=o("xglm"),Euo=o(" \u2014 "),NP=a("a"),yuo=o("XGLMForCausalLM"),wuo=o(" (XGLM model)"),Auo=l(),Cu=a("li"),Aee=a("strong"),Luo=o("xlm"),Buo=o(" \u2014 "),qP=a("a"),xuo=o("XLMWithLMHeadModel"),kuo=o(" (XLM model)"),Ruo=l(),Mu=a("li"),Lee=a("strong"),Suo=o("xlm-prophetnet"),Puo=o(" \u2014 "),GP=a("a"),$uo=o("XLMProphetNetForCausalLM"),Iuo=o(" (XLMProphetNet model)"),Duo=l(),Eu=a("li"),Bee=a("strong"),juo=o("xlm-roberta"),Nuo=o(" \u2014 "),OP=a("a"),quo=o("XLMRobertaForCausalLM"),Guo=o(" (XLM-RoBERTa model)"),Ouo=l(),yu=a("li"),xee=a("strong"),Xuo=o("xlm-roberta-xl"),Vuo=o(" \u2014 "),XP=a("a"),zuo=o("XLMRobertaXLForCausalLM"),Wuo=o(" (XLM-RoBERTa-XL model)"),Quo=l(),wu=a("li"),kee=a("strong"),Huo=o("xlnet"),Uuo=o(" \u2014 "),VP=a("a"),Juo=o("XLNetLMHeadModel"),Yuo=o(" (XLNet model)"),Kuo=l(),Au=a("p"),Zuo=o("The model is set in evaluation mode by default using "),Ree=a("code"),e0o=o("model.eval()"),o0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),See=a("code"),r0o=o("model.train()"),t0o=l(),Pee=a("p"),a0o=o("Examples:"),n0o=l(),f(BE.$$.fragment),bBe=l(),Yi=a("h2"),Lu=a("a"),$ee=a("span"),f(xE.$$.fragment),s0o=l(),Iee=a("span"),l0o=o("AutoModelForMaskedLM"),vBe=l(),Ko=a("div"),f(kE.$$.fragment),i0o=l(),Ki=a("p"),d0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Dee=a("code"),c0o=o("from_pretrained()"),f0o=o("class method or the "),jee=a("code"),m0o=o("from_config()"),g0o=o(`class
method.`),h0o=l(),RE=a("p"),p0o=o("This class cannot be instantiated directly using "),Nee=a("code"),_0o=o("__init__()"),u0o=o(" (throws an error)."),b0o=l(),zr=a("div"),f(SE.$$.fragment),v0o=l(),qee=a("p"),T0o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),F0o=l(),Zi=a("p"),C0o=o(`Note:
Loading a model from its configuration file does `),Gee=a("strong"),M0o=o("not"),E0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Oee=a("code"),y0o=o("from_pretrained()"),w0o=o("to load the model weights."),A0o=l(),Xee=a("p"),L0o=o("Examples:"),B0o=l(),f(PE.$$.fragment),x0o=l(),qe=a("div"),f($E.$$.fragment),k0o=l(),Vee=a("p"),R0o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),S0o=l(),Oa=a("p"),P0o=o("The model class to instantiate is selected based on the "),zee=a("code"),$0o=o("model_type"),I0o=o(` property of the config object (either
passed as an argument or loaded from `),Wee=a("code"),D0o=o("pretrained_model_name_or_path"),j0o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qee=a("code"),N0o=o("pretrained_model_name_or_path"),q0o=o(":"),G0o=l(),I=a("ul"),Bu=a("li"),Hee=a("strong"),O0o=o("albert"),X0o=o(" \u2014 "),zP=a("a"),V0o=o("AlbertForMaskedLM"),z0o=o(" (ALBERT model)"),W0o=l(),xu=a("li"),Uee=a("strong"),Q0o=o("bart"),H0o=o(" \u2014 "),WP=a("a"),U0o=o("BartForConditionalGeneration"),J0o=o(" (BART model)"),Y0o=l(),ku=a("li"),Jee=a("strong"),K0o=o("bert"),Z0o=o(" \u2014 "),QP=a("a"),e1o=o("BertForMaskedLM"),o1o=o(" (BERT model)"),r1o=l(),Ru=a("li"),Yee=a("strong"),t1o=o("big_bird"),a1o=o(" \u2014 "),HP=a("a"),n1o=o("BigBirdForMaskedLM"),s1o=o(" (BigBird model)"),l1o=l(),Su=a("li"),Kee=a("strong"),i1o=o("camembert"),d1o=o(" \u2014 "),UP=a("a"),c1o=o("CamembertForMaskedLM"),f1o=o(" (CamemBERT model)"),m1o=l(),Pu=a("li"),Zee=a("strong"),g1o=o("convbert"),h1o=o(" \u2014 "),JP=a("a"),p1o=o("ConvBertForMaskedLM"),_1o=o(" (ConvBERT model)"),u1o=l(),$u=a("li"),eoe=a("strong"),b1o=o("data2vec-text"),v1o=o(" \u2014 "),YP=a("a"),T1o=o("Data2VecTextForMaskedLM"),F1o=o(" (Data2VecText model)"),C1o=l(),Iu=a("li"),ooe=a("strong"),M1o=o("deberta"),E1o=o(" \u2014 "),KP=a("a"),y1o=o("DebertaForMaskedLM"),w1o=o(" (DeBERTa model)"),A1o=l(),Du=a("li"),roe=a("strong"),L1o=o("deberta-v2"),B1o=o(" \u2014 "),ZP=a("a"),x1o=o("DebertaV2ForMaskedLM"),k1o=o(" (DeBERTa-v2 model)"),R1o=l(),ju=a("li"),toe=a("strong"),S1o=o("distilbert"),P1o=o(" \u2014 "),e$=a("a"),$1o=o("DistilBertForMaskedLM"),I1o=o(" (DistilBERT model)"),D1o=l(),Nu=a("li"),aoe=a("strong"),j1o=o("electra"),N1o=o(" \u2014 "),o$=a("a"),q1o=o("ElectraForMaskedLM"),G1o=o(" (ELECTRA model)"),O1o=l(),qu=a("li"),noe=a("strong"),X1o=o("flaubert"),V1o=o(" \u2014 "),r$=a("a"),z1o=o("FlaubertWithLMHeadModel"),W1o=o(" (FlauBERT model)"),Q1o=l(),Gu=a("li"),soe=a("strong"),H1o=o("fnet"),U1o=o(" \u2014 "),t$=a("a"),J1o=o("FNetForMaskedLM"),Y1o=o(" (FNet model)"),K1o=l(),Ou=a("li"),loe=a("strong"),Z1o=o("funnel"),ebo=o(" \u2014 "),a$=a("a"),obo=o("FunnelForMaskedLM"),rbo=o(" (Funnel Transformer model)"),tbo=l(),Xu=a("li"),ioe=a("strong"),abo=o("ibert"),nbo=o(" \u2014 "),n$=a("a"),sbo=o("IBertForMaskedLM"),lbo=o(" (I-BERT model)"),ibo=l(),Vu=a("li"),doe=a("strong"),dbo=o("layoutlm"),cbo=o(" \u2014 "),s$=a("a"),fbo=o("LayoutLMForMaskedLM"),mbo=o(" (LayoutLM model)"),gbo=l(),zu=a("li"),coe=a("strong"),hbo=o("longformer"),pbo=o(" \u2014 "),l$=a("a"),_bo=o("LongformerForMaskedLM"),ubo=o(" (Longformer model)"),bbo=l(),Wu=a("li"),foe=a("strong"),vbo=o("mbart"),Tbo=o(" \u2014 "),i$=a("a"),Fbo=o("MBartForConditionalGeneration"),Cbo=o(" (mBART model)"),Mbo=l(),Qu=a("li"),moe=a("strong"),Ebo=o("megatron-bert"),ybo=o(" \u2014 "),d$=a("a"),wbo=o("MegatronBertForMaskedLM"),Abo=o(" (MegatronBert model)"),Lbo=l(),Hu=a("li"),goe=a("strong"),Bbo=o("mobilebert"),xbo=o(" \u2014 "),c$=a("a"),kbo=o("MobileBertForMaskedLM"),Rbo=o(" (MobileBERT model)"),Sbo=l(),Uu=a("li"),hoe=a("strong"),Pbo=o("mpnet"),$bo=o(" \u2014 "),f$=a("a"),Ibo=o("MPNetForMaskedLM"),Dbo=o(" (MPNet model)"),jbo=l(),Ju=a("li"),poe=a("strong"),Nbo=o("nystromformer"),qbo=o(" \u2014 "),m$=a("a"),Gbo=o("NystromformerForMaskedLM"),Obo=o(" (Nystromformer model)"),Xbo=l(),Yu=a("li"),_oe=a("strong"),Vbo=o("perceiver"),zbo=o(" \u2014 "),g$=a("a"),Wbo=o("PerceiverForMaskedLM"),Qbo=o(" (Perceiver model)"),Hbo=l(),Ku=a("li"),uoe=a("strong"),Ubo=o("qdqbert"),Jbo=o(" \u2014 "),h$=a("a"),Ybo=o("QDQBertForMaskedLM"),Kbo=o(" (QDQBert model)"),Zbo=l(),Zu=a("li"),boe=a("strong"),e5o=o("reformer"),o5o=o(" \u2014 "),p$=a("a"),r5o=o("ReformerForMaskedLM"),t5o=o(" (Reformer model)"),a5o=l(),e0=a("li"),voe=a("strong"),n5o=o("rembert"),s5o=o(" \u2014 "),_$=a("a"),l5o=o("RemBertForMaskedLM"),i5o=o(" (RemBERT model)"),d5o=l(),o0=a("li"),Toe=a("strong"),c5o=o("roberta"),f5o=o(" \u2014 "),u$=a("a"),m5o=o("RobertaForMaskedLM"),g5o=o(" (RoBERTa model)"),h5o=l(),r0=a("li"),Foe=a("strong"),p5o=o("roformer"),_5o=o(" \u2014 "),b$=a("a"),u5o=o("RoFormerForMaskedLM"),b5o=o(" (RoFormer model)"),v5o=l(),t0=a("li"),Coe=a("strong"),T5o=o("squeezebert"),F5o=o(" \u2014 "),v$=a("a"),C5o=o("SqueezeBertForMaskedLM"),M5o=o(" (SqueezeBERT model)"),E5o=l(),a0=a("li"),Moe=a("strong"),y5o=o("tapas"),w5o=o(" \u2014 "),T$=a("a"),A5o=o("TapasForMaskedLM"),L5o=o(" (TAPAS model)"),B5o=l(),n0=a("li"),Eoe=a("strong"),x5o=o("wav2vec2"),k5o=o(" \u2014 "),yoe=a("code"),R5o=o("Wav2Vec2ForMaskedLM"),S5o=o("(Wav2Vec2 model)"),P5o=l(),s0=a("li"),woe=a("strong"),$5o=o("xlm"),I5o=o(" \u2014 "),F$=a("a"),D5o=o("XLMWithLMHeadModel"),j5o=o(" (XLM model)"),N5o=l(),l0=a("li"),Aoe=a("strong"),q5o=o("xlm-roberta"),G5o=o(" \u2014 "),C$=a("a"),O5o=o("XLMRobertaForMaskedLM"),X5o=o(" (XLM-RoBERTa model)"),V5o=l(),i0=a("li"),Loe=a("strong"),z5o=o("xlm-roberta-xl"),W5o=o(" \u2014 "),M$=a("a"),Q5o=o("XLMRobertaXLForMaskedLM"),H5o=o(" (XLM-RoBERTa-XL model)"),U5o=l(),d0=a("li"),Boe=a("strong"),J5o=o("yoso"),Y5o=o(" \u2014 "),E$=a("a"),K5o=o("YosoForMaskedLM"),Z5o=o(" (YOSO model)"),e2o=l(),c0=a("p"),o2o=o("The model is set in evaluation mode by default using "),xoe=a("code"),r2o=o("model.eval()"),t2o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),koe=a("code"),a2o=o("model.train()"),n2o=l(),Roe=a("p"),s2o=o("Examples:"),l2o=l(),f(IE.$$.fragment),TBe=l(),ed=a("h2"),f0=a("a"),Soe=a("span"),f(DE.$$.fragment),i2o=l(),Poe=a("span"),d2o=o("AutoModelForSeq2SeqLM"),FBe=l(),Zo=a("div"),f(jE.$$.fragment),c2o=l(),od=a("p"),f2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$oe=a("code"),m2o=o("from_pretrained()"),g2o=o("class method or the "),Ioe=a("code"),h2o=o("from_config()"),p2o=o(`class
method.`),_2o=l(),NE=a("p"),u2o=o("This class cannot be instantiated directly using "),Doe=a("code"),b2o=o("__init__()"),v2o=o(" (throws an error)."),T2o=l(),Wr=a("div"),f(qE.$$.fragment),F2o=l(),joe=a("p"),C2o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),M2o=l(),rd=a("p"),E2o=o(`Note:
Loading a model from its configuration file does `),Noe=a("strong"),y2o=o("not"),w2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qoe=a("code"),A2o=o("from_pretrained()"),L2o=o("to load the model weights."),B2o=l(),Goe=a("p"),x2o=o("Examples:"),k2o=l(),f(GE.$$.fragment),R2o=l(),Ge=a("div"),f(OE.$$.fragment),S2o=l(),Ooe=a("p"),P2o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),$2o=l(),Xa=a("p"),I2o=o("The model class to instantiate is selected based on the "),Xoe=a("code"),D2o=o("model_type"),j2o=o(` property of the config object (either
passed as an argument or loaded from `),Voe=a("code"),N2o=o("pretrained_model_name_or_path"),q2o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zoe=a("code"),G2o=o("pretrained_model_name_or_path"),O2o=o(":"),X2o=l(),ae=a("ul"),m0=a("li"),Woe=a("strong"),V2o=o("bart"),z2o=o(" \u2014 "),y$=a("a"),W2o=o("BartForConditionalGeneration"),Q2o=o(" (BART model)"),H2o=l(),g0=a("li"),Qoe=a("strong"),U2o=o("bigbird_pegasus"),J2o=o(" \u2014 "),w$=a("a"),Y2o=o("BigBirdPegasusForConditionalGeneration"),K2o=o(" (BigBirdPegasus model)"),Z2o=l(),h0=a("li"),Hoe=a("strong"),evo=o("blenderbot"),ovo=o(" \u2014 "),A$=a("a"),rvo=o("BlenderbotForConditionalGeneration"),tvo=o(" (Blenderbot model)"),avo=l(),p0=a("li"),Uoe=a("strong"),nvo=o("blenderbot-small"),svo=o(" \u2014 "),L$=a("a"),lvo=o("BlenderbotSmallForConditionalGeneration"),ivo=o(" (BlenderbotSmall model)"),dvo=l(),_0=a("li"),Joe=a("strong"),cvo=o("encoder-decoder"),fvo=o(" \u2014 "),B$=a("a"),mvo=o("EncoderDecoderModel"),gvo=o(" (Encoder decoder model)"),hvo=l(),u0=a("li"),Yoe=a("strong"),pvo=o("fsmt"),_vo=o(" \u2014 "),x$=a("a"),uvo=o("FSMTForConditionalGeneration"),bvo=o(" (FairSeq Machine-Translation model)"),vvo=l(),b0=a("li"),Koe=a("strong"),Tvo=o("led"),Fvo=o(" \u2014 "),k$=a("a"),Cvo=o("LEDForConditionalGeneration"),Mvo=o(" (LED model)"),Evo=l(),v0=a("li"),Zoe=a("strong"),yvo=o("m2m_100"),wvo=o(" \u2014 "),R$=a("a"),Avo=o("M2M100ForConditionalGeneration"),Lvo=o(" (M2M100 model)"),Bvo=l(),T0=a("li"),ere=a("strong"),xvo=o("marian"),kvo=o(" \u2014 "),S$=a("a"),Rvo=o("MarianMTModel"),Svo=o(" (Marian model)"),Pvo=l(),F0=a("li"),ore=a("strong"),$vo=o("mbart"),Ivo=o(" \u2014 "),P$=a("a"),Dvo=o("MBartForConditionalGeneration"),jvo=o(" (mBART model)"),Nvo=l(),C0=a("li"),rre=a("strong"),qvo=o("mt5"),Gvo=o(" \u2014 "),$$=a("a"),Ovo=o("MT5ForConditionalGeneration"),Xvo=o(" (mT5 model)"),Vvo=l(),M0=a("li"),tre=a("strong"),zvo=o("pegasus"),Wvo=o(" \u2014 "),I$=a("a"),Qvo=o("PegasusForConditionalGeneration"),Hvo=o(" (Pegasus model)"),Uvo=l(),E0=a("li"),are=a("strong"),Jvo=o("plbart"),Yvo=o(" \u2014 "),D$=a("a"),Kvo=o("PLBartForConditionalGeneration"),Zvo=o(" (PLBart model)"),eTo=l(),y0=a("li"),nre=a("strong"),oTo=o("prophetnet"),rTo=o(" \u2014 "),j$=a("a"),tTo=o("ProphetNetForConditionalGeneration"),aTo=o(" (ProphetNet model)"),nTo=l(),w0=a("li"),sre=a("strong"),sTo=o("t5"),lTo=o(" \u2014 "),N$=a("a"),iTo=o("T5ForConditionalGeneration"),dTo=o(" (T5 model)"),cTo=l(),A0=a("li"),lre=a("strong"),fTo=o("xlm-prophetnet"),mTo=o(" \u2014 "),q$=a("a"),gTo=o("XLMProphetNetForConditionalGeneration"),hTo=o(" (XLMProphetNet model)"),pTo=l(),L0=a("p"),_To=o("The model is set in evaluation mode by default using "),ire=a("code"),uTo=o("model.eval()"),bTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dre=a("code"),vTo=o("model.train()"),TTo=l(),cre=a("p"),FTo=o("Examples:"),CTo=l(),f(XE.$$.fragment),CBe=l(),td=a("h2"),B0=a("a"),fre=a("span"),f(VE.$$.fragment),MTo=l(),mre=a("span"),ETo=o("AutoModelForSequenceClassification"),MBe=l(),er=a("div"),f(zE.$$.fragment),yTo=l(),ad=a("p"),wTo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),gre=a("code"),ATo=o("from_pretrained()"),LTo=o("class method or the "),hre=a("code"),BTo=o("from_config()"),xTo=o(`class
method.`),kTo=l(),WE=a("p"),RTo=o("This class cannot be instantiated directly using "),pre=a("code"),STo=o("__init__()"),PTo=o(" (throws an error)."),$To=l(),Qr=a("div"),f(QE.$$.fragment),ITo=l(),_re=a("p"),DTo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),jTo=l(),nd=a("p"),NTo=o(`Note:
Loading a model from its configuration file does `),ure=a("strong"),qTo=o("not"),GTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bre=a("code"),OTo=o("from_pretrained()"),XTo=o("to load the model weights."),VTo=l(),vre=a("p"),zTo=o("Examples:"),WTo=l(),f(HE.$$.fragment),QTo=l(),Oe=a("div"),f(UE.$$.fragment),HTo=l(),Tre=a("p"),UTo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),JTo=l(),Va=a("p"),YTo=o("The model class to instantiate is selected based on the "),Fre=a("code"),KTo=o("model_type"),ZTo=o(` property of the config object (either
passed as an argument or loaded from `),Cre=a("code"),eFo=o("pretrained_model_name_or_path"),oFo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mre=a("code"),rFo=o("pretrained_model_name_or_path"),tFo=o(":"),aFo=l(),A=a("ul"),x0=a("li"),Ere=a("strong"),nFo=o("albert"),sFo=o(" \u2014 "),G$=a("a"),lFo=o("AlbertForSequenceClassification"),iFo=o(" (ALBERT model)"),dFo=l(),k0=a("li"),yre=a("strong"),cFo=o("bart"),fFo=o(" \u2014 "),O$=a("a"),mFo=o("BartForSequenceClassification"),gFo=o(" (BART model)"),hFo=l(),R0=a("li"),wre=a("strong"),pFo=o("bert"),_Fo=o(" \u2014 "),X$=a("a"),uFo=o("BertForSequenceClassification"),bFo=o(" (BERT model)"),vFo=l(),S0=a("li"),Are=a("strong"),TFo=o("big_bird"),FFo=o(" \u2014 "),V$=a("a"),CFo=o("BigBirdForSequenceClassification"),MFo=o(" (BigBird model)"),EFo=l(),P0=a("li"),Lre=a("strong"),yFo=o("bigbird_pegasus"),wFo=o(" \u2014 "),z$=a("a"),AFo=o("BigBirdPegasusForSequenceClassification"),LFo=o(" (BigBirdPegasus model)"),BFo=l(),$0=a("li"),Bre=a("strong"),xFo=o("camembert"),kFo=o(" \u2014 "),W$=a("a"),RFo=o("CamembertForSequenceClassification"),SFo=o(" (CamemBERT model)"),PFo=l(),I0=a("li"),xre=a("strong"),$Fo=o("canine"),IFo=o(" \u2014 "),Q$=a("a"),DFo=o("CanineForSequenceClassification"),jFo=o(" (Canine model)"),NFo=l(),D0=a("li"),kre=a("strong"),qFo=o("convbert"),GFo=o(" \u2014 "),H$=a("a"),OFo=o("ConvBertForSequenceClassification"),XFo=o(" (ConvBERT model)"),VFo=l(),j0=a("li"),Rre=a("strong"),zFo=o("ctrl"),WFo=o(" \u2014 "),U$=a("a"),QFo=o("CTRLForSequenceClassification"),HFo=o(" (CTRL model)"),UFo=l(),N0=a("li"),Sre=a("strong"),JFo=o("data2vec-text"),YFo=o(" \u2014 "),J$=a("a"),KFo=o("Data2VecTextForSequenceClassification"),ZFo=o(" (Data2VecText model)"),e9o=l(),q0=a("li"),Pre=a("strong"),o9o=o("deberta"),r9o=o(" \u2014 "),Y$=a("a"),t9o=o("DebertaForSequenceClassification"),a9o=o(" (DeBERTa model)"),n9o=l(),G0=a("li"),$re=a("strong"),s9o=o("deberta-v2"),l9o=o(" \u2014 "),K$=a("a"),i9o=o("DebertaV2ForSequenceClassification"),d9o=o(" (DeBERTa-v2 model)"),c9o=l(),O0=a("li"),Ire=a("strong"),f9o=o("distilbert"),m9o=o(" \u2014 "),Z$=a("a"),g9o=o("DistilBertForSequenceClassification"),h9o=o(" (DistilBERT model)"),p9o=l(),X0=a("li"),Dre=a("strong"),_9o=o("electra"),u9o=o(" \u2014 "),eI=a("a"),b9o=o("ElectraForSequenceClassification"),v9o=o(" (ELECTRA model)"),T9o=l(),V0=a("li"),jre=a("strong"),F9o=o("flaubert"),C9o=o(" \u2014 "),oI=a("a"),M9o=o("FlaubertForSequenceClassification"),E9o=o(" (FlauBERT model)"),y9o=l(),z0=a("li"),Nre=a("strong"),w9o=o("fnet"),A9o=o(" \u2014 "),rI=a("a"),L9o=o("FNetForSequenceClassification"),B9o=o(" (FNet model)"),x9o=l(),W0=a("li"),qre=a("strong"),k9o=o("funnel"),R9o=o(" \u2014 "),tI=a("a"),S9o=o("FunnelForSequenceClassification"),P9o=o(" (Funnel Transformer model)"),$9o=l(),Q0=a("li"),Gre=a("strong"),I9o=o("gpt2"),D9o=o(" \u2014 "),aI=a("a"),j9o=o("GPT2ForSequenceClassification"),N9o=o(" (OpenAI GPT-2 model)"),q9o=l(),H0=a("li"),Ore=a("strong"),G9o=o("gpt_neo"),O9o=o(" \u2014 "),nI=a("a"),X9o=o("GPTNeoForSequenceClassification"),V9o=o(" (GPT Neo model)"),z9o=l(),U0=a("li"),Xre=a("strong"),W9o=o("gptj"),Q9o=o(" \u2014 "),sI=a("a"),H9o=o("GPTJForSequenceClassification"),U9o=o(" (GPT-J model)"),J9o=l(),J0=a("li"),Vre=a("strong"),Y9o=o("ibert"),K9o=o(" \u2014 "),lI=a("a"),Z9o=o("IBertForSequenceClassification"),eCo=o(" (I-BERT model)"),oCo=l(),Y0=a("li"),zre=a("strong"),rCo=o("layoutlm"),tCo=o(" \u2014 "),iI=a("a"),aCo=o("LayoutLMForSequenceClassification"),nCo=o(" (LayoutLM model)"),sCo=l(),K0=a("li"),Wre=a("strong"),lCo=o("layoutlmv2"),iCo=o(" \u2014 "),dI=a("a"),dCo=o("LayoutLMv2ForSequenceClassification"),cCo=o(" (LayoutLMv2 model)"),fCo=l(),Z0=a("li"),Qre=a("strong"),mCo=o("led"),gCo=o(" \u2014 "),cI=a("a"),hCo=o("LEDForSequenceClassification"),pCo=o(" (LED model)"),_Co=l(),e1=a("li"),Hre=a("strong"),uCo=o("longformer"),bCo=o(" \u2014 "),fI=a("a"),vCo=o("LongformerForSequenceClassification"),TCo=o(" (Longformer model)"),FCo=l(),o1=a("li"),Ure=a("strong"),CCo=o("mbart"),MCo=o(" \u2014 "),mI=a("a"),ECo=o("MBartForSequenceClassification"),yCo=o(" (mBART model)"),wCo=l(),r1=a("li"),Jre=a("strong"),ACo=o("megatron-bert"),LCo=o(" \u2014 "),gI=a("a"),BCo=o("MegatronBertForSequenceClassification"),xCo=o(" (MegatronBert model)"),kCo=l(),t1=a("li"),Yre=a("strong"),RCo=o("mobilebert"),SCo=o(" \u2014 "),hI=a("a"),PCo=o("MobileBertForSequenceClassification"),$Co=o(" (MobileBERT model)"),ICo=l(),a1=a("li"),Kre=a("strong"),DCo=o("mpnet"),jCo=o(" \u2014 "),pI=a("a"),NCo=o("MPNetForSequenceClassification"),qCo=o(" (MPNet model)"),GCo=l(),n1=a("li"),Zre=a("strong"),OCo=o("nystromformer"),XCo=o(" \u2014 "),_I=a("a"),VCo=o("NystromformerForSequenceClassification"),zCo=o(" (Nystromformer model)"),WCo=l(),s1=a("li"),ete=a("strong"),QCo=o("openai-gpt"),HCo=o(" \u2014 "),uI=a("a"),UCo=o("OpenAIGPTForSequenceClassification"),JCo=o(" (OpenAI GPT model)"),YCo=l(),l1=a("li"),ote=a("strong"),KCo=o("perceiver"),ZCo=o(" \u2014 "),bI=a("a"),eMo=o("PerceiverForSequenceClassification"),oMo=o(" (Perceiver model)"),rMo=l(),i1=a("li"),rte=a("strong"),tMo=o("plbart"),aMo=o(" \u2014 "),vI=a("a"),nMo=o("PLBartForSequenceClassification"),sMo=o(" (PLBart model)"),lMo=l(),d1=a("li"),tte=a("strong"),iMo=o("qdqbert"),dMo=o(" \u2014 "),TI=a("a"),cMo=o("QDQBertForSequenceClassification"),fMo=o(" (QDQBert model)"),mMo=l(),c1=a("li"),ate=a("strong"),gMo=o("reformer"),hMo=o(" \u2014 "),FI=a("a"),pMo=o("ReformerForSequenceClassification"),_Mo=o(" (Reformer model)"),uMo=l(),f1=a("li"),nte=a("strong"),bMo=o("rembert"),vMo=o(" \u2014 "),CI=a("a"),TMo=o("RemBertForSequenceClassification"),FMo=o(" (RemBERT model)"),CMo=l(),m1=a("li"),ste=a("strong"),MMo=o("roberta"),EMo=o(" \u2014 "),MI=a("a"),yMo=o("RobertaForSequenceClassification"),wMo=o(" (RoBERTa model)"),AMo=l(),g1=a("li"),lte=a("strong"),LMo=o("roformer"),BMo=o(" \u2014 "),EI=a("a"),xMo=o("RoFormerForSequenceClassification"),kMo=o(" (RoFormer model)"),RMo=l(),h1=a("li"),ite=a("strong"),SMo=o("squeezebert"),PMo=o(" \u2014 "),yI=a("a"),$Mo=o("SqueezeBertForSequenceClassification"),IMo=o(" (SqueezeBERT model)"),DMo=l(),p1=a("li"),dte=a("strong"),jMo=o("tapas"),NMo=o(" \u2014 "),wI=a("a"),qMo=o("TapasForSequenceClassification"),GMo=o(" (TAPAS model)"),OMo=l(),_1=a("li"),cte=a("strong"),XMo=o("transfo-xl"),VMo=o(" \u2014 "),AI=a("a"),zMo=o("TransfoXLForSequenceClassification"),WMo=o(" (Transformer-XL model)"),QMo=l(),u1=a("li"),fte=a("strong"),HMo=o("xlm"),UMo=o(" \u2014 "),LI=a("a"),JMo=o("XLMForSequenceClassification"),YMo=o(" (XLM model)"),KMo=l(),b1=a("li"),mte=a("strong"),ZMo=o("xlm-roberta"),e4o=o(" \u2014 "),BI=a("a"),o4o=o("XLMRobertaForSequenceClassification"),r4o=o(" (XLM-RoBERTa model)"),t4o=l(),v1=a("li"),gte=a("strong"),a4o=o("xlm-roberta-xl"),n4o=o(" \u2014 "),xI=a("a"),s4o=o("XLMRobertaXLForSequenceClassification"),l4o=o(" (XLM-RoBERTa-XL model)"),i4o=l(),T1=a("li"),hte=a("strong"),d4o=o("xlnet"),c4o=o(" \u2014 "),kI=a("a"),f4o=o("XLNetForSequenceClassification"),m4o=o(" (XLNet model)"),g4o=l(),F1=a("li"),pte=a("strong"),h4o=o("yoso"),p4o=o(" \u2014 "),RI=a("a"),_4o=o("YosoForSequenceClassification"),u4o=o(" (YOSO model)"),b4o=l(),C1=a("p"),v4o=o("The model is set in evaluation mode by default using "),_te=a("code"),T4o=o("model.eval()"),F4o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ute=a("code"),C4o=o("model.train()"),M4o=l(),bte=a("p"),E4o=o("Examples:"),y4o=l(),f(JE.$$.fragment),EBe=l(),sd=a("h2"),M1=a("a"),vte=a("span"),f(YE.$$.fragment),w4o=l(),Tte=a("span"),A4o=o("AutoModelForMultipleChoice"),yBe=l(),or=a("div"),f(KE.$$.fragment),L4o=l(),ld=a("p"),B4o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Fte=a("code"),x4o=o("from_pretrained()"),k4o=o("class method or the "),Cte=a("code"),R4o=o("from_config()"),S4o=o(`class
method.`),P4o=l(),ZE=a("p"),$4o=o("This class cannot be instantiated directly using "),Mte=a("code"),I4o=o("__init__()"),D4o=o(" (throws an error)."),j4o=l(),Hr=a("div"),f(e3.$$.fragment),N4o=l(),Ete=a("p"),q4o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),G4o=l(),id=a("p"),O4o=o(`Note:
Loading a model from its configuration file does `),yte=a("strong"),X4o=o("not"),V4o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wte=a("code"),z4o=o("from_pretrained()"),W4o=o("to load the model weights."),Q4o=l(),Ate=a("p"),H4o=o("Examples:"),U4o=l(),f(o3.$$.fragment),J4o=l(),Xe=a("div"),f(r3.$$.fragment),Y4o=l(),Lte=a("p"),K4o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Z4o=l(),za=a("p"),eEo=o("The model class to instantiate is selected based on the "),Bte=a("code"),oEo=o("model_type"),rEo=o(` property of the config object (either
passed as an argument or loaded from `),xte=a("code"),tEo=o("pretrained_model_name_or_path"),aEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kte=a("code"),nEo=o("pretrained_model_name_or_path"),sEo=o(":"),lEo=l(),G=a("ul"),E1=a("li"),Rte=a("strong"),iEo=o("albert"),dEo=o(" \u2014 "),SI=a("a"),cEo=o("AlbertForMultipleChoice"),fEo=o(" (ALBERT model)"),mEo=l(),y1=a("li"),Ste=a("strong"),gEo=o("bert"),hEo=o(" \u2014 "),PI=a("a"),pEo=o("BertForMultipleChoice"),_Eo=o(" (BERT model)"),uEo=l(),w1=a("li"),Pte=a("strong"),bEo=o("big_bird"),vEo=o(" \u2014 "),$I=a("a"),TEo=o("BigBirdForMultipleChoice"),FEo=o(" (BigBird model)"),CEo=l(),A1=a("li"),$te=a("strong"),MEo=o("camembert"),EEo=o(" \u2014 "),II=a("a"),yEo=o("CamembertForMultipleChoice"),wEo=o(" (CamemBERT model)"),AEo=l(),L1=a("li"),Ite=a("strong"),LEo=o("canine"),BEo=o(" \u2014 "),DI=a("a"),xEo=o("CanineForMultipleChoice"),kEo=o(" (Canine model)"),REo=l(),B1=a("li"),Dte=a("strong"),SEo=o("convbert"),PEo=o(" \u2014 "),jI=a("a"),$Eo=o("ConvBertForMultipleChoice"),IEo=o(" (ConvBERT model)"),DEo=l(),x1=a("li"),jte=a("strong"),jEo=o("data2vec-text"),NEo=o(" \u2014 "),NI=a("a"),qEo=o("Data2VecTextForMultipleChoice"),GEo=o(" (Data2VecText model)"),OEo=l(),k1=a("li"),Nte=a("strong"),XEo=o("distilbert"),VEo=o(" \u2014 "),qI=a("a"),zEo=o("DistilBertForMultipleChoice"),WEo=o(" (DistilBERT model)"),QEo=l(),R1=a("li"),qte=a("strong"),HEo=o("electra"),UEo=o(" \u2014 "),GI=a("a"),JEo=o("ElectraForMultipleChoice"),YEo=o(" (ELECTRA model)"),KEo=l(),S1=a("li"),Gte=a("strong"),ZEo=o("flaubert"),e3o=o(" \u2014 "),OI=a("a"),o3o=o("FlaubertForMultipleChoice"),r3o=o(" (FlauBERT model)"),t3o=l(),P1=a("li"),Ote=a("strong"),a3o=o("fnet"),n3o=o(" \u2014 "),XI=a("a"),s3o=o("FNetForMultipleChoice"),l3o=o(" (FNet model)"),i3o=l(),$1=a("li"),Xte=a("strong"),d3o=o("funnel"),c3o=o(" \u2014 "),VI=a("a"),f3o=o("FunnelForMultipleChoice"),m3o=o(" (Funnel Transformer model)"),g3o=l(),I1=a("li"),Vte=a("strong"),h3o=o("ibert"),p3o=o(" \u2014 "),zI=a("a"),_3o=o("IBertForMultipleChoice"),u3o=o(" (I-BERT model)"),b3o=l(),D1=a("li"),zte=a("strong"),v3o=o("longformer"),T3o=o(" \u2014 "),WI=a("a"),F3o=o("LongformerForMultipleChoice"),C3o=o(" (Longformer model)"),M3o=l(),j1=a("li"),Wte=a("strong"),E3o=o("megatron-bert"),y3o=o(" \u2014 "),QI=a("a"),w3o=o("MegatronBertForMultipleChoice"),A3o=o(" (MegatronBert model)"),L3o=l(),N1=a("li"),Qte=a("strong"),B3o=o("mobilebert"),x3o=o(" \u2014 "),HI=a("a"),k3o=o("MobileBertForMultipleChoice"),R3o=o(" (MobileBERT model)"),S3o=l(),q1=a("li"),Hte=a("strong"),P3o=o("mpnet"),$3o=o(" \u2014 "),UI=a("a"),I3o=o("MPNetForMultipleChoice"),D3o=o(" (MPNet model)"),j3o=l(),G1=a("li"),Ute=a("strong"),N3o=o("nystromformer"),q3o=o(" \u2014 "),JI=a("a"),G3o=o("NystromformerForMultipleChoice"),O3o=o(" (Nystromformer model)"),X3o=l(),O1=a("li"),Jte=a("strong"),V3o=o("qdqbert"),z3o=o(" \u2014 "),YI=a("a"),W3o=o("QDQBertForMultipleChoice"),Q3o=o(" (QDQBert model)"),H3o=l(),X1=a("li"),Yte=a("strong"),U3o=o("rembert"),J3o=o(" \u2014 "),KI=a("a"),Y3o=o("RemBertForMultipleChoice"),K3o=o(" (RemBERT model)"),Z3o=l(),V1=a("li"),Kte=a("strong"),eyo=o("roberta"),oyo=o(" \u2014 "),ZI=a("a"),ryo=o("RobertaForMultipleChoice"),tyo=o(" (RoBERTa model)"),ayo=l(),z1=a("li"),Zte=a("strong"),nyo=o("roformer"),syo=o(" \u2014 "),eD=a("a"),lyo=o("RoFormerForMultipleChoice"),iyo=o(" (RoFormer model)"),dyo=l(),W1=a("li"),eae=a("strong"),cyo=o("squeezebert"),fyo=o(" \u2014 "),oD=a("a"),myo=o("SqueezeBertForMultipleChoice"),gyo=o(" (SqueezeBERT model)"),hyo=l(),Q1=a("li"),oae=a("strong"),pyo=o("xlm"),_yo=o(" \u2014 "),rD=a("a"),uyo=o("XLMForMultipleChoice"),byo=o(" (XLM model)"),vyo=l(),H1=a("li"),rae=a("strong"),Tyo=o("xlm-roberta"),Fyo=o(" \u2014 "),tD=a("a"),Cyo=o("XLMRobertaForMultipleChoice"),Myo=o(" (XLM-RoBERTa model)"),Eyo=l(),U1=a("li"),tae=a("strong"),yyo=o("xlm-roberta-xl"),wyo=o(" \u2014 "),aD=a("a"),Ayo=o("XLMRobertaXLForMultipleChoice"),Lyo=o(" (XLM-RoBERTa-XL model)"),Byo=l(),J1=a("li"),aae=a("strong"),xyo=o("xlnet"),kyo=o(" \u2014 "),nD=a("a"),Ryo=o("XLNetForMultipleChoice"),Syo=o(" (XLNet model)"),Pyo=l(),Y1=a("li"),nae=a("strong"),$yo=o("yoso"),Iyo=o(" \u2014 "),sD=a("a"),Dyo=o("YosoForMultipleChoice"),jyo=o(" (YOSO model)"),Nyo=l(),K1=a("p"),qyo=o("The model is set in evaluation mode by default using "),sae=a("code"),Gyo=o("model.eval()"),Oyo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lae=a("code"),Xyo=o("model.train()"),Vyo=l(),iae=a("p"),zyo=o("Examples:"),Wyo=l(),f(t3.$$.fragment),wBe=l(),dd=a("h2"),Z1=a("a"),dae=a("span"),f(a3.$$.fragment),Qyo=l(),cae=a("span"),Hyo=o("AutoModelForNextSentencePrediction"),ABe=l(),rr=a("div"),f(n3.$$.fragment),Uyo=l(),cd=a("p"),Jyo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),fae=a("code"),Yyo=o("from_pretrained()"),Kyo=o("class method or the "),mae=a("code"),Zyo=o("from_config()"),ewo=o(`class
method.`),owo=l(),s3=a("p"),rwo=o("This class cannot be instantiated directly using "),gae=a("code"),two=o("__init__()"),awo=o(" (throws an error)."),nwo=l(),Ur=a("div"),f(l3.$$.fragment),swo=l(),hae=a("p"),lwo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),iwo=l(),fd=a("p"),dwo=o(`Note:
Loading a model from its configuration file does `),pae=a("strong"),cwo=o("not"),fwo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ae=a("code"),mwo=o("from_pretrained()"),gwo=o("to load the model weights."),hwo=l(),uae=a("p"),pwo=o("Examples:"),_wo=l(),f(i3.$$.fragment),uwo=l(),Ve=a("div"),f(d3.$$.fragment),bwo=l(),bae=a("p"),vwo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Two=l(),Wa=a("p"),Fwo=o("The model class to instantiate is selected based on the "),vae=a("code"),Cwo=o("model_type"),Mwo=o(` property of the config object (either
passed as an argument or loaded from `),Tae=a("code"),Ewo=o("pretrained_model_name_or_path"),ywo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fae=a("code"),wwo=o("pretrained_model_name_or_path"),Awo=o(":"),Lwo=l(),na=a("ul"),eb=a("li"),Cae=a("strong"),Bwo=o("bert"),xwo=o(" \u2014 "),lD=a("a"),kwo=o("BertForNextSentencePrediction"),Rwo=o(" (BERT model)"),Swo=l(),ob=a("li"),Mae=a("strong"),Pwo=o("fnet"),$wo=o(" \u2014 "),iD=a("a"),Iwo=o("FNetForNextSentencePrediction"),Dwo=o(" (FNet model)"),jwo=l(),rb=a("li"),Eae=a("strong"),Nwo=o("megatron-bert"),qwo=o(" \u2014 "),dD=a("a"),Gwo=o("MegatronBertForNextSentencePrediction"),Owo=o(" (MegatronBert model)"),Xwo=l(),tb=a("li"),yae=a("strong"),Vwo=o("mobilebert"),zwo=o(" \u2014 "),cD=a("a"),Wwo=o("MobileBertForNextSentencePrediction"),Qwo=o(" (MobileBERT model)"),Hwo=l(),ab=a("li"),wae=a("strong"),Uwo=o("qdqbert"),Jwo=o(" \u2014 "),fD=a("a"),Ywo=o("QDQBertForNextSentencePrediction"),Kwo=o(" (QDQBert model)"),Zwo=l(),nb=a("p"),e6o=o("The model is set in evaluation mode by default using "),Aae=a("code"),o6o=o("model.eval()"),r6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Lae=a("code"),t6o=o("model.train()"),a6o=l(),Bae=a("p"),n6o=o("Examples:"),s6o=l(),f(c3.$$.fragment),LBe=l(),md=a("h2"),sb=a("a"),xae=a("span"),f(f3.$$.fragment),l6o=l(),kae=a("span"),i6o=o("AutoModelForTokenClassification"),BBe=l(),tr=a("div"),f(m3.$$.fragment),d6o=l(),gd=a("p"),c6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Rae=a("code"),f6o=o("from_pretrained()"),m6o=o("class method or the "),Sae=a("code"),g6o=o("from_config()"),h6o=o(`class
method.`),p6o=l(),g3=a("p"),_6o=o("This class cannot be instantiated directly using "),Pae=a("code"),u6o=o("__init__()"),b6o=o(" (throws an error)."),v6o=l(),Jr=a("div"),f(h3.$$.fragment),T6o=l(),$ae=a("p"),F6o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),C6o=l(),hd=a("p"),M6o=o(`Note:
Loading a model from its configuration file does `),Iae=a("strong"),E6o=o("not"),y6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dae=a("code"),w6o=o("from_pretrained()"),A6o=o("to load the model weights."),L6o=l(),jae=a("p"),B6o=o("Examples:"),x6o=l(),f(p3.$$.fragment),k6o=l(),ze=a("div"),f(_3.$$.fragment),R6o=l(),Nae=a("p"),S6o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),P6o=l(),Qa=a("p"),$6o=o("The model class to instantiate is selected based on the "),qae=a("code"),I6o=o("model_type"),D6o=o(` property of the config object (either
passed as an argument or loaded from `),Gae=a("code"),j6o=o("pretrained_model_name_or_path"),N6o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Oae=a("code"),q6o=o("pretrained_model_name_or_path"),G6o=o(":"),O6o=l(),N=a("ul"),lb=a("li"),Xae=a("strong"),X6o=o("albert"),V6o=o(" \u2014 "),mD=a("a"),z6o=o("AlbertForTokenClassification"),W6o=o(" (ALBERT model)"),Q6o=l(),ib=a("li"),Vae=a("strong"),H6o=o("bert"),U6o=o(" \u2014 "),gD=a("a"),J6o=o("BertForTokenClassification"),Y6o=o(" (BERT model)"),K6o=l(),db=a("li"),zae=a("strong"),Z6o=o("big_bird"),eAo=o(" \u2014 "),hD=a("a"),oAo=o("BigBirdForTokenClassification"),rAo=o(" (BigBird model)"),tAo=l(),cb=a("li"),Wae=a("strong"),aAo=o("camembert"),nAo=o(" \u2014 "),pD=a("a"),sAo=o("CamembertForTokenClassification"),lAo=o(" (CamemBERT model)"),iAo=l(),fb=a("li"),Qae=a("strong"),dAo=o("canine"),cAo=o(" \u2014 "),_D=a("a"),fAo=o("CanineForTokenClassification"),mAo=o(" (Canine model)"),gAo=l(),mb=a("li"),Hae=a("strong"),hAo=o("convbert"),pAo=o(" \u2014 "),uD=a("a"),_Ao=o("ConvBertForTokenClassification"),uAo=o(" (ConvBERT model)"),bAo=l(),gb=a("li"),Uae=a("strong"),vAo=o("data2vec-text"),TAo=o(" \u2014 "),bD=a("a"),FAo=o("Data2VecTextForTokenClassification"),CAo=o(" (Data2VecText model)"),MAo=l(),hb=a("li"),Jae=a("strong"),EAo=o("deberta"),yAo=o(" \u2014 "),vD=a("a"),wAo=o("DebertaForTokenClassification"),AAo=o(" (DeBERTa model)"),LAo=l(),pb=a("li"),Yae=a("strong"),BAo=o("deberta-v2"),xAo=o(" \u2014 "),TD=a("a"),kAo=o("DebertaV2ForTokenClassification"),RAo=o(" (DeBERTa-v2 model)"),SAo=l(),_b=a("li"),Kae=a("strong"),PAo=o("distilbert"),$Ao=o(" \u2014 "),FD=a("a"),IAo=o("DistilBertForTokenClassification"),DAo=o(" (DistilBERT model)"),jAo=l(),ub=a("li"),Zae=a("strong"),NAo=o("electra"),qAo=o(" \u2014 "),CD=a("a"),GAo=o("ElectraForTokenClassification"),OAo=o(" (ELECTRA model)"),XAo=l(),bb=a("li"),ene=a("strong"),VAo=o("flaubert"),zAo=o(" \u2014 "),MD=a("a"),WAo=o("FlaubertForTokenClassification"),QAo=o(" (FlauBERT model)"),HAo=l(),vb=a("li"),one=a("strong"),UAo=o("fnet"),JAo=o(" \u2014 "),ED=a("a"),YAo=o("FNetForTokenClassification"),KAo=o(" (FNet model)"),ZAo=l(),Tb=a("li"),rne=a("strong"),eLo=o("funnel"),oLo=o(" \u2014 "),yD=a("a"),rLo=o("FunnelForTokenClassification"),tLo=o(" (Funnel Transformer model)"),aLo=l(),Fb=a("li"),tne=a("strong"),nLo=o("gpt2"),sLo=o(" \u2014 "),wD=a("a"),lLo=o("GPT2ForTokenClassification"),iLo=o(" (OpenAI GPT-2 model)"),dLo=l(),Cb=a("li"),ane=a("strong"),cLo=o("ibert"),fLo=o(" \u2014 "),AD=a("a"),mLo=o("IBertForTokenClassification"),gLo=o(" (I-BERT model)"),hLo=l(),Mb=a("li"),nne=a("strong"),pLo=o("layoutlm"),_Lo=o(" \u2014 "),LD=a("a"),uLo=o("LayoutLMForTokenClassification"),bLo=o(" (LayoutLM model)"),vLo=l(),Eb=a("li"),sne=a("strong"),TLo=o("layoutlmv2"),FLo=o(" \u2014 "),BD=a("a"),CLo=o("LayoutLMv2ForTokenClassification"),MLo=o(" (LayoutLMv2 model)"),ELo=l(),yb=a("li"),lne=a("strong"),yLo=o("longformer"),wLo=o(" \u2014 "),xD=a("a"),ALo=o("LongformerForTokenClassification"),LLo=o(" (Longformer model)"),BLo=l(),wb=a("li"),ine=a("strong"),xLo=o("megatron-bert"),kLo=o(" \u2014 "),kD=a("a"),RLo=o("MegatronBertForTokenClassification"),SLo=o(" (MegatronBert model)"),PLo=l(),Ab=a("li"),dne=a("strong"),$Lo=o("mobilebert"),ILo=o(" \u2014 "),RD=a("a"),DLo=o("MobileBertForTokenClassification"),jLo=o(" (MobileBERT model)"),NLo=l(),Lb=a("li"),cne=a("strong"),qLo=o("mpnet"),GLo=o(" \u2014 "),SD=a("a"),OLo=o("MPNetForTokenClassification"),XLo=o(" (MPNet model)"),VLo=l(),Bb=a("li"),fne=a("strong"),zLo=o("nystromformer"),WLo=o(" \u2014 "),PD=a("a"),QLo=o("NystromformerForTokenClassification"),HLo=o(" (Nystromformer model)"),ULo=l(),xb=a("li"),mne=a("strong"),JLo=o("qdqbert"),YLo=o(" \u2014 "),$D=a("a"),KLo=o("QDQBertForTokenClassification"),ZLo=o(" (QDQBert model)"),e8o=l(),kb=a("li"),gne=a("strong"),o8o=o("rembert"),r8o=o(" \u2014 "),ID=a("a"),t8o=o("RemBertForTokenClassification"),a8o=o(" (RemBERT model)"),n8o=l(),Rb=a("li"),hne=a("strong"),s8o=o("roberta"),l8o=o(" \u2014 "),DD=a("a"),i8o=o("RobertaForTokenClassification"),d8o=o(" (RoBERTa model)"),c8o=l(),Sb=a("li"),pne=a("strong"),f8o=o("roformer"),m8o=o(" \u2014 "),jD=a("a"),g8o=o("RoFormerForTokenClassification"),h8o=o(" (RoFormer model)"),p8o=l(),Pb=a("li"),_ne=a("strong"),_8o=o("squeezebert"),u8o=o(" \u2014 "),ND=a("a"),b8o=o("SqueezeBertForTokenClassification"),v8o=o(" (SqueezeBERT model)"),T8o=l(),$b=a("li"),une=a("strong"),F8o=o("xlm"),C8o=o(" \u2014 "),qD=a("a"),M8o=o("XLMForTokenClassification"),E8o=o(" (XLM model)"),y8o=l(),Ib=a("li"),bne=a("strong"),w8o=o("xlm-roberta"),A8o=o(" \u2014 "),GD=a("a"),L8o=o("XLMRobertaForTokenClassification"),B8o=o(" (XLM-RoBERTa model)"),x8o=l(),Db=a("li"),vne=a("strong"),k8o=o("xlm-roberta-xl"),R8o=o(" \u2014 "),OD=a("a"),S8o=o("XLMRobertaXLForTokenClassification"),P8o=o(" (XLM-RoBERTa-XL model)"),$8o=l(),jb=a("li"),Tne=a("strong"),I8o=o("xlnet"),D8o=o(" \u2014 "),XD=a("a"),j8o=o("XLNetForTokenClassification"),N8o=o(" (XLNet model)"),q8o=l(),Nb=a("li"),Fne=a("strong"),G8o=o("yoso"),O8o=o(" \u2014 "),VD=a("a"),X8o=o("YosoForTokenClassification"),V8o=o(" (YOSO model)"),z8o=l(),qb=a("p"),W8o=o("The model is set in evaluation mode by default using "),Cne=a("code"),Q8o=o("model.eval()"),H8o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Mne=a("code"),U8o=o("model.train()"),J8o=l(),Ene=a("p"),Y8o=o("Examples:"),K8o=l(),f(u3.$$.fragment),xBe=l(),pd=a("h2"),Gb=a("a"),yne=a("span"),f(b3.$$.fragment),Z8o=l(),wne=a("span"),e7o=o("AutoModelForQuestionAnswering"),kBe=l(),ar=a("div"),f(v3.$$.fragment),o7o=l(),_d=a("p"),r7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Ane=a("code"),t7o=o("from_pretrained()"),a7o=o("class method or the "),Lne=a("code"),n7o=o("from_config()"),s7o=o(`class
method.`),l7o=l(),T3=a("p"),i7o=o("This class cannot be instantiated directly using "),Bne=a("code"),d7o=o("__init__()"),c7o=o(" (throws an error)."),f7o=l(),Yr=a("div"),f(F3.$$.fragment),m7o=l(),xne=a("p"),g7o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),h7o=l(),ud=a("p"),p7o=o(`Note:
Loading a model from its configuration file does `),kne=a("strong"),_7o=o("not"),u7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Rne=a("code"),b7o=o("from_pretrained()"),v7o=o("to load the model weights."),T7o=l(),Sne=a("p"),F7o=o("Examples:"),C7o=l(),f(C3.$$.fragment),M7o=l(),We=a("div"),f(M3.$$.fragment),E7o=l(),Pne=a("p"),y7o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),w7o=l(),Ha=a("p"),A7o=o("The model class to instantiate is selected based on the "),$ne=a("code"),L7o=o("model_type"),B7o=o(` property of the config object (either
passed as an argument or loaded from `),Ine=a("code"),x7o=o("pretrained_model_name_or_path"),k7o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dne=a("code"),R7o=o("pretrained_model_name_or_path"),S7o=o(":"),P7o=l(),R=a("ul"),Ob=a("li"),jne=a("strong"),$7o=o("albert"),I7o=o(" \u2014 "),zD=a("a"),D7o=o("AlbertForQuestionAnswering"),j7o=o(" (ALBERT model)"),N7o=l(),Xb=a("li"),Nne=a("strong"),q7o=o("bart"),G7o=o(" \u2014 "),WD=a("a"),O7o=o("BartForQuestionAnswering"),X7o=o(" (BART model)"),V7o=l(),Vb=a("li"),qne=a("strong"),z7o=o("bert"),W7o=o(" \u2014 "),QD=a("a"),Q7o=o("BertForQuestionAnswering"),H7o=o(" (BERT model)"),U7o=l(),zb=a("li"),Gne=a("strong"),J7o=o("big_bird"),Y7o=o(" \u2014 "),HD=a("a"),K7o=o("BigBirdForQuestionAnswering"),Z7o=o(" (BigBird model)"),eBo=l(),Wb=a("li"),One=a("strong"),oBo=o("bigbird_pegasus"),rBo=o(" \u2014 "),UD=a("a"),tBo=o("BigBirdPegasusForQuestionAnswering"),aBo=o(" (BigBirdPegasus model)"),nBo=l(),Qb=a("li"),Xne=a("strong"),sBo=o("camembert"),lBo=o(" \u2014 "),JD=a("a"),iBo=o("CamembertForQuestionAnswering"),dBo=o(" (CamemBERT model)"),cBo=l(),Hb=a("li"),Vne=a("strong"),fBo=o("canine"),mBo=o(" \u2014 "),YD=a("a"),gBo=o("CanineForQuestionAnswering"),hBo=o(" (Canine model)"),pBo=l(),Ub=a("li"),zne=a("strong"),_Bo=o("convbert"),uBo=o(" \u2014 "),KD=a("a"),bBo=o("ConvBertForQuestionAnswering"),vBo=o(" (ConvBERT model)"),TBo=l(),Jb=a("li"),Wne=a("strong"),FBo=o("data2vec-text"),CBo=o(" \u2014 "),ZD=a("a"),MBo=o("Data2VecTextForQuestionAnswering"),EBo=o(" (Data2VecText model)"),yBo=l(),Yb=a("li"),Qne=a("strong"),wBo=o("deberta"),ABo=o(" \u2014 "),ej=a("a"),LBo=o("DebertaForQuestionAnswering"),BBo=o(" (DeBERTa model)"),xBo=l(),Kb=a("li"),Hne=a("strong"),kBo=o("deberta-v2"),RBo=o(" \u2014 "),oj=a("a"),SBo=o("DebertaV2ForQuestionAnswering"),PBo=o(" (DeBERTa-v2 model)"),$Bo=l(),Zb=a("li"),Une=a("strong"),IBo=o("distilbert"),DBo=o(" \u2014 "),rj=a("a"),jBo=o("DistilBertForQuestionAnswering"),NBo=o(" (DistilBERT model)"),qBo=l(),e5=a("li"),Jne=a("strong"),GBo=o("electra"),OBo=o(" \u2014 "),tj=a("a"),XBo=o("ElectraForQuestionAnswering"),VBo=o(" (ELECTRA model)"),zBo=l(),o5=a("li"),Yne=a("strong"),WBo=o("flaubert"),QBo=o(" \u2014 "),aj=a("a"),HBo=o("FlaubertForQuestionAnsweringSimple"),UBo=o(" (FlauBERT model)"),JBo=l(),r5=a("li"),Kne=a("strong"),YBo=o("fnet"),KBo=o(" \u2014 "),nj=a("a"),ZBo=o("FNetForQuestionAnswering"),exo=o(" (FNet model)"),oxo=l(),t5=a("li"),Zne=a("strong"),rxo=o("funnel"),txo=o(" \u2014 "),sj=a("a"),axo=o("FunnelForQuestionAnswering"),nxo=o(" (Funnel Transformer model)"),sxo=l(),a5=a("li"),ese=a("strong"),lxo=o("gptj"),ixo=o(" \u2014 "),lj=a("a"),dxo=o("GPTJForQuestionAnswering"),cxo=o(" (GPT-J model)"),fxo=l(),n5=a("li"),ose=a("strong"),mxo=o("ibert"),gxo=o(" \u2014 "),ij=a("a"),hxo=o("IBertForQuestionAnswering"),pxo=o(" (I-BERT model)"),_xo=l(),s5=a("li"),rse=a("strong"),uxo=o("layoutlmv2"),bxo=o(" \u2014 "),dj=a("a"),vxo=o("LayoutLMv2ForQuestionAnswering"),Txo=o(" (LayoutLMv2 model)"),Fxo=l(),l5=a("li"),tse=a("strong"),Cxo=o("led"),Mxo=o(" \u2014 "),cj=a("a"),Exo=o("LEDForQuestionAnswering"),yxo=o(" (LED model)"),wxo=l(),i5=a("li"),ase=a("strong"),Axo=o("longformer"),Lxo=o(" \u2014 "),fj=a("a"),Bxo=o("LongformerForQuestionAnswering"),xxo=o(" (Longformer model)"),kxo=l(),d5=a("li"),nse=a("strong"),Rxo=o("lxmert"),Sxo=o(" \u2014 "),mj=a("a"),Pxo=o("LxmertForQuestionAnswering"),$xo=o(" (LXMERT model)"),Ixo=l(),c5=a("li"),sse=a("strong"),Dxo=o("mbart"),jxo=o(" \u2014 "),gj=a("a"),Nxo=o("MBartForQuestionAnswering"),qxo=o(" (mBART model)"),Gxo=l(),f5=a("li"),lse=a("strong"),Oxo=o("megatron-bert"),Xxo=o(" \u2014 "),hj=a("a"),Vxo=o("MegatronBertForQuestionAnswering"),zxo=o(" (MegatronBert model)"),Wxo=l(),m5=a("li"),ise=a("strong"),Qxo=o("mobilebert"),Hxo=o(" \u2014 "),pj=a("a"),Uxo=o("MobileBertForQuestionAnswering"),Jxo=o(" (MobileBERT model)"),Yxo=l(),g5=a("li"),dse=a("strong"),Kxo=o("mpnet"),Zxo=o(" \u2014 "),_j=a("a"),eko=o("MPNetForQuestionAnswering"),oko=o(" (MPNet model)"),rko=l(),h5=a("li"),cse=a("strong"),tko=o("nystromformer"),ako=o(" \u2014 "),uj=a("a"),nko=o("NystromformerForQuestionAnswering"),sko=o(" (Nystromformer model)"),lko=l(),p5=a("li"),fse=a("strong"),iko=o("qdqbert"),dko=o(" \u2014 "),bj=a("a"),cko=o("QDQBertForQuestionAnswering"),fko=o(" (QDQBert model)"),mko=l(),_5=a("li"),mse=a("strong"),gko=o("reformer"),hko=o(" \u2014 "),vj=a("a"),pko=o("ReformerForQuestionAnswering"),_ko=o(" (Reformer model)"),uko=l(),u5=a("li"),gse=a("strong"),bko=o("rembert"),vko=o(" \u2014 "),Tj=a("a"),Tko=o("RemBertForQuestionAnswering"),Fko=o(" (RemBERT model)"),Cko=l(),b5=a("li"),hse=a("strong"),Mko=o("roberta"),Eko=o(" \u2014 "),Fj=a("a"),yko=o("RobertaForQuestionAnswering"),wko=o(" (RoBERTa model)"),Ako=l(),v5=a("li"),pse=a("strong"),Lko=o("roformer"),Bko=o(" \u2014 "),Cj=a("a"),xko=o("RoFormerForQuestionAnswering"),kko=o(" (RoFormer model)"),Rko=l(),T5=a("li"),_se=a("strong"),Sko=o("splinter"),Pko=o(" \u2014 "),Mj=a("a"),$ko=o("SplinterForQuestionAnswering"),Iko=o(" (Splinter model)"),Dko=l(),F5=a("li"),use=a("strong"),jko=o("squeezebert"),Nko=o(" \u2014 "),Ej=a("a"),qko=o("SqueezeBertForQuestionAnswering"),Gko=o(" (SqueezeBERT model)"),Oko=l(),C5=a("li"),bse=a("strong"),Xko=o("xlm"),Vko=o(" \u2014 "),yj=a("a"),zko=o("XLMForQuestionAnsweringSimple"),Wko=o(" (XLM model)"),Qko=l(),M5=a("li"),vse=a("strong"),Hko=o("xlm-roberta"),Uko=o(" \u2014 "),wj=a("a"),Jko=o("XLMRobertaForQuestionAnswering"),Yko=o(" (XLM-RoBERTa model)"),Kko=l(),E5=a("li"),Tse=a("strong"),Zko=o("xlm-roberta-xl"),eRo=o(" \u2014 "),Aj=a("a"),oRo=o("XLMRobertaXLForQuestionAnswering"),rRo=o(" (XLM-RoBERTa-XL model)"),tRo=l(),y5=a("li"),Fse=a("strong"),aRo=o("xlnet"),nRo=o(" \u2014 "),Lj=a("a"),sRo=o("XLNetForQuestionAnsweringSimple"),lRo=o(" (XLNet model)"),iRo=l(),w5=a("li"),Cse=a("strong"),dRo=o("yoso"),cRo=o(" \u2014 "),Bj=a("a"),fRo=o("YosoForQuestionAnswering"),mRo=o(" (YOSO model)"),gRo=l(),A5=a("p"),hRo=o("The model is set in evaluation mode by default using "),Mse=a("code"),pRo=o("model.eval()"),_Ro=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ese=a("code"),uRo=o("model.train()"),bRo=l(),yse=a("p"),vRo=o("Examples:"),TRo=l(),f(E3.$$.fragment),RBe=l(),bd=a("h2"),L5=a("a"),wse=a("span"),f(y3.$$.fragment),FRo=l(),Ase=a("span"),CRo=o("AutoModelForTableQuestionAnswering"),SBe=l(),nr=a("div"),f(w3.$$.fragment),MRo=l(),vd=a("p"),ERo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Lse=a("code"),yRo=o("from_pretrained()"),wRo=o("class method or the "),Bse=a("code"),ARo=o("from_config()"),LRo=o(`class
method.`),BRo=l(),A3=a("p"),xRo=o("This class cannot be instantiated directly using "),xse=a("code"),kRo=o("__init__()"),RRo=o(" (throws an error)."),SRo=l(),Kr=a("div"),f(L3.$$.fragment),PRo=l(),kse=a("p"),$Ro=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),IRo=l(),Td=a("p"),DRo=o(`Note:
Loading a model from its configuration file does `),Rse=a("strong"),jRo=o("not"),NRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sse=a("code"),qRo=o("from_pretrained()"),GRo=o("to load the model weights."),ORo=l(),Pse=a("p"),XRo=o("Examples:"),VRo=l(),f(B3.$$.fragment),zRo=l(),Qe=a("div"),f(x3.$$.fragment),WRo=l(),$se=a("p"),QRo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),HRo=l(),Ua=a("p"),URo=o("The model class to instantiate is selected based on the "),Ise=a("code"),JRo=o("model_type"),YRo=o(` property of the config object (either
passed as an argument or loaded from `),Dse=a("code"),KRo=o("pretrained_model_name_or_path"),ZRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jse=a("code"),eSo=o("pretrained_model_name_or_path"),oSo=o(":"),rSo=l(),Nse=a("ul"),B5=a("li"),qse=a("strong"),tSo=o("tapas"),aSo=o(" \u2014 "),xj=a("a"),nSo=o("TapasForQuestionAnswering"),sSo=o(" (TAPAS model)"),lSo=l(),x5=a("p"),iSo=o("The model is set in evaluation mode by default using "),Gse=a("code"),dSo=o("model.eval()"),cSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ose=a("code"),fSo=o("model.train()"),mSo=l(),Xse=a("p"),gSo=o("Examples:"),hSo=l(),f(k3.$$.fragment),PBe=l(),Fd=a("h2"),k5=a("a"),Vse=a("span"),f(R3.$$.fragment),pSo=l(),zse=a("span"),_So=o("AutoModelForImageClassification"),$Be=l(),sr=a("div"),f(S3.$$.fragment),uSo=l(),Cd=a("p"),bSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Wse=a("code"),vSo=o("from_pretrained()"),TSo=o("class method or the "),Qse=a("code"),FSo=o("from_config()"),CSo=o(`class
method.`),MSo=l(),P3=a("p"),ESo=o("This class cannot be instantiated directly using "),Hse=a("code"),ySo=o("__init__()"),wSo=o(" (throws an error)."),ASo=l(),Zr=a("div"),f($3.$$.fragment),LSo=l(),Use=a("p"),BSo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),xSo=l(),Md=a("p"),kSo=o(`Note:
Loading a model from its configuration file does `),Jse=a("strong"),RSo=o("not"),SSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yse=a("code"),PSo=o("from_pretrained()"),$So=o("to load the model weights."),ISo=l(),Kse=a("p"),DSo=o("Examples:"),jSo=l(),f(I3.$$.fragment),NSo=l(),He=a("div"),f(D3.$$.fragment),qSo=l(),Zse=a("p"),GSo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),OSo=l(),Ja=a("p"),XSo=o("The model class to instantiate is selected based on the "),ele=a("code"),VSo=o("model_type"),zSo=o(` property of the config object (either
passed as an argument or loaded from `),ole=a("code"),WSo=o("pretrained_model_name_or_path"),QSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rle=a("code"),HSo=o("pretrained_model_name_or_path"),USo=o(":"),JSo=l(),Fe=a("ul"),R5=a("li"),tle=a("strong"),YSo=o("beit"),KSo=o(" \u2014 "),kj=a("a"),ZSo=o("BeitForImageClassification"),ePo=o(" (BEiT model)"),oPo=l(),S5=a("li"),ale=a("strong"),rPo=o("convnext"),tPo=o(" \u2014 "),Rj=a("a"),aPo=o("ConvNextForImageClassification"),nPo=o(" (ConvNext model)"),sPo=l(),Ps=a("li"),nle=a("strong"),lPo=o("deit"),iPo=o(" \u2014 "),Sj=a("a"),dPo=o("DeiTForImageClassification"),cPo=o(" or "),Pj=a("a"),fPo=o("DeiTForImageClassificationWithTeacher"),mPo=o(" (DeiT model)"),gPo=l(),P5=a("li"),sle=a("strong"),hPo=o("imagegpt"),pPo=o(" \u2014 "),$j=a("a"),_Po=o("ImageGPTForImageClassification"),uPo=o(" (ImageGPT model)"),bPo=l(),la=a("li"),lle=a("strong"),vPo=o("perceiver"),TPo=o(" \u2014 "),Ij=a("a"),FPo=o("PerceiverForImageClassificationLearned"),CPo=o(" or "),Dj=a("a"),MPo=o("PerceiverForImageClassificationFourier"),EPo=o(" or "),jj=a("a"),yPo=o("PerceiverForImageClassificationConvProcessing"),wPo=o(" (Perceiver model)"),APo=l(),$5=a("li"),ile=a("strong"),LPo=o("poolformer"),BPo=o(" \u2014 "),Nj=a("a"),xPo=o("PoolFormerForImageClassification"),kPo=o(" (PoolFormer model)"),RPo=l(),I5=a("li"),dle=a("strong"),SPo=o("segformer"),PPo=o(" \u2014 "),qj=a("a"),$Po=o("SegformerForImageClassification"),IPo=o(" (SegFormer model)"),DPo=l(),D5=a("li"),cle=a("strong"),jPo=o("swin"),NPo=o(" \u2014 "),Gj=a("a"),qPo=o("SwinForImageClassification"),GPo=o(" (Swin model)"),OPo=l(),j5=a("li"),fle=a("strong"),XPo=o("vit"),VPo=o(" \u2014 "),Oj=a("a"),zPo=o("ViTForImageClassification"),WPo=o(" (ViT model)"),QPo=l(),N5=a("p"),HPo=o("The model is set in evaluation mode by default using "),mle=a("code"),UPo=o("model.eval()"),JPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gle=a("code"),YPo=o("model.train()"),KPo=l(),hle=a("p"),ZPo=o("Examples:"),e$o=l(),f(j3.$$.fragment),IBe=l(),Ed=a("h2"),q5=a("a"),ple=a("span"),f(N3.$$.fragment),o$o=l(),_le=a("span"),r$o=o("AutoModelForVision2Seq"),DBe=l(),lr=a("div"),f(q3.$$.fragment),t$o=l(),yd=a("p"),a$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ule=a("code"),n$o=o("from_pretrained()"),s$o=o("class method or the "),ble=a("code"),l$o=o("from_config()"),i$o=o(`class
method.`),d$o=l(),G3=a("p"),c$o=o("This class cannot be instantiated directly using "),vle=a("code"),f$o=o("__init__()"),m$o=o(" (throws an error)."),g$o=l(),et=a("div"),f(O3.$$.fragment),h$o=l(),Tle=a("p"),p$o=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),_$o=l(),wd=a("p"),u$o=o(`Note:
Loading a model from its configuration file does `),Fle=a("strong"),b$o=o("not"),v$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cle=a("code"),T$o=o("from_pretrained()"),F$o=o("to load the model weights."),C$o=l(),Mle=a("p"),M$o=o("Examples:"),E$o=l(),f(X3.$$.fragment),y$o=l(),Ue=a("div"),f(V3.$$.fragment),w$o=l(),Ele=a("p"),A$o=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),L$o=l(),Ya=a("p"),B$o=o("The model class to instantiate is selected based on the "),yle=a("code"),x$o=o("model_type"),k$o=o(` property of the config object (either
passed as an argument or loaded from `),wle=a("code"),R$o=o("pretrained_model_name_or_path"),S$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ale=a("code"),P$o=o("pretrained_model_name_or_path"),$$o=o(":"),I$o=l(),Lle=a("ul"),G5=a("li"),Ble=a("strong"),D$o=o("vision-encoder-decoder"),j$o=o(" \u2014 "),Xj=a("a"),N$o=o("VisionEncoderDecoderModel"),q$o=o(" (Vision Encoder decoder model)"),G$o=l(),O5=a("p"),O$o=o("The model is set in evaluation mode by default using "),xle=a("code"),X$o=o("model.eval()"),V$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kle=a("code"),z$o=o("model.train()"),W$o=l(),Rle=a("p"),Q$o=o("Examples:"),H$o=l(),f(z3.$$.fragment),jBe=l(),Ad=a("h2"),X5=a("a"),Sle=a("span"),f(W3.$$.fragment),U$o=l(),Ple=a("span"),J$o=o("AutoModelForAudioClassification"),NBe=l(),ir=a("div"),f(Q3.$$.fragment),Y$o=l(),Ld=a("p"),K$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),$le=a("code"),Z$o=o("from_pretrained()"),eIo=o("class method or the "),Ile=a("code"),oIo=o("from_config()"),rIo=o(`class
method.`),tIo=l(),H3=a("p"),aIo=o("This class cannot be instantiated directly using "),Dle=a("code"),nIo=o("__init__()"),sIo=o(" (throws an error)."),lIo=l(),ot=a("div"),f(U3.$$.fragment),iIo=l(),jle=a("p"),dIo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),cIo=l(),Bd=a("p"),fIo=o(`Note:
Loading a model from its configuration file does `),Nle=a("strong"),mIo=o("not"),gIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qle=a("code"),hIo=o("from_pretrained()"),pIo=o("to load the model weights."),_Io=l(),Gle=a("p"),uIo=o("Examples:"),bIo=l(),f(J3.$$.fragment),vIo=l(),Je=a("div"),f(Y3.$$.fragment),TIo=l(),Ole=a("p"),FIo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),CIo=l(),Ka=a("p"),MIo=o("The model class to instantiate is selected based on the "),Xle=a("code"),EIo=o("model_type"),yIo=o(` property of the config object (either
passed as an argument or loaded from `),Vle=a("code"),wIo=o("pretrained_model_name_or_path"),AIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zle=a("code"),LIo=o("pretrained_model_name_or_path"),BIo=o(":"),xIo=l(),xe=a("ul"),V5=a("li"),Wle=a("strong"),kIo=o("data2vec-audio"),RIo=o(" \u2014 "),Vj=a("a"),SIo=o("Data2VecAudioForSequenceClassification"),PIo=o(" (Data2VecAudio model)"),$Io=l(),z5=a("li"),Qle=a("strong"),IIo=o("hubert"),DIo=o(" \u2014 "),zj=a("a"),jIo=o("HubertForSequenceClassification"),NIo=o(" (Hubert model)"),qIo=l(),W5=a("li"),Hle=a("strong"),GIo=o("sew"),OIo=o(" \u2014 "),Wj=a("a"),XIo=o("SEWForSequenceClassification"),VIo=o(" (SEW model)"),zIo=l(),Q5=a("li"),Ule=a("strong"),WIo=o("sew-d"),QIo=o(" \u2014 "),Qj=a("a"),HIo=o("SEWDForSequenceClassification"),UIo=o(" (SEW-D model)"),JIo=l(),H5=a("li"),Jle=a("strong"),YIo=o("unispeech"),KIo=o(" \u2014 "),Hj=a("a"),ZIo=o("UniSpeechForSequenceClassification"),eDo=o(" (UniSpeech model)"),oDo=l(),U5=a("li"),Yle=a("strong"),rDo=o("unispeech-sat"),tDo=o(" \u2014 "),Uj=a("a"),aDo=o("UniSpeechSatForSequenceClassification"),nDo=o(" (UniSpeechSat model)"),sDo=l(),J5=a("li"),Kle=a("strong"),lDo=o("wav2vec2"),iDo=o(" \u2014 "),Jj=a("a"),dDo=o("Wav2Vec2ForSequenceClassification"),cDo=o(" (Wav2Vec2 model)"),fDo=l(),Y5=a("li"),Zle=a("strong"),mDo=o("wavlm"),gDo=o(" \u2014 "),Yj=a("a"),hDo=o("WavLMForSequenceClassification"),pDo=o(" (WavLM model)"),_Do=l(),K5=a("p"),uDo=o("The model is set in evaluation mode by default using "),eie=a("code"),bDo=o("model.eval()"),vDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=a("code"),TDo=o("model.train()"),FDo=l(),rie=a("p"),CDo=o("Examples:"),MDo=l(),f(K3.$$.fragment),qBe=l(),xd=a("h2"),Z5=a("a"),tie=a("span"),f(Z3.$$.fragment),EDo=l(),aie=a("span"),yDo=o("AutoModelForAudioFrameClassification"),GBe=l(),dr=a("div"),f(ey.$$.fragment),wDo=l(),kd=a("p"),ADo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),nie=a("code"),LDo=o("from_pretrained()"),BDo=o("class method or the "),sie=a("code"),xDo=o("from_config()"),kDo=o(`class
method.`),RDo=l(),oy=a("p"),SDo=o("This class cannot be instantiated directly using "),lie=a("code"),PDo=o("__init__()"),$Do=o(" (throws an error)."),IDo=l(),rt=a("div"),f(ry.$$.fragment),DDo=l(),iie=a("p"),jDo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),NDo=l(),Rd=a("p"),qDo=o(`Note:
Loading a model from its configuration file does `),die=a("strong"),GDo=o("not"),ODo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=a("code"),XDo=o("from_pretrained()"),VDo=o("to load the model weights."),zDo=l(),fie=a("p"),WDo=o("Examples:"),QDo=l(),f(ty.$$.fragment),HDo=l(),Ye=a("div"),f(ay.$$.fragment),UDo=l(),mie=a("p"),JDo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),YDo=l(),Za=a("p"),KDo=o("The model class to instantiate is selected based on the "),gie=a("code"),ZDo=o("model_type"),ejo=o(` property of the config object (either
passed as an argument or loaded from `),hie=a("code"),ojo=o("pretrained_model_name_or_path"),rjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=a("code"),tjo=o("pretrained_model_name_or_path"),ajo=o(":"),njo=l(),en=a("ul"),e2=a("li"),_ie=a("strong"),sjo=o("data2vec-audio"),ljo=o(" \u2014 "),Kj=a("a"),ijo=o("Data2VecAudioForAudioFrameClassification"),djo=o(" (Data2VecAudio model)"),cjo=l(),o2=a("li"),uie=a("strong"),fjo=o("unispeech-sat"),mjo=o(" \u2014 "),Zj=a("a"),gjo=o("UniSpeechSatForAudioFrameClassification"),hjo=o(" (UniSpeechSat model)"),pjo=l(),r2=a("li"),bie=a("strong"),_jo=o("wav2vec2"),ujo=o(" \u2014 "),eN=a("a"),bjo=o("Wav2Vec2ForAudioFrameClassification"),vjo=o(" (Wav2Vec2 model)"),Tjo=l(),t2=a("li"),vie=a("strong"),Fjo=o("wavlm"),Cjo=o(" \u2014 "),oN=a("a"),Mjo=o("WavLMForAudioFrameClassification"),Ejo=o(" (WavLM model)"),yjo=l(),a2=a("p"),wjo=o("The model is set in evaluation mode by default using "),Tie=a("code"),Ajo=o("model.eval()"),Ljo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fie=a("code"),Bjo=o("model.train()"),xjo=l(),Cie=a("p"),kjo=o("Examples:"),Rjo=l(),f(ny.$$.fragment),OBe=l(),Sd=a("h2"),n2=a("a"),Mie=a("span"),f(sy.$$.fragment),Sjo=l(),Eie=a("span"),Pjo=o("AutoModelForCTC"),XBe=l(),cr=a("div"),f(ly.$$.fragment),$jo=l(),Pd=a("p"),Ijo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),yie=a("code"),Djo=o("from_pretrained()"),jjo=o("class method or the "),wie=a("code"),Njo=o("from_config()"),qjo=o(`class
method.`),Gjo=l(),iy=a("p"),Ojo=o("This class cannot be instantiated directly using "),Aie=a("code"),Xjo=o("__init__()"),Vjo=o(" (throws an error)."),zjo=l(),tt=a("div"),f(dy.$$.fragment),Wjo=l(),Lie=a("p"),Qjo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),Hjo=l(),$d=a("p"),Ujo=o(`Note:
Loading a model from its configuration file does `),Bie=a("strong"),Jjo=o("not"),Yjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xie=a("code"),Kjo=o("from_pretrained()"),Zjo=o("to load the model weights."),eNo=l(),kie=a("p"),oNo=o("Examples:"),rNo=l(),f(cy.$$.fragment),tNo=l(),Ke=a("div"),f(fy.$$.fragment),aNo=l(),Rie=a("p"),nNo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),sNo=l(),on=a("p"),lNo=o("The model class to instantiate is selected based on the "),Sie=a("code"),iNo=o("model_type"),dNo=o(` property of the config object (either
passed as an argument or loaded from `),Pie=a("code"),cNo=o("pretrained_model_name_or_path"),fNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$ie=a("code"),mNo=o("pretrained_model_name_or_path"),gNo=o(":"),hNo=l(),ke=a("ul"),s2=a("li"),Iie=a("strong"),pNo=o("data2vec-audio"),_No=o(" \u2014 "),rN=a("a"),uNo=o("Data2VecAudioForCTC"),bNo=o(" (Data2VecAudio model)"),vNo=l(),l2=a("li"),Die=a("strong"),TNo=o("hubert"),FNo=o(" \u2014 "),tN=a("a"),CNo=o("HubertForCTC"),MNo=o(" (Hubert model)"),ENo=l(),i2=a("li"),jie=a("strong"),yNo=o("sew"),wNo=o(" \u2014 "),aN=a("a"),ANo=o("SEWForCTC"),LNo=o(" (SEW model)"),BNo=l(),d2=a("li"),Nie=a("strong"),xNo=o("sew-d"),kNo=o(" \u2014 "),nN=a("a"),RNo=o("SEWDForCTC"),SNo=o(" (SEW-D model)"),PNo=l(),c2=a("li"),qie=a("strong"),$No=o("unispeech"),INo=o(" \u2014 "),sN=a("a"),DNo=o("UniSpeechForCTC"),jNo=o(" (UniSpeech model)"),NNo=l(),f2=a("li"),Gie=a("strong"),qNo=o("unispeech-sat"),GNo=o(" \u2014 "),lN=a("a"),ONo=o("UniSpeechSatForCTC"),XNo=o(" (UniSpeechSat model)"),VNo=l(),m2=a("li"),Oie=a("strong"),zNo=o("wav2vec2"),WNo=o(" \u2014 "),iN=a("a"),QNo=o("Wav2Vec2ForCTC"),HNo=o(" (Wav2Vec2 model)"),UNo=l(),g2=a("li"),Xie=a("strong"),JNo=o("wavlm"),YNo=o(" \u2014 "),dN=a("a"),KNo=o("WavLMForCTC"),ZNo=o(" (WavLM model)"),eqo=l(),h2=a("p"),oqo=o("The model is set in evaluation mode by default using "),Vie=a("code"),rqo=o("model.eval()"),tqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zie=a("code"),aqo=o("model.train()"),nqo=l(),Wie=a("p"),sqo=o("Examples:"),lqo=l(),f(my.$$.fragment),VBe=l(),Id=a("h2"),p2=a("a"),Qie=a("span"),f(gy.$$.fragment),iqo=l(),Hie=a("span"),dqo=o("AutoModelForSpeechSeq2Seq"),zBe=l(),fr=a("div"),f(hy.$$.fragment),cqo=l(),Dd=a("p"),fqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Uie=a("code"),mqo=o("from_pretrained()"),gqo=o("class method or the "),Jie=a("code"),hqo=o("from_config()"),pqo=o(`class
method.`),_qo=l(),py=a("p"),uqo=o("This class cannot be instantiated directly using "),Yie=a("code"),bqo=o("__init__()"),vqo=o(" (throws an error)."),Tqo=l(),at=a("div"),f(_y.$$.fragment),Fqo=l(),Kie=a("p"),Cqo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Mqo=l(),jd=a("p"),Eqo=o(`Note:
Loading a model from its configuration file does `),Zie=a("strong"),yqo=o("not"),wqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ede=a("code"),Aqo=o("from_pretrained()"),Lqo=o("to load the model weights."),Bqo=l(),ode=a("p"),xqo=o("Examples:"),kqo=l(),f(uy.$$.fragment),Rqo=l(),Ze=a("div"),f(by.$$.fragment),Sqo=l(),rde=a("p"),Pqo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),$qo=l(),rn=a("p"),Iqo=o("The model class to instantiate is selected based on the "),tde=a("code"),Dqo=o("model_type"),jqo=o(` property of the config object (either
passed as an argument or loaded from `),ade=a("code"),Nqo=o("pretrained_model_name_or_path"),qqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nde=a("code"),Gqo=o("pretrained_model_name_or_path"),Oqo=o(":"),Xqo=l(),vy=a("ul"),_2=a("li"),sde=a("strong"),Vqo=o("speech-encoder-decoder"),zqo=o(" \u2014 "),cN=a("a"),Wqo=o("SpeechEncoderDecoderModel"),Qqo=o(" (Speech Encoder decoder model)"),Hqo=l(),u2=a("li"),lde=a("strong"),Uqo=o("speech_to_text"),Jqo=o(" \u2014 "),fN=a("a"),Yqo=o("Speech2TextForConditionalGeneration"),Kqo=o(" (Speech2Text model)"),Zqo=l(),b2=a("p"),eGo=o("The model is set in evaluation mode by default using "),ide=a("code"),oGo=o("model.eval()"),rGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dde=a("code"),tGo=o("model.train()"),aGo=l(),cde=a("p"),nGo=o("Examples:"),sGo=l(),f(Ty.$$.fragment),WBe=l(),Nd=a("h2"),v2=a("a"),fde=a("span"),f(Fy.$$.fragment),lGo=l(),mde=a("span"),iGo=o("AutoModelForAudioXVector"),QBe=l(),mr=a("div"),f(Cy.$$.fragment),dGo=l(),qd=a("p"),cGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),gde=a("code"),fGo=o("from_pretrained()"),mGo=o("class method or the "),hde=a("code"),gGo=o("from_config()"),hGo=o(`class
method.`),pGo=l(),My=a("p"),_Go=o("This class cannot be instantiated directly using "),pde=a("code"),uGo=o("__init__()"),bGo=o(" (throws an error)."),vGo=l(),nt=a("div"),f(Ey.$$.fragment),TGo=l(),_de=a("p"),FGo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),CGo=l(),Gd=a("p"),MGo=o(`Note:
Loading a model from its configuration file does `),ude=a("strong"),EGo=o("not"),yGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bde=a("code"),wGo=o("from_pretrained()"),AGo=o("to load the model weights."),LGo=l(),vde=a("p"),BGo=o("Examples:"),xGo=l(),f(yy.$$.fragment),kGo=l(),eo=a("div"),f(wy.$$.fragment),RGo=l(),Tde=a("p"),SGo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),PGo=l(),tn=a("p"),$Go=o("The model class to instantiate is selected based on the "),Fde=a("code"),IGo=o("model_type"),DGo=o(` property of the config object (either
passed as an argument or loaded from `),Cde=a("code"),jGo=o("pretrained_model_name_or_path"),NGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mde=a("code"),qGo=o("pretrained_model_name_or_path"),GGo=o(":"),OGo=l(),an=a("ul"),T2=a("li"),Ede=a("strong"),XGo=o("data2vec-audio"),VGo=o(" \u2014 "),mN=a("a"),zGo=o("Data2VecAudioForXVector"),WGo=o(" (Data2VecAudio model)"),QGo=l(),F2=a("li"),yde=a("strong"),HGo=o("unispeech-sat"),UGo=o(" \u2014 "),gN=a("a"),JGo=o("UniSpeechSatForXVector"),YGo=o(" (UniSpeechSat model)"),KGo=l(),C2=a("li"),wde=a("strong"),ZGo=o("wav2vec2"),eOo=o(" \u2014 "),hN=a("a"),oOo=o("Wav2Vec2ForXVector"),rOo=o(" (Wav2Vec2 model)"),tOo=l(),M2=a("li"),Ade=a("strong"),aOo=o("wavlm"),nOo=o(" \u2014 "),pN=a("a"),sOo=o("WavLMForXVector"),lOo=o(" (WavLM model)"),iOo=l(),E2=a("p"),dOo=o("The model is set in evaluation mode by default using "),Lde=a("code"),cOo=o("model.eval()"),fOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bde=a("code"),mOo=o("model.train()"),gOo=l(),xde=a("p"),hOo=o("Examples:"),pOo=l(),f(Ay.$$.fragment),HBe=l(),Od=a("h2"),y2=a("a"),kde=a("span"),f(Ly.$$.fragment),_Oo=l(),Rde=a("span"),uOo=o("AutoModelForMaskedImageModeling"),UBe=l(),gr=a("div"),f(By.$$.fragment),bOo=l(),Xd=a("p"),vOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Sde=a("code"),TOo=o("from_pretrained()"),FOo=o("class method or the "),Pde=a("code"),COo=o("from_config()"),MOo=o(`class
method.`),EOo=l(),xy=a("p"),yOo=o("This class cannot be instantiated directly using "),$de=a("code"),wOo=o("__init__()"),AOo=o(" (throws an error)."),LOo=l(),st=a("div"),f(ky.$$.fragment),BOo=l(),Ide=a("p"),xOo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),kOo=l(),Vd=a("p"),ROo=o(`Note:
Loading a model from its configuration file does `),Dde=a("strong"),SOo=o("not"),POo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jde=a("code"),$Oo=o("from_pretrained()"),IOo=o("to load the model weights."),DOo=l(),Nde=a("p"),jOo=o("Examples:"),NOo=l(),f(Ry.$$.fragment),qOo=l(),oo=a("div"),f(Sy.$$.fragment),GOo=l(),qde=a("p"),OOo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),XOo=l(),nn=a("p"),VOo=o("The model class to instantiate is selected based on the "),Gde=a("code"),zOo=o("model_type"),WOo=o(` property of the config object (either
passed as an argument or loaded from `),Ode=a("code"),QOo=o("pretrained_model_name_or_path"),HOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xde=a("code"),UOo=o("pretrained_model_name_or_path"),JOo=o(":"),YOo=l(),zd=a("ul"),w2=a("li"),Vde=a("strong"),KOo=o("deit"),ZOo=o(" \u2014 "),_N=a("a"),eXo=o("DeiTForMaskedImageModeling"),oXo=o(" (DeiT model)"),rXo=l(),A2=a("li"),zde=a("strong"),tXo=o("swin"),aXo=o(" \u2014 "),uN=a("a"),nXo=o("SwinForMaskedImageModeling"),sXo=o(" (Swin model)"),lXo=l(),L2=a("li"),Wde=a("strong"),iXo=o("vit"),dXo=o(" \u2014 "),bN=a("a"),cXo=o("ViTForMaskedImageModeling"),fXo=o(" (ViT model)"),mXo=l(),B2=a("p"),gXo=o("The model is set in evaluation mode by default using "),Qde=a("code"),hXo=o("model.eval()"),pXo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hde=a("code"),_Xo=o("model.train()"),uXo=l(),Ude=a("p"),bXo=o("Examples:"),vXo=l(),f(Py.$$.fragment),JBe=l(),Wd=a("h2"),x2=a("a"),Jde=a("span"),f($y.$$.fragment),TXo=l(),Yde=a("span"),FXo=o("AutoModelForObjectDetection"),YBe=l(),hr=a("div"),f(Iy.$$.fragment),CXo=l(),Qd=a("p"),MXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Kde=a("code"),EXo=o("from_pretrained()"),yXo=o("class method or the "),Zde=a("code"),wXo=o("from_config()"),AXo=o(`class
method.`),LXo=l(),Dy=a("p"),BXo=o("This class cannot be instantiated directly using "),ece=a("code"),xXo=o("__init__()"),kXo=o(" (throws an error)."),RXo=l(),lt=a("div"),f(jy.$$.fragment),SXo=l(),oce=a("p"),PXo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),$Xo=l(),Hd=a("p"),IXo=o(`Note:
Loading a model from its configuration file does `),rce=a("strong"),DXo=o("not"),jXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tce=a("code"),NXo=o("from_pretrained()"),qXo=o("to load the model weights."),GXo=l(),ace=a("p"),OXo=o("Examples:"),XXo=l(),f(Ny.$$.fragment),VXo=l(),ro=a("div"),f(qy.$$.fragment),zXo=l(),nce=a("p"),WXo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),QXo=l(),sn=a("p"),HXo=o("The model class to instantiate is selected based on the "),sce=a("code"),UXo=o("model_type"),JXo=o(` property of the config object (either
passed as an argument or loaded from `),lce=a("code"),YXo=o("pretrained_model_name_or_path"),KXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ice=a("code"),ZXo=o("pretrained_model_name_or_path"),eVo=o(":"),oVo=l(),dce=a("ul"),k2=a("li"),cce=a("strong"),rVo=o("detr"),tVo=o(" \u2014 "),vN=a("a"),aVo=o("DetrForObjectDetection"),nVo=o(" (DETR model)"),sVo=l(),R2=a("p"),lVo=o("The model is set in evaluation mode by default using "),fce=a("code"),iVo=o("model.eval()"),dVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mce=a("code"),cVo=o("model.train()"),fVo=l(),gce=a("p"),mVo=o("Examples:"),gVo=l(),f(Gy.$$.fragment),KBe=l(),Ud=a("h2"),S2=a("a"),hce=a("span"),f(Oy.$$.fragment),hVo=l(),pce=a("span"),pVo=o("AutoModelForImageSegmentation"),ZBe=l(),pr=a("div"),f(Xy.$$.fragment),_Vo=l(),Jd=a("p"),uVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),_ce=a("code"),bVo=o("from_pretrained()"),vVo=o("class method or the "),uce=a("code"),TVo=o("from_config()"),FVo=o(`class
method.`),CVo=l(),Vy=a("p"),MVo=o("This class cannot be instantiated directly using "),bce=a("code"),EVo=o("__init__()"),yVo=o(" (throws an error)."),wVo=l(),it=a("div"),f(zy.$$.fragment),AVo=l(),vce=a("p"),LVo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),BVo=l(),Yd=a("p"),xVo=o(`Note:
Loading a model from its configuration file does `),Tce=a("strong"),kVo=o("not"),RVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fce=a("code"),SVo=o("from_pretrained()"),PVo=o("to load the model weights."),$Vo=l(),Cce=a("p"),IVo=o("Examples:"),DVo=l(),f(Wy.$$.fragment),jVo=l(),to=a("div"),f(Qy.$$.fragment),NVo=l(),Mce=a("p"),qVo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),GVo=l(),ln=a("p"),OVo=o("The model class to instantiate is selected based on the "),Ece=a("code"),XVo=o("model_type"),VVo=o(` property of the config object (either
passed as an argument or loaded from `),yce=a("code"),zVo=o("pretrained_model_name_or_path"),WVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wce=a("code"),QVo=o("pretrained_model_name_or_path"),HVo=o(":"),UVo=l(),Ace=a("ul"),P2=a("li"),Lce=a("strong"),JVo=o("detr"),YVo=o(" \u2014 "),TN=a("a"),KVo=o("DetrForSegmentation"),ZVo=o(" (DETR model)"),ezo=l(),$2=a("p"),ozo=o("The model is set in evaluation mode by default using "),Bce=a("code"),rzo=o("model.eval()"),tzo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xce=a("code"),azo=o("model.train()"),nzo=l(),kce=a("p"),szo=o("Examples:"),lzo=l(),f(Hy.$$.fragment),exe=l(),Kd=a("h2"),I2=a("a"),Rce=a("span"),f(Uy.$$.fragment),izo=l(),Sce=a("span"),dzo=o("AutoModelForSemanticSegmentation"),oxe=l(),_r=a("div"),f(Jy.$$.fragment),czo=l(),Zd=a("p"),fzo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Pce=a("code"),mzo=o("from_pretrained()"),gzo=o("class method or the "),$ce=a("code"),hzo=o("from_config()"),pzo=o(`class
method.`),_zo=l(),Yy=a("p"),uzo=o("This class cannot be instantiated directly using "),Ice=a("code"),bzo=o("__init__()"),vzo=o(" (throws an error)."),Tzo=l(),dt=a("div"),f(Ky.$$.fragment),Fzo=l(),Dce=a("p"),Czo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Mzo=l(),ec=a("p"),Ezo=o(`Note:
Loading a model from its configuration file does `),jce=a("strong"),yzo=o("not"),wzo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Nce=a("code"),Azo=o("from_pretrained()"),Lzo=o("to load the model weights."),Bzo=l(),qce=a("p"),xzo=o("Examples:"),kzo=l(),f(Zy.$$.fragment),Rzo=l(),ao=a("div"),f(ew.$$.fragment),Szo=l(),Gce=a("p"),Pzo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),$zo=l(),dn=a("p"),Izo=o("The model class to instantiate is selected based on the "),Oce=a("code"),Dzo=o("model_type"),jzo=o(` property of the config object (either
passed as an argument or loaded from `),Xce=a("code"),Nzo=o("pretrained_model_name_or_path"),qzo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vce=a("code"),Gzo=o("pretrained_model_name_or_path"),Ozo=o(":"),Xzo=l(),ow=a("ul"),D2=a("li"),zce=a("strong"),Vzo=o("beit"),zzo=o(" \u2014 "),FN=a("a"),Wzo=o("BeitForSemanticSegmentation"),Qzo=o(" (BEiT model)"),Hzo=l(),j2=a("li"),Wce=a("strong"),Uzo=o("segformer"),Jzo=o(" \u2014 "),CN=a("a"),Yzo=o("SegformerForSemanticSegmentation"),Kzo=o(" (SegFormer model)"),Zzo=l(),N2=a("p"),eWo=o("The model is set in evaluation mode by default using "),Qce=a("code"),oWo=o("model.eval()"),rWo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hce=a("code"),tWo=o("model.train()"),aWo=l(),Uce=a("p"),nWo=o("Examples:"),sWo=l(),f(rw.$$.fragment),rxe=l(),oc=a("h2"),q2=a("a"),Jce=a("span"),f(tw.$$.fragment),lWo=l(),Yce=a("span"),iWo=o("TFAutoModel"),txe=l(),ur=a("div"),f(aw.$$.fragment),dWo=l(),rc=a("p"),cWo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Kce=a("code"),fWo=o("from_pretrained()"),mWo=o("class method or the "),Zce=a("code"),gWo=o("from_config()"),hWo=o(`class
method.`),pWo=l(),nw=a("p"),_Wo=o("This class cannot be instantiated directly using "),efe=a("code"),uWo=o("__init__()"),bWo=o(" (throws an error)."),vWo=l(),ct=a("div"),f(sw.$$.fragment),TWo=l(),ofe=a("p"),FWo=o("Instantiates one of the base model classes of the library from a configuration."),CWo=l(),tc=a("p"),MWo=o(`Note:
Loading a model from its configuration file does `),rfe=a("strong"),EWo=o("not"),yWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tfe=a("code"),wWo=o("from_pretrained()"),AWo=o("to load the model weights."),LWo=l(),afe=a("p"),BWo=o("Examples:"),xWo=l(),f(lw.$$.fragment),kWo=l(),go=a("div"),f(iw.$$.fragment),RWo=l(),nfe=a("p"),SWo=o("Instantiate one of the base model classes of the library from a pretrained model."),PWo=l(),cn=a("p"),$Wo=o("The model class to instantiate is selected based on the "),sfe=a("code"),IWo=o("model_type"),DWo=o(` property of the config object (either
passed as an argument or loaded from `),lfe=a("code"),jWo=o("pretrained_model_name_or_path"),NWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ife=a("code"),qWo=o("pretrained_model_name_or_path"),GWo=o(":"),OWo=l(),B=a("ul"),G2=a("li"),dfe=a("strong"),XWo=o("albert"),VWo=o(" \u2014 "),MN=a("a"),zWo=o("TFAlbertModel"),WWo=o(" (ALBERT model)"),QWo=l(),O2=a("li"),cfe=a("strong"),HWo=o("bart"),UWo=o(" \u2014 "),EN=a("a"),JWo=o("TFBartModel"),YWo=o(" (BART model)"),KWo=l(),X2=a("li"),ffe=a("strong"),ZWo=o("bert"),eQo=o(" \u2014 "),yN=a("a"),oQo=o("TFBertModel"),rQo=o(" (BERT model)"),tQo=l(),V2=a("li"),mfe=a("strong"),aQo=o("blenderbot"),nQo=o(" \u2014 "),wN=a("a"),sQo=o("TFBlenderbotModel"),lQo=o(" (Blenderbot model)"),iQo=l(),z2=a("li"),gfe=a("strong"),dQo=o("blenderbot-small"),cQo=o(" \u2014 "),AN=a("a"),fQo=o("TFBlenderbotSmallModel"),mQo=o(" (BlenderbotSmall model)"),gQo=l(),W2=a("li"),hfe=a("strong"),hQo=o("camembert"),pQo=o(" \u2014 "),LN=a("a"),_Qo=o("TFCamembertModel"),uQo=o(" (CamemBERT model)"),bQo=l(),Q2=a("li"),pfe=a("strong"),vQo=o("clip"),TQo=o(" \u2014 "),BN=a("a"),FQo=o("TFCLIPModel"),CQo=o(" (CLIP model)"),MQo=l(),H2=a("li"),_fe=a("strong"),EQo=o("convbert"),yQo=o(" \u2014 "),xN=a("a"),wQo=o("TFConvBertModel"),AQo=o(" (ConvBERT model)"),LQo=l(),U2=a("li"),ufe=a("strong"),BQo=o("convnext"),xQo=o(" \u2014 "),kN=a("a"),kQo=o("TFConvNextModel"),RQo=o(" (ConvNext model)"),SQo=l(),J2=a("li"),bfe=a("strong"),PQo=o("ctrl"),$Qo=o(" \u2014 "),RN=a("a"),IQo=o("TFCTRLModel"),DQo=o(" (CTRL model)"),jQo=l(),Y2=a("li"),vfe=a("strong"),NQo=o("deberta"),qQo=o(" \u2014 "),SN=a("a"),GQo=o("TFDebertaModel"),OQo=o(" (DeBERTa model)"),XQo=l(),K2=a("li"),Tfe=a("strong"),VQo=o("deberta-v2"),zQo=o(" \u2014 "),PN=a("a"),WQo=o("TFDebertaV2Model"),QQo=o(" (DeBERTa-v2 model)"),HQo=l(),Z2=a("li"),Ffe=a("strong"),UQo=o("distilbert"),JQo=o(" \u2014 "),$N=a("a"),YQo=o("TFDistilBertModel"),KQo=o(" (DistilBERT model)"),ZQo=l(),ev=a("li"),Cfe=a("strong"),eHo=o("dpr"),oHo=o(" \u2014 "),IN=a("a"),rHo=o("TFDPRQuestionEncoder"),tHo=o(" (DPR model)"),aHo=l(),ov=a("li"),Mfe=a("strong"),nHo=o("electra"),sHo=o(" \u2014 "),DN=a("a"),lHo=o("TFElectraModel"),iHo=o(" (ELECTRA model)"),dHo=l(),rv=a("li"),Efe=a("strong"),cHo=o("flaubert"),fHo=o(" \u2014 "),jN=a("a"),mHo=o("TFFlaubertModel"),gHo=o(" (FlauBERT model)"),hHo=l(),$s=a("li"),yfe=a("strong"),pHo=o("funnel"),_Ho=o(" \u2014 "),NN=a("a"),uHo=o("TFFunnelModel"),bHo=o(" or "),qN=a("a"),vHo=o("TFFunnelBaseModel"),THo=o(" (Funnel Transformer model)"),FHo=l(),tv=a("li"),wfe=a("strong"),CHo=o("gpt2"),MHo=o(" \u2014 "),GN=a("a"),EHo=o("TFGPT2Model"),yHo=o(" (OpenAI GPT-2 model)"),wHo=l(),av=a("li"),Afe=a("strong"),AHo=o("hubert"),LHo=o(" \u2014 "),ON=a("a"),BHo=o("TFHubertModel"),xHo=o(" (Hubert model)"),kHo=l(),nv=a("li"),Lfe=a("strong"),RHo=o("layoutlm"),SHo=o(" \u2014 "),XN=a("a"),PHo=o("TFLayoutLMModel"),$Ho=o(" (LayoutLM model)"),IHo=l(),sv=a("li"),Bfe=a("strong"),DHo=o("led"),jHo=o(" \u2014 "),VN=a("a"),NHo=o("TFLEDModel"),qHo=o(" (LED model)"),GHo=l(),lv=a("li"),xfe=a("strong"),OHo=o("longformer"),XHo=o(" \u2014 "),zN=a("a"),VHo=o("TFLongformerModel"),zHo=o(" (Longformer model)"),WHo=l(),iv=a("li"),kfe=a("strong"),QHo=o("lxmert"),HHo=o(" \u2014 "),WN=a("a"),UHo=o("TFLxmertModel"),JHo=o(" (LXMERT model)"),YHo=l(),dv=a("li"),Rfe=a("strong"),KHo=o("marian"),ZHo=o(" \u2014 "),QN=a("a"),eUo=o("TFMarianModel"),oUo=o(" (Marian model)"),rUo=l(),cv=a("li"),Sfe=a("strong"),tUo=o("mbart"),aUo=o(" \u2014 "),HN=a("a"),nUo=o("TFMBartModel"),sUo=o(" (mBART model)"),lUo=l(),fv=a("li"),Pfe=a("strong"),iUo=o("mobilebert"),dUo=o(" \u2014 "),UN=a("a"),cUo=o("TFMobileBertModel"),fUo=o(" (MobileBERT model)"),mUo=l(),mv=a("li"),$fe=a("strong"),gUo=o("mpnet"),hUo=o(" \u2014 "),JN=a("a"),pUo=o("TFMPNetModel"),_Uo=o(" (MPNet model)"),uUo=l(),gv=a("li"),Ife=a("strong"),bUo=o("mt5"),vUo=o(" \u2014 "),YN=a("a"),TUo=o("TFMT5Model"),FUo=o(" (mT5 model)"),CUo=l(),hv=a("li"),Dfe=a("strong"),MUo=o("openai-gpt"),EUo=o(" \u2014 "),KN=a("a"),yUo=o("TFOpenAIGPTModel"),wUo=o(" (OpenAI GPT model)"),AUo=l(),pv=a("li"),jfe=a("strong"),LUo=o("pegasus"),BUo=o(" \u2014 "),ZN=a("a"),xUo=o("TFPegasusModel"),kUo=o(" (Pegasus model)"),RUo=l(),_v=a("li"),Nfe=a("strong"),SUo=o("rembert"),PUo=o(" \u2014 "),eq=a("a"),$Uo=o("TFRemBertModel"),IUo=o(" (RemBERT model)"),DUo=l(),uv=a("li"),qfe=a("strong"),jUo=o("roberta"),NUo=o(" \u2014 "),oq=a("a"),qUo=o("TFRobertaModel"),GUo=o(" (RoBERTa model)"),OUo=l(),bv=a("li"),Gfe=a("strong"),XUo=o("roformer"),VUo=o(" \u2014 "),rq=a("a"),zUo=o("TFRoFormerModel"),WUo=o(" (RoFormer model)"),QUo=l(),vv=a("li"),Ofe=a("strong"),HUo=o("speech_to_text"),UUo=o(" \u2014 "),tq=a("a"),JUo=o("TFSpeech2TextModel"),YUo=o(" (Speech2Text model)"),KUo=l(),Tv=a("li"),Xfe=a("strong"),ZUo=o("t5"),eJo=o(" \u2014 "),aq=a("a"),oJo=o("TFT5Model"),rJo=o(" (T5 model)"),tJo=l(),Fv=a("li"),Vfe=a("strong"),aJo=o("tapas"),nJo=o(" \u2014 "),nq=a("a"),sJo=o("TFTapasModel"),lJo=o(" (TAPAS model)"),iJo=l(),Cv=a("li"),zfe=a("strong"),dJo=o("transfo-xl"),cJo=o(" \u2014 "),sq=a("a"),fJo=o("TFTransfoXLModel"),mJo=o(" (Transformer-XL model)"),gJo=l(),Mv=a("li"),Wfe=a("strong"),hJo=o("vit"),pJo=o(" \u2014 "),lq=a("a"),_Jo=o("TFViTModel"),uJo=o(" (ViT model)"),bJo=l(),Ev=a("li"),Qfe=a("strong"),vJo=o("wav2vec2"),TJo=o(" \u2014 "),iq=a("a"),FJo=o("TFWav2Vec2Model"),CJo=o(" (Wav2Vec2 model)"),MJo=l(),yv=a("li"),Hfe=a("strong"),EJo=o("xlm"),yJo=o(" \u2014 "),dq=a("a"),wJo=o("TFXLMModel"),AJo=o(" (XLM model)"),LJo=l(),wv=a("li"),Ufe=a("strong"),BJo=o("xlm-roberta"),xJo=o(" \u2014 "),cq=a("a"),kJo=o("TFXLMRobertaModel"),RJo=o(" (XLM-RoBERTa model)"),SJo=l(),Av=a("li"),Jfe=a("strong"),PJo=o("xlnet"),$Jo=o(" \u2014 "),fq=a("a"),IJo=o("TFXLNetModel"),DJo=o(" (XLNet model)"),jJo=l(),Yfe=a("p"),NJo=o("Examples:"),qJo=l(),f(dw.$$.fragment),axe=l(),ac=a("h2"),Lv=a("a"),Kfe=a("span"),f(cw.$$.fragment),GJo=l(),Zfe=a("span"),OJo=o("TFAutoModelForPreTraining"),nxe=l(),br=a("div"),f(fw.$$.fragment),XJo=l(),nc=a("p"),VJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),eme=a("code"),zJo=o("from_pretrained()"),WJo=o("class method or the "),ome=a("code"),QJo=o("from_config()"),HJo=o(`class
method.`),UJo=l(),mw=a("p"),JJo=o("This class cannot be instantiated directly using "),rme=a("code"),YJo=o("__init__()"),KJo=o(" (throws an error)."),ZJo=l(),ft=a("div"),f(gw.$$.fragment),eYo=l(),tme=a("p"),oYo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),rYo=l(),sc=a("p"),tYo=o(`Note:
Loading a model from its configuration file does `),ame=a("strong"),aYo=o("not"),nYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nme=a("code"),sYo=o("from_pretrained()"),lYo=o("to load the model weights."),iYo=l(),sme=a("p"),dYo=o("Examples:"),cYo=l(),f(hw.$$.fragment),fYo=l(),ho=a("div"),f(pw.$$.fragment),mYo=l(),lme=a("p"),gYo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),hYo=l(),fn=a("p"),pYo=o("The model class to instantiate is selected based on the "),ime=a("code"),_Yo=o("model_type"),uYo=o(` property of the config object (either
passed as an argument or loaded from `),dme=a("code"),bYo=o("pretrained_model_name_or_path"),vYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cme=a("code"),TYo=o("pretrained_model_name_or_path"),FYo=o(":"),CYo=l(),H=a("ul"),Bv=a("li"),fme=a("strong"),MYo=o("albert"),EYo=o(" \u2014 "),mq=a("a"),yYo=o("TFAlbertForPreTraining"),wYo=o(" (ALBERT model)"),AYo=l(),xv=a("li"),mme=a("strong"),LYo=o("bart"),BYo=o(" \u2014 "),gq=a("a"),xYo=o("TFBartForConditionalGeneration"),kYo=o(" (BART model)"),RYo=l(),kv=a("li"),gme=a("strong"),SYo=o("bert"),PYo=o(" \u2014 "),hq=a("a"),$Yo=o("TFBertForPreTraining"),IYo=o(" (BERT model)"),DYo=l(),Rv=a("li"),hme=a("strong"),jYo=o("camembert"),NYo=o(" \u2014 "),pq=a("a"),qYo=o("TFCamembertForMaskedLM"),GYo=o(" (CamemBERT model)"),OYo=l(),Sv=a("li"),pme=a("strong"),XYo=o("ctrl"),VYo=o(" \u2014 "),_q=a("a"),zYo=o("TFCTRLLMHeadModel"),WYo=o(" (CTRL model)"),QYo=l(),Pv=a("li"),_me=a("strong"),HYo=o("distilbert"),UYo=o(" \u2014 "),uq=a("a"),JYo=o("TFDistilBertForMaskedLM"),YYo=o(" (DistilBERT model)"),KYo=l(),$v=a("li"),ume=a("strong"),ZYo=o("electra"),eKo=o(" \u2014 "),bq=a("a"),oKo=o("TFElectraForPreTraining"),rKo=o(" (ELECTRA model)"),tKo=l(),Iv=a("li"),bme=a("strong"),aKo=o("flaubert"),nKo=o(" \u2014 "),vq=a("a"),sKo=o("TFFlaubertWithLMHeadModel"),lKo=o(" (FlauBERT model)"),iKo=l(),Dv=a("li"),vme=a("strong"),dKo=o("funnel"),cKo=o(" \u2014 "),Tq=a("a"),fKo=o("TFFunnelForPreTraining"),mKo=o(" (Funnel Transformer model)"),gKo=l(),jv=a("li"),Tme=a("strong"),hKo=o("gpt2"),pKo=o(" \u2014 "),Fq=a("a"),_Ko=o("TFGPT2LMHeadModel"),uKo=o(" (OpenAI GPT-2 model)"),bKo=l(),Nv=a("li"),Fme=a("strong"),vKo=o("layoutlm"),TKo=o(" \u2014 "),Cq=a("a"),FKo=o("TFLayoutLMForMaskedLM"),CKo=o(" (LayoutLM model)"),MKo=l(),qv=a("li"),Cme=a("strong"),EKo=o("lxmert"),yKo=o(" \u2014 "),Mq=a("a"),wKo=o("TFLxmertForPreTraining"),AKo=o(" (LXMERT model)"),LKo=l(),Gv=a("li"),Mme=a("strong"),BKo=o("mobilebert"),xKo=o(" \u2014 "),Eq=a("a"),kKo=o("TFMobileBertForPreTraining"),RKo=o(" (MobileBERT model)"),SKo=l(),Ov=a("li"),Eme=a("strong"),PKo=o("mpnet"),$Ko=o(" \u2014 "),yq=a("a"),IKo=o("TFMPNetForMaskedLM"),DKo=o(" (MPNet model)"),jKo=l(),Xv=a("li"),yme=a("strong"),NKo=o("openai-gpt"),qKo=o(" \u2014 "),wq=a("a"),GKo=o("TFOpenAIGPTLMHeadModel"),OKo=o(" (OpenAI GPT model)"),XKo=l(),Vv=a("li"),wme=a("strong"),VKo=o("roberta"),zKo=o(" \u2014 "),Aq=a("a"),WKo=o("TFRobertaForMaskedLM"),QKo=o(" (RoBERTa model)"),HKo=l(),zv=a("li"),Ame=a("strong"),UKo=o("t5"),JKo=o(" \u2014 "),Lq=a("a"),YKo=o("TFT5ForConditionalGeneration"),KKo=o(" (T5 model)"),ZKo=l(),Wv=a("li"),Lme=a("strong"),eZo=o("tapas"),oZo=o(" \u2014 "),Bq=a("a"),rZo=o("TFTapasForMaskedLM"),tZo=o(" (TAPAS model)"),aZo=l(),Qv=a("li"),Bme=a("strong"),nZo=o("transfo-xl"),sZo=o(" \u2014 "),xq=a("a"),lZo=o("TFTransfoXLLMHeadModel"),iZo=o(" (Transformer-XL model)"),dZo=l(),Hv=a("li"),xme=a("strong"),cZo=o("xlm"),fZo=o(" \u2014 "),kq=a("a"),mZo=o("TFXLMWithLMHeadModel"),gZo=o(" (XLM model)"),hZo=l(),Uv=a("li"),kme=a("strong"),pZo=o("xlm-roberta"),_Zo=o(" \u2014 "),Rq=a("a"),uZo=o("TFXLMRobertaForMaskedLM"),bZo=o(" (XLM-RoBERTa model)"),vZo=l(),Jv=a("li"),Rme=a("strong"),TZo=o("xlnet"),FZo=o(" \u2014 "),Sq=a("a"),CZo=o("TFXLNetLMHeadModel"),MZo=o(" (XLNet model)"),EZo=l(),Sme=a("p"),yZo=o("Examples:"),wZo=l(),f(_w.$$.fragment),sxe=l(),lc=a("h2"),Yv=a("a"),Pme=a("span"),f(uw.$$.fragment),AZo=l(),$me=a("span"),LZo=o("TFAutoModelForCausalLM"),lxe=l(),vr=a("div"),f(bw.$$.fragment),BZo=l(),ic=a("p"),xZo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Ime=a("code"),kZo=o("from_pretrained()"),RZo=o("class method or the "),Dme=a("code"),SZo=o("from_config()"),PZo=o(`class
method.`),$Zo=l(),vw=a("p"),IZo=o("This class cannot be instantiated directly using "),jme=a("code"),DZo=o("__init__()"),jZo=o(" (throws an error)."),NZo=l(),mt=a("div"),f(Tw.$$.fragment),qZo=l(),Nme=a("p"),GZo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),OZo=l(),dc=a("p"),XZo=o(`Note:
Loading a model from its configuration file does `),qme=a("strong"),VZo=o("not"),zZo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gme=a("code"),WZo=o("from_pretrained()"),QZo=o("to load the model weights."),HZo=l(),Ome=a("p"),UZo=o("Examples:"),JZo=l(),f(Fw.$$.fragment),YZo=l(),po=a("div"),f(Cw.$$.fragment),KZo=l(),Xme=a("p"),ZZo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),eer=l(),mn=a("p"),oer=o("The model class to instantiate is selected based on the "),Vme=a("code"),rer=o("model_type"),ter=o(` property of the config object (either
passed as an argument or loaded from `),zme=a("code"),aer=o("pretrained_model_name_or_path"),ner=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wme=a("code"),ser=o("pretrained_model_name_or_path"),ler=o(":"),ier=l(),pe=a("ul"),Kv=a("li"),Qme=a("strong"),der=o("bert"),cer=o(" \u2014 "),Pq=a("a"),fer=o("TFBertLMHeadModel"),mer=o(" (BERT model)"),ger=l(),Zv=a("li"),Hme=a("strong"),her=o("ctrl"),per=o(" \u2014 "),$q=a("a"),_er=o("TFCTRLLMHeadModel"),uer=o(" (CTRL model)"),ber=l(),eT=a("li"),Ume=a("strong"),ver=o("gpt2"),Ter=o(" \u2014 "),Iq=a("a"),Fer=o("TFGPT2LMHeadModel"),Cer=o(" (OpenAI GPT-2 model)"),Mer=l(),oT=a("li"),Jme=a("strong"),Eer=o("openai-gpt"),yer=o(" \u2014 "),Dq=a("a"),wer=o("TFOpenAIGPTLMHeadModel"),Aer=o(" (OpenAI GPT model)"),Ler=l(),rT=a("li"),Yme=a("strong"),Ber=o("rembert"),xer=o(" \u2014 "),jq=a("a"),ker=o("TFRemBertForCausalLM"),Rer=o(" (RemBERT model)"),Ser=l(),tT=a("li"),Kme=a("strong"),Per=o("roberta"),$er=o(" \u2014 "),Nq=a("a"),Ier=o("TFRobertaForCausalLM"),Der=o(" (RoBERTa model)"),jer=l(),aT=a("li"),Zme=a("strong"),Ner=o("roformer"),qer=o(" \u2014 "),qq=a("a"),Ger=o("TFRoFormerForCausalLM"),Oer=o(" (RoFormer model)"),Xer=l(),nT=a("li"),ege=a("strong"),Ver=o("transfo-xl"),zer=o(" \u2014 "),Gq=a("a"),Wer=o("TFTransfoXLLMHeadModel"),Qer=o(" (Transformer-XL model)"),Her=l(),sT=a("li"),oge=a("strong"),Uer=o("xlm"),Jer=o(" \u2014 "),Oq=a("a"),Yer=o("TFXLMWithLMHeadModel"),Ker=o(" (XLM model)"),Zer=l(),lT=a("li"),rge=a("strong"),eor=o("xlnet"),oor=o(" \u2014 "),Xq=a("a"),ror=o("TFXLNetLMHeadModel"),tor=o(" (XLNet model)"),aor=l(),tge=a("p"),nor=o("Examples:"),sor=l(),f(Mw.$$.fragment),ixe=l(),cc=a("h2"),iT=a("a"),age=a("span"),f(Ew.$$.fragment),lor=l(),nge=a("span"),ior=o("TFAutoModelForImageClassification"),dxe=l(),Tr=a("div"),f(yw.$$.fragment),dor=l(),fc=a("p"),cor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),sge=a("code"),mor=o("from_pretrained()"),gor=o("class method or the "),lge=a("code"),hor=o("from_config()"),por=o(`class
method.`),_or=l(),ww=a("p"),uor=o("This class cannot be instantiated directly using "),ige=a("code"),bor=o("__init__()"),vor=o(" (throws an error)."),Tor=l(),gt=a("div"),f(Aw.$$.fragment),For=l(),dge=a("p"),Cor=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Mor=l(),mc=a("p"),Eor=o(`Note:
Loading a model from its configuration file does `),cge=a("strong"),yor=o("not"),wor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fge=a("code"),Aor=o("from_pretrained()"),Lor=o("to load the model weights."),Bor=l(),mge=a("p"),xor=o("Examples:"),kor=l(),f(Lw.$$.fragment),Ror=l(),_o=a("div"),f(Bw.$$.fragment),Sor=l(),gge=a("p"),Por=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),$or=l(),gn=a("p"),Ior=o("The model class to instantiate is selected based on the "),hge=a("code"),Dor=o("model_type"),jor=o(` property of the config object (either
passed as an argument or loaded from `),pge=a("code"),Nor=o("pretrained_model_name_or_path"),qor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_ge=a("code"),Gor=o("pretrained_model_name_or_path"),Oor=o(":"),Xor=l(),xw=a("ul"),dT=a("li"),uge=a("strong"),Vor=o("convnext"),zor=o(" \u2014 "),Vq=a("a"),Wor=o("TFConvNextForImageClassification"),Qor=o(" (ConvNext model)"),Hor=l(),cT=a("li"),bge=a("strong"),Uor=o("vit"),Jor=o(" \u2014 "),zq=a("a"),Yor=o("TFViTForImageClassification"),Kor=o(" (ViT model)"),Zor=l(),vge=a("p"),err=o("Examples:"),orr=l(),f(kw.$$.fragment),cxe=l(),gc=a("h2"),fT=a("a"),Tge=a("span"),f(Rw.$$.fragment),rrr=l(),Fge=a("span"),trr=o("TFAutoModelForMaskedLM"),fxe=l(),Fr=a("div"),f(Sw.$$.fragment),arr=l(),hc=a("p"),nrr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Cge=a("code"),srr=o("from_pretrained()"),lrr=o("class method or the "),Mge=a("code"),irr=o("from_config()"),drr=o(`class
method.`),crr=l(),Pw=a("p"),frr=o("This class cannot be instantiated directly using "),Ege=a("code"),mrr=o("__init__()"),grr=o(" (throws an error)."),hrr=l(),ht=a("div"),f($w.$$.fragment),prr=l(),yge=a("p"),_rr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),urr=l(),pc=a("p"),brr=o(`Note:
Loading a model from its configuration file does `),wge=a("strong"),vrr=o("not"),Trr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Age=a("code"),Frr=o("from_pretrained()"),Crr=o("to load the model weights."),Mrr=l(),Lge=a("p"),Err=o("Examples:"),yrr=l(),f(Iw.$$.fragment),wrr=l(),uo=a("div"),f(Dw.$$.fragment),Arr=l(),Bge=a("p"),Lrr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Brr=l(),hn=a("p"),xrr=o("The model class to instantiate is selected based on the "),xge=a("code"),krr=o("model_type"),Rrr=o(` property of the config object (either
passed as an argument or loaded from `),kge=a("code"),Srr=o("pretrained_model_name_or_path"),Prr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rge=a("code"),$rr=o("pretrained_model_name_or_path"),Irr=o(":"),Drr=l(),Y=a("ul"),mT=a("li"),Sge=a("strong"),jrr=o("albert"),Nrr=o(" \u2014 "),Wq=a("a"),qrr=o("TFAlbertForMaskedLM"),Grr=o(" (ALBERT model)"),Orr=l(),gT=a("li"),Pge=a("strong"),Xrr=o("bert"),Vrr=o(" \u2014 "),Qq=a("a"),zrr=o("TFBertForMaskedLM"),Wrr=o(" (BERT model)"),Qrr=l(),hT=a("li"),$ge=a("strong"),Hrr=o("camembert"),Urr=o(" \u2014 "),Hq=a("a"),Jrr=o("TFCamembertForMaskedLM"),Yrr=o(" (CamemBERT model)"),Krr=l(),pT=a("li"),Ige=a("strong"),Zrr=o("convbert"),etr=o(" \u2014 "),Uq=a("a"),otr=o("TFConvBertForMaskedLM"),rtr=o(" (ConvBERT model)"),ttr=l(),_T=a("li"),Dge=a("strong"),atr=o("deberta"),ntr=o(" \u2014 "),Jq=a("a"),str=o("TFDebertaForMaskedLM"),ltr=o(" (DeBERTa model)"),itr=l(),uT=a("li"),jge=a("strong"),dtr=o("deberta-v2"),ctr=o(" \u2014 "),Yq=a("a"),ftr=o("TFDebertaV2ForMaskedLM"),mtr=o(" (DeBERTa-v2 model)"),gtr=l(),bT=a("li"),Nge=a("strong"),htr=o("distilbert"),ptr=o(" \u2014 "),Kq=a("a"),_tr=o("TFDistilBertForMaskedLM"),utr=o(" (DistilBERT model)"),btr=l(),vT=a("li"),qge=a("strong"),vtr=o("electra"),Ttr=o(" \u2014 "),Zq=a("a"),Ftr=o("TFElectraForMaskedLM"),Ctr=o(" (ELECTRA model)"),Mtr=l(),TT=a("li"),Gge=a("strong"),Etr=o("flaubert"),ytr=o(" \u2014 "),eG=a("a"),wtr=o("TFFlaubertWithLMHeadModel"),Atr=o(" (FlauBERT model)"),Ltr=l(),FT=a("li"),Oge=a("strong"),Btr=o("funnel"),xtr=o(" \u2014 "),oG=a("a"),ktr=o("TFFunnelForMaskedLM"),Rtr=o(" (Funnel Transformer model)"),Str=l(),CT=a("li"),Xge=a("strong"),Ptr=o("layoutlm"),$tr=o(" \u2014 "),rG=a("a"),Itr=o("TFLayoutLMForMaskedLM"),Dtr=o(" (LayoutLM model)"),jtr=l(),MT=a("li"),Vge=a("strong"),Ntr=o("longformer"),qtr=o(" \u2014 "),tG=a("a"),Gtr=o("TFLongformerForMaskedLM"),Otr=o(" (Longformer model)"),Xtr=l(),ET=a("li"),zge=a("strong"),Vtr=o("mobilebert"),ztr=o(" \u2014 "),aG=a("a"),Wtr=o("TFMobileBertForMaskedLM"),Qtr=o(" (MobileBERT model)"),Htr=l(),yT=a("li"),Wge=a("strong"),Utr=o("mpnet"),Jtr=o(" \u2014 "),nG=a("a"),Ytr=o("TFMPNetForMaskedLM"),Ktr=o(" (MPNet model)"),Ztr=l(),wT=a("li"),Qge=a("strong"),ear=o("rembert"),oar=o(" \u2014 "),sG=a("a"),rar=o("TFRemBertForMaskedLM"),tar=o(" (RemBERT model)"),aar=l(),AT=a("li"),Hge=a("strong"),nar=o("roberta"),sar=o(" \u2014 "),lG=a("a"),lar=o("TFRobertaForMaskedLM"),iar=o(" (RoBERTa model)"),dar=l(),LT=a("li"),Uge=a("strong"),car=o("roformer"),far=o(" \u2014 "),iG=a("a"),mar=o("TFRoFormerForMaskedLM"),gar=o(" (RoFormer model)"),har=l(),BT=a("li"),Jge=a("strong"),par=o("tapas"),_ar=o(" \u2014 "),dG=a("a"),uar=o("TFTapasForMaskedLM"),bar=o(" (TAPAS model)"),Tar=l(),xT=a("li"),Yge=a("strong"),Far=o("xlm"),Car=o(" \u2014 "),cG=a("a"),Mar=o("TFXLMWithLMHeadModel"),Ear=o(" (XLM model)"),yar=l(),kT=a("li"),Kge=a("strong"),war=o("xlm-roberta"),Aar=o(" \u2014 "),fG=a("a"),Lar=o("TFXLMRobertaForMaskedLM"),Bar=o(" (XLM-RoBERTa model)"),xar=l(),Zge=a("p"),kar=o("Examples:"),Rar=l(),f(jw.$$.fragment),mxe=l(),_c=a("h2"),RT=a("a"),ehe=a("span"),f(Nw.$$.fragment),Sar=l(),ohe=a("span"),Par=o("TFAutoModelForSeq2SeqLM"),gxe=l(),Cr=a("div"),f(qw.$$.fragment),$ar=l(),uc=a("p"),Iar=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),rhe=a("code"),Dar=o("from_pretrained()"),jar=o("class method or the "),the=a("code"),Nar=o("from_config()"),qar=o(`class
method.`),Gar=l(),Gw=a("p"),Oar=o("This class cannot be instantiated directly using "),ahe=a("code"),Xar=o("__init__()"),Var=o(" (throws an error)."),zar=l(),pt=a("div"),f(Ow.$$.fragment),War=l(),nhe=a("p"),Qar=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Har=l(),bc=a("p"),Uar=o(`Note:
Loading a model from its configuration file does `),she=a("strong"),Jar=o("not"),Yar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lhe=a("code"),Kar=o("from_pretrained()"),Zar=o("to load the model weights."),enr=l(),ihe=a("p"),onr=o("Examples:"),rnr=l(),f(Xw.$$.fragment),tnr=l(),bo=a("div"),f(Vw.$$.fragment),anr=l(),dhe=a("p"),nnr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),snr=l(),pn=a("p"),lnr=o("The model class to instantiate is selected based on the "),che=a("code"),inr=o("model_type"),dnr=o(` property of the config object (either
passed as an argument or loaded from `),fhe=a("code"),cnr=o("pretrained_model_name_or_path"),fnr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mhe=a("code"),mnr=o("pretrained_model_name_or_path"),gnr=o(":"),hnr=l(),_e=a("ul"),ST=a("li"),ghe=a("strong"),pnr=o("bart"),_nr=o(" \u2014 "),mG=a("a"),unr=o("TFBartForConditionalGeneration"),bnr=o(" (BART model)"),vnr=l(),PT=a("li"),hhe=a("strong"),Tnr=o("blenderbot"),Fnr=o(" \u2014 "),gG=a("a"),Cnr=o("TFBlenderbotForConditionalGeneration"),Mnr=o(" (Blenderbot model)"),Enr=l(),$T=a("li"),phe=a("strong"),ynr=o("blenderbot-small"),wnr=o(" \u2014 "),hG=a("a"),Anr=o("TFBlenderbotSmallForConditionalGeneration"),Lnr=o(" (BlenderbotSmall model)"),Bnr=l(),IT=a("li"),_he=a("strong"),xnr=o("encoder-decoder"),knr=o(" \u2014 "),pG=a("a"),Rnr=o("TFEncoderDecoderModel"),Snr=o(" (Encoder decoder model)"),Pnr=l(),DT=a("li"),uhe=a("strong"),$nr=o("led"),Inr=o(" \u2014 "),_G=a("a"),Dnr=o("TFLEDForConditionalGeneration"),jnr=o(" (LED model)"),Nnr=l(),jT=a("li"),bhe=a("strong"),qnr=o("marian"),Gnr=o(" \u2014 "),uG=a("a"),Onr=o("TFMarianMTModel"),Xnr=o(" (Marian model)"),Vnr=l(),NT=a("li"),vhe=a("strong"),znr=o("mbart"),Wnr=o(" \u2014 "),bG=a("a"),Qnr=o("TFMBartForConditionalGeneration"),Hnr=o(" (mBART model)"),Unr=l(),qT=a("li"),The=a("strong"),Jnr=o("mt5"),Ynr=o(" \u2014 "),vG=a("a"),Knr=o("TFMT5ForConditionalGeneration"),Znr=o(" (mT5 model)"),esr=l(),GT=a("li"),Fhe=a("strong"),osr=o("pegasus"),rsr=o(" \u2014 "),TG=a("a"),tsr=o("TFPegasusForConditionalGeneration"),asr=o(" (Pegasus model)"),nsr=l(),OT=a("li"),Che=a("strong"),ssr=o("t5"),lsr=o(" \u2014 "),FG=a("a"),isr=o("TFT5ForConditionalGeneration"),dsr=o(" (T5 model)"),csr=l(),Mhe=a("p"),fsr=o("Examples:"),msr=l(),f(zw.$$.fragment),hxe=l(),vc=a("h2"),XT=a("a"),Ehe=a("span"),f(Ww.$$.fragment),gsr=l(),yhe=a("span"),hsr=o("TFAutoModelForSequenceClassification"),pxe=l(),Mr=a("div"),f(Qw.$$.fragment),psr=l(),Tc=a("p"),_sr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),whe=a("code"),usr=o("from_pretrained()"),bsr=o("class method or the "),Ahe=a("code"),vsr=o("from_config()"),Tsr=o(`class
method.`),Fsr=l(),Hw=a("p"),Csr=o("This class cannot be instantiated directly using "),Lhe=a("code"),Msr=o("__init__()"),Esr=o(" (throws an error)."),ysr=l(),_t=a("div"),f(Uw.$$.fragment),wsr=l(),Bhe=a("p"),Asr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lsr=l(),Fc=a("p"),Bsr=o(`Note:
Loading a model from its configuration file does `),xhe=a("strong"),xsr=o("not"),ksr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),khe=a("code"),Rsr=o("from_pretrained()"),Ssr=o("to load the model weights."),Psr=l(),Rhe=a("p"),$sr=o("Examples:"),Isr=l(),f(Jw.$$.fragment),Dsr=l(),vo=a("div"),f(Yw.$$.fragment),jsr=l(),She=a("p"),Nsr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),qsr=l(),_n=a("p"),Gsr=o("The model class to instantiate is selected based on the "),Phe=a("code"),Osr=o("model_type"),Xsr=o(` property of the config object (either
passed as an argument or loaded from `),$he=a("code"),Vsr=o("pretrained_model_name_or_path"),zsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ihe=a("code"),Wsr=o("pretrained_model_name_or_path"),Qsr=o(":"),Hsr=l(),X=a("ul"),VT=a("li"),Dhe=a("strong"),Usr=o("albert"),Jsr=o(" \u2014 "),CG=a("a"),Ysr=o("TFAlbertForSequenceClassification"),Ksr=o(" (ALBERT model)"),Zsr=l(),zT=a("li"),jhe=a("strong"),elr=o("bert"),olr=o(" \u2014 "),MG=a("a"),rlr=o("TFBertForSequenceClassification"),tlr=o(" (BERT model)"),alr=l(),WT=a("li"),Nhe=a("strong"),nlr=o("camembert"),slr=o(" \u2014 "),EG=a("a"),llr=o("TFCamembertForSequenceClassification"),ilr=o(" (CamemBERT model)"),dlr=l(),QT=a("li"),qhe=a("strong"),clr=o("convbert"),flr=o(" \u2014 "),yG=a("a"),mlr=o("TFConvBertForSequenceClassification"),glr=o(" (ConvBERT model)"),hlr=l(),HT=a("li"),Ghe=a("strong"),plr=o("ctrl"),_lr=o(" \u2014 "),wG=a("a"),ulr=o("TFCTRLForSequenceClassification"),blr=o(" (CTRL model)"),vlr=l(),UT=a("li"),Ohe=a("strong"),Tlr=o("deberta"),Flr=o(" \u2014 "),AG=a("a"),Clr=o("TFDebertaForSequenceClassification"),Mlr=o(" (DeBERTa model)"),Elr=l(),JT=a("li"),Xhe=a("strong"),ylr=o("deberta-v2"),wlr=o(" \u2014 "),LG=a("a"),Alr=o("TFDebertaV2ForSequenceClassification"),Llr=o(" (DeBERTa-v2 model)"),Blr=l(),YT=a("li"),Vhe=a("strong"),xlr=o("distilbert"),klr=o(" \u2014 "),BG=a("a"),Rlr=o("TFDistilBertForSequenceClassification"),Slr=o(" (DistilBERT model)"),Plr=l(),KT=a("li"),zhe=a("strong"),$lr=o("electra"),Ilr=o(" \u2014 "),xG=a("a"),Dlr=o("TFElectraForSequenceClassification"),jlr=o(" (ELECTRA model)"),Nlr=l(),ZT=a("li"),Whe=a("strong"),qlr=o("flaubert"),Glr=o(" \u2014 "),kG=a("a"),Olr=o("TFFlaubertForSequenceClassification"),Xlr=o(" (FlauBERT model)"),Vlr=l(),eF=a("li"),Qhe=a("strong"),zlr=o("funnel"),Wlr=o(" \u2014 "),RG=a("a"),Qlr=o("TFFunnelForSequenceClassification"),Hlr=o(" (Funnel Transformer model)"),Ulr=l(),oF=a("li"),Hhe=a("strong"),Jlr=o("gpt2"),Ylr=o(" \u2014 "),SG=a("a"),Klr=o("TFGPT2ForSequenceClassification"),Zlr=o(" (OpenAI GPT-2 model)"),eir=l(),rF=a("li"),Uhe=a("strong"),oir=o("layoutlm"),rir=o(" \u2014 "),PG=a("a"),tir=o("TFLayoutLMForSequenceClassification"),air=o(" (LayoutLM model)"),nir=l(),tF=a("li"),Jhe=a("strong"),sir=o("longformer"),lir=o(" \u2014 "),$G=a("a"),iir=o("TFLongformerForSequenceClassification"),dir=o(" (Longformer model)"),cir=l(),aF=a("li"),Yhe=a("strong"),fir=o("mobilebert"),mir=o(" \u2014 "),IG=a("a"),gir=o("TFMobileBertForSequenceClassification"),hir=o(" (MobileBERT model)"),pir=l(),nF=a("li"),Khe=a("strong"),_ir=o("mpnet"),uir=o(" \u2014 "),DG=a("a"),bir=o("TFMPNetForSequenceClassification"),vir=o(" (MPNet model)"),Tir=l(),sF=a("li"),Zhe=a("strong"),Fir=o("openai-gpt"),Cir=o(" \u2014 "),jG=a("a"),Mir=o("TFOpenAIGPTForSequenceClassification"),Eir=o(" (OpenAI GPT model)"),yir=l(),lF=a("li"),epe=a("strong"),wir=o("rembert"),Air=o(" \u2014 "),NG=a("a"),Lir=o("TFRemBertForSequenceClassification"),Bir=o(" (RemBERT model)"),xir=l(),iF=a("li"),ope=a("strong"),kir=o("roberta"),Rir=o(" \u2014 "),qG=a("a"),Sir=o("TFRobertaForSequenceClassification"),Pir=o(" (RoBERTa model)"),$ir=l(),dF=a("li"),rpe=a("strong"),Iir=o("roformer"),Dir=o(" \u2014 "),GG=a("a"),jir=o("TFRoFormerForSequenceClassification"),Nir=o(" (RoFormer model)"),qir=l(),cF=a("li"),tpe=a("strong"),Gir=o("tapas"),Oir=o(" \u2014 "),OG=a("a"),Xir=o("TFTapasForSequenceClassification"),Vir=o(" (TAPAS model)"),zir=l(),fF=a("li"),ape=a("strong"),Wir=o("transfo-xl"),Qir=o(" \u2014 "),XG=a("a"),Hir=o("TFTransfoXLForSequenceClassification"),Uir=o(" (Transformer-XL model)"),Jir=l(),mF=a("li"),npe=a("strong"),Yir=o("xlm"),Kir=o(" \u2014 "),VG=a("a"),Zir=o("TFXLMForSequenceClassification"),edr=o(" (XLM model)"),odr=l(),gF=a("li"),spe=a("strong"),rdr=o("xlm-roberta"),tdr=o(" \u2014 "),zG=a("a"),adr=o("TFXLMRobertaForSequenceClassification"),ndr=o(" (XLM-RoBERTa model)"),sdr=l(),hF=a("li"),lpe=a("strong"),ldr=o("xlnet"),idr=o(" \u2014 "),WG=a("a"),ddr=o("TFXLNetForSequenceClassification"),cdr=o(" (XLNet model)"),fdr=l(),ipe=a("p"),mdr=o("Examples:"),gdr=l(),f(Kw.$$.fragment),_xe=l(),Cc=a("h2"),pF=a("a"),dpe=a("span"),f(Zw.$$.fragment),hdr=l(),cpe=a("span"),pdr=o("TFAutoModelForMultipleChoice"),uxe=l(),Er=a("div"),f(e6.$$.fragment),_dr=l(),Mc=a("p"),udr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),fpe=a("code"),bdr=o("from_pretrained()"),vdr=o("class method or the "),mpe=a("code"),Tdr=o("from_config()"),Fdr=o(`class
method.`),Cdr=l(),o6=a("p"),Mdr=o("This class cannot be instantiated directly using "),gpe=a("code"),Edr=o("__init__()"),ydr=o(" (throws an error)."),wdr=l(),ut=a("div"),f(r6.$$.fragment),Adr=l(),hpe=a("p"),Ldr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Bdr=l(),Ec=a("p"),xdr=o(`Note:
Loading a model from its configuration file does `),ppe=a("strong"),kdr=o("not"),Rdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_pe=a("code"),Sdr=o("from_pretrained()"),Pdr=o("to load the model weights."),$dr=l(),upe=a("p"),Idr=o("Examples:"),Ddr=l(),f(t6.$$.fragment),jdr=l(),To=a("div"),f(a6.$$.fragment),Ndr=l(),bpe=a("p"),qdr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Gdr=l(),un=a("p"),Odr=o("The model class to instantiate is selected based on the "),vpe=a("code"),Xdr=o("model_type"),Vdr=o(` property of the config object (either
passed as an argument or loaded from `),Tpe=a("code"),zdr=o("pretrained_model_name_or_path"),Wdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fpe=a("code"),Qdr=o("pretrained_model_name_or_path"),Hdr=o(":"),Udr=l(),te=a("ul"),_F=a("li"),Cpe=a("strong"),Jdr=o("albert"),Ydr=o(" \u2014 "),QG=a("a"),Kdr=o("TFAlbertForMultipleChoice"),Zdr=o(" (ALBERT model)"),ecr=l(),uF=a("li"),Mpe=a("strong"),ocr=o("bert"),rcr=o(" \u2014 "),HG=a("a"),tcr=o("TFBertForMultipleChoice"),acr=o(" (BERT model)"),ncr=l(),bF=a("li"),Epe=a("strong"),scr=o("camembert"),lcr=o(" \u2014 "),UG=a("a"),icr=o("TFCamembertForMultipleChoice"),dcr=o(" (CamemBERT model)"),ccr=l(),vF=a("li"),ype=a("strong"),fcr=o("convbert"),mcr=o(" \u2014 "),JG=a("a"),gcr=o("TFConvBertForMultipleChoice"),hcr=o(" (ConvBERT model)"),pcr=l(),TF=a("li"),wpe=a("strong"),_cr=o("distilbert"),ucr=o(" \u2014 "),YG=a("a"),bcr=o("TFDistilBertForMultipleChoice"),vcr=o(" (DistilBERT model)"),Tcr=l(),FF=a("li"),Ape=a("strong"),Fcr=o("electra"),Ccr=o(" \u2014 "),KG=a("a"),Mcr=o("TFElectraForMultipleChoice"),Ecr=o(" (ELECTRA model)"),ycr=l(),CF=a("li"),Lpe=a("strong"),wcr=o("flaubert"),Acr=o(" \u2014 "),ZG=a("a"),Lcr=o("TFFlaubertForMultipleChoice"),Bcr=o(" (FlauBERT model)"),xcr=l(),MF=a("li"),Bpe=a("strong"),kcr=o("funnel"),Rcr=o(" \u2014 "),eO=a("a"),Scr=o("TFFunnelForMultipleChoice"),Pcr=o(" (Funnel Transformer model)"),$cr=l(),EF=a("li"),xpe=a("strong"),Icr=o("longformer"),Dcr=o(" \u2014 "),oO=a("a"),jcr=o("TFLongformerForMultipleChoice"),Ncr=o(" (Longformer model)"),qcr=l(),yF=a("li"),kpe=a("strong"),Gcr=o("mobilebert"),Ocr=o(" \u2014 "),rO=a("a"),Xcr=o("TFMobileBertForMultipleChoice"),Vcr=o(" (MobileBERT model)"),zcr=l(),wF=a("li"),Rpe=a("strong"),Wcr=o("mpnet"),Qcr=o(" \u2014 "),tO=a("a"),Hcr=o("TFMPNetForMultipleChoice"),Ucr=o(" (MPNet model)"),Jcr=l(),AF=a("li"),Spe=a("strong"),Ycr=o("rembert"),Kcr=o(" \u2014 "),aO=a("a"),Zcr=o("TFRemBertForMultipleChoice"),efr=o(" (RemBERT model)"),ofr=l(),LF=a("li"),Ppe=a("strong"),rfr=o("roberta"),tfr=o(" \u2014 "),nO=a("a"),afr=o("TFRobertaForMultipleChoice"),nfr=o(" (RoBERTa model)"),sfr=l(),BF=a("li"),$pe=a("strong"),lfr=o("roformer"),ifr=o(" \u2014 "),sO=a("a"),dfr=o("TFRoFormerForMultipleChoice"),cfr=o(" (RoFormer model)"),ffr=l(),xF=a("li"),Ipe=a("strong"),mfr=o("xlm"),gfr=o(" \u2014 "),lO=a("a"),hfr=o("TFXLMForMultipleChoice"),pfr=o(" (XLM model)"),_fr=l(),kF=a("li"),Dpe=a("strong"),ufr=o("xlm-roberta"),bfr=o(" \u2014 "),iO=a("a"),vfr=o("TFXLMRobertaForMultipleChoice"),Tfr=o(" (XLM-RoBERTa model)"),Ffr=l(),RF=a("li"),jpe=a("strong"),Cfr=o("xlnet"),Mfr=o(" \u2014 "),dO=a("a"),Efr=o("TFXLNetForMultipleChoice"),yfr=o(" (XLNet model)"),wfr=l(),Npe=a("p"),Afr=o("Examples:"),Lfr=l(),f(n6.$$.fragment),bxe=l(),yc=a("h2"),SF=a("a"),qpe=a("span"),f(s6.$$.fragment),Bfr=l(),Gpe=a("span"),xfr=o("TFAutoModelForTableQuestionAnswering"),vxe=l(),yr=a("div"),f(l6.$$.fragment),kfr=l(),wc=a("p"),Rfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Ope=a("code"),Sfr=o("from_pretrained()"),Pfr=o("class method or the "),Xpe=a("code"),$fr=o("from_config()"),Ifr=o(`class
method.`),Dfr=l(),i6=a("p"),jfr=o("This class cannot be instantiated directly using "),Vpe=a("code"),Nfr=o("__init__()"),qfr=o(" (throws an error)."),Gfr=l(),bt=a("div"),f(d6.$$.fragment),Ofr=l(),zpe=a("p"),Xfr=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Vfr=l(),Ac=a("p"),zfr=o(`Note:
Loading a model from its configuration file does `),Wpe=a("strong"),Wfr=o("not"),Qfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Qpe=a("code"),Hfr=o("from_pretrained()"),Ufr=o("to load the model weights."),Jfr=l(),Hpe=a("p"),Yfr=o("Examples:"),Kfr=l(),f(c6.$$.fragment),Zfr=l(),Fo=a("div"),f(f6.$$.fragment),emr=l(),Upe=a("p"),omr=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),rmr=l(),bn=a("p"),tmr=o("The model class to instantiate is selected based on the "),Jpe=a("code"),amr=o("model_type"),nmr=o(` property of the config object (either
passed as an argument or loaded from `),Ype=a("code"),smr=o("pretrained_model_name_or_path"),lmr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Kpe=a("code"),imr=o("pretrained_model_name_or_path"),dmr=o(":"),cmr=l(),Zpe=a("ul"),PF=a("li"),e_e=a("strong"),fmr=o("tapas"),mmr=o(" \u2014 "),cO=a("a"),gmr=o("TFTapasForQuestionAnswering"),hmr=o(" (TAPAS model)"),pmr=l(),o_e=a("p"),_mr=o("Examples:"),umr=l(),f(m6.$$.fragment),Txe=l(),Lc=a("h2"),$F=a("a"),r_e=a("span"),f(g6.$$.fragment),bmr=l(),t_e=a("span"),vmr=o("TFAutoModelForTokenClassification"),Fxe=l(),wr=a("div"),f(h6.$$.fragment),Tmr=l(),Bc=a("p"),Fmr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),a_e=a("code"),Cmr=o("from_pretrained()"),Mmr=o("class method or the "),n_e=a("code"),Emr=o("from_config()"),ymr=o(`class
method.`),wmr=l(),p6=a("p"),Amr=o("This class cannot be instantiated directly using "),s_e=a("code"),Lmr=o("__init__()"),Bmr=o(" (throws an error)."),xmr=l(),vt=a("div"),f(_6.$$.fragment),kmr=l(),l_e=a("p"),Rmr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Smr=l(),xc=a("p"),Pmr=o(`Note:
Loading a model from its configuration file does `),i_e=a("strong"),$mr=o("not"),Imr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),d_e=a("code"),Dmr=o("from_pretrained()"),jmr=o("to load the model weights."),Nmr=l(),c_e=a("p"),qmr=o("Examples:"),Gmr=l(),f(u6.$$.fragment),Omr=l(),Co=a("div"),f(b6.$$.fragment),Xmr=l(),f_e=a("p"),Vmr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),zmr=l(),vn=a("p"),Wmr=o("The model class to instantiate is selected based on the "),m_e=a("code"),Qmr=o("model_type"),Hmr=o(` property of the config object (either
passed as an argument or loaded from `),g_e=a("code"),Umr=o("pretrained_model_name_or_path"),Jmr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h_e=a("code"),Ymr=o("pretrained_model_name_or_path"),Kmr=o(":"),Zmr=l(),K=a("ul"),IF=a("li"),p_e=a("strong"),egr=o("albert"),ogr=o(" \u2014 "),fO=a("a"),rgr=o("TFAlbertForTokenClassification"),tgr=o(" (ALBERT model)"),agr=l(),DF=a("li"),__e=a("strong"),ngr=o("bert"),sgr=o(" \u2014 "),mO=a("a"),lgr=o("TFBertForTokenClassification"),igr=o(" (BERT model)"),dgr=l(),jF=a("li"),u_e=a("strong"),cgr=o("camembert"),fgr=o(" \u2014 "),gO=a("a"),mgr=o("TFCamembertForTokenClassification"),ggr=o(" (CamemBERT model)"),hgr=l(),NF=a("li"),b_e=a("strong"),pgr=o("convbert"),_gr=o(" \u2014 "),hO=a("a"),ugr=o("TFConvBertForTokenClassification"),bgr=o(" (ConvBERT model)"),vgr=l(),qF=a("li"),v_e=a("strong"),Tgr=o("deberta"),Fgr=o(" \u2014 "),pO=a("a"),Cgr=o("TFDebertaForTokenClassification"),Mgr=o(" (DeBERTa model)"),Egr=l(),GF=a("li"),T_e=a("strong"),ygr=o("deberta-v2"),wgr=o(" \u2014 "),_O=a("a"),Agr=o("TFDebertaV2ForTokenClassification"),Lgr=o(" (DeBERTa-v2 model)"),Bgr=l(),OF=a("li"),F_e=a("strong"),xgr=o("distilbert"),kgr=o(" \u2014 "),uO=a("a"),Rgr=o("TFDistilBertForTokenClassification"),Sgr=o(" (DistilBERT model)"),Pgr=l(),XF=a("li"),C_e=a("strong"),$gr=o("electra"),Igr=o(" \u2014 "),bO=a("a"),Dgr=o("TFElectraForTokenClassification"),jgr=o(" (ELECTRA model)"),Ngr=l(),VF=a("li"),M_e=a("strong"),qgr=o("flaubert"),Ggr=o(" \u2014 "),vO=a("a"),Ogr=o("TFFlaubertForTokenClassification"),Xgr=o(" (FlauBERT model)"),Vgr=l(),zF=a("li"),E_e=a("strong"),zgr=o("funnel"),Wgr=o(" \u2014 "),TO=a("a"),Qgr=o("TFFunnelForTokenClassification"),Hgr=o(" (Funnel Transformer model)"),Ugr=l(),WF=a("li"),y_e=a("strong"),Jgr=o("layoutlm"),Ygr=o(" \u2014 "),FO=a("a"),Kgr=o("TFLayoutLMForTokenClassification"),Zgr=o(" (LayoutLM model)"),ehr=l(),QF=a("li"),w_e=a("strong"),ohr=o("longformer"),rhr=o(" \u2014 "),CO=a("a"),thr=o("TFLongformerForTokenClassification"),ahr=o(" (Longformer model)"),nhr=l(),HF=a("li"),A_e=a("strong"),shr=o("mobilebert"),lhr=o(" \u2014 "),MO=a("a"),ihr=o("TFMobileBertForTokenClassification"),dhr=o(" (MobileBERT model)"),chr=l(),UF=a("li"),L_e=a("strong"),fhr=o("mpnet"),mhr=o(" \u2014 "),EO=a("a"),ghr=o("TFMPNetForTokenClassification"),hhr=o(" (MPNet model)"),phr=l(),JF=a("li"),B_e=a("strong"),_hr=o("rembert"),uhr=o(" \u2014 "),yO=a("a"),bhr=o("TFRemBertForTokenClassification"),vhr=o(" (RemBERT model)"),Thr=l(),YF=a("li"),x_e=a("strong"),Fhr=o("roberta"),Chr=o(" \u2014 "),wO=a("a"),Mhr=o("TFRobertaForTokenClassification"),Ehr=o(" (RoBERTa model)"),yhr=l(),KF=a("li"),k_e=a("strong"),whr=o("roformer"),Ahr=o(" \u2014 "),AO=a("a"),Lhr=o("TFRoFormerForTokenClassification"),Bhr=o(" (RoFormer model)"),xhr=l(),ZF=a("li"),R_e=a("strong"),khr=o("xlm"),Rhr=o(" \u2014 "),LO=a("a"),Shr=o("TFXLMForTokenClassification"),Phr=o(" (XLM model)"),$hr=l(),e9=a("li"),S_e=a("strong"),Ihr=o("xlm-roberta"),Dhr=o(" \u2014 "),BO=a("a"),jhr=o("TFXLMRobertaForTokenClassification"),Nhr=o(" (XLM-RoBERTa model)"),qhr=l(),o9=a("li"),P_e=a("strong"),Ghr=o("xlnet"),Ohr=o(" \u2014 "),xO=a("a"),Xhr=o("TFXLNetForTokenClassification"),Vhr=o(" (XLNet model)"),zhr=l(),$_e=a("p"),Whr=o("Examples:"),Qhr=l(),f(v6.$$.fragment),Cxe=l(),kc=a("h2"),r9=a("a"),I_e=a("span"),f(T6.$$.fragment),Hhr=l(),D_e=a("span"),Uhr=o("TFAutoModelForQuestionAnswering"),Mxe=l(),Ar=a("div"),f(F6.$$.fragment),Jhr=l(),Rc=a("p"),Yhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),j_e=a("code"),Khr=o("from_pretrained()"),Zhr=o("class method or the "),N_e=a("code"),epr=o("from_config()"),opr=o(`class
method.`),rpr=l(),C6=a("p"),tpr=o("This class cannot be instantiated directly using "),q_e=a("code"),apr=o("__init__()"),npr=o(" (throws an error)."),spr=l(),Tt=a("div"),f(M6.$$.fragment),lpr=l(),G_e=a("p"),ipr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),dpr=l(),Sc=a("p"),cpr=o(`Note:
Loading a model from its configuration file does `),O_e=a("strong"),fpr=o("not"),mpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),X_e=a("code"),gpr=o("from_pretrained()"),hpr=o("to load the model weights."),ppr=l(),V_e=a("p"),_pr=o("Examples:"),upr=l(),f(E6.$$.fragment),bpr=l(),Mo=a("div"),f(y6.$$.fragment),vpr=l(),z_e=a("p"),Tpr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Fpr=l(),Tn=a("p"),Cpr=o("The model class to instantiate is selected based on the "),W_e=a("code"),Mpr=o("model_type"),Epr=o(` property of the config object (either
passed as an argument or loaded from `),Q_e=a("code"),ypr=o("pretrained_model_name_or_path"),wpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),H_e=a("code"),Apr=o("pretrained_model_name_or_path"),Lpr=o(":"),Bpr=l(),Z=a("ul"),t9=a("li"),U_e=a("strong"),xpr=o("albert"),kpr=o(" \u2014 "),kO=a("a"),Rpr=o("TFAlbertForQuestionAnswering"),Spr=o(" (ALBERT model)"),Ppr=l(),a9=a("li"),J_e=a("strong"),$pr=o("bert"),Ipr=o(" \u2014 "),RO=a("a"),Dpr=o("TFBertForQuestionAnswering"),jpr=o(" (BERT model)"),Npr=l(),n9=a("li"),Y_e=a("strong"),qpr=o("camembert"),Gpr=o(" \u2014 "),SO=a("a"),Opr=o("TFCamembertForQuestionAnswering"),Xpr=o(" (CamemBERT model)"),Vpr=l(),s9=a("li"),K_e=a("strong"),zpr=o("convbert"),Wpr=o(" \u2014 "),PO=a("a"),Qpr=o("TFConvBertForQuestionAnswering"),Hpr=o(" (ConvBERT model)"),Upr=l(),l9=a("li"),Z_e=a("strong"),Jpr=o("deberta"),Ypr=o(" \u2014 "),$O=a("a"),Kpr=o("TFDebertaForQuestionAnswering"),Zpr=o(" (DeBERTa model)"),e_r=l(),i9=a("li"),eue=a("strong"),o_r=o("deberta-v2"),r_r=o(" \u2014 "),IO=a("a"),t_r=o("TFDebertaV2ForQuestionAnswering"),a_r=o(" (DeBERTa-v2 model)"),n_r=l(),d9=a("li"),oue=a("strong"),s_r=o("distilbert"),l_r=o(" \u2014 "),DO=a("a"),i_r=o("TFDistilBertForQuestionAnswering"),d_r=o(" (DistilBERT model)"),c_r=l(),c9=a("li"),rue=a("strong"),f_r=o("electra"),m_r=o(" \u2014 "),jO=a("a"),g_r=o("TFElectraForQuestionAnswering"),h_r=o(" (ELECTRA model)"),p_r=l(),f9=a("li"),tue=a("strong"),__r=o("flaubert"),u_r=o(" \u2014 "),NO=a("a"),b_r=o("TFFlaubertForQuestionAnsweringSimple"),v_r=o(" (FlauBERT model)"),T_r=l(),m9=a("li"),aue=a("strong"),F_r=o("funnel"),C_r=o(" \u2014 "),qO=a("a"),M_r=o("TFFunnelForQuestionAnswering"),E_r=o(" (Funnel Transformer model)"),y_r=l(),g9=a("li"),nue=a("strong"),w_r=o("longformer"),A_r=o(" \u2014 "),GO=a("a"),L_r=o("TFLongformerForQuestionAnswering"),B_r=o(" (Longformer model)"),x_r=l(),h9=a("li"),sue=a("strong"),k_r=o("mobilebert"),R_r=o(" \u2014 "),OO=a("a"),S_r=o("TFMobileBertForQuestionAnswering"),P_r=o(" (MobileBERT model)"),$_r=l(),p9=a("li"),lue=a("strong"),I_r=o("mpnet"),D_r=o(" \u2014 "),XO=a("a"),j_r=o("TFMPNetForQuestionAnswering"),N_r=o(" (MPNet model)"),q_r=l(),_9=a("li"),iue=a("strong"),G_r=o("rembert"),O_r=o(" \u2014 "),VO=a("a"),X_r=o("TFRemBertForQuestionAnswering"),V_r=o(" (RemBERT model)"),z_r=l(),u9=a("li"),due=a("strong"),W_r=o("roberta"),Q_r=o(" \u2014 "),zO=a("a"),H_r=o("TFRobertaForQuestionAnswering"),U_r=o(" (RoBERTa model)"),J_r=l(),b9=a("li"),cue=a("strong"),Y_r=o("roformer"),K_r=o(" \u2014 "),WO=a("a"),Z_r=o("TFRoFormerForQuestionAnswering"),eur=o(" (RoFormer model)"),our=l(),v9=a("li"),fue=a("strong"),rur=o("xlm"),tur=o(" \u2014 "),QO=a("a"),aur=o("TFXLMForQuestionAnsweringSimple"),nur=o(" (XLM model)"),sur=l(),T9=a("li"),mue=a("strong"),lur=o("xlm-roberta"),iur=o(" \u2014 "),HO=a("a"),dur=o("TFXLMRobertaForQuestionAnswering"),cur=o(" (XLM-RoBERTa model)"),fur=l(),F9=a("li"),gue=a("strong"),mur=o("xlnet"),gur=o(" \u2014 "),UO=a("a"),hur=o("TFXLNetForQuestionAnsweringSimple"),pur=o(" (XLNet model)"),_ur=l(),hue=a("p"),uur=o("Examples:"),bur=l(),f(w6.$$.fragment),Exe=l(),Pc=a("h2"),C9=a("a"),pue=a("span"),f(A6.$$.fragment),vur=l(),_ue=a("span"),Tur=o("TFAutoModelForVision2Seq"),yxe=l(),Lr=a("div"),f(L6.$$.fragment),Fur=l(),$c=a("p"),Cur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),uue=a("code"),Mur=o("from_pretrained()"),Eur=o("class method or the "),bue=a("code"),yur=o("from_config()"),wur=o(`class
method.`),Aur=l(),B6=a("p"),Lur=o("This class cannot be instantiated directly using "),vue=a("code"),Bur=o("__init__()"),xur=o(" (throws an error)."),kur=l(),Ft=a("div"),f(x6.$$.fragment),Rur=l(),Tue=a("p"),Sur=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Pur=l(),Ic=a("p"),$ur=o(`Note:
Loading a model from its configuration file does `),Fue=a("strong"),Iur=o("not"),Dur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cue=a("code"),jur=o("from_pretrained()"),Nur=o("to load the model weights."),qur=l(),Mue=a("p"),Gur=o("Examples:"),Our=l(),f(k6.$$.fragment),Xur=l(),Eo=a("div"),f(R6.$$.fragment),Vur=l(),Eue=a("p"),zur=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Wur=l(),Fn=a("p"),Qur=o("The model class to instantiate is selected based on the "),yue=a("code"),Hur=o("model_type"),Uur=o(` property of the config object (either
passed as an argument or loaded from `),wue=a("code"),Jur=o("pretrained_model_name_or_path"),Yur=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aue=a("code"),Kur=o("pretrained_model_name_or_path"),Zur=o(":"),e0r=l(),Lue=a("ul"),M9=a("li"),Bue=a("strong"),o0r=o("vision-encoder-decoder"),r0r=o(" \u2014 "),JO=a("a"),t0r=o("TFVisionEncoderDecoderModel"),a0r=o(" (Vision Encoder decoder model)"),n0r=l(),xue=a("p"),s0r=o("Examples:"),l0r=l(),f(S6.$$.fragment),wxe=l(),Dc=a("h2"),E9=a("a"),kue=a("span"),f(P6.$$.fragment),i0r=l(),Rue=a("span"),d0r=o("TFAutoModelForSpeechSeq2Seq"),Axe=l(),Br=a("div"),f($6.$$.fragment),c0r=l(),jc=a("p"),f0r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Sue=a("code"),m0r=o("from_pretrained()"),g0r=o("class method or the "),Pue=a("code"),h0r=o("from_config()"),p0r=o(`class
method.`),_0r=l(),I6=a("p"),u0r=o("This class cannot be instantiated directly using "),$ue=a("code"),b0r=o("__init__()"),v0r=o(" (throws an error)."),T0r=l(),Ct=a("div"),f(D6.$$.fragment),F0r=l(),Iue=a("p"),C0r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),M0r=l(),Nc=a("p"),E0r=o(`Note:
Loading a model from its configuration file does `),Due=a("strong"),y0r=o("not"),w0r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jue=a("code"),A0r=o("from_pretrained()"),L0r=o("to load the model weights."),B0r=l(),Nue=a("p"),x0r=o("Examples:"),k0r=l(),f(j6.$$.fragment),R0r=l(),yo=a("div"),f(N6.$$.fragment),S0r=l(),que=a("p"),P0r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),$0r=l(),Cn=a("p"),I0r=o("The model class to instantiate is selected based on the "),Gue=a("code"),D0r=o("model_type"),j0r=o(` property of the config object (either
passed as an argument or loaded from `),Oue=a("code"),N0r=o("pretrained_model_name_or_path"),q0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xue=a("code"),G0r=o("pretrained_model_name_or_path"),O0r=o(":"),X0r=l(),Vue=a("ul"),y9=a("li"),zue=a("strong"),V0r=o("speech_to_text"),z0r=o(" \u2014 "),YO=a("a"),W0r=o("TFSpeech2TextForConditionalGeneration"),Q0r=o(" (Speech2Text model)"),H0r=l(),Wue=a("p"),U0r=o("Examples:"),J0r=l(),f(q6.$$.fragment),Lxe=l(),qc=a("h2"),w9=a("a"),Que=a("span"),f(G6.$$.fragment),Y0r=l(),Hue=a("span"),K0r=o("FlaxAutoModel"),Bxe=l(),xr=a("div"),f(O6.$$.fragment),Z0r=l(),Gc=a("p"),e1r=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Uue=a("code"),o1r=o("from_pretrained()"),r1r=o("class method or the "),Jue=a("code"),t1r=o("from_config()"),a1r=o(`class
method.`),n1r=l(),X6=a("p"),s1r=o("This class cannot be instantiated directly using "),Yue=a("code"),l1r=o("__init__()"),i1r=o(" (throws an error)."),d1r=l(),Mt=a("div"),f(V6.$$.fragment),c1r=l(),Kue=a("p"),f1r=o("Instantiates one of the base model classes of the library from a configuration."),m1r=l(),Oc=a("p"),g1r=o(`Note:
Loading a model from its configuration file does `),Zue=a("strong"),h1r=o("not"),p1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),e0e=a("code"),_1r=o("from_pretrained()"),u1r=o("to load the model weights."),b1r=l(),o0e=a("p"),v1r=o("Examples:"),T1r=l(),f(z6.$$.fragment),F1r=l(),wo=a("div"),f(W6.$$.fragment),C1r=l(),r0e=a("p"),M1r=o("Instantiate one of the base model classes of the library from a pretrained model."),E1r=l(),Mn=a("p"),y1r=o("The model class to instantiate is selected based on the "),t0e=a("code"),w1r=o("model_type"),A1r=o(` property of the config object (either
passed as an argument or loaded from `),a0e=a("code"),L1r=o("pretrained_model_name_or_path"),B1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),n0e=a("code"),x1r=o("pretrained_model_name_or_path"),k1r=o(":"),R1r=l(),V=a("ul"),A9=a("li"),s0e=a("strong"),S1r=o("albert"),P1r=o(" \u2014 "),KO=a("a"),$1r=o("FlaxAlbertModel"),I1r=o(" (ALBERT model)"),D1r=l(),L9=a("li"),l0e=a("strong"),j1r=o("bart"),N1r=o(" \u2014 "),ZO=a("a"),q1r=o("FlaxBartModel"),G1r=o(" (BART model)"),O1r=l(),B9=a("li"),i0e=a("strong"),X1r=o("beit"),V1r=o(" \u2014 "),eX=a("a"),z1r=o("FlaxBeitModel"),W1r=o(" (BEiT model)"),Q1r=l(),x9=a("li"),d0e=a("strong"),H1r=o("bert"),U1r=o(" \u2014 "),oX=a("a"),J1r=o("FlaxBertModel"),Y1r=o(" (BERT model)"),K1r=l(),k9=a("li"),c0e=a("strong"),Z1r=o("big_bird"),ebr=o(" \u2014 "),rX=a("a"),obr=o("FlaxBigBirdModel"),rbr=o(" (BigBird model)"),tbr=l(),R9=a("li"),f0e=a("strong"),abr=o("blenderbot"),nbr=o(" \u2014 "),tX=a("a"),sbr=o("FlaxBlenderbotModel"),lbr=o(" (Blenderbot model)"),ibr=l(),S9=a("li"),m0e=a("strong"),dbr=o("blenderbot-small"),cbr=o(" \u2014 "),aX=a("a"),fbr=o("FlaxBlenderbotSmallModel"),mbr=o(" (BlenderbotSmall model)"),gbr=l(),P9=a("li"),g0e=a("strong"),hbr=o("clip"),pbr=o(" \u2014 "),nX=a("a"),_br=o("FlaxCLIPModel"),ubr=o(" (CLIP model)"),bbr=l(),$9=a("li"),h0e=a("strong"),vbr=o("distilbert"),Tbr=o(" \u2014 "),sX=a("a"),Fbr=o("FlaxDistilBertModel"),Cbr=o(" (DistilBERT model)"),Mbr=l(),I9=a("li"),p0e=a("strong"),Ebr=o("electra"),ybr=o(" \u2014 "),lX=a("a"),wbr=o("FlaxElectraModel"),Abr=o(" (ELECTRA model)"),Lbr=l(),D9=a("li"),_0e=a("strong"),Bbr=o("gpt2"),xbr=o(" \u2014 "),iX=a("a"),kbr=o("FlaxGPT2Model"),Rbr=o(" (OpenAI GPT-2 model)"),Sbr=l(),j9=a("li"),u0e=a("strong"),Pbr=o("gpt_neo"),$br=o(" \u2014 "),dX=a("a"),Ibr=o("FlaxGPTNeoModel"),Dbr=o(" (GPT Neo model)"),jbr=l(),N9=a("li"),b0e=a("strong"),Nbr=o("gptj"),qbr=o(" \u2014 "),cX=a("a"),Gbr=o("FlaxGPTJModel"),Obr=o(" (GPT-J model)"),Xbr=l(),q9=a("li"),v0e=a("strong"),Vbr=o("marian"),zbr=o(" \u2014 "),fX=a("a"),Wbr=o("FlaxMarianModel"),Qbr=o(" (Marian model)"),Hbr=l(),G9=a("li"),T0e=a("strong"),Ubr=o("mbart"),Jbr=o(" \u2014 "),mX=a("a"),Ybr=o("FlaxMBartModel"),Kbr=o(" (mBART model)"),Zbr=l(),O9=a("li"),F0e=a("strong"),e5r=o("mt5"),o5r=o(" \u2014 "),gX=a("a"),r5r=o("FlaxMT5Model"),t5r=o(" (mT5 model)"),a5r=l(),X9=a("li"),C0e=a("strong"),n5r=o("pegasus"),s5r=o(" \u2014 "),hX=a("a"),l5r=o("FlaxPegasusModel"),i5r=o(" (Pegasus model)"),d5r=l(),V9=a("li"),M0e=a("strong"),c5r=o("roberta"),f5r=o(" \u2014 "),pX=a("a"),m5r=o("FlaxRobertaModel"),g5r=o(" (RoBERTa model)"),h5r=l(),z9=a("li"),E0e=a("strong"),p5r=o("roformer"),_5r=o(" \u2014 "),_X=a("a"),u5r=o("FlaxRoFormerModel"),b5r=o(" (RoFormer model)"),v5r=l(),W9=a("li"),y0e=a("strong"),T5r=o("t5"),F5r=o(" \u2014 "),uX=a("a"),C5r=o("FlaxT5Model"),M5r=o(" (T5 model)"),E5r=l(),Q9=a("li"),w0e=a("strong"),y5r=o("vision-text-dual-encoder"),w5r=o(" \u2014 "),bX=a("a"),A5r=o("FlaxVisionTextDualEncoderModel"),L5r=o(" (VisionTextDualEncoder model)"),B5r=l(),H9=a("li"),A0e=a("strong"),x5r=o("vit"),k5r=o(" \u2014 "),vX=a("a"),R5r=o("FlaxViTModel"),S5r=o(" (ViT model)"),P5r=l(),U9=a("li"),L0e=a("strong"),$5r=o("wav2vec2"),I5r=o(" \u2014 "),TX=a("a"),D5r=o("FlaxWav2Vec2Model"),j5r=o(" (Wav2Vec2 model)"),N5r=l(),J9=a("li"),B0e=a("strong"),q5r=o("xglm"),G5r=o(" \u2014 "),FX=a("a"),O5r=o("FlaxXGLMModel"),X5r=o(" (XGLM model)"),V5r=l(),Y9=a("li"),x0e=a("strong"),z5r=o("xlm-roberta"),W5r=o(" \u2014 "),k0e=a("code"),Q5r=o("FlaxXLMRobertaModel"),H5r=o("(XLM-RoBERTa model)"),U5r=l(),R0e=a("p"),J5r=o("Examples:"),Y5r=l(),f(Q6.$$.fragment),xxe=l(),Xc=a("h2"),K9=a("a"),S0e=a("span"),f(H6.$$.fragment),K5r=l(),P0e=a("span"),Z5r=o("FlaxAutoModelForCausalLM"),kxe=l(),kr=a("div"),f(U6.$$.fragment),e2r=l(),Vc=a("p"),o2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),$0e=a("code"),r2r=o("from_pretrained()"),t2r=o("class method or the "),I0e=a("code"),a2r=o("from_config()"),n2r=o(`class
method.`),s2r=l(),J6=a("p"),l2r=o("This class cannot be instantiated directly using "),D0e=a("code"),i2r=o("__init__()"),d2r=o(" (throws an error)."),c2r=l(),Et=a("div"),f(Y6.$$.fragment),f2r=l(),j0e=a("p"),m2r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),g2r=l(),zc=a("p"),h2r=o(`Note:
Loading a model from its configuration file does `),N0e=a("strong"),p2r=o("not"),_2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),q0e=a("code"),u2r=o("from_pretrained()"),b2r=o("to load the model weights."),v2r=l(),G0e=a("p"),T2r=o("Examples:"),F2r=l(),f(K6.$$.fragment),C2r=l(),Ao=a("div"),f(Z6.$$.fragment),M2r=l(),O0e=a("p"),E2r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),y2r=l(),En=a("p"),w2r=o("The model class to instantiate is selected based on the "),X0e=a("code"),A2r=o("model_type"),L2r=o(` property of the config object (either
passed as an argument or loaded from `),V0e=a("code"),B2r=o("pretrained_model_name_or_path"),x2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),z0e=a("code"),k2r=o("pretrained_model_name_or_path"),R2r=o(":"),S2r=l(),yn=a("ul"),Z9=a("li"),W0e=a("strong"),P2r=o("gpt2"),$2r=o(" \u2014 "),CX=a("a"),I2r=o("FlaxGPT2LMHeadModel"),D2r=o(" (OpenAI GPT-2 model)"),j2r=l(),eC=a("li"),Q0e=a("strong"),N2r=o("gpt_neo"),q2r=o(" \u2014 "),MX=a("a"),G2r=o("FlaxGPTNeoForCausalLM"),O2r=o(" (GPT Neo model)"),X2r=l(),oC=a("li"),H0e=a("strong"),V2r=o("gptj"),z2r=o(" \u2014 "),EX=a("a"),W2r=o("FlaxGPTJForCausalLM"),Q2r=o(" (GPT-J model)"),H2r=l(),rC=a("li"),U0e=a("strong"),U2r=o("xglm"),J2r=o(" \u2014 "),yX=a("a"),Y2r=o("FlaxXGLMForCausalLM"),K2r=o(" (XGLM model)"),Z2r=l(),J0e=a("p"),evr=o("Examples:"),ovr=l(),f(eA.$$.fragment),Rxe=l(),Wc=a("h2"),tC=a("a"),Y0e=a("span"),f(oA.$$.fragment),rvr=l(),K0e=a("span"),tvr=o("FlaxAutoModelForPreTraining"),Sxe=l(),Rr=a("div"),f(rA.$$.fragment),avr=l(),Qc=a("p"),nvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Z0e=a("code"),svr=o("from_pretrained()"),lvr=o("class method or the "),e1e=a("code"),ivr=o("from_config()"),dvr=o(`class
method.`),cvr=l(),tA=a("p"),fvr=o("This class cannot be instantiated directly using "),o1e=a("code"),mvr=o("__init__()"),gvr=o(" (throws an error)."),hvr=l(),yt=a("div"),f(aA.$$.fragment),pvr=l(),r1e=a("p"),_vr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),uvr=l(),Hc=a("p"),bvr=o(`Note:
Loading a model from its configuration file does `),t1e=a("strong"),vvr=o("not"),Tvr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),a1e=a("code"),Fvr=o("from_pretrained()"),Cvr=o("to load the model weights."),Mvr=l(),n1e=a("p"),Evr=o("Examples:"),yvr=l(),f(nA.$$.fragment),wvr=l(),Lo=a("div"),f(sA.$$.fragment),Avr=l(),s1e=a("p"),Lvr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Bvr=l(),wn=a("p"),xvr=o("The model class to instantiate is selected based on the "),l1e=a("code"),kvr=o("model_type"),Rvr=o(` property of the config object (either
passed as an argument or loaded from `),i1e=a("code"),Svr=o("pretrained_model_name_or_path"),Pvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d1e=a("code"),$vr=o("pretrained_model_name_or_path"),Ivr=o(":"),Dvr=l(),ce=a("ul"),aC=a("li"),c1e=a("strong"),jvr=o("albert"),Nvr=o(" \u2014 "),wX=a("a"),qvr=o("FlaxAlbertForPreTraining"),Gvr=o(" (ALBERT model)"),Ovr=l(),nC=a("li"),f1e=a("strong"),Xvr=o("bart"),Vvr=o(" \u2014 "),AX=a("a"),zvr=o("FlaxBartForConditionalGeneration"),Wvr=o(" (BART model)"),Qvr=l(),sC=a("li"),m1e=a("strong"),Hvr=o("bert"),Uvr=o(" \u2014 "),LX=a("a"),Jvr=o("FlaxBertForPreTraining"),Yvr=o(" (BERT model)"),Kvr=l(),lC=a("li"),g1e=a("strong"),Zvr=o("big_bird"),eTr=o(" \u2014 "),BX=a("a"),oTr=o("FlaxBigBirdForPreTraining"),rTr=o(" (BigBird model)"),tTr=l(),iC=a("li"),h1e=a("strong"),aTr=o("electra"),nTr=o(" \u2014 "),xX=a("a"),sTr=o("FlaxElectraForPreTraining"),lTr=o(" (ELECTRA model)"),iTr=l(),dC=a("li"),p1e=a("strong"),dTr=o("mbart"),cTr=o(" \u2014 "),kX=a("a"),fTr=o("FlaxMBartForConditionalGeneration"),mTr=o(" (mBART model)"),gTr=l(),cC=a("li"),_1e=a("strong"),hTr=o("mt5"),pTr=o(" \u2014 "),RX=a("a"),_Tr=o("FlaxMT5ForConditionalGeneration"),uTr=o(" (mT5 model)"),bTr=l(),fC=a("li"),u1e=a("strong"),vTr=o("roberta"),TTr=o(" \u2014 "),SX=a("a"),FTr=o("FlaxRobertaForMaskedLM"),CTr=o(" (RoBERTa model)"),MTr=l(),mC=a("li"),b1e=a("strong"),ETr=o("roformer"),yTr=o(" \u2014 "),PX=a("a"),wTr=o("FlaxRoFormerForMaskedLM"),ATr=o(" (RoFormer model)"),LTr=l(),gC=a("li"),v1e=a("strong"),BTr=o("t5"),xTr=o(" \u2014 "),$X=a("a"),kTr=o("FlaxT5ForConditionalGeneration"),RTr=o(" (T5 model)"),STr=l(),hC=a("li"),T1e=a("strong"),PTr=o("wav2vec2"),$Tr=o(" \u2014 "),IX=a("a"),ITr=o("FlaxWav2Vec2ForPreTraining"),DTr=o(" (Wav2Vec2 model)"),jTr=l(),pC=a("li"),F1e=a("strong"),NTr=o("xlm-roberta"),qTr=o(" \u2014 "),C1e=a("code"),GTr=o("FlaxXLMRobertaForMaskedLM"),OTr=o("(XLM-RoBERTa model)"),XTr=l(),M1e=a("p"),VTr=o("Examples:"),zTr=l(),f(lA.$$.fragment),Pxe=l(),Uc=a("h2"),_C=a("a"),E1e=a("span"),f(iA.$$.fragment),WTr=l(),y1e=a("span"),QTr=o("FlaxAutoModelForMaskedLM"),$xe=l(),Sr=a("div"),f(dA.$$.fragment),HTr=l(),Jc=a("p"),UTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),w1e=a("code"),JTr=o("from_pretrained()"),YTr=o("class method or the "),A1e=a("code"),KTr=o("from_config()"),ZTr=o(`class
method.`),eFr=l(),cA=a("p"),oFr=o("This class cannot be instantiated directly using "),L1e=a("code"),rFr=o("__init__()"),tFr=o(" (throws an error)."),aFr=l(),wt=a("div"),f(fA.$$.fragment),nFr=l(),B1e=a("p"),sFr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),lFr=l(),Yc=a("p"),iFr=o(`Note:
Loading a model from its configuration file does `),x1e=a("strong"),dFr=o("not"),cFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),k1e=a("code"),fFr=o("from_pretrained()"),mFr=o("to load the model weights."),gFr=l(),R1e=a("p"),hFr=o("Examples:"),pFr=l(),f(mA.$$.fragment),_Fr=l(),Bo=a("div"),f(gA.$$.fragment),uFr=l(),S1e=a("p"),bFr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),vFr=l(),An=a("p"),TFr=o("The model class to instantiate is selected based on the "),P1e=a("code"),FFr=o("model_type"),CFr=o(` property of the config object (either
passed as an argument or loaded from `),$1e=a("code"),MFr=o("pretrained_model_name_or_path"),EFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),I1e=a("code"),yFr=o("pretrained_model_name_or_path"),wFr=o(":"),AFr=l(),ue=a("ul"),uC=a("li"),D1e=a("strong"),LFr=o("albert"),BFr=o(" \u2014 "),DX=a("a"),xFr=o("FlaxAlbertForMaskedLM"),kFr=o(" (ALBERT model)"),RFr=l(),bC=a("li"),j1e=a("strong"),SFr=o("bart"),PFr=o(" \u2014 "),jX=a("a"),$Fr=o("FlaxBartForConditionalGeneration"),IFr=o(" (BART model)"),DFr=l(),vC=a("li"),N1e=a("strong"),jFr=o("bert"),NFr=o(" \u2014 "),NX=a("a"),qFr=o("FlaxBertForMaskedLM"),GFr=o(" (BERT model)"),OFr=l(),TC=a("li"),q1e=a("strong"),XFr=o("big_bird"),VFr=o(" \u2014 "),qX=a("a"),zFr=o("FlaxBigBirdForMaskedLM"),WFr=o(" (BigBird model)"),QFr=l(),FC=a("li"),G1e=a("strong"),HFr=o("distilbert"),UFr=o(" \u2014 "),GX=a("a"),JFr=o("FlaxDistilBertForMaskedLM"),YFr=o(" (DistilBERT model)"),KFr=l(),CC=a("li"),O1e=a("strong"),ZFr=o("electra"),e9r=o(" \u2014 "),OX=a("a"),o9r=o("FlaxElectraForMaskedLM"),r9r=o(" (ELECTRA model)"),t9r=l(),MC=a("li"),X1e=a("strong"),a9r=o("mbart"),n9r=o(" \u2014 "),XX=a("a"),s9r=o("FlaxMBartForConditionalGeneration"),l9r=o(" (mBART model)"),i9r=l(),EC=a("li"),V1e=a("strong"),d9r=o("roberta"),c9r=o(" \u2014 "),VX=a("a"),f9r=o("FlaxRobertaForMaskedLM"),m9r=o(" (RoBERTa model)"),g9r=l(),yC=a("li"),z1e=a("strong"),h9r=o("roformer"),p9r=o(" \u2014 "),zX=a("a"),_9r=o("FlaxRoFormerForMaskedLM"),u9r=o(" (RoFormer model)"),b9r=l(),wC=a("li"),W1e=a("strong"),v9r=o("xlm-roberta"),T9r=o(" \u2014 "),Q1e=a("code"),F9r=o("FlaxXLMRobertaForMaskedLM"),C9r=o("(XLM-RoBERTa model)"),M9r=l(),H1e=a("p"),E9r=o("Examples:"),y9r=l(),f(hA.$$.fragment),Ixe=l(),Kc=a("h2"),AC=a("a"),U1e=a("span"),f(pA.$$.fragment),w9r=l(),J1e=a("span"),A9r=o("FlaxAutoModelForSeq2SeqLM"),Dxe=l(),Pr=a("div"),f(_A.$$.fragment),L9r=l(),Zc=a("p"),B9r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Y1e=a("code"),x9r=o("from_pretrained()"),k9r=o("class method or the "),K1e=a("code"),R9r=o("from_config()"),S9r=o(`class
method.`),P9r=l(),uA=a("p"),$9r=o("This class cannot be instantiated directly using "),Z1e=a("code"),I9r=o("__init__()"),D9r=o(" (throws an error)."),j9r=l(),At=a("div"),f(bA.$$.fragment),N9r=l(),ebe=a("p"),q9r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),G9r=l(),ef=a("p"),O9r=o(`Note:
Loading a model from its configuration file does `),obe=a("strong"),X9r=o("not"),V9r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rbe=a("code"),z9r=o("from_pretrained()"),W9r=o("to load the model weights."),Q9r=l(),tbe=a("p"),H9r=o("Examples:"),U9r=l(),f(vA.$$.fragment),J9r=l(),xo=a("div"),f(TA.$$.fragment),Y9r=l(),abe=a("p"),K9r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Z9r=l(),Ln=a("p"),eCr=o("The model class to instantiate is selected based on the "),nbe=a("code"),oCr=o("model_type"),rCr=o(` property of the config object (either
passed as an argument or loaded from `),sbe=a("code"),tCr=o("pretrained_model_name_or_path"),aCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lbe=a("code"),nCr=o("pretrained_model_name_or_path"),sCr=o(":"),lCr=l(),Ce=a("ul"),LC=a("li"),ibe=a("strong"),iCr=o("bart"),dCr=o(" \u2014 "),WX=a("a"),cCr=o("FlaxBartForConditionalGeneration"),fCr=o(" (BART model)"),mCr=l(),BC=a("li"),dbe=a("strong"),gCr=o("blenderbot"),hCr=o(" \u2014 "),QX=a("a"),pCr=o("FlaxBlenderbotForConditionalGeneration"),_Cr=o(" (Blenderbot model)"),uCr=l(),xC=a("li"),cbe=a("strong"),bCr=o("blenderbot-small"),vCr=o(" \u2014 "),HX=a("a"),TCr=o("FlaxBlenderbotSmallForConditionalGeneration"),FCr=o(" (BlenderbotSmall model)"),CCr=l(),kC=a("li"),fbe=a("strong"),MCr=o("encoder-decoder"),ECr=o(" \u2014 "),UX=a("a"),yCr=o("FlaxEncoderDecoderModel"),wCr=o(" (Encoder decoder model)"),ACr=l(),RC=a("li"),mbe=a("strong"),LCr=o("marian"),BCr=o(" \u2014 "),JX=a("a"),xCr=o("FlaxMarianMTModel"),kCr=o(" (Marian model)"),RCr=l(),SC=a("li"),gbe=a("strong"),SCr=o("mbart"),PCr=o(" \u2014 "),YX=a("a"),$Cr=o("FlaxMBartForConditionalGeneration"),ICr=o(" (mBART model)"),DCr=l(),PC=a("li"),hbe=a("strong"),jCr=o("mt5"),NCr=o(" \u2014 "),KX=a("a"),qCr=o("FlaxMT5ForConditionalGeneration"),GCr=o(" (mT5 model)"),OCr=l(),$C=a("li"),pbe=a("strong"),XCr=o("pegasus"),VCr=o(" \u2014 "),ZX=a("a"),zCr=o("FlaxPegasusForConditionalGeneration"),WCr=o(" (Pegasus model)"),QCr=l(),IC=a("li"),_be=a("strong"),HCr=o("t5"),UCr=o(" \u2014 "),eV=a("a"),JCr=o("FlaxT5ForConditionalGeneration"),YCr=o(" (T5 model)"),KCr=l(),ube=a("p"),ZCr=o("Examples:"),eMr=l(),f(FA.$$.fragment),jxe=l(),of=a("h2"),DC=a("a"),bbe=a("span"),f(CA.$$.fragment),oMr=l(),vbe=a("span"),rMr=o("FlaxAutoModelForSequenceClassification"),Nxe=l(),$r=a("div"),f(MA.$$.fragment),tMr=l(),rf=a("p"),aMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Tbe=a("code"),nMr=o("from_pretrained()"),sMr=o("class method or the "),Fbe=a("code"),lMr=o("from_config()"),iMr=o(`class
method.`),dMr=l(),EA=a("p"),cMr=o("This class cannot be instantiated directly using "),Cbe=a("code"),fMr=o("__init__()"),mMr=o(" (throws an error)."),gMr=l(),Lt=a("div"),f(yA.$$.fragment),hMr=l(),Mbe=a("p"),pMr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),_Mr=l(),tf=a("p"),uMr=o(`Note:
Loading a model from its configuration file does `),Ebe=a("strong"),bMr=o("not"),vMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ybe=a("code"),TMr=o("from_pretrained()"),FMr=o("to load the model weights."),CMr=l(),wbe=a("p"),MMr=o("Examples:"),EMr=l(),f(wA.$$.fragment),yMr=l(),ko=a("div"),f(AA.$$.fragment),wMr=l(),Abe=a("p"),AMr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),LMr=l(),Bn=a("p"),BMr=o("The model class to instantiate is selected based on the "),Lbe=a("code"),xMr=o("model_type"),kMr=o(` property of the config object (either
passed as an argument or loaded from `),Bbe=a("code"),RMr=o("pretrained_model_name_or_path"),SMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xbe=a("code"),PMr=o("pretrained_model_name_or_path"),$Mr=o(":"),IMr=l(),be=a("ul"),jC=a("li"),kbe=a("strong"),DMr=o("albert"),jMr=o(" \u2014 "),oV=a("a"),NMr=o("FlaxAlbertForSequenceClassification"),qMr=o(" (ALBERT model)"),GMr=l(),NC=a("li"),Rbe=a("strong"),OMr=o("bart"),XMr=o(" \u2014 "),rV=a("a"),VMr=o("FlaxBartForSequenceClassification"),zMr=o(" (BART model)"),WMr=l(),qC=a("li"),Sbe=a("strong"),QMr=o("bert"),HMr=o(" \u2014 "),tV=a("a"),UMr=o("FlaxBertForSequenceClassification"),JMr=o(" (BERT model)"),YMr=l(),GC=a("li"),Pbe=a("strong"),KMr=o("big_bird"),ZMr=o(" \u2014 "),aV=a("a"),e4r=o("FlaxBigBirdForSequenceClassification"),o4r=o(" (BigBird model)"),r4r=l(),OC=a("li"),$be=a("strong"),t4r=o("distilbert"),a4r=o(" \u2014 "),nV=a("a"),n4r=o("FlaxDistilBertForSequenceClassification"),s4r=o(" (DistilBERT model)"),l4r=l(),XC=a("li"),Ibe=a("strong"),i4r=o("electra"),d4r=o(" \u2014 "),sV=a("a"),c4r=o("FlaxElectraForSequenceClassification"),f4r=o(" (ELECTRA model)"),m4r=l(),VC=a("li"),Dbe=a("strong"),g4r=o("mbart"),h4r=o(" \u2014 "),lV=a("a"),p4r=o("FlaxMBartForSequenceClassification"),_4r=o(" (mBART model)"),u4r=l(),zC=a("li"),jbe=a("strong"),b4r=o("roberta"),v4r=o(" \u2014 "),iV=a("a"),T4r=o("FlaxRobertaForSequenceClassification"),F4r=o(" (RoBERTa model)"),C4r=l(),WC=a("li"),Nbe=a("strong"),M4r=o("roformer"),E4r=o(" \u2014 "),dV=a("a"),y4r=o("FlaxRoFormerForSequenceClassification"),w4r=o(" (RoFormer model)"),A4r=l(),QC=a("li"),qbe=a("strong"),L4r=o("xlm-roberta"),B4r=o(" \u2014 "),Gbe=a("code"),x4r=o("FlaxXLMRobertaForSequenceClassification"),k4r=o("(XLM-RoBERTa model)"),R4r=l(),Obe=a("p"),S4r=o("Examples:"),P4r=l(),f(LA.$$.fragment),qxe=l(),af=a("h2"),HC=a("a"),Xbe=a("span"),f(BA.$$.fragment),$4r=l(),Vbe=a("span"),I4r=o("FlaxAutoModelForQuestionAnswering"),Gxe=l(),Ir=a("div"),f(xA.$$.fragment),D4r=l(),nf=a("p"),j4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),zbe=a("code"),N4r=o("from_pretrained()"),q4r=o("class method or the "),Wbe=a("code"),G4r=o("from_config()"),O4r=o(`class
method.`),X4r=l(),kA=a("p"),V4r=o("This class cannot be instantiated directly using "),Qbe=a("code"),z4r=o("__init__()"),W4r=o(" (throws an error)."),Q4r=l(),Bt=a("div"),f(RA.$$.fragment),H4r=l(),Hbe=a("p"),U4r=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),J4r=l(),sf=a("p"),Y4r=o(`Note:
Loading a model from its configuration file does `),Ube=a("strong"),K4r=o("not"),Z4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jbe=a("code"),eEr=o("from_pretrained()"),oEr=o("to load the model weights."),rEr=l(),Ybe=a("p"),tEr=o("Examples:"),aEr=l(),f(SA.$$.fragment),nEr=l(),Ro=a("div"),f(PA.$$.fragment),sEr=l(),Kbe=a("p"),lEr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),iEr=l(),xn=a("p"),dEr=o("The model class to instantiate is selected based on the "),Zbe=a("code"),cEr=o("model_type"),fEr=o(` property of the config object (either
passed as an argument or loaded from `),e5e=a("code"),mEr=o("pretrained_model_name_or_path"),gEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o5e=a("code"),hEr=o("pretrained_model_name_or_path"),pEr=o(":"),_Er=l(),ve=a("ul"),UC=a("li"),r5e=a("strong"),uEr=o("albert"),bEr=o(" \u2014 "),cV=a("a"),vEr=o("FlaxAlbertForQuestionAnswering"),TEr=o(" (ALBERT model)"),FEr=l(),JC=a("li"),t5e=a("strong"),CEr=o("bart"),MEr=o(" \u2014 "),fV=a("a"),EEr=o("FlaxBartForQuestionAnswering"),yEr=o(" (BART model)"),wEr=l(),YC=a("li"),a5e=a("strong"),AEr=o("bert"),LEr=o(" \u2014 "),mV=a("a"),BEr=o("FlaxBertForQuestionAnswering"),xEr=o(" (BERT model)"),kEr=l(),KC=a("li"),n5e=a("strong"),REr=o("big_bird"),SEr=o(" \u2014 "),gV=a("a"),PEr=o("FlaxBigBirdForQuestionAnswering"),$Er=o(" (BigBird model)"),IEr=l(),ZC=a("li"),s5e=a("strong"),DEr=o("distilbert"),jEr=o(" \u2014 "),hV=a("a"),NEr=o("FlaxDistilBertForQuestionAnswering"),qEr=o(" (DistilBERT model)"),GEr=l(),eM=a("li"),l5e=a("strong"),OEr=o("electra"),XEr=o(" \u2014 "),pV=a("a"),VEr=o("FlaxElectraForQuestionAnswering"),zEr=o(" (ELECTRA model)"),WEr=l(),oM=a("li"),i5e=a("strong"),QEr=o("mbart"),HEr=o(" \u2014 "),_V=a("a"),UEr=o("FlaxMBartForQuestionAnswering"),JEr=o(" (mBART model)"),YEr=l(),rM=a("li"),d5e=a("strong"),KEr=o("roberta"),ZEr=o(" \u2014 "),uV=a("a"),e3r=o("FlaxRobertaForQuestionAnswering"),o3r=o(" (RoBERTa model)"),r3r=l(),tM=a("li"),c5e=a("strong"),t3r=o("roformer"),a3r=o(" \u2014 "),bV=a("a"),n3r=o("FlaxRoFormerForQuestionAnswering"),s3r=o(" (RoFormer model)"),l3r=l(),aM=a("li"),f5e=a("strong"),i3r=o("xlm-roberta"),d3r=o(" \u2014 "),m5e=a("code"),c3r=o("FlaxXLMRobertaForQuestionAnswering"),f3r=o("(XLM-RoBERTa model)"),m3r=l(),g5e=a("p"),g3r=o("Examples:"),h3r=l(),f($A.$$.fragment),Oxe=l(),lf=a("h2"),nM=a("a"),h5e=a("span"),f(IA.$$.fragment),p3r=l(),p5e=a("span"),_3r=o("FlaxAutoModelForTokenClassification"),Xxe=l(),Dr=a("div"),f(DA.$$.fragment),u3r=l(),df=a("p"),b3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),_5e=a("code"),v3r=o("from_pretrained()"),T3r=o("class method or the "),u5e=a("code"),F3r=o("from_config()"),C3r=o(`class
method.`),M3r=l(),jA=a("p"),E3r=o("This class cannot be instantiated directly using "),b5e=a("code"),y3r=o("__init__()"),w3r=o(" (throws an error)."),A3r=l(),xt=a("div"),f(NA.$$.fragment),L3r=l(),v5e=a("p"),B3r=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),x3r=l(),cf=a("p"),k3r=o(`Note:
Loading a model from its configuration file does `),T5e=a("strong"),R3r=o("not"),S3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F5e=a("code"),P3r=o("from_pretrained()"),$3r=o("to load the model weights."),I3r=l(),C5e=a("p"),D3r=o("Examples:"),j3r=l(),f(qA.$$.fragment),N3r=l(),So=a("div"),f(GA.$$.fragment),q3r=l(),M5e=a("p"),G3r=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),O3r=l(),kn=a("p"),X3r=o("The model class to instantiate is selected based on the "),E5e=a("code"),V3r=o("model_type"),z3r=o(` property of the config object (either
passed as an argument or loaded from `),y5e=a("code"),W3r=o("pretrained_model_name_or_path"),Q3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w5e=a("code"),H3r=o("pretrained_model_name_or_path"),U3r=o(":"),J3r=l(),Re=a("ul"),sM=a("li"),A5e=a("strong"),Y3r=o("albert"),K3r=o(" \u2014 "),vV=a("a"),Z3r=o("FlaxAlbertForTokenClassification"),eyr=o(" (ALBERT model)"),oyr=l(),lM=a("li"),L5e=a("strong"),ryr=o("bert"),tyr=o(" \u2014 "),TV=a("a"),ayr=o("FlaxBertForTokenClassification"),nyr=o(" (BERT model)"),syr=l(),iM=a("li"),B5e=a("strong"),lyr=o("big_bird"),iyr=o(" \u2014 "),FV=a("a"),dyr=o("FlaxBigBirdForTokenClassification"),cyr=o(" (BigBird model)"),fyr=l(),dM=a("li"),x5e=a("strong"),myr=o("distilbert"),gyr=o(" \u2014 "),CV=a("a"),hyr=o("FlaxDistilBertForTokenClassification"),pyr=o(" (DistilBERT model)"),_yr=l(),cM=a("li"),k5e=a("strong"),uyr=o("electra"),byr=o(" \u2014 "),MV=a("a"),vyr=o("FlaxElectraForTokenClassification"),Tyr=o(" (ELECTRA model)"),Fyr=l(),fM=a("li"),R5e=a("strong"),Cyr=o("roberta"),Myr=o(" \u2014 "),EV=a("a"),Eyr=o("FlaxRobertaForTokenClassification"),yyr=o(" (RoBERTa model)"),wyr=l(),mM=a("li"),S5e=a("strong"),Ayr=o("roformer"),Lyr=o(" \u2014 "),yV=a("a"),Byr=o("FlaxRoFormerForTokenClassification"),xyr=o(" (RoFormer model)"),kyr=l(),gM=a("li"),P5e=a("strong"),Ryr=o("xlm-roberta"),Syr=o(" \u2014 "),$5e=a("code"),Pyr=o("FlaxXLMRobertaForTokenClassification"),$yr=o("(XLM-RoBERTa model)"),Iyr=l(),I5e=a("p"),Dyr=o("Examples:"),jyr=l(),f(OA.$$.fragment),Vxe=l(),ff=a("h2"),hM=a("a"),D5e=a("span"),f(XA.$$.fragment),Nyr=l(),j5e=a("span"),qyr=o("FlaxAutoModelForMultipleChoice"),zxe=l(),jr=a("div"),f(VA.$$.fragment),Gyr=l(),mf=a("p"),Oyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),N5e=a("code"),Xyr=o("from_pretrained()"),Vyr=o("class method or the "),q5e=a("code"),zyr=o("from_config()"),Wyr=o(`class
method.`),Qyr=l(),zA=a("p"),Hyr=o("This class cannot be instantiated directly using "),G5e=a("code"),Uyr=o("__init__()"),Jyr=o(" (throws an error)."),Yyr=l(),kt=a("div"),f(WA.$$.fragment),Kyr=l(),O5e=a("p"),Zyr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),ewr=l(),gf=a("p"),owr=o(`Note:
Loading a model from its configuration file does `),X5e=a("strong"),rwr=o("not"),twr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),V5e=a("code"),awr=o("from_pretrained()"),nwr=o("to load the model weights."),swr=l(),z5e=a("p"),lwr=o("Examples:"),iwr=l(),f(QA.$$.fragment),dwr=l(),Po=a("div"),f(HA.$$.fragment),cwr=l(),W5e=a("p"),fwr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),mwr=l(),Rn=a("p"),gwr=o("The model class to instantiate is selected based on the "),Q5e=a("code"),hwr=o("model_type"),pwr=o(` property of the config object (either
passed as an argument or loaded from `),H5e=a("code"),_wr=o("pretrained_model_name_or_path"),uwr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U5e=a("code"),bwr=o("pretrained_model_name_or_path"),vwr=o(":"),Twr=l(),Se=a("ul"),pM=a("li"),J5e=a("strong"),Fwr=o("albert"),Cwr=o(" \u2014 "),wV=a("a"),Mwr=o("FlaxAlbertForMultipleChoice"),Ewr=o(" (ALBERT model)"),ywr=l(),_M=a("li"),Y5e=a("strong"),wwr=o("bert"),Awr=o(" \u2014 "),AV=a("a"),Lwr=o("FlaxBertForMultipleChoice"),Bwr=o(" (BERT model)"),xwr=l(),uM=a("li"),K5e=a("strong"),kwr=o("big_bird"),Rwr=o(" \u2014 "),LV=a("a"),Swr=o("FlaxBigBirdForMultipleChoice"),Pwr=o(" (BigBird model)"),$wr=l(),bM=a("li"),Z5e=a("strong"),Iwr=o("distilbert"),Dwr=o(" \u2014 "),BV=a("a"),jwr=o("FlaxDistilBertForMultipleChoice"),Nwr=o(" (DistilBERT model)"),qwr=l(),vM=a("li"),e2e=a("strong"),Gwr=o("electra"),Owr=o(" \u2014 "),xV=a("a"),Xwr=o("FlaxElectraForMultipleChoice"),Vwr=o(" (ELECTRA model)"),zwr=l(),TM=a("li"),o2e=a("strong"),Wwr=o("roberta"),Qwr=o(" \u2014 "),kV=a("a"),Hwr=o("FlaxRobertaForMultipleChoice"),Uwr=o(" (RoBERTa model)"),Jwr=l(),FM=a("li"),r2e=a("strong"),Ywr=o("roformer"),Kwr=o(" \u2014 "),RV=a("a"),Zwr=o("FlaxRoFormerForMultipleChoice"),e6r=o(" (RoFormer model)"),o6r=l(),CM=a("li"),t2e=a("strong"),r6r=o("xlm-roberta"),t6r=o(" \u2014 "),a2e=a("code"),a6r=o("FlaxXLMRobertaForMultipleChoice"),n6r=o("(XLM-RoBERTa model)"),s6r=l(),n2e=a("p"),l6r=o("Examples:"),i6r=l(),f(UA.$$.fragment),Wxe=l(),hf=a("h2"),MM=a("a"),s2e=a("span"),f(JA.$$.fragment),d6r=l(),l2e=a("span"),c6r=o("FlaxAutoModelForNextSentencePrediction"),Qxe=l(),Nr=a("div"),f(YA.$$.fragment),f6r=l(),pf=a("p"),m6r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),i2e=a("code"),g6r=o("from_pretrained()"),h6r=o("class method or the "),d2e=a("code"),p6r=o("from_config()"),_6r=o(`class
method.`),u6r=l(),KA=a("p"),b6r=o("This class cannot be instantiated directly using "),c2e=a("code"),v6r=o("__init__()"),T6r=o(" (throws an error)."),F6r=l(),Rt=a("div"),f(ZA.$$.fragment),C6r=l(),f2e=a("p"),M6r=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),E6r=l(),_f=a("p"),y6r=o(`Note:
Loading a model from its configuration file does `),m2e=a("strong"),w6r=o("not"),A6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),g2e=a("code"),L6r=o("from_pretrained()"),B6r=o("to load the model weights."),x6r=l(),h2e=a("p"),k6r=o("Examples:"),R6r=l(),f(eL.$$.fragment),S6r=l(),$o=a("div"),f(oL.$$.fragment),P6r=l(),p2e=a("p"),$6r=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),I6r=l(),Sn=a("p"),D6r=o("The model class to instantiate is selected based on the "),_2e=a("code"),j6r=o("model_type"),N6r=o(` property of the config object (either
passed as an argument or loaded from `),u2e=a("code"),q6r=o("pretrained_model_name_or_path"),G6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),b2e=a("code"),O6r=o("pretrained_model_name_or_path"),X6r=o(":"),V6r=l(),v2e=a("ul"),EM=a("li"),T2e=a("strong"),z6r=o("bert"),W6r=o(" \u2014 "),SV=a("a"),Q6r=o("FlaxBertForNextSentencePrediction"),H6r=o(" (BERT model)"),U6r=l(),F2e=a("p"),J6r=o("Examples:"),Y6r=l(),f(rL.$$.fragment),Hxe=l(),uf=a("h2"),yM=a("a"),C2e=a("span"),f(tL.$$.fragment),K6r=l(),M2e=a("span"),Z6r=o("FlaxAutoModelForImageClassification"),Uxe=l(),qr=a("div"),f(aL.$$.fragment),eAr=l(),bf=a("p"),oAr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),E2e=a("code"),rAr=o("from_pretrained()"),tAr=o("class method or the "),y2e=a("code"),aAr=o("from_config()"),nAr=o(`class
method.`),sAr=l(),nL=a("p"),lAr=o("This class cannot be instantiated directly using "),w2e=a("code"),iAr=o("__init__()"),dAr=o(" (throws an error)."),cAr=l(),St=a("div"),f(sL.$$.fragment),fAr=l(),A2e=a("p"),mAr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),gAr=l(),vf=a("p"),hAr=o(`Note:
Loading a model from its configuration file does `),L2e=a("strong"),pAr=o("not"),_Ar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),B2e=a("code"),uAr=o("from_pretrained()"),bAr=o("to load the model weights."),vAr=l(),x2e=a("p"),TAr=o("Examples:"),FAr=l(),f(lL.$$.fragment),CAr=l(),Io=a("div"),f(iL.$$.fragment),MAr=l(),k2e=a("p"),EAr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),yAr=l(),Pn=a("p"),wAr=o("The model class to instantiate is selected based on the "),R2e=a("code"),AAr=o("model_type"),LAr=o(` property of the config object (either
passed as an argument or loaded from `),S2e=a("code"),BAr=o("pretrained_model_name_or_path"),xAr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P2e=a("code"),kAr=o("pretrained_model_name_or_path"),RAr=o(":"),SAr=l(),dL=a("ul"),wM=a("li"),$2e=a("strong"),PAr=o("beit"),$Ar=o(" \u2014 "),PV=a("a"),IAr=o("FlaxBeitForImageClassification"),DAr=o(" (BEiT model)"),jAr=l(),AM=a("li"),I2e=a("strong"),NAr=o("vit"),qAr=o(" \u2014 "),$V=a("a"),GAr=o("FlaxViTForImageClassification"),OAr=o(" (ViT model)"),XAr=l(),D2e=a("p"),VAr=o("Examples:"),zAr=l(),f(cL.$$.fragment),Jxe=l(),Tf=a("h2"),LM=a("a"),j2e=a("span"),f(fL.$$.fragment),WAr=l(),N2e=a("span"),QAr=o("FlaxAutoModelForVision2Seq"),Yxe=l(),Gr=a("div"),f(mL.$$.fragment),HAr=l(),Ff=a("p"),UAr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),q2e=a("code"),JAr=o("from_pretrained()"),YAr=o("class method or the "),G2e=a("code"),KAr=o("from_config()"),ZAr=o(`class
method.`),eLr=l(),gL=a("p"),oLr=o("This class cannot be instantiated directly using "),O2e=a("code"),rLr=o("__init__()"),tLr=o(" (throws an error)."),aLr=l(),Pt=a("div"),f(hL.$$.fragment),nLr=l(),X2e=a("p"),sLr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),lLr=l(),Cf=a("p"),iLr=o(`Note:
Loading a model from its configuration file does `),V2e=a("strong"),dLr=o("not"),cLr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z2e=a("code"),fLr=o("from_pretrained()"),mLr=o("to load the model weights."),gLr=l(),W2e=a("p"),hLr=o("Examples:"),pLr=l(),f(pL.$$.fragment),_Lr=l(),Do=a("div"),f(_L.$$.fragment),uLr=l(),Q2e=a("p"),bLr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),vLr=l(),$n=a("p"),TLr=o("The model class to instantiate is selected based on the "),H2e=a("code"),FLr=o("model_type"),CLr=o(` property of the config object (either
passed as an argument or loaded from `),U2e=a("code"),MLr=o("pretrained_model_name_or_path"),ELr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),J2e=a("code"),yLr=o("pretrained_model_name_or_path"),wLr=o(":"),ALr=l(),Y2e=a("ul"),BM=a("li"),K2e=a("strong"),LLr=o("vision-encoder-decoder"),BLr=o(" \u2014 "),IV=a("a"),xLr=o("FlaxVisionEncoderDecoderModel"),kLr=o(" (Vision Encoder decoder model)"),RLr=l(),Z2e=a("p"),SLr=o("Examples:"),PLr=l(),f(uL.$$.fragment),this.h()},l(c){const u=I5t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Pe=i(c),ie=n(c,"H1",{class:!0});var bL=s(ie);ge=n(bL,"A",{id:!0,class:!0,href:!0});var eve=s(ge);lo=n(eve,"SPAN",{});var ove=s(lo);m(fe.$$.fragment,ove),ove.forEach(t),eve.forEach(t),Te=i(bL),Xo=n(bL,"SPAN",{});var ILr=s(Xo);Li=r(ILr,"Auto Classes"),ILr.forEach(t),bL.forEach(t),Ef=i(c),sa=n(c,"P",{});var Zxe=s(sa);Bi=r(Zxe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),xi=n(Zxe,"CODE",{});var DLr=s(xi);B4=r(DLr,"from_pretrained()"),DLr.forEach(t),yf=r(Zxe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Zxe.forEach(t),Le=i(c),io=n(c,"P",{});var xM=s(io);ki=r(xM,"Instantiating one of "),In=n(xM,"A",{href:!0});var jLr=s(In);x4=r(jLr,"AutoConfig"),jLr.forEach(t),Dn=r(xM,", "),jn=n(xM,"A",{href:!0});var NLr=s(jn);k4=r(NLr,"AutoModel"),NLr.forEach(t),Ri=r(xM,`, and
`),Nn=n(xM,"A",{href:!0});var qLr=s(Nn);R4=r(qLr,"AutoTokenizer"),qLr.forEach(t),Si=r(xM," will directly create a class of the relevant architecture. For instance"),xM.forEach(t),wf=i(c),m($a.$$.fragment,c),co=i(c),he=n(c,"P",{});var eke=s(he);h8=r(eke,"will create a model that is an instance of "),Pi=n(eke,"A",{href:!0});var GLr=s(Pi);p8=r(GLr,"BertModel"),GLr.forEach(t),_8=r(eke,"."),eke.forEach(t),Vo=i(c),Ia=n(c,"P",{});var oke=s(Ia);u8=r(oke,"There is one class of "),Af=n(oke,"CODE",{});var OLr=s(Af);b8=r(OLr,"AutoModel"),OLr.forEach(t),dSe=r(oke," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),oke.forEach(t),Z7e=i(c),$i=n(c,"H2",{class:!0});var rke=s($i);Lf=n(rke,"A",{id:!0,class:!0,href:!0});var XLr=s(Lf);AW=n(XLr,"SPAN",{});var VLr=s(AW);m(S4.$$.fragment,VLr),VLr.forEach(t),XLr.forEach(t),cSe=i(rke),LW=n(rke,"SPAN",{});var zLr=s(LW);fSe=r(zLr,"Extending the Auto Classes"),zLr.forEach(t),rke.forEach(t),eBe=i(c),qn=n(c,"P",{});var DV=s(qn);mSe=r(DV,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),BW=n(DV,"CODE",{});var WLr=s(BW);gSe=r(WLr,"NewModel"),WLr.forEach(t),hSe=r(DV,", make sure you have a "),xW=n(DV,"CODE",{});var QLr=s(xW);pSe=r(QLr,"NewModelConfig"),QLr.forEach(t),_Se=r(DV,` then you can add those to the auto
classes like this:`),DV.forEach(t),oBe=i(c),m(P4.$$.fragment,c),rBe=i(c),v8=n(c,"P",{});var HLr=s(v8);uSe=r(HLr,"You will then be able to use the auto classes like you would usually do!"),HLr.forEach(t),tBe=i(c),m(Bf.$$.fragment,c),aBe=i(c),Ii=n(c,"H2",{class:!0});var tke=s(Ii);xf=n(tke,"A",{id:!0,class:!0,href:!0});var ULr=s(xf);kW=n(ULr,"SPAN",{});var JLr=s(kW);m($4.$$.fragment,JLr),JLr.forEach(t),ULr.forEach(t),bSe=i(tke),RW=n(tke,"SPAN",{});var YLr=s(RW);vSe=r(YLr,"AutoConfig"),YLr.forEach(t),tke.forEach(t),nBe=i(c),zo=n(c,"DIV",{class:!0});var Is=s(zo);m(I4.$$.fragment,Is),TSe=i(Is),D4=n(Is,"P",{});var ake=s(D4);FSe=r(ake,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),T8=n(ake,"A",{href:!0});var KLr=s(T8);CSe=r(KLr,"from_pretrained()"),KLr.forEach(t),MSe=r(ake," class method."),ake.forEach(t),ESe=i(Is),j4=n(Is,"P",{});var nke=s(j4);ySe=r(nke,"This class cannot be instantiated directly using "),SW=n(nke,"CODE",{});var ZLr=s(SW);wSe=r(ZLr,"__init__()"),ZLr.forEach(t),ASe=r(nke," (throws an error)."),nke.forEach(t),LSe=i(Is),fo=n(Is,"DIV",{class:!0});var ia=s(fo);m(N4.$$.fragment,ia),BSe=i(ia),PW=n(ia,"P",{});var e8r=s(PW);xSe=r(e8r,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),e8r.forEach(t),kSe=i(ia),Di=n(ia,"P",{});var jV=s(Di);RSe=r(jV,"The configuration class to instantiate is selected based on the "),$W=n(jV,"CODE",{});var o8r=s($W);SSe=r(o8r,"model_type"),o8r.forEach(t),PSe=r(jV,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),IW=n(jV,"CODE",{});var r8r=s(IW);$Se=r(r8r,"pretrained_model_name_or_path"),r8r.forEach(t),ISe=r(jV,":"),jV.forEach(t),DSe=i(ia),v=n(ia,"UL",{});var T=s(v);kf=n(T,"LI",{});var rve=s(kf);DW=n(rve,"STRONG",{});var t8r=s(DW);jSe=r(t8r,"albert"),t8r.forEach(t),NSe=r(rve," \u2014 "),F8=n(rve,"A",{href:!0});var a8r=s(F8);qSe=r(a8r,"AlbertConfig"),a8r.forEach(t),GSe=r(rve," (ALBERT model)"),rve.forEach(t),OSe=i(T),Rf=n(T,"LI",{});var tve=s(Rf);jW=n(tve,"STRONG",{});var n8r=s(jW);XSe=r(n8r,"bart"),n8r.forEach(t),VSe=r(tve," \u2014 "),C8=n(tve,"A",{href:!0});var s8r=s(C8);zSe=r(s8r,"BartConfig"),s8r.forEach(t),WSe=r(tve," (BART model)"),tve.forEach(t),QSe=i(T),Sf=n(T,"LI",{});var ave=s(Sf);NW=n(ave,"STRONG",{});var l8r=s(NW);HSe=r(l8r,"beit"),l8r.forEach(t),USe=r(ave," \u2014 "),M8=n(ave,"A",{href:!0});var i8r=s(M8);JSe=r(i8r,"BeitConfig"),i8r.forEach(t),YSe=r(ave," (BEiT model)"),ave.forEach(t),KSe=i(T),Pf=n(T,"LI",{});var nve=s(Pf);qW=n(nve,"STRONG",{});var d8r=s(qW);ZSe=r(d8r,"bert"),d8r.forEach(t),ePe=r(nve," \u2014 "),E8=n(nve,"A",{href:!0});var c8r=s(E8);oPe=r(c8r,"BertConfig"),c8r.forEach(t),rPe=r(nve," (BERT model)"),nve.forEach(t),tPe=i(T),$f=n(T,"LI",{});var sve=s($f);GW=n(sve,"STRONG",{});var f8r=s(GW);aPe=r(f8r,"bert-generation"),f8r.forEach(t),nPe=r(sve," \u2014 "),y8=n(sve,"A",{href:!0});var m8r=s(y8);sPe=r(m8r,"BertGenerationConfig"),m8r.forEach(t),lPe=r(sve," (Bert Generation model)"),sve.forEach(t),iPe=i(T),If=n(T,"LI",{});var lve=s(If);OW=n(lve,"STRONG",{});var g8r=s(OW);dPe=r(g8r,"big_bird"),g8r.forEach(t),cPe=r(lve," \u2014 "),w8=n(lve,"A",{href:!0});var h8r=s(w8);fPe=r(h8r,"BigBirdConfig"),h8r.forEach(t),mPe=r(lve," (BigBird model)"),lve.forEach(t),gPe=i(T),Df=n(T,"LI",{});var ive=s(Df);XW=n(ive,"STRONG",{});var p8r=s(XW);hPe=r(p8r,"bigbird_pegasus"),p8r.forEach(t),pPe=r(ive," \u2014 "),A8=n(ive,"A",{href:!0});var _8r=s(A8);_Pe=r(_8r,"BigBirdPegasusConfig"),_8r.forEach(t),uPe=r(ive," (BigBirdPegasus model)"),ive.forEach(t),bPe=i(T),jf=n(T,"LI",{});var dve=s(jf);VW=n(dve,"STRONG",{});var u8r=s(VW);vPe=r(u8r,"blenderbot"),u8r.forEach(t),TPe=r(dve," \u2014 "),L8=n(dve,"A",{href:!0});var b8r=s(L8);FPe=r(b8r,"BlenderbotConfig"),b8r.forEach(t),CPe=r(dve," (Blenderbot model)"),dve.forEach(t),MPe=i(T),Nf=n(T,"LI",{});var cve=s(Nf);zW=n(cve,"STRONG",{});var v8r=s(zW);EPe=r(v8r,"blenderbot-small"),v8r.forEach(t),yPe=r(cve," \u2014 "),B8=n(cve,"A",{href:!0});var T8r=s(B8);wPe=r(T8r,"BlenderbotSmallConfig"),T8r.forEach(t),APe=r(cve," (BlenderbotSmall model)"),cve.forEach(t),LPe=i(T),qf=n(T,"LI",{});var fve=s(qf);WW=n(fve,"STRONG",{});var F8r=s(WW);BPe=r(F8r,"camembert"),F8r.forEach(t),xPe=r(fve," \u2014 "),x8=n(fve,"A",{href:!0});var C8r=s(x8);kPe=r(C8r,"CamembertConfig"),C8r.forEach(t),RPe=r(fve," (CamemBERT model)"),fve.forEach(t),SPe=i(T),Gf=n(T,"LI",{});var mve=s(Gf);QW=n(mve,"STRONG",{});var M8r=s(QW);PPe=r(M8r,"canine"),M8r.forEach(t),$Pe=r(mve," \u2014 "),k8=n(mve,"A",{href:!0});var E8r=s(k8);IPe=r(E8r,"CanineConfig"),E8r.forEach(t),DPe=r(mve," (Canine model)"),mve.forEach(t),jPe=i(T),Of=n(T,"LI",{});var gve=s(Of);HW=n(gve,"STRONG",{});var y8r=s(HW);NPe=r(y8r,"clip"),y8r.forEach(t),qPe=r(gve," \u2014 "),R8=n(gve,"A",{href:!0});var w8r=s(R8);GPe=r(w8r,"CLIPConfig"),w8r.forEach(t),OPe=r(gve," (CLIP model)"),gve.forEach(t),XPe=i(T),Xf=n(T,"LI",{});var hve=s(Xf);UW=n(hve,"STRONG",{});var A8r=s(UW);VPe=r(A8r,"convbert"),A8r.forEach(t),zPe=r(hve," \u2014 "),S8=n(hve,"A",{href:!0});var L8r=s(S8);WPe=r(L8r,"ConvBertConfig"),L8r.forEach(t),QPe=r(hve," (ConvBERT model)"),hve.forEach(t),HPe=i(T),Vf=n(T,"LI",{});var pve=s(Vf);JW=n(pve,"STRONG",{});var B8r=s(JW);UPe=r(B8r,"convnext"),B8r.forEach(t),JPe=r(pve," \u2014 "),P8=n(pve,"A",{href:!0});var x8r=s(P8);YPe=r(x8r,"ConvNextConfig"),x8r.forEach(t),KPe=r(pve," (ConvNext model)"),pve.forEach(t),ZPe=i(T),zf=n(T,"LI",{});var _ve=s(zf);YW=n(_ve,"STRONG",{});var k8r=s(YW);e$e=r(k8r,"ctrl"),k8r.forEach(t),o$e=r(_ve," \u2014 "),$8=n(_ve,"A",{href:!0});var R8r=s($8);r$e=r(R8r,"CTRLConfig"),R8r.forEach(t),t$e=r(_ve," (CTRL model)"),_ve.forEach(t),a$e=i(T),Wf=n(T,"LI",{});var uve=s(Wf);KW=n(uve,"STRONG",{});var S8r=s(KW);n$e=r(S8r,"data2vec-audio"),S8r.forEach(t),s$e=r(uve," \u2014 "),I8=n(uve,"A",{href:!0});var P8r=s(I8);l$e=r(P8r,"Data2VecAudioConfig"),P8r.forEach(t),i$e=r(uve," (Data2VecAudio model)"),uve.forEach(t),d$e=i(T),Qf=n(T,"LI",{});var bve=s(Qf);ZW=n(bve,"STRONG",{});var $8r=s(ZW);c$e=r($8r,"data2vec-text"),$8r.forEach(t),f$e=r(bve," \u2014 "),D8=n(bve,"A",{href:!0});var I8r=s(D8);m$e=r(I8r,"Data2VecTextConfig"),I8r.forEach(t),g$e=r(bve," (Data2VecText model)"),bve.forEach(t),h$e=i(T),Hf=n(T,"LI",{});var vve=s(Hf);eQ=n(vve,"STRONG",{});var D8r=s(eQ);p$e=r(D8r,"deberta"),D8r.forEach(t),_$e=r(vve," \u2014 "),j8=n(vve,"A",{href:!0});var j8r=s(j8);u$e=r(j8r,"DebertaConfig"),j8r.forEach(t),b$e=r(vve," (DeBERTa model)"),vve.forEach(t),v$e=i(T),Uf=n(T,"LI",{});var Tve=s(Uf);oQ=n(Tve,"STRONG",{});var N8r=s(oQ);T$e=r(N8r,"deberta-v2"),N8r.forEach(t),F$e=r(Tve," \u2014 "),N8=n(Tve,"A",{href:!0});var q8r=s(N8);C$e=r(q8r,"DebertaV2Config"),q8r.forEach(t),M$e=r(Tve," (DeBERTa-v2 model)"),Tve.forEach(t),E$e=i(T),Jf=n(T,"LI",{});var Fve=s(Jf);rQ=n(Fve,"STRONG",{});var G8r=s(rQ);y$e=r(G8r,"deit"),G8r.forEach(t),w$e=r(Fve," \u2014 "),q8=n(Fve,"A",{href:!0});var O8r=s(q8);A$e=r(O8r,"DeiTConfig"),O8r.forEach(t),L$e=r(Fve," (DeiT model)"),Fve.forEach(t),B$e=i(T),Yf=n(T,"LI",{});var Cve=s(Yf);tQ=n(Cve,"STRONG",{});var X8r=s(tQ);x$e=r(X8r,"detr"),X8r.forEach(t),k$e=r(Cve," \u2014 "),G8=n(Cve,"A",{href:!0});var V8r=s(G8);R$e=r(V8r,"DetrConfig"),V8r.forEach(t),S$e=r(Cve," (DETR model)"),Cve.forEach(t),P$e=i(T),Kf=n(T,"LI",{});var Mve=s(Kf);aQ=n(Mve,"STRONG",{});var z8r=s(aQ);$$e=r(z8r,"distilbert"),z8r.forEach(t),I$e=r(Mve," \u2014 "),O8=n(Mve,"A",{href:!0});var W8r=s(O8);D$e=r(W8r,"DistilBertConfig"),W8r.forEach(t),j$e=r(Mve," (DistilBERT model)"),Mve.forEach(t),N$e=i(T),Zf=n(T,"LI",{});var Eve=s(Zf);nQ=n(Eve,"STRONG",{});var Q8r=s(nQ);q$e=r(Q8r,"dpr"),Q8r.forEach(t),G$e=r(Eve," \u2014 "),X8=n(Eve,"A",{href:!0});var H8r=s(X8);O$e=r(H8r,"DPRConfig"),H8r.forEach(t),X$e=r(Eve," (DPR model)"),Eve.forEach(t),V$e=i(T),em=n(T,"LI",{});var yve=s(em);sQ=n(yve,"STRONG",{});var U8r=s(sQ);z$e=r(U8r,"electra"),U8r.forEach(t),W$e=r(yve," \u2014 "),V8=n(yve,"A",{href:!0});var J8r=s(V8);Q$e=r(J8r,"ElectraConfig"),J8r.forEach(t),H$e=r(yve," (ELECTRA model)"),yve.forEach(t),U$e=i(T),om=n(T,"LI",{});var wve=s(om);lQ=n(wve,"STRONG",{});var Y8r=s(lQ);J$e=r(Y8r,"encoder-decoder"),Y8r.forEach(t),Y$e=r(wve," \u2014 "),z8=n(wve,"A",{href:!0});var K8r=s(z8);K$e=r(K8r,"EncoderDecoderConfig"),K8r.forEach(t),Z$e=r(wve," (Encoder decoder model)"),wve.forEach(t),eIe=i(T),rm=n(T,"LI",{});var Ave=s(rm);iQ=n(Ave,"STRONG",{});var Z8r=s(iQ);oIe=r(Z8r,"flaubert"),Z8r.forEach(t),rIe=r(Ave," \u2014 "),W8=n(Ave,"A",{href:!0});var e7r=s(W8);tIe=r(e7r,"FlaubertConfig"),e7r.forEach(t),aIe=r(Ave," (FlauBERT model)"),Ave.forEach(t),nIe=i(T),tm=n(T,"LI",{});var Lve=s(tm);dQ=n(Lve,"STRONG",{});var o7r=s(dQ);sIe=r(o7r,"fnet"),o7r.forEach(t),lIe=r(Lve," \u2014 "),Q8=n(Lve,"A",{href:!0});var r7r=s(Q8);iIe=r(r7r,"FNetConfig"),r7r.forEach(t),dIe=r(Lve," (FNet model)"),Lve.forEach(t),cIe=i(T),am=n(T,"LI",{});var Bve=s(am);cQ=n(Bve,"STRONG",{});var t7r=s(cQ);fIe=r(t7r,"fsmt"),t7r.forEach(t),mIe=r(Bve," \u2014 "),H8=n(Bve,"A",{href:!0});var a7r=s(H8);gIe=r(a7r,"FSMTConfig"),a7r.forEach(t),hIe=r(Bve," (FairSeq Machine-Translation model)"),Bve.forEach(t),pIe=i(T),nm=n(T,"LI",{});var xve=s(nm);fQ=n(xve,"STRONG",{});var n7r=s(fQ);_Ie=r(n7r,"funnel"),n7r.forEach(t),uIe=r(xve," \u2014 "),U8=n(xve,"A",{href:!0});var s7r=s(U8);bIe=r(s7r,"FunnelConfig"),s7r.forEach(t),vIe=r(xve," (Funnel Transformer model)"),xve.forEach(t),TIe=i(T),sm=n(T,"LI",{});var kve=s(sm);mQ=n(kve,"STRONG",{});var l7r=s(mQ);FIe=r(l7r,"gpt2"),l7r.forEach(t),CIe=r(kve," \u2014 "),J8=n(kve,"A",{href:!0});var i7r=s(J8);MIe=r(i7r,"GPT2Config"),i7r.forEach(t),EIe=r(kve," (OpenAI GPT-2 model)"),kve.forEach(t),yIe=i(T),lm=n(T,"LI",{});var Rve=s(lm);gQ=n(Rve,"STRONG",{});var d7r=s(gQ);wIe=r(d7r,"gpt_neo"),d7r.forEach(t),AIe=r(Rve," \u2014 "),Y8=n(Rve,"A",{href:!0});var c7r=s(Y8);LIe=r(c7r,"GPTNeoConfig"),c7r.forEach(t),BIe=r(Rve," (GPT Neo model)"),Rve.forEach(t),xIe=i(T),im=n(T,"LI",{});var Sve=s(im);hQ=n(Sve,"STRONG",{});var f7r=s(hQ);kIe=r(f7r,"gptj"),f7r.forEach(t),RIe=r(Sve," \u2014 "),K8=n(Sve,"A",{href:!0});var m7r=s(K8);SIe=r(m7r,"GPTJConfig"),m7r.forEach(t),PIe=r(Sve," (GPT-J model)"),Sve.forEach(t),$Ie=i(T),dm=n(T,"LI",{});var Pve=s(dm);pQ=n(Pve,"STRONG",{});var g7r=s(pQ);IIe=r(g7r,"hubert"),g7r.forEach(t),DIe=r(Pve," \u2014 "),Z8=n(Pve,"A",{href:!0});var h7r=s(Z8);jIe=r(h7r,"HubertConfig"),h7r.forEach(t),NIe=r(Pve," (Hubert model)"),Pve.forEach(t),qIe=i(T),cm=n(T,"LI",{});var $ve=s(cm);_Q=n($ve,"STRONG",{});var p7r=s(_Q);GIe=r(p7r,"ibert"),p7r.forEach(t),OIe=r($ve," \u2014 "),e7=n($ve,"A",{href:!0});var _7r=s(e7);XIe=r(_7r,"IBertConfig"),_7r.forEach(t),VIe=r($ve," (I-BERT model)"),$ve.forEach(t),zIe=i(T),fm=n(T,"LI",{});var Ive=s(fm);uQ=n(Ive,"STRONG",{});var u7r=s(uQ);WIe=r(u7r,"imagegpt"),u7r.forEach(t),QIe=r(Ive," \u2014 "),o7=n(Ive,"A",{href:!0});var b7r=s(o7);HIe=r(b7r,"ImageGPTConfig"),b7r.forEach(t),UIe=r(Ive," (ImageGPT model)"),Ive.forEach(t),JIe=i(T),mm=n(T,"LI",{});var Dve=s(mm);bQ=n(Dve,"STRONG",{});var v7r=s(bQ);YIe=r(v7r,"layoutlm"),v7r.forEach(t),KIe=r(Dve," \u2014 "),r7=n(Dve,"A",{href:!0});var T7r=s(r7);ZIe=r(T7r,"LayoutLMConfig"),T7r.forEach(t),eDe=r(Dve," (LayoutLM model)"),Dve.forEach(t),oDe=i(T),gm=n(T,"LI",{});var jve=s(gm);vQ=n(jve,"STRONG",{});var F7r=s(vQ);rDe=r(F7r,"layoutlmv2"),F7r.forEach(t),tDe=r(jve," \u2014 "),t7=n(jve,"A",{href:!0});var C7r=s(t7);aDe=r(C7r,"LayoutLMv2Config"),C7r.forEach(t),nDe=r(jve," (LayoutLMv2 model)"),jve.forEach(t),sDe=i(T),hm=n(T,"LI",{});var Nve=s(hm);TQ=n(Nve,"STRONG",{});var M7r=s(TQ);lDe=r(M7r,"led"),M7r.forEach(t),iDe=r(Nve," \u2014 "),a7=n(Nve,"A",{href:!0});var E7r=s(a7);dDe=r(E7r,"LEDConfig"),E7r.forEach(t),cDe=r(Nve," (LED model)"),Nve.forEach(t),fDe=i(T),pm=n(T,"LI",{});var qve=s(pm);FQ=n(qve,"STRONG",{});var y7r=s(FQ);mDe=r(y7r,"longformer"),y7r.forEach(t),gDe=r(qve," \u2014 "),n7=n(qve,"A",{href:!0});var w7r=s(n7);hDe=r(w7r,"LongformerConfig"),w7r.forEach(t),pDe=r(qve," (Longformer model)"),qve.forEach(t),_De=i(T),_m=n(T,"LI",{});var Gve=s(_m);CQ=n(Gve,"STRONG",{});var A7r=s(CQ);uDe=r(A7r,"luke"),A7r.forEach(t),bDe=r(Gve," \u2014 "),s7=n(Gve,"A",{href:!0});var L7r=s(s7);vDe=r(L7r,"LukeConfig"),L7r.forEach(t),TDe=r(Gve," (LUKE model)"),Gve.forEach(t),FDe=i(T),um=n(T,"LI",{});var Ove=s(um);MQ=n(Ove,"STRONG",{});var B7r=s(MQ);CDe=r(B7r,"lxmert"),B7r.forEach(t),MDe=r(Ove," \u2014 "),l7=n(Ove,"A",{href:!0});var x7r=s(l7);EDe=r(x7r,"LxmertConfig"),x7r.forEach(t),yDe=r(Ove," (LXMERT model)"),Ove.forEach(t),wDe=i(T),bm=n(T,"LI",{});var Xve=s(bm);EQ=n(Xve,"STRONG",{});var k7r=s(EQ);ADe=r(k7r,"m2m_100"),k7r.forEach(t),LDe=r(Xve," \u2014 "),i7=n(Xve,"A",{href:!0});var R7r=s(i7);BDe=r(R7r,"M2M100Config"),R7r.forEach(t),xDe=r(Xve," (M2M100 model)"),Xve.forEach(t),kDe=i(T),vm=n(T,"LI",{});var Vve=s(vm);yQ=n(Vve,"STRONG",{});var S7r=s(yQ);RDe=r(S7r,"marian"),S7r.forEach(t),SDe=r(Vve," \u2014 "),d7=n(Vve,"A",{href:!0});var P7r=s(d7);PDe=r(P7r,"MarianConfig"),P7r.forEach(t),$De=r(Vve," (Marian model)"),Vve.forEach(t),IDe=i(T),Tm=n(T,"LI",{});var zve=s(Tm);wQ=n(zve,"STRONG",{});var $7r=s(wQ);DDe=r($7r,"maskformer"),$7r.forEach(t),jDe=r(zve," \u2014 "),c7=n(zve,"A",{href:!0});var I7r=s(c7);NDe=r(I7r,"MaskFormerConfig"),I7r.forEach(t),qDe=r(zve," (MaskFormer model)"),zve.forEach(t),GDe=i(T),Fm=n(T,"LI",{});var Wve=s(Fm);AQ=n(Wve,"STRONG",{});var D7r=s(AQ);ODe=r(D7r,"mbart"),D7r.forEach(t),XDe=r(Wve," \u2014 "),f7=n(Wve,"A",{href:!0});var j7r=s(f7);VDe=r(j7r,"MBartConfig"),j7r.forEach(t),zDe=r(Wve," (mBART model)"),Wve.forEach(t),WDe=i(T),Cm=n(T,"LI",{});var Qve=s(Cm);LQ=n(Qve,"STRONG",{});var N7r=s(LQ);QDe=r(N7r,"megatron-bert"),N7r.forEach(t),HDe=r(Qve," \u2014 "),m7=n(Qve,"A",{href:!0});var q7r=s(m7);UDe=r(q7r,"MegatronBertConfig"),q7r.forEach(t),JDe=r(Qve," (MegatronBert model)"),Qve.forEach(t),YDe=i(T),Mm=n(T,"LI",{});var Hve=s(Mm);BQ=n(Hve,"STRONG",{});var G7r=s(BQ);KDe=r(G7r,"mobilebert"),G7r.forEach(t),ZDe=r(Hve," \u2014 "),g7=n(Hve,"A",{href:!0});var O7r=s(g7);eje=r(O7r,"MobileBertConfig"),O7r.forEach(t),oje=r(Hve," (MobileBERT model)"),Hve.forEach(t),rje=i(T),Em=n(T,"LI",{});var Uve=s(Em);xQ=n(Uve,"STRONG",{});var X7r=s(xQ);tje=r(X7r,"mpnet"),X7r.forEach(t),aje=r(Uve," \u2014 "),h7=n(Uve,"A",{href:!0});var V7r=s(h7);nje=r(V7r,"MPNetConfig"),V7r.forEach(t),sje=r(Uve," (MPNet model)"),Uve.forEach(t),lje=i(T),ym=n(T,"LI",{});var Jve=s(ym);kQ=n(Jve,"STRONG",{});var z7r=s(kQ);ije=r(z7r,"mt5"),z7r.forEach(t),dje=r(Jve," \u2014 "),p7=n(Jve,"A",{href:!0});var W7r=s(p7);cje=r(W7r,"MT5Config"),W7r.forEach(t),fje=r(Jve," (mT5 model)"),Jve.forEach(t),mje=i(T),wm=n(T,"LI",{});var Yve=s(wm);RQ=n(Yve,"STRONG",{});var Q7r=s(RQ);gje=r(Q7r,"nystromformer"),Q7r.forEach(t),hje=r(Yve," \u2014 "),_7=n(Yve,"A",{href:!0});var H7r=s(_7);pje=r(H7r,"NystromformerConfig"),H7r.forEach(t),_je=r(Yve," (Nystromformer model)"),Yve.forEach(t),uje=i(T),Am=n(T,"LI",{});var Kve=s(Am);SQ=n(Kve,"STRONG",{});var U7r=s(SQ);bje=r(U7r,"openai-gpt"),U7r.forEach(t),vje=r(Kve," \u2014 "),u7=n(Kve,"A",{href:!0});var J7r=s(u7);Tje=r(J7r,"OpenAIGPTConfig"),J7r.forEach(t),Fje=r(Kve," (OpenAI GPT model)"),Kve.forEach(t),Cje=i(T),Lm=n(T,"LI",{});var Zve=s(Lm);PQ=n(Zve,"STRONG",{});var Y7r=s(PQ);Mje=r(Y7r,"pegasus"),Y7r.forEach(t),Eje=r(Zve," \u2014 "),b7=n(Zve,"A",{href:!0});var K7r=s(b7);yje=r(K7r,"PegasusConfig"),K7r.forEach(t),wje=r(Zve," (Pegasus model)"),Zve.forEach(t),Aje=i(T),Bm=n(T,"LI",{});var eTe=s(Bm);$Q=n(eTe,"STRONG",{});var Z7r=s($Q);Lje=r(Z7r,"perceiver"),Z7r.forEach(t),Bje=r(eTe," \u2014 "),v7=n(eTe,"A",{href:!0});var eBr=s(v7);xje=r(eBr,"PerceiverConfig"),eBr.forEach(t),kje=r(eTe," (Perceiver model)"),eTe.forEach(t),Rje=i(T),xm=n(T,"LI",{});var oTe=s(xm);IQ=n(oTe,"STRONG",{});var oBr=s(IQ);Sje=r(oBr,"plbart"),oBr.forEach(t),Pje=r(oTe," \u2014 "),T7=n(oTe,"A",{href:!0});var rBr=s(T7);$je=r(rBr,"PLBartConfig"),rBr.forEach(t),Ije=r(oTe," (PLBart model)"),oTe.forEach(t),Dje=i(T),km=n(T,"LI",{});var rTe=s(km);DQ=n(rTe,"STRONG",{});var tBr=s(DQ);jje=r(tBr,"poolformer"),tBr.forEach(t),Nje=r(rTe," \u2014 "),F7=n(rTe,"A",{href:!0});var aBr=s(F7);qje=r(aBr,"PoolFormerConfig"),aBr.forEach(t),Gje=r(rTe," (PoolFormer model)"),rTe.forEach(t),Oje=i(T),Rm=n(T,"LI",{});var tTe=s(Rm);jQ=n(tTe,"STRONG",{});var nBr=s(jQ);Xje=r(nBr,"prophetnet"),nBr.forEach(t),Vje=r(tTe," \u2014 "),C7=n(tTe,"A",{href:!0});var sBr=s(C7);zje=r(sBr,"ProphetNetConfig"),sBr.forEach(t),Wje=r(tTe," (ProphetNet model)"),tTe.forEach(t),Qje=i(T),Sm=n(T,"LI",{});var aTe=s(Sm);NQ=n(aTe,"STRONG",{});var lBr=s(NQ);Hje=r(lBr,"qdqbert"),lBr.forEach(t),Uje=r(aTe," \u2014 "),M7=n(aTe,"A",{href:!0});var iBr=s(M7);Jje=r(iBr,"QDQBertConfig"),iBr.forEach(t),Yje=r(aTe," (QDQBert model)"),aTe.forEach(t),Kje=i(T),Pm=n(T,"LI",{});var nTe=s(Pm);qQ=n(nTe,"STRONG",{});var dBr=s(qQ);Zje=r(dBr,"rag"),dBr.forEach(t),eNe=r(nTe," \u2014 "),E7=n(nTe,"A",{href:!0});var cBr=s(E7);oNe=r(cBr,"RagConfig"),cBr.forEach(t),rNe=r(nTe," (RAG model)"),nTe.forEach(t),tNe=i(T),$m=n(T,"LI",{});var sTe=s($m);GQ=n(sTe,"STRONG",{});var fBr=s(GQ);aNe=r(fBr,"realm"),fBr.forEach(t),nNe=r(sTe," \u2014 "),y7=n(sTe,"A",{href:!0});var mBr=s(y7);sNe=r(mBr,"RealmConfig"),mBr.forEach(t),lNe=r(sTe," (Realm model)"),sTe.forEach(t),iNe=i(T),Im=n(T,"LI",{});var lTe=s(Im);OQ=n(lTe,"STRONG",{});var gBr=s(OQ);dNe=r(gBr,"reformer"),gBr.forEach(t),cNe=r(lTe," \u2014 "),w7=n(lTe,"A",{href:!0});var hBr=s(w7);fNe=r(hBr,"ReformerConfig"),hBr.forEach(t),mNe=r(lTe," (Reformer model)"),lTe.forEach(t),gNe=i(T),Dm=n(T,"LI",{});var iTe=s(Dm);XQ=n(iTe,"STRONG",{});var pBr=s(XQ);hNe=r(pBr,"rembert"),pBr.forEach(t),pNe=r(iTe," \u2014 "),A7=n(iTe,"A",{href:!0});var _Br=s(A7);_Ne=r(_Br,"RemBertConfig"),_Br.forEach(t),uNe=r(iTe," (RemBERT model)"),iTe.forEach(t),bNe=i(T),jm=n(T,"LI",{});var dTe=s(jm);VQ=n(dTe,"STRONG",{});var uBr=s(VQ);vNe=r(uBr,"retribert"),uBr.forEach(t),TNe=r(dTe," \u2014 "),L7=n(dTe,"A",{href:!0});var bBr=s(L7);FNe=r(bBr,"RetriBertConfig"),bBr.forEach(t),CNe=r(dTe," (RetriBERT model)"),dTe.forEach(t),MNe=i(T),Nm=n(T,"LI",{});var cTe=s(Nm);zQ=n(cTe,"STRONG",{});var vBr=s(zQ);ENe=r(vBr,"roberta"),vBr.forEach(t),yNe=r(cTe," \u2014 "),B7=n(cTe,"A",{href:!0});var TBr=s(B7);wNe=r(TBr,"RobertaConfig"),TBr.forEach(t),ANe=r(cTe," (RoBERTa model)"),cTe.forEach(t),LNe=i(T),qm=n(T,"LI",{});var fTe=s(qm);WQ=n(fTe,"STRONG",{});var FBr=s(WQ);BNe=r(FBr,"roformer"),FBr.forEach(t),xNe=r(fTe," \u2014 "),x7=n(fTe,"A",{href:!0});var CBr=s(x7);kNe=r(CBr,"RoFormerConfig"),CBr.forEach(t),RNe=r(fTe," (RoFormer model)"),fTe.forEach(t),SNe=i(T),Gm=n(T,"LI",{});var mTe=s(Gm);QQ=n(mTe,"STRONG",{});var MBr=s(QQ);PNe=r(MBr,"segformer"),MBr.forEach(t),$Ne=r(mTe," \u2014 "),k7=n(mTe,"A",{href:!0});var EBr=s(k7);INe=r(EBr,"SegformerConfig"),EBr.forEach(t),DNe=r(mTe," (SegFormer model)"),mTe.forEach(t),jNe=i(T),Om=n(T,"LI",{});var gTe=s(Om);HQ=n(gTe,"STRONG",{});var yBr=s(HQ);NNe=r(yBr,"sew"),yBr.forEach(t),qNe=r(gTe," \u2014 "),R7=n(gTe,"A",{href:!0});var wBr=s(R7);GNe=r(wBr,"SEWConfig"),wBr.forEach(t),ONe=r(gTe," (SEW model)"),gTe.forEach(t),XNe=i(T),Xm=n(T,"LI",{});var hTe=s(Xm);UQ=n(hTe,"STRONG",{});var ABr=s(UQ);VNe=r(ABr,"sew-d"),ABr.forEach(t),zNe=r(hTe," \u2014 "),S7=n(hTe,"A",{href:!0});var LBr=s(S7);WNe=r(LBr,"SEWDConfig"),LBr.forEach(t),QNe=r(hTe," (SEW-D model)"),hTe.forEach(t),HNe=i(T),Vm=n(T,"LI",{});var pTe=s(Vm);JQ=n(pTe,"STRONG",{});var BBr=s(JQ);UNe=r(BBr,"speech-encoder-decoder"),BBr.forEach(t),JNe=r(pTe," \u2014 "),P7=n(pTe,"A",{href:!0});var xBr=s(P7);YNe=r(xBr,"SpeechEncoderDecoderConfig"),xBr.forEach(t),KNe=r(pTe," (Speech Encoder decoder model)"),pTe.forEach(t),ZNe=i(T),zm=n(T,"LI",{});var _Te=s(zm);YQ=n(_Te,"STRONG",{});var kBr=s(YQ);eqe=r(kBr,"speech_to_text"),kBr.forEach(t),oqe=r(_Te," \u2014 "),$7=n(_Te,"A",{href:!0});var RBr=s($7);rqe=r(RBr,"Speech2TextConfig"),RBr.forEach(t),tqe=r(_Te," (Speech2Text model)"),_Te.forEach(t),aqe=i(T),Wm=n(T,"LI",{});var uTe=s(Wm);KQ=n(uTe,"STRONG",{});var SBr=s(KQ);nqe=r(SBr,"speech_to_text_2"),SBr.forEach(t),sqe=r(uTe," \u2014 "),I7=n(uTe,"A",{href:!0});var PBr=s(I7);lqe=r(PBr,"Speech2Text2Config"),PBr.forEach(t),iqe=r(uTe," (Speech2Text2 model)"),uTe.forEach(t),dqe=i(T),Qm=n(T,"LI",{});var bTe=s(Qm);ZQ=n(bTe,"STRONG",{});var $Br=s(ZQ);cqe=r($Br,"splinter"),$Br.forEach(t),fqe=r(bTe," \u2014 "),D7=n(bTe,"A",{href:!0});var IBr=s(D7);mqe=r(IBr,"SplinterConfig"),IBr.forEach(t),gqe=r(bTe," (Splinter model)"),bTe.forEach(t),hqe=i(T),Hm=n(T,"LI",{});var vTe=s(Hm);eH=n(vTe,"STRONG",{});var DBr=s(eH);pqe=r(DBr,"squeezebert"),DBr.forEach(t),_qe=r(vTe," \u2014 "),j7=n(vTe,"A",{href:!0});var jBr=s(j7);uqe=r(jBr,"SqueezeBertConfig"),jBr.forEach(t),bqe=r(vTe," (SqueezeBERT model)"),vTe.forEach(t),vqe=i(T),Um=n(T,"LI",{});var TTe=s(Um);oH=n(TTe,"STRONG",{});var NBr=s(oH);Tqe=r(NBr,"swin"),NBr.forEach(t),Fqe=r(TTe," \u2014 "),N7=n(TTe,"A",{href:!0});var qBr=s(N7);Cqe=r(qBr,"SwinConfig"),qBr.forEach(t),Mqe=r(TTe," (Swin model)"),TTe.forEach(t),Eqe=i(T),Jm=n(T,"LI",{});var FTe=s(Jm);rH=n(FTe,"STRONG",{});var GBr=s(rH);yqe=r(GBr,"t5"),GBr.forEach(t),wqe=r(FTe," \u2014 "),q7=n(FTe,"A",{href:!0});var OBr=s(q7);Aqe=r(OBr,"T5Config"),OBr.forEach(t),Lqe=r(FTe," (T5 model)"),FTe.forEach(t),Bqe=i(T),Ym=n(T,"LI",{});var CTe=s(Ym);tH=n(CTe,"STRONG",{});var XBr=s(tH);xqe=r(XBr,"tapas"),XBr.forEach(t),kqe=r(CTe," \u2014 "),G7=n(CTe,"A",{href:!0});var VBr=s(G7);Rqe=r(VBr,"TapasConfig"),VBr.forEach(t),Sqe=r(CTe," (TAPAS model)"),CTe.forEach(t),Pqe=i(T),Km=n(T,"LI",{});var MTe=s(Km);aH=n(MTe,"STRONG",{});var zBr=s(aH);$qe=r(zBr,"transfo-xl"),zBr.forEach(t),Iqe=r(MTe," \u2014 "),O7=n(MTe,"A",{href:!0});var WBr=s(O7);Dqe=r(WBr,"TransfoXLConfig"),WBr.forEach(t),jqe=r(MTe," (Transformer-XL model)"),MTe.forEach(t),Nqe=i(T),Zm=n(T,"LI",{});var ETe=s(Zm);nH=n(ETe,"STRONG",{});var QBr=s(nH);qqe=r(QBr,"trocr"),QBr.forEach(t),Gqe=r(ETe," \u2014 "),X7=n(ETe,"A",{href:!0});var HBr=s(X7);Oqe=r(HBr,"TrOCRConfig"),HBr.forEach(t),Xqe=r(ETe," (TrOCR model)"),ETe.forEach(t),Vqe=i(T),eg=n(T,"LI",{});var yTe=s(eg);sH=n(yTe,"STRONG",{});var UBr=s(sH);zqe=r(UBr,"unispeech"),UBr.forEach(t),Wqe=r(yTe," \u2014 "),V7=n(yTe,"A",{href:!0});var JBr=s(V7);Qqe=r(JBr,"UniSpeechConfig"),JBr.forEach(t),Hqe=r(yTe," (UniSpeech model)"),yTe.forEach(t),Uqe=i(T),og=n(T,"LI",{});var wTe=s(og);lH=n(wTe,"STRONG",{});var YBr=s(lH);Jqe=r(YBr,"unispeech-sat"),YBr.forEach(t),Yqe=r(wTe," \u2014 "),z7=n(wTe,"A",{href:!0});var KBr=s(z7);Kqe=r(KBr,"UniSpeechSatConfig"),KBr.forEach(t),Zqe=r(wTe," (UniSpeechSat model)"),wTe.forEach(t),eGe=i(T),rg=n(T,"LI",{});var ATe=s(rg);iH=n(ATe,"STRONG",{});var ZBr=s(iH);oGe=r(ZBr,"vilt"),ZBr.forEach(t),rGe=r(ATe," \u2014 "),W7=n(ATe,"A",{href:!0});var exr=s(W7);tGe=r(exr,"ViltConfig"),exr.forEach(t),aGe=r(ATe," (ViLT model)"),ATe.forEach(t),nGe=i(T),tg=n(T,"LI",{});var LTe=s(tg);dH=n(LTe,"STRONG",{});var oxr=s(dH);sGe=r(oxr,"vision-encoder-decoder"),oxr.forEach(t),lGe=r(LTe," \u2014 "),Q7=n(LTe,"A",{href:!0});var rxr=s(Q7);iGe=r(rxr,"VisionEncoderDecoderConfig"),rxr.forEach(t),dGe=r(LTe," (Vision Encoder decoder model)"),LTe.forEach(t),cGe=i(T),ag=n(T,"LI",{});var BTe=s(ag);cH=n(BTe,"STRONG",{});var txr=s(cH);fGe=r(txr,"vision-text-dual-encoder"),txr.forEach(t),mGe=r(BTe," \u2014 "),H7=n(BTe,"A",{href:!0});var axr=s(H7);gGe=r(axr,"VisionTextDualEncoderConfig"),axr.forEach(t),hGe=r(BTe," (VisionTextDualEncoder model)"),BTe.forEach(t),pGe=i(T),ng=n(T,"LI",{});var xTe=s(ng);fH=n(xTe,"STRONG",{});var nxr=s(fH);_Ge=r(nxr,"visual_bert"),nxr.forEach(t),uGe=r(xTe," \u2014 "),U7=n(xTe,"A",{href:!0});var sxr=s(U7);bGe=r(sxr,"VisualBertConfig"),sxr.forEach(t),vGe=r(xTe," (VisualBert model)"),xTe.forEach(t),TGe=i(T),sg=n(T,"LI",{});var kTe=s(sg);mH=n(kTe,"STRONG",{});var lxr=s(mH);FGe=r(lxr,"vit"),lxr.forEach(t),CGe=r(kTe," \u2014 "),J7=n(kTe,"A",{href:!0});var ixr=s(J7);MGe=r(ixr,"ViTConfig"),ixr.forEach(t),EGe=r(kTe," (ViT model)"),kTe.forEach(t),yGe=i(T),lg=n(T,"LI",{});var RTe=s(lg);gH=n(RTe,"STRONG",{});var dxr=s(gH);wGe=r(dxr,"vit_mae"),dxr.forEach(t),AGe=r(RTe," \u2014 "),Y7=n(RTe,"A",{href:!0});var cxr=s(Y7);LGe=r(cxr,"ViTMAEConfig"),cxr.forEach(t),BGe=r(RTe," (ViTMAE model)"),RTe.forEach(t),xGe=i(T),ig=n(T,"LI",{});var STe=s(ig);hH=n(STe,"STRONG",{});var fxr=s(hH);kGe=r(fxr,"wav2vec2"),fxr.forEach(t),RGe=r(STe," \u2014 "),K7=n(STe,"A",{href:!0});var mxr=s(K7);SGe=r(mxr,"Wav2Vec2Config"),mxr.forEach(t),PGe=r(STe," (Wav2Vec2 model)"),STe.forEach(t),$Ge=i(T),dg=n(T,"LI",{});var PTe=s(dg);pH=n(PTe,"STRONG",{});var gxr=s(pH);IGe=r(gxr,"wavlm"),gxr.forEach(t),DGe=r(PTe," \u2014 "),Z7=n(PTe,"A",{href:!0});var hxr=s(Z7);jGe=r(hxr,"WavLMConfig"),hxr.forEach(t),NGe=r(PTe," (WavLM model)"),PTe.forEach(t),qGe=i(T),cg=n(T,"LI",{});var $Te=s(cg);_H=n($Te,"STRONG",{});var pxr=s(_H);GGe=r(pxr,"xglm"),pxr.forEach(t),OGe=r($Te," \u2014 "),eB=n($Te,"A",{href:!0});var _xr=s(eB);XGe=r(_xr,"XGLMConfig"),_xr.forEach(t),VGe=r($Te," (XGLM model)"),$Te.forEach(t),zGe=i(T),fg=n(T,"LI",{});var ITe=s(fg);uH=n(ITe,"STRONG",{});var uxr=s(uH);WGe=r(uxr,"xlm"),uxr.forEach(t),QGe=r(ITe," \u2014 "),oB=n(ITe,"A",{href:!0});var bxr=s(oB);HGe=r(bxr,"XLMConfig"),bxr.forEach(t),UGe=r(ITe," (XLM model)"),ITe.forEach(t),JGe=i(T),mg=n(T,"LI",{});var DTe=s(mg);bH=n(DTe,"STRONG",{});var vxr=s(bH);YGe=r(vxr,"xlm-prophetnet"),vxr.forEach(t),KGe=r(DTe," \u2014 "),rB=n(DTe,"A",{href:!0});var Txr=s(rB);ZGe=r(Txr,"XLMProphetNetConfig"),Txr.forEach(t),eOe=r(DTe," (XLMProphetNet model)"),DTe.forEach(t),oOe=i(T),gg=n(T,"LI",{});var jTe=s(gg);vH=n(jTe,"STRONG",{});var Fxr=s(vH);rOe=r(Fxr,"xlm-roberta"),Fxr.forEach(t),tOe=r(jTe," \u2014 "),tB=n(jTe,"A",{href:!0});var Cxr=s(tB);aOe=r(Cxr,"XLMRobertaConfig"),Cxr.forEach(t),nOe=r(jTe," (XLM-RoBERTa model)"),jTe.forEach(t),sOe=i(T),hg=n(T,"LI",{});var NTe=s(hg);TH=n(NTe,"STRONG",{});var Mxr=s(TH);lOe=r(Mxr,"xlm-roberta-xl"),Mxr.forEach(t),iOe=r(NTe," \u2014 "),aB=n(NTe,"A",{href:!0});var Exr=s(aB);dOe=r(Exr,"XLMRobertaXLConfig"),Exr.forEach(t),cOe=r(NTe," (XLM-RoBERTa-XL model)"),NTe.forEach(t),fOe=i(T),pg=n(T,"LI",{});var qTe=s(pg);FH=n(qTe,"STRONG",{});var yxr=s(FH);mOe=r(yxr,"xlnet"),yxr.forEach(t),gOe=r(qTe," \u2014 "),nB=n(qTe,"A",{href:!0});var wxr=s(nB);hOe=r(wxr,"XLNetConfig"),wxr.forEach(t),pOe=r(qTe," (XLNet model)"),qTe.forEach(t),_Oe=i(T),_g=n(T,"LI",{});var GTe=s(_g);CH=n(GTe,"STRONG",{});var Axr=s(CH);uOe=r(Axr,"yoso"),Axr.forEach(t),bOe=r(GTe," \u2014 "),sB=n(GTe,"A",{href:!0});var Lxr=s(sB);vOe=r(Lxr,"YosoConfig"),Lxr.forEach(t),TOe=r(GTe," (YOSO model)"),GTe.forEach(t),T.forEach(t),FOe=i(ia),MH=n(ia,"P",{});var Bxr=s(MH);COe=r(Bxr,"Examples:"),Bxr.forEach(t),MOe=i(ia),m(q4.$$.fragment,ia),ia.forEach(t),EOe=i(Is),ug=n(Is,"DIV",{class:!0});var ske=s(ug);m(G4.$$.fragment,ske),yOe=i(ske),EH=n(ske,"P",{});var xxr=s(EH);wOe=r(xxr,"Register a new configuration for this class."),xxr.forEach(t),ske.forEach(t),Is.forEach(t),sBe=i(c),ji=n(c,"H2",{class:!0});var lke=s(ji);bg=n(lke,"A",{id:!0,class:!0,href:!0});var kxr=s(bg);yH=n(kxr,"SPAN",{});var Rxr=s(yH);m(O4.$$.fragment,Rxr),Rxr.forEach(t),kxr.forEach(t),AOe=i(lke),wH=n(lke,"SPAN",{});var Sxr=s(wH);LOe=r(Sxr,"AutoTokenizer"),Sxr.forEach(t),lke.forEach(t),lBe=i(c),Wo=n(c,"DIV",{class:!0});var Ds=s(Wo);m(X4.$$.fragment,Ds),BOe=i(Ds),V4=n(Ds,"P",{});var ike=s(V4);xOe=r(ike,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),lB=n(ike,"A",{href:!0});var Pxr=s(lB);kOe=r(Pxr,"AutoTokenizer.from_pretrained()"),Pxr.forEach(t),ROe=r(ike," class method."),ike.forEach(t),SOe=i(Ds),z4=n(Ds,"P",{});var dke=s(z4);POe=r(dke,"This class cannot be instantiated directly using "),AH=n(dke,"CODE",{});var $xr=s(AH);$Oe=r($xr,"__init__()"),$xr.forEach(t),IOe=r(dke," (throws an error)."),dke.forEach(t),DOe=i(Ds),mo=n(Ds,"DIV",{class:!0});var da=s(mo);m(W4.$$.fragment,da),jOe=i(da),LH=n(da,"P",{});var Ixr=s(LH);NOe=r(Ixr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Ixr.forEach(t),qOe=i(da),Da=n(da,"P",{});var kM=s(Da);GOe=r(kM,"The tokenizer class to instantiate is selected based on the "),BH=n(kM,"CODE",{});var Dxr=s(BH);OOe=r(Dxr,"model_type"),Dxr.forEach(t),XOe=r(kM,` property of the config object (either
passed as an argument or loaded from `),xH=n(kM,"CODE",{});var jxr=s(xH);VOe=r(jxr,"pretrained_model_name_or_path"),jxr.forEach(t),zOe=r(kM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kH=n(kM,"CODE",{});var Nxr=s(kH);WOe=r(Nxr,"pretrained_model_name_or_path"),Nxr.forEach(t),QOe=r(kM,":"),kM.forEach(t),HOe=i(da),M=n(da,"UL",{});var y=s(M);Gn=n(y,"LI",{});var vL=s(Gn);RH=n(vL,"STRONG",{});var qxr=s(RH);UOe=r(qxr,"albert"),qxr.forEach(t),JOe=r(vL," \u2014 "),iB=n(vL,"A",{href:!0});var Gxr=s(iB);YOe=r(Gxr,"AlbertTokenizer"),Gxr.forEach(t),KOe=r(vL," or "),dB=n(vL,"A",{href:!0});var Oxr=s(dB);ZOe=r(Oxr,"AlbertTokenizerFast"),Oxr.forEach(t),eXe=r(vL," (ALBERT model)"),vL.forEach(t),oXe=i(y),On=n(y,"LI",{});var TL=s(On);SH=n(TL,"STRONG",{});var Xxr=s(SH);rXe=r(Xxr,"bart"),Xxr.forEach(t),tXe=r(TL," \u2014 "),cB=n(TL,"A",{href:!0});var Vxr=s(cB);aXe=r(Vxr,"BartTokenizer"),Vxr.forEach(t),nXe=r(TL," or "),fB=n(TL,"A",{href:!0});var zxr=s(fB);sXe=r(zxr,"BartTokenizerFast"),zxr.forEach(t),lXe=r(TL," (BART model)"),TL.forEach(t),iXe=i(y),Xn=n(y,"LI",{});var FL=s(Xn);PH=n(FL,"STRONG",{});var Wxr=s(PH);dXe=r(Wxr,"barthez"),Wxr.forEach(t),cXe=r(FL," \u2014 "),mB=n(FL,"A",{href:!0});var Qxr=s(mB);fXe=r(Qxr,"BarthezTokenizer"),Qxr.forEach(t),mXe=r(FL," or "),gB=n(FL,"A",{href:!0});var Hxr=s(gB);gXe=r(Hxr,"BarthezTokenizerFast"),Hxr.forEach(t),hXe=r(FL," (BARThez model)"),FL.forEach(t),pXe=i(y),vg=n(y,"LI",{});var OTe=s(vg);$H=n(OTe,"STRONG",{});var Uxr=s($H);_Xe=r(Uxr,"bartpho"),Uxr.forEach(t),uXe=r(OTe," \u2014 "),hB=n(OTe,"A",{href:!0});var Jxr=s(hB);bXe=r(Jxr,"BartphoTokenizer"),Jxr.forEach(t),vXe=r(OTe," (BARTpho model)"),OTe.forEach(t),TXe=i(y),Vn=n(y,"LI",{});var CL=s(Vn);IH=n(CL,"STRONG",{});var Yxr=s(IH);FXe=r(Yxr,"bert"),Yxr.forEach(t),CXe=r(CL," \u2014 "),pB=n(CL,"A",{href:!0});var Kxr=s(pB);MXe=r(Kxr,"BertTokenizer"),Kxr.forEach(t),EXe=r(CL," or "),_B=n(CL,"A",{href:!0});var Zxr=s(_B);yXe=r(Zxr,"BertTokenizerFast"),Zxr.forEach(t),wXe=r(CL," (BERT model)"),CL.forEach(t),AXe=i(y),Tg=n(y,"LI",{});var XTe=s(Tg);DH=n(XTe,"STRONG",{});var ekr=s(DH);LXe=r(ekr,"bert-generation"),ekr.forEach(t),BXe=r(XTe," \u2014 "),uB=n(XTe,"A",{href:!0});var okr=s(uB);xXe=r(okr,"BertGenerationTokenizer"),okr.forEach(t),kXe=r(XTe," (Bert Generation model)"),XTe.forEach(t),RXe=i(y),Fg=n(y,"LI",{});var VTe=s(Fg);jH=n(VTe,"STRONG",{});var rkr=s(jH);SXe=r(rkr,"bert-japanese"),rkr.forEach(t),PXe=r(VTe," \u2014 "),bB=n(VTe,"A",{href:!0});var tkr=s(bB);$Xe=r(tkr,"BertJapaneseTokenizer"),tkr.forEach(t),IXe=r(VTe," (BertJapanese model)"),VTe.forEach(t),DXe=i(y),Cg=n(y,"LI",{});var zTe=s(Cg);NH=n(zTe,"STRONG",{});var akr=s(NH);jXe=r(akr,"bertweet"),akr.forEach(t),NXe=r(zTe," \u2014 "),vB=n(zTe,"A",{href:!0});var nkr=s(vB);qXe=r(nkr,"BertweetTokenizer"),nkr.forEach(t),GXe=r(zTe," (Bertweet model)"),zTe.forEach(t),OXe=i(y),zn=n(y,"LI",{});var ML=s(zn);qH=n(ML,"STRONG",{});var skr=s(qH);XXe=r(skr,"big_bird"),skr.forEach(t),VXe=r(ML," \u2014 "),TB=n(ML,"A",{href:!0});var lkr=s(TB);zXe=r(lkr,"BigBirdTokenizer"),lkr.forEach(t),WXe=r(ML," or "),FB=n(ML,"A",{href:!0});var ikr=s(FB);QXe=r(ikr,"BigBirdTokenizerFast"),ikr.forEach(t),HXe=r(ML," (BigBird model)"),ML.forEach(t),UXe=i(y),Wn=n(y,"LI",{});var EL=s(Wn);GH=n(EL,"STRONG",{});var dkr=s(GH);JXe=r(dkr,"bigbird_pegasus"),dkr.forEach(t),YXe=r(EL," \u2014 "),CB=n(EL,"A",{href:!0});var ckr=s(CB);KXe=r(ckr,"PegasusTokenizer"),ckr.forEach(t),ZXe=r(EL," or "),MB=n(EL,"A",{href:!0});var fkr=s(MB);eVe=r(fkr,"PegasusTokenizerFast"),fkr.forEach(t),oVe=r(EL," (BigBirdPegasus model)"),EL.forEach(t),rVe=i(y),Qn=n(y,"LI",{});var yL=s(Qn);OH=n(yL,"STRONG",{});var mkr=s(OH);tVe=r(mkr,"blenderbot"),mkr.forEach(t),aVe=r(yL," \u2014 "),EB=n(yL,"A",{href:!0});var gkr=s(EB);nVe=r(gkr,"BlenderbotTokenizer"),gkr.forEach(t),sVe=r(yL," or "),yB=n(yL,"A",{href:!0});var hkr=s(yB);lVe=r(hkr,"BlenderbotTokenizerFast"),hkr.forEach(t),iVe=r(yL," (Blenderbot model)"),yL.forEach(t),dVe=i(y),Mg=n(y,"LI",{});var WTe=s(Mg);XH=n(WTe,"STRONG",{});var pkr=s(XH);cVe=r(pkr,"blenderbot-small"),pkr.forEach(t),fVe=r(WTe," \u2014 "),wB=n(WTe,"A",{href:!0});var _kr=s(wB);mVe=r(_kr,"BlenderbotSmallTokenizer"),_kr.forEach(t),gVe=r(WTe," (BlenderbotSmall model)"),WTe.forEach(t),hVe=i(y),Eg=n(y,"LI",{});var QTe=s(Eg);VH=n(QTe,"STRONG",{});var ukr=s(VH);pVe=r(ukr,"byt5"),ukr.forEach(t),_Ve=r(QTe," \u2014 "),AB=n(QTe,"A",{href:!0});var bkr=s(AB);uVe=r(bkr,"ByT5Tokenizer"),bkr.forEach(t),bVe=r(QTe," (ByT5 model)"),QTe.forEach(t),vVe=i(y),Hn=n(y,"LI",{});var wL=s(Hn);zH=n(wL,"STRONG",{});var vkr=s(zH);TVe=r(vkr,"camembert"),vkr.forEach(t),FVe=r(wL," \u2014 "),LB=n(wL,"A",{href:!0});var Tkr=s(LB);CVe=r(Tkr,"CamembertTokenizer"),Tkr.forEach(t),MVe=r(wL," or "),BB=n(wL,"A",{href:!0});var Fkr=s(BB);EVe=r(Fkr,"CamembertTokenizerFast"),Fkr.forEach(t),yVe=r(wL," (CamemBERT model)"),wL.forEach(t),wVe=i(y),yg=n(y,"LI",{});var HTe=s(yg);WH=n(HTe,"STRONG",{});var Ckr=s(WH);AVe=r(Ckr,"canine"),Ckr.forEach(t),LVe=r(HTe," \u2014 "),xB=n(HTe,"A",{href:!0});var Mkr=s(xB);BVe=r(Mkr,"CanineTokenizer"),Mkr.forEach(t),xVe=r(HTe," (Canine model)"),HTe.forEach(t),kVe=i(y),Un=n(y,"LI",{});var AL=s(Un);QH=n(AL,"STRONG",{});var Ekr=s(QH);RVe=r(Ekr,"clip"),Ekr.forEach(t),SVe=r(AL," \u2014 "),kB=n(AL,"A",{href:!0});var ykr=s(kB);PVe=r(ykr,"CLIPTokenizer"),ykr.forEach(t),$Ve=r(AL," or "),RB=n(AL,"A",{href:!0});var wkr=s(RB);IVe=r(wkr,"CLIPTokenizerFast"),wkr.forEach(t),DVe=r(AL," (CLIP model)"),AL.forEach(t),jVe=i(y),Jn=n(y,"LI",{});var LL=s(Jn);HH=n(LL,"STRONG",{});var Akr=s(HH);NVe=r(Akr,"convbert"),Akr.forEach(t),qVe=r(LL," \u2014 "),SB=n(LL,"A",{href:!0});var Lkr=s(SB);GVe=r(Lkr,"ConvBertTokenizer"),Lkr.forEach(t),OVe=r(LL," or "),PB=n(LL,"A",{href:!0});var Bkr=s(PB);XVe=r(Bkr,"ConvBertTokenizerFast"),Bkr.forEach(t),VVe=r(LL," (ConvBERT model)"),LL.forEach(t),zVe=i(y),Yn=n(y,"LI",{});var BL=s(Yn);UH=n(BL,"STRONG",{});var xkr=s(UH);WVe=r(xkr,"cpm"),xkr.forEach(t),QVe=r(BL," \u2014 "),$B=n(BL,"A",{href:!0});var kkr=s($B);HVe=r(kkr,"CpmTokenizer"),kkr.forEach(t),UVe=r(BL," or "),JH=n(BL,"CODE",{});var Rkr=s(JH);JVe=r(Rkr,"CpmTokenizerFast"),Rkr.forEach(t),YVe=r(BL," (CPM model)"),BL.forEach(t),KVe=i(y),wg=n(y,"LI",{});var UTe=s(wg);YH=n(UTe,"STRONG",{});var Skr=s(YH);ZVe=r(Skr,"ctrl"),Skr.forEach(t),eze=r(UTe," \u2014 "),IB=n(UTe,"A",{href:!0});var Pkr=s(IB);oze=r(Pkr,"CTRLTokenizer"),Pkr.forEach(t),rze=r(UTe," (CTRL model)"),UTe.forEach(t),tze=i(y),Kn=n(y,"LI",{});var xL=s(Kn);KH=n(xL,"STRONG",{});var $kr=s(KH);aze=r($kr,"deberta"),$kr.forEach(t),nze=r(xL," \u2014 "),DB=n(xL,"A",{href:!0});var Ikr=s(DB);sze=r(Ikr,"DebertaTokenizer"),Ikr.forEach(t),lze=r(xL," or "),jB=n(xL,"A",{href:!0});var Dkr=s(jB);ize=r(Dkr,"DebertaTokenizerFast"),Dkr.forEach(t),dze=r(xL," (DeBERTa model)"),xL.forEach(t),cze=i(y),Ag=n(y,"LI",{});var JTe=s(Ag);ZH=n(JTe,"STRONG",{});var jkr=s(ZH);fze=r(jkr,"deberta-v2"),jkr.forEach(t),mze=r(JTe," \u2014 "),NB=n(JTe,"A",{href:!0});var Nkr=s(NB);gze=r(Nkr,"DebertaV2Tokenizer"),Nkr.forEach(t),hze=r(JTe," (DeBERTa-v2 model)"),JTe.forEach(t),pze=i(y),Zn=n(y,"LI",{});var kL=s(Zn);eU=n(kL,"STRONG",{});var qkr=s(eU);_ze=r(qkr,"distilbert"),qkr.forEach(t),uze=r(kL," \u2014 "),qB=n(kL,"A",{href:!0});var Gkr=s(qB);bze=r(Gkr,"DistilBertTokenizer"),Gkr.forEach(t),vze=r(kL," or "),GB=n(kL,"A",{href:!0});var Okr=s(GB);Tze=r(Okr,"DistilBertTokenizerFast"),Okr.forEach(t),Fze=r(kL," (DistilBERT model)"),kL.forEach(t),Cze=i(y),es=n(y,"LI",{});var RL=s(es);oU=n(RL,"STRONG",{});var Xkr=s(oU);Mze=r(Xkr,"dpr"),Xkr.forEach(t),Eze=r(RL," \u2014 "),OB=n(RL,"A",{href:!0});var Vkr=s(OB);yze=r(Vkr,"DPRQuestionEncoderTokenizer"),Vkr.forEach(t),wze=r(RL," or "),XB=n(RL,"A",{href:!0});var zkr=s(XB);Aze=r(zkr,"DPRQuestionEncoderTokenizerFast"),zkr.forEach(t),Lze=r(RL," (DPR model)"),RL.forEach(t),Bze=i(y),os=n(y,"LI",{});var SL=s(os);rU=n(SL,"STRONG",{});var Wkr=s(rU);xze=r(Wkr,"electra"),Wkr.forEach(t),kze=r(SL," \u2014 "),VB=n(SL,"A",{href:!0});var Qkr=s(VB);Rze=r(Qkr,"ElectraTokenizer"),Qkr.forEach(t),Sze=r(SL," or "),zB=n(SL,"A",{href:!0});var Hkr=s(zB);Pze=r(Hkr,"ElectraTokenizerFast"),Hkr.forEach(t),$ze=r(SL," (ELECTRA model)"),SL.forEach(t),Ize=i(y),Lg=n(y,"LI",{});var YTe=s(Lg);tU=n(YTe,"STRONG",{});var Ukr=s(tU);Dze=r(Ukr,"flaubert"),Ukr.forEach(t),jze=r(YTe," \u2014 "),WB=n(YTe,"A",{href:!0});var Jkr=s(WB);Nze=r(Jkr,"FlaubertTokenizer"),Jkr.forEach(t),qze=r(YTe," (FlauBERT model)"),YTe.forEach(t),Gze=i(y),rs=n(y,"LI",{});var PL=s(rs);aU=n(PL,"STRONG",{});var Ykr=s(aU);Oze=r(Ykr,"fnet"),Ykr.forEach(t),Xze=r(PL," \u2014 "),QB=n(PL,"A",{href:!0});var Kkr=s(QB);Vze=r(Kkr,"FNetTokenizer"),Kkr.forEach(t),zze=r(PL," or "),HB=n(PL,"A",{href:!0});var Zkr=s(HB);Wze=r(Zkr,"FNetTokenizerFast"),Zkr.forEach(t),Qze=r(PL," (FNet model)"),PL.forEach(t),Hze=i(y),Bg=n(y,"LI",{});var KTe=s(Bg);nU=n(KTe,"STRONG",{});var eRr=s(nU);Uze=r(eRr,"fsmt"),eRr.forEach(t),Jze=r(KTe," \u2014 "),UB=n(KTe,"A",{href:!0});var oRr=s(UB);Yze=r(oRr,"FSMTTokenizer"),oRr.forEach(t),Kze=r(KTe," (FairSeq Machine-Translation model)"),KTe.forEach(t),Zze=i(y),ts=n(y,"LI",{});var $L=s(ts);sU=n($L,"STRONG",{});var rRr=s(sU);eWe=r(rRr,"funnel"),rRr.forEach(t),oWe=r($L," \u2014 "),JB=n($L,"A",{href:!0});var tRr=s(JB);rWe=r(tRr,"FunnelTokenizer"),tRr.forEach(t),tWe=r($L," or "),YB=n($L,"A",{href:!0});var aRr=s(YB);aWe=r(aRr,"FunnelTokenizerFast"),aRr.forEach(t),nWe=r($L," (Funnel Transformer model)"),$L.forEach(t),sWe=i(y),as=n(y,"LI",{});var IL=s(as);lU=n(IL,"STRONG",{});var nRr=s(lU);lWe=r(nRr,"gpt2"),nRr.forEach(t),iWe=r(IL," \u2014 "),KB=n(IL,"A",{href:!0});var sRr=s(KB);dWe=r(sRr,"GPT2Tokenizer"),sRr.forEach(t),cWe=r(IL," or "),ZB=n(IL,"A",{href:!0});var lRr=s(ZB);fWe=r(lRr,"GPT2TokenizerFast"),lRr.forEach(t),mWe=r(IL," (OpenAI GPT-2 model)"),IL.forEach(t),gWe=i(y),ns=n(y,"LI",{});var DL=s(ns);iU=n(DL,"STRONG",{});var iRr=s(iU);hWe=r(iRr,"gpt_neo"),iRr.forEach(t),pWe=r(DL," \u2014 "),ex=n(DL,"A",{href:!0});var dRr=s(ex);_We=r(dRr,"GPT2Tokenizer"),dRr.forEach(t),uWe=r(DL," or "),ox=n(DL,"A",{href:!0});var cRr=s(ox);bWe=r(cRr,"GPT2TokenizerFast"),cRr.forEach(t),vWe=r(DL," (GPT Neo model)"),DL.forEach(t),TWe=i(y),ss=n(y,"LI",{});var jL=s(ss);dU=n(jL,"STRONG",{});var fRr=s(dU);FWe=r(fRr,"herbert"),fRr.forEach(t),CWe=r(jL," \u2014 "),rx=n(jL,"A",{href:!0});var mRr=s(rx);MWe=r(mRr,"HerbertTokenizer"),mRr.forEach(t),EWe=r(jL," or "),tx=n(jL,"A",{href:!0});var gRr=s(tx);yWe=r(gRr,"HerbertTokenizerFast"),gRr.forEach(t),wWe=r(jL," (HerBERT model)"),jL.forEach(t),AWe=i(y),xg=n(y,"LI",{});var ZTe=s(xg);cU=n(ZTe,"STRONG",{});var hRr=s(cU);LWe=r(hRr,"hubert"),hRr.forEach(t),BWe=r(ZTe," \u2014 "),ax=n(ZTe,"A",{href:!0});var pRr=s(ax);xWe=r(pRr,"Wav2Vec2CTCTokenizer"),pRr.forEach(t),kWe=r(ZTe," (Hubert model)"),ZTe.forEach(t),RWe=i(y),ls=n(y,"LI",{});var NL=s(ls);fU=n(NL,"STRONG",{});var _Rr=s(fU);SWe=r(_Rr,"ibert"),_Rr.forEach(t),PWe=r(NL," \u2014 "),nx=n(NL,"A",{href:!0});var uRr=s(nx);$We=r(uRr,"RobertaTokenizer"),uRr.forEach(t),IWe=r(NL," or "),sx=n(NL,"A",{href:!0});var bRr=s(sx);DWe=r(bRr,"RobertaTokenizerFast"),bRr.forEach(t),jWe=r(NL," (I-BERT model)"),NL.forEach(t),NWe=i(y),is=n(y,"LI",{});var qL=s(is);mU=n(qL,"STRONG",{});var vRr=s(mU);qWe=r(vRr,"layoutlm"),vRr.forEach(t),GWe=r(qL," \u2014 "),lx=n(qL,"A",{href:!0});var TRr=s(lx);OWe=r(TRr,"LayoutLMTokenizer"),TRr.forEach(t),XWe=r(qL," or "),ix=n(qL,"A",{href:!0});var FRr=s(ix);VWe=r(FRr,"LayoutLMTokenizerFast"),FRr.forEach(t),zWe=r(qL," (LayoutLM model)"),qL.forEach(t),WWe=i(y),ds=n(y,"LI",{});var GL=s(ds);gU=n(GL,"STRONG",{});var CRr=s(gU);QWe=r(CRr,"layoutlmv2"),CRr.forEach(t),HWe=r(GL," \u2014 "),dx=n(GL,"A",{href:!0});var MRr=s(dx);UWe=r(MRr,"LayoutLMv2Tokenizer"),MRr.forEach(t),JWe=r(GL," or "),cx=n(GL,"A",{href:!0});var ERr=s(cx);YWe=r(ERr,"LayoutLMv2TokenizerFast"),ERr.forEach(t),KWe=r(GL," (LayoutLMv2 model)"),GL.forEach(t),ZWe=i(y),cs=n(y,"LI",{});var OL=s(cs);hU=n(OL,"STRONG",{});var yRr=s(hU);eQe=r(yRr,"layoutxlm"),yRr.forEach(t),oQe=r(OL," \u2014 "),fx=n(OL,"A",{href:!0});var wRr=s(fx);rQe=r(wRr,"LayoutXLMTokenizer"),wRr.forEach(t),tQe=r(OL," or "),mx=n(OL,"A",{href:!0});var ARr=s(mx);aQe=r(ARr,"LayoutXLMTokenizerFast"),ARr.forEach(t),nQe=r(OL," (LayoutXLM model)"),OL.forEach(t),sQe=i(y),fs=n(y,"LI",{});var XL=s(fs);pU=n(XL,"STRONG",{});var LRr=s(pU);lQe=r(LRr,"led"),LRr.forEach(t),iQe=r(XL," \u2014 "),gx=n(XL,"A",{href:!0});var BRr=s(gx);dQe=r(BRr,"LEDTokenizer"),BRr.forEach(t),cQe=r(XL," or "),hx=n(XL,"A",{href:!0});var xRr=s(hx);fQe=r(xRr,"LEDTokenizerFast"),xRr.forEach(t),mQe=r(XL," (LED model)"),XL.forEach(t),gQe=i(y),ms=n(y,"LI",{});var VL=s(ms);_U=n(VL,"STRONG",{});var kRr=s(_U);hQe=r(kRr,"longformer"),kRr.forEach(t),pQe=r(VL," \u2014 "),px=n(VL,"A",{href:!0});var RRr=s(px);_Qe=r(RRr,"LongformerTokenizer"),RRr.forEach(t),uQe=r(VL," or "),_x=n(VL,"A",{href:!0});var SRr=s(_x);bQe=r(SRr,"LongformerTokenizerFast"),SRr.forEach(t),vQe=r(VL," (Longformer model)"),VL.forEach(t),TQe=i(y),kg=n(y,"LI",{});var eFe=s(kg);uU=n(eFe,"STRONG",{});var PRr=s(uU);FQe=r(PRr,"luke"),PRr.forEach(t),CQe=r(eFe," \u2014 "),ux=n(eFe,"A",{href:!0});var $Rr=s(ux);MQe=r($Rr,"LukeTokenizer"),$Rr.forEach(t),EQe=r(eFe," (LUKE model)"),eFe.forEach(t),yQe=i(y),gs=n(y,"LI",{});var zL=s(gs);bU=n(zL,"STRONG",{});var IRr=s(bU);wQe=r(IRr,"lxmert"),IRr.forEach(t),AQe=r(zL," \u2014 "),bx=n(zL,"A",{href:!0});var DRr=s(bx);LQe=r(DRr,"LxmertTokenizer"),DRr.forEach(t),BQe=r(zL," or "),vx=n(zL,"A",{href:!0});var jRr=s(vx);xQe=r(jRr,"LxmertTokenizerFast"),jRr.forEach(t),kQe=r(zL," (LXMERT model)"),zL.forEach(t),RQe=i(y),Rg=n(y,"LI",{});var oFe=s(Rg);vU=n(oFe,"STRONG",{});var NRr=s(vU);SQe=r(NRr,"m2m_100"),NRr.forEach(t),PQe=r(oFe," \u2014 "),Tx=n(oFe,"A",{href:!0});var qRr=s(Tx);$Qe=r(qRr,"M2M100Tokenizer"),qRr.forEach(t),IQe=r(oFe," (M2M100 model)"),oFe.forEach(t),DQe=i(y),Sg=n(y,"LI",{});var rFe=s(Sg);TU=n(rFe,"STRONG",{});var GRr=s(TU);jQe=r(GRr,"marian"),GRr.forEach(t),NQe=r(rFe," \u2014 "),Fx=n(rFe,"A",{href:!0});var ORr=s(Fx);qQe=r(ORr,"MarianTokenizer"),ORr.forEach(t),GQe=r(rFe," (Marian model)"),rFe.forEach(t),OQe=i(y),hs=n(y,"LI",{});var WL=s(hs);FU=n(WL,"STRONG",{});var XRr=s(FU);XQe=r(XRr,"mbart"),XRr.forEach(t),VQe=r(WL," \u2014 "),Cx=n(WL,"A",{href:!0});var VRr=s(Cx);zQe=r(VRr,"MBartTokenizer"),VRr.forEach(t),WQe=r(WL," or "),Mx=n(WL,"A",{href:!0});var zRr=s(Mx);QQe=r(zRr,"MBartTokenizerFast"),zRr.forEach(t),HQe=r(WL," (mBART model)"),WL.forEach(t),UQe=i(y),ps=n(y,"LI",{});var QL=s(ps);CU=n(QL,"STRONG",{});var WRr=s(CU);JQe=r(WRr,"mbart50"),WRr.forEach(t),YQe=r(QL," \u2014 "),Ex=n(QL,"A",{href:!0});var QRr=s(Ex);KQe=r(QRr,"MBart50Tokenizer"),QRr.forEach(t),ZQe=r(QL," or "),yx=n(QL,"A",{href:!0});var HRr=s(yx);eHe=r(HRr,"MBart50TokenizerFast"),HRr.forEach(t),oHe=r(QL," (mBART-50 model)"),QL.forEach(t),rHe=i(y),Pg=n(y,"LI",{});var tFe=s(Pg);MU=n(tFe,"STRONG",{});var URr=s(MU);tHe=r(URr,"mluke"),URr.forEach(t),aHe=r(tFe," \u2014 "),wx=n(tFe,"A",{href:!0});var JRr=s(wx);nHe=r(JRr,"MLukeTokenizer"),JRr.forEach(t),sHe=r(tFe," (mLUKE model)"),tFe.forEach(t),lHe=i(y),_s=n(y,"LI",{});var HL=s(_s);EU=n(HL,"STRONG",{});var YRr=s(EU);iHe=r(YRr,"mobilebert"),YRr.forEach(t),dHe=r(HL," \u2014 "),Ax=n(HL,"A",{href:!0});var KRr=s(Ax);cHe=r(KRr,"MobileBertTokenizer"),KRr.forEach(t),fHe=r(HL," or "),Lx=n(HL,"A",{href:!0});var ZRr=s(Lx);mHe=r(ZRr,"MobileBertTokenizerFast"),ZRr.forEach(t),gHe=r(HL," (MobileBERT model)"),HL.forEach(t),hHe=i(y),us=n(y,"LI",{});var UL=s(us);yU=n(UL,"STRONG",{});var eSr=s(yU);pHe=r(eSr,"mpnet"),eSr.forEach(t),_He=r(UL," \u2014 "),Bx=n(UL,"A",{href:!0});var oSr=s(Bx);uHe=r(oSr,"MPNetTokenizer"),oSr.forEach(t),bHe=r(UL," or "),xx=n(UL,"A",{href:!0});var rSr=s(xx);vHe=r(rSr,"MPNetTokenizerFast"),rSr.forEach(t),THe=r(UL," (MPNet model)"),UL.forEach(t),FHe=i(y),bs=n(y,"LI",{});var JL=s(bs);wU=n(JL,"STRONG",{});var tSr=s(wU);CHe=r(tSr,"mt5"),tSr.forEach(t),MHe=r(JL," \u2014 "),kx=n(JL,"A",{href:!0});var aSr=s(kx);EHe=r(aSr,"MT5Tokenizer"),aSr.forEach(t),yHe=r(JL," or "),Rx=n(JL,"A",{href:!0});var nSr=s(Rx);wHe=r(nSr,"MT5TokenizerFast"),nSr.forEach(t),AHe=r(JL," (mT5 model)"),JL.forEach(t),LHe=i(y),vs=n(y,"LI",{});var YL=s(vs);AU=n(YL,"STRONG",{});var sSr=s(AU);BHe=r(sSr,"openai-gpt"),sSr.forEach(t),xHe=r(YL," \u2014 "),Sx=n(YL,"A",{href:!0});var lSr=s(Sx);kHe=r(lSr,"OpenAIGPTTokenizer"),lSr.forEach(t),RHe=r(YL," or "),Px=n(YL,"A",{href:!0});var iSr=s(Px);SHe=r(iSr,"OpenAIGPTTokenizerFast"),iSr.forEach(t),PHe=r(YL," (OpenAI GPT model)"),YL.forEach(t),$He=i(y),Ts=n(y,"LI",{});var KL=s(Ts);LU=n(KL,"STRONG",{});var dSr=s(LU);IHe=r(dSr,"pegasus"),dSr.forEach(t),DHe=r(KL," \u2014 "),$x=n(KL,"A",{href:!0});var cSr=s($x);jHe=r(cSr,"PegasusTokenizer"),cSr.forEach(t),NHe=r(KL," or "),Ix=n(KL,"A",{href:!0});var fSr=s(Ix);qHe=r(fSr,"PegasusTokenizerFast"),fSr.forEach(t),GHe=r(KL," (Pegasus model)"),KL.forEach(t),OHe=i(y),$g=n(y,"LI",{});var aFe=s($g);BU=n(aFe,"STRONG",{});var mSr=s(BU);XHe=r(mSr,"perceiver"),mSr.forEach(t),VHe=r(aFe," \u2014 "),Dx=n(aFe,"A",{href:!0});var gSr=s(Dx);zHe=r(gSr,"PerceiverTokenizer"),gSr.forEach(t),WHe=r(aFe," (Perceiver model)"),aFe.forEach(t),QHe=i(y),Ig=n(y,"LI",{});var nFe=s(Ig);xU=n(nFe,"STRONG",{});var hSr=s(xU);HHe=r(hSr,"phobert"),hSr.forEach(t),UHe=r(nFe," \u2014 "),jx=n(nFe,"A",{href:!0});var pSr=s(jx);JHe=r(pSr,"PhobertTokenizer"),pSr.forEach(t),YHe=r(nFe," (PhoBERT model)"),nFe.forEach(t),KHe=i(y),Dg=n(y,"LI",{});var sFe=s(Dg);kU=n(sFe,"STRONG",{});var _Sr=s(kU);ZHe=r(_Sr,"plbart"),_Sr.forEach(t),eUe=r(sFe," \u2014 "),Nx=n(sFe,"A",{href:!0});var uSr=s(Nx);oUe=r(uSr,"PLBartTokenizer"),uSr.forEach(t),rUe=r(sFe," (PLBart model)"),sFe.forEach(t),tUe=i(y),jg=n(y,"LI",{});var lFe=s(jg);RU=n(lFe,"STRONG",{});var bSr=s(RU);aUe=r(bSr,"prophetnet"),bSr.forEach(t),nUe=r(lFe," \u2014 "),qx=n(lFe,"A",{href:!0});var vSr=s(qx);sUe=r(vSr,"ProphetNetTokenizer"),vSr.forEach(t),lUe=r(lFe," (ProphetNet model)"),lFe.forEach(t),iUe=i(y),Fs=n(y,"LI",{});var ZL=s(Fs);SU=n(ZL,"STRONG",{});var TSr=s(SU);dUe=r(TSr,"qdqbert"),TSr.forEach(t),cUe=r(ZL," \u2014 "),Gx=n(ZL,"A",{href:!0});var FSr=s(Gx);fUe=r(FSr,"BertTokenizer"),FSr.forEach(t),mUe=r(ZL," or "),Ox=n(ZL,"A",{href:!0});var CSr=s(Ox);gUe=r(CSr,"BertTokenizerFast"),CSr.forEach(t),hUe=r(ZL," (QDQBert model)"),ZL.forEach(t),pUe=i(y),Ng=n(y,"LI",{});var iFe=s(Ng);PU=n(iFe,"STRONG",{});var MSr=s(PU);_Ue=r(MSr,"rag"),MSr.forEach(t),uUe=r(iFe," \u2014 "),Xx=n(iFe,"A",{href:!0});var ESr=s(Xx);bUe=r(ESr,"RagTokenizer"),ESr.forEach(t),vUe=r(iFe," (RAG model)"),iFe.forEach(t),TUe=i(y),Cs=n(y,"LI",{});var e8=s(Cs);$U=n(e8,"STRONG",{});var ySr=s($U);FUe=r(ySr,"reformer"),ySr.forEach(t),CUe=r(e8," \u2014 "),Vx=n(e8,"A",{href:!0});var wSr=s(Vx);MUe=r(wSr,"ReformerTokenizer"),wSr.forEach(t),EUe=r(e8," or "),zx=n(e8,"A",{href:!0});var ASr=s(zx);yUe=r(ASr,"ReformerTokenizerFast"),ASr.forEach(t),wUe=r(e8," (Reformer model)"),e8.forEach(t),AUe=i(y),Ms=n(y,"LI",{});var o8=s(Ms);IU=n(o8,"STRONG",{});var LSr=s(IU);LUe=r(LSr,"rembert"),LSr.forEach(t),BUe=r(o8," \u2014 "),Wx=n(o8,"A",{href:!0});var BSr=s(Wx);xUe=r(BSr,"RemBertTokenizer"),BSr.forEach(t),kUe=r(o8," or "),Qx=n(o8,"A",{href:!0});var xSr=s(Qx);RUe=r(xSr,"RemBertTokenizerFast"),xSr.forEach(t),SUe=r(o8," (RemBERT model)"),o8.forEach(t),PUe=i(y),Es=n(y,"LI",{});var r8=s(Es);DU=n(r8,"STRONG",{});var kSr=s(DU);$Ue=r(kSr,"retribert"),kSr.forEach(t),IUe=r(r8," \u2014 "),Hx=n(r8,"A",{href:!0});var RSr=s(Hx);DUe=r(RSr,"RetriBertTokenizer"),RSr.forEach(t),jUe=r(r8," or "),Ux=n(r8,"A",{href:!0});var SSr=s(Ux);NUe=r(SSr,"RetriBertTokenizerFast"),SSr.forEach(t),qUe=r(r8," (RetriBERT model)"),r8.forEach(t),GUe=i(y),ys=n(y,"LI",{});var t8=s(ys);jU=n(t8,"STRONG",{});var PSr=s(jU);OUe=r(PSr,"roberta"),PSr.forEach(t),XUe=r(t8," \u2014 "),Jx=n(t8,"A",{href:!0});var $Sr=s(Jx);VUe=r($Sr,"RobertaTokenizer"),$Sr.forEach(t),zUe=r(t8," or "),Yx=n(t8,"A",{href:!0});var ISr=s(Yx);WUe=r(ISr,"RobertaTokenizerFast"),ISr.forEach(t),QUe=r(t8," (RoBERTa model)"),t8.forEach(t),HUe=i(y),ws=n(y,"LI",{});var a8=s(ws);NU=n(a8,"STRONG",{});var DSr=s(NU);UUe=r(DSr,"roformer"),DSr.forEach(t),JUe=r(a8," \u2014 "),Kx=n(a8,"A",{href:!0});var jSr=s(Kx);YUe=r(jSr,"RoFormerTokenizer"),jSr.forEach(t),KUe=r(a8," or "),Zx=n(a8,"A",{href:!0});var NSr=s(Zx);ZUe=r(NSr,"RoFormerTokenizerFast"),NSr.forEach(t),eJe=r(a8," (RoFormer model)"),a8.forEach(t),oJe=i(y),qg=n(y,"LI",{});var dFe=s(qg);qU=n(dFe,"STRONG",{});var qSr=s(qU);rJe=r(qSr,"speech_to_text"),qSr.forEach(t),tJe=r(dFe," \u2014 "),ek=n(dFe,"A",{href:!0});var GSr=s(ek);aJe=r(GSr,"Speech2TextTokenizer"),GSr.forEach(t),nJe=r(dFe," (Speech2Text model)"),dFe.forEach(t),sJe=i(y),Gg=n(y,"LI",{});var cFe=s(Gg);GU=n(cFe,"STRONG",{});var OSr=s(GU);lJe=r(OSr,"speech_to_text_2"),OSr.forEach(t),iJe=r(cFe," \u2014 "),ok=n(cFe,"A",{href:!0});var XSr=s(ok);dJe=r(XSr,"Speech2Text2Tokenizer"),XSr.forEach(t),cJe=r(cFe," (Speech2Text2 model)"),cFe.forEach(t),fJe=i(y),As=n(y,"LI",{});var n8=s(As);OU=n(n8,"STRONG",{});var VSr=s(OU);mJe=r(VSr,"splinter"),VSr.forEach(t),gJe=r(n8," \u2014 "),rk=n(n8,"A",{href:!0});var zSr=s(rk);hJe=r(zSr,"SplinterTokenizer"),zSr.forEach(t),pJe=r(n8," or "),tk=n(n8,"A",{href:!0});var WSr=s(tk);_Je=r(WSr,"SplinterTokenizerFast"),WSr.forEach(t),uJe=r(n8," (Splinter model)"),n8.forEach(t),bJe=i(y),Ls=n(y,"LI",{});var s8=s(Ls);XU=n(s8,"STRONG",{});var QSr=s(XU);vJe=r(QSr,"squeezebert"),QSr.forEach(t),TJe=r(s8," \u2014 "),ak=n(s8,"A",{href:!0});var HSr=s(ak);FJe=r(HSr,"SqueezeBertTokenizer"),HSr.forEach(t),CJe=r(s8," or "),nk=n(s8,"A",{href:!0});var USr=s(nk);MJe=r(USr,"SqueezeBertTokenizerFast"),USr.forEach(t),EJe=r(s8," (SqueezeBERT model)"),s8.forEach(t),yJe=i(y),Bs=n(y,"LI",{});var l8=s(Bs);VU=n(l8,"STRONG",{});var JSr=s(VU);wJe=r(JSr,"t5"),JSr.forEach(t),AJe=r(l8," \u2014 "),sk=n(l8,"A",{href:!0});var YSr=s(sk);LJe=r(YSr,"T5Tokenizer"),YSr.forEach(t),BJe=r(l8," or "),lk=n(l8,"A",{href:!0});var KSr=s(lk);xJe=r(KSr,"T5TokenizerFast"),KSr.forEach(t),kJe=r(l8," (T5 model)"),l8.forEach(t),RJe=i(y),Og=n(y,"LI",{});var fFe=s(Og);zU=n(fFe,"STRONG",{});var ZSr=s(zU);SJe=r(ZSr,"tapas"),ZSr.forEach(t),PJe=r(fFe," \u2014 "),ik=n(fFe,"A",{href:!0});var ePr=s(ik);$Je=r(ePr,"TapasTokenizer"),ePr.forEach(t),IJe=r(fFe," (TAPAS model)"),fFe.forEach(t),DJe=i(y),Xg=n(y,"LI",{});var mFe=s(Xg);WU=n(mFe,"STRONG",{});var oPr=s(WU);jJe=r(oPr,"transfo-xl"),oPr.forEach(t),NJe=r(mFe," \u2014 "),dk=n(mFe,"A",{href:!0});var rPr=s(dk);qJe=r(rPr,"TransfoXLTokenizer"),rPr.forEach(t),GJe=r(mFe," (Transformer-XL model)"),mFe.forEach(t),OJe=i(y),Vg=n(y,"LI",{});var gFe=s(Vg);QU=n(gFe,"STRONG",{});var tPr=s(QU);XJe=r(tPr,"wav2vec2"),tPr.forEach(t),VJe=r(gFe," \u2014 "),ck=n(gFe,"A",{href:!0});var aPr=s(ck);zJe=r(aPr,"Wav2Vec2CTCTokenizer"),aPr.forEach(t),WJe=r(gFe," (Wav2Vec2 model)"),gFe.forEach(t),QJe=i(y),zg=n(y,"LI",{});var hFe=s(zg);HU=n(hFe,"STRONG",{});var nPr=s(HU);HJe=r(nPr,"wav2vec2_phoneme"),nPr.forEach(t),UJe=r(hFe," \u2014 "),fk=n(hFe,"A",{href:!0});var sPr=s(fk);JJe=r(sPr,"Wav2Vec2PhonemeCTCTokenizer"),sPr.forEach(t),YJe=r(hFe," (Wav2Vec2Phoneme model)"),hFe.forEach(t),KJe=i(y),xs=n(y,"LI",{});var i8=s(xs);UU=n(i8,"STRONG",{});var lPr=s(UU);ZJe=r(lPr,"xglm"),lPr.forEach(t),eYe=r(i8," \u2014 "),mk=n(i8,"A",{href:!0});var iPr=s(mk);oYe=r(iPr,"XGLMTokenizer"),iPr.forEach(t),rYe=r(i8," or "),gk=n(i8,"A",{href:!0});var dPr=s(gk);tYe=r(dPr,"XGLMTokenizerFast"),dPr.forEach(t),aYe=r(i8," (XGLM model)"),i8.forEach(t),nYe=i(y),Wg=n(y,"LI",{});var pFe=s(Wg);JU=n(pFe,"STRONG",{});var cPr=s(JU);sYe=r(cPr,"xlm"),cPr.forEach(t),lYe=r(pFe," \u2014 "),hk=n(pFe,"A",{href:!0});var fPr=s(hk);iYe=r(fPr,"XLMTokenizer"),fPr.forEach(t),dYe=r(pFe," (XLM model)"),pFe.forEach(t),cYe=i(y),Qg=n(y,"LI",{});var _Fe=s(Qg);YU=n(_Fe,"STRONG",{});var mPr=s(YU);fYe=r(mPr,"xlm-prophetnet"),mPr.forEach(t),mYe=r(_Fe," \u2014 "),pk=n(_Fe,"A",{href:!0});var gPr=s(pk);gYe=r(gPr,"XLMProphetNetTokenizer"),gPr.forEach(t),hYe=r(_Fe," (XLMProphetNet model)"),_Fe.forEach(t),pYe=i(y),ks=n(y,"LI",{});var d8=s(ks);KU=n(d8,"STRONG",{});var hPr=s(KU);_Ye=r(hPr,"xlm-roberta"),hPr.forEach(t),uYe=r(d8," \u2014 "),_k=n(d8,"A",{href:!0});var pPr=s(_k);bYe=r(pPr,"XLMRobertaTokenizer"),pPr.forEach(t),vYe=r(d8," or "),uk=n(d8,"A",{href:!0});var _Pr=s(uk);TYe=r(_Pr,"XLMRobertaTokenizerFast"),_Pr.forEach(t),FYe=r(d8," (XLM-RoBERTa model)"),d8.forEach(t),CYe=i(y),Rs=n(y,"LI",{});var c8=s(Rs);ZU=n(c8,"STRONG",{});var uPr=s(ZU);MYe=r(uPr,"xlnet"),uPr.forEach(t),EYe=r(c8," \u2014 "),bk=n(c8,"A",{href:!0});var bPr=s(bk);yYe=r(bPr,"XLNetTokenizer"),bPr.forEach(t),wYe=r(c8," or "),vk=n(c8,"A",{href:!0});var vPr=s(vk);AYe=r(vPr,"XLNetTokenizerFast"),vPr.forEach(t),LYe=r(c8," (XLNet model)"),c8.forEach(t),y.forEach(t),BYe=i(da),eJ=n(da,"P",{});var TPr=s(eJ);xYe=r(TPr,"Examples:"),TPr.forEach(t),kYe=i(da),m(Q4.$$.fragment,da),da.forEach(t),RYe=i(Ds),Hg=n(Ds,"DIV",{class:!0});var cke=s(Hg);m(H4.$$.fragment,cke),SYe=i(cke),oJ=n(cke,"P",{});var FPr=s(oJ);PYe=r(FPr,"Register a new tokenizer in this mapping."),FPr.forEach(t),cke.forEach(t),Ds.forEach(t),iBe=i(c),Ni=n(c,"H2",{class:!0});var fke=s(Ni);Ug=n(fke,"A",{id:!0,class:!0,href:!0});var CPr=s(Ug);rJ=n(CPr,"SPAN",{});var MPr=s(rJ);m(U4.$$.fragment,MPr),MPr.forEach(t),CPr.forEach(t),$Ye=i(fke),tJ=n(fke,"SPAN",{});var EPr=s(tJ);IYe=r(EPr,"AutoFeatureExtractor"),EPr.forEach(t),fke.forEach(t),dBe=i(c),Qo=n(c,"DIV",{class:!0});var js=s(Qo);m(J4.$$.fragment,js),DYe=i(js),Y4=n(js,"P",{});var mke=s(Y4);jYe=r(mke,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Tk=n(mke,"A",{href:!0});var yPr=s(Tk);NYe=r(yPr,"AutoFeatureExtractor.from_pretrained()"),yPr.forEach(t),qYe=r(mke," class method."),mke.forEach(t),GYe=i(js),K4=n(js,"P",{});var gke=s(K4);OYe=r(gke,"This class cannot be instantiated directly using "),aJ=n(gke,"CODE",{});var wPr=s(aJ);XYe=r(wPr,"__init__()"),wPr.forEach(t),VYe=r(gke," (throws an error)."),gke.forEach(t),zYe=i(js),$e=n(js,"DIV",{class:!0});var $t=s($e);m(Z4.$$.fragment,$t),WYe=i($t),nJ=n($t,"P",{});var APr=s(nJ);QYe=r(APr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),APr.forEach(t),HYe=i($t),ja=n($t,"P",{});var RM=s(ja);UYe=r(RM,"The feature extractor class to instantiate is selected based on the "),sJ=n(RM,"CODE",{});var LPr=s(sJ);JYe=r(LPr,"model_type"),LPr.forEach(t),YYe=r(RM,` property of the config object
(either passed as an argument or loaded from `),lJ=n(RM,"CODE",{});var BPr=s(lJ);KYe=r(BPr,"pretrained_model_name_or_path"),BPr.forEach(t),ZYe=r(RM,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),iJ=n(RM,"CODE",{});var xPr=s(iJ);eKe=r(xPr,"pretrained_model_name_or_path"),xPr.forEach(t),oKe=r(RM,":"),RM.forEach(t),rKe=i($t),se=n($t,"UL",{});var de=s(se);Jg=n(de,"LI",{});var uFe=s(Jg);dJ=n(uFe,"STRONG",{});var kPr=s(dJ);tKe=r(kPr,"beit"),kPr.forEach(t),aKe=r(uFe," \u2014 "),Fk=n(uFe,"A",{href:!0});var RPr=s(Fk);nKe=r(RPr,"BeitFeatureExtractor"),RPr.forEach(t),sKe=r(uFe," (BEiT model)"),uFe.forEach(t),lKe=i(de),Yg=n(de,"LI",{});var bFe=s(Yg);cJ=n(bFe,"STRONG",{});var SPr=s(cJ);iKe=r(SPr,"clip"),SPr.forEach(t),dKe=r(bFe," \u2014 "),Ck=n(bFe,"A",{href:!0});var PPr=s(Ck);cKe=r(PPr,"CLIPFeatureExtractor"),PPr.forEach(t),fKe=r(bFe," (CLIP model)"),bFe.forEach(t),mKe=i(de),Kg=n(de,"LI",{});var vFe=s(Kg);fJ=n(vFe,"STRONG",{});var $Pr=s(fJ);gKe=r($Pr,"convnext"),$Pr.forEach(t),hKe=r(vFe," \u2014 "),Mk=n(vFe,"A",{href:!0});var IPr=s(Mk);pKe=r(IPr,"ConvNextFeatureExtractor"),IPr.forEach(t),_Ke=r(vFe," (ConvNext model)"),vFe.forEach(t),uKe=i(de),Zg=n(de,"LI",{});var TFe=s(Zg);mJ=n(TFe,"STRONG",{});var DPr=s(mJ);bKe=r(DPr,"deit"),DPr.forEach(t),vKe=r(TFe," \u2014 "),Ek=n(TFe,"A",{href:!0});var jPr=s(Ek);TKe=r(jPr,"DeiTFeatureExtractor"),jPr.forEach(t),FKe=r(TFe," (DeiT model)"),TFe.forEach(t),CKe=i(de),eh=n(de,"LI",{});var FFe=s(eh);gJ=n(FFe,"STRONG",{});var NPr=s(gJ);MKe=r(NPr,"detr"),NPr.forEach(t),EKe=r(FFe," \u2014 "),yk=n(FFe,"A",{href:!0});var qPr=s(yk);yKe=r(qPr,"DetrFeatureExtractor"),qPr.forEach(t),wKe=r(FFe," (DETR model)"),FFe.forEach(t),AKe=i(de),oh=n(de,"LI",{});var CFe=s(oh);hJ=n(CFe,"STRONG",{});var GPr=s(hJ);LKe=r(GPr,"hubert"),GPr.forEach(t),BKe=r(CFe," \u2014 "),wk=n(CFe,"A",{href:!0});var OPr=s(wk);xKe=r(OPr,"Wav2Vec2FeatureExtractor"),OPr.forEach(t),kKe=r(CFe," (Hubert model)"),CFe.forEach(t),RKe=i(de),rh=n(de,"LI",{});var MFe=s(rh);pJ=n(MFe,"STRONG",{});var XPr=s(pJ);SKe=r(XPr,"layoutlmv2"),XPr.forEach(t),PKe=r(MFe," \u2014 "),Ak=n(MFe,"A",{href:!0});var VPr=s(Ak);$Ke=r(VPr,"LayoutLMv2FeatureExtractor"),VPr.forEach(t),IKe=r(MFe," (LayoutLMv2 model)"),MFe.forEach(t),DKe=i(de),th=n(de,"LI",{});var EFe=s(th);_J=n(EFe,"STRONG",{});var zPr=s(_J);jKe=r(zPr,"perceiver"),zPr.forEach(t),NKe=r(EFe," \u2014 "),Lk=n(EFe,"A",{href:!0});var WPr=s(Lk);qKe=r(WPr,"PerceiverFeatureExtractor"),WPr.forEach(t),GKe=r(EFe," (Perceiver model)"),EFe.forEach(t),OKe=i(de),ah=n(de,"LI",{});var yFe=s(ah);uJ=n(yFe,"STRONG",{});var QPr=s(uJ);XKe=r(QPr,"poolformer"),QPr.forEach(t),VKe=r(yFe," \u2014 "),Bk=n(yFe,"A",{href:!0});var HPr=s(Bk);zKe=r(HPr,"PoolFormerFeatureExtractor"),HPr.forEach(t),WKe=r(yFe," (PoolFormer model)"),yFe.forEach(t),QKe=i(de),nh=n(de,"LI",{});var wFe=s(nh);bJ=n(wFe,"STRONG",{});var UPr=s(bJ);HKe=r(UPr,"segformer"),UPr.forEach(t),UKe=r(wFe," \u2014 "),xk=n(wFe,"A",{href:!0});var JPr=s(xk);JKe=r(JPr,"SegformerFeatureExtractor"),JPr.forEach(t),YKe=r(wFe," (SegFormer model)"),wFe.forEach(t),KKe=i(de),sh=n(de,"LI",{});var AFe=s(sh);vJ=n(AFe,"STRONG",{});var YPr=s(vJ);ZKe=r(YPr,"speech_to_text"),YPr.forEach(t),eZe=r(AFe," \u2014 "),kk=n(AFe,"A",{href:!0});var KPr=s(kk);oZe=r(KPr,"Speech2TextFeatureExtractor"),KPr.forEach(t),rZe=r(AFe," (Speech2Text model)"),AFe.forEach(t),tZe=i(de),lh=n(de,"LI",{});var LFe=s(lh);TJ=n(LFe,"STRONG",{});var ZPr=s(TJ);aZe=r(ZPr,"swin"),ZPr.forEach(t),nZe=r(LFe," \u2014 "),Rk=n(LFe,"A",{href:!0});var e$r=s(Rk);sZe=r(e$r,"ViTFeatureExtractor"),e$r.forEach(t),lZe=r(LFe," (Swin model)"),LFe.forEach(t),iZe=i(de),ih=n(de,"LI",{});var BFe=s(ih);FJ=n(BFe,"STRONG",{});var o$r=s(FJ);dZe=r(o$r,"vit"),o$r.forEach(t),cZe=r(BFe," \u2014 "),Sk=n(BFe,"A",{href:!0});var r$r=s(Sk);fZe=r(r$r,"ViTFeatureExtractor"),r$r.forEach(t),mZe=r(BFe," (ViT model)"),BFe.forEach(t),gZe=i(de),dh=n(de,"LI",{});var xFe=s(dh);CJ=n(xFe,"STRONG",{});var t$r=s(CJ);hZe=r(t$r,"vit_mae"),t$r.forEach(t),pZe=r(xFe," \u2014 "),Pk=n(xFe,"A",{href:!0});var a$r=s(Pk);_Ze=r(a$r,"ViTFeatureExtractor"),a$r.forEach(t),uZe=r(xFe," (ViTMAE model)"),xFe.forEach(t),bZe=i(de),ch=n(de,"LI",{});var kFe=s(ch);MJ=n(kFe,"STRONG",{});var n$r=s(MJ);vZe=r(n$r,"wav2vec2"),n$r.forEach(t),TZe=r(kFe," \u2014 "),$k=n(kFe,"A",{href:!0});var s$r=s($k);FZe=r(s$r,"Wav2Vec2FeatureExtractor"),s$r.forEach(t),CZe=r(kFe," (Wav2Vec2 model)"),kFe.forEach(t),de.forEach(t),MZe=i($t),m(fh.$$.fragment,$t),EZe=i($t),EJ=n($t,"P",{});var l$r=s(EJ);yZe=r(l$r,"Examples:"),l$r.forEach(t),wZe=i($t),m(eE.$$.fragment,$t),$t.forEach(t),AZe=i(js),mh=n(js,"DIV",{class:!0});var hke=s(mh);m(oE.$$.fragment,hke),LZe=i(hke),yJ=n(hke,"P",{});var i$r=s(yJ);BZe=r(i$r,"Register a new feature extractor for this class."),i$r.forEach(t),hke.forEach(t),js.forEach(t),cBe=i(c),qi=n(c,"H2",{class:!0});var pke=s(qi);gh=n(pke,"A",{id:!0,class:!0,href:!0});var d$r=s(gh);wJ=n(d$r,"SPAN",{});var c$r=s(wJ);m(rE.$$.fragment,c$r),c$r.forEach(t),d$r.forEach(t),xZe=i(pke),AJ=n(pke,"SPAN",{});var f$r=s(AJ);kZe=r(f$r,"AutoProcessor"),f$r.forEach(t),pke.forEach(t),fBe=i(c),Ho=n(c,"DIV",{class:!0});var Ns=s(Ho);m(tE.$$.fragment,Ns),RZe=i(Ns),aE=n(Ns,"P",{});var _ke=s(aE);SZe=r(_ke,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),Ik=n(_ke,"A",{href:!0});var m$r=s(Ik);PZe=r(m$r,"AutoProcessor.from_pretrained()"),m$r.forEach(t),$Ze=r(_ke," class method."),_ke.forEach(t),IZe=i(Ns),nE=n(Ns,"P",{});var uke=s(nE);DZe=r(uke,"This class cannot be instantiated directly using "),LJ=n(uke,"CODE",{});var g$r=s(LJ);jZe=r(g$r,"__init__()"),g$r.forEach(t),NZe=r(uke," (throws an error)."),uke.forEach(t),qZe=i(Ns),Ie=n(Ns,"DIV",{class:!0});var It=s(Ie);m(sE.$$.fragment,It),GZe=i(It),BJ=n(It,"P",{});var h$r=s(BJ);OZe=r(h$r,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),h$r.forEach(t),XZe=i(It),Gi=n(It,"P",{});var NV=s(Gi);VZe=r(NV,"The processor class to instantiate is selected based on the "),xJ=n(NV,"CODE",{});var p$r=s(xJ);zZe=r(p$r,"model_type"),p$r.forEach(t),WZe=r(NV,` property of the config object (either
passed as an argument or loaded from `),kJ=n(NV,"CODE",{});var _$r=s(kJ);QZe=r(_$r,"pretrained_model_name_or_path"),_$r.forEach(t),HZe=r(NV," if possible):"),NV.forEach(t),UZe=i(It),Be=n(It,"UL",{});var jo=s(Be);hh=n(jo,"LI",{});var RFe=s(hh);RJ=n(RFe,"STRONG",{});var u$r=s(RJ);JZe=r(u$r,"clip"),u$r.forEach(t),YZe=r(RFe," \u2014 "),Dk=n(RFe,"A",{href:!0});var b$r=s(Dk);KZe=r(b$r,"CLIPProcessor"),b$r.forEach(t),ZZe=r(RFe," (CLIP model)"),RFe.forEach(t),eeo=i(jo),ph=n(jo,"LI",{});var SFe=s(ph);SJ=n(SFe,"STRONG",{});var v$r=s(SJ);oeo=r(v$r,"layoutlmv2"),v$r.forEach(t),reo=r(SFe," \u2014 "),jk=n(SFe,"A",{href:!0});var T$r=s(jk);teo=r(T$r,"LayoutLMv2Processor"),T$r.forEach(t),aeo=r(SFe," (LayoutLMv2 model)"),SFe.forEach(t),neo=i(jo),_h=n(jo,"LI",{});var PFe=s(_h);PJ=n(PFe,"STRONG",{});var F$r=s(PJ);seo=r(F$r,"layoutxlm"),F$r.forEach(t),leo=r(PFe," \u2014 "),Nk=n(PFe,"A",{href:!0});var C$r=s(Nk);ieo=r(C$r,"LayoutXLMProcessor"),C$r.forEach(t),deo=r(PFe," (LayoutXLM model)"),PFe.forEach(t),ceo=i(jo),uh=n(jo,"LI",{});var $Fe=s(uh);$J=n($Fe,"STRONG",{});var M$r=s($J);feo=r(M$r,"speech_to_text"),M$r.forEach(t),meo=r($Fe," \u2014 "),qk=n($Fe,"A",{href:!0});var E$r=s(qk);geo=r(E$r,"Speech2TextProcessor"),E$r.forEach(t),heo=r($Fe," (Speech2Text model)"),$Fe.forEach(t),peo=i(jo),bh=n(jo,"LI",{});var IFe=s(bh);IJ=n(IFe,"STRONG",{});var y$r=s(IJ);_eo=r(y$r,"speech_to_text_2"),y$r.forEach(t),ueo=r(IFe," \u2014 "),Gk=n(IFe,"A",{href:!0});var w$r=s(Gk);beo=r(w$r,"Speech2Text2Processor"),w$r.forEach(t),veo=r(IFe," (Speech2Text2 model)"),IFe.forEach(t),Teo=i(jo),vh=n(jo,"LI",{});var DFe=s(vh);DJ=n(DFe,"STRONG",{});var A$r=s(DJ);Feo=r(A$r,"trocr"),A$r.forEach(t),Ceo=r(DFe," \u2014 "),Ok=n(DFe,"A",{href:!0});var L$r=s(Ok);Meo=r(L$r,"TrOCRProcessor"),L$r.forEach(t),Eeo=r(DFe," (TrOCR model)"),DFe.forEach(t),yeo=i(jo),Th=n(jo,"LI",{});var jFe=s(Th);jJ=n(jFe,"STRONG",{});var B$r=s(jJ);weo=r(B$r,"vision-text-dual-encoder"),B$r.forEach(t),Aeo=r(jFe," \u2014 "),Xk=n(jFe,"A",{href:!0});var x$r=s(Xk);Leo=r(x$r,"VisionTextDualEncoderProcessor"),x$r.forEach(t),Beo=r(jFe," (VisionTextDualEncoder model)"),jFe.forEach(t),xeo=i(jo),Fh=n(jo,"LI",{});var NFe=s(Fh);NJ=n(NFe,"STRONG",{});var k$r=s(NJ);keo=r(k$r,"wav2vec2"),k$r.forEach(t),Reo=r(NFe," \u2014 "),Vk=n(NFe,"A",{href:!0});var R$r=s(Vk);Seo=r(R$r,"Wav2Vec2Processor"),R$r.forEach(t),Peo=r(NFe," (Wav2Vec2 model)"),NFe.forEach(t),jo.forEach(t),$eo=i(It),m(Ch.$$.fragment,It),Ieo=i(It),qJ=n(It,"P",{});var S$r=s(qJ);Deo=r(S$r,"Examples:"),S$r.forEach(t),jeo=i(It),m(lE.$$.fragment,It),It.forEach(t),Neo=i(Ns),Mh=n(Ns,"DIV",{class:!0});var bke=s(Mh);m(iE.$$.fragment,bke),qeo=i(bke),GJ=n(bke,"P",{});var P$r=s(GJ);Geo=r(P$r,"Register a new processor for this class."),P$r.forEach(t),bke.forEach(t),Ns.forEach(t),mBe=i(c),Oi=n(c,"H2",{class:!0});var vke=s(Oi);Eh=n(vke,"A",{id:!0,class:!0,href:!0});var $$r=s(Eh);OJ=n($$r,"SPAN",{});var I$r=s(OJ);m(dE.$$.fragment,I$r),I$r.forEach(t),$$r.forEach(t),Oeo=i(vke),XJ=n(vke,"SPAN",{});var D$r=s(XJ);Xeo=r(D$r,"AutoModel"),D$r.forEach(t),vke.forEach(t),gBe=i(c),Uo=n(c,"DIV",{class:!0});var qs=s(Uo);m(cE.$$.fragment,qs),Veo=i(qs),Xi=n(qs,"P",{});var qV=s(Xi);zeo=r(qV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),VJ=n(qV,"CODE",{});var j$r=s(VJ);Weo=r(j$r,"from_pretrained()"),j$r.forEach(t),Qeo=r(qV,"class method or the "),zJ=n(qV,"CODE",{});var N$r=s(zJ);Heo=r(N$r,"from_config()"),N$r.forEach(t),Ueo=r(qV,`class
method.`),qV.forEach(t),Jeo=i(qs),fE=n(qs,"P",{});var Tke=s(fE);Yeo=r(Tke,"This class cannot be instantiated directly using "),WJ=n(Tke,"CODE",{});var q$r=s(WJ);Keo=r(q$r,"__init__()"),q$r.forEach(t),Zeo=r(Tke," (throws an error)."),Tke.forEach(t),eoo=i(qs),Or=n(qs,"DIV",{class:!0});var Gs=s(Or);m(mE.$$.fragment,Gs),ooo=i(Gs),QJ=n(Gs,"P",{});var G$r=s(QJ);roo=r(G$r,"Instantiates one of the base model classes of the library from a configuration."),G$r.forEach(t),too=i(Gs),Vi=n(Gs,"P",{});var GV=s(Vi);aoo=r(GV,`Note:
Loading a model from its configuration file does `),HJ=n(GV,"STRONG",{});var O$r=s(HJ);noo=r(O$r,"not"),O$r.forEach(t),soo=r(GV,` load the model weights. It only affects the
model\u2019s configuration. Use `),UJ=n(GV,"CODE",{});var X$r=s(UJ);loo=r(X$r,"from_pretrained()"),X$r.forEach(t),ioo=r(GV,"to load the model weights."),GV.forEach(t),doo=i(Gs),JJ=n(Gs,"P",{});var V$r=s(JJ);coo=r(V$r,"Examples:"),V$r.forEach(t),foo=i(Gs),m(gE.$$.fragment,Gs),Gs.forEach(t),moo=i(qs),De=n(qs,"DIV",{class:!0});var Dt=s(De);m(hE.$$.fragment,Dt),goo=i(Dt),YJ=n(Dt,"P",{});var z$r=s(YJ);hoo=r(z$r,"Instantiate one of the base model classes of the library from a pretrained model."),z$r.forEach(t),poo=i(Dt),Na=n(Dt,"P",{});var SM=s(Na);_oo=r(SM,"The model class to instantiate is selected based on the "),KJ=n(SM,"CODE",{});var W$r=s(KJ);uoo=r(W$r,"model_type"),W$r.forEach(t),boo=r(SM,` property of the config object (either
passed as an argument or loaded from `),ZJ=n(SM,"CODE",{});var Q$r=s(ZJ);voo=r(Q$r,"pretrained_model_name_or_path"),Q$r.forEach(t),Too=r(SM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eY=n(SM,"CODE",{});var H$r=s(eY);Foo=r(H$r,"pretrained_model_name_or_path"),H$r.forEach(t),Coo=r(SM,":"),SM.forEach(t),Moo=i(Dt),F=n(Dt,"UL",{});var C=s(F);yh=n(C,"LI",{});var qFe=s(yh);oY=n(qFe,"STRONG",{});var U$r=s(oY);Eoo=r(U$r,"albert"),U$r.forEach(t),yoo=r(qFe," \u2014 "),zk=n(qFe,"A",{href:!0});var J$r=s(zk);woo=r(J$r,"AlbertModel"),J$r.forEach(t),Aoo=r(qFe," (ALBERT model)"),qFe.forEach(t),Loo=i(C),wh=n(C,"LI",{});var GFe=s(wh);rY=n(GFe,"STRONG",{});var Y$r=s(rY);Boo=r(Y$r,"bart"),Y$r.forEach(t),xoo=r(GFe," \u2014 "),Wk=n(GFe,"A",{href:!0});var K$r=s(Wk);koo=r(K$r,"BartModel"),K$r.forEach(t),Roo=r(GFe," (BART model)"),GFe.forEach(t),Soo=i(C),Ah=n(C,"LI",{});var OFe=s(Ah);tY=n(OFe,"STRONG",{});var Z$r=s(tY);Poo=r(Z$r,"beit"),Z$r.forEach(t),$oo=r(OFe," \u2014 "),Qk=n(OFe,"A",{href:!0});var eIr=s(Qk);Ioo=r(eIr,"BeitModel"),eIr.forEach(t),Doo=r(OFe," (BEiT model)"),OFe.forEach(t),joo=i(C),Lh=n(C,"LI",{});var XFe=s(Lh);aY=n(XFe,"STRONG",{});var oIr=s(aY);Noo=r(oIr,"bert"),oIr.forEach(t),qoo=r(XFe," \u2014 "),Hk=n(XFe,"A",{href:!0});var rIr=s(Hk);Goo=r(rIr,"BertModel"),rIr.forEach(t),Ooo=r(XFe," (BERT model)"),XFe.forEach(t),Xoo=i(C),Bh=n(C,"LI",{});var VFe=s(Bh);nY=n(VFe,"STRONG",{});var tIr=s(nY);Voo=r(tIr,"bert-generation"),tIr.forEach(t),zoo=r(VFe," \u2014 "),Uk=n(VFe,"A",{href:!0});var aIr=s(Uk);Woo=r(aIr,"BertGenerationEncoder"),aIr.forEach(t),Qoo=r(VFe," (Bert Generation model)"),VFe.forEach(t),Hoo=i(C),xh=n(C,"LI",{});var zFe=s(xh);sY=n(zFe,"STRONG",{});var nIr=s(sY);Uoo=r(nIr,"big_bird"),nIr.forEach(t),Joo=r(zFe," \u2014 "),Jk=n(zFe,"A",{href:!0});var sIr=s(Jk);Yoo=r(sIr,"BigBirdModel"),sIr.forEach(t),Koo=r(zFe," (BigBird model)"),zFe.forEach(t),Zoo=i(C),kh=n(C,"LI",{});var WFe=s(kh);lY=n(WFe,"STRONG",{});var lIr=s(lY);ero=r(lIr,"bigbird_pegasus"),lIr.forEach(t),oro=r(WFe," \u2014 "),Yk=n(WFe,"A",{href:!0});var iIr=s(Yk);rro=r(iIr,"BigBirdPegasusModel"),iIr.forEach(t),tro=r(WFe," (BigBirdPegasus model)"),WFe.forEach(t),aro=i(C),Rh=n(C,"LI",{});var QFe=s(Rh);iY=n(QFe,"STRONG",{});var dIr=s(iY);nro=r(dIr,"blenderbot"),dIr.forEach(t),sro=r(QFe," \u2014 "),Kk=n(QFe,"A",{href:!0});var cIr=s(Kk);lro=r(cIr,"BlenderbotModel"),cIr.forEach(t),iro=r(QFe," (Blenderbot model)"),QFe.forEach(t),dro=i(C),Sh=n(C,"LI",{});var HFe=s(Sh);dY=n(HFe,"STRONG",{});var fIr=s(dY);cro=r(fIr,"blenderbot-small"),fIr.forEach(t),fro=r(HFe," \u2014 "),Zk=n(HFe,"A",{href:!0});var mIr=s(Zk);mro=r(mIr,"BlenderbotSmallModel"),mIr.forEach(t),gro=r(HFe," (BlenderbotSmall model)"),HFe.forEach(t),hro=i(C),Ph=n(C,"LI",{});var UFe=s(Ph);cY=n(UFe,"STRONG",{});var gIr=s(cY);pro=r(gIr,"camembert"),gIr.forEach(t),_ro=r(UFe," \u2014 "),eR=n(UFe,"A",{href:!0});var hIr=s(eR);uro=r(hIr,"CamembertModel"),hIr.forEach(t),bro=r(UFe," (CamemBERT model)"),UFe.forEach(t),vro=i(C),$h=n(C,"LI",{});var JFe=s($h);fY=n(JFe,"STRONG",{});var pIr=s(fY);Tro=r(pIr,"canine"),pIr.forEach(t),Fro=r(JFe," \u2014 "),oR=n(JFe,"A",{href:!0});var _Ir=s(oR);Cro=r(_Ir,"CanineModel"),_Ir.forEach(t),Mro=r(JFe," (Canine model)"),JFe.forEach(t),Ero=i(C),Ih=n(C,"LI",{});var YFe=s(Ih);mY=n(YFe,"STRONG",{});var uIr=s(mY);yro=r(uIr,"clip"),uIr.forEach(t),wro=r(YFe," \u2014 "),rR=n(YFe,"A",{href:!0});var bIr=s(rR);Aro=r(bIr,"CLIPModel"),bIr.forEach(t),Lro=r(YFe," (CLIP model)"),YFe.forEach(t),Bro=i(C),Dh=n(C,"LI",{});var KFe=s(Dh);gY=n(KFe,"STRONG",{});var vIr=s(gY);xro=r(vIr,"convbert"),vIr.forEach(t),kro=r(KFe," \u2014 "),tR=n(KFe,"A",{href:!0});var TIr=s(tR);Rro=r(TIr,"ConvBertModel"),TIr.forEach(t),Sro=r(KFe," (ConvBERT model)"),KFe.forEach(t),Pro=i(C),jh=n(C,"LI",{});var ZFe=s(jh);hY=n(ZFe,"STRONG",{});var FIr=s(hY);$ro=r(FIr,"convnext"),FIr.forEach(t),Iro=r(ZFe," \u2014 "),aR=n(ZFe,"A",{href:!0});var CIr=s(aR);Dro=r(CIr,"ConvNextModel"),CIr.forEach(t),jro=r(ZFe," (ConvNext model)"),ZFe.forEach(t),Nro=i(C),Nh=n(C,"LI",{});var e9e=s(Nh);pY=n(e9e,"STRONG",{});var MIr=s(pY);qro=r(MIr,"ctrl"),MIr.forEach(t),Gro=r(e9e," \u2014 "),nR=n(e9e,"A",{href:!0});var EIr=s(nR);Oro=r(EIr,"CTRLModel"),EIr.forEach(t),Xro=r(e9e," (CTRL model)"),e9e.forEach(t),Vro=i(C),qh=n(C,"LI",{});var o9e=s(qh);_Y=n(o9e,"STRONG",{});var yIr=s(_Y);zro=r(yIr,"data2vec-audio"),yIr.forEach(t),Wro=r(o9e," \u2014 "),sR=n(o9e,"A",{href:!0});var wIr=s(sR);Qro=r(wIr,"Data2VecAudioModel"),wIr.forEach(t),Hro=r(o9e," (Data2VecAudio model)"),o9e.forEach(t),Uro=i(C),Gh=n(C,"LI",{});var r9e=s(Gh);uY=n(r9e,"STRONG",{});var AIr=s(uY);Jro=r(AIr,"data2vec-text"),AIr.forEach(t),Yro=r(r9e," \u2014 "),lR=n(r9e,"A",{href:!0});var LIr=s(lR);Kro=r(LIr,"Data2VecTextModel"),LIr.forEach(t),Zro=r(r9e," (Data2VecText model)"),r9e.forEach(t),eto=i(C),Oh=n(C,"LI",{});var t9e=s(Oh);bY=n(t9e,"STRONG",{});var BIr=s(bY);oto=r(BIr,"deberta"),BIr.forEach(t),rto=r(t9e," \u2014 "),iR=n(t9e,"A",{href:!0});var xIr=s(iR);tto=r(xIr,"DebertaModel"),xIr.forEach(t),ato=r(t9e," (DeBERTa model)"),t9e.forEach(t),nto=i(C),Xh=n(C,"LI",{});var a9e=s(Xh);vY=n(a9e,"STRONG",{});var kIr=s(vY);sto=r(kIr,"deberta-v2"),kIr.forEach(t),lto=r(a9e," \u2014 "),dR=n(a9e,"A",{href:!0});var RIr=s(dR);ito=r(RIr,"DebertaV2Model"),RIr.forEach(t),dto=r(a9e," (DeBERTa-v2 model)"),a9e.forEach(t),cto=i(C),Vh=n(C,"LI",{});var n9e=s(Vh);TY=n(n9e,"STRONG",{});var SIr=s(TY);fto=r(SIr,"deit"),SIr.forEach(t),mto=r(n9e," \u2014 "),cR=n(n9e,"A",{href:!0});var PIr=s(cR);gto=r(PIr,"DeiTModel"),PIr.forEach(t),hto=r(n9e," (DeiT model)"),n9e.forEach(t),pto=i(C),zh=n(C,"LI",{});var s9e=s(zh);FY=n(s9e,"STRONG",{});var $Ir=s(FY);_to=r($Ir,"detr"),$Ir.forEach(t),uto=r(s9e," \u2014 "),fR=n(s9e,"A",{href:!0});var IIr=s(fR);bto=r(IIr,"DetrModel"),IIr.forEach(t),vto=r(s9e," (DETR model)"),s9e.forEach(t),Tto=i(C),Wh=n(C,"LI",{});var l9e=s(Wh);CY=n(l9e,"STRONG",{});var DIr=s(CY);Fto=r(DIr,"distilbert"),DIr.forEach(t),Cto=r(l9e," \u2014 "),mR=n(l9e,"A",{href:!0});var jIr=s(mR);Mto=r(jIr,"DistilBertModel"),jIr.forEach(t),Eto=r(l9e," (DistilBERT model)"),l9e.forEach(t),yto=i(C),Qh=n(C,"LI",{});var i9e=s(Qh);MY=n(i9e,"STRONG",{});var NIr=s(MY);wto=r(NIr,"dpr"),NIr.forEach(t),Ato=r(i9e," \u2014 "),gR=n(i9e,"A",{href:!0});var qIr=s(gR);Lto=r(qIr,"DPRQuestionEncoder"),qIr.forEach(t),Bto=r(i9e," (DPR model)"),i9e.forEach(t),xto=i(C),Hh=n(C,"LI",{});var d9e=s(Hh);EY=n(d9e,"STRONG",{});var GIr=s(EY);kto=r(GIr,"electra"),GIr.forEach(t),Rto=r(d9e," \u2014 "),hR=n(d9e,"A",{href:!0});var OIr=s(hR);Sto=r(OIr,"ElectraModel"),OIr.forEach(t),Pto=r(d9e," (ELECTRA model)"),d9e.forEach(t),$to=i(C),Uh=n(C,"LI",{});var c9e=s(Uh);yY=n(c9e,"STRONG",{});var XIr=s(yY);Ito=r(XIr,"flaubert"),XIr.forEach(t),Dto=r(c9e," \u2014 "),pR=n(c9e,"A",{href:!0});var VIr=s(pR);jto=r(VIr,"FlaubertModel"),VIr.forEach(t),Nto=r(c9e," (FlauBERT model)"),c9e.forEach(t),qto=i(C),Jh=n(C,"LI",{});var f9e=s(Jh);wY=n(f9e,"STRONG",{});var zIr=s(wY);Gto=r(zIr,"fnet"),zIr.forEach(t),Oto=r(f9e," \u2014 "),_R=n(f9e,"A",{href:!0});var WIr=s(_R);Xto=r(WIr,"FNetModel"),WIr.forEach(t),Vto=r(f9e," (FNet model)"),f9e.forEach(t),zto=i(C),Yh=n(C,"LI",{});var m9e=s(Yh);AY=n(m9e,"STRONG",{});var QIr=s(AY);Wto=r(QIr,"fsmt"),QIr.forEach(t),Qto=r(m9e," \u2014 "),uR=n(m9e,"A",{href:!0});var HIr=s(uR);Hto=r(HIr,"FSMTModel"),HIr.forEach(t),Uto=r(m9e," (FairSeq Machine-Translation model)"),m9e.forEach(t),Jto=i(C),Ss=n(C,"LI",{});var f8=s(Ss);LY=n(f8,"STRONG",{});var UIr=s(LY);Yto=r(UIr,"funnel"),UIr.forEach(t),Kto=r(f8," \u2014 "),bR=n(f8,"A",{href:!0});var JIr=s(bR);Zto=r(JIr,"FunnelModel"),JIr.forEach(t),eao=r(f8," or "),vR=n(f8,"A",{href:!0});var YIr=s(vR);oao=r(YIr,"FunnelBaseModel"),YIr.forEach(t),rao=r(f8," (Funnel Transformer model)"),f8.forEach(t),tao=i(C),Kh=n(C,"LI",{});var g9e=s(Kh);BY=n(g9e,"STRONG",{});var KIr=s(BY);aao=r(KIr,"gpt2"),KIr.forEach(t),nao=r(g9e," \u2014 "),TR=n(g9e,"A",{href:!0});var ZIr=s(TR);sao=r(ZIr,"GPT2Model"),ZIr.forEach(t),lao=r(g9e," (OpenAI GPT-2 model)"),g9e.forEach(t),iao=i(C),Zh=n(C,"LI",{});var h9e=s(Zh);xY=n(h9e,"STRONG",{});var eDr=s(xY);dao=r(eDr,"gpt_neo"),eDr.forEach(t),cao=r(h9e," \u2014 "),FR=n(h9e,"A",{href:!0});var oDr=s(FR);fao=r(oDr,"GPTNeoModel"),oDr.forEach(t),mao=r(h9e," (GPT Neo model)"),h9e.forEach(t),gao=i(C),ep=n(C,"LI",{});var p9e=s(ep);kY=n(p9e,"STRONG",{});var rDr=s(kY);hao=r(rDr,"gptj"),rDr.forEach(t),pao=r(p9e," \u2014 "),CR=n(p9e,"A",{href:!0});var tDr=s(CR);_ao=r(tDr,"GPTJModel"),tDr.forEach(t),uao=r(p9e," (GPT-J model)"),p9e.forEach(t),bao=i(C),op=n(C,"LI",{});var _9e=s(op);RY=n(_9e,"STRONG",{});var aDr=s(RY);vao=r(aDr,"hubert"),aDr.forEach(t),Tao=r(_9e," \u2014 "),MR=n(_9e,"A",{href:!0});var nDr=s(MR);Fao=r(nDr,"HubertModel"),nDr.forEach(t),Cao=r(_9e," (Hubert model)"),_9e.forEach(t),Mao=i(C),rp=n(C,"LI",{});var u9e=s(rp);SY=n(u9e,"STRONG",{});var sDr=s(SY);Eao=r(sDr,"ibert"),sDr.forEach(t),yao=r(u9e," \u2014 "),ER=n(u9e,"A",{href:!0});var lDr=s(ER);wao=r(lDr,"IBertModel"),lDr.forEach(t),Aao=r(u9e," (I-BERT model)"),u9e.forEach(t),Lao=i(C),tp=n(C,"LI",{});var b9e=s(tp);PY=n(b9e,"STRONG",{});var iDr=s(PY);Bao=r(iDr,"imagegpt"),iDr.forEach(t),xao=r(b9e," \u2014 "),yR=n(b9e,"A",{href:!0});var dDr=s(yR);kao=r(dDr,"ImageGPTModel"),dDr.forEach(t),Rao=r(b9e," (ImageGPT model)"),b9e.forEach(t),Sao=i(C),ap=n(C,"LI",{});var v9e=s(ap);$Y=n(v9e,"STRONG",{});var cDr=s($Y);Pao=r(cDr,"layoutlm"),cDr.forEach(t),$ao=r(v9e," \u2014 "),wR=n(v9e,"A",{href:!0});var fDr=s(wR);Iao=r(fDr,"LayoutLMModel"),fDr.forEach(t),Dao=r(v9e," (LayoutLM model)"),v9e.forEach(t),jao=i(C),np=n(C,"LI",{});var T9e=s(np);IY=n(T9e,"STRONG",{});var mDr=s(IY);Nao=r(mDr,"layoutlmv2"),mDr.forEach(t),qao=r(T9e," \u2014 "),AR=n(T9e,"A",{href:!0});var gDr=s(AR);Gao=r(gDr,"LayoutLMv2Model"),gDr.forEach(t),Oao=r(T9e," (LayoutLMv2 model)"),T9e.forEach(t),Xao=i(C),sp=n(C,"LI",{});var F9e=s(sp);DY=n(F9e,"STRONG",{});var hDr=s(DY);Vao=r(hDr,"led"),hDr.forEach(t),zao=r(F9e," \u2014 "),LR=n(F9e,"A",{href:!0});var pDr=s(LR);Wao=r(pDr,"LEDModel"),pDr.forEach(t),Qao=r(F9e," (LED model)"),F9e.forEach(t),Hao=i(C),lp=n(C,"LI",{});var C9e=s(lp);jY=n(C9e,"STRONG",{});var _Dr=s(jY);Uao=r(_Dr,"longformer"),_Dr.forEach(t),Jao=r(C9e," \u2014 "),BR=n(C9e,"A",{href:!0});var uDr=s(BR);Yao=r(uDr,"LongformerModel"),uDr.forEach(t),Kao=r(C9e," (Longformer model)"),C9e.forEach(t),Zao=i(C),ip=n(C,"LI",{});var M9e=s(ip);NY=n(M9e,"STRONG",{});var bDr=s(NY);eno=r(bDr,"luke"),bDr.forEach(t),ono=r(M9e," \u2014 "),xR=n(M9e,"A",{href:!0});var vDr=s(xR);rno=r(vDr,"LukeModel"),vDr.forEach(t),tno=r(M9e," (LUKE model)"),M9e.forEach(t),ano=i(C),dp=n(C,"LI",{});var E9e=s(dp);qY=n(E9e,"STRONG",{});var TDr=s(qY);nno=r(TDr,"lxmert"),TDr.forEach(t),sno=r(E9e," \u2014 "),kR=n(E9e,"A",{href:!0});var FDr=s(kR);lno=r(FDr,"LxmertModel"),FDr.forEach(t),ino=r(E9e," (LXMERT model)"),E9e.forEach(t),dno=i(C),cp=n(C,"LI",{});var y9e=s(cp);GY=n(y9e,"STRONG",{});var CDr=s(GY);cno=r(CDr,"m2m_100"),CDr.forEach(t),fno=r(y9e," \u2014 "),RR=n(y9e,"A",{href:!0});var MDr=s(RR);mno=r(MDr,"M2M100Model"),MDr.forEach(t),gno=r(y9e," (M2M100 model)"),y9e.forEach(t),hno=i(C),fp=n(C,"LI",{});var w9e=s(fp);OY=n(w9e,"STRONG",{});var EDr=s(OY);pno=r(EDr,"marian"),EDr.forEach(t),_no=r(w9e," \u2014 "),SR=n(w9e,"A",{href:!0});var yDr=s(SR);uno=r(yDr,"MarianModel"),yDr.forEach(t),bno=r(w9e," (Marian model)"),w9e.forEach(t),vno=i(C),mp=n(C,"LI",{});var A9e=s(mp);XY=n(A9e,"STRONG",{});var wDr=s(XY);Tno=r(wDr,"maskformer"),wDr.forEach(t),Fno=r(A9e," \u2014 "),PR=n(A9e,"A",{href:!0});var ADr=s(PR);Cno=r(ADr,"MaskFormerModel"),ADr.forEach(t),Mno=r(A9e," (MaskFormer model)"),A9e.forEach(t),Eno=i(C),gp=n(C,"LI",{});var L9e=s(gp);VY=n(L9e,"STRONG",{});var LDr=s(VY);yno=r(LDr,"mbart"),LDr.forEach(t),wno=r(L9e," \u2014 "),$R=n(L9e,"A",{href:!0});var BDr=s($R);Ano=r(BDr,"MBartModel"),BDr.forEach(t),Lno=r(L9e," (mBART model)"),L9e.forEach(t),Bno=i(C),hp=n(C,"LI",{});var B9e=s(hp);zY=n(B9e,"STRONG",{});var xDr=s(zY);xno=r(xDr,"megatron-bert"),xDr.forEach(t),kno=r(B9e," \u2014 "),IR=n(B9e,"A",{href:!0});var kDr=s(IR);Rno=r(kDr,"MegatronBertModel"),kDr.forEach(t),Sno=r(B9e," (MegatronBert model)"),B9e.forEach(t),Pno=i(C),pp=n(C,"LI",{});var x9e=s(pp);WY=n(x9e,"STRONG",{});var RDr=s(WY);$no=r(RDr,"mobilebert"),RDr.forEach(t),Ino=r(x9e," \u2014 "),DR=n(x9e,"A",{href:!0});var SDr=s(DR);Dno=r(SDr,"MobileBertModel"),SDr.forEach(t),jno=r(x9e," (MobileBERT model)"),x9e.forEach(t),Nno=i(C),_p=n(C,"LI",{});var k9e=s(_p);QY=n(k9e,"STRONG",{});var PDr=s(QY);qno=r(PDr,"mpnet"),PDr.forEach(t),Gno=r(k9e," \u2014 "),jR=n(k9e,"A",{href:!0});var $Dr=s(jR);Ono=r($Dr,"MPNetModel"),$Dr.forEach(t),Xno=r(k9e," (MPNet model)"),k9e.forEach(t),Vno=i(C),up=n(C,"LI",{});var R9e=s(up);HY=n(R9e,"STRONG",{});var IDr=s(HY);zno=r(IDr,"mt5"),IDr.forEach(t),Wno=r(R9e," \u2014 "),NR=n(R9e,"A",{href:!0});var DDr=s(NR);Qno=r(DDr,"MT5Model"),DDr.forEach(t),Hno=r(R9e," (mT5 model)"),R9e.forEach(t),Uno=i(C),bp=n(C,"LI",{});var S9e=s(bp);UY=n(S9e,"STRONG",{});var jDr=s(UY);Jno=r(jDr,"nystromformer"),jDr.forEach(t),Yno=r(S9e," \u2014 "),qR=n(S9e,"A",{href:!0});var NDr=s(qR);Kno=r(NDr,"NystromformerModel"),NDr.forEach(t),Zno=r(S9e," (Nystromformer model)"),S9e.forEach(t),eso=i(C),vp=n(C,"LI",{});var P9e=s(vp);JY=n(P9e,"STRONG",{});var qDr=s(JY);oso=r(qDr,"openai-gpt"),qDr.forEach(t),rso=r(P9e," \u2014 "),GR=n(P9e,"A",{href:!0});var GDr=s(GR);tso=r(GDr,"OpenAIGPTModel"),GDr.forEach(t),aso=r(P9e," (OpenAI GPT model)"),P9e.forEach(t),nso=i(C),Tp=n(C,"LI",{});var $9e=s(Tp);YY=n($9e,"STRONG",{});var ODr=s(YY);sso=r(ODr,"pegasus"),ODr.forEach(t),lso=r($9e," \u2014 "),OR=n($9e,"A",{href:!0});var XDr=s(OR);iso=r(XDr,"PegasusModel"),XDr.forEach(t),dso=r($9e," (Pegasus model)"),$9e.forEach(t),cso=i(C),Fp=n(C,"LI",{});var I9e=s(Fp);KY=n(I9e,"STRONG",{});var VDr=s(KY);fso=r(VDr,"perceiver"),VDr.forEach(t),mso=r(I9e," \u2014 "),XR=n(I9e,"A",{href:!0});var zDr=s(XR);gso=r(zDr,"PerceiverModel"),zDr.forEach(t),hso=r(I9e," (Perceiver model)"),I9e.forEach(t),pso=i(C),Cp=n(C,"LI",{});var D9e=s(Cp);ZY=n(D9e,"STRONG",{});var WDr=s(ZY);_so=r(WDr,"plbart"),WDr.forEach(t),uso=r(D9e," \u2014 "),VR=n(D9e,"A",{href:!0});var QDr=s(VR);bso=r(QDr,"PLBartModel"),QDr.forEach(t),vso=r(D9e," (PLBart model)"),D9e.forEach(t),Tso=i(C),Mp=n(C,"LI",{});var j9e=s(Mp);eK=n(j9e,"STRONG",{});var HDr=s(eK);Fso=r(HDr,"poolformer"),HDr.forEach(t),Cso=r(j9e," \u2014 "),zR=n(j9e,"A",{href:!0});var UDr=s(zR);Mso=r(UDr,"PoolFormerModel"),UDr.forEach(t),Eso=r(j9e," (PoolFormer model)"),j9e.forEach(t),yso=i(C),Ep=n(C,"LI",{});var N9e=s(Ep);oK=n(N9e,"STRONG",{});var JDr=s(oK);wso=r(JDr,"prophetnet"),JDr.forEach(t),Aso=r(N9e," \u2014 "),WR=n(N9e,"A",{href:!0});var YDr=s(WR);Lso=r(YDr,"ProphetNetModel"),YDr.forEach(t),Bso=r(N9e," (ProphetNet model)"),N9e.forEach(t),xso=i(C),yp=n(C,"LI",{});var q9e=s(yp);rK=n(q9e,"STRONG",{});var KDr=s(rK);kso=r(KDr,"qdqbert"),KDr.forEach(t),Rso=r(q9e," \u2014 "),QR=n(q9e,"A",{href:!0});var ZDr=s(QR);Sso=r(ZDr,"QDQBertModel"),ZDr.forEach(t),Pso=r(q9e," (QDQBert model)"),q9e.forEach(t),$so=i(C),wp=n(C,"LI",{});var G9e=s(wp);tK=n(G9e,"STRONG",{});var ejr=s(tK);Iso=r(ejr,"reformer"),ejr.forEach(t),Dso=r(G9e," \u2014 "),HR=n(G9e,"A",{href:!0});var ojr=s(HR);jso=r(ojr,"ReformerModel"),ojr.forEach(t),Nso=r(G9e," (Reformer model)"),G9e.forEach(t),qso=i(C),Ap=n(C,"LI",{});var O9e=s(Ap);aK=n(O9e,"STRONG",{});var rjr=s(aK);Gso=r(rjr,"rembert"),rjr.forEach(t),Oso=r(O9e," \u2014 "),UR=n(O9e,"A",{href:!0});var tjr=s(UR);Xso=r(tjr,"RemBertModel"),tjr.forEach(t),Vso=r(O9e," (RemBERT model)"),O9e.forEach(t),zso=i(C),Lp=n(C,"LI",{});var X9e=s(Lp);nK=n(X9e,"STRONG",{});var ajr=s(nK);Wso=r(ajr,"retribert"),ajr.forEach(t),Qso=r(X9e," \u2014 "),JR=n(X9e,"A",{href:!0});var njr=s(JR);Hso=r(njr,"RetriBertModel"),njr.forEach(t),Uso=r(X9e," (RetriBERT model)"),X9e.forEach(t),Jso=i(C),Bp=n(C,"LI",{});var V9e=s(Bp);sK=n(V9e,"STRONG",{});var sjr=s(sK);Yso=r(sjr,"roberta"),sjr.forEach(t),Kso=r(V9e," \u2014 "),YR=n(V9e,"A",{href:!0});var ljr=s(YR);Zso=r(ljr,"RobertaModel"),ljr.forEach(t),elo=r(V9e," (RoBERTa model)"),V9e.forEach(t),olo=i(C),xp=n(C,"LI",{});var z9e=s(xp);lK=n(z9e,"STRONG",{});var ijr=s(lK);rlo=r(ijr,"roformer"),ijr.forEach(t),tlo=r(z9e," \u2014 "),KR=n(z9e,"A",{href:!0});var djr=s(KR);alo=r(djr,"RoFormerModel"),djr.forEach(t),nlo=r(z9e," (RoFormer model)"),z9e.forEach(t),slo=i(C),kp=n(C,"LI",{});var W9e=s(kp);iK=n(W9e,"STRONG",{});var cjr=s(iK);llo=r(cjr,"segformer"),cjr.forEach(t),ilo=r(W9e," \u2014 "),ZR=n(W9e,"A",{href:!0});var fjr=s(ZR);dlo=r(fjr,"SegformerModel"),fjr.forEach(t),clo=r(W9e," (SegFormer model)"),W9e.forEach(t),flo=i(C),Rp=n(C,"LI",{});var Q9e=s(Rp);dK=n(Q9e,"STRONG",{});var mjr=s(dK);mlo=r(mjr,"sew"),mjr.forEach(t),glo=r(Q9e," \u2014 "),eS=n(Q9e,"A",{href:!0});var gjr=s(eS);hlo=r(gjr,"SEWModel"),gjr.forEach(t),plo=r(Q9e," (SEW model)"),Q9e.forEach(t),_lo=i(C),Sp=n(C,"LI",{});var H9e=s(Sp);cK=n(H9e,"STRONG",{});var hjr=s(cK);ulo=r(hjr,"sew-d"),hjr.forEach(t),blo=r(H9e," \u2014 "),oS=n(H9e,"A",{href:!0});var pjr=s(oS);vlo=r(pjr,"SEWDModel"),pjr.forEach(t),Tlo=r(H9e," (SEW-D model)"),H9e.forEach(t),Flo=i(C),Pp=n(C,"LI",{});var U9e=s(Pp);fK=n(U9e,"STRONG",{});var _jr=s(fK);Clo=r(_jr,"speech_to_text"),_jr.forEach(t),Mlo=r(U9e," \u2014 "),rS=n(U9e,"A",{href:!0});var ujr=s(rS);Elo=r(ujr,"Speech2TextModel"),ujr.forEach(t),ylo=r(U9e," (Speech2Text model)"),U9e.forEach(t),wlo=i(C),$p=n(C,"LI",{});var J9e=s($p);mK=n(J9e,"STRONG",{});var bjr=s(mK);Alo=r(bjr,"splinter"),bjr.forEach(t),Llo=r(J9e," \u2014 "),tS=n(J9e,"A",{href:!0});var vjr=s(tS);Blo=r(vjr,"SplinterModel"),vjr.forEach(t),xlo=r(J9e," (Splinter model)"),J9e.forEach(t),klo=i(C),Ip=n(C,"LI",{});var Y9e=s(Ip);gK=n(Y9e,"STRONG",{});var Tjr=s(gK);Rlo=r(Tjr,"squeezebert"),Tjr.forEach(t),Slo=r(Y9e," \u2014 "),aS=n(Y9e,"A",{href:!0});var Fjr=s(aS);Plo=r(Fjr,"SqueezeBertModel"),Fjr.forEach(t),$lo=r(Y9e," (SqueezeBERT model)"),Y9e.forEach(t),Ilo=i(C),Dp=n(C,"LI",{});var K9e=s(Dp);hK=n(K9e,"STRONG",{});var Cjr=s(hK);Dlo=r(Cjr,"swin"),Cjr.forEach(t),jlo=r(K9e," \u2014 "),nS=n(K9e,"A",{href:!0});var Mjr=s(nS);Nlo=r(Mjr,"SwinModel"),Mjr.forEach(t),qlo=r(K9e," (Swin model)"),K9e.forEach(t),Glo=i(C),jp=n(C,"LI",{});var Z9e=s(jp);pK=n(Z9e,"STRONG",{});var Ejr=s(pK);Olo=r(Ejr,"t5"),Ejr.forEach(t),Xlo=r(Z9e," \u2014 "),sS=n(Z9e,"A",{href:!0});var yjr=s(sS);Vlo=r(yjr,"T5Model"),yjr.forEach(t),zlo=r(Z9e," (T5 model)"),Z9e.forEach(t),Wlo=i(C),Np=n(C,"LI",{});var eCe=s(Np);_K=n(eCe,"STRONG",{});var wjr=s(_K);Qlo=r(wjr,"tapas"),wjr.forEach(t),Hlo=r(eCe," \u2014 "),lS=n(eCe,"A",{href:!0});var Ajr=s(lS);Ulo=r(Ajr,"TapasModel"),Ajr.forEach(t),Jlo=r(eCe," (TAPAS model)"),eCe.forEach(t),Ylo=i(C),qp=n(C,"LI",{});var oCe=s(qp);uK=n(oCe,"STRONG",{});var Ljr=s(uK);Klo=r(Ljr,"transfo-xl"),Ljr.forEach(t),Zlo=r(oCe," \u2014 "),iS=n(oCe,"A",{href:!0});var Bjr=s(iS);eio=r(Bjr,"TransfoXLModel"),Bjr.forEach(t),oio=r(oCe," (Transformer-XL model)"),oCe.forEach(t),rio=i(C),Gp=n(C,"LI",{});var rCe=s(Gp);bK=n(rCe,"STRONG",{});var xjr=s(bK);tio=r(xjr,"unispeech"),xjr.forEach(t),aio=r(rCe," \u2014 "),dS=n(rCe,"A",{href:!0});var kjr=s(dS);nio=r(kjr,"UniSpeechModel"),kjr.forEach(t),sio=r(rCe," (UniSpeech model)"),rCe.forEach(t),lio=i(C),Op=n(C,"LI",{});var tCe=s(Op);vK=n(tCe,"STRONG",{});var Rjr=s(vK);iio=r(Rjr,"unispeech-sat"),Rjr.forEach(t),dio=r(tCe," \u2014 "),cS=n(tCe,"A",{href:!0});var Sjr=s(cS);cio=r(Sjr,"UniSpeechSatModel"),Sjr.forEach(t),fio=r(tCe," (UniSpeechSat model)"),tCe.forEach(t),mio=i(C),Xp=n(C,"LI",{});var aCe=s(Xp);TK=n(aCe,"STRONG",{});var Pjr=s(TK);gio=r(Pjr,"vilt"),Pjr.forEach(t),hio=r(aCe," \u2014 "),fS=n(aCe,"A",{href:!0});var $jr=s(fS);pio=r($jr,"ViltModel"),$jr.forEach(t),_io=r(aCe," (ViLT model)"),aCe.forEach(t),uio=i(C),Vp=n(C,"LI",{});var nCe=s(Vp);FK=n(nCe,"STRONG",{});var Ijr=s(FK);bio=r(Ijr,"vision-text-dual-encoder"),Ijr.forEach(t),vio=r(nCe," \u2014 "),mS=n(nCe,"A",{href:!0});var Djr=s(mS);Tio=r(Djr,"VisionTextDualEncoderModel"),Djr.forEach(t),Fio=r(nCe," (VisionTextDualEncoder model)"),nCe.forEach(t),Cio=i(C),zp=n(C,"LI",{});var sCe=s(zp);CK=n(sCe,"STRONG",{});var jjr=s(CK);Mio=r(jjr,"visual_bert"),jjr.forEach(t),Eio=r(sCe," \u2014 "),gS=n(sCe,"A",{href:!0});var Njr=s(gS);yio=r(Njr,"VisualBertModel"),Njr.forEach(t),wio=r(sCe," (VisualBert model)"),sCe.forEach(t),Aio=i(C),Wp=n(C,"LI",{});var lCe=s(Wp);MK=n(lCe,"STRONG",{});var qjr=s(MK);Lio=r(qjr,"vit"),qjr.forEach(t),Bio=r(lCe," \u2014 "),hS=n(lCe,"A",{href:!0});var Gjr=s(hS);xio=r(Gjr,"ViTModel"),Gjr.forEach(t),kio=r(lCe," (ViT model)"),lCe.forEach(t),Rio=i(C),Qp=n(C,"LI",{});var iCe=s(Qp);EK=n(iCe,"STRONG",{});var Ojr=s(EK);Sio=r(Ojr,"vit_mae"),Ojr.forEach(t),Pio=r(iCe," \u2014 "),pS=n(iCe,"A",{href:!0});var Xjr=s(pS);$io=r(Xjr,"ViTMAEModel"),Xjr.forEach(t),Iio=r(iCe," (ViTMAE model)"),iCe.forEach(t),Dio=i(C),Hp=n(C,"LI",{});var dCe=s(Hp);yK=n(dCe,"STRONG",{});var Vjr=s(yK);jio=r(Vjr,"wav2vec2"),Vjr.forEach(t),Nio=r(dCe," \u2014 "),_S=n(dCe,"A",{href:!0});var zjr=s(_S);qio=r(zjr,"Wav2Vec2Model"),zjr.forEach(t),Gio=r(dCe," (Wav2Vec2 model)"),dCe.forEach(t),Oio=i(C),Up=n(C,"LI",{});var cCe=s(Up);wK=n(cCe,"STRONG",{});var Wjr=s(wK);Xio=r(Wjr,"wavlm"),Wjr.forEach(t),Vio=r(cCe," \u2014 "),uS=n(cCe,"A",{href:!0});var Qjr=s(uS);zio=r(Qjr,"WavLMModel"),Qjr.forEach(t),Wio=r(cCe," (WavLM model)"),cCe.forEach(t),Qio=i(C),Jp=n(C,"LI",{});var fCe=s(Jp);AK=n(fCe,"STRONG",{});var Hjr=s(AK);Hio=r(Hjr,"xglm"),Hjr.forEach(t),Uio=r(fCe," \u2014 "),bS=n(fCe,"A",{href:!0});var Ujr=s(bS);Jio=r(Ujr,"XGLMModel"),Ujr.forEach(t),Yio=r(fCe," (XGLM model)"),fCe.forEach(t),Kio=i(C),Yp=n(C,"LI",{});var mCe=s(Yp);LK=n(mCe,"STRONG",{});var Jjr=s(LK);Zio=r(Jjr,"xlm"),Jjr.forEach(t),edo=r(mCe," \u2014 "),vS=n(mCe,"A",{href:!0});var Yjr=s(vS);odo=r(Yjr,"XLMModel"),Yjr.forEach(t),rdo=r(mCe," (XLM model)"),mCe.forEach(t),tdo=i(C),Kp=n(C,"LI",{});var gCe=s(Kp);BK=n(gCe,"STRONG",{});var Kjr=s(BK);ado=r(Kjr,"xlm-prophetnet"),Kjr.forEach(t),ndo=r(gCe," \u2014 "),TS=n(gCe,"A",{href:!0});var Zjr=s(TS);sdo=r(Zjr,"XLMProphetNetModel"),Zjr.forEach(t),ldo=r(gCe," (XLMProphetNet model)"),gCe.forEach(t),ido=i(C),Zp=n(C,"LI",{});var hCe=s(Zp);xK=n(hCe,"STRONG",{});var eNr=s(xK);ddo=r(eNr,"xlm-roberta"),eNr.forEach(t),cdo=r(hCe," \u2014 "),FS=n(hCe,"A",{href:!0});var oNr=s(FS);fdo=r(oNr,"XLMRobertaModel"),oNr.forEach(t),mdo=r(hCe," (XLM-RoBERTa model)"),hCe.forEach(t),gdo=i(C),e_=n(C,"LI",{});var pCe=s(e_);kK=n(pCe,"STRONG",{});var rNr=s(kK);hdo=r(rNr,"xlm-roberta-xl"),rNr.forEach(t),pdo=r(pCe," \u2014 "),CS=n(pCe,"A",{href:!0});var tNr=s(CS);_do=r(tNr,"XLMRobertaXLModel"),tNr.forEach(t),udo=r(pCe," (XLM-RoBERTa-XL model)"),pCe.forEach(t),bdo=i(C),o_=n(C,"LI",{});var _Ce=s(o_);RK=n(_Ce,"STRONG",{});var aNr=s(RK);vdo=r(aNr,"xlnet"),aNr.forEach(t),Tdo=r(_Ce," \u2014 "),MS=n(_Ce,"A",{href:!0});var nNr=s(MS);Fdo=r(nNr,"XLNetModel"),nNr.forEach(t),Cdo=r(_Ce," (XLNet model)"),_Ce.forEach(t),Mdo=i(C),r_=n(C,"LI",{});var uCe=s(r_);SK=n(uCe,"STRONG",{});var sNr=s(SK);Edo=r(sNr,"yoso"),sNr.forEach(t),ydo=r(uCe," \u2014 "),ES=n(uCe,"A",{href:!0});var lNr=s(ES);wdo=r(lNr,"YosoModel"),lNr.forEach(t),Ado=r(uCe," (YOSO model)"),uCe.forEach(t),C.forEach(t),Ldo=i(Dt),t_=n(Dt,"P",{});var bCe=s(t_);Bdo=r(bCe,"The model is set in evaluation mode by default using "),PK=n(bCe,"CODE",{});var iNr=s(PK);xdo=r(iNr,"model.eval()"),iNr.forEach(t),kdo=r(bCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$K=n(bCe,"CODE",{});var dNr=s($K);Rdo=r(dNr,"model.train()"),dNr.forEach(t),bCe.forEach(t),Sdo=i(Dt),IK=n(Dt,"P",{});var cNr=s(IK);Pdo=r(cNr,"Examples:"),cNr.forEach(t),$do=i(Dt),m(pE.$$.fragment,Dt),Dt.forEach(t),qs.forEach(t),hBe=i(c),zi=n(c,"H2",{class:!0});var Fke=s(zi);a_=n(Fke,"A",{id:!0,class:!0,href:!0});var fNr=s(a_);DK=n(fNr,"SPAN",{});var mNr=s(DK);m(_E.$$.fragment,mNr),mNr.forEach(t),fNr.forEach(t),Ido=i(Fke),jK=n(Fke,"SPAN",{});var gNr=s(jK);Ddo=r(gNr,"AutoModelForPreTraining"),gNr.forEach(t),Fke.forEach(t),pBe=i(c),Jo=n(c,"DIV",{class:!0});var Os=s(Jo);m(uE.$$.fragment,Os),jdo=i(Os),Wi=n(Os,"P",{});var OV=s(Wi);Ndo=r(OV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),NK=n(OV,"CODE",{});var hNr=s(NK);qdo=r(hNr,"from_pretrained()"),hNr.forEach(t),Gdo=r(OV,"class method or the "),qK=n(OV,"CODE",{});var pNr=s(qK);Odo=r(pNr,"from_config()"),pNr.forEach(t),Xdo=r(OV,`class
method.`),OV.forEach(t),Vdo=i(Os),bE=n(Os,"P",{});var Cke=s(bE);zdo=r(Cke,"This class cannot be instantiated directly using "),GK=n(Cke,"CODE",{});var _Nr=s(GK);Wdo=r(_Nr,"__init__()"),_Nr.forEach(t),Qdo=r(Cke," (throws an error)."),Cke.forEach(t),Hdo=i(Os),Xr=n(Os,"DIV",{class:!0});var Xs=s(Xr);m(vE.$$.fragment,Xs),Udo=i(Xs),OK=n(Xs,"P",{});var uNr=s(OK);Jdo=r(uNr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),uNr.forEach(t),Ydo=i(Xs),Qi=n(Xs,"P",{});var XV=s(Qi);Kdo=r(XV,`Note:
Loading a model from its configuration file does `),XK=n(XV,"STRONG",{});var bNr=s(XK);Zdo=r(bNr,"not"),bNr.forEach(t),eco=r(XV,` load the model weights. It only affects the
model\u2019s configuration. Use `),VK=n(XV,"CODE",{});var vNr=s(VK);oco=r(vNr,"from_pretrained()"),vNr.forEach(t),rco=r(XV,"to load the model weights."),XV.forEach(t),tco=i(Xs),zK=n(Xs,"P",{});var TNr=s(zK);aco=r(TNr,"Examples:"),TNr.forEach(t),nco=i(Xs),m(TE.$$.fragment,Xs),Xs.forEach(t),sco=i(Os),je=n(Os,"DIV",{class:!0});var jt=s(je);m(FE.$$.fragment,jt),lco=i(jt),WK=n(jt,"P",{});var FNr=s(WK);ico=r(FNr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),FNr.forEach(t),dco=i(jt),qa=n(jt,"P",{});var PM=s(qa);cco=r(PM,"The model class to instantiate is selected based on the "),QK=n(PM,"CODE",{});var CNr=s(QK);fco=r(CNr,"model_type"),CNr.forEach(t),mco=r(PM,` property of the config object (either
passed as an argument or loaded from `),HK=n(PM,"CODE",{});var MNr=s(HK);gco=r(MNr,"pretrained_model_name_or_path"),MNr.forEach(t),hco=r(PM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),UK=n(PM,"CODE",{});var ENr=s(UK);pco=r(ENr,"pretrained_model_name_or_path"),ENr.forEach(t),_co=r(PM,":"),PM.forEach(t),uco=i(jt),k=n(jt,"UL",{});var S=s(k);n_=n(S,"LI",{});var vCe=s(n_);JK=n(vCe,"STRONG",{});var yNr=s(JK);bco=r(yNr,"albert"),yNr.forEach(t),vco=r(vCe," \u2014 "),yS=n(vCe,"A",{href:!0});var wNr=s(yS);Tco=r(wNr,"AlbertForPreTraining"),wNr.forEach(t),Fco=r(vCe," (ALBERT model)"),vCe.forEach(t),Cco=i(S),s_=n(S,"LI",{});var TCe=s(s_);YK=n(TCe,"STRONG",{});var ANr=s(YK);Mco=r(ANr,"bart"),ANr.forEach(t),Eco=r(TCe," \u2014 "),wS=n(TCe,"A",{href:!0});var LNr=s(wS);yco=r(LNr,"BartForConditionalGeneration"),LNr.forEach(t),wco=r(TCe," (BART model)"),TCe.forEach(t),Aco=i(S),l_=n(S,"LI",{});var FCe=s(l_);KK=n(FCe,"STRONG",{});var BNr=s(KK);Lco=r(BNr,"bert"),BNr.forEach(t),Bco=r(FCe," \u2014 "),AS=n(FCe,"A",{href:!0});var xNr=s(AS);xco=r(xNr,"BertForPreTraining"),xNr.forEach(t),kco=r(FCe," (BERT model)"),FCe.forEach(t),Rco=i(S),i_=n(S,"LI",{});var CCe=s(i_);ZK=n(CCe,"STRONG",{});var kNr=s(ZK);Sco=r(kNr,"big_bird"),kNr.forEach(t),Pco=r(CCe," \u2014 "),LS=n(CCe,"A",{href:!0});var RNr=s(LS);$co=r(RNr,"BigBirdForPreTraining"),RNr.forEach(t),Ico=r(CCe," (BigBird model)"),CCe.forEach(t),Dco=i(S),d_=n(S,"LI",{});var MCe=s(d_);eZ=n(MCe,"STRONG",{});var SNr=s(eZ);jco=r(SNr,"camembert"),SNr.forEach(t),Nco=r(MCe," \u2014 "),BS=n(MCe,"A",{href:!0});var PNr=s(BS);qco=r(PNr,"CamembertForMaskedLM"),PNr.forEach(t),Gco=r(MCe," (CamemBERT model)"),MCe.forEach(t),Oco=i(S),c_=n(S,"LI",{});var ECe=s(c_);oZ=n(ECe,"STRONG",{});var $Nr=s(oZ);Xco=r($Nr,"ctrl"),$Nr.forEach(t),Vco=r(ECe," \u2014 "),xS=n(ECe,"A",{href:!0});var INr=s(xS);zco=r(INr,"CTRLLMHeadModel"),INr.forEach(t),Wco=r(ECe," (CTRL model)"),ECe.forEach(t),Qco=i(S),f_=n(S,"LI",{});var yCe=s(f_);rZ=n(yCe,"STRONG",{});var DNr=s(rZ);Hco=r(DNr,"data2vec-text"),DNr.forEach(t),Uco=r(yCe," \u2014 "),kS=n(yCe,"A",{href:!0});var jNr=s(kS);Jco=r(jNr,"Data2VecTextForMaskedLM"),jNr.forEach(t),Yco=r(yCe," (Data2VecText model)"),yCe.forEach(t),Kco=i(S),m_=n(S,"LI",{});var wCe=s(m_);tZ=n(wCe,"STRONG",{});var NNr=s(tZ);Zco=r(NNr,"deberta"),NNr.forEach(t),efo=r(wCe," \u2014 "),RS=n(wCe,"A",{href:!0});var qNr=s(RS);ofo=r(qNr,"DebertaForMaskedLM"),qNr.forEach(t),rfo=r(wCe," (DeBERTa model)"),wCe.forEach(t),tfo=i(S),g_=n(S,"LI",{});var ACe=s(g_);aZ=n(ACe,"STRONG",{});var GNr=s(aZ);afo=r(GNr,"deberta-v2"),GNr.forEach(t),nfo=r(ACe," \u2014 "),SS=n(ACe,"A",{href:!0});var ONr=s(SS);sfo=r(ONr,"DebertaV2ForMaskedLM"),ONr.forEach(t),lfo=r(ACe," (DeBERTa-v2 model)"),ACe.forEach(t),ifo=i(S),h_=n(S,"LI",{});var LCe=s(h_);nZ=n(LCe,"STRONG",{});var XNr=s(nZ);dfo=r(XNr,"distilbert"),XNr.forEach(t),cfo=r(LCe," \u2014 "),PS=n(LCe,"A",{href:!0});var VNr=s(PS);ffo=r(VNr,"DistilBertForMaskedLM"),VNr.forEach(t),mfo=r(LCe," (DistilBERT model)"),LCe.forEach(t),gfo=i(S),p_=n(S,"LI",{});var BCe=s(p_);sZ=n(BCe,"STRONG",{});var zNr=s(sZ);hfo=r(zNr,"electra"),zNr.forEach(t),pfo=r(BCe," \u2014 "),$S=n(BCe,"A",{href:!0});var WNr=s($S);_fo=r(WNr,"ElectraForPreTraining"),WNr.forEach(t),ufo=r(BCe," (ELECTRA model)"),BCe.forEach(t),bfo=i(S),__=n(S,"LI",{});var xCe=s(__);lZ=n(xCe,"STRONG",{});var QNr=s(lZ);vfo=r(QNr,"flaubert"),QNr.forEach(t),Tfo=r(xCe," \u2014 "),IS=n(xCe,"A",{href:!0});var HNr=s(IS);Ffo=r(HNr,"FlaubertWithLMHeadModel"),HNr.forEach(t),Cfo=r(xCe," (FlauBERT model)"),xCe.forEach(t),Mfo=i(S),u_=n(S,"LI",{});var kCe=s(u_);iZ=n(kCe,"STRONG",{});var UNr=s(iZ);Efo=r(UNr,"fnet"),UNr.forEach(t),yfo=r(kCe," \u2014 "),DS=n(kCe,"A",{href:!0});var JNr=s(DS);wfo=r(JNr,"FNetForPreTraining"),JNr.forEach(t),Afo=r(kCe," (FNet model)"),kCe.forEach(t),Lfo=i(S),b_=n(S,"LI",{});var RCe=s(b_);dZ=n(RCe,"STRONG",{});var YNr=s(dZ);Bfo=r(YNr,"fsmt"),YNr.forEach(t),xfo=r(RCe," \u2014 "),jS=n(RCe,"A",{href:!0});var KNr=s(jS);kfo=r(KNr,"FSMTForConditionalGeneration"),KNr.forEach(t),Rfo=r(RCe," (FairSeq Machine-Translation model)"),RCe.forEach(t),Sfo=i(S),v_=n(S,"LI",{});var SCe=s(v_);cZ=n(SCe,"STRONG",{});var ZNr=s(cZ);Pfo=r(ZNr,"funnel"),ZNr.forEach(t),$fo=r(SCe," \u2014 "),NS=n(SCe,"A",{href:!0});var eqr=s(NS);Ifo=r(eqr,"FunnelForPreTraining"),eqr.forEach(t),Dfo=r(SCe," (Funnel Transformer model)"),SCe.forEach(t),jfo=i(S),T_=n(S,"LI",{});var PCe=s(T_);fZ=n(PCe,"STRONG",{});var oqr=s(fZ);Nfo=r(oqr,"gpt2"),oqr.forEach(t),qfo=r(PCe," \u2014 "),qS=n(PCe,"A",{href:!0});var rqr=s(qS);Gfo=r(rqr,"GPT2LMHeadModel"),rqr.forEach(t),Ofo=r(PCe," (OpenAI GPT-2 model)"),PCe.forEach(t),Xfo=i(S),F_=n(S,"LI",{});var $Ce=s(F_);mZ=n($Ce,"STRONG",{});var tqr=s(mZ);Vfo=r(tqr,"ibert"),tqr.forEach(t),zfo=r($Ce," \u2014 "),GS=n($Ce,"A",{href:!0});var aqr=s(GS);Wfo=r(aqr,"IBertForMaskedLM"),aqr.forEach(t),Qfo=r($Ce," (I-BERT model)"),$Ce.forEach(t),Hfo=i(S),C_=n(S,"LI",{});var ICe=s(C_);gZ=n(ICe,"STRONG",{});var nqr=s(gZ);Ufo=r(nqr,"layoutlm"),nqr.forEach(t),Jfo=r(ICe," \u2014 "),OS=n(ICe,"A",{href:!0});var sqr=s(OS);Yfo=r(sqr,"LayoutLMForMaskedLM"),sqr.forEach(t),Kfo=r(ICe," (LayoutLM model)"),ICe.forEach(t),Zfo=i(S),M_=n(S,"LI",{});var DCe=s(M_);hZ=n(DCe,"STRONG",{});var lqr=s(hZ);emo=r(lqr,"longformer"),lqr.forEach(t),omo=r(DCe," \u2014 "),XS=n(DCe,"A",{href:!0});var iqr=s(XS);rmo=r(iqr,"LongformerForMaskedLM"),iqr.forEach(t),tmo=r(DCe," (Longformer model)"),DCe.forEach(t),amo=i(S),E_=n(S,"LI",{});var jCe=s(E_);pZ=n(jCe,"STRONG",{});var dqr=s(pZ);nmo=r(dqr,"lxmert"),dqr.forEach(t),smo=r(jCe," \u2014 "),VS=n(jCe,"A",{href:!0});var cqr=s(VS);lmo=r(cqr,"LxmertForPreTraining"),cqr.forEach(t),imo=r(jCe," (LXMERT model)"),jCe.forEach(t),dmo=i(S),y_=n(S,"LI",{});var NCe=s(y_);_Z=n(NCe,"STRONG",{});var fqr=s(_Z);cmo=r(fqr,"megatron-bert"),fqr.forEach(t),fmo=r(NCe," \u2014 "),zS=n(NCe,"A",{href:!0});var mqr=s(zS);mmo=r(mqr,"MegatronBertForPreTraining"),mqr.forEach(t),gmo=r(NCe," (MegatronBert model)"),NCe.forEach(t),hmo=i(S),w_=n(S,"LI",{});var qCe=s(w_);uZ=n(qCe,"STRONG",{});var gqr=s(uZ);pmo=r(gqr,"mobilebert"),gqr.forEach(t),_mo=r(qCe," \u2014 "),WS=n(qCe,"A",{href:!0});var hqr=s(WS);umo=r(hqr,"MobileBertForPreTraining"),hqr.forEach(t),bmo=r(qCe," (MobileBERT model)"),qCe.forEach(t),vmo=i(S),A_=n(S,"LI",{});var GCe=s(A_);bZ=n(GCe,"STRONG",{});var pqr=s(bZ);Tmo=r(pqr,"mpnet"),pqr.forEach(t),Fmo=r(GCe," \u2014 "),QS=n(GCe,"A",{href:!0});var _qr=s(QS);Cmo=r(_qr,"MPNetForMaskedLM"),_qr.forEach(t),Mmo=r(GCe," (MPNet model)"),GCe.forEach(t),Emo=i(S),L_=n(S,"LI",{});var OCe=s(L_);vZ=n(OCe,"STRONG",{});var uqr=s(vZ);ymo=r(uqr,"openai-gpt"),uqr.forEach(t),wmo=r(OCe," \u2014 "),HS=n(OCe,"A",{href:!0});var bqr=s(HS);Amo=r(bqr,"OpenAIGPTLMHeadModel"),bqr.forEach(t),Lmo=r(OCe," (OpenAI GPT model)"),OCe.forEach(t),Bmo=i(S),B_=n(S,"LI",{});var XCe=s(B_);TZ=n(XCe,"STRONG",{});var vqr=s(TZ);xmo=r(vqr,"retribert"),vqr.forEach(t),kmo=r(XCe," \u2014 "),US=n(XCe,"A",{href:!0});var Tqr=s(US);Rmo=r(Tqr,"RetriBertModel"),Tqr.forEach(t),Smo=r(XCe," (RetriBERT model)"),XCe.forEach(t),Pmo=i(S),x_=n(S,"LI",{});var VCe=s(x_);FZ=n(VCe,"STRONG",{});var Fqr=s(FZ);$mo=r(Fqr,"roberta"),Fqr.forEach(t),Imo=r(VCe," \u2014 "),JS=n(VCe,"A",{href:!0});var Cqr=s(JS);Dmo=r(Cqr,"RobertaForMaskedLM"),Cqr.forEach(t),jmo=r(VCe," (RoBERTa model)"),VCe.forEach(t),Nmo=i(S),k_=n(S,"LI",{});var zCe=s(k_);CZ=n(zCe,"STRONG",{});var Mqr=s(CZ);qmo=r(Mqr,"squeezebert"),Mqr.forEach(t),Gmo=r(zCe," \u2014 "),YS=n(zCe,"A",{href:!0});var Eqr=s(YS);Omo=r(Eqr,"SqueezeBertForMaskedLM"),Eqr.forEach(t),Xmo=r(zCe," (SqueezeBERT model)"),zCe.forEach(t),Vmo=i(S),R_=n(S,"LI",{});var WCe=s(R_);MZ=n(WCe,"STRONG",{});var yqr=s(MZ);zmo=r(yqr,"t5"),yqr.forEach(t),Wmo=r(WCe," \u2014 "),KS=n(WCe,"A",{href:!0});var wqr=s(KS);Qmo=r(wqr,"T5ForConditionalGeneration"),wqr.forEach(t),Hmo=r(WCe," (T5 model)"),WCe.forEach(t),Umo=i(S),S_=n(S,"LI",{});var QCe=s(S_);EZ=n(QCe,"STRONG",{});var Aqr=s(EZ);Jmo=r(Aqr,"tapas"),Aqr.forEach(t),Ymo=r(QCe," \u2014 "),ZS=n(QCe,"A",{href:!0});var Lqr=s(ZS);Kmo=r(Lqr,"TapasForMaskedLM"),Lqr.forEach(t),Zmo=r(QCe," (TAPAS model)"),QCe.forEach(t),ego=i(S),P_=n(S,"LI",{});var HCe=s(P_);yZ=n(HCe,"STRONG",{});var Bqr=s(yZ);ogo=r(Bqr,"transfo-xl"),Bqr.forEach(t),rgo=r(HCe," \u2014 "),eP=n(HCe,"A",{href:!0});var xqr=s(eP);tgo=r(xqr,"TransfoXLLMHeadModel"),xqr.forEach(t),ago=r(HCe," (Transformer-XL model)"),HCe.forEach(t),ngo=i(S),$_=n(S,"LI",{});var UCe=s($_);wZ=n(UCe,"STRONG",{});var kqr=s(wZ);sgo=r(kqr,"unispeech"),kqr.forEach(t),lgo=r(UCe," \u2014 "),oP=n(UCe,"A",{href:!0});var Rqr=s(oP);igo=r(Rqr,"UniSpeechForPreTraining"),Rqr.forEach(t),dgo=r(UCe," (UniSpeech model)"),UCe.forEach(t),cgo=i(S),I_=n(S,"LI",{});var JCe=s(I_);AZ=n(JCe,"STRONG",{});var Sqr=s(AZ);fgo=r(Sqr,"unispeech-sat"),Sqr.forEach(t),mgo=r(JCe," \u2014 "),rP=n(JCe,"A",{href:!0});var Pqr=s(rP);ggo=r(Pqr,"UniSpeechSatForPreTraining"),Pqr.forEach(t),hgo=r(JCe," (UniSpeechSat model)"),JCe.forEach(t),pgo=i(S),D_=n(S,"LI",{});var YCe=s(D_);LZ=n(YCe,"STRONG",{});var $qr=s(LZ);_go=r($qr,"visual_bert"),$qr.forEach(t),ugo=r(YCe," \u2014 "),tP=n(YCe,"A",{href:!0});var Iqr=s(tP);bgo=r(Iqr,"VisualBertForPreTraining"),Iqr.forEach(t),vgo=r(YCe," (VisualBert model)"),YCe.forEach(t),Tgo=i(S),j_=n(S,"LI",{});var KCe=s(j_);BZ=n(KCe,"STRONG",{});var Dqr=s(BZ);Fgo=r(Dqr,"vit_mae"),Dqr.forEach(t),Cgo=r(KCe," \u2014 "),aP=n(KCe,"A",{href:!0});var jqr=s(aP);Mgo=r(jqr,"ViTMAEForPreTraining"),jqr.forEach(t),Ego=r(KCe," (ViTMAE model)"),KCe.forEach(t),ygo=i(S),N_=n(S,"LI",{});var ZCe=s(N_);xZ=n(ZCe,"STRONG",{});var Nqr=s(xZ);wgo=r(Nqr,"wav2vec2"),Nqr.forEach(t),Ago=r(ZCe," \u2014 "),nP=n(ZCe,"A",{href:!0});var qqr=s(nP);Lgo=r(qqr,"Wav2Vec2ForPreTraining"),qqr.forEach(t),Bgo=r(ZCe," (Wav2Vec2 model)"),ZCe.forEach(t),xgo=i(S),q_=n(S,"LI",{});var eMe=s(q_);kZ=n(eMe,"STRONG",{});var Gqr=s(kZ);kgo=r(Gqr,"xlm"),Gqr.forEach(t),Rgo=r(eMe," \u2014 "),sP=n(eMe,"A",{href:!0});var Oqr=s(sP);Sgo=r(Oqr,"XLMWithLMHeadModel"),Oqr.forEach(t),Pgo=r(eMe," (XLM model)"),eMe.forEach(t),$go=i(S),G_=n(S,"LI",{});var oMe=s(G_);RZ=n(oMe,"STRONG",{});var Xqr=s(RZ);Igo=r(Xqr,"xlm-roberta"),Xqr.forEach(t),Dgo=r(oMe," \u2014 "),lP=n(oMe,"A",{href:!0});var Vqr=s(lP);jgo=r(Vqr,"XLMRobertaForMaskedLM"),Vqr.forEach(t),Ngo=r(oMe," (XLM-RoBERTa model)"),oMe.forEach(t),qgo=i(S),O_=n(S,"LI",{});var rMe=s(O_);SZ=n(rMe,"STRONG",{});var zqr=s(SZ);Ggo=r(zqr,"xlm-roberta-xl"),zqr.forEach(t),Ogo=r(rMe," \u2014 "),iP=n(rMe,"A",{href:!0});var Wqr=s(iP);Xgo=r(Wqr,"XLMRobertaXLForMaskedLM"),Wqr.forEach(t),Vgo=r(rMe," (XLM-RoBERTa-XL model)"),rMe.forEach(t),zgo=i(S),X_=n(S,"LI",{});var tMe=s(X_);PZ=n(tMe,"STRONG",{});var Qqr=s(PZ);Wgo=r(Qqr,"xlnet"),Qqr.forEach(t),Qgo=r(tMe," \u2014 "),dP=n(tMe,"A",{href:!0});var Hqr=s(dP);Hgo=r(Hqr,"XLNetLMHeadModel"),Hqr.forEach(t),Ugo=r(tMe," (XLNet model)"),tMe.forEach(t),S.forEach(t),Jgo=i(jt),V_=n(jt,"P",{});var aMe=s(V_);Ygo=r(aMe,"The model is set in evaluation mode by default using "),$Z=n(aMe,"CODE",{});var Uqr=s($Z);Kgo=r(Uqr,"model.eval()"),Uqr.forEach(t),Zgo=r(aMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),IZ=n(aMe,"CODE",{});var Jqr=s(IZ);eho=r(Jqr,"model.train()"),Jqr.forEach(t),aMe.forEach(t),oho=i(jt),DZ=n(jt,"P",{});var Yqr=s(DZ);rho=r(Yqr,"Examples:"),Yqr.forEach(t),tho=i(jt),m(CE.$$.fragment,jt),jt.forEach(t),Os.forEach(t),_Be=i(c),Hi=n(c,"H2",{class:!0});var Mke=s(Hi);z_=n(Mke,"A",{id:!0,class:!0,href:!0});var Kqr=s(z_);jZ=n(Kqr,"SPAN",{});var Zqr=s(jZ);m(ME.$$.fragment,Zqr),Zqr.forEach(t),Kqr.forEach(t),aho=i(Mke),NZ=n(Mke,"SPAN",{});var eGr=s(NZ);nho=r(eGr,"AutoModelForCausalLM"),eGr.forEach(t),Mke.forEach(t),uBe=i(c),Yo=n(c,"DIV",{class:!0});var Vs=s(Yo);m(EE.$$.fragment,Vs),sho=i(Vs),Ui=n(Vs,"P",{});var VV=s(Ui);lho=r(VV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),qZ=n(VV,"CODE",{});var oGr=s(qZ);iho=r(oGr,"from_pretrained()"),oGr.forEach(t),dho=r(VV,"class method or the "),GZ=n(VV,"CODE",{});var rGr=s(GZ);cho=r(rGr,"from_config()"),rGr.forEach(t),fho=r(VV,`class
method.`),VV.forEach(t),mho=i(Vs),yE=n(Vs,"P",{});var Eke=s(yE);gho=r(Eke,"This class cannot be instantiated directly using "),OZ=n(Eke,"CODE",{});var tGr=s(OZ);hho=r(tGr,"__init__()"),tGr.forEach(t),pho=r(Eke," (throws an error)."),Eke.forEach(t),_ho=i(Vs),Vr=n(Vs,"DIV",{class:!0});var zs=s(Vr);m(wE.$$.fragment,zs),uho=i(zs),XZ=n(zs,"P",{});var aGr=s(XZ);bho=r(aGr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),aGr.forEach(t),vho=i(zs),Ji=n(zs,"P",{});var zV=s(Ji);Tho=r(zV,`Note:
Loading a model from its configuration file does `),VZ=n(zV,"STRONG",{});var nGr=s(VZ);Fho=r(nGr,"not"),nGr.forEach(t),Cho=r(zV,` load the model weights. It only affects the
model\u2019s configuration. Use `),zZ=n(zV,"CODE",{});var sGr=s(zZ);Mho=r(sGr,"from_pretrained()"),sGr.forEach(t),Eho=r(zV,"to load the model weights."),zV.forEach(t),yho=i(zs),WZ=n(zs,"P",{});var lGr=s(WZ);who=r(lGr,"Examples:"),lGr.forEach(t),Aho=i(zs),m(AE.$$.fragment,zs),zs.forEach(t),Lho=i(Vs),Ne=n(Vs,"DIV",{class:!0});var Nt=s(Ne);m(LE.$$.fragment,Nt),Bho=i(Nt),QZ=n(Nt,"P",{});var iGr=s(QZ);xho=r(iGr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),iGr.forEach(t),kho=i(Nt),Ga=n(Nt,"P",{});var $M=s(Ga);Rho=r($M,"The model class to instantiate is selected based on the "),HZ=n($M,"CODE",{});var dGr=s(HZ);Sho=r(dGr,"model_type"),dGr.forEach(t),Pho=r($M,` property of the config object (either
passed as an argument or loaded from `),UZ=n($M,"CODE",{});var cGr=s(UZ);$ho=r(cGr,"pretrained_model_name_or_path"),cGr.forEach(t),Iho=r($M,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),JZ=n($M,"CODE",{});var fGr=s(JZ);Dho=r(fGr,"pretrained_model_name_or_path"),fGr.forEach(t),jho=r($M,":"),$M.forEach(t),Nho=i(Nt),$=n(Nt,"UL",{});var D=s($);W_=n(D,"LI",{});var nMe=s(W_);YZ=n(nMe,"STRONG",{});var mGr=s(YZ);qho=r(mGr,"bart"),mGr.forEach(t),Gho=r(nMe," \u2014 "),cP=n(nMe,"A",{href:!0});var gGr=s(cP);Oho=r(gGr,"BartForCausalLM"),gGr.forEach(t),Xho=r(nMe," (BART model)"),nMe.forEach(t),Vho=i(D),Q_=n(D,"LI",{});var sMe=s(Q_);KZ=n(sMe,"STRONG",{});var hGr=s(KZ);zho=r(hGr,"bert"),hGr.forEach(t),Who=r(sMe," \u2014 "),fP=n(sMe,"A",{href:!0});var pGr=s(fP);Qho=r(pGr,"BertLMHeadModel"),pGr.forEach(t),Hho=r(sMe," (BERT model)"),sMe.forEach(t),Uho=i(D),H_=n(D,"LI",{});var lMe=s(H_);ZZ=n(lMe,"STRONG",{});var _Gr=s(ZZ);Jho=r(_Gr,"bert-generation"),_Gr.forEach(t),Yho=r(lMe," \u2014 "),mP=n(lMe,"A",{href:!0});var uGr=s(mP);Kho=r(uGr,"BertGenerationDecoder"),uGr.forEach(t),Zho=r(lMe," (Bert Generation model)"),lMe.forEach(t),epo=i(D),U_=n(D,"LI",{});var iMe=s(U_);eee=n(iMe,"STRONG",{});var bGr=s(eee);opo=r(bGr,"big_bird"),bGr.forEach(t),rpo=r(iMe," \u2014 "),gP=n(iMe,"A",{href:!0});var vGr=s(gP);tpo=r(vGr,"BigBirdForCausalLM"),vGr.forEach(t),apo=r(iMe," (BigBird model)"),iMe.forEach(t),npo=i(D),J_=n(D,"LI",{});var dMe=s(J_);oee=n(dMe,"STRONG",{});var TGr=s(oee);spo=r(TGr,"bigbird_pegasus"),TGr.forEach(t),lpo=r(dMe," \u2014 "),hP=n(dMe,"A",{href:!0});var FGr=s(hP);ipo=r(FGr,"BigBirdPegasusForCausalLM"),FGr.forEach(t),dpo=r(dMe," (BigBirdPegasus model)"),dMe.forEach(t),cpo=i(D),Y_=n(D,"LI",{});var cMe=s(Y_);ree=n(cMe,"STRONG",{});var CGr=s(ree);fpo=r(CGr,"blenderbot"),CGr.forEach(t),mpo=r(cMe," \u2014 "),pP=n(cMe,"A",{href:!0});var MGr=s(pP);gpo=r(MGr,"BlenderbotForCausalLM"),MGr.forEach(t),hpo=r(cMe," (Blenderbot model)"),cMe.forEach(t),ppo=i(D),K_=n(D,"LI",{});var fMe=s(K_);tee=n(fMe,"STRONG",{});var EGr=s(tee);_po=r(EGr,"blenderbot-small"),EGr.forEach(t),upo=r(fMe," \u2014 "),_P=n(fMe,"A",{href:!0});var yGr=s(_P);bpo=r(yGr,"BlenderbotSmallForCausalLM"),yGr.forEach(t),vpo=r(fMe," (BlenderbotSmall model)"),fMe.forEach(t),Tpo=i(D),Z_=n(D,"LI",{});var mMe=s(Z_);aee=n(mMe,"STRONG",{});var wGr=s(aee);Fpo=r(wGr,"camembert"),wGr.forEach(t),Cpo=r(mMe," \u2014 "),uP=n(mMe,"A",{href:!0});var AGr=s(uP);Mpo=r(AGr,"CamembertForCausalLM"),AGr.forEach(t),Epo=r(mMe," (CamemBERT model)"),mMe.forEach(t),ypo=i(D),eu=n(D,"LI",{});var gMe=s(eu);nee=n(gMe,"STRONG",{});var LGr=s(nee);wpo=r(LGr,"ctrl"),LGr.forEach(t),Apo=r(gMe," \u2014 "),bP=n(gMe,"A",{href:!0});var BGr=s(bP);Lpo=r(BGr,"CTRLLMHeadModel"),BGr.forEach(t),Bpo=r(gMe," (CTRL model)"),gMe.forEach(t),xpo=i(D),ou=n(D,"LI",{});var hMe=s(ou);see=n(hMe,"STRONG",{});var xGr=s(see);kpo=r(xGr,"data2vec-text"),xGr.forEach(t),Rpo=r(hMe," \u2014 "),vP=n(hMe,"A",{href:!0});var kGr=s(vP);Spo=r(kGr,"Data2VecTextForCausalLM"),kGr.forEach(t),Ppo=r(hMe," (Data2VecText model)"),hMe.forEach(t),$po=i(D),ru=n(D,"LI",{});var pMe=s(ru);lee=n(pMe,"STRONG",{});var RGr=s(lee);Ipo=r(RGr,"electra"),RGr.forEach(t),Dpo=r(pMe," \u2014 "),TP=n(pMe,"A",{href:!0});var SGr=s(TP);jpo=r(SGr,"ElectraForCausalLM"),SGr.forEach(t),Npo=r(pMe," (ELECTRA model)"),pMe.forEach(t),qpo=i(D),tu=n(D,"LI",{});var _Me=s(tu);iee=n(_Me,"STRONG",{});var PGr=s(iee);Gpo=r(PGr,"gpt2"),PGr.forEach(t),Opo=r(_Me," \u2014 "),FP=n(_Me,"A",{href:!0});var $Gr=s(FP);Xpo=r($Gr,"GPT2LMHeadModel"),$Gr.forEach(t),Vpo=r(_Me," (OpenAI GPT-2 model)"),_Me.forEach(t),zpo=i(D),au=n(D,"LI",{});var uMe=s(au);dee=n(uMe,"STRONG",{});var IGr=s(dee);Wpo=r(IGr,"gpt_neo"),IGr.forEach(t),Qpo=r(uMe," \u2014 "),CP=n(uMe,"A",{href:!0});var DGr=s(CP);Hpo=r(DGr,"GPTNeoForCausalLM"),DGr.forEach(t),Upo=r(uMe," (GPT Neo model)"),uMe.forEach(t),Jpo=i(D),nu=n(D,"LI",{});var bMe=s(nu);cee=n(bMe,"STRONG",{});var jGr=s(cee);Ypo=r(jGr,"gptj"),jGr.forEach(t),Kpo=r(bMe," \u2014 "),MP=n(bMe,"A",{href:!0});var NGr=s(MP);Zpo=r(NGr,"GPTJForCausalLM"),NGr.forEach(t),e_o=r(bMe," (GPT-J model)"),bMe.forEach(t),o_o=i(D),su=n(D,"LI",{});var vMe=s(su);fee=n(vMe,"STRONG",{});var qGr=s(fee);r_o=r(qGr,"marian"),qGr.forEach(t),t_o=r(vMe," \u2014 "),EP=n(vMe,"A",{href:!0});var GGr=s(EP);a_o=r(GGr,"MarianForCausalLM"),GGr.forEach(t),n_o=r(vMe," (Marian model)"),vMe.forEach(t),s_o=i(D),lu=n(D,"LI",{});var TMe=s(lu);mee=n(TMe,"STRONG",{});var OGr=s(mee);l_o=r(OGr,"mbart"),OGr.forEach(t),i_o=r(TMe," \u2014 "),yP=n(TMe,"A",{href:!0});var XGr=s(yP);d_o=r(XGr,"MBartForCausalLM"),XGr.forEach(t),c_o=r(TMe," (mBART model)"),TMe.forEach(t),f_o=i(D),iu=n(D,"LI",{});var FMe=s(iu);gee=n(FMe,"STRONG",{});var VGr=s(gee);m_o=r(VGr,"megatron-bert"),VGr.forEach(t),g_o=r(FMe," \u2014 "),wP=n(FMe,"A",{href:!0});var zGr=s(wP);h_o=r(zGr,"MegatronBertForCausalLM"),zGr.forEach(t),p_o=r(FMe," (MegatronBert model)"),FMe.forEach(t),__o=i(D),du=n(D,"LI",{});var CMe=s(du);hee=n(CMe,"STRONG",{});var WGr=s(hee);u_o=r(WGr,"openai-gpt"),WGr.forEach(t),b_o=r(CMe," \u2014 "),AP=n(CMe,"A",{href:!0});var QGr=s(AP);v_o=r(QGr,"OpenAIGPTLMHeadModel"),QGr.forEach(t),T_o=r(CMe," (OpenAI GPT model)"),CMe.forEach(t),F_o=i(D),cu=n(D,"LI",{});var MMe=s(cu);pee=n(MMe,"STRONG",{});var HGr=s(pee);C_o=r(HGr,"pegasus"),HGr.forEach(t),M_o=r(MMe," \u2014 "),LP=n(MMe,"A",{href:!0});var UGr=s(LP);E_o=r(UGr,"PegasusForCausalLM"),UGr.forEach(t),y_o=r(MMe," (Pegasus model)"),MMe.forEach(t),w_o=i(D),fu=n(D,"LI",{});var EMe=s(fu);_ee=n(EMe,"STRONG",{});var JGr=s(_ee);A_o=r(JGr,"plbart"),JGr.forEach(t),L_o=r(EMe," \u2014 "),BP=n(EMe,"A",{href:!0});var YGr=s(BP);B_o=r(YGr,"PLBartForCausalLM"),YGr.forEach(t),x_o=r(EMe," (PLBart model)"),EMe.forEach(t),k_o=i(D),mu=n(D,"LI",{});var yMe=s(mu);uee=n(yMe,"STRONG",{});var KGr=s(uee);R_o=r(KGr,"prophetnet"),KGr.forEach(t),S_o=r(yMe," \u2014 "),xP=n(yMe,"A",{href:!0});var ZGr=s(xP);P_o=r(ZGr,"ProphetNetForCausalLM"),ZGr.forEach(t),$_o=r(yMe," (ProphetNet model)"),yMe.forEach(t),I_o=i(D),gu=n(D,"LI",{});var wMe=s(gu);bee=n(wMe,"STRONG",{});var eOr=s(bee);D_o=r(eOr,"qdqbert"),eOr.forEach(t),j_o=r(wMe," \u2014 "),kP=n(wMe,"A",{href:!0});var oOr=s(kP);N_o=r(oOr,"QDQBertLMHeadModel"),oOr.forEach(t),q_o=r(wMe," (QDQBert model)"),wMe.forEach(t),G_o=i(D),hu=n(D,"LI",{});var AMe=s(hu);vee=n(AMe,"STRONG",{});var rOr=s(vee);O_o=r(rOr,"reformer"),rOr.forEach(t),X_o=r(AMe," \u2014 "),RP=n(AMe,"A",{href:!0});var tOr=s(RP);V_o=r(tOr,"ReformerModelWithLMHead"),tOr.forEach(t),z_o=r(AMe," (Reformer model)"),AMe.forEach(t),W_o=i(D),pu=n(D,"LI",{});var LMe=s(pu);Tee=n(LMe,"STRONG",{});var aOr=s(Tee);Q_o=r(aOr,"rembert"),aOr.forEach(t),H_o=r(LMe," \u2014 "),SP=n(LMe,"A",{href:!0});var nOr=s(SP);U_o=r(nOr,"RemBertForCausalLM"),nOr.forEach(t),J_o=r(LMe," (RemBERT model)"),LMe.forEach(t),Y_o=i(D),_u=n(D,"LI",{});var BMe=s(_u);Fee=n(BMe,"STRONG",{});var sOr=s(Fee);K_o=r(sOr,"roberta"),sOr.forEach(t),Z_o=r(BMe," \u2014 "),PP=n(BMe,"A",{href:!0});var lOr=s(PP);euo=r(lOr,"RobertaForCausalLM"),lOr.forEach(t),ouo=r(BMe," (RoBERTa model)"),BMe.forEach(t),ruo=i(D),uu=n(D,"LI",{});var xMe=s(uu);Cee=n(xMe,"STRONG",{});var iOr=s(Cee);tuo=r(iOr,"roformer"),iOr.forEach(t),auo=r(xMe," \u2014 "),$P=n(xMe,"A",{href:!0});var dOr=s($P);nuo=r(dOr,"RoFormerForCausalLM"),dOr.forEach(t),suo=r(xMe," (RoFormer model)"),xMe.forEach(t),luo=i(D),bu=n(D,"LI",{});var kMe=s(bu);Mee=n(kMe,"STRONG",{});var cOr=s(Mee);iuo=r(cOr,"speech_to_text_2"),cOr.forEach(t),duo=r(kMe," \u2014 "),IP=n(kMe,"A",{href:!0});var fOr=s(IP);cuo=r(fOr,"Speech2Text2ForCausalLM"),fOr.forEach(t),fuo=r(kMe," (Speech2Text2 model)"),kMe.forEach(t),muo=i(D),vu=n(D,"LI",{});var RMe=s(vu);Eee=n(RMe,"STRONG",{});var mOr=s(Eee);guo=r(mOr,"transfo-xl"),mOr.forEach(t),huo=r(RMe," \u2014 "),DP=n(RMe,"A",{href:!0});var gOr=s(DP);puo=r(gOr,"TransfoXLLMHeadModel"),gOr.forEach(t),_uo=r(RMe," (Transformer-XL model)"),RMe.forEach(t),uuo=i(D),Tu=n(D,"LI",{});var SMe=s(Tu);yee=n(SMe,"STRONG",{});var hOr=s(yee);buo=r(hOr,"trocr"),hOr.forEach(t),vuo=r(SMe," \u2014 "),jP=n(SMe,"A",{href:!0});var pOr=s(jP);Tuo=r(pOr,"TrOCRForCausalLM"),pOr.forEach(t),Fuo=r(SMe," (TrOCR model)"),SMe.forEach(t),Cuo=i(D),Fu=n(D,"LI",{});var PMe=s(Fu);wee=n(PMe,"STRONG",{});var _Or=s(wee);Muo=r(_Or,"xglm"),_Or.forEach(t),Euo=r(PMe," \u2014 "),NP=n(PMe,"A",{href:!0});var uOr=s(NP);yuo=r(uOr,"XGLMForCausalLM"),uOr.forEach(t),wuo=r(PMe," (XGLM model)"),PMe.forEach(t),Auo=i(D),Cu=n(D,"LI",{});var $Me=s(Cu);Aee=n($Me,"STRONG",{});var bOr=s(Aee);Luo=r(bOr,"xlm"),bOr.forEach(t),Buo=r($Me," \u2014 "),qP=n($Me,"A",{href:!0});var vOr=s(qP);xuo=r(vOr,"XLMWithLMHeadModel"),vOr.forEach(t),kuo=r($Me," (XLM model)"),$Me.forEach(t),Ruo=i(D),Mu=n(D,"LI",{});var IMe=s(Mu);Lee=n(IMe,"STRONG",{});var TOr=s(Lee);Suo=r(TOr,"xlm-prophetnet"),TOr.forEach(t),Puo=r(IMe," \u2014 "),GP=n(IMe,"A",{href:!0});var FOr=s(GP);$uo=r(FOr,"XLMProphetNetForCausalLM"),FOr.forEach(t),Iuo=r(IMe," (XLMProphetNet model)"),IMe.forEach(t),Duo=i(D),Eu=n(D,"LI",{});var DMe=s(Eu);Bee=n(DMe,"STRONG",{});var COr=s(Bee);juo=r(COr,"xlm-roberta"),COr.forEach(t),Nuo=r(DMe," \u2014 "),OP=n(DMe,"A",{href:!0});var MOr=s(OP);quo=r(MOr,"XLMRobertaForCausalLM"),MOr.forEach(t),Guo=r(DMe," (XLM-RoBERTa model)"),DMe.forEach(t),Ouo=i(D),yu=n(D,"LI",{});var jMe=s(yu);xee=n(jMe,"STRONG",{});var EOr=s(xee);Xuo=r(EOr,"xlm-roberta-xl"),EOr.forEach(t),Vuo=r(jMe," \u2014 "),XP=n(jMe,"A",{href:!0});var yOr=s(XP);zuo=r(yOr,"XLMRobertaXLForCausalLM"),yOr.forEach(t),Wuo=r(jMe," (XLM-RoBERTa-XL model)"),jMe.forEach(t),Quo=i(D),wu=n(D,"LI",{});var NMe=s(wu);kee=n(NMe,"STRONG",{});var wOr=s(kee);Huo=r(wOr,"xlnet"),wOr.forEach(t),Uuo=r(NMe," \u2014 "),VP=n(NMe,"A",{href:!0});var AOr=s(VP);Juo=r(AOr,"XLNetLMHeadModel"),AOr.forEach(t),Yuo=r(NMe," (XLNet model)"),NMe.forEach(t),D.forEach(t),Kuo=i(Nt),Au=n(Nt,"P",{});var qMe=s(Au);Zuo=r(qMe,"The model is set in evaluation mode by default using "),Ree=n(qMe,"CODE",{});var LOr=s(Ree);e0o=r(LOr,"model.eval()"),LOr.forEach(t),o0o=r(qMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),See=n(qMe,"CODE",{});var BOr=s(See);r0o=r(BOr,"model.train()"),BOr.forEach(t),qMe.forEach(t),t0o=i(Nt),Pee=n(Nt,"P",{});var xOr=s(Pee);a0o=r(xOr,"Examples:"),xOr.forEach(t),n0o=i(Nt),m(BE.$$.fragment,Nt),Nt.forEach(t),Vs.forEach(t),bBe=i(c),Yi=n(c,"H2",{class:!0});var yke=s(Yi);Lu=n(yke,"A",{id:!0,class:!0,href:!0});var kOr=s(Lu);$ee=n(kOr,"SPAN",{});var ROr=s($ee);m(xE.$$.fragment,ROr),ROr.forEach(t),kOr.forEach(t),s0o=i(yke),Iee=n(yke,"SPAN",{});var SOr=s(Iee);l0o=r(SOr,"AutoModelForMaskedLM"),SOr.forEach(t),yke.forEach(t),vBe=i(c),Ko=n(c,"DIV",{class:!0});var Ws=s(Ko);m(kE.$$.fragment,Ws),i0o=i(Ws),Ki=n(Ws,"P",{});var WV=s(Ki);d0o=r(WV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Dee=n(WV,"CODE",{});var POr=s(Dee);c0o=r(POr,"from_pretrained()"),POr.forEach(t),f0o=r(WV,"class method or the "),jee=n(WV,"CODE",{});var $Or=s(jee);m0o=r($Or,"from_config()"),$Or.forEach(t),g0o=r(WV,`class
method.`),WV.forEach(t),h0o=i(Ws),RE=n(Ws,"P",{});var wke=s(RE);p0o=r(wke,"This class cannot be instantiated directly using "),Nee=n(wke,"CODE",{});var IOr=s(Nee);_0o=r(IOr,"__init__()"),IOr.forEach(t),u0o=r(wke," (throws an error)."),wke.forEach(t),b0o=i(Ws),zr=n(Ws,"DIV",{class:!0});var Qs=s(zr);m(SE.$$.fragment,Qs),v0o=i(Qs),qee=n(Qs,"P",{});var DOr=s(qee);T0o=r(DOr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),DOr.forEach(t),F0o=i(Qs),Zi=n(Qs,"P",{});var QV=s(Zi);C0o=r(QV,`Note:
Loading a model from its configuration file does `),Gee=n(QV,"STRONG",{});var jOr=s(Gee);M0o=r(jOr,"not"),jOr.forEach(t),E0o=r(QV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Oee=n(QV,"CODE",{});var NOr=s(Oee);y0o=r(NOr,"from_pretrained()"),NOr.forEach(t),w0o=r(QV,"to load the model weights."),QV.forEach(t),A0o=i(Qs),Xee=n(Qs,"P",{});var qOr=s(Xee);L0o=r(qOr,"Examples:"),qOr.forEach(t),B0o=i(Qs),m(PE.$$.fragment,Qs),Qs.forEach(t),x0o=i(Ws),qe=n(Ws,"DIV",{class:!0});var qt=s(qe);m($E.$$.fragment,qt),k0o=i(qt),Vee=n(qt,"P",{});var GOr=s(Vee);R0o=r(GOr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),GOr.forEach(t),S0o=i(qt),Oa=n(qt,"P",{});var IM=s(Oa);P0o=r(IM,"The model class to instantiate is selected based on the "),zee=n(IM,"CODE",{});var OOr=s(zee);$0o=r(OOr,"model_type"),OOr.forEach(t),I0o=r(IM,` property of the config object (either
passed as an argument or loaded from `),Wee=n(IM,"CODE",{});var XOr=s(Wee);D0o=r(XOr,"pretrained_model_name_or_path"),XOr.forEach(t),j0o=r(IM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qee=n(IM,"CODE",{});var VOr=s(Qee);N0o=r(VOr,"pretrained_model_name_or_path"),VOr.forEach(t),q0o=r(IM,":"),IM.forEach(t),G0o=i(qt),I=n(qt,"UL",{});var j=s(I);Bu=n(j,"LI",{});var GMe=s(Bu);Hee=n(GMe,"STRONG",{});var zOr=s(Hee);O0o=r(zOr,"albert"),zOr.forEach(t),X0o=r(GMe," \u2014 "),zP=n(GMe,"A",{href:!0});var WOr=s(zP);V0o=r(WOr,"AlbertForMaskedLM"),WOr.forEach(t),z0o=r(GMe," (ALBERT model)"),GMe.forEach(t),W0o=i(j),xu=n(j,"LI",{});var OMe=s(xu);Uee=n(OMe,"STRONG",{});var QOr=s(Uee);Q0o=r(QOr,"bart"),QOr.forEach(t),H0o=r(OMe," \u2014 "),WP=n(OMe,"A",{href:!0});var HOr=s(WP);U0o=r(HOr,"BartForConditionalGeneration"),HOr.forEach(t),J0o=r(OMe," (BART model)"),OMe.forEach(t),Y0o=i(j),ku=n(j,"LI",{});var XMe=s(ku);Jee=n(XMe,"STRONG",{});var UOr=s(Jee);K0o=r(UOr,"bert"),UOr.forEach(t),Z0o=r(XMe," \u2014 "),QP=n(XMe,"A",{href:!0});var JOr=s(QP);e1o=r(JOr,"BertForMaskedLM"),JOr.forEach(t),o1o=r(XMe," (BERT model)"),XMe.forEach(t),r1o=i(j),Ru=n(j,"LI",{});var VMe=s(Ru);Yee=n(VMe,"STRONG",{});var YOr=s(Yee);t1o=r(YOr,"big_bird"),YOr.forEach(t),a1o=r(VMe," \u2014 "),HP=n(VMe,"A",{href:!0});var KOr=s(HP);n1o=r(KOr,"BigBirdForMaskedLM"),KOr.forEach(t),s1o=r(VMe," (BigBird model)"),VMe.forEach(t),l1o=i(j),Su=n(j,"LI",{});var zMe=s(Su);Kee=n(zMe,"STRONG",{});var ZOr=s(Kee);i1o=r(ZOr,"camembert"),ZOr.forEach(t),d1o=r(zMe," \u2014 "),UP=n(zMe,"A",{href:!0});var eXr=s(UP);c1o=r(eXr,"CamembertForMaskedLM"),eXr.forEach(t),f1o=r(zMe," (CamemBERT model)"),zMe.forEach(t),m1o=i(j),Pu=n(j,"LI",{});var WMe=s(Pu);Zee=n(WMe,"STRONG",{});var oXr=s(Zee);g1o=r(oXr,"convbert"),oXr.forEach(t),h1o=r(WMe," \u2014 "),JP=n(WMe,"A",{href:!0});var rXr=s(JP);p1o=r(rXr,"ConvBertForMaskedLM"),rXr.forEach(t),_1o=r(WMe," (ConvBERT model)"),WMe.forEach(t),u1o=i(j),$u=n(j,"LI",{});var QMe=s($u);eoe=n(QMe,"STRONG",{});var tXr=s(eoe);b1o=r(tXr,"data2vec-text"),tXr.forEach(t),v1o=r(QMe," \u2014 "),YP=n(QMe,"A",{href:!0});var aXr=s(YP);T1o=r(aXr,"Data2VecTextForMaskedLM"),aXr.forEach(t),F1o=r(QMe," (Data2VecText model)"),QMe.forEach(t),C1o=i(j),Iu=n(j,"LI",{});var HMe=s(Iu);ooe=n(HMe,"STRONG",{});var nXr=s(ooe);M1o=r(nXr,"deberta"),nXr.forEach(t),E1o=r(HMe," \u2014 "),KP=n(HMe,"A",{href:!0});var sXr=s(KP);y1o=r(sXr,"DebertaForMaskedLM"),sXr.forEach(t),w1o=r(HMe," (DeBERTa model)"),HMe.forEach(t),A1o=i(j),Du=n(j,"LI",{});var UMe=s(Du);roe=n(UMe,"STRONG",{});var lXr=s(roe);L1o=r(lXr,"deberta-v2"),lXr.forEach(t),B1o=r(UMe," \u2014 "),ZP=n(UMe,"A",{href:!0});var iXr=s(ZP);x1o=r(iXr,"DebertaV2ForMaskedLM"),iXr.forEach(t),k1o=r(UMe," (DeBERTa-v2 model)"),UMe.forEach(t),R1o=i(j),ju=n(j,"LI",{});var JMe=s(ju);toe=n(JMe,"STRONG",{});var dXr=s(toe);S1o=r(dXr,"distilbert"),dXr.forEach(t),P1o=r(JMe," \u2014 "),e$=n(JMe,"A",{href:!0});var cXr=s(e$);$1o=r(cXr,"DistilBertForMaskedLM"),cXr.forEach(t),I1o=r(JMe," (DistilBERT model)"),JMe.forEach(t),D1o=i(j),Nu=n(j,"LI",{});var YMe=s(Nu);aoe=n(YMe,"STRONG",{});var fXr=s(aoe);j1o=r(fXr,"electra"),fXr.forEach(t),N1o=r(YMe," \u2014 "),o$=n(YMe,"A",{href:!0});var mXr=s(o$);q1o=r(mXr,"ElectraForMaskedLM"),mXr.forEach(t),G1o=r(YMe," (ELECTRA model)"),YMe.forEach(t),O1o=i(j),qu=n(j,"LI",{});var KMe=s(qu);noe=n(KMe,"STRONG",{});var gXr=s(noe);X1o=r(gXr,"flaubert"),gXr.forEach(t),V1o=r(KMe," \u2014 "),r$=n(KMe,"A",{href:!0});var hXr=s(r$);z1o=r(hXr,"FlaubertWithLMHeadModel"),hXr.forEach(t),W1o=r(KMe," (FlauBERT model)"),KMe.forEach(t),Q1o=i(j),Gu=n(j,"LI",{});var ZMe=s(Gu);soe=n(ZMe,"STRONG",{});var pXr=s(soe);H1o=r(pXr,"fnet"),pXr.forEach(t),U1o=r(ZMe," \u2014 "),t$=n(ZMe,"A",{href:!0});var _Xr=s(t$);J1o=r(_Xr,"FNetForMaskedLM"),_Xr.forEach(t),Y1o=r(ZMe," (FNet model)"),ZMe.forEach(t),K1o=i(j),Ou=n(j,"LI",{});var e4e=s(Ou);loe=n(e4e,"STRONG",{});var uXr=s(loe);Z1o=r(uXr,"funnel"),uXr.forEach(t),ebo=r(e4e," \u2014 "),a$=n(e4e,"A",{href:!0});var bXr=s(a$);obo=r(bXr,"FunnelForMaskedLM"),bXr.forEach(t),rbo=r(e4e," (Funnel Transformer model)"),e4e.forEach(t),tbo=i(j),Xu=n(j,"LI",{});var o4e=s(Xu);ioe=n(o4e,"STRONG",{});var vXr=s(ioe);abo=r(vXr,"ibert"),vXr.forEach(t),nbo=r(o4e," \u2014 "),n$=n(o4e,"A",{href:!0});var TXr=s(n$);sbo=r(TXr,"IBertForMaskedLM"),TXr.forEach(t),lbo=r(o4e," (I-BERT model)"),o4e.forEach(t),ibo=i(j),Vu=n(j,"LI",{});var r4e=s(Vu);doe=n(r4e,"STRONG",{});var FXr=s(doe);dbo=r(FXr,"layoutlm"),FXr.forEach(t),cbo=r(r4e," \u2014 "),s$=n(r4e,"A",{href:!0});var CXr=s(s$);fbo=r(CXr,"LayoutLMForMaskedLM"),CXr.forEach(t),mbo=r(r4e," (LayoutLM model)"),r4e.forEach(t),gbo=i(j),zu=n(j,"LI",{});var t4e=s(zu);coe=n(t4e,"STRONG",{});var MXr=s(coe);hbo=r(MXr,"longformer"),MXr.forEach(t),pbo=r(t4e," \u2014 "),l$=n(t4e,"A",{href:!0});var EXr=s(l$);_bo=r(EXr,"LongformerForMaskedLM"),EXr.forEach(t),ubo=r(t4e," (Longformer model)"),t4e.forEach(t),bbo=i(j),Wu=n(j,"LI",{});var a4e=s(Wu);foe=n(a4e,"STRONG",{});var yXr=s(foe);vbo=r(yXr,"mbart"),yXr.forEach(t),Tbo=r(a4e," \u2014 "),i$=n(a4e,"A",{href:!0});var wXr=s(i$);Fbo=r(wXr,"MBartForConditionalGeneration"),wXr.forEach(t),Cbo=r(a4e," (mBART model)"),a4e.forEach(t),Mbo=i(j),Qu=n(j,"LI",{});var n4e=s(Qu);moe=n(n4e,"STRONG",{});var AXr=s(moe);Ebo=r(AXr,"megatron-bert"),AXr.forEach(t),ybo=r(n4e," \u2014 "),d$=n(n4e,"A",{href:!0});var LXr=s(d$);wbo=r(LXr,"MegatronBertForMaskedLM"),LXr.forEach(t),Abo=r(n4e," (MegatronBert model)"),n4e.forEach(t),Lbo=i(j),Hu=n(j,"LI",{});var s4e=s(Hu);goe=n(s4e,"STRONG",{});var BXr=s(goe);Bbo=r(BXr,"mobilebert"),BXr.forEach(t),xbo=r(s4e," \u2014 "),c$=n(s4e,"A",{href:!0});var xXr=s(c$);kbo=r(xXr,"MobileBertForMaskedLM"),xXr.forEach(t),Rbo=r(s4e," (MobileBERT model)"),s4e.forEach(t),Sbo=i(j),Uu=n(j,"LI",{});var l4e=s(Uu);hoe=n(l4e,"STRONG",{});var kXr=s(hoe);Pbo=r(kXr,"mpnet"),kXr.forEach(t),$bo=r(l4e," \u2014 "),f$=n(l4e,"A",{href:!0});var RXr=s(f$);Ibo=r(RXr,"MPNetForMaskedLM"),RXr.forEach(t),Dbo=r(l4e," (MPNet model)"),l4e.forEach(t),jbo=i(j),Ju=n(j,"LI",{});var i4e=s(Ju);poe=n(i4e,"STRONG",{});var SXr=s(poe);Nbo=r(SXr,"nystromformer"),SXr.forEach(t),qbo=r(i4e," \u2014 "),m$=n(i4e,"A",{href:!0});var PXr=s(m$);Gbo=r(PXr,"NystromformerForMaskedLM"),PXr.forEach(t),Obo=r(i4e," (Nystromformer model)"),i4e.forEach(t),Xbo=i(j),Yu=n(j,"LI",{});var d4e=s(Yu);_oe=n(d4e,"STRONG",{});var $Xr=s(_oe);Vbo=r($Xr,"perceiver"),$Xr.forEach(t),zbo=r(d4e," \u2014 "),g$=n(d4e,"A",{href:!0});var IXr=s(g$);Wbo=r(IXr,"PerceiverForMaskedLM"),IXr.forEach(t),Qbo=r(d4e," (Perceiver model)"),d4e.forEach(t),Hbo=i(j),Ku=n(j,"LI",{});var c4e=s(Ku);uoe=n(c4e,"STRONG",{});var DXr=s(uoe);Ubo=r(DXr,"qdqbert"),DXr.forEach(t),Jbo=r(c4e," \u2014 "),h$=n(c4e,"A",{href:!0});var jXr=s(h$);Ybo=r(jXr,"QDQBertForMaskedLM"),jXr.forEach(t),Kbo=r(c4e," (QDQBert model)"),c4e.forEach(t),Zbo=i(j),Zu=n(j,"LI",{});var f4e=s(Zu);boe=n(f4e,"STRONG",{});var NXr=s(boe);e5o=r(NXr,"reformer"),NXr.forEach(t),o5o=r(f4e," \u2014 "),p$=n(f4e,"A",{href:!0});var qXr=s(p$);r5o=r(qXr,"ReformerForMaskedLM"),qXr.forEach(t),t5o=r(f4e," (Reformer model)"),f4e.forEach(t),a5o=i(j),e0=n(j,"LI",{});var m4e=s(e0);voe=n(m4e,"STRONG",{});var GXr=s(voe);n5o=r(GXr,"rembert"),GXr.forEach(t),s5o=r(m4e," \u2014 "),_$=n(m4e,"A",{href:!0});var OXr=s(_$);l5o=r(OXr,"RemBertForMaskedLM"),OXr.forEach(t),i5o=r(m4e," (RemBERT model)"),m4e.forEach(t),d5o=i(j),o0=n(j,"LI",{});var g4e=s(o0);Toe=n(g4e,"STRONG",{});var XXr=s(Toe);c5o=r(XXr,"roberta"),XXr.forEach(t),f5o=r(g4e," \u2014 "),u$=n(g4e,"A",{href:!0});var VXr=s(u$);m5o=r(VXr,"RobertaForMaskedLM"),VXr.forEach(t),g5o=r(g4e," (RoBERTa model)"),g4e.forEach(t),h5o=i(j),r0=n(j,"LI",{});var h4e=s(r0);Foe=n(h4e,"STRONG",{});var zXr=s(Foe);p5o=r(zXr,"roformer"),zXr.forEach(t),_5o=r(h4e," \u2014 "),b$=n(h4e,"A",{href:!0});var WXr=s(b$);u5o=r(WXr,"RoFormerForMaskedLM"),WXr.forEach(t),b5o=r(h4e," (RoFormer model)"),h4e.forEach(t),v5o=i(j),t0=n(j,"LI",{});var p4e=s(t0);Coe=n(p4e,"STRONG",{});var QXr=s(Coe);T5o=r(QXr,"squeezebert"),QXr.forEach(t),F5o=r(p4e," \u2014 "),v$=n(p4e,"A",{href:!0});var HXr=s(v$);C5o=r(HXr,"SqueezeBertForMaskedLM"),HXr.forEach(t),M5o=r(p4e," (SqueezeBERT model)"),p4e.forEach(t),E5o=i(j),a0=n(j,"LI",{});var _4e=s(a0);Moe=n(_4e,"STRONG",{});var UXr=s(Moe);y5o=r(UXr,"tapas"),UXr.forEach(t),w5o=r(_4e," \u2014 "),T$=n(_4e,"A",{href:!0});var JXr=s(T$);A5o=r(JXr,"TapasForMaskedLM"),JXr.forEach(t),L5o=r(_4e," (TAPAS model)"),_4e.forEach(t),B5o=i(j),n0=n(j,"LI",{});var u4e=s(n0);Eoe=n(u4e,"STRONG",{});var YXr=s(Eoe);x5o=r(YXr,"wav2vec2"),YXr.forEach(t),k5o=r(u4e," \u2014 "),yoe=n(u4e,"CODE",{});var KXr=s(yoe);R5o=r(KXr,"Wav2Vec2ForMaskedLM"),KXr.forEach(t),S5o=r(u4e,"(Wav2Vec2 model)"),u4e.forEach(t),P5o=i(j),s0=n(j,"LI",{});var b4e=s(s0);woe=n(b4e,"STRONG",{});var ZXr=s(woe);$5o=r(ZXr,"xlm"),ZXr.forEach(t),I5o=r(b4e," \u2014 "),F$=n(b4e,"A",{href:!0});var eVr=s(F$);D5o=r(eVr,"XLMWithLMHeadModel"),eVr.forEach(t),j5o=r(b4e," (XLM model)"),b4e.forEach(t),N5o=i(j),l0=n(j,"LI",{});var v4e=s(l0);Aoe=n(v4e,"STRONG",{});var oVr=s(Aoe);q5o=r(oVr,"xlm-roberta"),oVr.forEach(t),G5o=r(v4e," \u2014 "),C$=n(v4e,"A",{href:!0});var rVr=s(C$);O5o=r(rVr,"XLMRobertaForMaskedLM"),rVr.forEach(t),X5o=r(v4e," (XLM-RoBERTa model)"),v4e.forEach(t),V5o=i(j),i0=n(j,"LI",{});var T4e=s(i0);Loe=n(T4e,"STRONG",{});var tVr=s(Loe);z5o=r(tVr,"xlm-roberta-xl"),tVr.forEach(t),W5o=r(T4e," \u2014 "),M$=n(T4e,"A",{href:!0});var aVr=s(M$);Q5o=r(aVr,"XLMRobertaXLForMaskedLM"),aVr.forEach(t),H5o=r(T4e," (XLM-RoBERTa-XL model)"),T4e.forEach(t),U5o=i(j),d0=n(j,"LI",{});var F4e=s(d0);Boe=n(F4e,"STRONG",{});var nVr=s(Boe);J5o=r(nVr,"yoso"),nVr.forEach(t),Y5o=r(F4e," \u2014 "),E$=n(F4e,"A",{href:!0});var sVr=s(E$);K5o=r(sVr,"YosoForMaskedLM"),sVr.forEach(t),Z5o=r(F4e," (YOSO model)"),F4e.forEach(t),j.forEach(t),e2o=i(qt),c0=n(qt,"P",{});var C4e=s(c0);o2o=r(C4e,"The model is set in evaluation mode by default using "),xoe=n(C4e,"CODE",{});var lVr=s(xoe);r2o=r(lVr,"model.eval()"),lVr.forEach(t),t2o=r(C4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),koe=n(C4e,"CODE",{});var iVr=s(koe);a2o=r(iVr,"model.train()"),iVr.forEach(t),C4e.forEach(t),n2o=i(qt),Roe=n(qt,"P",{});var dVr=s(Roe);s2o=r(dVr,"Examples:"),dVr.forEach(t),l2o=i(qt),m(IE.$$.fragment,qt),qt.forEach(t),Ws.forEach(t),TBe=i(c),ed=n(c,"H2",{class:!0});var Ake=s(ed);f0=n(Ake,"A",{id:!0,class:!0,href:!0});var cVr=s(f0);Soe=n(cVr,"SPAN",{});var fVr=s(Soe);m(DE.$$.fragment,fVr),fVr.forEach(t),cVr.forEach(t),i2o=i(Ake),Poe=n(Ake,"SPAN",{});var mVr=s(Poe);d2o=r(mVr,"AutoModelForSeq2SeqLM"),mVr.forEach(t),Ake.forEach(t),FBe=i(c),Zo=n(c,"DIV",{class:!0});var Hs=s(Zo);m(jE.$$.fragment,Hs),c2o=i(Hs),od=n(Hs,"P",{});var HV=s(od);f2o=r(HV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$oe=n(HV,"CODE",{});var gVr=s($oe);m2o=r(gVr,"from_pretrained()"),gVr.forEach(t),g2o=r(HV,"class method or the "),Ioe=n(HV,"CODE",{});var hVr=s(Ioe);h2o=r(hVr,"from_config()"),hVr.forEach(t),p2o=r(HV,`class
method.`),HV.forEach(t),_2o=i(Hs),NE=n(Hs,"P",{});var Lke=s(NE);u2o=r(Lke,"This class cannot be instantiated directly using "),Doe=n(Lke,"CODE",{});var pVr=s(Doe);b2o=r(pVr,"__init__()"),pVr.forEach(t),v2o=r(Lke," (throws an error)."),Lke.forEach(t),T2o=i(Hs),Wr=n(Hs,"DIV",{class:!0});var Us=s(Wr);m(qE.$$.fragment,Us),F2o=i(Us),joe=n(Us,"P",{});var _Vr=s(joe);C2o=r(_Vr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),_Vr.forEach(t),M2o=i(Us),rd=n(Us,"P",{});var UV=s(rd);E2o=r(UV,`Note:
Loading a model from its configuration file does `),Noe=n(UV,"STRONG",{});var uVr=s(Noe);y2o=r(uVr,"not"),uVr.forEach(t),w2o=r(UV,` load the model weights. It only affects the
model\u2019s configuration. Use `),qoe=n(UV,"CODE",{});var bVr=s(qoe);A2o=r(bVr,"from_pretrained()"),bVr.forEach(t),L2o=r(UV,"to load the model weights."),UV.forEach(t),B2o=i(Us),Goe=n(Us,"P",{});var vVr=s(Goe);x2o=r(vVr,"Examples:"),vVr.forEach(t),k2o=i(Us),m(GE.$$.fragment,Us),Us.forEach(t),R2o=i(Hs),Ge=n(Hs,"DIV",{class:!0});var Gt=s(Ge);m(OE.$$.fragment,Gt),S2o=i(Gt),Ooe=n(Gt,"P",{});var TVr=s(Ooe);P2o=r(TVr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),TVr.forEach(t),$2o=i(Gt),Xa=n(Gt,"P",{});var DM=s(Xa);I2o=r(DM,"The model class to instantiate is selected based on the "),Xoe=n(DM,"CODE",{});var FVr=s(Xoe);D2o=r(FVr,"model_type"),FVr.forEach(t),j2o=r(DM,` property of the config object (either
passed as an argument or loaded from `),Voe=n(DM,"CODE",{});var CVr=s(Voe);N2o=r(CVr,"pretrained_model_name_or_path"),CVr.forEach(t),q2o=r(DM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zoe=n(DM,"CODE",{});var MVr=s(zoe);G2o=r(MVr,"pretrained_model_name_or_path"),MVr.forEach(t),O2o=r(DM,":"),DM.forEach(t),X2o=i(Gt),ae=n(Gt,"UL",{});var le=s(ae);m0=n(le,"LI",{});var M4e=s(m0);Woe=n(M4e,"STRONG",{});var EVr=s(Woe);V2o=r(EVr,"bart"),EVr.forEach(t),z2o=r(M4e," \u2014 "),y$=n(M4e,"A",{href:!0});var yVr=s(y$);W2o=r(yVr,"BartForConditionalGeneration"),yVr.forEach(t),Q2o=r(M4e," (BART model)"),M4e.forEach(t),H2o=i(le),g0=n(le,"LI",{});var E4e=s(g0);Qoe=n(E4e,"STRONG",{});var wVr=s(Qoe);U2o=r(wVr,"bigbird_pegasus"),wVr.forEach(t),J2o=r(E4e," \u2014 "),w$=n(E4e,"A",{href:!0});var AVr=s(w$);Y2o=r(AVr,"BigBirdPegasusForConditionalGeneration"),AVr.forEach(t),K2o=r(E4e," (BigBirdPegasus model)"),E4e.forEach(t),Z2o=i(le),h0=n(le,"LI",{});var y4e=s(h0);Hoe=n(y4e,"STRONG",{});var LVr=s(Hoe);evo=r(LVr,"blenderbot"),LVr.forEach(t),ovo=r(y4e," \u2014 "),A$=n(y4e,"A",{href:!0});var BVr=s(A$);rvo=r(BVr,"BlenderbotForConditionalGeneration"),BVr.forEach(t),tvo=r(y4e," (Blenderbot model)"),y4e.forEach(t),avo=i(le),p0=n(le,"LI",{});var w4e=s(p0);Uoe=n(w4e,"STRONG",{});var xVr=s(Uoe);nvo=r(xVr,"blenderbot-small"),xVr.forEach(t),svo=r(w4e," \u2014 "),L$=n(w4e,"A",{href:!0});var kVr=s(L$);lvo=r(kVr,"BlenderbotSmallForConditionalGeneration"),kVr.forEach(t),ivo=r(w4e," (BlenderbotSmall model)"),w4e.forEach(t),dvo=i(le),_0=n(le,"LI",{});var A4e=s(_0);Joe=n(A4e,"STRONG",{});var RVr=s(Joe);cvo=r(RVr,"encoder-decoder"),RVr.forEach(t),fvo=r(A4e," \u2014 "),B$=n(A4e,"A",{href:!0});var SVr=s(B$);mvo=r(SVr,"EncoderDecoderModel"),SVr.forEach(t),gvo=r(A4e," (Encoder decoder model)"),A4e.forEach(t),hvo=i(le),u0=n(le,"LI",{});var L4e=s(u0);Yoe=n(L4e,"STRONG",{});var PVr=s(Yoe);pvo=r(PVr,"fsmt"),PVr.forEach(t),_vo=r(L4e," \u2014 "),x$=n(L4e,"A",{href:!0});var $Vr=s(x$);uvo=r($Vr,"FSMTForConditionalGeneration"),$Vr.forEach(t),bvo=r(L4e," (FairSeq Machine-Translation model)"),L4e.forEach(t),vvo=i(le),b0=n(le,"LI",{});var B4e=s(b0);Koe=n(B4e,"STRONG",{});var IVr=s(Koe);Tvo=r(IVr,"led"),IVr.forEach(t),Fvo=r(B4e," \u2014 "),k$=n(B4e,"A",{href:!0});var DVr=s(k$);Cvo=r(DVr,"LEDForConditionalGeneration"),DVr.forEach(t),Mvo=r(B4e," (LED model)"),B4e.forEach(t),Evo=i(le),v0=n(le,"LI",{});var x4e=s(v0);Zoe=n(x4e,"STRONG",{});var jVr=s(Zoe);yvo=r(jVr,"m2m_100"),jVr.forEach(t),wvo=r(x4e," \u2014 "),R$=n(x4e,"A",{href:!0});var NVr=s(R$);Avo=r(NVr,"M2M100ForConditionalGeneration"),NVr.forEach(t),Lvo=r(x4e," (M2M100 model)"),x4e.forEach(t),Bvo=i(le),T0=n(le,"LI",{});var k4e=s(T0);ere=n(k4e,"STRONG",{});var qVr=s(ere);xvo=r(qVr,"marian"),qVr.forEach(t),kvo=r(k4e," \u2014 "),S$=n(k4e,"A",{href:!0});var GVr=s(S$);Rvo=r(GVr,"MarianMTModel"),GVr.forEach(t),Svo=r(k4e," (Marian model)"),k4e.forEach(t),Pvo=i(le),F0=n(le,"LI",{});var R4e=s(F0);ore=n(R4e,"STRONG",{});var OVr=s(ore);$vo=r(OVr,"mbart"),OVr.forEach(t),Ivo=r(R4e," \u2014 "),P$=n(R4e,"A",{href:!0});var XVr=s(P$);Dvo=r(XVr,"MBartForConditionalGeneration"),XVr.forEach(t),jvo=r(R4e," (mBART model)"),R4e.forEach(t),Nvo=i(le),C0=n(le,"LI",{});var S4e=s(C0);rre=n(S4e,"STRONG",{});var VVr=s(rre);qvo=r(VVr,"mt5"),VVr.forEach(t),Gvo=r(S4e," \u2014 "),$$=n(S4e,"A",{href:!0});var zVr=s($$);Ovo=r(zVr,"MT5ForConditionalGeneration"),zVr.forEach(t),Xvo=r(S4e," (mT5 model)"),S4e.forEach(t),Vvo=i(le),M0=n(le,"LI",{});var P4e=s(M0);tre=n(P4e,"STRONG",{});var WVr=s(tre);zvo=r(WVr,"pegasus"),WVr.forEach(t),Wvo=r(P4e," \u2014 "),I$=n(P4e,"A",{href:!0});var QVr=s(I$);Qvo=r(QVr,"PegasusForConditionalGeneration"),QVr.forEach(t),Hvo=r(P4e," (Pegasus model)"),P4e.forEach(t),Uvo=i(le),E0=n(le,"LI",{});var $4e=s(E0);are=n($4e,"STRONG",{});var HVr=s(are);Jvo=r(HVr,"plbart"),HVr.forEach(t),Yvo=r($4e," \u2014 "),D$=n($4e,"A",{href:!0});var UVr=s(D$);Kvo=r(UVr,"PLBartForConditionalGeneration"),UVr.forEach(t),Zvo=r($4e," (PLBart model)"),$4e.forEach(t),eTo=i(le),y0=n(le,"LI",{});var I4e=s(y0);nre=n(I4e,"STRONG",{});var JVr=s(nre);oTo=r(JVr,"prophetnet"),JVr.forEach(t),rTo=r(I4e," \u2014 "),j$=n(I4e,"A",{href:!0});var YVr=s(j$);tTo=r(YVr,"ProphetNetForConditionalGeneration"),YVr.forEach(t),aTo=r(I4e," (ProphetNet model)"),I4e.forEach(t),nTo=i(le),w0=n(le,"LI",{});var D4e=s(w0);sre=n(D4e,"STRONG",{});var KVr=s(sre);sTo=r(KVr,"t5"),KVr.forEach(t),lTo=r(D4e," \u2014 "),N$=n(D4e,"A",{href:!0});var ZVr=s(N$);iTo=r(ZVr,"T5ForConditionalGeneration"),ZVr.forEach(t),dTo=r(D4e," (T5 model)"),D4e.forEach(t),cTo=i(le),A0=n(le,"LI",{});var j4e=s(A0);lre=n(j4e,"STRONG",{});var ezr=s(lre);fTo=r(ezr,"xlm-prophetnet"),ezr.forEach(t),mTo=r(j4e," \u2014 "),q$=n(j4e,"A",{href:!0});var ozr=s(q$);gTo=r(ozr,"XLMProphetNetForConditionalGeneration"),ozr.forEach(t),hTo=r(j4e," (XLMProphetNet model)"),j4e.forEach(t),le.forEach(t),pTo=i(Gt),L0=n(Gt,"P",{});var N4e=s(L0);_To=r(N4e,"The model is set in evaluation mode by default using "),ire=n(N4e,"CODE",{});var rzr=s(ire);uTo=r(rzr,"model.eval()"),rzr.forEach(t),bTo=r(N4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dre=n(N4e,"CODE",{});var tzr=s(dre);vTo=r(tzr,"model.train()"),tzr.forEach(t),N4e.forEach(t),TTo=i(Gt),cre=n(Gt,"P",{});var azr=s(cre);FTo=r(azr,"Examples:"),azr.forEach(t),CTo=i(Gt),m(XE.$$.fragment,Gt),Gt.forEach(t),Hs.forEach(t),CBe=i(c),td=n(c,"H2",{class:!0});var Bke=s(td);B0=n(Bke,"A",{id:!0,class:!0,href:!0});var nzr=s(B0);fre=n(nzr,"SPAN",{});var szr=s(fre);m(VE.$$.fragment,szr),szr.forEach(t),nzr.forEach(t),MTo=i(Bke),mre=n(Bke,"SPAN",{});var lzr=s(mre);ETo=r(lzr,"AutoModelForSequenceClassification"),lzr.forEach(t),Bke.forEach(t),MBe=i(c),er=n(c,"DIV",{class:!0});var Js=s(er);m(zE.$$.fragment,Js),yTo=i(Js),ad=n(Js,"P",{});var JV=s(ad);wTo=r(JV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),gre=n(JV,"CODE",{});var izr=s(gre);ATo=r(izr,"from_pretrained()"),izr.forEach(t),LTo=r(JV,"class method or the "),hre=n(JV,"CODE",{});var dzr=s(hre);BTo=r(dzr,"from_config()"),dzr.forEach(t),xTo=r(JV,`class
method.`),JV.forEach(t),kTo=i(Js),WE=n(Js,"P",{});var xke=s(WE);RTo=r(xke,"This class cannot be instantiated directly using "),pre=n(xke,"CODE",{});var czr=s(pre);STo=r(czr,"__init__()"),czr.forEach(t),PTo=r(xke," (throws an error)."),xke.forEach(t),$To=i(Js),Qr=n(Js,"DIV",{class:!0});var Ys=s(Qr);m(QE.$$.fragment,Ys),ITo=i(Ys),_re=n(Ys,"P",{});var fzr=s(_re);DTo=r(fzr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),fzr.forEach(t),jTo=i(Ys),nd=n(Ys,"P",{});var YV=s(nd);NTo=r(YV,`Note:
Loading a model from its configuration file does `),ure=n(YV,"STRONG",{});var mzr=s(ure);qTo=r(mzr,"not"),mzr.forEach(t),GTo=r(YV,` load the model weights. It only affects the
model\u2019s configuration. Use `),bre=n(YV,"CODE",{});var gzr=s(bre);OTo=r(gzr,"from_pretrained()"),gzr.forEach(t),XTo=r(YV,"to load the model weights."),YV.forEach(t),VTo=i(Ys),vre=n(Ys,"P",{});var hzr=s(vre);zTo=r(hzr,"Examples:"),hzr.forEach(t),WTo=i(Ys),m(HE.$$.fragment,Ys),Ys.forEach(t),QTo=i(Js),Oe=n(Js,"DIV",{class:!0});var Ot=s(Oe);m(UE.$$.fragment,Ot),HTo=i(Ot),Tre=n(Ot,"P",{});var pzr=s(Tre);UTo=r(pzr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),pzr.forEach(t),JTo=i(Ot),Va=n(Ot,"P",{});var jM=s(Va);YTo=r(jM,"The model class to instantiate is selected based on the "),Fre=n(jM,"CODE",{});var _zr=s(Fre);KTo=r(_zr,"model_type"),_zr.forEach(t),ZTo=r(jM,` property of the config object (either
passed as an argument or loaded from `),Cre=n(jM,"CODE",{});var uzr=s(Cre);eFo=r(uzr,"pretrained_model_name_or_path"),uzr.forEach(t),oFo=r(jM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mre=n(jM,"CODE",{});var bzr=s(Mre);rFo=r(bzr,"pretrained_model_name_or_path"),bzr.forEach(t),tFo=r(jM,":"),jM.forEach(t),aFo=i(Ot),A=n(Ot,"UL",{});var L=s(A);x0=n(L,"LI",{});var q4e=s(x0);Ere=n(q4e,"STRONG",{});var vzr=s(Ere);nFo=r(vzr,"albert"),vzr.forEach(t),sFo=r(q4e," \u2014 "),G$=n(q4e,"A",{href:!0});var Tzr=s(G$);lFo=r(Tzr,"AlbertForSequenceClassification"),Tzr.forEach(t),iFo=r(q4e," (ALBERT model)"),q4e.forEach(t),dFo=i(L),k0=n(L,"LI",{});var G4e=s(k0);yre=n(G4e,"STRONG",{});var Fzr=s(yre);cFo=r(Fzr,"bart"),Fzr.forEach(t),fFo=r(G4e," \u2014 "),O$=n(G4e,"A",{href:!0});var Czr=s(O$);mFo=r(Czr,"BartForSequenceClassification"),Czr.forEach(t),gFo=r(G4e," (BART model)"),G4e.forEach(t),hFo=i(L),R0=n(L,"LI",{});var O4e=s(R0);wre=n(O4e,"STRONG",{});var Mzr=s(wre);pFo=r(Mzr,"bert"),Mzr.forEach(t),_Fo=r(O4e," \u2014 "),X$=n(O4e,"A",{href:!0});var Ezr=s(X$);uFo=r(Ezr,"BertForSequenceClassification"),Ezr.forEach(t),bFo=r(O4e," (BERT model)"),O4e.forEach(t),vFo=i(L),S0=n(L,"LI",{});var X4e=s(S0);Are=n(X4e,"STRONG",{});var yzr=s(Are);TFo=r(yzr,"big_bird"),yzr.forEach(t),FFo=r(X4e," \u2014 "),V$=n(X4e,"A",{href:!0});var wzr=s(V$);CFo=r(wzr,"BigBirdForSequenceClassification"),wzr.forEach(t),MFo=r(X4e," (BigBird model)"),X4e.forEach(t),EFo=i(L),P0=n(L,"LI",{});var V4e=s(P0);Lre=n(V4e,"STRONG",{});var Azr=s(Lre);yFo=r(Azr,"bigbird_pegasus"),Azr.forEach(t),wFo=r(V4e," \u2014 "),z$=n(V4e,"A",{href:!0});var Lzr=s(z$);AFo=r(Lzr,"BigBirdPegasusForSequenceClassification"),Lzr.forEach(t),LFo=r(V4e," (BigBirdPegasus model)"),V4e.forEach(t),BFo=i(L),$0=n(L,"LI",{});var z4e=s($0);Bre=n(z4e,"STRONG",{});var Bzr=s(Bre);xFo=r(Bzr,"camembert"),Bzr.forEach(t),kFo=r(z4e," \u2014 "),W$=n(z4e,"A",{href:!0});var xzr=s(W$);RFo=r(xzr,"CamembertForSequenceClassification"),xzr.forEach(t),SFo=r(z4e," (CamemBERT model)"),z4e.forEach(t),PFo=i(L),I0=n(L,"LI",{});var W4e=s(I0);xre=n(W4e,"STRONG",{});var kzr=s(xre);$Fo=r(kzr,"canine"),kzr.forEach(t),IFo=r(W4e," \u2014 "),Q$=n(W4e,"A",{href:!0});var Rzr=s(Q$);DFo=r(Rzr,"CanineForSequenceClassification"),Rzr.forEach(t),jFo=r(W4e," (Canine model)"),W4e.forEach(t),NFo=i(L),D0=n(L,"LI",{});var Q4e=s(D0);kre=n(Q4e,"STRONG",{});var Szr=s(kre);qFo=r(Szr,"convbert"),Szr.forEach(t),GFo=r(Q4e," \u2014 "),H$=n(Q4e,"A",{href:!0});var Pzr=s(H$);OFo=r(Pzr,"ConvBertForSequenceClassification"),Pzr.forEach(t),XFo=r(Q4e," (ConvBERT model)"),Q4e.forEach(t),VFo=i(L),j0=n(L,"LI",{});var H4e=s(j0);Rre=n(H4e,"STRONG",{});var $zr=s(Rre);zFo=r($zr,"ctrl"),$zr.forEach(t),WFo=r(H4e," \u2014 "),U$=n(H4e,"A",{href:!0});var Izr=s(U$);QFo=r(Izr,"CTRLForSequenceClassification"),Izr.forEach(t),HFo=r(H4e," (CTRL model)"),H4e.forEach(t),UFo=i(L),N0=n(L,"LI",{});var U4e=s(N0);Sre=n(U4e,"STRONG",{});var Dzr=s(Sre);JFo=r(Dzr,"data2vec-text"),Dzr.forEach(t),YFo=r(U4e," \u2014 "),J$=n(U4e,"A",{href:!0});var jzr=s(J$);KFo=r(jzr,"Data2VecTextForSequenceClassification"),jzr.forEach(t),ZFo=r(U4e," (Data2VecText model)"),U4e.forEach(t),e9o=i(L),q0=n(L,"LI",{});var J4e=s(q0);Pre=n(J4e,"STRONG",{});var Nzr=s(Pre);o9o=r(Nzr,"deberta"),Nzr.forEach(t),r9o=r(J4e," \u2014 "),Y$=n(J4e,"A",{href:!0});var qzr=s(Y$);t9o=r(qzr,"DebertaForSequenceClassification"),qzr.forEach(t),a9o=r(J4e," (DeBERTa model)"),J4e.forEach(t),n9o=i(L),G0=n(L,"LI",{});var Y4e=s(G0);$re=n(Y4e,"STRONG",{});var Gzr=s($re);s9o=r(Gzr,"deberta-v2"),Gzr.forEach(t),l9o=r(Y4e," \u2014 "),K$=n(Y4e,"A",{href:!0});var Ozr=s(K$);i9o=r(Ozr,"DebertaV2ForSequenceClassification"),Ozr.forEach(t),d9o=r(Y4e," (DeBERTa-v2 model)"),Y4e.forEach(t),c9o=i(L),O0=n(L,"LI",{});var K4e=s(O0);Ire=n(K4e,"STRONG",{});var Xzr=s(Ire);f9o=r(Xzr,"distilbert"),Xzr.forEach(t),m9o=r(K4e," \u2014 "),Z$=n(K4e,"A",{href:!0});var Vzr=s(Z$);g9o=r(Vzr,"DistilBertForSequenceClassification"),Vzr.forEach(t),h9o=r(K4e," (DistilBERT model)"),K4e.forEach(t),p9o=i(L),X0=n(L,"LI",{});var Z4e=s(X0);Dre=n(Z4e,"STRONG",{});var zzr=s(Dre);_9o=r(zzr,"electra"),zzr.forEach(t),u9o=r(Z4e," \u2014 "),eI=n(Z4e,"A",{href:!0});var Wzr=s(eI);b9o=r(Wzr,"ElectraForSequenceClassification"),Wzr.forEach(t),v9o=r(Z4e," (ELECTRA model)"),Z4e.forEach(t),T9o=i(L),V0=n(L,"LI",{});var eEe=s(V0);jre=n(eEe,"STRONG",{});var Qzr=s(jre);F9o=r(Qzr,"flaubert"),Qzr.forEach(t),C9o=r(eEe," \u2014 "),oI=n(eEe,"A",{href:!0});var Hzr=s(oI);M9o=r(Hzr,"FlaubertForSequenceClassification"),Hzr.forEach(t),E9o=r(eEe," (FlauBERT model)"),eEe.forEach(t),y9o=i(L),z0=n(L,"LI",{});var oEe=s(z0);Nre=n(oEe,"STRONG",{});var Uzr=s(Nre);w9o=r(Uzr,"fnet"),Uzr.forEach(t),A9o=r(oEe," \u2014 "),rI=n(oEe,"A",{href:!0});var Jzr=s(rI);L9o=r(Jzr,"FNetForSequenceClassification"),Jzr.forEach(t),B9o=r(oEe," (FNet model)"),oEe.forEach(t),x9o=i(L),W0=n(L,"LI",{});var rEe=s(W0);qre=n(rEe,"STRONG",{});var Yzr=s(qre);k9o=r(Yzr,"funnel"),Yzr.forEach(t),R9o=r(rEe," \u2014 "),tI=n(rEe,"A",{href:!0});var Kzr=s(tI);S9o=r(Kzr,"FunnelForSequenceClassification"),Kzr.forEach(t),P9o=r(rEe," (Funnel Transformer model)"),rEe.forEach(t),$9o=i(L),Q0=n(L,"LI",{});var tEe=s(Q0);Gre=n(tEe,"STRONG",{});var Zzr=s(Gre);I9o=r(Zzr,"gpt2"),Zzr.forEach(t),D9o=r(tEe," \u2014 "),aI=n(tEe,"A",{href:!0});var eWr=s(aI);j9o=r(eWr,"GPT2ForSequenceClassification"),eWr.forEach(t),N9o=r(tEe," (OpenAI GPT-2 model)"),tEe.forEach(t),q9o=i(L),H0=n(L,"LI",{});var aEe=s(H0);Ore=n(aEe,"STRONG",{});var oWr=s(Ore);G9o=r(oWr,"gpt_neo"),oWr.forEach(t),O9o=r(aEe," \u2014 "),nI=n(aEe,"A",{href:!0});var rWr=s(nI);X9o=r(rWr,"GPTNeoForSequenceClassification"),rWr.forEach(t),V9o=r(aEe," (GPT Neo model)"),aEe.forEach(t),z9o=i(L),U0=n(L,"LI",{});var nEe=s(U0);Xre=n(nEe,"STRONG",{});var tWr=s(Xre);W9o=r(tWr,"gptj"),tWr.forEach(t),Q9o=r(nEe," \u2014 "),sI=n(nEe,"A",{href:!0});var aWr=s(sI);H9o=r(aWr,"GPTJForSequenceClassification"),aWr.forEach(t),U9o=r(nEe," (GPT-J model)"),nEe.forEach(t),J9o=i(L),J0=n(L,"LI",{});var sEe=s(J0);Vre=n(sEe,"STRONG",{});var nWr=s(Vre);Y9o=r(nWr,"ibert"),nWr.forEach(t),K9o=r(sEe," \u2014 "),lI=n(sEe,"A",{href:!0});var sWr=s(lI);Z9o=r(sWr,"IBertForSequenceClassification"),sWr.forEach(t),eCo=r(sEe," (I-BERT model)"),sEe.forEach(t),oCo=i(L),Y0=n(L,"LI",{});var lEe=s(Y0);zre=n(lEe,"STRONG",{});var lWr=s(zre);rCo=r(lWr,"layoutlm"),lWr.forEach(t),tCo=r(lEe," \u2014 "),iI=n(lEe,"A",{href:!0});var iWr=s(iI);aCo=r(iWr,"LayoutLMForSequenceClassification"),iWr.forEach(t),nCo=r(lEe," (LayoutLM model)"),lEe.forEach(t),sCo=i(L),K0=n(L,"LI",{});var iEe=s(K0);Wre=n(iEe,"STRONG",{});var dWr=s(Wre);lCo=r(dWr,"layoutlmv2"),dWr.forEach(t),iCo=r(iEe," \u2014 "),dI=n(iEe,"A",{href:!0});var cWr=s(dI);dCo=r(cWr,"LayoutLMv2ForSequenceClassification"),cWr.forEach(t),cCo=r(iEe," (LayoutLMv2 model)"),iEe.forEach(t),fCo=i(L),Z0=n(L,"LI",{});var dEe=s(Z0);Qre=n(dEe,"STRONG",{});var fWr=s(Qre);mCo=r(fWr,"led"),fWr.forEach(t),gCo=r(dEe," \u2014 "),cI=n(dEe,"A",{href:!0});var mWr=s(cI);hCo=r(mWr,"LEDForSequenceClassification"),mWr.forEach(t),pCo=r(dEe," (LED model)"),dEe.forEach(t),_Co=i(L),e1=n(L,"LI",{});var cEe=s(e1);Hre=n(cEe,"STRONG",{});var gWr=s(Hre);uCo=r(gWr,"longformer"),gWr.forEach(t),bCo=r(cEe," \u2014 "),fI=n(cEe,"A",{href:!0});var hWr=s(fI);vCo=r(hWr,"LongformerForSequenceClassification"),hWr.forEach(t),TCo=r(cEe," (Longformer model)"),cEe.forEach(t),FCo=i(L),o1=n(L,"LI",{});var fEe=s(o1);Ure=n(fEe,"STRONG",{});var pWr=s(Ure);CCo=r(pWr,"mbart"),pWr.forEach(t),MCo=r(fEe," \u2014 "),mI=n(fEe,"A",{href:!0});var _Wr=s(mI);ECo=r(_Wr,"MBartForSequenceClassification"),_Wr.forEach(t),yCo=r(fEe," (mBART model)"),fEe.forEach(t),wCo=i(L),r1=n(L,"LI",{});var mEe=s(r1);Jre=n(mEe,"STRONG",{});var uWr=s(Jre);ACo=r(uWr,"megatron-bert"),uWr.forEach(t),LCo=r(mEe," \u2014 "),gI=n(mEe,"A",{href:!0});var bWr=s(gI);BCo=r(bWr,"MegatronBertForSequenceClassification"),bWr.forEach(t),xCo=r(mEe," (MegatronBert model)"),mEe.forEach(t),kCo=i(L),t1=n(L,"LI",{});var gEe=s(t1);Yre=n(gEe,"STRONG",{});var vWr=s(Yre);RCo=r(vWr,"mobilebert"),vWr.forEach(t),SCo=r(gEe," \u2014 "),hI=n(gEe,"A",{href:!0});var TWr=s(hI);PCo=r(TWr,"MobileBertForSequenceClassification"),TWr.forEach(t),$Co=r(gEe," (MobileBERT model)"),gEe.forEach(t),ICo=i(L),a1=n(L,"LI",{});var hEe=s(a1);Kre=n(hEe,"STRONG",{});var FWr=s(Kre);DCo=r(FWr,"mpnet"),FWr.forEach(t),jCo=r(hEe," \u2014 "),pI=n(hEe,"A",{href:!0});var CWr=s(pI);NCo=r(CWr,"MPNetForSequenceClassification"),CWr.forEach(t),qCo=r(hEe," (MPNet model)"),hEe.forEach(t),GCo=i(L),n1=n(L,"LI",{});var pEe=s(n1);Zre=n(pEe,"STRONG",{});var MWr=s(Zre);OCo=r(MWr,"nystromformer"),MWr.forEach(t),XCo=r(pEe," \u2014 "),_I=n(pEe,"A",{href:!0});var EWr=s(_I);VCo=r(EWr,"NystromformerForSequenceClassification"),EWr.forEach(t),zCo=r(pEe," (Nystromformer model)"),pEe.forEach(t),WCo=i(L),s1=n(L,"LI",{});var _Ee=s(s1);ete=n(_Ee,"STRONG",{});var yWr=s(ete);QCo=r(yWr,"openai-gpt"),yWr.forEach(t),HCo=r(_Ee," \u2014 "),uI=n(_Ee,"A",{href:!0});var wWr=s(uI);UCo=r(wWr,"OpenAIGPTForSequenceClassification"),wWr.forEach(t),JCo=r(_Ee," (OpenAI GPT model)"),_Ee.forEach(t),YCo=i(L),l1=n(L,"LI",{});var uEe=s(l1);ote=n(uEe,"STRONG",{});var AWr=s(ote);KCo=r(AWr,"perceiver"),AWr.forEach(t),ZCo=r(uEe," \u2014 "),bI=n(uEe,"A",{href:!0});var LWr=s(bI);eMo=r(LWr,"PerceiverForSequenceClassification"),LWr.forEach(t),oMo=r(uEe," (Perceiver model)"),uEe.forEach(t),rMo=i(L),i1=n(L,"LI",{});var bEe=s(i1);rte=n(bEe,"STRONG",{});var BWr=s(rte);tMo=r(BWr,"plbart"),BWr.forEach(t),aMo=r(bEe," \u2014 "),vI=n(bEe,"A",{href:!0});var xWr=s(vI);nMo=r(xWr,"PLBartForSequenceClassification"),xWr.forEach(t),sMo=r(bEe," (PLBart model)"),bEe.forEach(t),lMo=i(L),d1=n(L,"LI",{});var vEe=s(d1);tte=n(vEe,"STRONG",{});var kWr=s(tte);iMo=r(kWr,"qdqbert"),kWr.forEach(t),dMo=r(vEe," \u2014 "),TI=n(vEe,"A",{href:!0});var RWr=s(TI);cMo=r(RWr,"QDQBertForSequenceClassification"),RWr.forEach(t),fMo=r(vEe," (QDQBert model)"),vEe.forEach(t),mMo=i(L),c1=n(L,"LI",{});var TEe=s(c1);ate=n(TEe,"STRONG",{});var SWr=s(ate);gMo=r(SWr,"reformer"),SWr.forEach(t),hMo=r(TEe," \u2014 "),FI=n(TEe,"A",{href:!0});var PWr=s(FI);pMo=r(PWr,"ReformerForSequenceClassification"),PWr.forEach(t),_Mo=r(TEe," (Reformer model)"),TEe.forEach(t),uMo=i(L),f1=n(L,"LI",{});var FEe=s(f1);nte=n(FEe,"STRONG",{});var $Wr=s(nte);bMo=r($Wr,"rembert"),$Wr.forEach(t),vMo=r(FEe," \u2014 "),CI=n(FEe,"A",{href:!0});var IWr=s(CI);TMo=r(IWr,"RemBertForSequenceClassification"),IWr.forEach(t),FMo=r(FEe," (RemBERT model)"),FEe.forEach(t),CMo=i(L),m1=n(L,"LI",{});var CEe=s(m1);ste=n(CEe,"STRONG",{});var DWr=s(ste);MMo=r(DWr,"roberta"),DWr.forEach(t),EMo=r(CEe," \u2014 "),MI=n(CEe,"A",{href:!0});var jWr=s(MI);yMo=r(jWr,"RobertaForSequenceClassification"),jWr.forEach(t),wMo=r(CEe," (RoBERTa model)"),CEe.forEach(t),AMo=i(L),g1=n(L,"LI",{});var MEe=s(g1);lte=n(MEe,"STRONG",{});var NWr=s(lte);LMo=r(NWr,"roformer"),NWr.forEach(t),BMo=r(MEe," \u2014 "),EI=n(MEe,"A",{href:!0});var qWr=s(EI);xMo=r(qWr,"RoFormerForSequenceClassification"),qWr.forEach(t),kMo=r(MEe," (RoFormer model)"),MEe.forEach(t),RMo=i(L),h1=n(L,"LI",{});var EEe=s(h1);ite=n(EEe,"STRONG",{});var GWr=s(ite);SMo=r(GWr,"squeezebert"),GWr.forEach(t),PMo=r(EEe," \u2014 "),yI=n(EEe,"A",{href:!0});var OWr=s(yI);$Mo=r(OWr,"SqueezeBertForSequenceClassification"),OWr.forEach(t),IMo=r(EEe," (SqueezeBERT model)"),EEe.forEach(t),DMo=i(L),p1=n(L,"LI",{});var yEe=s(p1);dte=n(yEe,"STRONG",{});var XWr=s(dte);jMo=r(XWr,"tapas"),XWr.forEach(t),NMo=r(yEe," \u2014 "),wI=n(yEe,"A",{href:!0});var VWr=s(wI);qMo=r(VWr,"TapasForSequenceClassification"),VWr.forEach(t),GMo=r(yEe," (TAPAS model)"),yEe.forEach(t),OMo=i(L),_1=n(L,"LI",{});var wEe=s(_1);cte=n(wEe,"STRONG",{});var zWr=s(cte);XMo=r(zWr,"transfo-xl"),zWr.forEach(t),VMo=r(wEe," \u2014 "),AI=n(wEe,"A",{href:!0});var WWr=s(AI);zMo=r(WWr,"TransfoXLForSequenceClassification"),WWr.forEach(t),WMo=r(wEe," (Transformer-XL model)"),wEe.forEach(t),QMo=i(L),u1=n(L,"LI",{});var AEe=s(u1);fte=n(AEe,"STRONG",{});var QWr=s(fte);HMo=r(QWr,"xlm"),QWr.forEach(t),UMo=r(AEe," \u2014 "),LI=n(AEe,"A",{href:!0});var HWr=s(LI);JMo=r(HWr,"XLMForSequenceClassification"),HWr.forEach(t),YMo=r(AEe," (XLM model)"),AEe.forEach(t),KMo=i(L),b1=n(L,"LI",{});var LEe=s(b1);mte=n(LEe,"STRONG",{});var UWr=s(mte);ZMo=r(UWr,"xlm-roberta"),UWr.forEach(t),e4o=r(LEe," \u2014 "),BI=n(LEe,"A",{href:!0});var JWr=s(BI);o4o=r(JWr,"XLMRobertaForSequenceClassification"),JWr.forEach(t),r4o=r(LEe," (XLM-RoBERTa model)"),LEe.forEach(t),t4o=i(L),v1=n(L,"LI",{});var BEe=s(v1);gte=n(BEe,"STRONG",{});var YWr=s(gte);a4o=r(YWr,"xlm-roberta-xl"),YWr.forEach(t),n4o=r(BEe," \u2014 "),xI=n(BEe,"A",{href:!0});var KWr=s(xI);s4o=r(KWr,"XLMRobertaXLForSequenceClassification"),KWr.forEach(t),l4o=r(BEe," (XLM-RoBERTa-XL model)"),BEe.forEach(t),i4o=i(L),T1=n(L,"LI",{});var xEe=s(T1);hte=n(xEe,"STRONG",{});var ZWr=s(hte);d4o=r(ZWr,"xlnet"),ZWr.forEach(t),c4o=r(xEe," \u2014 "),kI=n(xEe,"A",{href:!0});var eQr=s(kI);f4o=r(eQr,"XLNetForSequenceClassification"),eQr.forEach(t),m4o=r(xEe," (XLNet model)"),xEe.forEach(t),g4o=i(L),F1=n(L,"LI",{});var kEe=s(F1);pte=n(kEe,"STRONG",{});var oQr=s(pte);h4o=r(oQr,"yoso"),oQr.forEach(t),p4o=r(kEe," \u2014 "),RI=n(kEe,"A",{href:!0});var rQr=s(RI);_4o=r(rQr,"YosoForSequenceClassification"),rQr.forEach(t),u4o=r(kEe," (YOSO model)"),kEe.forEach(t),L.forEach(t),b4o=i(Ot),C1=n(Ot,"P",{});var REe=s(C1);v4o=r(REe,"The model is set in evaluation mode by default using "),_te=n(REe,"CODE",{});var tQr=s(_te);T4o=r(tQr,"model.eval()"),tQr.forEach(t),F4o=r(REe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ute=n(REe,"CODE",{});var aQr=s(ute);C4o=r(aQr,"model.train()"),aQr.forEach(t),REe.forEach(t),M4o=i(Ot),bte=n(Ot,"P",{});var nQr=s(bte);E4o=r(nQr,"Examples:"),nQr.forEach(t),y4o=i(Ot),m(JE.$$.fragment,Ot),Ot.forEach(t),Js.forEach(t),EBe=i(c),sd=n(c,"H2",{class:!0});var kke=s(sd);M1=n(kke,"A",{id:!0,class:!0,href:!0});var sQr=s(M1);vte=n(sQr,"SPAN",{});var lQr=s(vte);m(YE.$$.fragment,lQr),lQr.forEach(t),sQr.forEach(t),w4o=i(kke),Tte=n(kke,"SPAN",{});var iQr=s(Tte);A4o=r(iQr,"AutoModelForMultipleChoice"),iQr.forEach(t),kke.forEach(t),yBe=i(c),or=n(c,"DIV",{class:!0});var Ks=s(or);m(KE.$$.fragment,Ks),L4o=i(Ks),ld=n(Ks,"P",{});var KV=s(ld);B4o=r(KV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Fte=n(KV,"CODE",{});var dQr=s(Fte);x4o=r(dQr,"from_pretrained()"),dQr.forEach(t),k4o=r(KV,"class method or the "),Cte=n(KV,"CODE",{});var cQr=s(Cte);R4o=r(cQr,"from_config()"),cQr.forEach(t),S4o=r(KV,`class
method.`),KV.forEach(t),P4o=i(Ks),ZE=n(Ks,"P",{});var Rke=s(ZE);$4o=r(Rke,"This class cannot be instantiated directly using "),Mte=n(Rke,"CODE",{});var fQr=s(Mte);I4o=r(fQr,"__init__()"),fQr.forEach(t),D4o=r(Rke," (throws an error)."),Rke.forEach(t),j4o=i(Ks),Hr=n(Ks,"DIV",{class:!0});var Zs=s(Hr);m(e3.$$.fragment,Zs),N4o=i(Zs),Ete=n(Zs,"P",{});var mQr=s(Ete);q4o=r(mQr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),mQr.forEach(t),G4o=i(Zs),id=n(Zs,"P",{});var ZV=s(id);O4o=r(ZV,`Note:
Loading a model from its configuration file does `),yte=n(ZV,"STRONG",{});var gQr=s(yte);X4o=r(gQr,"not"),gQr.forEach(t),V4o=r(ZV,` load the model weights. It only affects the
model\u2019s configuration. Use `),wte=n(ZV,"CODE",{});var hQr=s(wte);z4o=r(hQr,"from_pretrained()"),hQr.forEach(t),W4o=r(ZV,"to load the model weights."),ZV.forEach(t),Q4o=i(Zs),Ate=n(Zs,"P",{});var pQr=s(Ate);H4o=r(pQr,"Examples:"),pQr.forEach(t),U4o=i(Zs),m(o3.$$.fragment,Zs),Zs.forEach(t),J4o=i(Ks),Xe=n(Ks,"DIV",{class:!0});var Xt=s(Xe);m(r3.$$.fragment,Xt),Y4o=i(Xt),Lte=n(Xt,"P",{});var _Qr=s(Lte);K4o=r(_Qr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),_Qr.forEach(t),Z4o=i(Xt),za=n(Xt,"P",{});var NM=s(za);eEo=r(NM,"The model class to instantiate is selected based on the "),Bte=n(NM,"CODE",{});var uQr=s(Bte);oEo=r(uQr,"model_type"),uQr.forEach(t),rEo=r(NM,` property of the config object (either
passed as an argument or loaded from `),xte=n(NM,"CODE",{});var bQr=s(xte);tEo=r(bQr,"pretrained_model_name_or_path"),bQr.forEach(t),aEo=r(NM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kte=n(NM,"CODE",{});var vQr=s(kte);nEo=r(vQr,"pretrained_model_name_or_path"),vQr.forEach(t),sEo=r(NM,":"),NM.forEach(t),lEo=i(Xt),G=n(Xt,"UL",{});var O=s(G);E1=n(O,"LI",{});var SEe=s(E1);Rte=n(SEe,"STRONG",{});var TQr=s(Rte);iEo=r(TQr,"albert"),TQr.forEach(t),dEo=r(SEe," \u2014 "),SI=n(SEe,"A",{href:!0});var FQr=s(SI);cEo=r(FQr,"AlbertForMultipleChoice"),FQr.forEach(t),fEo=r(SEe," (ALBERT model)"),SEe.forEach(t),mEo=i(O),y1=n(O,"LI",{});var PEe=s(y1);Ste=n(PEe,"STRONG",{});var CQr=s(Ste);gEo=r(CQr,"bert"),CQr.forEach(t),hEo=r(PEe," \u2014 "),PI=n(PEe,"A",{href:!0});var MQr=s(PI);pEo=r(MQr,"BertForMultipleChoice"),MQr.forEach(t),_Eo=r(PEe," (BERT model)"),PEe.forEach(t),uEo=i(O),w1=n(O,"LI",{});var $Ee=s(w1);Pte=n($Ee,"STRONG",{});var EQr=s(Pte);bEo=r(EQr,"big_bird"),EQr.forEach(t),vEo=r($Ee," \u2014 "),$I=n($Ee,"A",{href:!0});var yQr=s($I);TEo=r(yQr,"BigBirdForMultipleChoice"),yQr.forEach(t),FEo=r($Ee," (BigBird model)"),$Ee.forEach(t),CEo=i(O),A1=n(O,"LI",{});var IEe=s(A1);$te=n(IEe,"STRONG",{});var wQr=s($te);MEo=r(wQr,"camembert"),wQr.forEach(t),EEo=r(IEe," \u2014 "),II=n(IEe,"A",{href:!0});var AQr=s(II);yEo=r(AQr,"CamembertForMultipleChoice"),AQr.forEach(t),wEo=r(IEe," (CamemBERT model)"),IEe.forEach(t),AEo=i(O),L1=n(O,"LI",{});var DEe=s(L1);Ite=n(DEe,"STRONG",{});var LQr=s(Ite);LEo=r(LQr,"canine"),LQr.forEach(t),BEo=r(DEe," \u2014 "),DI=n(DEe,"A",{href:!0});var BQr=s(DI);xEo=r(BQr,"CanineForMultipleChoice"),BQr.forEach(t),kEo=r(DEe," (Canine model)"),DEe.forEach(t),REo=i(O),B1=n(O,"LI",{});var jEe=s(B1);Dte=n(jEe,"STRONG",{});var xQr=s(Dte);SEo=r(xQr,"convbert"),xQr.forEach(t),PEo=r(jEe," \u2014 "),jI=n(jEe,"A",{href:!0});var kQr=s(jI);$Eo=r(kQr,"ConvBertForMultipleChoice"),kQr.forEach(t),IEo=r(jEe," (ConvBERT model)"),jEe.forEach(t),DEo=i(O),x1=n(O,"LI",{});var NEe=s(x1);jte=n(NEe,"STRONG",{});var RQr=s(jte);jEo=r(RQr,"data2vec-text"),RQr.forEach(t),NEo=r(NEe," \u2014 "),NI=n(NEe,"A",{href:!0});var SQr=s(NI);qEo=r(SQr,"Data2VecTextForMultipleChoice"),SQr.forEach(t),GEo=r(NEe," (Data2VecText model)"),NEe.forEach(t),OEo=i(O),k1=n(O,"LI",{});var qEe=s(k1);Nte=n(qEe,"STRONG",{});var PQr=s(Nte);XEo=r(PQr,"distilbert"),PQr.forEach(t),VEo=r(qEe," \u2014 "),qI=n(qEe,"A",{href:!0});var $Qr=s(qI);zEo=r($Qr,"DistilBertForMultipleChoice"),$Qr.forEach(t),WEo=r(qEe," (DistilBERT model)"),qEe.forEach(t),QEo=i(O),R1=n(O,"LI",{});var GEe=s(R1);qte=n(GEe,"STRONG",{});var IQr=s(qte);HEo=r(IQr,"electra"),IQr.forEach(t),UEo=r(GEe," \u2014 "),GI=n(GEe,"A",{href:!0});var DQr=s(GI);JEo=r(DQr,"ElectraForMultipleChoice"),DQr.forEach(t),YEo=r(GEe," (ELECTRA model)"),GEe.forEach(t),KEo=i(O),S1=n(O,"LI",{});var OEe=s(S1);Gte=n(OEe,"STRONG",{});var jQr=s(Gte);ZEo=r(jQr,"flaubert"),jQr.forEach(t),e3o=r(OEe," \u2014 "),OI=n(OEe,"A",{href:!0});var NQr=s(OI);o3o=r(NQr,"FlaubertForMultipleChoice"),NQr.forEach(t),r3o=r(OEe," (FlauBERT model)"),OEe.forEach(t),t3o=i(O),P1=n(O,"LI",{});var XEe=s(P1);Ote=n(XEe,"STRONG",{});var qQr=s(Ote);a3o=r(qQr,"fnet"),qQr.forEach(t),n3o=r(XEe," \u2014 "),XI=n(XEe,"A",{href:!0});var GQr=s(XI);s3o=r(GQr,"FNetForMultipleChoice"),GQr.forEach(t),l3o=r(XEe," (FNet model)"),XEe.forEach(t),i3o=i(O),$1=n(O,"LI",{});var VEe=s($1);Xte=n(VEe,"STRONG",{});var OQr=s(Xte);d3o=r(OQr,"funnel"),OQr.forEach(t),c3o=r(VEe," \u2014 "),VI=n(VEe,"A",{href:!0});var XQr=s(VI);f3o=r(XQr,"FunnelForMultipleChoice"),XQr.forEach(t),m3o=r(VEe," (Funnel Transformer model)"),VEe.forEach(t),g3o=i(O),I1=n(O,"LI",{});var zEe=s(I1);Vte=n(zEe,"STRONG",{});var VQr=s(Vte);h3o=r(VQr,"ibert"),VQr.forEach(t),p3o=r(zEe," \u2014 "),zI=n(zEe,"A",{href:!0});var zQr=s(zI);_3o=r(zQr,"IBertForMultipleChoice"),zQr.forEach(t),u3o=r(zEe," (I-BERT model)"),zEe.forEach(t),b3o=i(O),D1=n(O,"LI",{});var WEe=s(D1);zte=n(WEe,"STRONG",{});var WQr=s(zte);v3o=r(WQr,"longformer"),WQr.forEach(t),T3o=r(WEe," \u2014 "),WI=n(WEe,"A",{href:!0});var QQr=s(WI);F3o=r(QQr,"LongformerForMultipleChoice"),QQr.forEach(t),C3o=r(WEe," (Longformer model)"),WEe.forEach(t),M3o=i(O),j1=n(O,"LI",{});var QEe=s(j1);Wte=n(QEe,"STRONG",{});var HQr=s(Wte);E3o=r(HQr,"megatron-bert"),HQr.forEach(t),y3o=r(QEe," \u2014 "),QI=n(QEe,"A",{href:!0});var UQr=s(QI);w3o=r(UQr,"MegatronBertForMultipleChoice"),UQr.forEach(t),A3o=r(QEe," (MegatronBert model)"),QEe.forEach(t),L3o=i(O),N1=n(O,"LI",{});var HEe=s(N1);Qte=n(HEe,"STRONG",{});var JQr=s(Qte);B3o=r(JQr,"mobilebert"),JQr.forEach(t),x3o=r(HEe," \u2014 "),HI=n(HEe,"A",{href:!0});var YQr=s(HI);k3o=r(YQr,"MobileBertForMultipleChoice"),YQr.forEach(t),R3o=r(HEe," (MobileBERT model)"),HEe.forEach(t),S3o=i(O),q1=n(O,"LI",{});var UEe=s(q1);Hte=n(UEe,"STRONG",{});var KQr=s(Hte);P3o=r(KQr,"mpnet"),KQr.forEach(t),$3o=r(UEe," \u2014 "),UI=n(UEe,"A",{href:!0});var ZQr=s(UI);I3o=r(ZQr,"MPNetForMultipleChoice"),ZQr.forEach(t),D3o=r(UEe," (MPNet model)"),UEe.forEach(t),j3o=i(O),G1=n(O,"LI",{});var JEe=s(G1);Ute=n(JEe,"STRONG",{});var eHr=s(Ute);N3o=r(eHr,"nystromformer"),eHr.forEach(t),q3o=r(JEe," \u2014 "),JI=n(JEe,"A",{href:!0});var oHr=s(JI);G3o=r(oHr,"NystromformerForMultipleChoice"),oHr.forEach(t),O3o=r(JEe," (Nystromformer model)"),JEe.forEach(t),X3o=i(O),O1=n(O,"LI",{});var YEe=s(O1);Jte=n(YEe,"STRONG",{});var rHr=s(Jte);V3o=r(rHr,"qdqbert"),rHr.forEach(t),z3o=r(YEe," \u2014 "),YI=n(YEe,"A",{href:!0});var tHr=s(YI);W3o=r(tHr,"QDQBertForMultipleChoice"),tHr.forEach(t),Q3o=r(YEe," (QDQBert model)"),YEe.forEach(t),H3o=i(O),X1=n(O,"LI",{});var KEe=s(X1);Yte=n(KEe,"STRONG",{});var aHr=s(Yte);U3o=r(aHr,"rembert"),aHr.forEach(t),J3o=r(KEe," \u2014 "),KI=n(KEe,"A",{href:!0});var nHr=s(KI);Y3o=r(nHr,"RemBertForMultipleChoice"),nHr.forEach(t),K3o=r(KEe," (RemBERT model)"),KEe.forEach(t),Z3o=i(O),V1=n(O,"LI",{});var ZEe=s(V1);Kte=n(ZEe,"STRONG",{});var sHr=s(Kte);eyo=r(sHr,"roberta"),sHr.forEach(t),oyo=r(ZEe," \u2014 "),ZI=n(ZEe,"A",{href:!0});var lHr=s(ZI);ryo=r(lHr,"RobertaForMultipleChoice"),lHr.forEach(t),tyo=r(ZEe," (RoBERTa model)"),ZEe.forEach(t),ayo=i(O),z1=n(O,"LI",{});var e3e=s(z1);Zte=n(e3e,"STRONG",{});var iHr=s(Zte);nyo=r(iHr,"roformer"),iHr.forEach(t),syo=r(e3e," \u2014 "),eD=n(e3e,"A",{href:!0});var dHr=s(eD);lyo=r(dHr,"RoFormerForMultipleChoice"),dHr.forEach(t),iyo=r(e3e," (RoFormer model)"),e3e.forEach(t),dyo=i(O),W1=n(O,"LI",{});var o3e=s(W1);eae=n(o3e,"STRONG",{});var cHr=s(eae);cyo=r(cHr,"squeezebert"),cHr.forEach(t),fyo=r(o3e," \u2014 "),oD=n(o3e,"A",{href:!0});var fHr=s(oD);myo=r(fHr,"SqueezeBertForMultipleChoice"),fHr.forEach(t),gyo=r(o3e," (SqueezeBERT model)"),o3e.forEach(t),hyo=i(O),Q1=n(O,"LI",{});var r3e=s(Q1);oae=n(r3e,"STRONG",{});var mHr=s(oae);pyo=r(mHr,"xlm"),mHr.forEach(t),_yo=r(r3e," \u2014 "),rD=n(r3e,"A",{href:!0});var gHr=s(rD);uyo=r(gHr,"XLMForMultipleChoice"),gHr.forEach(t),byo=r(r3e," (XLM model)"),r3e.forEach(t),vyo=i(O),H1=n(O,"LI",{});var t3e=s(H1);rae=n(t3e,"STRONG",{});var hHr=s(rae);Tyo=r(hHr,"xlm-roberta"),hHr.forEach(t),Fyo=r(t3e," \u2014 "),tD=n(t3e,"A",{href:!0});var pHr=s(tD);Cyo=r(pHr,"XLMRobertaForMultipleChoice"),pHr.forEach(t),Myo=r(t3e," (XLM-RoBERTa model)"),t3e.forEach(t),Eyo=i(O),U1=n(O,"LI",{});var a3e=s(U1);tae=n(a3e,"STRONG",{});var _Hr=s(tae);yyo=r(_Hr,"xlm-roberta-xl"),_Hr.forEach(t),wyo=r(a3e," \u2014 "),aD=n(a3e,"A",{href:!0});var uHr=s(aD);Ayo=r(uHr,"XLMRobertaXLForMultipleChoice"),uHr.forEach(t),Lyo=r(a3e," (XLM-RoBERTa-XL model)"),a3e.forEach(t),Byo=i(O),J1=n(O,"LI",{});var n3e=s(J1);aae=n(n3e,"STRONG",{});var bHr=s(aae);xyo=r(bHr,"xlnet"),bHr.forEach(t),kyo=r(n3e," \u2014 "),nD=n(n3e,"A",{href:!0});var vHr=s(nD);Ryo=r(vHr,"XLNetForMultipleChoice"),vHr.forEach(t),Syo=r(n3e," (XLNet model)"),n3e.forEach(t),Pyo=i(O),Y1=n(O,"LI",{});var s3e=s(Y1);nae=n(s3e,"STRONG",{});var THr=s(nae);$yo=r(THr,"yoso"),THr.forEach(t),Iyo=r(s3e," \u2014 "),sD=n(s3e,"A",{href:!0});var FHr=s(sD);Dyo=r(FHr,"YosoForMultipleChoice"),FHr.forEach(t),jyo=r(s3e," (YOSO model)"),s3e.forEach(t),O.forEach(t),Nyo=i(Xt),K1=n(Xt,"P",{});var l3e=s(K1);qyo=r(l3e,"The model is set in evaluation mode by default using "),sae=n(l3e,"CODE",{});var CHr=s(sae);Gyo=r(CHr,"model.eval()"),CHr.forEach(t),Oyo=r(l3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lae=n(l3e,"CODE",{});var MHr=s(lae);Xyo=r(MHr,"model.train()"),MHr.forEach(t),l3e.forEach(t),Vyo=i(Xt),iae=n(Xt,"P",{});var EHr=s(iae);zyo=r(EHr,"Examples:"),EHr.forEach(t),Wyo=i(Xt),m(t3.$$.fragment,Xt),Xt.forEach(t),Ks.forEach(t),wBe=i(c),dd=n(c,"H2",{class:!0});var Ske=s(dd);Z1=n(Ske,"A",{id:!0,class:!0,href:!0});var yHr=s(Z1);dae=n(yHr,"SPAN",{});var wHr=s(dae);m(a3.$$.fragment,wHr),wHr.forEach(t),yHr.forEach(t),Qyo=i(Ske),cae=n(Ske,"SPAN",{});var AHr=s(cae);Hyo=r(AHr,"AutoModelForNextSentencePrediction"),AHr.forEach(t),Ske.forEach(t),ABe=i(c),rr=n(c,"DIV",{class:!0});var el=s(rr);m(n3.$$.fragment,el),Uyo=i(el),cd=n(el,"P",{});var ez=s(cd);Jyo=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),fae=n(ez,"CODE",{});var LHr=s(fae);Yyo=r(LHr,"from_pretrained()"),LHr.forEach(t),Kyo=r(ez,"class method or the "),mae=n(ez,"CODE",{});var BHr=s(mae);Zyo=r(BHr,"from_config()"),BHr.forEach(t),ewo=r(ez,`class
method.`),ez.forEach(t),owo=i(el),s3=n(el,"P",{});var Pke=s(s3);rwo=r(Pke,"This class cannot be instantiated directly using "),gae=n(Pke,"CODE",{});var xHr=s(gae);two=r(xHr,"__init__()"),xHr.forEach(t),awo=r(Pke," (throws an error)."),Pke.forEach(t),nwo=i(el),Ur=n(el,"DIV",{class:!0});var ol=s(Ur);m(l3.$$.fragment,ol),swo=i(ol),hae=n(ol,"P",{});var kHr=s(hae);lwo=r(kHr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),kHr.forEach(t),iwo=i(ol),fd=n(ol,"P",{});var oz=s(fd);dwo=r(oz,`Note:
Loading a model from its configuration file does `),pae=n(oz,"STRONG",{});var RHr=s(pae);cwo=r(RHr,"not"),RHr.forEach(t),fwo=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ae=n(oz,"CODE",{});var SHr=s(_ae);mwo=r(SHr,"from_pretrained()"),SHr.forEach(t),gwo=r(oz,"to load the model weights."),oz.forEach(t),hwo=i(ol),uae=n(ol,"P",{});var PHr=s(uae);pwo=r(PHr,"Examples:"),PHr.forEach(t),_wo=i(ol),m(i3.$$.fragment,ol),ol.forEach(t),uwo=i(el),Ve=n(el,"DIV",{class:!0});var Vt=s(Ve);m(d3.$$.fragment,Vt),bwo=i(Vt),bae=n(Vt,"P",{});var $Hr=s(bae);vwo=r($Hr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),$Hr.forEach(t),Two=i(Vt),Wa=n(Vt,"P",{});var qM=s(Wa);Fwo=r(qM,"The model class to instantiate is selected based on the "),vae=n(qM,"CODE",{});var IHr=s(vae);Cwo=r(IHr,"model_type"),IHr.forEach(t),Mwo=r(qM,` property of the config object (either
passed as an argument or loaded from `),Tae=n(qM,"CODE",{});var DHr=s(Tae);Ewo=r(DHr,"pretrained_model_name_or_path"),DHr.forEach(t),ywo=r(qM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fae=n(qM,"CODE",{});var jHr=s(Fae);wwo=r(jHr,"pretrained_model_name_or_path"),jHr.forEach(t),Awo=r(qM,":"),qM.forEach(t),Lwo=i(Vt),na=n(Vt,"UL",{});var rl=s(na);eb=n(rl,"LI",{});var i3e=s(eb);Cae=n(i3e,"STRONG",{});var NHr=s(Cae);Bwo=r(NHr,"bert"),NHr.forEach(t),xwo=r(i3e," \u2014 "),lD=n(i3e,"A",{href:!0});var qHr=s(lD);kwo=r(qHr,"BertForNextSentencePrediction"),qHr.forEach(t),Rwo=r(i3e," (BERT model)"),i3e.forEach(t),Swo=i(rl),ob=n(rl,"LI",{});var d3e=s(ob);Mae=n(d3e,"STRONG",{});var GHr=s(Mae);Pwo=r(GHr,"fnet"),GHr.forEach(t),$wo=r(d3e," \u2014 "),iD=n(d3e,"A",{href:!0});var OHr=s(iD);Iwo=r(OHr,"FNetForNextSentencePrediction"),OHr.forEach(t),Dwo=r(d3e," (FNet model)"),d3e.forEach(t),jwo=i(rl),rb=n(rl,"LI",{});var c3e=s(rb);Eae=n(c3e,"STRONG",{});var XHr=s(Eae);Nwo=r(XHr,"megatron-bert"),XHr.forEach(t),qwo=r(c3e," \u2014 "),dD=n(c3e,"A",{href:!0});var VHr=s(dD);Gwo=r(VHr,"MegatronBertForNextSentencePrediction"),VHr.forEach(t),Owo=r(c3e," (MegatronBert model)"),c3e.forEach(t),Xwo=i(rl),tb=n(rl,"LI",{});var f3e=s(tb);yae=n(f3e,"STRONG",{});var zHr=s(yae);Vwo=r(zHr,"mobilebert"),zHr.forEach(t),zwo=r(f3e," \u2014 "),cD=n(f3e,"A",{href:!0});var WHr=s(cD);Wwo=r(WHr,"MobileBertForNextSentencePrediction"),WHr.forEach(t),Qwo=r(f3e," (MobileBERT model)"),f3e.forEach(t),Hwo=i(rl),ab=n(rl,"LI",{});var m3e=s(ab);wae=n(m3e,"STRONG",{});var QHr=s(wae);Uwo=r(QHr,"qdqbert"),QHr.forEach(t),Jwo=r(m3e," \u2014 "),fD=n(m3e,"A",{href:!0});var HHr=s(fD);Ywo=r(HHr,"QDQBertForNextSentencePrediction"),HHr.forEach(t),Kwo=r(m3e," (QDQBert model)"),m3e.forEach(t),rl.forEach(t),Zwo=i(Vt),nb=n(Vt,"P",{});var g3e=s(nb);e6o=r(g3e,"The model is set in evaluation mode by default using "),Aae=n(g3e,"CODE",{});var UHr=s(Aae);o6o=r(UHr,"model.eval()"),UHr.forEach(t),r6o=r(g3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Lae=n(g3e,"CODE",{});var JHr=s(Lae);t6o=r(JHr,"model.train()"),JHr.forEach(t),g3e.forEach(t),a6o=i(Vt),Bae=n(Vt,"P",{});var YHr=s(Bae);n6o=r(YHr,"Examples:"),YHr.forEach(t),s6o=i(Vt),m(c3.$$.fragment,Vt),Vt.forEach(t),el.forEach(t),LBe=i(c),md=n(c,"H2",{class:!0});var $ke=s(md);sb=n($ke,"A",{id:!0,class:!0,href:!0});var KHr=s(sb);xae=n(KHr,"SPAN",{});var ZHr=s(xae);m(f3.$$.fragment,ZHr),ZHr.forEach(t),KHr.forEach(t),l6o=i($ke),kae=n($ke,"SPAN",{});var eUr=s(kae);i6o=r(eUr,"AutoModelForTokenClassification"),eUr.forEach(t),$ke.forEach(t),BBe=i(c),tr=n(c,"DIV",{class:!0});var tl=s(tr);m(m3.$$.fragment,tl),d6o=i(tl),gd=n(tl,"P",{});var rz=s(gd);c6o=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Rae=n(rz,"CODE",{});var oUr=s(Rae);f6o=r(oUr,"from_pretrained()"),oUr.forEach(t),m6o=r(rz,"class method or the "),Sae=n(rz,"CODE",{});var rUr=s(Sae);g6o=r(rUr,"from_config()"),rUr.forEach(t),h6o=r(rz,`class
method.`),rz.forEach(t),p6o=i(tl),g3=n(tl,"P",{});var Ike=s(g3);_6o=r(Ike,"This class cannot be instantiated directly using "),Pae=n(Ike,"CODE",{});var tUr=s(Pae);u6o=r(tUr,"__init__()"),tUr.forEach(t),b6o=r(Ike," (throws an error)."),Ike.forEach(t),v6o=i(tl),Jr=n(tl,"DIV",{class:!0});var al=s(Jr);m(h3.$$.fragment,al),T6o=i(al),$ae=n(al,"P",{});var aUr=s($ae);F6o=r(aUr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),aUr.forEach(t),C6o=i(al),hd=n(al,"P",{});var tz=s(hd);M6o=r(tz,`Note:
Loading a model from its configuration file does `),Iae=n(tz,"STRONG",{});var nUr=s(Iae);E6o=r(nUr,"not"),nUr.forEach(t),y6o=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dae=n(tz,"CODE",{});var sUr=s(Dae);w6o=r(sUr,"from_pretrained()"),sUr.forEach(t),A6o=r(tz,"to load the model weights."),tz.forEach(t),L6o=i(al),jae=n(al,"P",{});var lUr=s(jae);B6o=r(lUr,"Examples:"),lUr.forEach(t),x6o=i(al),m(p3.$$.fragment,al),al.forEach(t),k6o=i(tl),ze=n(tl,"DIV",{class:!0});var zt=s(ze);m(_3.$$.fragment,zt),R6o=i(zt),Nae=n(zt,"P",{});var iUr=s(Nae);S6o=r(iUr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),iUr.forEach(t),P6o=i(zt),Qa=n(zt,"P",{});var GM=s(Qa);$6o=r(GM,"The model class to instantiate is selected based on the "),qae=n(GM,"CODE",{});var dUr=s(qae);I6o=r(dUr,"model_type"),dUr.forEach(t),D6o=r(GM,` property of the config object (either
passed as an argument or loaded from `),Gae=n(GM,"CODE",{});var cUr=s(Gae);j6o=r(cUr,"pretrained_model_name_or_path"),cUr.forEach(t),N6o=r(GM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Oae=n(GM,"CODE",{});var fUr=s(Oae);q6o=r(fUr,"pretrained_model_name_or_path"),fUr.forEach(t),G6o=r(GM,":"),GM.forEach(t),O6o=i(zt),N=n(zt,"UL",{});var q=s(N);lb=n(q,"LI",{});var h3e=s(lb);Xae=n(h3e,"STRONG",{});var mUr=s(Xae);X6o=r(mUr,"albert"),mUr.forEach(t),V6o=r(h3e," \u2014 "),mD=n(h3e,"A",{href:!0});var gUr=s(mD);z6o=r(gUr,"AlbertForTokenClassification"),gUr.forEach(t),W6o=r(h3e," (ALBERT model)"),h3e.forEach(t),Q6o=i(q),ib=n(q,"LI",{});var p3e=s(ib);Vae=n(p3e,"STRONG",{});var hUr=s(Vae);H6o=r(hUr,"bert"),hUr.forEach(t),U6o=r(p3e," \u2014 "),gD=n(p3e,"A",{href:!0});var pUr=s(gD);J6o=r(pUr,"BertForTokenClassification"),pUr.forEach(t),Y6o=r(p3e," (BERT model)"),p3e.forEach(t),K6o=i(q),db=n(q,"LI",{});var _3e=s(db);zae=n(_3e,"STRONG",{});var _Ur=s(zae);Z6o=r(_Ur,"big_bird"),_Ur.forEach(t),eAo=r(_3e," \u2014 "),hD=n(_3e,"A",{href:!0});var uUr=s(hD);oAo=r(uUr,"BigBirdForTokenClassification"),uUr.forEach(t),rAo=r(_3e," (BigBird model)"),_3e.forEach(t),tAo=i(q),cb=n(q,"LI",{});var u3e=s(cb);Wae=n(u3e,"STRONG",{});var bUr=s(Wae);aAo=r(bUr,"camembert"),bUr.forEach(t),nAo=r(u3e," \u2014 "),pD=n(u3e,"A",{href:!0});var vUr=s(pD);sAo=r(vUr,"CamembertForTokenClassification"),vUr.forEach(t),lAo=r(u3e," (CamemBERT model)"),u3e.forEach(t),iAo=i(q),fb=n(q,"LI",{});var b3e=s(fb);Qae=n(b3e,"STRONG",{});var TUr=s(Qae);dAo=r(TUr,"canine"),TUr.forEach(t),cAo=r(b3e," \u2014 "),_D=n(b3e,"A",{href:!0});var FUr=s(_D);fAo=r(FUr,"CanineForTokenClassification"),FUr.forEach(t),mAo=r(b3e," (Canine model)"),b3e.forEach(t),gAo=i(q),mb=n(q,"LI",{});var v3e=s(mb);Hae=n(v3e,"STRONG",{});var CUr=s(Hae);hAo=r(CUr,"convbert"),CUr.forEach(t),pAo=r(v3e," \u2014 "),uD=n(v3e,"A",{href:!0});var MUr=s(uD);_Ao=r(MUr,"ConvBertForTokenClassification"),MUr.forEach(t),uAo=r(v3e," (ConvBERT model)"),v3e.forEach(t),bAo=i(q),gb=n(q,"LI",{});var T3e=s(gb);Uae=n(T3e,"STRONG",{});var EUr=s(Uae);vAo=r(EUr,"data2vec-text"),EUr.forEach(t),TAo=r(T3e," \u2014 "),bD=n(T3e,"A",{href:!0});var yUr=s(bD);FAo=r(yUr,"Data2VecTextForTokenClassification"),yUr.forEach(t),CAo=r(T3e," (Data2VecText model)"),T3e.forEach(t),MAo=i(q),hb=n(q,"LI",{});var F3e=s(hb);Jae=n(F3e,"STRONG",{});var wUr=s(Jae);EAo=r(wUr,"deberta"),wUr.forEach(t),yAo=r(F3e," \u2014 "),vD=n(F3e,"A",{href:!0});var AUr=s(vD);wAo=r(AUr,"DebertaForTokenClassification"),AUr.forEach(t),AAo=r(F3e," (DeBERTa model)"),F3e.forEach(t),LAo=i(q),pb=n(q,"LI",{});var C3e=s(pb);Yae=n(C3e,"STRONG",{});var LUr=s(Yae);BAo=r(LUr,"deberta-v2"),LUr.forEach(t),xAo=r(C3e," \u2014 "),TD=n(C3e,"A",{href:!0});var BUr=s(TD);kAo=r(BUr,"DebertaV2ForTokenClassification"),BUr.forEach(t),RAo=r(C3e," (DeBERTa-v2 model)"),C3e.forEach(t),SAo=i(q),_b=n(q,"LI",{});var M3e=s(_b);Kae=n(M3e,"STRONG",{});var xUr=s(Kae);PAo=r(xUr,"distilbert"),xUr.forEach(t),$Ao=r(M3e," \u2014 "),FD=n(M3e,"A",{href:!0});var kUr=s(FD);IAo=r(kUr,"DistilBertForTokenClassification"),kUr.forEach(t),DAo=r(M3e," (DistilBERT model)"),M3e.forEach(t),jAo=i(q),ub=n(q,"LI",{});var E3e=s(ub);Zae=n(E3e,"STRONG",{});var RUr=s(Zae);NAo=r(RUr,"electra"),RUr.forEach(t),qAo=r(E3e," \u2014 "),CD=n(E3e,"A",{href:!0});var SUr=s(CD);GAo=r(SUr,"ElectraForTokenClassification"),SUr.forEach(t),OAo=r(E3e," (ELECTRA model)"),E3e.forEach(t),XAo=i(q),bb=n(q,"LI",{});var y3e=s(bb);ene=n(y3e,"STRONG",{});var PUr=s(ene);VAo=r(PUr,"flaubert"),PUr.forEach(t),zAo=r(y3e," \u2014 "),MD=n(y3e,"A",{href:!0});var $Ur=s(MD);WAo=r($Ur,"FlaubertForTokenClassification"),$Ur.forEach(t),QAo=r(y3e," (FlauBERT model)"),y3e.forEach(t),HAo=i(q),vb=n(q,"LI",{});var w3e=s(vb);one=n(w3e,"STRONG",{});var IUr=s(one);UAo=r(IUr,"fnet"),IUr.forEach(t),JAo=r(w3e," \u2014 "),ED=n(w3e,"A",{href:!0});var DUr=s(ED);YAo=r(DUr,"FNetForTokenClassification"),DUr.forEach(t),KAo=r(w3e," (FNet model)"),w3e.forEach(t),ZAo=i(q),Tb=n(q,"LI",{});var A3e=s(Tb);rne=n(A3e,"STRONG",{});var jUr=s(rne);eLo=r(jUr,"funnel"),jUr.forEach(t),oLo=r(A3e," \u2014 "),yD=n(A3e,"A",{href:!0});var NUr=s(yD);rLo=r(NUr,"FunnelForTokenClassification"),NUr.forEach(t),tLo=r(A3e," (Funnel Transformer model)"),A3e.forEach(t),aLo=i(q),Fb=n(q,"LI",{});var L3e=s(Fb);tne=n(L3e,"STRONG",{});var qUr=s(tne);nLo=r(qUr,"gpt2"),qUr.forEach(t),sLo=r(L3e," \u2014 "),wD=n(L3e,"A",{href:!0});var GUr=s(wD);lLo=r(GUr,"GPT2ForTokenClassification"),GUr.forEach(t),iLo=r(L3e," (OpenAI GPT-2 model)"),L3e.forEach(t),dLo=i(q),Cb=n(q,"LI",{});var B3e=s(Cb);ane=n(B3e,"STRONG",{});var OUr=s(ane);cLo=r(OUr,"ibert"),OUr.forEach(t),fLo=r(B3e," \u2014 "),AD=n(B3e,"A",{href:!0});var XUr=s(AD);mLo=r(XUr,"IBertForTokenClassification"),XUr.forEach(t),gLo=r(B3e," (I-BERT model)"),B3e.forEach(t),hLo=i(q),Mb=n(q,"LI",{});var x3e=s(Mb);nne=n(x3e,"STRONG",{});var VUr=s(nne);pLo=r(VUr,"layoutlm"),VUr.forEach(t),_Lo=r(x3e," \u2014 "),LD=n(x3e,"A",{href:!0});var zUr=s(LD);uLo=r(zUr,"LayoutLMForTokenClassification"),zUr.forEach(t),bLo=r(x3e," (LayoutLM model)"),x3e.forEach(t),vLo=i(q),Eb=n(q,"LI",{});var k3e=s(Eb);sne=n(k3e,"STRONG",{});var WUr=s(sne);TLo=r(WUr,"layoutlmv2"),WUr.forEach(t),FLo=r(k3e," \u2014 "),BD=n(k3e,"A",{href:!0});var QUr=s(BD);CLo=r(QUr,"LayoutLMv2ForTokenClassification"),QUr.forEach(t),MLo=r(k3e," (LayoutLMv2 model)"),k3e.forEach(t),ELo=i(q),yb=n(q,"LI",{});var R3e=s(yb);lne=n(R3e,"STRONG",{});var HUr=s(lne);yLo=r(HUr,"longformer"),HUr.forEach(t),wLo=r(R3e," \u2014 "),xD=n(R3e,"A",{href:!0});var UUr=s(xD);ALo=r(UUr,"LongformerForTokenClassification"),UUr.forEach(t),LLo=r(R3e," (Longformer model)"),R3e.forEach(t),BLo=i(q),wb=n(q,"LI",{});var S3e=s(wb);ine=n(S3e,"STRONG",{});var JUr=s(ine);xLo=r(JUr,"megatron-bert"),JUr.forEach(t),kLo=r(S3e," \u2014 "),kD=n(S3e,"A",{href:!0});var YUr=s(kD);RLo=r(YUr,"MegatronBertForTokenClassification"),YUr.forEach(t),SLo=r(S3e," (MegatronBert model)"),S3e.forEach(t),PLo=i(q),Ab=n(q,"LI",{});var P3e=s(Ab);dne=n(P3e,"STRONG",{});var KUr=s(dne);$Lo=r(KUr,"mobilebert"),KUr.forEach(t),ILo=r(P3e," \u2014 "),RD=n(P3e,"A",{href:!0});var ZUr=s(RD);DLo=r(ZUr,"MobileBertForTokenClassification"),ZUr.forEach(t),jLo=r(P3e," (MobileBERT model)"),P3e.forEach(t),NLo=i(q),Lb=n(q,"LI",{});var $3e=s(Lb);cne=n($3e,"STRONG",{});var eJr=s(cne);qLo=r(eJr,"mpnet"),eJr.forEach(t),GLo=r($3e," \u2014 "),SD=n($3e,"A",{href:!0});var oJr=s(SD);OLo=r(oJr,"MPNetForTokenClassification"),oJr.forEach(t),XLo=r($3e," (MPNet model)"),$3e.forEach(t),VLo=i(q),Bb=n(q,"LI",{});var I3e=s(Bb);fne=n(I3e,"STRONG",{});var rJr=s(fne);zLo=r(rJr,"nystromformer"),rJr.forEach(t),WLo=r(I3e," \u2014 "),PD=n(I3e,"A",{href:!0});var tJr=s(PD);QLo=r(tJr,"NystromformerForTokenClassification"),tJr.forEach(t),HLo=r(I3e," (Nystromformer model)"),I3e.forEach(t),ULo=i(q),xb=n(q,"LI",{});var D3e=s(xb);mne=n(D3e,"STRONG",{});var aJr=s(mne);JLo=r(aJr,"qdqbert"),aJr.forEach(t),YLo=r(D3e," \u2014 "),$D=n(D3e,"A",{href:!0});var nJr=s($D);KLo=r(nJr,"QDQBertForTokenClassification"),nJr.forEach(t),ZLo=r(D3e," (QDQBert model)"),D3e.forEach(t),e8o=i(q),kb=n(q,"LI",{});var j3e=s(kb);gne=n(j3e,"STRONG",{});var sJr=s(gne);o8o=r(sJr,"rembert"),sJr.forEach(t),r8o=r(j3e," \u2014 "),ID=n(j3e,"A",{href:!0});var lJr=s(ID);t8o=r(lJr,"RemBertForTokenClassification"),lJr.forEach(t),a8o=r(j3e," (RemBERT model)"),j3e.forEach(t),n8o=i(q),Rb=n(q,"LI",{});var N3e=s(Rb);hne=n(N3e,"STRONG",{});var iJr=s(hne);s8o=r(iJr,"roberta"),iJr.forEach(t),l8o=r(N3e," \u2014 "),DD=n(N3e,"A",{href:!0});var dJr=s(DD);i8o=r(dJr,"RobertaForTokenClassification"),dJr.forEach(t),d8o=r(N3e," (RoBERTa model)"),N3e.forEach(t),c8o=i(q),Sb=n(q,"LI",{});var q3e=s(Sb);pne=n(q3e,"STRONG",{});var cJr=s(pne);f8o=r(cJr,"roformer"),cJr.forEach(t),m8o=r(q3e," \u2014 "),jD=n(q3e,"A",{href:!0});var fJr=s(jD);g8o=r(fJr,"RoFormerForTokenClassification"),fJr.forEach(t),h8o=r(q3e," (RoFormer model)"),q3e.forEach(t),p8o=i(q),Pb=n(q,"LI",{});var G3e=s(Pb);_ne=n(G3e,"STRONG",{});var mJr=s(_ne);_8o=r(mJr,"squeezebert"),mJr.forEach(t),u8o=r(G3e," \u2014 "),ND=n(G3e,"A",{href:!0});var gJr=s(ND);b8o=r(gJr,"SqueezeBertForTokenClassification"),gJr.forEach(t),v8o=r(G3e," (SqueezeBERT model)"),G3e.forEach(t),T8o=i(q),$b=n(q,"LI",{});var O3e=s($b);une=n(O3e,"STRONG",{});var hJr=s(une);F8o=r(hJr,"xlm"),hJr.forEach(t),C8o=r(O3e," \u2014 "),qD=n(O3e,"A",{href:!0});var pJr=s(qD);M8o=r(pJr,"XLMForTokenClassification"),pJr.forEach(t),E8o=r(O3e," (XLM model)"),O3e.forEach(t),y8o=i(q),Ib=n(q,"LI",{});var X3e=s(Ib);bne=n(X3e,"STRONG",{});var _Jr=s(bne);w8o=r(_Jr,"xlm-roberta"),_Jr.forEach(t),A8o=r(X3e," \u2014 "),GD=n(X3e,"A",{href:!0});var uJr=s(GD);L8o=r(uJr,"XLMRobertaForTokenClassification"),uJr.forEach(t),B8o=r(X3e," (XLM-RoBERTa model)"),X3e.forEach(t),x8o=i(q),Db=n(q,"LI",{});var V3e=s(Db);vne=n(V3e,"STRONG",{});var bJr=s(vne);k8o=r(bJr,"xlm-roberta-xl"),bJr.forEach(t),R8o=r(V3e," \u2014 "),OD=n(V3e,"A",{href:!0});var vJr=s(OD);S8o=r(vJr,"XLMRobertaXLForTokenClassification"),vJr.forEach(t),P8o=r(V3e," (XLM-RoBERTa-XL model)"),V3e.forEach(t),$8o=i(q),jb=n(q,"LI",{});var z3e=s(jb);Tne=n(z3e,"STRONG",{});var TJr=s(Tne);I8o=r(TJr,"xlnet"),TJr.forEach(t),D8o=r(z3e," \u2014 "),XD=n(z3e,"A",{href:!0});var FJr=s(XD);j8o=r(FJr,"XLNetForTokenClassification"),FJr.forEach(t),N8o=r(z3e," (XLNet model)"),z3e.forEach(t),q8o=i(q),Nb=n(q,"LI",{});var W3e=s(Nb);Fne=n(W3e,"STRONG",{});var CJr=s(Fne);G8o=r(CJr,"yoso"),CJr.forEach(t),O8o=r(W3e," \u2014 "),VD=n(W3e,"A",{href:!0});var MJr=s(VD);X8o=r(MJr,"YosoForTokenClassification"),MJr.forEach(t),V8o=r(W3e," (YOSO model)"),W3e.forEach(t),q.forEach(t),z8o=i(zt),qb=n(zt,"P",{});var Q3e=s(qb);W8o=r(Q3e,"The model is set in evaluation mode by default using "),Cne=n(Q3e,"CODE",{});var EJr=s(Cne);Q8o=r(EJr,"model.eval()"),EJr.forEach(t),H8o=r(Q3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Mne=n(Q3e,"CODE",{});var yJr=s(Mne);U8o=r(yJr,"model.train()"),yJr.forEach(t),Q3e.forEach(t),J8o=i(zt),Ene=n(zt,"P",{});var wJr=s(Ene);Y8o=r(wJr,"Examples:"),wJr.forEach(t),K8o=i(zt),m(u3.$$.fragment,zt),zt.forEach(t),tl.forEach(t),xBe=i(c),pd=n(c,"H2",{class:!0});var Dke=s(pd);Gb=n(Dke,"A",{id:!0,class:!0,href:!0});var AJr=s(Gb);yne=n(AJr,"SPAN",{});var LJr=s(yne);m(b3.$$.fragment,LJr),LJr.forEach(t),AJr.forEach(t),Z8o=i(Dke),wne=n(Dke,"SPAN",{});var BJr=s(wne);e7o=r(BJr,"AutoModelForQuestionAnswering"),BJr.forEach(t),Dke.forEach(t),kBe=i(c),ar=n(c,"DIV",{class:!0});var nl=s(ar);m(v3.$$.fragment,nl),o7o=i(nl),_d=n(nl,"P",{});var az=s(_d);r7o=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Ane=n(az,"CODE",{});var xJr=s(Ane);t7o=r(xJr,"from_pretrained()"),xJr.forEach(t),a7o=r(az,"class method or the "),Lne=n(az,"CODE",{});var kJr=s(Lne);n7o=r(kJr,"from_config()"),kJr.forEach(t),s7o=r(az,`class
method.`),az.forEach(t),l7o=i(nl),T3=n(nl,"P",{});var jke=s(T3);i7o=r(jke,"This class cannot be instantiated directly using "),Bne=n(jke,"CODE",{});var RJr=s(Bne);d7o=r(RJr,"__init__()"),RJr.forEach(t),c7o=r(jke," (throws an error)."),jke.forEach(t),f7o=i(nl),Yr=n(nl,"DIV",{class:!0});var sl=s(Yr);m(F3.$$.fragment,sl),m7o=i(sl),xne=n(sl,"P",{});var SJr=s(xne);g7o=r(SJr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),SJr.forEach(t),h7o=i(sl),ud=n(sl,"P",{});var nz=s(ud);p7o=r(nz,`Note:
Loading a model from its configuration file does `),kne=n(nz,"STRONG",{});var PJr=s(kne);_7o=r(PJr,"not"),PJr.forEach(t),u7o=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Rne=n(nz,"CODE",{});var $Jr=s(Rne);b7o=r($Jr,"from_pretrained()"),$Jr.forEach(t),v7o=r(nz,"to load the model weights."),nz.forEach(t),T7o=i(sl),Sne=n(sl,"P",{});var IJr=s(Sne);F7o=r(IJr,"Examples:"),IJr.forEach(t),C7o=i(sl),m(C3.$$.fragment,sl),sl.forEach(t),M7o=i(nl),We=n(nl,"DIV",{class:!0});var Wt=s(We);m(M3.$$.fragment,Wt),E7o=i(Wt),Pne=n(Wt,"P",{});var DJr=s(Pne);y7o=r(DJr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),DJr.forEach(t),w7o=i(Wt),Ha=n(Wt,"P",{});var OM=s(Ha);A7o=r(OM,"The model class to instantiate is selected based on the "),$ne=n(OM,"CODE",{});var jJr=s($ne);L7o=r(jJr,"model_type"),jJr.forEach(t),B7o=r(OM,` property of the config object (either
passed as an argument or loaded from `),Ine=n(OM,"CODE",{});var NJr=s(Ine);x7o=r(NJr,"pretrained_model_name_or_path"),NJr.forEach(t),k7o=r(OM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dne=n(OM,"CODE",{});var qJr=s(Dne);R7o=r(qJr,"pretrained_model_name_or_path"),qJr.forEach(t),S7o=r(OM,":"),OM.forEach(t),P7o=i(Wt),R=n(Wt,"UL",{});var P=s(R);Ob=n(P,"LI",{});var H3e=s(Ob);jne=n(H3e,"STRONG",{});var GJr=s(jne);$7o=r(GJr,"albert"),GJr.forEach(t),I7o=r(H3e," \u2014 "),zD=n(H3e,"A",{href:!0});var OJr=s(zD);D7o=r(OJr,"AlbertForQuestionAnswering"),OJr.forEach(t),j7o=r(H3e," (ALBERT model)"),H3e.forEach(t),N7o=i(P),Xb=n(P,"LI",{});var U3e=s(Xb);Nne=n(U3e,"STRONG",{});var XJr=s(Nne);q7o=r(XJr,"bart"),XJr.forEach(t),G7o=r(U3e," \u2014 "),WD=n(U3e,"A",{href:!0});var VJr=s(WD);O7o=r(VJr,"BartForQuestionAnswering"),VJr.forEach(t),X7o=r(U3e," (BART model)"),U3e.forEach(t),V7o=i(P),Vb=n(P,"LI",{});var J3e=s(Vb);qne=n(J3e,"STRONG",{});var zJr=s(qne);z7o=r(zJr,"bert"),zJr.forEach(t),W7o=r(J3e," \u2014 "),QD=n(J3e,"A",{href:!0});var WJr=s(QD);Q7o=r(WJr,"BertForQuestionAnswering"),WJr.forEach(t),H7o=r(J3e," (BERT model)"),J3e.forEach(t),U7o=i(P),zb=n(P,"LI",{});var Y3e=s(zb);Gne=n(Y3e,"STRONG",{});var QJr=s(Gne);J7o=r(QJr,"big_bird"),QJr.forEach(t),Y7o=r(Y3e," \u2014 "),HD=n(Y3e,"A",{href:!0});var HJr=s(HD);K7o=r(HJr,"BigBirdForQuestionAnswering"),HJr.forEach(t),Z7o=r(Y3e," (BigBird model)"),Y3e.forEach(t),eBo=i(P),Wb=n(P,"LI",{});var K3e=s(Wb);One=n(K3e,"STRONG",{});var UJr=s(One);oBo=r(UJr,"bigbird_pegasus"),UJr.forEach(t),rBo=r(K3e," \u2014 "),UD=n(K3e,"A",{href:!0});var JJr=s(UD);tBo=r(JJr,"BigBirdPegasusForQuestionAnswering"),JJr.forEach(t),aBo=r(K3e," (BigBirdPegasus model)"),K3e.forEach(t),nBo=i(P),Qb=n(P,"LI",{});var Z3e=s(Qb);Xne=n(Z3e,"STRONG",{});var YJr=s(Xne);sBo=r(YJr,"camembert"),YJr.forEach(t),lBo=r(Z3e," \u2014 "),JD=n(Z3e,"A",{href:!0});var KJr=s(JD);iBo=r(KJr,"CamembertForQuestionAnswering"),KJr.forEach(t),dBo=r(Z3e," (CamemBERT model)"),Z3e.forEach(t),cBo=i(P),Hb=n(P,"LI",{});var eye=s(Hb);Vne=n(eye,"STRONG",{});var ZJr=s(Vne);fBo=r(ZJr,"canine"),ZJr.forEach(t),mBo=r(eye," \u2014 "),YD=n(eye,"A",{href:!0});var eYr=s(YD);gBo=r(eYr,"CanineForQuestionAnswering"),eYr.forEach(t),hBo=r(eye," (Canine model)"),eye.forEach(t),pBo=i(P),Ub=n(P,"LI",{});var oye=s(Ub);zne=n(oye,"STRONG",{});var oYr=s(zne);_Bo=r(oYr,"convbert"),oYr.forEach(t),uBo=r(oye," \u2014 "),KD=n(oye,"A",{href:!0});var rYr=s(KD);bBo=r(rYr,"ConvBertForQuestionAnswering"),rYr.forEach(t),vBo=r(oye," (ConvBERT model)"),oye.forEach(t),TBo=i(P),Jb=n(P,"LI",{});var rye=s(Jb);Wne=n(rye,"STRONG",{});var tYr=s(Wne);FBo=r(tYr,"data2vec-text"),tYr.forEach(t),CBo=r(rye," \u2014 "),ZD=n(rye,"A",{href:!0});var aYr=s(ZD);MBo=r(aYr,"Data2VecTextForQuestionAnswering"),aYr.forEach(t),EBo=r(rye," (Data2VecText model)"),rye.forEach(t),yBo=i(P),Yb=n(P,"LI",{});var tye=s(Yb);Qne=n(tye,"STRONG",{});var nYr=s(Qne);wBo=r(nYr,"deberta"),nYr.forEach(t),ABo=r(tye," \u2014 "),ej=n(tye,"A",{href:!0});var sYr=s(ej);LBo=r(sYr,"DebertaForQuestionAnswering"),sYr.forEach(t),BBo=r(tye," (DeBERTa model)"),tye.forEach(t),xBo=i(P),Kb=n(P,"LI",{});var aye=s(Kb);Hne=n(aye,"STRONG",{});var lYr=s(Hne);kBo=r(lYr,"deberta-v2"),lYr.forEach(t),RBo=r(aye," \u2014 "),oj=n(aye,"A",{href:!0});var iYr=s(oj);SBo=r(iYr,"DebertaV2ForQuestionAnswering"),iYr.forEach(t),PBo=r(aye," (DeBERTa-v2 model)"),aye.forEach(t),$Bo=i(P),Zb=n(P,"LI",{});var nye=s(Zb);Une=n(nye,"STRONG",{});var dYr=s(Une);IBo=r(dYr,"distilbert"),dYr.forEach(t),DBo=r(nye," \u2014 "),rj=n(nye,"A",{href:!0});var cYr=s(rj);jBo=r(cYr,"DistilBertForQuestionAnswering"),cYr.forEach(t),NBo=r(nye," (DistilBERT model)"),nye.forEach(t),qBo=i(P),e5=n(P,"LI",{});var sye=s(e5);Jne=n(sye,"STRONG",{});var fYr=s(Jne);GBo=r(fYr,"electra"),fYr.forEach(t),OBo=r(sye," \u2014 "),tj=n(sye,"A",{href:!0});var mYr=s(tj);XBo=r(mYr,"ElectraForQuestionAnswering"),mYr.forEach(t),VBo=r(sye," (ELECTRA model)"),sye.forEach(t),zBo=i(P),o5=n(P,"LI",{});var lye=s(o5);Yne=n(lye,"STRONG",{});var gYr=s(Yne);WBo=r(gYr,"flaubert"),gYr.forEach(t),QBo=r(lye," \u2014 "),aj=n(lye,"A",{href:!0});var hYr=s(aj);HBo=r(hYr,"FlaubertForQuestionAnsweringSimple"),hYr.forEach(t),UBo=r(lye," (FlauBERT model)"),lye.forEach(t),JBo=i(P),r5=n(P,"LI",{});var iye=s(r5);Kne=n(iye,"STRONG",{});var pYr=s(Kne);YBo=r(pYr,"fnet"),pYr.forEach(t),KBo=r(iye," \u2014 "),nj=n(iye,"A",{href:!0});var _Yr=s(nj);ZBo=r(_Yr,"FNetForQuestionAnswering"),_Yr.forEach(t),exo=r(iye," (FNet model)"),iye.forEach(t),oxo=i(P),t5=n(P,"LI",{});var dye=s(t5);Zne=n(dye,"STRONG",{});var uYr=s(Zne);rxo=r(uYr,"funnel"),uYr.forEach(t),txo=r(dye," \u2014 "),sj=n(dye,"A",{href:!0});var bYr=s(sj);axo=r(bYr,"FunnelForQuestionAnswering"),bYr.forEach(t),nxo=r(dye," (Funnel Transformer model)"),dye.forEach(t),sxo=i(P),a5=n(P,"LI",{});var cye=s(a5);ese=n(cye,"STRONG",{});var vYr=s(ese);lxo=r(vYr,"gptj"),vYr.forEach(t),ixo=r(cye," \u2014 "),lj=n(cye,"A",{href:!0});var TYr=s(lj);dxo=r(TYr,"GPTJForQuestionAnswering"),TYr.forEach(t),cxo=r(cye," (GPT-J model)"),cye.forEach(t),fxo=i(P),n5=n(P,"LI",{});var fye=s(n5);ose=n(fye,"STRONG",{});var FYr=s(ose);mxo=r(FYr,"ibert"),FYr.forEach(t),gxo=r(fye," \u2014 "),ij=n(fye,"A",{href:!0});var CYr=s(ij);hxo=r(CYr,"IBertForQuestionAnswering"),CYr.forEach(t),pxo=r(fye," (I-BERT model)"),fye.forEach(t),_xo=i(P),s5=n(P,"LI",{});var mye=s(s5);rse=n(mye,"STRONG",{});var MYr=s(rse);uxo=r(MYr,"layoutlmv2"),MYr.forEach(t),bxo=r(mye," \u2014 "),dj=n(mye,"A",{href:!0});var EYr=s(dj);vxo=r(EYr,"LayoutLMv2ForQuestionAnswering"),EYr.forEach(t),Txo=r(mye," (LayoutLMv2 model)"),mye.forEach(t),Fxo=i(P),l5=n(P,"LI",{});var gye=s(l5);tse=n(gye,"STRONG",{});var yYr=s(tse);Cxo=r(yYr,"led"),yYr.forEach(t),Mxo=r(gye," \u2014 "),cj=n(gye,"A",{href:!0});var wYr=s(cj);Exo=r(wYr,"LEDForQuestionAnswering"),wYr.forEach(t),yxo=r(gye," (LED model)"),gye.forEach(t),wxo=i(P),i5=n(P,"LI",{});var hye=s(i5);ase=n(hye,"STRONG",{});var AYr=s(ase);Axo=r(AYr,"longformer"),AYr.forEach(t),Lxo=r(hye," \u2014 "),fj=n(hye,"A",{href:!0});var LYr=s(fj);Bxo=r(LYr,"LongformerForQuestionAnswering"),LYr.forEach(t),xxo=r(hye," (Longformer model)"),hye.forEach(t),kxo=i(P),d5=n(P,"LI",{});var pye=s(d5);nse=n(pye,"STRONG",{});var BYr=s(nse);Rxo=r(BYr,"lxmert"),BYr.forEach(t),Sxo=r(pye," \u2014 "),mj=n(pye,"A",{href:!0});var xYr=s(mj);Pxo=r(xYr,"LxmertForQuestionAnswering"),xYr.forEach(t),$xo=r(pye," (LXMERT model)"),pye.forEach(t),Ixo=i(P),c5=n(P,"LI",{});var _ye=s(c5);sse=n(_ye,"STRONG",{});var kYr=s(sse);Dxo=r(kYr,"mbart"),kYr.forEach(t),jxo=r(_ye," \u2014 "),gj=n(_ye,"A",{href:!0});var RYr=s(gj);Nxo=r(RYr,"MBartForQuestionAnswering"),RYr.forEach(t),qxo=r(_ye," (mBART model)"),_ye.forEach(t),Gxo=i(P),f5=n(P,"LI",{});var uye=s(f5);lse=n(uye,"STRONG",{});var SYr=s(lse);Oxo=r(SYr,"megatron-bert"),SYr.forEach(t),Xxo=r(uye," \u2014 "),hj=n(uye,"A",{href:!0});var PYr=s(hj);Vxo=r(PYr,"MegatronBertForQuestionAnswering"),PYr.forEach(t),zxo=r(uye," (MegatronBert model)"),uye.forEach(t),Wxo=i(P),m5=n(P,"LI",{});var bye=s(m5);ise=n(bye,"STRONG",{});var $Yr=s(ise);Qxo=r($Yr,"mobilebert"),$Yr.forEach(t),Hxo=r(bye," \u2014 "),pj=n(bye,"A",{href:!0});var IYr=s(pj);Uxo=r(IYr,"MobileBertForQuestionAnswering"),IYr.forEach(t),Jxo=r(bye," (MobileBERT model)"),bye.forEach(t),Yxo=i(P),g5=n(P,"LI",{});var vye=s(g5);dse=n(vye,"STRONG",{});var DYr=s(dse);Kxo=r(DYr,"mpnet"),DYr.forEach(t),Zxo=r(vye," \u2014 "),_j=n(vye,"A",{href:!0});var jYr=s(_j);eko=r(jYr,"MPNetForQuestionAnswering"),jYr.forEach(t),oko=r(vye," (MPNet model)"),vye.forEach(t),rko=i(P),h5=n(P,"LI",{});var Tye=s(h5);cse=n(Tye,"STRONG",{});var NYr=s(cse);tko=r(NYr,"nystromformer"),NYr.forEach(t),ako=r(Tye," \u2014 "),uj=n(Tye,"A",{href:!0});var qYr=s(uj);nko=r(qYr,"NystromformerForQuestionAnswering"),qYr.forEach(t),sko=r(Tye," (Nystromformer model)"),Tye.forEach(t),lko=i(P),p5=n(P,"LI",{});var Fye=s(p5);fse=n(Fye,"STRONG",{});var GYr=s(fse);iko=r(GYr,"qdqbert"),GYr.forEach(t),dko=r(Fye," \u2014 "),bj=n(Fye,"A",{href:!0});var OYr=s(bj);cko=r(OYr,"QDQBertForQuestionAnswering"),OYr.forEach(t),fko=r(Fye," (QDQBert model)"),Fye.forEach(t),mko=i(P),_5=n(P,"LI",{});var Cye=s(_5);mse=n(Cye,"STRONG",{});var XYr=s(mse);gko=r(XYr,"reformer"),XYr.forEach(t),hko=r(Cye," \u2014 "),vj=n(Cye,"A",{href:!0});var VYr=s(vj);pko=r(VYr,"ReformerForQuestionAnswering"),VYr.forEach(t),_ko=r(Cye," (Reformer model)"),Cye.forEach(t),uko=i(P),u5=n(P,"LI",{});var Mye=s(u5);gse=n(Mye,"STRONG",{});var zYr=s(gse);bko=r(zYr,"rembert"),zYr.forEach(t),vko=r(Mye," \u2014 "),Tj=n(Mye,"A",{href:!0});var WYr=s(Tj);Tko=r(WYr,"RemBertForQuestionAnswering"),WYr.forEach(t),Fko=r(Mye," (RemBERT model)"),Mye.forEach(t),Cko=i(P),b5=n(P,"LI",{});var Eye=s(b5);hse=n(Eye,"STRONG",{});var QYr=s(hse);Mko=r(QYr,"roberta"),QYr.forEach(t),Eko=r(Eye," \u2014 "),Fj=n(Eye,"A",{href:!0});var HYr=s(Fj);yko=r(HYr,"RobertaForQuestionAnswering"),HYr.forEach(t),wko=r(Eye," (RoBERTa model)"),Eye.forEach(t),Ako=i(P),v5=n(P,"LI",{});var yye=s(v5);pse=n(yye,"STRONG",{});var UYr=s(pse);Lko=r(UYr,"roformer"),UYr.forEach(t),Bko=r(yye," \u2014 "),Cj=n(yye,"A",{href:!0});var JYr=s(Cj);xko=r(JYr,"RoFormerForQuestionAnswering"),JYr.forEach(t),kko=r(yye," (RoFormer model)"),yye.forEach(t),Rko=i(P),T5=n(P,"LI",{});var wye=s(T5);_se=n(wye,"STRONG",{});var YYr=s(_se);Sko=r(YYr,"splinter"),YYr.forEach(t),Pko=r(wye," \u2014 "),Mj=n(wye,"A",{href:!0});var KYr=s(Mj);$ko=r(KYr,"SplinterForQuestionAnswering"),KYr.forEach(t),Iko=r(wye," (Splinter model)"),wye.forEach(t),Dko=i(P),F5=n(P,"LI",{});var Aye=s(F5);use=n(Aye,"STRONG",{});var ZYr=s(use);jko=r(ZYr,"squeezebert"),ZYr.forEach(t),Nko=r(Aye," \u2014 "),Ej=n(Aye,"A",{href:!0});var eKr=s(Ej);qko=r(eKr,"SqueezeBertForQuestionAnswering"),eKr.forEach(t),Gko=r(Aye," (SqueezeBERT model)"),Aye.forEach(t),Oko=i(P),C5=n(P,"LI",{});var Lye=s(C5);bse=n(Lye,"STRONG",{});var oKr=s(bse);Xko=r(oKr,"xlm"),oKr.forEach(t),Vko=r(Lye," \u2014 "),yj=n(Lye,"A",{href:!0});var rKr=s(yj);zko=r(rKr,"XLMForQuestionAnsweringSimple"),rKr.forEach(t),Wko=r(Lye," (XLM model)"),Lye.forEach(t),Qko=i(P),M5=n(P,"LI",{});var Bye=s(M5);vse=n(Bye,"STRONG",{});var tKr=s(vse);Hko=r(tKr,"xlm-roberta"),tKr.forEach(t),Uko=r(Bye," \u2014 "),wj=n(Bye,"A",{href:!0});var aKr=s(wj);Jko=r(aKr,"XLMRobertaForQuestionAnswering"),aKr.forEach(t),Yko=r(Bye," (XLM-RoBERTa model)"),Bye.forEach(t),Kko=i(P),E5=n(P,"LI",{});var xye=s(E5);Tse=n(xye,"STRONG",{});var nKr=s(Tse);Zko=r(nKr,"xlm-roberta-xl"),nKr.forEach(t),eRo=r(xye," \u2014 "),Aj=n(xye,"A",{href:!0});var sKr=s(Aj);oRo=r(sKr,"XLMRobertaXLForQuestionAnswering"),sKr.forEach(t),rRo=r(xye," (XLM-RoBERTa-XL model)"),xye.forEach(t),tRo=i(P),y5=n(P,"LI",{});var kye=s(y5);Fse=n(kye,"STRONG",{});var lKr=s(Fse);aRo=r(lKr,"xlnet"),lKr.forEach(t),nRo=r(kye," \u2014 "),Lj=n(kye,"A",{href:!0});var iKr=s(Lj);sRo=r(iKr,"XLNetForQuestionAnsweringSimple"),iKr.forEach(t),lRo=r(kye," (XLNet model)"),kye.forEach(t),iRo=i(P),w5=n(P,"LI",{});var Rye=s(w5);Cse=n(Rye,"STRONG",{});var dKr=s(Cse);dRo=r(dKr,"yoso"),dKr.forEach(t),cRo=r(Rye," \u2014 "),Bj=n(Rye,"A",{href:!0});var cKr=s(Bj);fRo=r(cKr,"YosoForQuestionAnswering"),cKr.forEach(t),mRo=r(Rye," (YOSO model)"),Rye.forEach(t),P.forEach(t),gRo=i(Wt),A5=n(Wt,"P",{});var Sye=s(A5);hRo=r(Sye,"The model is set in evaluation mode by default using "),Mse=n(Sye,"CODE",{});var fKr=s(Mse);pRo=r(fKr,"model.eval()"),fKr.forEach(t),_Ro=r(Sye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ese=n(Sye,"CODE",{});var mKr=s(Ese);uRo=r(mKr,"model.train()"),mKr.forEach(t),Sye.forEach(t),bRo=i(Wt),yse=n(Wt,"P",{});var gKr=s(yse);vRo=r(gKr,"Examples:"),gKr.forEach(t),TRo=i(Wt),m(E3.$$.fragment,Wt),Wt.forEach(t),nl.forEach(t),RBe=i(c),bd=n(c,"H2",{class:!0});var Nke=s(bd);L5=n(Nke,"A",{id:!0,class:!0,href:!0});var hKr=s(L5);wse=n(hKr,"SPAN",{});var pKr=s(wse);m(y3.$$.fragment,pKr),pKr.forEach(t),hKr.forEach(t),FRo=i(Nke),Ase=n(Nke,"SPAN",{});var _Kr=s(Ase);CRo=r(_Kr,"AutoModelForTableQuestionAnswering"),_Kr.forEach(t),Nke.forEach(t),SBe=i(c),nr=n(c,"DIV",{class:!0});var ll=s(nr);m(w3.$$.fragment,ll),MRo=i(ll),vd=n(ll,"P",{});var sz=s(vd);ERo=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Lse=n(sz,"CODE",{});var uKr=s(Lse);yRo=r(uKr,"from_pretrained()"),uKr.forEach(t),wRo=r(sz,"class method or the "),Bse=n(sz,"CODE",{});var bKr=s(Bse);ARo=r(bKr,"from_config()"),bKr.forEach(t),LRo=r(sz,`class
method.`),sz.forEach(t),BRo=i(ll),A3=n(ll,"P",{});var qke=s(A3);xRo=r(qke,"This class cannot be instantiated directly using "),xse=n(qke,"CODE",{});var vKr=s(xse);kRo=r(vKr,"__init__()"),vKr.forEach(t),RRo=r(qke," (throws an error)."),qke.forEach(t),SRo=i(ll),Kr=n(ll,"DIV",{class:!0});var il=s(Kr);m(L3.$$.fragment,il),PRo=i(il),kse=n(il,"P",{});var TKr=s(kse);$Ro=r(TKr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),TKr.forEach(t),IRo=i(il),Td=n(il,"P",{});var lz=s(Td);DRo=r(lz,`Note:
Loading a model from its configuration file does `),Rse=n(lz,"STRONG",{});var FKr=s(Rse);jRo=r(FKr,"not"),FKr.forEach(t),NRo=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sse=n(lz,"CODE",{});var CKr=s(Sse);qRo=r(CKr,"from_pretrained()"),CKr.forEach(t),GRo=r(lz,"to load the model weights."),lz.forEach(t),ORo=i(il),Pse=n(il,"P",{});var MKr=s(Pse);XRo=r(MKr,"Examples:"),MKr.forEach(t),VRo=i(il),m(B3.$$.fragment,il),il.forEach(t),zRo=i(ll),Qe=n(ll,"DIV",{class:!0});var Qt=s(Qe);m(x3.$$.fragment,Qt),WRo=i(Qt),$se=n(Qt,"P",{});var EKr=s($se);QRo=r(EKr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),EKr.forEach(t),HRo=i(Qt),Ua=n(Qt,"P",{});var XM=s(Ua);URo=r(XM,"The model class to instantiate is selected based on the "),Ise=n(XM,"CODE",{});var yKr=s(Ise);JRo=r(yKr,"model_type"),yKr.forEach(t),YRo=r(XM,` property of the config object (either
passed as an argument or loaded from `),Dse=n(XM,"CODE",{});var wKr=s(Dse);KRo=r(wKr,"pretrained_model_name_or_path"),wKr.forEach(t),ZRo=r(XM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jse=n(XM,"CODE",{});var AKr=s(jse);eSo=r(AKr,"pretrained_model_name_or_path"),AKr.forEach(t),oSo=r(XM,":"),XM.forEach(t),rSo=i(Qt),Nse=n(Qt,"UL",{});var LKr=s(Nse);B5=n(LKr,"LI",{});var Pye=s(B5);qse=n(Pye,"STRONG",{});var BKr=s(qse);tSo=r(BKr,"tapas"),BKr.forEach(t),aSo=r(Pye," \u2014 "),xj=n(Pye,"A",{href:!0});var xKr=s(xj);nSo=r(xKr,"TapasForQuestionAnswering"),xKr.forEach(t),sSo=r(Pye," (TAPAS model)"),Pye.forEach(t),LKr.forEach(t),lSo=i(Qt),x5=n(Qt,"P",{});var $ye=s(x5);iSo=r($ye,"The model is set in evaluation mode by default using "),Gse=n($ye,"CODE",{});var kKr=s(Gse);dSo=r(kKr,"model.eval()"),kKr.forEach(t),cSo=r($ye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ose=n($ye,"CODE",{});var RKr=s(Ose);fSo=r(RKr,"model.train()"),RKr.forEach(t),$ye.forEach(t),mSo=i(Qt),Xse=n(Qt,"P",{});var SKr=s(Xse);gSo=r(SKr,"Examples:"),SKr.forEach(t),hSo=i(Qt),m(k3.$$.fragment,Qt),Qt.forEach(t),ll.forEach(t),PBe=i(c),Fd=n(c,"H2",{class:!0});var Gke=s(Fd);k5=n(Gke,"A",{id:!0,class:!0,href:!0});var PKr=s(k5);Vse=n(PKr,"SPAN",{});var $Kr=s(Vse);m(R3.$$.fragment,$Kr),$Kr.forEach(t),PKr.forEach(t),pSo=i(Gke),zse=n(Gke,"SPAN",{});var IKr=s(zse);_So=r(IKr,"AutoModelForImageClassification"),IKr.forEach(t),Gke.forEach(t),$Be=i(c),sr=n(c,"DIV",{class:!0});var dl=s(sr);m(S3.$$.fragment,dl),uSo=i(dl),Cd=n(dl,"P",{});var iz=s(Cd);bSo=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Wse=n(iz,"CODE",{});var DKr=s(Wse);vSo=r(DKr,"from_pretrained()"),DKr.forEach(t),TSo=r(iz,"class method or the "),Qse=n(iz,"CODE",{});var jKr=s(Qse);FSo=r(jKr,"from_config()"),jKr.forEach(t),CSo=r(iz,`class
method.`),iz.forEach(t),MSo=i(dl),P3=n(dl,"P",{});var Oke=s(P3);ESo=r(Oke,"This class cannot be instantiated directly using "),Hse=n(Oke,"CODE",{});var NKr=s(Hse);ySo=r(NKr,"__init__()"),NKr.forEach(t),wSo=r(Oke," (throws an error)."),Oke.forEach(t),ASo=i(dl),Zr=n(dl,"DIV",{class:!0});var cl=s(Zr);m($3.$$.fragment,cl),LSo=i(cl),Use=n(cl,"P",{});var qKr=s(Use);BSo=r(qKr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qKr.forEach(t),xSo=i(cl),Md=n(cl,"P",{});var dz=s(Md);kSo=r(dz,`Note:
Loading a model from its configuration file does `),Jse=n(dz,"STRONG",{});var GKr=s(Jse);RSo=r(GKr,"not"),GKr.forEach(t),SSo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yse=n(dz,"CODE",{});var OKr=s(Yse);PSo=r(OKr,"from_pretrained()"),OKr.forEach(t),$So=r(dz,"to load the model weights."),dz.forEach(t),ISo=i(cl),Kse=n(cl,"P",{});var XKr=s(Kse);DSo=r(XKr,"Examples:"),XKr.forEach(t),jSo=i(cl),m(I3.$$.fragment,cl),cl.forEach(t),NSo=i(dl),He=n(dl,"DIV",{class:!0});var Ht=s(He);m(D3.$$.fragment,Ht),qSo=i(Ht),Zse=n(Ht,"P",{});var VKr=s(Zse);GSo=r(VKr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),VKr.forEach(t),OSo=i(Ht),Ja=n(Ht,"P",{});var VM=s(Ja);XSo=r(VM,"The model class to instantiate is selected based on the "),ele=n(VM,"CODE",{});var zKr=s(ele);VSo=r(zKr,"model_type"),zKr.forEach(t),zSo=r(VM,` property of the config object (either
passed as an argument or loaded from `),ole=n(VM,"CODE",{});var WKr=s(ole);WSo=r(WKr,"pretrained_model_name_or_path"),WKr.forEach(t),QSo=r(VM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rle=n(VM,"CODE",{});var QKr=s(rle);HSo=r(QKr,"pretrained_model_name_or_path"),QKr.forEach(t),USo=r(VM,":"),VM.forEach(t),JSo=i(Ht),Fe=n(Ht,"UL",{});var no=s(Fe);R5=n(no,"LI",{});var Iye=s(R5);tle=n(Iye,"STRONG",{});var HKr=s(tle);YSo=r(HKr,"beit"),HKr.forEach(t),KSo=r(Iye," \u2014 "),kj=n(Iye,"A",{href:!0});var UKr=s(kj);ZSo=r(UKr,"BeitForImageClassification"),UKr.forEach(t),ePo=r(Iye," (BEiT model)"),Iye.forEach(t),oPo=i(no),S5=n(no,"LI",{});var Dye=s(S5);ale=n(Dye,"STRONG",{});var JKr=s(ale);rPo=r(JKr,"convnext"),JKr.forEach(t),tPo=r(Dye," \u2014 "),Rj=n(Dye,"A",{href:!0});var YKr=s(Rj);aPo=r(YKr,"ConvNextForImageClassification"),YKr.forEach(t),nPo=r(Dye," (ConvNext model)"),Dye.forEach(t),sPo=i(no),Ps=n(no,"LI",{});var m8=s(Ps);nle=n(m8,"STRONG",{});var KKr=s(nle);lPo=r(KKr,"deit"),KKr.forEach(t),iPo=r(m8," \u2014 "),Sj=n(m8,"A",{href:!0});var ZKr=s(Sj);dPo=r(ZKr,"DeiTForImageClassification"),ZKr.forEach(t),cPo=r(m8," or "),Pj=n(m8,"A",{href:!0});var eZr=s(Pj);fPo=r(eZr,"DeiTForImageClassificationWithTeacher"),eZr.forEach(t),mPo=r(m8," (DeiT model)"),m8.forEach(t),gPo=i(no),P5=n(no,"LI",{});var jye=s(P5);sle=n(jye,"STRONG",{});var oZr=s(sle);hPo=r(oZr,"imagegpt"),oZr.forEach(t),pPo=r(jye," \u2014 "),$j=n(jye,"A",{href:!0});var rZr=s($j);_Po=r(rZr,"ImageGPTForImageClassification"),rZr.forEach(t),uPo=r(jye," (ImageGPT model)"),jye.forEach(t),bPo=i(no),la=n(no,"LI",{});var Mf=s(la);lle=n(Mf,"STRONG",{});var tZr=s(lle);vPo=r(tZr,"perceiver"),tZr.forEach(t),TPo=r(Mf," \u2014 "),Ij=n(Mf,"A",{href:!0});var aZr=s(Ij);FPo=r(aZr,"PerceiverForImageClassificationLearned"),aZr.forEach(t),CPo=r(Mf," or "),Dj=n(Mf,"A",{href:!0});var nZr=s(Dj);MPo=r(nZr,"PerceiverForImageClassificationFourier"),nZr.forEach(t),EPo=r(Mf," or "),jj=n(Mf,"A",{href:!0});var sZr=s(jj);yPo=r(sZr,"PerceiverForImageClassificationConvProcessing"),sZr.forEach(t),wPo=r(Mf," (Perceiver model)"),Mf.forEach(t),APo=i(no),$5=n(no,"LI",{});var Nye=s($5);ile=n(Nye,"STRONG",{});var lZr=s(ile);LPo=r(lZr,"poolformer"),lZr.forEach(t),BPo=r(Nye," \u2014 "),Nj=n(Nye,"A",{href:!0});var iZr=s(Nj);xPo=r(iZr,"PoolFormerForImageClassification"),iZr.forEach(t),kPo=r(Nye," (PoolFormer model)"),Nye.forEach(t),RPo=i(no),I5=n(no,"LI",{});var qye=s(I5);dle=n(qye,"STRONG",{});var dZr=s(dle);SPo=r(dZr,"segformer"),dZr.forEach(t),PPo=r(qye," \u2014 "),qj=n(qye,"A",{href:!0});var cZr=s(qj);$Po=r(cZr,"SegformerForImageClassification"),cZr.forEach(t),IPo=r(qye," (SegFormer model)"),qye.forEach(t),DPo=i(no),D5=n(no,"LI",{});var Gye=s(D5);cle=n(Gye,"STRONG",{});var fZr=s(cle);jPo=r(fZr,"swin"),fZr.forEach(t),NPo=r(Gye," \u2014 "),Gj=n(Gye,"A",{href:!0});var mZr=s(Gj);qPo=r(mZr,"SwinForImageClassification"),mZr.forEach(t),GPo=r(Gye," (Swin model)"),Gye.forEach(t),OPo=i(no),j5=n(no,"LI",{});var Oye=s(j5);fle=n(Oye,"STRONG",{});var gZr=s(fle);XPo=r(gZr,"vit"),gZr.forEach(t),VPo=r(Oye," \u2014 "),Oj=n(Oye,"A",{href:!0});var hZr=s(Oj);zPo=r(hZr,"ViTForImageClassification"),hZr.forEach(t),WPo=r(Oye," (ViT model)"),Oye.forEach(t),no.forEach(t),QPo=i(Ht),N5=n(Ht,"P",{});var Xye=s(N5);HPo=r(Xye,"The model is set in evaluation mode by default using "),mle=n(Xye,"CODE",{});var pZr=s(mle);UPo=r(pZr,"model.eval()"),pZr.forEach(t),JPo=r(Xye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gle=n(Xye,"CODE",{});var _Zr=s(gle);YPo=r(_Zr,"model.train()"),_Zr.forEach(t),Xye.forEach(t),KPo=i(Ht),hle=n(Ht,"P",{});var uZr=s(hle);ZPo=r(uZr,"Examples:"),uZr.forEach(t),e$o=i(Ht),m(j3.$$.fragment,Ht),Ht.forEach(t),dl.forEach(t),IBe=i(c),Ed=n(c,"H2",{class:!0});var Xke=s(Ed);q5=n(Xke,"A",{id:!0,class:!0,href:!0});var bZr=s(q5);ple=n(bZr,"SPAN",{});var vZr=s(ple);m(N3.$$.fragment,vZr),vZr.forEach(t),bZr.forEach(t),o$o=i(Xke),_le=n(Xke,"SPAN",{});var TZr=s(_le);r$o=r(TZr,"AutoModelForVision2Seq"),TZr.forEach(t),Xke.forEach(t),DBe=i(c),lr=n(c,"DIV",{class:!0});var fl=s(lr);m(q3.$$.fragment,fl),t$o=i(fl),yd=n(fl,"P",{});var cz=s(yd);a$o=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ule=n(cz,"CODE",{});var FZr=s(ule);n$o=r(FZr,"from_pretrained()"),FZr.forEach(t),s$o=r(cz,"class method or the "),ble=n(cz,"CODE",{});var CZr=s(ble);l$o=r(CZr,"from_config()"),CZr.forEach(t),i$o=r(cz,`class
method.`),cz.forEach(t),d$o=i(fl),G3=n(fl,"P",{});var Vke=s(G3);c$o=r(Vke,"This class cannot be instantiated directly using "),vle=n(Vke,"CODE",{});var MZr=s(vle);f$o=r(MZr,"__init__()"),MZr.forEach(t),m$o=r(Vke," (throws an error)."),Vke.forEach(t),g$o=i(fl),et=n(fl,"DIV",{class:!0});var ml=s(et);m(O3.$$.fragment,ml),h$o=i(ml),Tle=n(ml,"P",{});var EZr=s(Tle);p$o=r(EZr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),EZr.forEach(t),_$o=i(ml),wd=n(ml,"P",{});var fz=s(wd);u$o=r(fz,`Note:
Loading a model from its configuration file does `),Fle=n(fz,"STRONG",{});var yZr=s(Fle);b$o=r(yZr,"not"),yZr.forEach(t),v$o=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cle=n(fz,"CODE",{});var wZr=s(Cle);T$o=r(wZr,"from_pretrained()"),wZr.forEach(t),F$o=r(fz,"to load the model weights."),fz.forEach(t),C$o=i(ml),Mle=n(ml,"P",{});var AZr=s(Mle);M$o=r(AZr,"Examples:"),AZr.forEach(t),E$o=i(ml),m(X3.$$.fragment,ml),ml.forEach(t),y$o=i(fl),Ue=n(fl,"DIV",{class:!0});var Ut=s(Ue);m(V3.$$.fragment,Ut),w$o=i(Ut),Ele=n(Ut,"P",{});var LZr=s(Ele);A$o=r(LZr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),LZr.forEach(t),L$o=i(Ut),Ya=n(Ut,"P",{});var zM=s(Ya);B$o=r(zM,"The model class to instantiate is selected based on the "),yle=n(zM,"CODE",{});var BZr=s(yle);x$o=r(BZr,"model_type"),BZr.forEach(t),k$o=r(zM,` property of the config object (either
passed as an argument or loaded from `),wle=n(zM,"CODE",{});var xZr=s(wle);R$o=r(xZr,"pretrained_model_name_or_path"),xZr.forEach(t),S$o=r(zM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ale=n(zM,"CODE",{});var kZr=s(Ale);P$o=r(kZr,"pretrained_model_name_or_path"),kZr.forEach(t),$$o=r(zM,":"),zM.forEach(t),I$o=i(Ut),Lle=n(Ut,"UL",{});var RZr=s(Lle);G5=n(RZr,"LI",{});var Vye=s(G5);Ble=n(Vye,"STRONG",{});var SZr=s(Ble);D$o=r(SZr,"vision-encoder-decoder"),SZr.forEach(t),j$o=r(Vye," \u2014 "),Xj=n(Vye,"A",{href:!0});var PZr=s(Xj);N$o=r(PZr,"VisionEncoderDecoderModel"),PZr.forEach(t),q$o=r(Vye," (Vision Encoder decoder model)"),Vye.forEach(t),RZr.forEach(t),G$o=i(Ut),O5=n(Ut,"P",{});var zye=s(O5);O$o=r(zye,"The model is set in evaluation mode by default using "),xle=n(zye,"CODE",{});var $Zr=s(xle);X$o=r($Zr,"model.eval()"),$Zr.forEach(t),V$o=r(zye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kle=n(zye,"CODE",{});var IZr=s(kle);z$o=r(IZr,"model.train()"),IZr.forEach(t),zye.forEach(t),W$o=i(Ut),Rle=n(Ut,"P",{});var DZr=s(Rle);Q$o=r(DZr,"Examples:"),DZr.forEach(t),H$o=i(Ut),m(z3.$$.fragment,Ut),Ut.forEach(t),fl.forEach(t),jBe=i(c),Ad=n(c,"H2",{class:!0});var zke=s(Ad);X5=n(zke,"A",{id:!0,class:!0,href:!0});var jZr=s(X5);Sle=n(jZr,"SPAN",{});var NZr=s(Sle);m(W3.$$.fragment,NZr),NZr.forEach(t),jZr.forEach(t),U$o=i(zke),Ple=n(zke,"SPAN",{});var qZr=s(Ple);J$o=r(qZr,"AutoModelForAudioClassification"),qZr.forEach(t),zke.forEach(t),NBe=i(c),ir=n(c,"DIV",{class:!0});var gl=s(ir);m(Q3.$$.fragment,gl),Y$o=i(gl),Ld=n(gl,"P",{});var mz=s(Ld);K$o=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),$le=n(mz,"CODE",{});var GZr=s($le);Z$o=r(GZr,"from_pretrained()"),GZr.forEach(t),eIo=r(mz,"class method or the "),Ile=n(mz,"CODE",{});var OZr=s(Ile);oIo=r(OZr,"from_config()"),OZr.forEach(t),rIo=r(mz,`class
method.`),mz.forEach(t),tIo=i(gl),H3=n(gl,"P",{});var Wke=s(H3);aIo=r(Wke,"This class cannot be instantiated directly using "),Dle=n(Wke,"CODE",{});var XZr=s(Dle);nIo=r(XZr,"__init__()"),XZr.forEach(t),sIo=r(Wke," (throws an error)."),Wke.forEach(t),lIo=i(gl),ot=n(gl,"DIV",{class:!0});var hl=s(ot);m(U3.$$.fragment,hl),iIo=i(hl),jle=n(hl,"P",{});var VZr=s(jle);dIo=r(VZr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),VZr.forEach(t),cIo=i(hl),Bd=n(hl,"P",{});var gz=s(Bd);fIo=r(gz,`Note:
Loading a model from its configuration file does `),Nle=n(gz,"STRONG",{});var zZr=s(Nle);mIo=r(zZr,"not"),zZr.forEach(t),gIo=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),qle=n(gz,"CODE",{});var WZr=s(qle);hIo=r(WZr,"from_pretrained()"),WZr.forEach(t),pIo=r(gz,"to load the model weights."),gz.forEach(t),_Io=i(hl),Gle=n(hl,"P",{});var QZr=s(Gle);uIo=r(QZr,"Examples:"),QZr.forEach(t),bIo=i(hl),m(J3.$$.fragment,hl),hl.forEach(t),vIo=i(gl),Je=n(gl,"DIV",{class:!0});var Jt=s(Je);m(Y3.$$.fragment,Jt),TIo=i(Jt),Ole=n(Jt,"P",{});var HZr=s(Ole);FIo=r(HZr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),HZr.forEach(t),CIo=i(Jt),Ka=n(Jt,"P",{});var WM=s(Ka);MIo=r(WM,"The model class to instantiate is selected based on the "),Xle=n(WM,"CODE",{});var UZr=s(Xle);EIo=r(UZr,"model_type"),UZr.forEach(t),yIo=r(WM,` property of the config object (either
passed as an argument or loaded from `),Vle=n(WM,"CODE",{});var JZr=s(Vle);wIo=r(JZr,"pretrained_model_name_or_path"),JZr.forEach(t),AIo=r(WM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zle=n(WM,"CODE",{});var YZr=s(zle);LIo=r(YZr,"pretrained_model_name_or_path"),YZr.forEach(t),BIo=r(WM,":"),WM.forEach(t),xIo=i(Jt),xe=n(Jt,"UL",{});var No=s(xe);V5=n(No,"LI",{});var Wye=s(V5);Wle=n(Wye,"STRONG",{});var KZr=s(Wle);kIo=r(KZr,"data2vec-audio"),KZr.forEach(t),RIo=r(Wye," \u2014 "),Vj=n(Wye,"A",{href:!0});var ZZr=s(Vj);SIo=r(ZZr,"Data2VecAudioForSequenceClassification"),ZZr.forEach(t),PIo=r(Wye," (Data2VecAudio model)"),Wye.forEach(t),$Io=i(No),z5=n(No,"LI",{});var Qye=s(z5);Qle=n(Qye,"STRONG",{});var eet=s(Qle);IIo=r(eet,"hubert"),eet.forEach(t),DIo=r(Qye," \u2014 "),zj=n(Qye,"A",{href:!0});var oet=s(zj);jIo=r(oet,"HubertForSequenceClassification"),oet.forEach(t),NIo=r(Qye," (Hubert model)"),Qye.forEach(t),qIo=i(No),W5=n(No,"LI",{});var Hye=s(W5);Hle=n(Hye,"STRONG",{});var ret=s(Hle);GIo=r(ret,"sew"),ret.forEach(t),OIo=r(Hye," \u2014 "),Wj=n(Hye,"A",{href:!0});var tet=s(Wj);XIo=r(tet,"SEWForSequenceClassification"),tet.forEach(t),VIo=r(Hye," (SEW model)"),Hye.forEach(t),zIo=i(No),Q5=n(No,"LI",{});var Uye=s(Q5);Ule=n(Uye,"STRONG",{});var aet=s(Ule);WIo=r(aet,"sew-d"),aet.forEach(t),QIo=r(Uye," \u2014 "),Qj=n(Uye,"A",{href:!0});var net=s(Qj);HIo=r(net,"SEWDForSequenceClassification"),net.forEach(t),UIo=r(Uye," (SEW-D model)"),Uye.forEach(t),JIo=i(No),H5=n(No,"LI",{});var Jye=s(H5);Jle=n(Jye,"STRONG",{});var set=s(Jle);YIo=r(set,"unispeech"),set.forEach(t),KIo=r(Jye," \u2014 "),Hj=n(Jye,"A",{href:!0});var iet=s(Hj);ZIo=r(iet,"UniSpeechForSequenceClassification"),iet.forEach(t),eDo=r(Jye," (UniSpeech model)"),Jye.forEach(t),oDo=i(No),U5=n(No,"LI",{});var Yye=s(U5);Yle=n(Yye,"STRONG",{});var det=s(Yle);rDo=r(det,"unispeech-sat"),det.forEach(t),tDo=r(Yye," \u2014 "),Uj=n(Yye,"A",{href:!0});var cet=s(Uj);aDo=r(cet,"UniSpeechSatForSequenceClassification"),cet.forEach(t),nDo=r(Yye," (UniSpeechSat model)"),Yye.forEach(t),sDo=i(No),J5=n(No,"LI",{});var Kye=s(J5);Kle=n(Kye,"STRONG",{});var fet=s(Kle);lDo=r(fet,"wav2vec2"),fet.forEach(t),iDo=r(Kye," \u2014 "),Jj=n(Kye,"A",{href:!0});var met=s(Jj);dDo=r(met,"Wav2Vec2ForSequenceClassification"),met.forEach(t),cDo=r(Kye," (Wav2Vec2 model)"),Kye.forEach(t),fDo=i(No),Y5=n(No,"LI",{});var Zye=s(Y5);Zle=n(Zye,"STRONG",{});var get=s(Zle);mDo=r(get,"wavlm"),get.forEach(t),gDo=r(Zye," \u2014 "),Yj=n(Zye,"A",{href:!0});var het=s(Yj);hDo=r(het,"WavLMForSequenceClassification"),het.forEach(t),pDo=r(Zye," (WavLM model)"),Zye.forEach(t),No.forEach(t),_Do=i(Jt),K5=n(Jt,"P",{});var ewe=s(K5);uDo=r(ewe,"The model is set in evaluation mode by default using "),eie=n(ewe,"CODE",{});var pet=s(eie);bDo=r(pet,"model.eval()"),pet.forEach(t),vDo=r(ewe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=n(ewe,"CODE",{});var _et=s(oie);TDo=r(_et,"model.train()"),_et.forEach(t),ewe.forEach(t),FDo=i(Jt),rie=n(Jt,"P",{});var uet=s(rie);CDo=r(uet,"Examples:"),uet.forEach(t),MDo=i(Jt),m(K3.$$.fragment,Jt),Jt.forEach(t),gl.forEach(t),qBe=i(c),xd=n(c,"H2",{class:!0});var Qke=s(xd);Z5=n(Qke,"A",{id:!0,class:!0,href:!0});var bet=s(Z5);tie=n(bet,"SPAN",{});var vet=s(tie);m(Z3.$$.fragment,vet),vet.forEach(t),bet.forEach(t),EDo=i(Qke),aie=n(Qke,"SPAN",{});var Tet=s(aie);yDo=r(Tet,"AutoModelForAudioFrameClassification"),Tet.forEach(t),Qke.forEach(t),GBe=i(c),dr=n(c,"DIV",{class:!0});var pl=s(dr);m(ey.$$.fragment,pl),wDo=i(pl),kd=n(pl,"P",{});var hz=s(kd);ADo=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),nie=n(hz,"CODE",{});var Fet=s(nie);LDo=r(Fet,"from_pretrained()"),Fet.forEach(t),BDo=r(hz,"class method or the "),sie=n(hz,"CODE",{});var Cet=s(sie);xDo=r(Cet,"from_config()"),Cet.forEach(t),kDo=r(hz,`class
method.`),hz.forEach(t),RDo=i(pl),oy=n(pl,"P",{});var Hke=s(oy);SDo=r(Hke,"This class cannot be instantiated directly using "),lie=n(Hke,"CODE",{});var Met=s(lie);PDo=r(Met,"__init__()"),Met.forEach(t),$Do=r(Hke," (throws an error)."),Hke.forEach(t),IDo=i(pl),rt=n(pl,"DIV",{class:!0});var _l=s(rt);m(ry.$$.fragment,_l),DDo=i(_l),iie=n(_l,"P",{});var Eet=s(iie);jDo=r(Eet,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),Eet.forEach(t),NDo=i(_l),Rd=n(_l,"P",{});var pz=s(Rd);qDo=r(pz,`Note:
Loading a model from its configuration file does `),die=n(pz,"STRONG",{});var yet=s(die);GDo=r(yet,"not"),yet.forEach(t),ODo=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=n(pz,"CODE",{});var wet=s(cie);XDo=r(wet,"from_pretrained()"),wet.forEach(t),VDo=r(pz,"to load the model weights."),pz.forEach(t),zDo=i(_l),fie=n(_l,"P",{});var Aet=s(fie);WDo=r(Aet,"Examples:"),Aet.forEach(t),QDo=i(_l),m(ty.$$.fragment,_l),_l.forEach(t),HDo=i(pl),Ye=n(pl,"DIV",{class:!0});var Yt=s(Ye);m(ay.$$.fragment,Yt),UDo=i(Yt),mie=n(Yt,"P",{});var Let=s(mie);JDo=r(Let,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),Let.forEach(t),YDo=i(Yt),Za=n(Yt,"P",{});var QM=s(Za);KDo=r(QM,"The model class to instantiate is selected based on the "),gie=n(QM,"CODE",{});var Bet=s(gie);ZDo=r(Bet,"model_type"),Bet.forEach(t),ejo=r(QM,` property of the config object (either
passed as an argument or loaded from `),hie=n(QM,"CODE",{});var xet=s(hie);ojo=r(xet,"pretrained_model_name_or_path"),xet.forEach(t),rjo=r(QM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=n(QM,"CODE",{});var ket=s(pie);tjo=r(ket,"pretrained_model_name_or_path"),ket.forEach(t),ajo=r(QM,":"),QM.forEach(t),njo=i(Yt),en=n(Yt,"UL",{});var HM=s(en);e2=n(HM,"LI",{});var owe=s(e2);_ie=n(owe,"STRONG",{});var Ret=s(_ie);sjo=r(Ret,"data2vec-audio"),Ret.forEach(t),ljo=r(owe," \u2014 "),Kj=n(owe,"A",{href:!0});var Set=s(Kj);ijo=r(Set,"Data2VecAudioForAudioFrameClassification"),Set.forEach(t),djo=r(owe," (Data2VecAudio model)"),owe.forEach(t),cjo=i(HM),o2=n(HM,"LI",{});var rwe=s(o2);uie=n(rwe,"STRONG",{});var Pet=s(uie);fjo=r(Pet,"unispeech-sat"),Pet.forEach(t),mjo=r(rwe," \u2014 "),Zj=n(rwe,"A",{href:!0});var $et=s(Zj);gjo=r($et,"UniSpeechSatForAudioFrameClassification"),$et.forEach(t),hjo=r(rwe," (UniSpeechSat model)"),rwe.forEach(t),pjo=i(HM),r2=n(HM,"LI",{});var twe=s(r2);bie=n(twe,"STRONG",{});var Iet=s(bie);_jo=r(Iet,"wav2vec2"),Iet.forEach(t),ujo=r(twe," \u2014 "),eN=n(twe,"A",{href:!0});var Det=s(eN);bjo=r(Det,"Wav2Vec2ForAudioFrameClassification"),Det.forEach(t),vjo=r(twe," (Wav2Vec2 model)"),twe.forEach(t),Tjo=i(HM),t2=n(HM,"LI",{});var awe=s(t2);vie=n(awe,"STRONG",{});var jet=s(vie);Fjo=r(jet,"wavlm"),jet.forEach(t),Cjo=r(awe," \u2014 "),oN=n(awe,"A",{href:!0});var Net=s(oN);Mjo=r(Net,"WavLMForAudioFrameClassification"),Net.forEach(t),Ejo=r(awe," (WavLM model)"),awe.forEach(t),HM.forEach(t),yjo=i(Yt),a2=n(Yt,"P",{});var nwe=s(a2);wjo=r(nwe,"The model is set in evaluation mode by default using "),Tie=n(nwe,"CODE",{});var qet=s(Tie);Ajo=r(qet,"model.eval()"),qet.forEach(t),Ljo=r(nwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fie=n(nwe,"CODE",{});var Get=s(Fie);Bjo=r(Get,"model.train()"),Get.forEach(t),nwe.forEach(t),xjo=i(Yt),Cie=n(Yt,"P",{});var Oet=s(Cie);kjo=r(Oet,"Examples:"),Oet.forEach(t),Rjo=i(Yt),m(ny.$$.fragment,Yt),Yt.forEach(t),pl.forEach(t),OBe=i(c),Sd=n(c,"H2",{class:!0});var Uke=s(Sd);n2=n(Uke,"A",{id:!0,class:!0,href:!0});var Xet=s(n2);Mie=n(Xet,"SPAN",{});var Vet=s(Mie);m(sy.$$.fragment,Vet),Vet.forEach(t),Xet.forEach(t),Sjo=i(Uke),Eie=n(Uke,"SPAN",{});var zet=s(Eie);Pjo=r(zet,"AutoModelForCTC"),zet.forEach(t),Uke.forEach(t),XBe=i(c),cr=n(c,"DIV",{class:!0});var ul=s(cr);m(ly.$$.fragment,ul),$jo=i(ul),Pd=n(ul,"P",{});var _z=s(Pd);Ijo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),yie=n(_z,"CODE",{});var Wet=s(yie);Djo=r(Wet,"from_pretrained()"),Wet.forEach(t),jjo=r(_z,"class method or the "),wie=n(_z,"CODE",{});var Qet=s(wie);Njo=r(Qet,"from_config()"),Qet.forEach(t),qjo=r(_z,`class
method.`),_z.forEach(t),Gjo=i(ul),iy=n(ul,"P",{});var Jke=s(iy);Ojo=r(Jke,"This class cannot be instantiated directly using "),Aie=n(Jke,"CODE",{});var Het=s(Aie);Xjo=r(Het,"__init__()"),Het.forEach(t),Vjo=r(Jke," (throws an error)."),Jke.forEach(t),zjo=i(ul),tt=n(ul,"DIV",{class:!0});var bl=s(tt);m(dy.$$.fragment,bl),Wjo=i(bl),Lie=n(bl,"P",{});var Uet=s(Lie);Qjo=r(Uet,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),Uet.forEach(t),Hjo=i(bl),$d=n(bl,"P",{});var uz=s($d);Ujo=r(uz,`Note:
Loading a model from its configuration file does `),Bie=n(uz,"STRONG",{});var Jet=s(Bie);Jjo=r(Jet,"not"),Jet.forEach(t),Yjo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xie=n(uz,"CODE",{});var Yet=s(xie);Kjo=r(Yet,"from_pretrained()"),Yet.forEach(t),Zjo=r(uz,"to load the model weights."),uz.forEach(t),eNo=i(bl),kie=n(bl,"P",{});var Ket=s(kie);oNo=r(Ket,"Examples:"),Ket.forEach(t),rNo=i(bl),m(cy.$$.fragment,bl),bl.forEach(t),tNo=i(ul),Ke=n(ul,"DIV",{class:!0});var Kt=s(Ke);m(fy.$$.fragment,Kt),aNo=i(Kt),Rie=n(Kt,"P",{});var Zet=s(Rie);nNo=r(Zet,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),Zet.forEach(t),sNo=i(Kt),on=n(Kt,"P",{});var UM=s(on);lNo=r(UM,"The model class to instantiate is selected based on the "),Sie=n(UM,"CODE",{});var eot=s(Sie);iNo=r(eot,"model_type"),eot.forEach(t),dNo=r(UM,` property of the config object (either
passed as an argument or loaded from `),Pie=n(UM,"CODE",{});var oot=s(Pie);cNo=r(oot,"pretrained_model_name_or_path"),oot.forEach(t),fNo=r(UM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$ie=n(UM,"CODE",{});var rot=s($ie);mNo=r(rot,"pretrained_model_name_or_path"),rot.forEach(t),gNo=r(UM,":"),UM.forEach(t),hNo=i(Kt),ke=n(Kt,"UL",{});var qo=s(ke);s2=n(qo,"LI",{});var swe=s(s2);Iie=n(swe,"STRONG",{});var tot=s(Iie);pNo=r(tot,"data2vec-audio"),tot.forEach(t),_No=r(swe," \u2014 "),rN=n(swe,"A",{href:!0});var aot=s(rN);uNo=r(aot,"Data2VecAudioForCTC"),aot.forEach(t),bNo=r(swe," (Data2VecAudio model)"),swe.forEach(t),vNo=i(qo),l2=n(qo,"LI",{});var lwe=s(l2);Die=n(lwe,"STRONG",{});var not=s(Die);TNo=r(not,"hubert"),not.forEach(t),FNo=r(lwe," \u2014 "),tN=n(lwe,"A",{href:!0});var sot=s(tN);CNo=r(sot,"HubertForCTC"),sot.forEach(t),MNo=r(lwe," (Hubert model)"),lwe.forEach(t),ENo=i(qo),i2=n(qo,"LI",{});var iwe=s(i2);jie=n(iwe,"STRONG",{});var lot=s(jie);yNo=r(lot,"sew"),lot.forEach(t),wNo=r(iwe," \u2014 "),aN=n(iwe,"A",{href:!0});var iot=s(aN);ANo=r(iot,"SEWForCTC"),iot.forEach(t),LNo=r(iwe," (SEW model)"),iwe.forEach(t),BNo=i(qo),d2=n(qo,"LI",{});var dwe=s(d2);Nie=n(dwe,"STRONG",{});var dot=s(Nie);xNo=r(dot,"sew-d"),dot.forEach(t),kNo=r(dwe," \u2014 "),nN=n(dwe,"A",{href:!0});var cot=s(nN);RNo=r(cot,"SEWDForCTC"),cot.forEach(t),SNo=r(dwe," (SEW-D model)"),dwe.forEach(t),PNo=i(qo),c2=n(qo,"LI",{});var cwe=s(c2);qie=n(cwe,"STRONG",{});var fot=s(qie);$No=r(fot,"unispeech"),fot.forEach(t),INo=r(cwe," \u2014 "),sN=n(cwe,"A",{href:!0});var mot=s(sN);DNo=r(mot,"UniSpeechForCTC"),mot.forEach(t),jNo=r(cwe," (UniSpeech model)"),cwe.forEach(t),NNo=i(qo),f2=n(qo,"LI",{});var fwe=s(f2);Gie=n(fwe,"STRONG",{});var got=s(Gie);qNo=r(got,"unispeech-sat"),got.forEach(t),GNo=r(fwe," \u2014 "),lN=n(fwe,"A",{href:!0});var hot=s(lN);ONo=r(hot,"UniSpeechSatForCTC"),hot.forEach(t),XNo=r(fwe," (UniSpeechSat model)"),fwe.forEach(t),VNo=i(qo),m2=n(qo,"LI",{});var mwe=s(m2);Oie=n(mwe,"STRONG",{});var pot=s(Oie);zNo=r(pot,"wav2vec2"),pot.forEach(t),WNo=r(mwe," \u2014 "),iN=n(mwe,"A",{href:!0});var _ot=s(iN);QNo=r(_ot,"Wav2Vec2ForCTC"),_ot.forEach(t),HNo=r(mwe," (Wav2Vec2 model)"),mwe.forEach(t),UNo=i(qo),g2=n(qo,"LI",{});var gwe=s(g2);Xie=n(gwe,"STRONG",{});var uot=s(Xie);JNo=r(uot,"wavlm"),uot.forEach(t),YNo=r(gwe," \u2014 "),dN=n(gwe,"A",{href:!0});var bot=s(dN);KNo=r(bot,"WavLMForCTC"),bot.forEach(t),ZNo=r(gwe," (WavLM model)"),gwe.forEach(t),qo.forEach(t),eqo=i(Kt),h2=n(Kt,"P",{});var hwe=s(h2);oqo=r(hwe,"The model is set in evaluation mode by default using "),Vie=n(hwe,"CODE",{});var vot=s(Vie);rqo=r(vot,"model.eval()"),vot.forEach(t),tqo=r(hwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zie=n(hwe,"CODE",{});var Tot=s(zie);aqo=r(Tot,"model.train()"),Tot.forEach(t),hwe.forEach(t),nqo=i(Kt),Wie=n(Kt,"P",{});var Fot=s(Wie);sqo=r(Fot,"Examples:"),Fot.forEach(t),lqo=i(Kt),m(my.$$.fragment,Kt),Kt.forEach(t),ul.forEach(t),VBe=i(c),Id=n(c,"H2",{class:!0});var Yke=s(Id);p2=n(Yke,"A",{id:!0,class:!0,href:!0});var Cot=s(p2);Qie=n(Cot,"SPAN",{});var Mot=s(Qie);m(gy.$$.fragment,Mot),Mot.forEach(t),Cot.forEach(t),iqo=i(Yke),Hie=n(Yke,"SPAN",{});var Eot=s(Hie);dqo=r(Eot,"AutoModelForSpeechSeq2Seq"),Eot.forEach(t),Yke.forEach(t),zBe=i(c),fr=n(c,"DIV",{class:!0});var vl=s(fr);m(hy.$$.fragment,vl),cqo=i(vl),Dd=n(vl,"P",{});var bz=s(Dd);fqo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Uie=n(bz,"CODE",{});var yot=s(Uie);mqo=r(yot,"from_pretrained()"),yot.forEach(t),gqo=r(bz,"class method or the "),Jie=n(bz,"CODE",{});var wot=s(Jie);hqo=r(wot,"from_config()"),wot.forEach(t),pqo=r(bz,`class
method.`),bz.forEach(t),_qo=i(vl),py=n(vl,"P",{});var Kke=s(py);uqo=r(Kke,"This class cannot be instantiated directly using "),Yie=n(Kke,"CODE",{});var Aot=s(Yie);bqo=r(Aot,"__init__()"),Aot.forEach(t),vqo=r(Kke," (throws an error)."),Kke.forEach(t),Tqo=i(vl),at=n(vl,"DIV",{class:!0});var Tl=s(at);m(_y.$$.fragment,Tl),Fqo=i(Tl),Kie=n(Tl,"P",{});var Lot=s(Kie);Cqo=r(Lot,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Lot.forEach(t),Mqo=i(Tl),jd=n(Tl,"P",{});var vz=s(jd);Eqo=r(vz,`Note:
Loading a model from its configuration file does `),Zie=n(vz,"STRONG",{});var Bot=s(Zie);yqo=r(Bot,"not"),Bot.forEach(t),wqo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ede=n(vz,"CODE",{});var xot=s(ede);Aqo=r(xot,"from_pretrained()"),xot.forEach(t),Lqo=r(vz,"to load the model weights."),vz.forEach(t),Bqo=i(Tl),ode=n(Tl,"P",{});var kot=s(ode);xqo=r(kot,"Examples:"),kot.forEach(t),kqo=i(Tl),m(uy.$$.fragment,Tl),Tl.forEach(t),Rqo=i(vl),Ze=n(vl,"DIV",{class:!0});var Zt=s(Ze);m(by.$$.fragment,Zt),Sqo=i(Zt),rde=n(Zt,"P",{});var Rot=s(rde);Pqo=r(Rot,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Rot.forEach(t),$qo=i(Zt),rn=n(Zt,"P",{});var JM=s(rn);Iqo=r(JM,"The model class to instantiate is selected based on the "),tde=n(JM,"CODE",{});var Sot=s(tde);Dqo=r(Sot,"model_type"),Sot.forEach(t),jqo=r(JM,` property of the config object (either
passed as an argument or loaded from `),ade=n(JM,"CODE",{});var Pot=s(ade);Nqo=r(Pot,"pretrained_model_name_or_path"),Pot.forEach(t),qqo=r(JM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nde=n(JM,"CODE",{});var $ot=s(nde);Gqo=r($ot,"pretrained_model_name_or_path"),$ot.forEach(t),Oqo=r(JM,":"),JM.forEach(t),Xqo=i(Zt),vy=n(Zt,"UL",{});var Zke=s(vy);_2=n(Zke,"LI",{});var pwe=s(_2);sde=n(pwe,"STRONG",{});var Iot=s(sde);Vqo=r(Iot,"speech-encoder-decoder"),Iot.forEach(t),zqo=r(pwe," \u2014 "),cN=n(pwe,"A",{href:!0});var Dot=s(cN);Wqo=r(Dot,"SpeechEncoderDecoderModel"),Dot.forEach(t),Qqo=r(pwe," (Speech Encoder decoder model)"),pwe.forEach(t),Hqo=i(Zke),u2=n(Zke,"LI",{});var _we=s(u2);lde=n(_we,"STRONG",{});var jot=s(lde);Uqo=r(jot,"speech_to_text"),jot.forEach(t),Jqo=r(_we," \u2014 "),fN=n(_we,"A",{href:!0});var Not=s(fN);Yqo=r(Not,"Speech2TextForConditionalGeneration"),Not.forEach(t),Kqo=r(_we," (Speech2Text model)"),_we.forEach(t),Zke.forEach(t),Zqo=i(Zt),b2=n(Zt,"P",{});var uwe=s(b2);eGo=r(uwe,"The model is set in evaluation mode by default using "),ide=n(uwe,"CODE",{});var qot=s(ide);oGo=r(qot,"model.eval()"),qot.forEach(t),rGo=r(uwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),dde=n(uwe,"CODE",{});var Got=s(dde);tGo=r(Got,"model.train()"),Got.forEach(t),uwe.forEach(t),aGo=i(Zt),cde=n(Zt,"P",{});var Oot=s(cde);nGo=r(Oot,"Examples:"),Oot.forEach(t),sGo=i(Zt),m(Ty.$$.fragment,Zt),Zt.forEach(t),vl.forEach(t),WBe=i(c),Nd=n(c,"H2",{class:!0});var eRe=s(Nd);v2=n(eRe,"A",{id:!0,class:!0,href:!0});var Xot=s(v2);fde=n(Xot,"SPAN",{});var Vot=s(fde);m(Fy.$$.fragment,Vot),Vot.forEach(t),Xot.forEach(t),lGo=i(eRe),mde=n(eRe,"SPAN",{});var zot=s(mde);iGo=r(zot,"AutoModelForAudioXVector"),zot.forEach(t),eRe.forEach(t),QBe=i(c),mr=n(c,"DIV",{class:!0});var Fl=s(mr);m(Cy.$$.fragment,Fl),dGo=i(Fl),qd=n(Fl,"P",{});var Tz=s(qd);cGo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),gde=n(Tz,"CODE",{});var Wot=s(gde);fGo=r(Wot,"from_pretrained()"),Wot.forEach(t),mGo=r(Tz,"class method or the "),hde=n(Tz,"CODE",{});var Qot=s(hde);gGo=r(Qot,"from_config()"),Qot.forEach(t),hGo=r(Tz,`class
method.`),Tz.forEach(t),pGo=i(Fl),My=n(Fl,"P",{});var oRe=s(My);_Go=r(oRe,"This class cannot be instantiated directly using "),pde=n(oRe,"CODE",{});var Hot=s(pde);uGo=r(Hot,"__init__()"),Hot.forEach(t),bGo=r(oRe," (throws an error)."),oRe.forEach(t),vGo=i(Fl),nt=n(Fl,"DIV",{class:!0});var Cl=s(nt);m(Ey.$$.fragment,Cl),TGo=i(Cl),_de=n(Cl,"P",{});var Uot=s(_de);FGo=r(Uot,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),Uot.forEach(t),CGo=i(Cl),Gd=n(Cl,"P",{});var Fz=s(Gd);MGo=r(Fz,`Note:
Loading a model from its configuration file does `),ude=n(Fz,"STRONG",{});var Jot=s(ude);EGo=r(Jot,"not"),Jot.forEach(t),yGo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),bde=n(Fz,"CODE",{});var Yot=s(bde);wGo=r(Yot,"from_pretrained()"),Yot.forEach(t),AGo=r(Fz,"to load the model weights."),Fz.forEach(t),LGo=i(Cl),vde=n(Cl,"P",{});var Kot=s(vde);BGo=r(Kot,"Examples:"),Kot.forEach(t),xGo=i(Cl),m(yy.$$.fragment,Cl),Cl.forEach(t),kGo=i(Fl),eo=n(Fl,"DIV",{class:!0});var ea=s(eo);m(wy.$$.fragment,ea),RGo=i(ea),Tde=n(ea,"P",{});var Zot=s(Tde);SGo=r(Zot,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),Zot.forEach(t),PGo=i(ea),tn=n(ea,"P",{});var YM=s(tn);$Go=r(YM,"The model class to instantiate is selected based on the "),Fde=n(YM,"CODE",{});var ert=s(Fde);IGo=r(ert,"model_type"),ert.forEach(t),DGo=r(YM,` property of the config object (either
passed as an argument or loaded from `),Cde=n(YM,"CODE",{});var ort=s(Cde);jGo=r(ort,"pretrained_model_name_or_path"),ort.forEach(t),NGo=r(YM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mde=n(YM,"CODE",{});var rrt=s(Mde);qGo=r(rrt,"pretrained_model_name_or_path"),rrt.forEach(t),GGo=r(YM,":"),YM.forEach(t),OGo=i(ea),an=n(ea,"UL",{});var KM=s(an);T2=n(KM,"LI",{});var bwe=s(T2);Ede=n(bwe,"STRONG",{});var trt=s(Ede);XGo=r(trt,"data2vec-audio"),trt.forEach(t),VGo=r(bwe," \u2014 "),mN=n(bwe,"A",{href:!0});var art=s(mN);zGo=r(art,"Data2VecAudioForXVector"),art.forEach(t),WGo=r(bwe," (Data2VecAudio model)"),bwe.forEach(t),QGo=i(KM),F2=n(KM,"LI",{});var vwe=s(F2);yde=n(vwe,"STRONG",{});var nrt=s(yde);HGo=r(nrt,"unispeech-sat"),nrt.forEach(t),UGo=r(vwe," \u2014 "),gN=n(vwe,"A",{href:!0});var srt=s(gN);JGo=r(srt,"UniSpeechSatForXVector"),srt.forEach(t),YGo=r(vwe," (UniSpeechSat model)"),vwe.forEach(t),KGo=i(KM),C2=n(KM,"LI",{});var Twe=s(C2);wde=n(Twe,"STRONG",{});var lrt=s(wde);ZGo=r(lrt,"wav2vec2"),lrt.forEach(t),eOo=r(Twe," \u2014 "),hN=n(Twe,"A",{href:!0});var irt=s(hN);oOo=r(irt,"Wav2Vec2ForXVector"),irt.forEach(t),rOo=r(Twe," (Wav2Vec2 model)"),Twe.forEach(t),tOo=i(KM),M2=n(KM,"LI",{});var Fwe=s(M2);Ade=n(Fwe,"STRONG",{});var drt=s(Ade);aOo=r(drt,"wavlm"),drt.forEach(t),nOo=r(Fwe," \u2014 "),pN=n(Fwe,"A",{href:!0});var crt=s(pN);sOo=r(crt,"WavLMForXVector"),crt.forEach(t),lOo=r(Fwe," (WavLM model)"),Fwe.forEach(t),KM.forEach(t),iOo=i(ea),E2=n(ea,"P",{});var Cwe=s(E2);dOo=r(Cwe,"The model is set in evaluation mode by default using "),Lde=n(Cwe,"CODE",{});var frt=s(Lde);cOo=r(frt,"model.eval()"),frt.forEach(t),fOo=r(Cwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bde=n(Cwe,"CODE",{});var mrt=s(Bde);mOo=r(mrt,"model.train()"),mrt.forEach(t),Cwe.forEach(t),gOo=i(ea),xde=n(ea,"P",{});var grt=s(xde);hOo=r(grt,"Examples:"),grt.forEach(t),pOo=i(ea),m(Ay.$$.fragment,ea),ea.forEach(t),Fl.forEach(t),HBe=i(c),Od=n(c,"H2",{class:!0});var rRe=s(Od);y2=n(rRe,"A",{id:!0,class:!0,href:!0});var hrt=s(y2);kde=n(hrt,"SPAN",{});var prt=s(kde);m(Ly.$$.fragment,prt),prt.forEach(t),hrt.forEach(t),_Oo=i(rRe),Rde=n(rRe,"SPAN",{});var _rt=s(Rde);uOo=r(_rt,"AutoModelForMaskedImageModeling"),_rt.forEach(t),rRe.forEach(t),UBe=i(c),gr=n(c,"DIV",{class:!0});var Ml=s(gr);m(By.$$.fragment,Ml),bOo=i(Ml),Xd=n(Ml,"P",{});var Cz=s(Xd);vOo=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Sde=n(Cz,"CODE",{});var urt=s(Sde);TOo=r(urt,"from_pretrained()"),urt.forEach(t),FOo=r(Cz,"class method or the "),Pde=n(Cz,"CODE",{});var brt=s(Pde);COo=r(brt,"from_config()"),brt.forEach(t),MOo=r(Cz,`class
method.`),Cz.forEach(t),EOo=i(Ml),xy=n(Ml,"P",{});var tRe=s(xy);yOo=r(tRe,"This class cannot be instantiated directly using "),$de=n(tRe,"CODE",{});var vrt=s($de);wOo=r(vrt,"__init__()"),vrt.forEach(t),AOo=r(tRe," (throws an error)."),tRe.forEach(t),LOo=i(Ml),st=n(Ml,"DIV",{class:!0});var El=s(st);m(ky.$$.fragment,El),BOo=i(El),Ide=n(El,"P",{});var Trt=s(Ide);xOo=r(Trt,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),Trt.forEach(t),kOo=i(El),Vd=n(El,"P",{});var Mz=s(Vd);ROo=r(Mz,`Note:
Loading a model from its configuration file does `),Dde=n(Mz,"STRONG",{});var Frt=s(Dde);SOo=r(Frt,"not"),Frt.forEach(t),POo=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),jde=n(Mz,"CODE",{});var Crt=s(jde);$Oo=r(Crt,"from_pretrained()"),Crt.forEach(t),IOo=r(Mz,"to load the model weights."),Mz.forEach(t),DOo=i(El),Nde=n(El,"P",{});var Mrt=s(Nde);jOo=r(Mrt,"Examples:"),Mrt.forEach(t),NOo=i(El),m(Ry.$$.fragment,El),El.forEach(t),qOo=i(Ml),oo=n(Ml,"DIV",{class:!0});var oa=s(oo);m(Sy.$$.fragment,oa),GOo=i(oa),qde=n(oa,"P",{});var Ert=s(qde);OOo=r(Ert,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),Ert.forEach(t),XOo=i(oa),nn=n(oa,"P",{});var ZM=s(nn);VOo=r(ZM,"The model class to instantiate is selected based on the "),Gde=n(ZM,"CODE",{});var yrt=s(Gde);zOo=r(yrt,"model_type"),yrt.forEach(t),WOo=r(ZM,` property of the config object (either
passed as an argument or loaded from `),Ode=n(ZM,"CODE",{});var wrt=s(Ode);QOo=r(wrt,"pretrained_model_name_or_path"),wrt.forEach(t),HOo=r(ZM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xde=n(ZM,"CODE",{});var Art=s(Xde);UOo=r(Art,"pretrained_model_name_or_path"),Art.forEach(t),JOo=r(ZM,":"),ZM.forEach(t),YOo=i(oa),zd=n(oa,"UL",{});var Ez=s(zd);w2=n(Ez,"LI",{});var Mwe=s(w2);Vde=n(Mwe,"STRONG",{});var Lrt=s(Vde);KOo=r(Lrt,"deit"),Lrt.forEach(t),ZOo=r(Mwe," \u2014 "),_N=n(Mwe,"A",{href:!0});var Brt=s(_N);eXo=r(Brt,"DeiTForMaskedImageModeling"),Brt.forEach(t),oXo=r(Mwe," (DeiT model)"),Mwe.forEach(t),rXo=i(Ez),A2=n(Ez,"LI",{});var Ewe=s(A2);zde=n(Ewe,"STRONG",{});var xrt=s(zde);tXo=r(xrt,"swin"),xrt.forEach(t),aXo=r(Ewe," \u2014 "),uN=n(Ewe,"A",{href:!0});var krt=s(uN);nXo=r(krt,"SwinForMaskedImageModeling"),krt.forEach(t),sXo=r(Ewe," (Swin model)"),Ewe.forEach(t),lXo=i(Ez),L2=n(Ez,"LI",{});var ywe=s(L2);Wde=n(ywe,"STRONG",{});var Rrt=s(Wde);iXo=r(Rrt,"vit"),Rrt.forEach(t),dXo=r(ywe," \u2014 "),bN=n(ywe,"A",{href:!0});var Srt=s(bN);cXo=r(Srt,"ViTForMaskedImageModeling"),Srt.forEach(t),fXo=r(ywe," (ViT model)"),ywe.forEach(t),Ez.forEach(t),mXo=i(oa),B2=n(oa,"P",{});var wwe=s(B2);gXo=r(wwe,"The model is set in evaluation mode by default using "),Qde=n(wwe,"CODE",{});var Prt=s(Qde);hXo=r(Prt,"model.eval()"),Prt.forEach(t),pXo=r(wwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hde=n(wwe,"CODE",{});var $rt=s(Hde);_Xo=r($rt,"model.train()"),$rt.forEach(t),wwe.forEach(t),uXo=i(oa),Ude=n(oa,"P",{});var Irt=s(Ude);bXo=r(Irt,"Examples:"),Irt.forEach(t),vXo=i(oa),m(Py.$$.fragment,oa),oa.forEach(t),Ml.forEach(t),JBe=i(c),Wd=n(c,"H2",{class:!0});var aRe=s(Wd);x2=n(aRe,"A",{id:!0,class:!0,href:!0});var Drt=s(x2);Jde=n(Drt,"SPAN",{});var jrt=s(Jde);m($y.$$.fragment,jrt),jrt.forEach(t),Drt.forEach(t),TXo=i(aRe),Yde=n(aRe,"SPAN",{});var Nrt=s(Yde);FXo=r(Nrt,"AutoModelForObjectDetection"),Nrt.forEach(t),aRe.forEach(t),YBe=i(c),hr=n(c,"DIV",{class:!0});var yl=s(hr);m(Iy.$$.fragment,yl),CXo=i(yl),Qd=n(yl,"P",{});var yz=s(Qd);MXo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Kde=n(yz,"CODE",{});var qrt=s(Kde);EXo=r(qrt,"from_pretrained()"),qrt.forEach(t),yXo=r(yz,"class method or the "),Zde=n(yz,"CODE",{});var Grt=s(Zde);wXo=r(Grt,"from_config()"),Grt.forEach(t),AXo=r(yz,`class
method.`),yz.forEach(t),LXo=i(yl),Dy=n(yl,"P",{});var nRe=s(Dy);BXo=r(nRe,"This class cannot be instantiated directly using "),ece=n(nRe,"CODE",{});var Ort=s(ece);xXo=r(Ort,"__init__()"),Ort.forEach(t),kXo=r(nRe," (throws an error)."),nRe.forEach(t),RXo=i(yl),lt=n(yl,"DIV",{class:!0});var wl=s(lt);m(jy.$$.fragment,wl),SXo=i(wl),oce=n(wl,"P",{});var Xrt=s(oce);PXo=r(Xrt,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),Xrt.forEach(t),$Xo=i(wl),Hd=n(wl,"P",{});var wz=s(Hd);IXo=r(wz,`Note:
Loading a model from its configuration file does `),rce=n(wz,"STRONG",{});var Vrt=s(rce);DXo=r(Vrt,"not"),Vrt.forEach(t),jXo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tce=n(wz,"CODE",{});var zrt=s(tce);NXo=r(zrt,"from_pretrained()"),zrt.forEach(t),qXo=r(wz,"to load the model weights."),wz.forEach(t),GXo=i(wl),ace=n(wl,"P",{});var Wrt=s(ace);OXo=r(Wrt,"Examples:"),Wrt.forEach(t),XXo=i(wl),m(Ny.$$.fragment,wl),wl.forEach(t),VXo=i(yl),ro=n(yl,"DIV",{class:!0});var ra=s(ro);m(qy.$$.fragment,ra),zXo=i(ra),nce=n(ra,"P",{});var Qrt=s(nce);WXo=r(Qrt,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),Qrt.forEach(t),QXo=i(ra),sn=n(ra,"P",{});var e4=s(sn);HXo=r(e4,"The model class to instantiate is selected based on the "),sce=n(e4,"CODE",{});var Hrt=s(sce);UXo=r(Hrt,"model_type"),Hrt.forEach(t),JXo=r(e4,` property of the config object (either
passed as an argument or loaded from `),lce=n(e4,"CODE",{});var Urt=s(lce);YXo=r(Urt,"pretrained_model_name_or_path"),Urt.forEach(t),KXo=r(e4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ice=n(e4,"CODE",{});var Jrt=s(ice);ZXo=r(Jrt,"pretrained_model_name_or_path"),Jrt.forEach(t),eVo=r(e4,":"),e4.forEach(t),oVo=i(ra),dce=n(ra,"UL",{});var Yrt=s(dce);k2=n(Yrt,"LI",{});var Awe=s(k2);cce=n(Awe,"STRONG",{});var Krt=s(cce);rVo=r(Krt,"detr"),Krt.forEach(t),tVo=r(Awe," \u2014 "),vN=n(Awe,"A",{href:!0});var Zrt=s(vN);aVo=r(Zrt,"DetrForObjectDetection"),Zrt.forEach(t),nVo=r(Awe," (DETR model)"),Awe.forEach(t),Yrt.forEach(t),sVo=i(ra),R2=n(ra,"P",{});var Lwe=s(R2);lVo=r(Lwe,"The model is set in evaluation mode by default using "),fce=n(Lwe,"CODE",{});var ett=s(fce);iVo=r(ett,"model.eval()"),ett.forEach(t),dVo=r(Lwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mce=n(Lwe,"CODE",{});var ott=s(mce);cVo=r(ott,"model.train()"),ott.forEach(t),Lwe.forEach(t),fVo=i(ra),gce=n(ra,"P",{});var rtt=s(gce);mVo=r(rtt,"Examples:"),rtt.forEach(t),gVo=i(ra),m(Gy.$$.fragment,ra),ra.forEach(t),yl.forEach(t),KBe=i(c),Ud=n(c,"H2",{class:!0});var sRe=s(Ud);S2=n(sRe,"A",{id:!0,class:!0,href:!0});var ttt=s(S2);hce=n(ttt,"SPAN",{});var att=s(hce);m(Oy.$$.fragment,att),att.forEach(t),ttt.forEach(t),hVo=i(sRe),pce=n(sRe,"SPAN",{});var ntt=s(pce);pVo=r(ntt,"AutoModelForImageSegmentation"),ntt.forEach(t),sRe.forEach(t),ZBe=i(c),pr=n(c,"DIV",{class:!0});var Al=s(pr);m(Xy.$$.fragment,Al),_Vo=i(Al),Jd=n(Al,"P",{});var Az=s(Jd);uVo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),_ce=n(Az,"CODE",{});var stt=s(_ce);bVo=r(stt,"from_pretrained()"),stt.forEach(t),vVo=r(Az,"class method or the "),uce=n(Az,"CODE",{});var ltt=s(uce);TVo=r(ltt,"from_config()"),ltt.forEach(t),FVo=r(Az,`class
method.`),Az.forEach(t),CVo=i(Al),Vy=n(Al,"P",{});var lRe=s(Vy);MVo=r(lRe,"This class cannot be instantiated directly using "),bce=n(lRe,"CODE",{});var itt=s(bce);EVo=r(itt,"__init__()"),itt.forEach(t),yVo=r(lRe," (throws an error)."),lRe.forEach(t),wVo=i(Al),it=n(Al,"DIV",{class:!0});var Ll=s(it);m(zy.$$.fragment,Ll),AVo=i(Ll),vce=n(Ll,"P",{});var dtt=s(vce);LVo=r(dtt,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),dtt.forEach(t),BVo=i(Ll),Yd=n(Ll,"P",{});var Lz=s(Yd);xVo=r(Lz,`Note:
Loading a model from its configuration file does `),Tce=n(Lz,"STRONG",{});var ctt=s(Tce);kVo=r(ctt,"not"),ctt.forEach(t),RVo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fce=n(Lz,"CODE",{});var ftt=s(Fce);SVo=r(ftt,"from_pretrained()"),ftt.forEach(t),PVo=r(Lz,"to load the model weights."),Lz.forEach(t),$Vo=i(Ll),Cce=n(Ll,"P",{});var mtt=s(Cce);IVo=r(mtt,"Examples:"),mtt.forEach(t),DVo=i(Ll),m(Wy.$$.fragment,Ll),Ll.forEach(t),jVo=i(Al),to=n(Al,"DIV",{class:!0});var ta=s(to);m(Qy.$$.fragment,ta),NVo=i(ta),Mce=n(ta,"P",{});var gtt=s(Mce);qVo=r(gtt,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gtt.forEach(t),GVo=i(ta),ln=n(ta,"P",{});var o4=s(ln);OVo=r(o4,"The model class to instantiate is selected based on the "),Ece=n(o4,"CODE",{});var htt=s(Ece);XVo=r(htt,"model_type"),htt.forEach(t),VVo=r(o4,` property of the config object (either
passed as an argument or loaded from `),yce=n(o4,"CODE",{});var ptt=s(yce);zVo=r(ptt,"pretrained_model_name_or_path"),ptt.forEach(t),WVo=r(o4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wce=n(o4,"CODE",{});var _tt=s(wce);QVo=r(_tt,"pretrained_model_name_or_path"),_tt.forEach(t),HVo=r(o4,":"),o4.forEach(t),UVo=i(ta),Ace=n(ta,"UL",{});var utt=s(Ace);P2=n(utt,"LI",{});var Bwe=s(P2);Lce=n(Bwe,"STRONG",{});var btt=s(Lce);JVo=r(btt,"detr"),btt.forEach(t),YVo=r(Bwe," \u2014 "),TN=n(Bwe,"A",{href:!0});var vtt=s(TN);KVo=r(vtt,"DetrForSegmentation"),vtt.forEach(t),ZVo=r(Bwe," (DETR model)"),Bwe.forEach(t),utt.forEach(t),ezo=i(ta),$2=n(ta,"P",{});var xwe=s($2);ozo=r(xwe,"The model is set in evaluation mode by default using "),Bce=n(xwe,"CODE",{});var Ttt=s(Bce);rzo=r(Ttt,"model.eval()"),Ttt.forEach(t),tzo=r(xwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),xce=n(xwe,"CODE",{});var Ftt=s(xce);azo=r(Ftt,"model.train()"),Ftt.forEach(t),xwe.forEach(t),nzo=i(ta),kce=n(ta,"P",{});var Ctt=s(kce);szo=r(Ctt,"Examples:"),Ctt.forEach(t),lzo=i(ta),m(Hy.$$.fragment,ta),ta.forEach(t),Al.forEach(t),exe=i(c),Kd=n(c,"H2",{class:!0});var iRe=s(Kd);I2=n(iRe,"A",{id:!0,class:!0,href:!0});var Mtt=s(I2);Rce=n(Mtt,"SPAN",{});var Ett=s(Rce);m(Uy.$$.fragment,Ett),Ett.forEach(t),Mtt.forEach(t),izo=i(iRe),Sce=n(iRe,"SPAN",{});var ytt=s(Sce);dzo=r(ytt,"AutoModelForSemanticSegmentation"),ytt.forEach(t),iRe.forEach(t),oxe=i(c),_r=n(c,"DIV",{class:!0});var Bl=s(_r);m(Jy.$$.fragment,Bl),czo=i(Bl),Zd=n(Bl,"P",{});var Bz=s(Zd);fzo=r(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Pce=n(Bz,"CODE",{});var wtt=s(Pce);mzo=r(wtt,"from_pretrained()"),wtt.forEach(t),gzo=r(Bz,"class method or the "),$ce=n(Bz,"CODE",{});var Att=s($ce);hzo=r(Att,"from_config()"),Att.forEach(t),pzo=r(Bz,`class
method.`),Bz.forEach(t),_zo=i(Bl),Yy=n(Bl,"P",{});var dRe=s(Yy);uzo=r(dRe,"This class cannot be instantiated directly using "),Ice=n(dRe,"CODE",{});var Ltt=s(Ice);bzo=r(Ltt,"__init__()"),Ltt.forEach(t),vzo=r(dRe," (throws an error)."),dRe.forEach(t),Tzo=i(Bl),dt=n(Bl,"DIV",{class:!0});var xl=s(dt);m(Ky.$$.fragment,xl),Fzo=i(xl),Dce=n(xl,"P",{});var Btt=s(Dce);Czo=r(Btt,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Btt.forEach(t),Mzo=i(xl),ec=n(xl,"P",{});var xz=s(ec);Ezo=r(xz,`Note:
Loading a model from its configuration file does `),jce=n(xz,"STRONG",{});var xtt=s(jce);yzo=r(xtt,"not"),xtt.forEach(t),wzo=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Nce=n(xz,"CODE",{});var ktt=s(Nce);Azo=r(ktt,"from_pretrained()"),ktt.forEach(t),Lzo=r(xz,"to load the model weights."),xz.forEach(t),Bzo=i(xl),qce=n(xl,"P",{});var Rtt=s(qce);xzo=r(Rtt,"Examples:"),Rtt.forEach(t),kzo=i(xl),m(Zy.$$.fragment,xl),xl.forEach(t),Rzo=i(Bl),ao=n(Bl,"DIV",{class:!0});var aa=s(ao);m(ew.$$.fragment,aa),Szo=i(aa),Gce=n(aa,"P",{});var Stt=s(Gce);Pzo=r(Stt,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),Stt.forEach(t),$zo=i(aa),dn=n(aa,"P",{});var r4=s(dn);Izo=r(r4,"The model class to instantiate is selected based on the "),Oce=n(r4,"CODE",{});var Ptt=s(Oce);Dzo=r(Ptt,"model_type"),Ptt.forEach(t),jzo=r(r4,` property of the config object (either
passed as an argument or loaded from `),Xce=n(r4,"CODE",{});var $tt=s(Xce);Nzo=r($tt,"pretrained_model_name_or_path"),$tt.forEach(t),qzo=r(r4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vce=n(r4,"CODE",{});var Itt=s(Vce);Gzo=r(Itt,"pretrained_model_name_or_path"),Itt.forEach(t),Ozo=r(r4,":"),r4.forEach(t),Xzo=i(aa),ow=n(aa,"UL",{});var cRe=s(ow);D2=n(cRe,"LI",{});var kwe=s(D2);zce=n(kwe,"STRONG",{});var Dtt=s(zce);Vzo=r(Dtt,"beit"),Dtt.forEach(t),zzo=r(kwe," \u2014 "),FN=n(kwe,"A",{href:!0});var jtt=s(FN);Wzo=r(jtt,"BeitForSemanticSegmentation"),jtt.forEach(t),Qzo=r(kwe," (BEiT model)"),kwe.forEach(t),Hzo=i(cRe),j2=n(cRe,"LI",{});var Rwe=s(j2);Wce=n(Rwe,"STRONG",{});var Ntt=s(Wce);Uzo=r(Ntt,"segformer"),Ntt.forEach(t),Jzo=r(Rwe," \u2014 "),CN=n(Rwe,"A",{href:!0});var qtt=s(CN);Yzo=r(qtt,"SegformerForSemanticSegmentation"),qtt.forEach(t),Kzo=r(Rwe," (SegFormer model)"),Rwe.forEach(t),cRe.forEach(t),Zzo=i(aa),N2=n(aa,"P",{});var Swe=s(N2);eWo=r(Swe,"The model is set in evaluation mode by default using "),Qce=n(Swe,"CODE",{});var Gtt=s(Qce);oWo=r(Gtt,"model.eval()"),Gtt.forEach(t),rWo=r(Swe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hce=n(Swe,"CODE",{});var Ott=s(Hce);tWo=r(Ott,"model.train()"),Ott.forEach(t),Swe.forEach(t),aWo=i(aa),Uce=n(aa,"P",{});var Xtt=s(Uce);nWo=r(Xtt,"Examples:"),Xtt.forEach(t),sWo=i(aa),m(rw.$$.fragment,aa),aa.forEach(t),Bl.forEach(t),rxe=i(c),oc=n(c,"H2",{class:!0});var fRe=s(oc);q2=n(fRe,"A",{id:!0,class:!0,href:!0});var Vtt=s(q2);Jce=n(Vtt,"SPAN",{});var ztt=s(Jce);m(tw.$$.fragment,ztt),ztt.forEach(t),Vtt.forEach(t),lWo=i(fRe),Yce=n(fRe,"SPAN",{});var Wtt=s(Yce);iWo=r(Wtt,"TFAutoModel"),Wtt.forEach(t),fRe.forEach(t),txe=i(c),ur=n(c,"DIV",{class:!0});var kl=s(ur);m(aw.$$.fragment,kl),dWo=i(kl),rc=n(kl,"P",{});var kz=s(rc);cWo=r(kz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Kce=n(kz,"CODE",{});var Qtt=s(Kce);fWo=r(Qtt,"from_pretrained()"),Qtt.forEach(t),mWo=r(kz,"class method or the "),Zce=n(kz,"CODE",{});var Htt=s(Zce);gWo=r(Htt,"from_config()"),Htt.forEach(t),hWo=r(kz,`class
method.`),kz.forEach(t),pWo=i(kl),nw=n(kl,"P",{});var mRe=s(nw);_Wo=r(mRe,"This class cannot be instantiated directly using "),efe=n(mRe,"CODE",{});var Utt=s(efe);uWo=r(Utt,"__init__()"),Utt.forEach(t),bWo=r(mRe," (throws an error)."),mRe.forEach(t),vWo=i(kl),ct=n(kl,"DIV",{class:!0});var Rl=s(ct);m(sw.$$.fragment,Rl),TWo=i(Rl),ofe=n(Rl,"P",{});var Jtt=s(ofe);FWo=r(Jtt,"Instantiates one of the base model classes of the library from a configuration."),Jtt.forEach(t),CWo=i(Rl),tc=n(Rl,"P",{});var Rz=s(tc);MWo=r(Rz,`Note:
Loading a model from its configuration file does `),rfe=n(Rz,"STRONG",{});var Ytt=s(rfe);EWo=r(Ytt,"not"),Ytt.forEach(t),yWo=r(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tfe=n(Rz,"CODE",{});var Ktt=s(tfe);wWo=r(Ktt,"from_pretrained()"),Ktt.forEach(t),AWo=r(Rz,"to load the model weights."),Rz.forEach(t),LWo=i(Rl),afe=n(Rl,"P",{});var Ztt=s(afe);BWo=r(Ztt,"Examples:"),Ztt.forEach(t),xWo=i(Rl),m(lw.$$.fragment,Rl),Rl.forEach(t),kWo=i(kl),go=n(kl,"DIV",{class:!0});var ca=s(go);m(iw.$$.fragment,ca),RWo=i(ca),nfe=n(ca,"P",{});var eat=s(nfe);SWo=r(eat,"Instantiate one of the base model classes of the library from a pretrained model."),eat.forEach(t),PWo=i(ca),cn=n(ca,"P",{});var t4=s(cn);$Wo=r(t4,"The model class to instantiate is selected based on the "),sfe=n(t4,"CODE",{});var oat=s(sfe);IWo=r(oat,"model_type"),oat.forEach(t),DWo=r(t4,` property of the config object (either
passed as an argument or loaded from `),lfe=n(t4,"CODE",{});var rat=s(lfe);jWo=r(rat,"pretrained_model_name_or_path"),rat.forEach(t),NWo=r(t4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ife=n(t4,"CODE",{});var tat=s(ife);qWo=r(tat,"pretrained_model_name_or_path"),tat.forEach(t),GWo=r(t4,":"),t4.forEach(t),OWo=i(ca),B=n(ca,"UL",{});var x=s(B);G2=n(x,"LI",{});var Pwe=s(G2);dfe=n(Pwe,"STRONG",{});var aat=s(dfe);XWo=r(aat,"albert"),aat.forEach(t),VWo=r(Pwe," \u2014 "),MN=n(Pwe,"A",{href:!0});var nat=s(MN);zWo=r(nat,"TFAlbertModel"),nat.forEach(t),WWo=r(Pwe," (ALBERT model)"),Pwe.forEach(t),QWo=i(x),O2=n(x,"LI",{});var $we=s(O2);cfe=n($we,"STRONG",{});var sat=s(cfe);HWo=r(sat,"bart"),sat.forEach(t),UWo=r($we," \u2014 "),EN=n($we,"A",{href:!0});var lat=s(EN);JWo=r(lat,"TFBartModel"),lat.forEach(t),YWo=r($we," (BART model)"),$we.forEach(t),KWo=i(x),X2=n(x,"LI",{});var Iwe=s(X2);ffe=n(Iwe,"STRONG",{});var iat=s(ffe);ZWo=r(iat,"bert"),iat.forEach(t),eQo=r(Iwe," \u2014 "),yN=n(Iwe,"A",{href:!0});var dat=s(yN);oQo=r(dat,"TFBertModel"),dat.forEach(t),rQo=r(Iwe," (BERT model)"),Iwe.forEach(t),tQo=i(x),V2=n(x,"LI",{});var Dwe=s(V2);mfe=n(Dwe,"STRONG",{});var cat=s(mfe);aQo=r(cat,"blenderbot"),cat.forEach(t),nQo=r(Dwe," \u2014 "),wN=n(Dwe,"A",{href:!0});var fat=s(wN);sQo=r(fat,"TFBlenderbotModel"),fat.forEach(t),lQo=r(Dwe," (Blenderbot model)"),Dwe.forEach(t),iQo=i(x),z2=n(x,"LI",{});var jwe=s(z2);gfe=n(jwe,"STRONG",{});var mat=s(gfe);dQo=r(mat,"blenderbot-small"),mat.forEach(t),cQo=r(jwe," \u2014 "),AN=n(jwe,"A",{href:!0});var gat=s(AN);fQo=r(gat,"TFBlenderbotSmallModel"),gat.forEach(t),mQo=r(jwe," (BlenderbotSmall model)"),jwe.forEach(t),gQo=i(x),W2=n(x,"LI",{});var Nwe=s(W2);hfe=n(Nwe,"STRONG",{});var hat=s(hfe);hQo=r(hat,"camembert"),hat.forEach(t),pQo=r(Nwe," \u2014 "),LN=n(Nwe,"A",{href:!0});var pat=s(LN);_Qo=r(pat,"TFCamembertModel"),pat.forEach(t),uQo=r(Nwe," (CamemBERT model)"),Nwe.forEach(t),bQo=i(x),Q2=n(x,"LI",{});var qwe=s(Q2);pfe=n(qwe,"STRONG",{});var _at=s(pfe);vQo=r(_at,"clip"),_at.forEach(t),TQo=r(qwe," \u2014 "),BN=n(qwe,"A",{href:!0});var uat=s(BN);FQo=r(uat,"TFCLIPModel"),uat.forEach(t),CQo=r(qwe," (CLIP model)"),qwe.forEach(t),MQo=i(x),H2=n(x,"LI",{});var Gwe=s(H2);_fe=n(Gwe,"STRONG",{});var bat=s(_fe);EQo=r(bat,"convbert"),bat.forEach(t),yQo=r(Gwe," \u2014 "),xN=n(Gwe,"A",{href:!0});var vat=s(xN);wQo=r(vat,"TFConvBertModel"),vat.forEach(t),AQo=r(Gwe," (ConvBERT model)"),Gwe.forEach(t),LQo=i(x),U2=n(x,"LI",{});var Owe=s(U2);ufe=n(Owe,"STRONG",{});var Tat=s(ufe);BQo=r(Tat,"convnext"),Tat.forEach(t),xQo=r(Owe," \u2014 "),kN=n(Owe,"A",{href:!0});var Fat=s(kN);kQo=r(Fat,"TFConvNextModel"),Fat.forEach(t),RQo=r(Owe," (ConvNext model)"),Owe.forEach(t),SQo=i(x),J2=n(x,"LI",{});var Xwe=s(J2);bfe=n(Xwe,"STRONG",{});var Cat=s(bfe);PQo=r(Cat,"ctrl"),Cat.forEach(t),$Qo=r(Xwe," \u2014 "),RN=n(Xwe,"A",{href:!0});var Mat=s(RN);IQo=r(Mat,"TFCTRLModel"),Mat.forEach(t),DQo=r(Xwe," (CTRL model)"),Xwe.forEach(t),jQo=i(x),Y2=n(x,"LI",{});var Vwe=s(Y2);vfe=n(Vwe,"STRONG",{});var Eat=s(vfe);NQo=r(Eat,"deberta"),Eat.forEach(t),qQo=r(Vwe," \u2014 "),SN=n(Vwe,"A",{href:!0});var yat=s(SN);GQo=r(yat,"TFDebertaModel"),yat.forEach(t),OQo=r(Vwe," (DeBERTa model)"),Vwe.forEach(t),XQo=i(x),K2=n(x,"LI",{});var zwe=s(K2);Tfe=n(zwe,"STRONG",{});var wat=s(Tfe);VQo=r(wat,"deberta-v2"),wat.forEach(t),zQo=r(zwe," \u2014 "),PN=n(zwe,"A",{href:!0});var Aat=s(PN);WQo=r(Aat,"TFDebertaV2Model"),Aat.forEach(t),QQo=r(zwe," (DeBERTa-v2 model)"),zwe.forEach(t),HQo=i(x),Z2=n(x,"LI",{});var Wwe=s(Z2);Ffe=n(Wwe,"STRONG",{});var Lat=s(Ffe);UQo=r(Lat,"distilbert"),Lat.forEach(t),JQo=r(Wwe," \u2014 "),$N=n(Wwe,"A",{href:!0});var Bat=s($N);YQo=r(Bat,"TFDistilBertModel"),Bat.forEach(t),KQo=r(Wwe," (DistilBERT model)"),Wwe.forEach(t),ZQo=i(x),ev=n(x,"LI",{});var Qwe=s(ev);Cfe=n(Qwe,"STRONG",{});var xat=s(Cfe);eHo=r(xat,"dpr"),xat.forEach(t),oHo=r(Qwe," \u2014 "),IN=n(Qwe,"A",{href:!0});var kat=s(IN);rHo=r(kat,"TFDPRQuestionEncoder"),kat.forEach(t),tHo=r(Qwe," (DPR model)"),Qwe.forEach(t),aHo=i(x),ov=n(x,"LI",{});var Hwe=s(ov);Mfe=n(Hwe,"STRONG",{});var Rat=s(Mfe);nHo=r(Rat,"electra"),Rat.forEach(t),sHo=r(Hwe," \u2014 "),DN=n(Hwe,"A",{href:!0});var Sat=s(DN);lHo=r(Sat,"TFElectraModel"),Sat.forEach(t),iHo=r(Hwe," (ELECTRA model)"),Hwe.forEach(t),dHo=i(x),rv=n(x,"LI",{});var Uwe=s(rv);Efe=n(Uwe,"STRONG",{});var Pat=s(Efe);cHo=r(Pat,"flaubert"),Pat.forEach(t),fHo=r(Uwe," \u2014 "),jN=n(Uwe,"A",{href:!0});var $at=s(jN);mHo=r($at,"TFFlaubertModel"),$at.forEach(t),gHo=r(Uwe," (FlauBERT model)"),Uwe.forEach(t),hHo=i(x),$s=n(x,"LI",{});var g8=s($s);yfe=n(g8,"STRONG",{});var Iat=s(yfe);pHo=r(Iat,"funnel"),Iat.forEach(t),_Ho=r(g8," \u2014 "),NN=n(g8,"A",{href:!0});var Dat=s(NN);uHo=r(Dat,"TFFunnelModel"),Dat.forEach(t),bHo=r(g8," or "),qN=n(g8,"A",{href:!0});var jat=s(qN);vHo=r(jat,"TFFunnelBaseModel"),jat.forEach(t),THo=r(g8," (Funnel Transformer model)"),g8.forEach(t),FHo=i(x),tv=n(x,"LI",{});var Jwe=s(tv);wfe=n(Jwe,"STRONG",{});var Nat=s(wfe);CHo=r(Nat,"gpt2"),Nat.forEach(t),MHo=r(Jwe," \u2014 "),GN=n(Jwe,"A",{href:!0});var qat=s(GN);EHo=r(qat,"TFGPT2Model"),qat.forEach(t),yHo=r(Jwe," (OpenAI GPT-2 model)"),Jwe.forEach(t),wHo=i(x),av=n(x,"LI",{});var Ywe=s(av);Afe=n(Ywe,"STRONG",{});var Gat=s(Afe);AHo=r(Gat,"hubert"),Gat.forEach(t),LHo=r(Ywe," \u2014 "),ON=n(Ywe,"A",{href:!0});var Oat=s(ON);BHo=r(Oat,"TFHubertModel"),Oat.forEach(t),xHo=r(Ywe," (Hubert model)"),Ywe.forEach(t),kHo=i(x),nv=n(x,"LI",{});var Kwe=s(nv);Lfe=n(Kwe,"STRONG",{});var Xat=s(Lfe);RHo=r(Xat,"layoutlm"),Xat.forEach(t),SHo=r(Kwe," \u2014 "),XN=n(Kwe,"A",{href:!0});var Vat=s(XN);PHo=r(Vat,"TFLayoutLMModel"),Vat.forEach(t),$Ho=r(Kwe," (LayoutLM model)"),Kwe.forEach(t),IHo=i(x),sv=n(x,"LI",{});var Zwe=s(sv);Bfe=n(Zwe,"STRONG",{});var zat=s(Bfe);DHo=r(zat,"led"),zat.forEach(t),jHo=r(Zwe," \u2014 "),VN=n(Zwe,"A",{href:!0});var Wat=s(VN);NHo=r(Wat,"TFLEDModel"),Wat.forEach(t),qHo=r(Zwe," (LED model)"),Zwe.forEach(t),GHo=i(x),lv=n(x,"LI",{});var e6e=s(lv);xfe=n(e6e,"STRONG",{});var Qat=s(xfe);OHo=r(Qat,"longformer"),Qat.forEach(t),XHo=r(e6e," \u2014 "),zN=n(e6e,"A",{href:!0});var Hat=s(zN);VHo=r(Hat,"TFLongformerModel"),Hat.forEach(t),zHo=r(e6e," (Longformer model)"),e6e.forEach(t),WHo=i(x),iv=n(x,"LI",{});var o6e=s(iv);kfe=n(o6e,"STRONG",{});var Uat=s(kfe);QHo=r(Uat,"lxmert"),Uat.forEach(t),HHo=r(o6e," \u2014 "),WN=n(o6e,"A",{href:!0});var Jat=s(WN);UHo=r(Jat,"TFLxmertModel"),Jat.forEach(t),JHo=r(o6e," (LXMERT model)"),o6e.forEach(t),YHo=i(x),dv=n(x,"LI",{});var r6e=s(dv);Rfe=n(r6e,"STRONG",{});var Yat=s(Rfe);KHo=r(Yat,"marian"),Yat.forEach(t),ZHo=r(r6e," \u2014 "),QN=n(r6e,"A",{href:!0});var Kat=s(QN);eUo=r(Kat,"TFMarianModel"),Kat.forEach(t),oUo=r(r6e," (Marian model)"),r6e.forEach(t),rUo=i(x),cv=n(x,"LI",{});var t6e=s(cv);Sfe=n(t6e,"STRONG",{});var Zat=s(Sfe);tUo=r(Zat,"mbart"),Zat.forEach(t),aUo=r(t6e," \u2014 "),HN=n(t6e,"A",{href:!0});var ent=s(HN);nUo=r(ent,"TFMBartModel"),ent.forEach(t),sUo=r(t6e," (mBART model)"),t6e.forEach(t),lUo=i(x),fv=n(x,"LI",{});var a6e=s(fv);Pfe=n(a6e,"STRONG",{});var ont=s(Pfe);iUo=r(ont,"mobilebert"),ont.forEach(t),dUo=r(a6e," \u2014 "),UN=n(a6e,"A",{href:!0});var rnt=s(UN);cUo=r(rnt,"TFMobileBertModel"),rnt.forEach(t),fUo=r(a6e," (MobileBERT model)"),a6e.forEach(t),mUo=i(x),mv=n(x,"LI",{});var n6e=s(mv);$fe=n(n6e,"STRONG",{});var tnt=s($fe);gUo=r(tnt,"mpnet"),tnt.forEach(t),hUo=r(n6e," \u2014 "),JN=n(n6e,"A",{href:!0});var ant=s(JN);pUo=r(ant,"TFMPNetModel"),ant.forEach(t),_Uo=r(n6e," (MPNet model)"),n6e.forEach(t),uUo=i(x),gv=n(x,"LI",{});var s6e=s(gv);Ife=n(s6e,"STRONG",{});var nnt=s(Ife);bUo=r(nnt,"mt5"),nnt.forEach(t),vUo=r(s6e," \u2014 "),YN=n(s6e,"A",{href:!0});var snt=s(YN);TUo=r(snt,"TFMT5Model"),snt.forEach(t),FUo=r(s6e," (mT5 model)"),s6e.forEach(t),CUo=i(x),hv=n(x,"LI",{});var l6e=s(hv);Dfe=n(l6e,"STRONG",{});var lnt=s(Dfe);MUo=r(lnt,"openai-gpt"),lnt.forEach(t),EUo=r(l6e," \u2014 "),KN=n(l6e,"A",{href:!0});var int=s(KN);yUo=r(int,"TFOpenAIGPTModel"),int.forEach(t),wUo=r(l6e," (OpenAI GPT model)"),l6e.forEach(t),AUo=i(x),pv=n(x,"LI",{});var i6e=s(pv);jfe=n(i6e,"STRONG",{});var dnt=s(jfe);LUo=r(dnt,"pegasus"),dnt.forEach(t),BUo=r(i6e," \u2014 "),ZN=n(i6e,"A",{href:!0});var cnt=s(ZN);xUo=r(cnt,"TFPegasusModel"),cnt.forEach(t),kUo=r(i6e," (Pegasus model)"),i6e.forEach(t),RUo=i(x),_v=n(x,"LI",{});var d6e=s(_v);Nfe=n(d6e,"STRONG",{});var fnt=s(Nfe);SUo=r(fnt,"rembert"),fnt.forEach(t),PUo=r(d6e," \u2014 "),eq=n(d6e,"A",{href:!0});var mnt=s(eq);$Uo=r(mnt,"TFRemBertModel"),mnt.forEach(t),IUo=r(d6e," (RemBERT model)"),d6e.forEach(t),DUo=i(x),uv=n(x,"LI",{});var c6e=s(uv);qfe=n(c6e,"STRONG",{});var gnt=s(qfe);jUo=r(gnt,"roberta"),gnt.forEach(t),NUo=r(c6e," \u2014 "),oq=n(c6e,"A",{href:!0});var hnt=s(oq);qUo=r(hnt,"TFRobertaModel"),hnt.forEach(t),GUo=r(c6e," (RoBERTa model)"),c6e.forEach(t),OUo=i(x),bv=n(x,"LI",{});var f6e=s(bv);Gfe=n(f6e,"STRONG",{});var pnt=s(Gfe);XUo=r(pnt,"roformer"),pnt.forEach(t),VUo=r(f6e," \u2014 "),rq=n(f6e,"A",{href:!0});var _nt=s(rq);zUo=r(_nt,"TFRoFormerModel"),_nt.forEach(t),WUo=r(f6e," (RoFormer model)"),f6e.forEach(t),QUo=i(x),vv=n(x,"LI",{});var m6e=s(vv);Ofe=n(m6e,"STRONG",{});var unt=s(Ofe);HUo=r(unt,"speech_to_text"),unt.forEach(t),UUo=r(m6e," \u2014 "),tq=n(m6e,"A",{href:!0});var bnt=s(tq);JUo=r(bnt,"TFSpeech2TextModel"),bnt.forEach(t),YUo=r(m6e," (Speech2Text model)"),m6e.forEach(t),KUo=i(x),Tv=n(x,"LI",{});var g6e=s(Tv);Xfe=n(g6e,"STRONG",{});var vnt=s(Xfe);ZUo=r(vnt,"t5"),vnt.forEach(t),eJo=r(g6e," \u2014 "),aq=n(g6e,"A",{href:!0});var Tnt=s(aq);oJo=r(Tnt,"TFT5Model"),Tnt.forEach(t),rJo=r(g6e," (T5 model)"),g6e.forEach(t),tJo=i(x),Fv=n(x,"LI",{});var h6e=s(Fv);Vfe=n(h6e,"STRONG",{});var Fnt=s(Vfe);aJo=r(Fnt,"tapas"),Fnt.forEach(t),nJo=r(h6e," \u2014 "),nq=n(h6e,"A",{href:!0});var Cnt=s(nq);sJo=r(Cnt,"TFTapasModel"),Cnt.forEach(t),lJo=r(h6e," (TAPAS model)"),h6e.forEach(t),iJo=i(x),Cv=n(x,"LI",{});var p6e=s(Cv);zfe=n(p6e,"STRONG",{});var Mnt=s(zfe);dJo=r(Mnt,"transfo-xl"),Mnt.forEach(t),cJo=r(p6e," \u2014 "),sq=n(p6e,"A",{href:!0});var Ent=s(sq);fJo=r(Ent,"TFTransfoXLModel"),Ent.forEach(t),mJo=r(p6e," (Transformer-XL model)"),p6e.forEach(t),gJo=i(x),Mv=n(x,"LI",{});var _6e=s(Mv);Wfe=n(_6e,"STRONG",{});var ynt=s(Wfe);hJo=r(ynt,"vit"),ynt.forEach(t),pJo=r(_6e," \u2014 "),lq=n(_6e,"A",{href:!0});var wnt=s(lq);_Jo=r(wnt,"TFViTModel"),wnt.forEach(t),uJo=r(_6e," (ViT model)"),_6e.forEach(t),bJo=i(x),Ev=n(x,"LI",{});var u6e=s(Ev);Qfe=n(u6e,"STRONG",{});var Ant=s(Qfe);vJo=r(Ant,"wav2vec2"),Ant.forEach(t),TJo=r(u6e," \u2014 "),iq=n(u6e,"A",{href:!0});var Lnt=s(iq);FJo=r(Lnt,"TFWav2Vec2Model"),Lnt.forEach(t),CJo=r(u6e," (Wav2Vec2 model)"),u6e.forEach(t),MJo=i(x),yv=n(x,"LI",{});var b6e=s(yv);Hfe=n(b6e,"STRONG",{});var Bnt=s(Hfe);EJo=r(Bnt,"xlm"),Bnt.forEach(t),yJo=r(b6e," \u2014 "),dq=n(b6e,"A",{href:!0});var xnt=s(dq);wJo=r(xnt,"TFXLMModel"),xnt.forEach(t),AJo=r(b6e," (XLM model)"),b6e.forEach(t),LJo=i(x),wv=n(x,"LI",{});var v6e=s(wv);Ufe=n(v6e,"STRONG",{});var knt=s(Ufe);BJo=r(knt,"xlm-roberta"),knt.forEach(t),xJo=r(v6e," \u2014 "),cq=n(v6e,"A",{href:!0});var Rnt=s(cq);kJo=r(Rnt,"TFXLMRobertaModel"),Rnt.forEach(t),RJo=r(v6e," (XLM-RoBERTa model)"),v6e.forEach(t),SJo=i(x),Av=n(x,"LI",{});var T6e=s(Av);Jfe=n(T6e,"STRONG",{});var Snt=s(Jfe);PJo=r(Snt,"xlnet"),Snt.forEach(t),$Jo=r(T6e," \u2014 "),fq=n(T6e,"A",{href:!0});var Pnt=s(fq);IJo=r(Pnt,"TFXLNetModel"),Pnt.forEach(t),DJo=r(T6e," (XLNet model)"),T6e.forEach(t),x.forEach(t),jJo=i(ca),Yfe=n(ca,"P",{});var $nt=s(Yfe);NJo=r($nt,"Examples:"),$nt.forEach(t),qJo=i(ca),m(dw.$$.fragment,ca),ca.forEach(t),kl.forEach(t),axe=i(c),ac=n(c,"H2",{class:!0});var gRe=s(ac);Lv=n(gRe,"A",{id:!0,class:!0,href:!0});var Int=s(Lv);Kfe=n(Int,"SPAN",{});var Dnt=s(Kfe);m(cw.$$.fragment,Dnt),Dnt.forEach(t),Int.forEach(t),GJo=i(gRe),Zfe=n(gRe,"SPAN",{});var jnt=s(Zfe);OJo=r(jnt,"TFAutoModelForPreTraining"),jnt.forEach(t),gRe.forEach(t),nxe=i(c),br=n(c,"DIV",{class:!0});var Sl=s(br);m(fw.$$.fragment,Sl),XJo=i(Sl),nc=n(Sl,"P",{});var Sz=s(nc);VJo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),eme=n(Sz,"CODE",{});var Nnt=s(eme);zJo=r(Nnt,"from_pretrained()"),Nnt.forEach(t),WJo=r(Sz,"class method or the "),ome=n(Sz,"CODE",{});var qnt=s(ome);QJo=r(qnt,"from_config()"),qnt.forEach(t),HJo=r(Sz,`class
method.`),Sz.forEach(t),UJo=i(Sl),mw=n(Sl,"P",{});var hRe=s(mw);JJo=r(hRe,"This class cannot be instantiated directly using "),rme=n(hRe,"CODE",{});var Gnt=s(rme);YJo=r(Gnt,"__init__()"),Gnt.forEach(t),KJo=r(hRe," (throws an error)."),hRe.forEach(t),ZJo=i(Sl),ft=n(Sl,"DIV",{class:!0});var Pl=s(ft);m(gw.$$.fragment,Pl),eYo=i(Pl),tme=n(Pl,"P",{});var Ont=s(tme);oYo=r(Ont,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Ont.forEach(t),rYo=i(Pl),sc=n(Pl,"P",{});var Pz=s(sc);tYo=r(Pz,`Note:
Loading a model from its configuration file does `),ame=n(Pz,"STRONG",{});var Xnt=s(ame);aYo=r(Xnt,"not"),Xnt.forEach(t),nYo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),nme=n(Pz,"CODE",{});var Vnt=s(nme);sYo=r(Vnt,"from_pretrained()"),Vnt.forEach(t),lYo=r(Pz,"to load the model weights."),Pz.forEach(t),iYo=i(Pl),sme=n(Pl,"P",{});var znt=s(sme);dYo=r(znt,"Examples:"),znt.forEach(t),cYo=i(Pl),m(hw.$$.fragment,Pl),Pl.forEach(t),fYo=i(Sl),ho=n(Sl,"DIV",{class:!0});var fa=s(ho);m(pw.$$.fragment,fa),mYo=i(fa),lme=n(fa,"P",{});var Wnt=s(lme);gYo=r(Wnt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Wnt.forEach(t),hYo=i(fa),fn=n(fa,"P",{});var a4=s(fn);pYo=r(a4,"The model class to instantiate is selected based on the "),ime=n(a4,"CODE",{});var Qnt=s(ime);_Yo=r(Qnt,"model_type"),Qnt.forEach(t),uYo=r(a4,` property of the config object (either
passed as an argument or loaded from `),dme=n(a4,"CODE",{});var Hnt=s(dme);bYo=r(Hnt,"pretrained_model_name_or_path"),Hnt.forEach(t),vYo=r(a4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cme=n(a4,"CODE",{});var Unt=s(cme);TYo=r(Unt,"pretrained_model_name_or_path"),Unt.forEach(t),FYo=r(a4,":"),a4.forEach(t),CYo=i(fa),H=n(fa,"UL",{});var U=s(H);Bv=n(U,"LI",{});var F6e=s(Bv);fme=n(F6e,"STRONG",{});var Jnt=s(fme);MYo=r(Jnt,"albert"),Jnt.forEach(t),EYo=r(F6e," \u2014 "),mq=n(F6e,"A",{href:!0});var Ynt=s(mq);yYo=r(Ynt,"TFAlbertForPreTraining"),Ynt.forEach(t),wYo=r(F6e," (ALBERT model)"),F6e.forEach(t),AYo=i(U),xv=n(U,"LI",{});var C6e=s(xv);mme=n(C6e,"STRONG",{});var Knt=s(mme);LYo=r(Knt,"bart"),Knt.forEach(t),BYo=r(C6e," \u2014 "),gq=n(C6e,"A",{href:!0});var Znt=s(gq);xYo=r(Znt,"TFBartForConditionalGeneration"),Znt.forEach(t),kYo=r(C6e," (BART model)"),C6e.forEach(t),RYo=i(U),kv=n(U,"LI",{});var M6e=s(kv);gme=n(M6e,"STRONG",{});var est=s(gme);SYo=r(est,"bert"),est.forEach(t),PYo=r(M6e," \u2014 "),hq=n(M6e,"A",{href:!0});var ost=s(hq);$Yo=r(ost,"TFBertForPreTraining"),ost.forEach(t),IYo=r(M6e," (BERT model)"),M6e.forEach(t),DYo=i(U),Rv=n(U,"LI",{});var E6e=s(Rv);hme=n(E6e,"STRONG",{});var rst=s(hme);jYo=r(rst,"camembert"),rst.forEach(t),NYo=r(E6e," \u2014 "),pq=n(E6e,"A",{href:!0});var tst=s(pq);qYo=r(tst,"TFCamembertForMaskedLM"),tst.forEach(t),GYo=r(E6e," (CamemBERT model)"),E6e.forEach(t),OYo=i(U),Sv=n(U,"LI",{});var y6e=s(Sv);pme=n(y6e,"STRONG",{});var ast=s(pme);XYo=r(ast,"ctrl"),ast.forEach(t),VYo=r(y6e," \u2014 "),_q=n(y6e,"A",{href:!0});var nst=s(_q);zYo=r(nst,"TFCTRLLMHeadModel"),nst.forEach(t),WYo=r(y6e," (CTRL model)"),y6e.forEach(t),QYo=i(U),Pv=n(U,"LI",{});var w6e=s(Pv);_me=n(w6e,"STRONG",{});var sst=s(_me);HYo=r(sst,"distilbert"),sst.forEach(t),UYo=r(w6e," \u2014 "),uq=n(w6e,"A",{href:!0});var lst=s(uq);JYo=r(lst,"TFDistilBertForMaskedLM"),lst.forEach(t),YYo=r(w6e," (DistilBERT model)"),w6e.forEach(t),KYo=i(U),$v=n(U,"LI",{});var A6e=s($v);ume=n(A6e,"STRONG",{});var ist=s(ume);ZYo=r(ist,"electra"),ist.forEach(t),eKo=r(A6e," \u2014 "),bq=n(A6e,"A",{href:!0});var dst=s(bq);oKo=r(dst,"TFElectraForPreTraining"),dst.forEach(t),rKo=r(A6e," (ELECTRA model)"),A6e.forEach(t),tKo=i(U),Iv=n(U,"LI",{});var L6e=s(Iv);bme=n(L6e,"STRONG",{});var cst=s(bme);aKo=r(cst,"flaubert"),cst.forEach(t),nKo=r(L6e," \u2014 "),vq=n(L6e,"A",{href:!0});var fst=s(vq);sKo=r(fst,"TFFlaubertWithLMHeadModel"),fst.forEach(t),lKo=r(L6e," (FlauBERT model)"),L6e.forEach(t),iKo=i(U),Dv=n(U,"LI",{});var B6e=s(Dv);vme=n(B6e,"STRONG",{});var mst=s(vme);dKo=r(mst,"funnel"),mst.forEach(t),cKo=r(B6e," \u2014 "),Tq=n(B6e,"A",{href:!0});var gst=s(Tq);fKo=r(gst,"TFFunnelForPreTraining"),gst.forEach(t),mKo=r(B6e," (Funnel Transformer model)"),B6e.forEach(t),gKo=i(U),jv=n(U,"LI",{});var x6e=s(jv);Tme=n(x6e,"STRONG",{});var hst=s(Tme);hKo=r(hst,"gpt2"),hst.forEach(t),pKo=r(x6e," \u2014 "),Fq=n(x6e,"A",{href:!0});var pst=s(Fq);_Ko=r(pst,"TFGPT2LMHeadModel"),pst.forEach(t),uKo=r(x6e," (OpenAI GPT-2 model)"),x6e.forEach(t),bKo=i(U),Nv=n(U,"LI",{});var k6e=s(Nv);Fme=n(k6e,"STRONG",{});var _st=s(Fme);vKo=r(_st,"layoutlm"),_st.forEach(t),TKo=r(k6e," \u2014 "),Cq=n(k6e,"A",{href:!0});var ust=s(Cq);FKo=r(ust,"TFLayoutLMForMaskedLM"),ust.forEach(t),CKo=r(k6e," (LayoutLM model)"),k6e.forEach(t),MKo=i(U),qv=n(U,"LI",{});var R6e=s(qv);Cme=n(R6e,"STRONG",{});var bst=s(Cme);EKo=r(bst,"lxmert"),bst.forEach(t),yKo=r(R6e," \u2014 "),Mq=n(R6e,"A",{href:!0});var vst=s(Mq);wKo=r(vst,"TFLxmertForPreTraining"),vst.forEach(t),AKo=r(R6e," (LXMERT model)"),R6e.forEach(t),LKo=i(U),Gv=n(U,"LI",{});var S6e=s(Gv);Mme=n(S6e,"STRONG",{});var Tst=s(Mme);BKo=r(Tst,"mobilebert"),Tst.forEach(t),xKo=r(S6e," \u2014 "),Eq=n(S6e,"A",{href:!0});var Fst=s(Eq);kKo=r(Fst,"TFMobileBertForPreTraining"),Fst.forEach(t),RKo=r(S6e," (MobileBERT model)"),S6e.forEach(t),SKo=i(U),Ov=n(U,"LI",{});var P6e=s(Ov);Eme=n(P6e,"STRONG",{});var Cst=s(Eme);PKo=r(Cst,"mpnet"),Cst.forEach(t),$Ko=r(P6e," \u2014 "),yq=n(P6e,"A",{href:!0});var Mst=s(yq);IKo=r(Mst,"TFMPNetForMaskedLM"),Mst.forEach(t),DKo=r(P6e," (MPNet model)"),P6e.forEach(t),jKo=i(U),Xv=n(U,"LI",{});var $6e=s(Xv);yme=n($6e,"STRONG",{});var Est=s(yme);NKo=r(Est,"openai-gpt"),Est.forEach(t),qKo=r($6e," \u2014 "),wq=n($6e,"A",{href:!0});var yst=s(wq);GKo=r(yst,"TFOpenAIGPTLMHeadModel"),yst.forEach(t),OKo=r($6e," (OpenAI GPT model)"),$6e.forEach(t),XKo=i(U),Vv=n(U,"LI",{});var I6e=s(Vv);wme=n(I6e,"STRONG",{});var wst=s(wme);VKo=r(wst,"roberta"),wst.forEach(t),zKo=r(I6e," \u2014 "),Aq=n(I6e,"A",{href:!0});var Ast=s(Aq);WKo=r(Ast,"TFRobertaForMaskedLM"),Ast.forEach(t),QKo=r(I6e," (RoBERTa model)"),I6e.forEach(t),HKo=i(U),zv=n(U,"LI",{});var D6e=s(zv);Ame=n(D6e,"STRONG",{});var Lst=s(Ame);UKo=r(Lst,"t5"),Lst.forEach(t),JKo=r(D6e," \u2014 "),Lq=n(D6e,"A",{href:!0});var Bst=s(Lq);YKo=r(Bst,"TFT5ForConditionalGeneration"),Bst.forEach(t),KKo=r(D6e," (T5 model)"),D6e.forEach(t),ZKo=i(U),Wv=n(U,"LI",{});var j6e=s(Wv);Lme=n(j6e,"STRONG",{});var xst=s(Lme);eZo=r(xst,"tapas"),xst.forEach(t),oZo=r(j6e," \u2014 "),Bq=n(j6e,"A",{href:!0});var kst=s(Bq);rZo=r(kst,"TFTapasForMaskedLM"),kst.forEach(t),tZo=r(j6e," (TAPAS model)"),j6e.forEach(t),aZo=i(U),Qv=n(U,"LI",{});var N6e=s(Qv);Bme=n(N6e,"STRONG",{});var Rst=s(Bme);nZo=r(Rst,"transfo-xl"),Rst.forEach(t),sZo=r(N6e," \u2014 "),xq=n(N6e,"A",{href:!0});var Sst=s(xq);lZo=r(Sst,"TFTransfoXLLMHeadModel"),Sst.forEach(t),iZo=r(N6e," (Transformer-XL model)"),N6e.forEach(t),dZo=i(U),Hv=n(U,"LI",{});var q6e=s(Hv);xme=n(q6e,"STRONG",{});var Pst=s(xme);cZo=r(Pst,"xlm"),Pst.forEach(t),fZo=r(q6e," \u2014 "),kq=n(q6e,"A",{href:!0});var $st=s(kq);mZo=r($st,"TFXLMWithLMHeadModel"),$st.forEach(t),gZo=r(q6e," (XLM model)"),q6e.forEach(t),hZo=i(U),Uv=n(U,"LI",{});var G6e=s(Uv);kme=n(G6e,"STRONG",{});var Ist=s(kme);pZo=r(Ist,"xlm-roberta"),Ist.forEach(t),_Zo=r(G6e," \u2014 "),Rq=n(G6e,"A",{href:!0});var Dst=s(Rq);uZo=r(Dst,"TFXLMRobertaForMaskedLM"),Dst.forEach(t),bZo=r(G6e," (XLM-RoBERTa model)"),G6e.forEach(t),vZo=i(U),Jv=n(U,"LI",{});var O6e=s(Jv);Rme=n(O6e,"STRONG",{});var jst=s(Rme);TZo=r(jst,"xlnet"),jst.forEach(t),FZo=r(O6e," \u2014 "),Sq=n(O6e,"A",{href:!0});var Nst=s(Sq);CZo=r(Nst,"TFXLNetLMHeadModel"),Nst.forEach(t),MZo=r(O6e," (XLNet model)"),O6e.forEach(t),U.forEach(t),EZo=i(fa),Sme=n(fa,"P",{});var qst=s(Sme);yZo=r(qst,"Examples:"),qst.forEach(t),wZo=i(fa),m(_w.$$.fragment,fa),fa.forEach(t),Sl.forEach(t),sxe=i(c),lc=n(c,"H2",{class:!0});var pRe=s(lc);Yv=n(pRe,"A",{id:!0,class:!0,href:!0});var Gst=s(Yv);Pme=n(Gst,"SPAN",{});var Ost=s(Pme);m(uw.$$.fragment,Ost),Ost.forEach(t),Gst.forEach(t),AZo=i(pRe),$me=n(pRe,"SPAN",{});var Xst=s($me);LZo=r(Xst,"TFAutoModelForCausalLM"),Xst.forEach(t),pRe.forEach(t),lxe=i(c),vr=n(c,"DIV",{class:!0});var $l=s(vr);m(bw.$$.fragment,$l),BZo=i($l),ic=n($l,"P",{});var $z=s(ic);xZo=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Ime=n($z,"CODE",{});var Vst=s(Ime);kZo=r(Vst,"from_pretrained()"),Vst.forEach(t),RZo=r($z,"class method or the "),Dme=n($z,"CODE",{});var zst=s(Dme);SZo=r(zst,"from_config()"),zst.forEach(t),PZo=r($z,`class
method.`),$z.forEach(t),$Zo=i($l),vw=n($l,"P",{});var _Re=s(vw);IZo=r(_Re,"This class cannot be instantiated directly using "),jme=n(_Re,"CODE",{});var Wst=s(jme);DZo=r(Wst,"__init__()"),Wst.forEach(t),jZo=r(_Re," (throws an error)."),_Re.forEach(t),NZo=i($l),mt=n($l,"DIV",{class:!0});var Il=s(mt);m(Tw.$$.fragment,Il),qZo=i(Il),Nme=n(Il,"P",{});var Qst=s(Nme);GZo=r(Qst,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Qst.forEach(t),OZo=i(Il),dc=n(Il,"P",{});var Iz=s(dc);XZo=r(Iz,`Note:
Loading a model from its configuration file does `),qme=n(Iz,"STRONG",{});var Hst=s(qme);VZo=r(Hst,"not"),Hst.forEach(t),zZo=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gme=n(Iz,"CODE",{});var Ust=s(Gme);WZo=r(Ust,"from_pretrained()"),Ust.forEach(t),QZo=r(Iz,"to load the model weights."),Iz.forEach(t),HZo=i(Il),Ome=n(Il,"P",{});var Jst=s(Ome);UZo=r(Jst,"Examples:"),Jst.forEach(t),JZo=i(Il),m(Fw.$$.fragment,Il),Il.forEach(t),YZo=i($l),po=n($l,"DIV",{class:!0});var ma=s(po);m(Cw.$$.fragment,ma),KZo=i(ma),Xme=n(ma,"P",{});var Yst=s(Xme);ZZo=r(Yst,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Yst.forEach(t),eer=i(ma),mn=n(ma,"P",{});var n4=s(mn);oer=r(n4,"The model class to instantiate is selected based on the "),Vme=n(n4,"CODE",{});var Kst=s(Vme);rer=r(Kst,"model_type"),Kst.forEach(t),ter=r(n4,` property of the config object (either
passed as an argument or loaded from `),zme=n(n4,"CODE",{});var Zst=s(zme);aer=r(Zst,"pretrained_model_name_or_path"),Zst.forEach(t),ner=r(n4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wme=n(n4,"CODE",{});var elt=s(Wme);ser=r(elt,"pretrained_model_name_or_path"),elt.forEach(t),ler=r(n4,":"),n4.forEach(t),ier=i(ma),pe=n(ma,"UL",{});var Me=s(pe);Kv=n(Me,"LI",{});var X6e=s(Kv);Qme=n(X6e,"STRONG",{});var olt=s(Qme);der=r(olt,"bert"),olt.forEach(t),cer=r(X6e," \u2014 "),Pq=n(X6e,"A",{href:!0});var rlt=s(Pq);fer=r(rlt,"TFBertLMHeadModel"),rlt.forEach(t),mer=r(X6e," (BERT model)"),X6e.forEach(t),ger=i(Me),Zv=n(Me,"LI",{});var V6e=s(Zv);Hme=n(V6e,"STRONG",{});var tlt=s(Hme);her=r(tlt,"ctrl"),tlt.forEach(t),per=r(V6e," \u2014 "),$q=n(V6e,"A",{href:!0});var alt=s($q);_er=r(alt,"TFCTRLLMHeadModel"),alt.forEach(t),uer=r(V6e," (CTRL model)"),V6e.forEach(t),ber=i(Me),eT=n(Me,"LI",{});var z6e=s(eT);Ume=n(z6e,"STRONG",{});var nlt=s(Ume);ver=r(nlt,"gpt2"),nlt.forEach(t),Ter=r(z6e," \u2014 "),Iq=n(z6e,"A",{href:!0});var slt=s(Iq);Fer=r(slt,"TFGPT2LMHeadModel"),slt.forEach(t),Cer=r(z6e," (OpenAI GPT-2 model)"),z6e.forEach(t),Mer=i(Me),oT=n(Me,"LI",{});var W6e=s(oT);Jme=n(W6e,"STRONG",{});var llt=s(Jme);Eer=r(llt,"openai-gpt"),llt.forEach(t),yer=r(W6e," \u2014 "),Dq=n(W6e,"A",{href:!0});var ilt=s(Dq);wer=r(ilt,"TFOpenAIGPTLMHeadModel"),ilt.forEach(t),Aer=r(W6e," (OpenAI GPT model)"),W6e.forEach(t),Ler=i(Me),rT=n(Me,"LI",{});var Q6e=s(rT);Yme=n(Q6e,"STRONG",{});var dlt=s(Yme);Ber=r(dlt,"rembert"),dlt.forEach(t),xer=r(Q6e," \u2014 "),jq=n(Q6e,"A",{href:!0});var clt=s(jq);ker=r(clt,"TFRemBertForCausalLM"),clt.forEach(t),Rer=r(Q6e," (RemBERT model)"),Q6e.forEach(t),Ser=i(Me),tT=n(Me,"LI",{});var H6e=s(tT);Kme=n(H6e,"STRONG",{});var flt=s(Kme);Per=r(flt,"roberta"),flt.forEach(t),$er=r(H6e," \u2014 "),Nq=n(H6e,"A",{href:!0});var mlt=s(Nq);Ier=r(mlt,"TFRobertaForCausalLM"),mlt.forEach(t),Der=r(H6e," (RoBERTa model)"),H6e.forEach(t),jer=i(Me),aT=n(Me,"LI",{});var U6e=s(aT);Zme=n(U6e,"STRONG",{});var glt=s(Zme);Ner=r(glt,"roformer"),glt.forEach(t),qer=r(U6e," \u2014 "),qq=n(U6e,"A",{href:!0});var hlt=s(qq);Ger=r(hlt,"TFRoFormerForCausalLM"),hlt.forEach(t),Oer=r(U6e," (RoFormer model)"),U6e.forEach(t),Xer=i(Me),nT=n(Me,"LI",{});var J6e=s(nT);ege=n(J6e,"STRONG",{});var plt=s(ege);Ver=r(plt,"transfo-xl"),plt.forEach(t),zer=r(J6e," \u2014 "),Gq=n(J6e,"A",{href:!0});var _lt=s(Gq);Wer=r(_lt,"TFTransfoXLLMHeadModel"),_lt.forEach(t),Qer=r(J6e," (Transformer-XL model)"),J6e.forEach(t),Her=i(Me),sT=n(Me,"LI",{});var Y6e=s(sT);oge=n(Y6e,"STRONG",{});var ult=s(oge);Uer=r(ult,"xlm"),ult.forEach(t),Jer=r(Y6e," \u2014 "),Oq=n(Y6e,"A",{href:!0});var blt=s(Oq);Yer=r(blt,"TFXLMWithLMHeadModel"),blt.forEach(t),Ker=r(Y6e," (XLM model)"),Y6e.forEach(t),Zer=i(Me),lT=n(Me,"LI",{});var K6e=s(lT);rge=n(K6e,"STRONG",{});var vlt=s(rge);eor=r(vlt,"xlnet"),vlt.forEach(t),oor=r(K6e," \u2014 "),Xq=n(K6e,"A",{href:!0});var Tlt=s(Xq);ror=r(Tlt,"TFXLNetLMHeadModel"),Tlt.forEach(t),tor=r(K6e," (XLNet model)"),K6e.forEach(t),Me.forEach(t),aor=i(ma),tge=n(ma,"P",{});var Flt=s(tge);nor=r(Flt,"Examples:"),Flt.forEach(t),sor=i(ma),m(Mw.$$.fragment,ma),ma.forEach(t),$l.forEach(t),ixe=i(c),cc=n(c,"H2",{class:!0});var uRe=s(cc);iT=n(uRe,"A",{id:!0,class:!0,href:!0});var Clt=s(iT);age=n(Clt,"SPAN",{});var Mlt=s(age);m(Ew.$$.fragment,Mlt),Mlt.forEach(t),Clt.forEach(t),lor=i(uRe),nge=n(uRe,"SPAN",{});var Elt=s(nge);ior=r(Elt,"TFAutoModelForImageClassification"),Elt.forEach(t),uRe.forEach(t),dxe=i(c),Tr=n(c,"DIV",{class:!0});var Dl=s(Tr);m(yw.$$.fragment,Dl),dor=i(Dl),fc=n(Dl,"P",{});var Dz=s(fc);cor=r(Dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),sge=n(Dz,"CODE",{});var ylt=s(sge);mor=r(ylt,"from_pretrained()"),ylt.forEach(t),gor=r(Dz,"class method or the "),lge=n(Dz,"CODE",{});var wlt=s(lge);hor=r(wlt,"from_config()"),wlt.forEach(t),por=r(Dz,`class
method.`),Dz.forEach(t),_or=i(Dl),ww=n(Dl,"P",{});var bRe=s(ww);uor=r(bRe,"This class cannot be instantiated directly using "),ige=n(bRe,"CODE",{});var Alt=s(ige);bor=r(Alt,"__init__()"),Alt.forEach(t),vor=r(bRe," (throws an error)."),bRe.forEach(t),Tor=i(Dl),gt=n(Dl,"DIV",{class:!0});var jl=s(gt);m(Aw.$$.fragment,jl),For=i(jl),dge=n(jl,"P",{});var Llt=s(dge);Cor=r(Llt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Llt.forEach(t),Mor=i(jl),mc=n(jl,"P",{});var jz=s(mc);Eor=r(jz,`Note:
Loading a model from its configuration file does `),cge=n(jz,"STRONG",{});var Blt=s(cge);yor=r(Blt,"not"),Blt.forEach(t),wor=r(jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),fge=n(jz,"CODE",{});var xlt=s(fge);Aor=r(xlt,"from_pretrained()"),xlt.forEach(t),Lor=r(jz,"to load the model weights."),jz.forEach(t),Bor=i(jl),mge=n(jl,"P",{});var klt=s(mge);xor=r(klt,"Examples:"),klt.forEach(t),kor=i(jl),m(Lw.$$.fragment,jl),jl.forEach(t),Ror=i(Dl),_o=n(Dl,"DIV",{class:!0});var ga=s(_o);m(Bw.$$.fragment,ga),Sor=i(ga),gge=n(ga,"P",{});var Rlt=s(gge);Por=r(Rlt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Rlt.forEach(t),$or=i(ga),gn=n(ga,"P",{});var s4=s(gn);Ior=r(s4,"The model class to instantiate is selected based on the "),hge=n(s4,"CODE",{});var Slt=s(hge);Dor=r(Slt,"model_type"),Slt.forEach(t),jor=r(s4,` property of the config object (either
passed as an argument or loaded from `),pge=n(s4,"CODE",{});var Plt=s(pge);Nor=r(Plt,"pretrained_model_name_or_path"),Plt.forEach(t),qor=r(s4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_ge=n(s4,"CODE",{});var $lt=s(_ge);Gor=r($lt,"pretrained_model_name_or_path"),$lt.forEach(t),Oor=r(s4,":"),s4.forEach(t),Xor=i(ga),xw=n(ga,"UL",{});var vRe=s(xw);dT=n(vRe,"LI",{});var Z6e=s(dT);uge=n(Z6e,"STRONG",{});var Ilt=s(uge);Vor=r(Ilt,"convnext"),Ilt.forEach(t),zor=r(Z6e," \u2014 "),Vq=n(Z6e,"A",{href:!0});var Dlt=s(Vq);Wor=r(Dlt,"TFConvNextForImageClassification"),Dlt.forEach(t),Qor=r(Z6e," (ConvNext model)"),Z6e.forEach(t),Hor=i(vRe),cT=n(vRe,"LI",{});var eAe=s(cT);bge=n(eAe,"STRONG",{});var jlt=s(bge);Uor=r(jlt,"vit"),jlt.forEach(t),Jor=r(eAe," \u2014 "),zq=n(eAe,"A",{href:!0});var Nlt=s(zq);Yor=r(Nlt,"TFViTForImageClassification"),Nlt.forEach(t),Kor=r(eAe," (ViT model)"),eAe.forEach(t),vRe.forEach(t),Zor=i(ga),vge=n(ga,"P",{});var qlt=s(vge);err=r(qlt,"Examples:"),qlt.forEach(t),orr=i(ga),m(kw.$$.fragment,ga),ga.forEach(t),Dl.forEach(t),cxe=i(c),gc=n(c,"H2",{class:!0});var TRe=s(gc);fT=n(TRe,"A",{id:!0,class:!0,href:!0});var Glt=s(fT);Tge=n(Glt,"SPAN",{});var Olt=s(Tge);m(Rw.$$.fragment,Olt),Olt.forEach(t),Glt.forEach(t),rrr=i(TRe),Fge=n(TRe,"SPAN",{});var Xlt=s(Fge);trr=r(Xlt,"TFAutoModelForMaskedLM"),Xlt.forEach(t),TRe.forEach(t),fxe=i(c),Fr=n(c,"DIV",{class:!0});var Nl=s(Fr);m(Sw.$$.fragment,Nl),arr=i(Nl),hc=n(Nl,"P",{});var Nz=s(hc);nrr=r(Nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Cge=n(Nz,"CODE",{});var Vlt=s(Cge);srr=r(Vlt,"from_pretrained()"),Vlt.forEach(t),lrr=r(Nz,"class method or the "),Mge=n(Nz,"CODE",{});var zlt=s(Mge);irr=r(zlt,"from_config()"),zlt.forEach(t),drr=r(Nz,`class
method.`),Nz.forEach(t),crr=i(Nl),Pw=n(Nl,"P",{});var FRe=s(Pw);frr=r(FRe,"This class cannot be instantiated directly using "),Ege=n(FRe,"CODE",{});var Wlt=s(Ege);mrr=r(Wlt,"__init__()"),Wlt.forEach(t),grr=r(FRe," (throws an error)."),FRe.forEach(t),hrr=i(Nl),ht=n(Nl,"DIV",{class:!0});var ql=s(ht);m($w.$$.fragment,ql),prr=i(ql),yge=n(ql,"P",{});var Qlt=s(yge);_rr=r(Qlt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Qlt.forEach(t),urr=i(ql),pc=n(ql,"P",{});var qz=s(pc);brr=r(qz,`Note:
Loading a model from its configuration file does `),wge=n(qz,"STRONG",{});var Hlt=s(wge);vrr=r(Hlt,"not"),Hlt.forEach(t),Trr=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Age=n(qz,"CODE",{});var Ult=s(Age);Frr=r(Ult,"from_pretrained()"),Ult.forEach(t),Crr=r(qz,"to load the model weights."),qz.forEach(t),Mrr=i(ql),Lge=n(ql,"P",{});var Jlt=s(Lge);Err=r(Jlt,"Examples:"),Jlt.forEach(t),yrr=i(ql),m(Iw.$$.fragment,ql),ql.forEach(t),wrr=i(Nl),uo=n(Nl,"DIV",{class:!0});var ha=s(uo);m(Dw.$$.fragment,ha),Arr=i(ha),Bge=n(ha,"P",{});var Ylt=s(Bge);Lrr=r(Ylt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Ylt.forEach(t),Brr=i(ha),hn=n(ha,"P",{});var l4=s(hn);xrr=r(l4,"The model class to instantiate is selected based on the "),xge=n(l4,"CODE",{});var Klt=s(xge);krr=r(Klt,"model_type"),Klt.forEach(t),Rrr=r(l4,` property of the config object (either
passed as an argument or loaded from `),kge=n(l4,"CODE",{});var Zlt=s(kge);Srr=r(Zlt,"pretrained_model_name_or_path"),Zlt.forEach(t),Prr=r(l4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rge=n(l4,"CODE",{});var eit=s(Rge);$rr=r(eit,"pretrained_model_name_or_path"),eit.forEach(t),Irr=r(l4,":"),l4.forEach(t),Drr=i(ha),Y=n(ha,"UL",{});var ee=s(Y);mT=n(ee,"LI",{});var oAe=s(mT);Sge=n(oAe,"STRONG",{});var oit=s(Sge);jrr=r(oit,"albert"),oit.forEach(t),Nrr=r(oAe," \u2014 "),Wq=n(oAe,"A",{href:!0});var rit=s(Wq);qrr=r(rit,"TFAlbertForMaskedLM"),rit.forEach(t),Grr=r(oAe," (ALBERT model)"),oAe.forEach(t),Orr=i(ee),gT=n(ee,"LI",{});var rAe=s(gT);Pge=n(rAe,"STRONG",{});var tit=s(Pge);Xrr=r(tit,"bert"),tit.forEach(t),Vrr=r(rAe," \u2014 "),Qq=n(rAe,"A",{href:!0});var ait=s(Qq);zrr=r(ait,"TFBertForMaskedLM"),ait.forEach(t),Wrr=r(rAe," (BERT model)"),rAe.forEach(t),Qrr=i(ee),hT=n(ee,"LI",{});var tAe=s(hT);$ge=n(tAe,"STRONG",{});var nit=s($ge);Hrr=r(nit,"camembert"),nit.forEach(t),Urr=r(tAe," \u2014 "),Hq=n(tAe,"A",{href:!0});var sit=s(Hq);Jrr=r(sit,"TFCamembertForMaskedLM"),sit.forEach(t),Yrr=r(tAe," (CamemBERT model)"),tAe.forEach(t),Krr=i(ee),pT=n(ee,"LI",{});var aAe=s(pT);Ige=n(aAe,"STRONG",{});var lit=s(Ige);Zrr=r(lit,"convbert"),lit.forEach(t),etr=r(aAe," \u2014 "),Uq=n(aAe,"A",{href:!0});var iit=s(Uq);otr=r(iit,"TFConvBertForMaskedLM"),iit.forEach(t),rtr=r(aAe," (ConvBERT model)"),aAe.forEach(t),ttr=i(ee),_T=n(ee,"LI",{});var nAe=s(_T);Dge=n(nAe,"STRONG",{});var dit=s(Dge);atr=r(dit,"deberta"),dit.forEach(t),ntr=r(nAe," \u2014 "),Jq=n(nAe,"A",{href:!0});var cit=s(Jq);str=r(cit,"TFDebertaForMaskedLM"),cit.forEach(t),ltr=r(nAe," (DeBERTa model)"),nAe.forEach(t),itr=i(ee),uT=n(ee,"LI",{});var sAe=s(uT);jge=n(sAe,"STRONG",{});var fit=s(jge);dtr=r(fit,"deberta-v2"),fit.forEach(t),ctr=r(sAe," \u2014 "),Yq=n(sAe,"A",{href:!0});var mit=s(Yq);ftr=r(mit,"TFDebertaV2ForMaskedLM"),mit.forEach(t),mtr=r(sAe," (DeBERTa-v2 model)"),sAe.forEach(t),gtr=i(ee),bT=n(ee,"LI",{});var lAe=s(bT);Nge=n(lAe,"STRONG",{});var git=s(Nge);htr=r(git,"distilbert"),git.forEach(t),ptr=r(lAe," \u2014 "),Kq=n(lAe,"A",{href:!0});var hit=s(Kq);_tr=r(hit,"TFDistilBertForMaskedLM"),hit.forEach(t),utr=r(lAe," (DistilBERT model)"),lAe.forEach(t),btr=i(ee),vT=n(ee,"LI",{});var iAe=s(vT);qge=n(iAe,"STRONG",{});var pit=s(qge);vtr=r(pit,"electra"),pit.forEach(t),Ttr=r(iAe," \u2014 "),Zq=n(iAe,"A",{href:!0});var _it=s(Zq);Ftr=r(_it,"TFElectraForMaskedLM"),_it.forEach(t),Ctr=r(iAe," (ELECTRA model)"),iAe.forEach(t),Mtr=i(ee),TT=n(ee,"LI",{});var dAe=s(TT);Gge=n(dAe,"STRONG",{});var uit=s(Gge);Etr=r(uit,"flaubert"),uit.forEach(t),ytr=r(dAe," \u2014 "),eG=n(dAe,"A",{href:!0});var bit=s(eG);wtr=r(bit,"TFFlaubertWithLMHeadModel"),bit.forEach(t),Atr=r(dAe," (FlauBERT model)"),dAe.forEach(t),Ltr=i(ee),FT=n(ee,"LI",{});var cAe=s(FT);Oge=n(cAe,"STRONG",{});var vit=s(Oge);Btr=r(vit,"funnel"),vit.forEach(t),xtr=r(cAe," \u2014 "),oG=n(cAe,"A",{href:!0});var Tit=s(oG);ktr=r(Tit,"TFFunnelForMaskedLM"),Tit.forEach(t),Rtr=r(cAe," (Funnel Transformer model)"),cAe.forEach(t),Str=i(ee),CT=n(ee,"LI",{});var fAe=s(CT);Xge=n(fAe,"STRONG",{});var Fit=s(Xge);Ptr=r(Fit,"layoutlm"),Fit.forEach(t),$tr=r(fAe," \u2014 "),rG=n(fAe,"A",{href:!0});var Cit=s(rG);Itr=r(Cit,"TFLayoutLMForMaskedLM"),Cit.forEach(t),Dtr=r(fAe," (LayoutLM model)"),fAe.forEach(t),jtr=i(ee),MT=n(ee,"LI",{});var mAe=s(MT);Vge=n(mAe,"STRONG",{});var Mit=s(Vge);Ntr=r(Mit,"longformer"),Mit.forEach(t),qtr=r(mAe," \u2014 "),tG=n(mAe,"A",{href:!0});var Eit=s(tG);Gtr=r(Eit,"TFLongformerForMaskedLM"),Eit.forEach(t),Otr=r(mAe," (Longformer model)"),mAe.forEach(t),Xtr=i(ee),ET=n(ee,"LI",{});var gAe=s(ET);zge=n(gAe,"STRONG",{});var yit=s(zge);Vtr=r(yit,"mobilebert"),yit.forEach(t),ztr=r(gAe," \u2014 "),aG=n(gAe,"A",{href:!0});var wit=s(aG);Wtr=r(wit,"TFMobileBertForMaskedLM"),wit.forEach(t),Qtr=r(gAe," (MobileBERT model)"),gAe.forEach(t),Htr=i(ee),yT=n(ee,"LI",{});var hAe=s(yT);Wge=n(hAe,"STRONG",{});var Ait=s(Wge);Utr=r(Ait,"mpnet"),Ait.forEach(t),Jtr=r(hAe," \u2014 "),nG=n(hAe,"A",{href:!0});var Lit=s(nG);Ytr=r(Lit,"TFMPNetForMaskedLM"),Lit.forEach(t),Ktr=r(hAe," (MPNet model)"),hAe.forEach(t),Ztr=i(ee),wT=n(ee,"LI",{});var pAe=s(wT);Qge=n(pAe,"STRONG",{});var Bit=s(Qge);ear=r(Bit,"rembert"),Bit.forEach(t),oar=r(pAe," \u2014 "),sG=n(pAe,"A",{href:!0});var xit=s(sG);rar=r(xit,"TFRemBertForMaskedLM"),xit.forEach(t),tar=r(pAe," (RemBERT model)"),pAe.forEach(t),aar=i(ee),AT=n(ee,"LI",{});var _Ae=s(AT);Hge=n(_Ae,"STRONG",{});var kit=s(Hge);nar=r(kit,"roberta"),kit.forEach(t),sar=r(_Ae," \u2014 "),lG=n(_Ae,"A",{href:!0});var Rit=s(lG);lar=r(Rit,"TFRobertaForMaskedLM"),Rit.forEach(t),iar=r(_Ae," (RoBERTa model)"),_Ae.forEach(t),dar=i(ee),LT=n(ee,"LI",{});var uAe=s(LT);Uge=n(uAe,"STRONG",{});var Sit=s(Uge);car=r(Sit,"roformer"),Sit.forEach(t),far=r(uAe," \u2014 "),iG=n(uAe,"A",{href:!0});var Pit=s(iG);mar=r(Pit,"TFRoFormerForMaskedLM"),Pit.forEach(t),gar=r(uAe," (RoFormer model)"),uAe.forEach(t),har=i(ee),BT=n(ee,"LI",{});var bAe=s(BT);Jge=n(bAe,"STRONG",{});var $it=s(Jge);par=r($it,"tapas"),$it.forEach(t),_ar=r(bAe," \u2014 "),dG=n(bAe,"A",{href:!0});var Iit=s(dG);uar=r(Iit,"TFTapasForMaskedLM"),Iit.forEach(t),bar=r(bAe," (TAPAS model)"),bAe.forEach(t),Tar=i(ee),xT=n(ee,"LI",{});var vAe=s(xT);Yge=n(vAe,"STRONG",{});var Dit=s(Yge);Far=r(Dit,"xlm"),Dit.forEach(t),Car=r(vAe," \u2014 "),cG=n(vAe,"A",{href:!0});var jit=s(cG);Mar=r(jit,"TFXLMWithLMHeadModel"),jit.forEach(t),Ear=r(vAe," (XLM model)"),vAe.forEach(t),yar=i(ee),kT=n(ee,"LI",{});var TAe=s(kT);Kge=n(TAe,"STRONG",{});var Nit=s(Kge);war=r(Nit,"xlm-roberta"),Nit.forEach(t),Aar=r(TAe," \u2014 "),fG=n(TAe,"A",{href:!0});var qit=s(fG);Lar=r(qit,"TFXLMRobertaForMaskedLM"),qit.forEach(t),Bar=r(TAe," (XLM-RoBERTa model)"),TAe.forEach(t),ee.forEach(t),xar=i(ha),Zge=n(ha,"P",{});var Git=s(Zge);kar=r(Git,"Examples:"),Git.forEach(t),Rar=i(ha),m(jw.$$.fragment,ha),ha.forEach(t),Nl.forEach(t),mxe=i(c),_c=n(c,"H2",{class:!0});var CRe=s(_c);RT=n(CRe,"A",{id:!0,class:!0,href:!0});var Oit=s(RT);ehe=n(Oit,"SPAN",{});var Xit=s(ehe);m(Nw.$$.fragment,Xit),Xit.forEach(t),Oit.forEach(t),Sar=i(CRe),ohe=n(CRe,"SPAN",{});var Vit=s(ohe);Par=r(Vit,"TFAutoModelForSeq2SeqLM"),Vit.forEach(t),CRe.forEach(t),gxe=i(c),Cr=n(c,"DIV",{class:!0});var Gl=s(Cr);m(qw.$$.fragment,Gl),$ar=i(Gl),uc=n(Gl,"P",{});var Gz=s(uc);Iar=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),rhe=n(Gz,"CODE",{});var zit=s(rhe);Dar=r(zit,"from_pretrained()"),zit.forEach(t),jar=r(Gz,"class method or the "),the=n(Gz,"CODE",{});var Wit=s(the);Nar=r(Wit,"from_config()"),Wit.forEach(t),qar=r(Gz,`class
method.`),Gz.forEach(t),Gar=i(Gl),Gw=n(Gl,"P",{});var MRe=s(Gw);Oar=r(MRe,"This class cannot be instantiated directly using "),ahe=n(MRe,"CODE",{});var Qit=s(ahe);Xar=r(Qit,"__init__()"),Qit.forEach(t),Var=r(MRe," (throws an error)."),MRe.forEach(t),zar=i(Gl),pt=n(Gl,"DIV",{class:!0});var Ol=s(pt);m(Ow.$$.fragment,Ol),War=i(Ol),nhe=n(Ol,"P",{});var Hit=s(nhe);Qar=r(Hit,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Hit.forEach(t),Har=i(Ol),bc=n(Ol,"P",{});var Oz=s(bc);Uar=r(Oz,`Note:
Loading a model from its configuration file does `),she=n(Oz,"STRONG",{});var Uit=s(she);Jar=r(Uit,"not"),Uit.forEach(t),Yar=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),lhe=n(Oz,"CODE",{});var Jit=s(lhe);Kar=r(Jit,"from_pretrained()"),Jit.forEach(t),Zar=r(Oz,"to load the model weights."),Oz.forEach(t),enr=i(Ol),ihe=n(Ol,"P",{});var Yit=s(ihe);onr=r(Yit,"Examples:"),Yit.forEach(t),rnr=i(Ol),m(Xw.$$.fragment,Ol),Ol.forEach(t),tnr=i(Gl),bo=n(Gl,"DIV",{class:!0});var pa=s(bo);m(Vw.$$.fragment,pa),anr=i(pa),dhe=n(pa,"P",{});var Kit=s(dhe);nnr=r(Kit,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Kit.forEach(t),snr=i(pa),pn=n(pa,"P",{});var i4=s(pn);lnr=r(i4,"The model class to instantiate is selected based on the "),che=n(i4,"CODE",{});var Zit=s(che);inr=r(Zit,"model_type"),Zit.forEach(t),dnr=r(i4,` property of the config object (either
passed as an argument or loaded from `),fhe=n(i4,"CODE",{});var edt=s(fhe);cnr=r(edt,"pretrained_model_name_or_path"),edt.forEach(t),fnr=r(i4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mhe=n(i4,"CODE",{});var odt=s(mhe);mnr=r(odt,"pretrained_model_name_or_path"),odt.forEach(t),gnr=r(i4,":"),i4.forEach(t),hnr=i(pa),_e=n(pa,"UL",{});var Ee=s(_e);ST=n(Ee,"LI",{});var FAe=s(ST);ghe=n(FAe,"STRONG",{});var rdt=s(ghe);pnr=r(rdt,"bart"),rdt.forEach(t),_nr=r(FAe," \u2014 "),mG=n(FAe,"A",{href:!0});var tdt=s(mG);unr=r(tdt,"TFBartForConditionalGeneration"),tdt.forEach(t),bnr=r(FAe," (BART model)"),FAe.forEach(t),vnr=i(Ee),PT=n(Ee,"LI",{});var CAe=s(PT);hhe=n(CAe,"STRONG",{});var adt=s(hhe);Tnr=r(adt,"blenderbot"),adt.forEach(t),Fnr=r(CAe," \u2014 "),gG=n(CAe,"A",{href:!0});var ndt=s(gG);Cnr=r(ndt,"TFBlenderbotForConditionalGeneration"),ndt.forEach(t),Mnr=r(CAe," (Blenderbot model)"),CAe.forEach(t),Enr=i(Ee),$T=n(Ee,"LI",{});var MAe=s($T);phe=n(MAe,"STRONG",{});var sdt=s(phe);ynr=r(sdt,"blenderbot-small"),sdt.forEach(t),wnr=r(MAe," \u2014 "),hG=n(MAe,"A",{href:!0});var ldt=s(hG);Anr=r(ldt,"TFBlenderbotSmallForConditionalGeneration"),ldt.forEach(t),Lnr=r(MAe," (BlenderbotSmall model)"),MAe.forEach(t),Bnr=i(Ee),IT=n(Ee,"LI",{});var EAe=s(IT);_he=n(EAe,"STRONG",{});var idt=s(_he);xnr=r(idt,"encoder-decoder"),idt.forEach(t),knr=r(EAe," \u2014 "),pG=n(EAe,"A",{href:!0});var ddt=s(pG);Rnr=r(ddt,"TFEncoderDecoderModel"),ddt.forEach(t),Snr=r(EAe," (Encoder decoder model)"),EAe.forEach(t),Pnr=i(Ee),DT=n(Ee,"LI",{});var yAe=s(DT);uhe=n(yAe,"STRONG",{});var cdt=s(uhe);$nr=r(cdt,"led"),cdt.forEach(t),Inr=r(yAe," \u2014 "),_G=n(yAe,"A",{href:!0});var fdt=s(_G);Dnr=r(fdt,"TFLEDForConditionalGeneration"),fdt.forEach(t),jnr=r(yAe," (LED model)"),yAe.forEach(t),Nnr=i(Ee),jT=n(Ee,"LI",{});var wAe=s(jT);bhe=n(wAe,"STRONG",{});var mdt=s(bhe);qnr=r(mdt,"marian"),mdt.forEach(t),Gnr=r(wAe," \u2014 "),uG=n(wAe,"A",{href:!0});var gdt=s(uG);Onr=r(gdt,"TFMarianMTModel"),gdt.forEach(t),Xnr=r(wAe," (Marian model)"),wAe.forEach(t),Vnr=i(Ee),NT=n(Ee,"LI",{});var AAe=s(NT);vhe=n(AAe,"STRONG",{});var hdt=s(vhe);znr=r(hdt,"mbart"),hdt.forEach(t),Wnr=r(AAe," \u2014 "),bG=n(AAe,"A",{href:!0});var pdt=s(bG);Qnr=r(pdt,"TFMBartForConditionalGeneration"),pdt.forEach(t),Hnr=r(AAe," (mBART model)"),AAe.forEach(t),Unr=i(Ee),qT=n(Ee,"LI",{});var LAe=s(qT);The=n(LAe,"STRONG",{});var _dt=s(The);Jnr=r(_dt,"mt5"),_dt.forEach(t),Ynr=r(LAe," \u2014 "),vG=n(LAe,"A",{href:!0});var udt=s(vG);Knr=r(udt,"TFMT5ForConditionalGeneration"),udt.forEach(t),Znr=r(LAe," (mT5 model)"),LAe.forEach(t),esr=i(Ee),GT=n(Ee,"LI",{});var BAe=s(GT);Fhe=n(BAe,"STRONG",{});var bdt=s(Fhe);osr=r(bdt,"pegasus"),bdt.forEach(t),rsr=r(BAe," \u2014 "),TG=n(BAe,"A",{href:!0});var vdt=s(TG);tsr=r(vdt,"TFPegasusForConditionalGeneration"),vdt.forEach(t),asr=r(BAe," (Pegasus model)"),BAe.forEach(t),nsr=i(Ee),OT=n(Ee,"LI",{});var xAe=s(OT);Che=n(xAe,"STRONG",{});var Tdt=s(Che);ssr=r(Tdt,"t5"),Tdt.forEach(t),lsr=r(xAe," \u2014 "),FG=n(xAe,"A",{href:!0});var Fdt=s(FG);isr=r(Fdt,"TFT5ForConditionalGeneration"),Fdt.forEach(t),dsr=r(xAe," (T5 model)"),xAe.forEach(t),Ee.forEach(t),csr=i(pa),Mhe=n(pa,"P",{});var Cdt=s(Mhe);fsr=r(Cdt,"Examples:"),Cdt.forEach(t),msr=i(pa),m(zw.$$.fragment,pa),pa.forEach(t),Gl.forEach(t),hxe=i(c),vc=n(c,"H2",{class:!0});var ERe=s(vc);XT=n(ERe,"A",{id:!0,class:!0,href:!0});var Mdt=s(XT);Ehe=n(Mdt,"SPAN",{});var Edt=s(Ehe);m(Ww.$$.fragment,Edt),Edt.forEach(t),Mdt.forEach(t),gsr=i(ERe),yhe=n(ERe,"SPAN",{});var ydt=s(yhe);hsr=r(ydt,"TFAutoModelForSequenceClassification"),ydt.forEach(t),ERe.forEach(t),pxe=i(c),Mr=n(c,"DIV",{class:!0});var Xl=s(Mr);m(Qw.$$.fragment,Xl),psr=i(Xl),Tc=n(Xl,"P",{});var Xz=s(Tc);_sr=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),whe=n(Xz,"CODE",{});var wdt=s(whe);usr=r(wdt,"from_pretrained()"),wdt.forEach(t),bsr=r(Xz,"class method or the "),Ahe=n(Xz,"CODE",{});var Adt=s(Ahe);vsr=r(Adt,"from_config()"),Adt.forEach(t),Tsr=r(Xz,`class
method.`),Xz.forEach(t),Fsr=i(Xl),Hw=n(Xl,"P",{});var yRe=s(Hw);Csr=r(yRe,"This class cannot be instantiated directly using "),Lhe=n(yRe,"CODE",{});var Ldt=s(Lhe);Msr=r(Ldt,"__init__()"),Ldt.forEach(t),Esr=r(yRe," (throws an error)."),yRe.forEach(t),ysr=i(Xl),_t=n(Xl,"DIV",{class:!0});var Vl=s(_t);m(Uw.$$.fragment,Vl),wsr=i(Vl),Bhe=n(Vl,"P",{});var Bdt=s(Bhe);Asr=r(Bdt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Bdt.forEach(t),Lsr=i(Vl),Fc=n(Vl,"P",{});var Vz=s(Fc);Bsr=r(Vz,`Note:
Loading a model from its configuration file does `),xhe=n(Vz,"STRONG",{});var xdt=s(xhe);xsr=r(xdt,"not"),xdt.forEach(t),ksr=r(Vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),khe=n(Vz,"CODE",{});var kdt=s(khe);Rsr=r(kdt,"from_pretrained()"),kdt.forEach(t),Ssr=r(Vz,"to load the model weights."),Vz.forEach(t),Psr=i(Vl),Rhe=n(Vl,"P",{});var Rdt=s(Rhe);$sr=r(Rdt,"Examples:"),Rdt.forEach(t),Isr=i(Vl),m(Jw.$$.fragment,Vl),Vl.forEach(t),Dsr=i(Xl),vo=n(Xl,"DIV",{class:!0});var _a=s(vo);m(Yw.$$.fragment,_a),jsr=i(_a),She=n(_a,"P",{});var Sdt=s(She);Nsr=r(Sdt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Sdt.forEach(t),qsr=i(_a),_n=n(_a,"P",{});var d4=s(_n);Gsr=r(d4,"The model class to instantiate is selected based on the "),Phe=n(d4,"CODE",{});var Pdt=s(Phe);Osr=r(Pdt,"model_type"),Pdt.forEach(t),Xsr=r(d4,` property of the config object (either
passed as an argument or loaded from `),$he=n(d4,"CODE",{});var $dt=s($he);Vsr=r($dt,"pretrained_model_name_or_path"),$dt.forEach(t),zsr=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ihe=n(d4,"CODE",{});var Idt=s(Ihe);Wsr=r(Idt,"pretrained_model_name_or_path"),Idt.forEach(t),Qsr=r(d4,":"),d4.forEach(t),Hsr=i(_a),X=n(_a,"UL",{});var W=s(X);VT=n(W,"LI",{});var kAe=s(VT);Dhe=n(kAe,"STRONG",{});var Ddt=s(Dhe);Usr=r(Ddt,"albert"),Ddt.forEach(t),Jsr=r(kAe," \u2014 "),CG=n(kAe,"A",{href:!0});var jdt=s(CG);Ysr=r(jdt,"TFAlbertForSequenceClassification"),jdt.forEach(t),Ksr=r(kAe," (ALBERT model)"),kAe.forEach(t),Zsr=i(W),zT=n(W,"LI",{});var RAe=s(zT);jhe=n(RAe,"STRONG",{});var Ndt=s(jhe);elr=r(Ndt,"bert"),Ndt.forEach(t),olr=r(RAe," \u2014 "),MG=n(RAe,"A",{href:!0});var qdt=s(MG);rlr=r(qdt,"TFBertForSequenceClassification"),qdt.forEach(t),tlr=r(RAe," (BERT model)"),RAe.forEach(t),alr=i(W),WT=n(W,"LI",{});var SAe=s(WT);Nhe=n(SAe,"STRONG",{});var Gdt=s(Nhe);nlr=r(Gdt,"camembert"),Gdt.forEach(t),slr=r(SAe," \u2014 "),EG=n(SAe,"A",{href:!0});var Odt=s(EG);llr=r(Odt,"TFCamembertForSequenceClassification"),Odt.forEach(t),ilr=r(SAe," (CamemBERT model)"),SAe.forEach(t),dlr=i(W),QT=n(W,"LI",{});var PAe=s(QT);qhe=n(PAe,"STRONG",{});var Xdt=s(qhe);clr=r(Xdt,"convbert"),Xdt.forEach(t),flr=r(PAe," \u2014 "),yG=n(PAe,"A",{href:!0});var Vdt=s(yG);mlr=r(Vdt,"TFConvBertForSequenceClassification"),Vdt.forEach(t),glr=r(PAe," (ConvBERT model)"),PAe.forEach(t),hlr=i(W),HT=n(W,"LI",{});var $Ae=s(HT);Ghe=n($Ae,"STRONG",{});var zdt=s(Ghe);plr=r(zdt,"ctrl"),zdt.forEach(t),_lr=r($Ae," \u2014 "),wG=n($Ae,"A",{href:!0});var Wdt=s(wG);ulr=r(Wdt,"TFCTRLForSequenceClassification"),Wdt.forEach(t),blr=r($Ae," (CTRL model)"),$Ae.forEach(t),vlr=i(W),UT=n(W,"LI",{});var IAe=s(UT);Ohe=n(IAe,"STRONG",{});var Qdt=s(Ohe);Tlr=r(Qdt,"deberta"),Qdt.forEach(t),Flr=r(IAe," \u2014 "),AG=n(IAe,"A",{href:!0});var Hdt=s(AG);Clr=r(Hdt,"TFDebertaForSequenceClassification"),Hdt.forEach(t),Mlr=r(IAe," (DeBERTa model)"),IAe.forEach(t),Elr=i(W),JT=n(W,"LI",{});var DAe=s(JT);Xhe=n(DAe,"STRONG",{});var Udt=s(Xhe);ylr=r(Udt,"deberta-v2"),Udt.forEach(t),wlr=r(DAe," \u2014 "),LG=n(DAe,"A",{href:!0});var Jdt=s(LG);Alr=r(Jdt,"TFDebertaV2ForSequenceClassification"),Jdt.forEach(t),Llr=r(DAe," (DeBERTa-v2 model)"),DAe.forEach(t),Blr=i(W),YT=n(W,"LI",{});var jAe=s(YT);Vhe=n(jAe,"STRONG",{});var Ydt=s(Vhe);xlr=r(Ydt,"distilbert"),Ydt.forEach(t),klr=r(jAe," \u2014 "),BG=n(jAe,"A",{href:!0});var Kdt=s(BG);Rlr=r(Kdt,"TFDistilBertForSequenceClassification"),Kdt.forEach(t),Slr=r(jAe," (DistilBERT model)"),jAe.forEach(t),Plr=i(W),KT=n(W,"LI",{});var NAe=s(KT);zhe=n(NAe,"STRONG",{});var Zdt=s(zhe);$lr=r(Zdt,"electra"),Zdt.forEach(t),Ilr=r(NAe," \u2014 "),xG=n(NAe,"A",{href:!0});var ect=s(xG);Dlr=r(ect,"TFElectraForSequenceClassification"),ect.forEach(t),jlr=r(NAe," (ELECTRA model)"),NAe.forEach(t),Nlr=i(W),ZT=n(W,"LI",{});var qAe=s(ZT);Whe=n(qAe,"STRONG",{});var oct=s(Whe);qlr=r(oct,"flaubert"),oct.forEach(t),Glr=r(qAe," \u2014 "),kG=n(qAe,"A",{href:!0});var rct=s(kG);Olr=r(rct,"TFFlaubertForSequenceClassification"),rct.forEach(t),Xlr=r(qAe," (FlauBERT model)"),qAe.forEach(t),Vlr=i(W),eF=n(W,"LI",{});var GAe=s(eF);Qhe=n(GAe,"STRONG",{});var tct=s(Qhe);zlr=r(tct,"funnel"),tct.forEach(t),Wlr=r(GAe," \u2014 "),RG=n(GAe,"A",{href:!0});var act=s(RG);Qlr=r(act,"TFFunnelForSequenceClassification"),act.forEach(t),Hlr=r(GAe," (Funnel Transformer model)"),GAe.forEach(t),Ulr=i(W),oF=n(W,"LI",{});var OAe=s(oF);Hhe=n(OAe,"STRONG",{});var nct=s(Hhe);Jlr=r(nct,"gpt2"),nct.forEach(t),Ylr=r(OAe," \u2014 "),SG=n(OAe,"A",{href:!0});var sct=s(SG);Klr=r(sct,"TFGPT2ForSequenceClassification"),sct.forEach(t),Zlr=r(OAe," (OpenAI GPT-2 model)"),OAe.forEach(t),eir=i(W),rF=n(W,"LI",{});var XAe=s(rF);Uhe=n(XAe,"STRONG",{});var lct=s(Uhe);oir=r(lct,"layoutlm"),lct.forEach(t),rir=r(XAe," \u2014 "),PG=n(XAe,"A",{href:!0});var ict=s(PG);tir=r(ict,"TFLayoutLMForSequenceClassification"),ict.forEach(t),air=r(XAe," (LayoutLM model)"),XAe.forEach(t),nir=i(W),tF=n(W,"LI",{});var VAe=s(tF);Jhe=n(VAe,"STRONG",{});var dct=s(Jhe);sir=r(dct,"longformer"),dct.forEach(t),lir=r(VAe," \u2014 "),$G=n(VAe,"A",{href:!0});var cct=s($G);iir=r(cct,"TFLongformerForSequenceClassification"),cct.forEach(t),dir=r(VAe," (Longformer model)"),VAe.forEach(t),cir=i(W),aF=n(W,"LI",{});var zAe=s(aF);Yhe=n(zAe,"STRONG",{});var fct=s(Yhe);fir=r(fct,"mobilebert"),fct.forEach(t),mir=r(zAe," \u2014 "),IG=n(zAe,"A",{href:!0});var mct=s(IG);gir=r(mct,"TFMobileBertForSequenceClassification"),mct.forEach(t),hir=r(zAe," (MobileBERT model)"),zAe.forEach(t),pir=i(W),nF=n(W,"LI",{});var WAe=s(nF);Khe=n(WAe,"STRONG",{});var gct=s(Khe);_ir=r(gct,"mpnet"),gct.forEach(t),uir=r(WAe," \u2014 "),DG=n(WAe,"A",{href:!0});var hct=s(DG);bir=r(hct,"TFMPNetForSequenceClassification"),hct.forEach(t),vir=r(WAe," (MPNet model)"),WAe.forEach(t),Tir=i(W),sF=n(W,"LI",{});var QAe=s(sF);Zhe=n(QAe,"STRONG",{});var pct=s(Zhe);Fir=r(pct,"openai-gpt"),pct.forEach(t),Cir=r(QAe," \u2014 "),jG=n(QAe,"A",{href:!0});var _ct=s(jG);Mir=r(_ct,"TFOpenAIGPTForSequenceClassification"),_ct.forEach(t),Eir=r(QAe," (OpenAI GPT model)"),QAe.forEach(t),yir=i(W),lF=n(W,"LI",{});var HAe=s(lF);epe=n(HAe,"STRONG",{});var uct=s(epe);wir=r(uct,"rembert"),uct.forEach(t),Air=r(HAe," \u2014 "),NG=n(HAe,"A",{href:!0});var bct=s(NG);Lir=r(bct,"TFRemBertForSequenceClassification"),bct.forEach(t),Bir=r(HAe," (RemBERT model)"),HAe.forEach(t),xir=i(W),iF=n(W,"LI",{});var UAe=s(iF);ope=n(UAe,"STRONG",{});var vct=s(ope);kir=r(vct,"roberta"),vct.forEach(t),Rir=r(UAe," \u2014 "),qG=n(UAe,"A",{href:!0});var Tct=s(qG);Sir=r(Tct,"TFRobertaForSequenceClassification"),Tct.forEach(t),Pir=r(UAe," (RoBERTa model)"),UAe.forEach(t),$ir=i(W),dF=n(W,"LI",{});var JAe=s(dF);rpe=n(JAe,"STRONG",{});var Fct=s(rpe);Iir=r(Fct,"roformer"),Fct.forEach(t),Dir=r(JAe," \u2014 "),GG=n(JAe,"A",{href:!0});var Cct=s(GG);jir=r(Cct,"TFRoFormerForSequenceClassification"),Cct.forEach(t),Nir=r(JAe," (RoFormer model)"),JAe.forEach(t),qir=i(W),cF=n(W,"LI",{});var YAe=s(cF);tpe=n(YAe,"STRONG",{});var Mct=s(tpe);Gir=r(Mct,"tapas"),Mct.forEach(t),Oir=r(YAe," \u2014 "),OG=n(YAe,"A",{href:!0});var Ect=s(OG);Xir=r(Ect,"TFTapasForSequenceClassification"),Ect.forEach(t),Vir=r(YAe," (TAPAS model)"),YAe.forEach(t),zir=i(W),fF=n(W,"LI",{});var KAe=s(fF);ape=n(KAe,"STRONG",{});var yct=s(ape);Wir=r(yct,"transfo-xl"),yct.forEach(t),Qir=r(KAe," \u2014 "),XG=n(KAe,"A",{href:!0});var wct=s(XG);Hir=r(wct,"TFTransfoXLForSequenceClassification"),wct.forEach(t),Uir=r(KAe," (Transformer-XL model)"),KAe.forEach(t),Jir=i(W),mF=n(W,"LI",{});var ZAe=s(mF);npe=n(ZAe,"STRONG",{});var Act=s(npe);Yir=r(Act,"xlm"),Act.forEach(t),Kir=r(ZAe," \u2014 "),VG=n(ZAe,"A",{href:!0});var Lct=s(VG);Zir=r(Lct,"TFXLMForSequenceClassification"),Lct.forEach(t),edr=r(ZAe," (XLM model)"),ZAe.forEach(t),odr=i(W),gF=n(W,"LI",{});var eLe=s(gF);spe=n(eLe,"STRONG",{});var Bct=s(spe);rdr=r(Bct,"xlm-roberta"),Bct.forEach(t),tdr=r(eLe," \u2014 "),zG=n(eLe,"A",{href:!0});var xct=s(zG);adr=r(xct,"TFXLMRobertaForSequenceClassification"),xct.forEach(t),ndr=r(eLe," (XLM-RoBERTa model)"),eLe.forEach(t),sdr=i(W),hF=n(W,"LI",{});var oLe=s(hF);lpe=n(oLe,"STRONG",{});var kct=s(lpe);ldr=r(kct,"xlnet"),kct.forEach(t),idr=r(oLe," \u2014 "),WG=n(oLe,"A",{href:!0});var Rct=s(WG);ddr=r(Rct,"TFXLNetForSequenceClassification"),Rct.forEach(t),cdr=r(oLe," (XLNet model)"),oLe.forEach(t),W.forEach(t),fdr=i(_a),ipe=n(_a,"P",{});var Sct=s(ipe);mdr=r(Sct,"Examples:"),Sct.forEach(t),gdr=i(_a),m(Kw.$$.fragment,_a),_a.forEach(t),Xl.forEach(t),_xe=i(c),Cc=n(c,"H2",{class:!0});var wRe=s(Cc);pF=n(wRe,"A",{id:!0,class:!0,href:!0});var Pct=s(pF);dpe=n(Pct,"SPAN",{});var $ct=s(dpe);m(Zw.$$.fragment,$ct),$ct.forEach(t),Pct.forEach(t),hdr=i(wRe),cpe=n(wRe,"SPAN",{});var Ict=s(cpe);pdr=r(Ict,"TFAutoModelForMultipleChoice"),Ict.forEach(t),wRe.forEach(t),uxe=i(c),Er=n(c,"DIV",{class:!0});var zl=s(Er);m(e6.$$.fragment,zl),_dr=i(zl),Mc=n(zl,"P",{});var zz=s(Mc);udr=r(zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),fpe=n(zz,"CODE",{});var Dct=s(fpe);bdr=r(Dct,"from_pretrained()"),Dct.forEach(t),vdr=r(zz,"class method or the "),mpe=n(zz,"CODE",{});var jct=s(mpe);Tdr=r(jct,"from_config()"),jct.forEach(t),Fdr=r(zz,`class
method.`),zz.forEach(t),Cdr=i(zl),o6=n(zl,"P",{});var ARe=s(o6);Mdr=r(ARe,"This class cannot be instantiated directly using "),gpe=n(ARe,"CODE",{});var Nct=s(gpe);Edr=r(Nct,"__init__()"),Nct.forEach(t),ydr=r(ARe," (throws an error)."),ARe.forEach(t),wdr=i(zl),ut=n(zl,"DIV",{class:!0});var Wl=s(ut);m(r6.$$.fragment,Wl),Adr=i(Wl),hpe=n(Wl,"P",{});var qct=s(hpe);Ldr=r(qct,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),qct.forEach(t),Bdr=i(Wl),Ec=n(Wl,"P",{});var Wz=s(Ec);xdr=r(Wz,`Note:
Loading a model from its configuration file does `),ppe=n(Wz,"STRONG",{});var Gct=s(ppe);kdr=r(Gct,"not"),Gct.forEach(t),Rdr=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_pe=n(Wz,"CODE",{});var Oct=s(_pe);Sdr=r(Oct,"from_pretrained()"),Oct.forEach(t),Pdr=r(Wz,"to load the model weights."),Wz.forEach(t),$dr=i(Wl),upe=n(Wl,"P",{});var Xct=s(upe);Idr=r(Xct,"Examples:"),Xct.forEach(t),Ddr=i(Wl),m(t6.$$.fragment,Wl),Wl.forEach(t),jdr=i(zl),To=n(zl,"DIV",{class:!0});var ua=s(To);m(a6.$$.fragment,ua),Ndr=i(ua),bpe=n(ua,"P",{});var Vct=s(bpe);qdr=r(Vct,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Vct.forEach(t),Gdr=i(ua),un=n(ua,"P",{});var c4=s(un);Odr=r(c4,"The model class to instantiate is selected based on the "),vpe=n(c4,"CODE",{});var zct=s(vpe);Xdr=r(zct,"model_type"),zct.forEach(t),Vdr=r(c4,` property of the config object (either
passed as an argument or loaded from `),Tpe=n(c4,"CODE",{});var Wct=s(Tpe);zdr=r(Wct,"pretrained_model_name_or_path"),Wct.forEach(t),Wdr=r(c4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fpe=n(c4,"CODE",{});var Qct=s(Fpe);Qdr=r(Qct,"pretrained_model_name_or_path"),Qct.forEach(t),Hdr=r(c4,":"),c4.forEach(t),Udr=i(ua),te=n(ua,"UL",{});var ne=s(te);_F=n(ne,"LI",{});var rLe=s(_F);Cpe=n(rLe,"STRONG",{});var Hct=s(Cpe);Jdr=r(Hct,"albert"),Hct.forEach(t),Ydr=r(rLe," \u2014 "),QG=n(rLe,"A",{href:!0});var Uct=s(QG);Kdr=r(Uct,"TFAlbertForMultipleChoice"),Uct.forEach(t),Zdr=r(rLe," (ALBERT model)"),rLe.forEach(t),ecr=i(ne),uF=n(ne,"LI",{});var tLe=s(uF);Mpe=n(tLe,"STRONG",{});var Jct=s(Mpe);ocr=r(Jct,"bert"),Jct.forEach(t),rcr=r(tLe," \u2014 "),HG=n(tLe,"A",{href:!0});var Yct=s(HG);tcr=r(Yct,"TFBertForMultipleChoice"),Yct.forEach(t),acr=r(tLe," (BERT model)"),tLe.forEach(t),ncr=i(ne),bF=n(ne,"LI",{});var aLe=s(bF);Epe=n(aLe,"STRONG",{});var Kct=s(Epe);scr=r(Kct,"camembert"),Kct.forEach(t),lcr=r(aLe," \u2014 "),UG=n(aLe,"A",{href:!0});var Zct=s(UG);icr=r(Zct,"TFCamembertForMultipleChoice"),Zct.forEach(t),dcr=r(aLe," (CamemBERT model)"),aLe.forEach(t),ccr=i(ne),vF=n(ne,"LI",{});var nLe=s(vF);ype=n(nLe,"STRONG",{});var eft=s(ype);fcr=r(eft,"convbert"),eft.forEach(t),mcr=r(nLe," \u2014 "),JG=n(nLe,"A",{href:!0});var oft=s(JG);gcr=r(oft,"TFConvBertForMultipleChoice"),oft.forEach(t),hcr=r(nLe," (ConvBERT model)"),nLe.forEach(t),pcr=i(ne),TF=n(ne,"LI",{});var sLe=s(TF);wpe=n(sLe,"STRONG",{});var rft=s(wpe);_cr=r(rft,"distilbert"),rft.forEach(t),ucr=r(sLe," \u2014 "),YG=n(sLe,"A",{href:!0});var tft=s(YG);bcr=r(tft,"TFDistilBertForMultipleChoice"),tft.forEach(t),vcr=r(sLe," (DistilBERT model)"),sLe.forEach(t),Tcr=i(ne),FF=n(ne,"LI",{});var lLe=s(FF);Ape=n(lLe,"STRONG",{});var aft=s(Ape);Fcr=r(aft,"electra"),aft.forEach(t),Ccr=r(lLe," \u2014 "),KG=n(lLe,"A",{href:!0});var nft=s(KG);Mcr=r(nft,"TFElectraForMultipleChoice"),nft.forEach(t),Ecr=r(lLe," (ELECTRA model)"),lLe.forEach(t),ycr=i(ne),CF=n(ne,"LI",{});var iLe=s(CF);Lpe=n(iLe,"STRONG",{});var sft=s(Lpe);wcr=r(sft,"flaubert"),sft.forEach(t),Acr=r(iLe," \u2014 "),ZG=n(iLe,"A",{href:!0});var lft=s(ZG);Lcr=r(lft,"TFFlaubertForMultipleChoice"),lft.forEach(t),Bcr=r(iLe," (FlauBERT model)"),iLe.forEach(t),xcr=i(ne),MF=n(ne,"LI",{});var dLe=s(MF);Bpe=n(dLe,"STRONG",{});var ift=s(Bpe);kcr=r(ift,"funnel"),ift.forEach(t),Rcr=r(dLe," \u2014 "),eO=n(dLe,"A",{href:!0});var dft=s(eO);Scr=r(dft,"TFFunnelForMultipleChoice"),dft.forEach(t),Pcr=r(dLe," (Funnel Transformer model)"),dLe.forEach(t),$cr=i(ne),EF=n(ne,"LI",{});var cLe=s(EF);xpe=n(cLe,"STRONG",{});var cft=s(xpe);Icr=r(cft,"longformer"),cft.forEach(t),Dcr=r(cLe," \u2014 "),oO=n(cLe,"A",{href:!0});var fft=s(oO);jcr=r(fft,"TFLongformerForMultipleChoice"),fft.forEach(t),Ncr=r(cLe," (Longformer model)"),cLe.forEach(t),qcr=i(ne),yF=n(ne,"LI",{});var fLe=s(yF);kpe=n(fLe,"STRONG",{});var mft=s(kpe);Gcr=r(mft,"mobilebert"),mft.forEach(t),Ocr=r(fLe," \u2014 "),rO=n(fLe,"A",{href:!0});var gft=s(rO);Xcr=r(gft,"TFMobileBertForMultipleChoice"),gft.forEach(t),Vcr=r(fLe," (MobileBERT model)"),fLe.forEach(t),zcr=i(ne),wF=n(ne,"LI",{});var mLe=s(wF);Rpe=n(mLe,"STRONG",{});var hft=s(Rpe);Wcr=r(hft,"mpnet"),hft.forEach(t),Qcr=r(mLe," \u2014 "),tO=n(mLe,"A",{href:!0});var pft=s(tO);Hcr=r(pft,"TFMPNetForMultipleChoice"),pft.forEach(t),Ucr=r(mLe," (MPNet model)"),mLe.forEach(t),Jcr=i(ne),AF=n(ne,"LI",{});var gLe=s(AF);Spe=n(gLe,"STRONG",{});var _ft=s(Spe);Ycr=r(_ft,"rembert"),_ft.forEach(t),Kcr=r(gLe," \u2014 "),aO=n(gLe,"A",{href:!0});var uft=s(aO);Zcr=r(uft,"TFRemBertForMultipleChoice"),uft.forEach(t),efr=r(gLe," (RemBERT model)"),gLe.forEach(t),ofr=i(ne),LF=n(ne,"LI",{});var hLe=s(LF);Ppe=n(hLe,"STRONG",{});var bft=s(Ppe);rfr=r(bft,"roberta"),bft.forEach(t),tfr=r(hLe," \u2014 "),nO=n(hLe,"A",{href:!0});var vft=s(nO);afr=r(vft,"TFRobertaForMultipleChoice"),vft.forEach(t),nfr=r(hLe," (RoBERTa model)"),hLe.forEach(t),sfr=i(ne),BF=n(ne,"LI",{});var pLe=s(BF);$pe=n(pLe,"STRONG",{});var Tft=s($pe);lfr=r(Tft,"roformer"),Tft.forEach(t),ifr=r(pLe," \u2014 "),sO=n(pLe,"A",{href:!0});var Fft=s(sO);dfr=r(Fft,"TFRoFormerForMultipleChoice"),Fft.forEach(t),cfr=r(pLe," (RoFormer model)"),pLe.forEach(t),ffr=i(ne),xF=n(ne,"LI",{});var _Le=s(xF);Ipe=n(_Le,"STRONG",{});var Cft=s(Ipe);mfr=r(Cft,"xlm"),Cft.forEach(t),gfr=r(_Le," \u2014 "),lO=n(_Le,"A",{href:!0});var Mft=s(lO);hfr=r(Mft,"TFXLMForMultipleChoice"),Mft.forEach(t),pfr=r(_Le," (XLM model)"),_Le.forEach(t),_fr=i(ne),kF=n(ne,"LI",{});var uLe=s(kF);Dpe=n(uLe,"STRONG",{});var Eft=s(Dpe);ufr=r(Eft,"xlm-roberta"),Eft.forEach(t),bfr=r(uLe," \u2014 "),iO=n(uLe,"A",{href:!0});var yft=s(iO);vfr=r(yft,"TFXLMRobertaForMultipleChoice"),yft.forEach(t),Tfr=r(uLe," (XLM-RoBERTa model)"),uLe.forEach(t),Ffr=i(ne),RF=n(ne,"LI",{});var bLe=s(RF);jpe=n(bLe,"STRONG",{});var wft=s(jpe);Cfr=r(wft,"xlnet"),wft.forEach(t),Mfr=r(bLe," \u2014 "),dO=n(bLe,"A",{href:!0});var Aft=s(dO);Efr=r(Aft,"TFXLNetForMultipleChoice"),Aft.forEach(t),yfr=r(bLe," (XLNet model)"),bLe.forEach(t),ne.forEach(t),wfr=i(ua),Npe=n(ua,"P",{});var Lft=s(Npe);Afr=r(Lft,"Examples:"),Lft.forEach(t),Lfr=i(ua),m(n6.$$.fragment,ua),ua.forEach(t),zl.forEach(t),bxe=i(c),yc=n(c,"H2",{class:!0});var LRe=s(yc);SF=n(LRe,"A",{id:!0,class:!0,href:!0});var Bft=s(SF);qpe=n(Bft,"SPAN",{});var xft=s(qpe);m(s6.$$.fragment,xft),xft.forEach(t),Bft.forEach(t),Bfr=i(LRe),Gpe=n(LRe,"SPAN",{});var kft=s(Gpe);xfr=r(kft,"TFAutoModelForTableQuestionAnswering"),kft.forEach(t),LRe.forEach(t),vxe=i(c),yr=n(c,"DIV",{class:!0});var Ql=s(yr);m(l6.$$.fragment,Ql),kfr=i(Ql),wc=n(Ql,"P",{});var Qz=s(wc);Rfr=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Ope=n(Qz,"CODE",{});var Rft=s(Ope);Sfr=r(Rft,"from_pretrained()"),Rft.forEach(t),Pfr=r(Qz,"class method or the "),Xpe=n(Qz,"CODE",{});var Sft=s(Xpe);$fr=r(Sft,"from_config()"),Sft.forEach(t),Ifr=r(Qz,`class
method.`),Qz.forEach(t),Dfr=i(Ql),i6=n(Ql,"P",{});var BRe=s(i6);jfr=r(BRe,"This class cannot be instantiated directly using "),Vpe=n(BRe,"CODE",{});var Pft=s(Vpe);Nfr=r(Pft,"__init__()"),Pft.forEach(t),qfr=r(BRe," (throws an error)."),BRe.forEach(t),Gfr=i(Ql),bt=n(Ql,"DIV",{class:!0});var Hl=s(bt);m(d6.$$.fragment,Hl),Ofr=i(Hl),zpe=n(Hl,"P",{});var $ft=s(zpe);Xfr=r($ft,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),$ft.forEach(t),Vfr=i(Hl),Ac=n(Hl,"P",{});var Hz=s(Ac);zfr=r(Hz,`Note:
Loading a model from its configuration file does `),Wpe=n(Hz,"STRONG",{});var Ift=s(Wpe);Wfr=r(Ift,"not"),Ift.forEach(t),Qfr=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Qpe=n(Hz,"CODE",{});var Dft=s(Qpe);Hfr=r(Dft,"from_pretrained()"),Dft.forEach(t),Ufr=r(Hz,"to load the model weights."),Hz.forEach(t),Jfr=i(Hl),Hpe=n(Hl,"P",{});var jft=s(Hpe);Yfr=r(jft,"Examples:"),jft.forEach(t),Kfr=i(Hl),m(c6.$$.fragment,Hl),Hl.forEach(t),Zfr=i(Ql),Fo=n(Ql,"DIV",{class:!0});var ba=s(Fo);m(f6.$$.fragment,ba),emr=i(ba),Upe=n(ba,"P",{});var Nft=s(Upe);omr=r(Nft,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Nft.forEach(t),rmr=i(ba),bn=n(ba,"P",{});var f4=s(bn);tmr=r(f4,"The model class to instantiate is selected based on the "),Jpe=n(f4,"CODE",{});var qft=s(Jpe);amr=r(qft,"model_type"),qft.forEach(t),nmr=r(f4,` property of the config object (either
passed as an argument or loaded from `),Ype=n(f4,"CODE",{});var Gft=s(Ype);smr=r(Gft,"pretrained_model_name_or_path"),Gft.forEach(t),lmr=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Kpe=n(f4,"CODE",{});var Oft=s(Kpe);imr=r(Oft,"pretrained_model_name_or_path"),Oft.forEach(t),dmr=r(f4,":"),f4.forEach(t),cmr=i(ba),Zpe=n(ba,"UL",{});var Xft=s(Zpe);PF=n(Xft,"LI",{});var vLe=s(PF);e_e=n(vLe,"STRONG",{});var Vft=s(e_e);fmr=r(Vft,"tapas"),Vft.forEach(t),mmr=r(vLe," \u2014 "),cO=n(vLe,"A",{href:!0});var zft=s(cO);gmr=r(zft,"TFTapasForQuestionAnswering"),zft.forEach(t),hmr=r(vLe," (TAPAS model)"),vLe.forEach(t),Xft.forEach(t),pmr=i(ba),o_e=n(ba,"P",{});var Wft=s(o_e);_mr=r(Wft,"Examples:"),Wft.forEach(t),umr=i(ba),m(m6.$$.fragment,ba),ba.forEach(t),Ql.forEach(t),Txe=i(c),Lc=n(c,"H2",{class:!0});var xRe=s(Lc);$F=n(xRe,"A",{id:!0,class:!0,href:!0});var Qft=s($F);r_e=n(Qft,"SPAN",{});var Hft=s(r_e);m(g6.$$.fragment,Hft),Hft.forEach(t),Qft.forEach(t),bmr=i(xRe),t_e=n(xRe,"SPAN",{});var Uft=s(t_e);vmr=r(Uft,"TFAutoModelForTokenClassification"),Uft.forEach(t),xRe.forEach(t),Fxe=i(c),wr=n(c,"DIV",{class:!0});var Ul=s(wr);m(h6.$$.fragment,Ul),Tmr=i(Ul),Bc=n(Ul,"P",{});var Uz=s(Bc);Fmr=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),a_e=n(Uz,"CODE",{});var Jft=s(a_e);Cmr=r(Jft,"from_pretrained()"),Jft.forEach(t),Mmr=r(Uz,"class method or the "),n_e=n(Uz,"CODE",{});var Yft=s(n_e);Emr=r(Yft,"from_config()"),Yft.forEach(t),ymr=r(Uz,`class
method.`),Uz.forEach(t),wmr=i(Ul),p6=n(Ul,"P",{});var kRe=s(p6);Amr=r(kRe,"This class cannot be instantiated directly using "),s_e=n(kRe,"CODE",{});var Kft=s(s_e);Lmr=r(Kft,"__init__()"),Kft.forEach(t),Bmr=r(kRe," (throws an error)."),kRe.forEach(t),xmr=i(Ul),vt=n(Ul,"DIV",{class:!0});var Jl=s(vt);m(_6.$$.fragment,Jl),kmr=i(Jl),l_e=n(Jl,"P",{});var Zft=s(l_e);Rmr=r(Zft,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Zft.forEach(t),Smr=i(Jl),xc=n(Jl,"P",{});var Jz=s(xc);Pmr=r(Jz,`Note:
Loading a model from its configuration file does `),i_e=n(Jz,"STRONG",{});var emt=s(i_e);$mr=r(emt,"not"),emt.forEach(t),Imr=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),d_e=n(Jz,"CODE",{});var omt=s(d_e);Dmr=r(omt,"from_pretrained()"),omt.forEach(t),jmr=r(Jz,"to load the model weights."),Jz.forEach(t),Nmr=i(Jl),c_e=n(Jl,"P",{});var rmt=s(c_e);qmr=r(rmt,"Examples:"),rmt.forEach(t),Gmr=i(Jl),m(u6.$$.fragment,Jl),Jl.forEach(t),Omr=i(Ul),Co=n(Ul,"DIV",{class:!0});var va=s(Co);m(b6.$$.fragment,va),Xmr=i(va),f_e=n(va,"P",{});var tmt=s(f_e);Vmr=r(tmt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),tmt.forEach(t),zmr=i(va),vn=n(va,"P",{});var m4=s(vn);Wmr=r(m4,"The model class to instantiate is selected based on the "),m_e=n(m4,"CODE",{});var amt=s(m_e);Qmr=r(amt,"model_type"),amt.forEach(t),Hmr=r(m4,` property of the config object (either
passed as an argument or loaded from `),g_e=n(m4,"CODE",{});var nmt=s(g_e);Umr=r(nmt,"pretrained_model_name_or_path"),nmt.forEach(t),Jmr=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h_e=n(m4,"CODE",{});var smt=s(h_e);Ymr=r(smt,"pretrained_model_name_or_path"),smt.forEach(t),Kmr=r(m4,":"),m4.forEach(t),Zmr=i(va),K=n(va,"UL",{});var oe=s(K);IF=n(oe,"LI",{});var TLe=s(IF);p_e=n(TLe,"STRONG",{});var lmt=s(p_e);egr=r(lmt,"albert"),lmt.forEach(t),ogr=r(TLe," \u2014 "),fO=n(TLe,"A",{href:!0});var imt=s(fO);rgr=r(imt,"TFAlbertForTokenClassification"),imt.forEach(t),tgr=r(TLe," (ALBERT model)"),TLe.forEach(t),agr=i(oe),DF=n(oe,"LI",{});var FLe=s(DF);__e=n(FLe,"STRONG",{});var dmt=s(__e);ngr=r(dmt,"bert"),dmt.forEach(t),sgr=r(FLe," \u2014 "),mO=n(FLe,"A",{href:!0});var cmt=s(mO);lgr=r(cmt,"TFBertForTokenClassification"),cmt.forEach(t),igr=r(FLe," (BERT model)"),FLe.forEach(t),dgr=i(oe),jF=n(oe,"LI",{});var CLe=s(jF);u_e=n(CLe,"STRONG",{});var fmt=s(u_e);cgr=r(fmt,"camembert"),fmt.forEach(t),fgr=r(CLe," \u2014 "),gO=n(CLe,"A",{href:!0});var mmt=s(gO);mgr=r(mmt,"TFCamembertForTokenClassification"),mmt.forEach(t),ggr=r(CLe," (CamemBERT model)"),CLe.forEach(t),hgr=i(oe),NF=n(oe,"LI",{});var MLe=s(NF);b_e=n(MLe,"STRONG",{});var gmt=s(b_e);pgr=r(gmt,"convbert"),gmt.forEach(t),_gr=r(MLe," \u2014 "),hO=n(MLe,"A",{href:!0});var hmt=s(hO);ugr=r(hmt,"TFConvBertForTokenClassification"),hmt.forEach(t),bgr=r(MLe," (ConvBERT model)"),MLe.forEach(t),vgr=i(oe),qF=n(oe,"LI",{});var ELe=s(qF);v_e=n(ELe,"STRONG",{});var pmt=s(v_e);Tgr=r(pmt,"deberta"),pmt.forEach(t),Fgr=r(ELe," \u2014 "),pO=n(ELe,"A",{href:!0});var _mt=s(pO);Cgr=r(_mt,"TFDebertaForTokenClassification"),_mt.forEach(t),Mgr=r(ELe," (DeBERTa model)"),ELe.forEach(t),Egr=i(oe),GF=n(oe,"LI",{});var yLe=s(GF);T_e=n(yLe,"STRONG",{});var umt=s(T_e);ygr=r(umt,"deberta-v2"),umt.forEach(t),wgr=r(yLe," \u2014 "),_O=n(yLe,"A",{href:!0});var bmt=s(_O);Agr=r(bmt,"TFDebertaV2ForTokenClassification"),bmt.forEach(t),Lgr=r(yLe," (DeBERTa-v2 model)"),yLe.forEach(t),Bgr=i(oe),OF=n(oe,"LI",{});var wLe=s(OF);F_e=n(wLe,"STRONG",{});var vmt=s(F_e);xgr=r(vmt,"distilbert"),vmt.forEach(t),kgr=r(wLe," \u2014 "),uO=n(wLe,"A",{href:!0});var Tmt=s(uO);Rgr=r(Tmt,"TFDistilBertForTokenClassification"),Tmt.forEach(t),Sgr=r(wLe," (DistilBERT model)"),wLe.forEach(t),Pgr=i(oe),XF=n(oe,"LI",{});var ALe=s(XF);C_e=n(ALe,"STRONG",{});var Fmt=s(C_e);$gr=r(Fmt,"electra"),Fmt.forEach(t),Igr=r(ALe," \u2014 "),bO=n(ALe,"A",{href:!0});var Cmt=s(bO);Dgr=r(Cmt,"TFElectraForTokenClassification"),Cmt.forEach(t),jgr=r(ALe," (ELECTRA model)"),ALe.forEach(t),Ngr=i(oe),VF=n(oe,"LI",{});var LLe=s(VF);M_e=n(LLe,"STRONG",{});var Mmt=s(M_e);qgr=r(Mmt,"flaubert"),Mmt.forEach(t),Ggr=r(LLe," \u2014 "),vO=n(LLe,"A",{href:!0});var Emt=s(vO);Ogr=r(Emt,"TFFlaubertForTokenClassification"),Emt.forEach(t),Xgr=r(LLe," (FlauBERT model)"),LLe.forEach(t),Vgr=i(oe),zF=n(oe,"LI",{});var BLe=s(zF);E_e=n(BLe,"STRONG",{});var ymt=s(E_e);zgr=r(ymt,"funnel"),ymt.forEach(t),Wgr=r(BLe," \u2014 "),TO=n(BLe,"A",{href:!0});var wmt=s(TO);Qgr=r(wmt,"TFFunnelForTokenClassification"),wmt.forEach(t),Hgr=r(BLe," (Funnel Transformer model)"),BLe.forEach(t),Ugr=i(oe),WF=n(oe,"LI",{});var xLe=s(WF);y_e=n(xLe,"STRONG",{});var Amt=s(y_e);Jgr=r(Amt,"layoutlm"),Amt.forEach(t),Ygr=r(xLe," \u2014 "),FO=n(xLe,"A",{href:!0});var Lmt=s(FO);Kgr=r(Lmt,"TFLayoutLMForTokenClassification"),Lmt.forEach(t),Zgr=r(xLe," (LayoutLM model)"),xLe.forEach(t),ehr=i(oe),QF=n(oe,"LI",{});var kLe=s(QF);w_e=n(kLe,"STRONG",{});var Bmt=s(w_e);ohr=r(Bmt,"longformer"),Bmt.forEach(t),rhr=r(kLe," \u2014 "),CO=n(kLe,"A",{href:!0});var xmt=s(CO);thr=r(xmt,"TFLongformerForTokenClassification"),xmt.forEach(t),ahr=r(kLe," (Longformer model)"),kLe.forEach(t),nhr=i(oe),HF=n(oe,"LI",{});var RLe=s(HF);A_e=n(RLe,"STRONG",{});var kmt=s(A_e);shr=r(kmt,"mobilebert"),kmt.forEach(t),lhr=r(RLe," \u2014 "),MO=n(RLe,"A",{href:!0});var Rmt=s(MO);ihr=r(Rmt,"TFMobileBertForTokenClassification"),Rmt.forEach(t),dhr=r(RLe," (MobileBERT model)"),RLe.forEach(t),chr=i(oe),UF=n(oe,"LI",{});var SLe=s(UF);L_e=n(SLe,"STRONG",{});var Smt=s(L_e);fhr=r(Smt,"mpnet"),Smt.forEach(t),mhr=r(SLe," \u2014 "),EO=n(SLe,"A",{href:!0});var Pmt=s(EO);ghr=r(Pmt,"TFMPNetForTokenClassification"),Pmt.forEach(t),hhr=r(SLe," (MPNet model)"),SLe.forEach(t),phr=i(oe),JF=n(oe,"LI",{});var PLe=s(JF);B_e=n(PLe,"STRONG",{});var $mt=s(B_e);_hr=r($mt,"rembert"),$mt.forEach(t),uhr=r(PLe," \u2014 "),yO=n(PLe,"A",{href:!0});var Imt=s(yO);bhr=r(Imt,"TFRemBertForTokenClassification"),Imt.forEach(t),vhr=r(PLe," (RemBERT model)"),PLe.forEach(t),Thr=i(oe),YF=n(oe,"LI",{});var $Le=s(YF);x_e=n($Le,"STRONG",{});var Dmt=s(x_e);Fhr=r(Dmt,"roberta"),Dmt.forEach(t),Chr=r($Le," \u2014 "),wO=n($Le,"A",{href:!0});var jmt=s(wO);Mhr=r(jmt,"TFRobertaForTokenClassification"),jmt.forEach(t),Ehr=r($Le," (RoBERTa model)"),$Le.forEach(t),yhr=i(oe),KF=n(oe,"LI",{});var ILe=s(KF);k_e=n(ILe,"STRONG",{});var Nmt=s(k_e);whr=r(Nmt,"roformer"),Nmt.forEach(t),Ahr=r(ILe," \u2014 "),AO=n(ILe,"A",{href:!0});var qmt=s(AO);Lhr=r(qmt,"TFRoFormerForTokenClassification"),qmt.forEach(t),Bhr=r(ILe," (RoFormer model)"),ILe.forEach(t),xhr=i(oe),ZF=n(oe,"LI",{});var DLe=s(ZF);R_e=n(DLe,"STRONG",{});var Gmt=s(R_e);khr=r(Gmt,"xlm"),Gmt.forEach(t),Rhr=r(DLe," \u2014 "),LO=n(DLe,"A",{href:!0});var Omt=s(LO);Shr=r(Omt,"TFXLMForTokenClassification"),Omt.forEach(t),Phr=r(DLe," (XLM model)"),DLe.forEach(t),$hr=i(oe),e9=n(oe,"LI",{});var jLe=s(e9);S_e=n(jLe,"STRONG",{});var Xmt=s(S_e);Ihr=r(Xmt,"xlm-roberta"),Xmt.forEach(t),Dhr=r(jLe," \u2014 "),BO=n(jLe,"A",{href:!0});var Vmt=s(BO);jhr=r(Vmt,"TFXLMRobertaForTokenClassification"),Vmt.forEach(t),Nhr=r(jLe," (XLM-RoBERTa model)"),jLe.forEach(t),qhr=i(oe),o9=n(oe,"LI",{});var NLe=s(o9);P_e=n(NLe,"STRONG",{});var zmt=s(P_e);Ghr=r(zmt,"xlnet"),zmt.forEach(t),Ohr=r(NLe," \u2014 "),xO=n(NLe,"A",{href:!0});var Wmt=s(xO);Xhr=r(Wmt,"TFXLNetForTokenClassification"),Wmt.forEach(t),Vhr=r(NLe," (XLNet model)"),NLe.forEach(t),oe.forEach(t),zhr=i(va),$_e=n(va,"P",{});var Qmt=s($_e);Whr=r(Qmt,"Examples:"),Qmt.forEach(t),Qhr=i(va),m(v6.$$.fragment,va),va.forEach(t),Ul.forEach(t),Cxe=i(c),kc=n(c,"H2",{class:!0});var RRe=s(kc);r9=n(RRe,"A",{id:!0,class:!0,href:!0});var Hmt=s(r9);I_e=n(Hmt,"SPAN",{});var Umt=s(I_e);m(T6.$$.fragment,Umt),Umt.forEach(t),Hmt.forEach(t),Hhr=i(RRe),D_e=n(RRe,"SPAN",{});var Jmt=s(D_e);Uhr=r(Jmt,"TFAutoModelForQuestionAnswering"),Jmt.forEach(t),RRe.forEach(t),Mxe=i(c),Ar=n(c,"DIV",{class:!0});var Yl=s(Ar);m(F6.$$.fragment,Yl),Jhr=i(Yl),Rc=n(Yl,"P",{});var Yz=s(Rc);Yhr=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),j_e=n(Yz,"CODE",{});var Ymt=s(j_e);Khr=r(Ymt,"from_pretrained()"),Ymt.forEach(t),Zhr=r(Yz,"class method or the "),N_e=n(Yz,"CODE",{});var Kmt=s(N_e);epr=r(Kmt,"from_config()"),Kmt.forEach(t),opr=r(Yz,`class
method.`),Yz.forEach(t),rpr=i(Yl),C6=n(Yl,"P",{});var SRe=s(C6);tpr=r(SRe,"This class cannot be instantiated directly using "),q_e=n(SRe,"CODE",{});var Zmt=s(q_e);apr=r(Zmt,"__init__()"),Zmt.forEach(t),npr=r(SRe," (throws an error)."),SRe.forEach(t),spr=i(Yl),Tt=n(Yl,"DIV",{class:!0});var Kl=s(Tt);m(M6.$$.fragment,Kl),lpr=i(Kl),G_e=n(Kl,"P",{});var egt=s(G_e);ipr=r(egt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),egt.forEach(t),dpr=i(Kl),Sc=n(Kl,"P",{});var Kz=s(Sc);cpr=r(Kz,`Note:
Loading a model from its configuration file does `),O_e=n(Kz,"STRONG",{});var ogt=s(O_e);fpr=r(ogt,"not"),ogt.forEach(t),mpr=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),X_e=n(Kz,"CODE",{});var rgt=s(X_e);gpr=r(rgt,"from_pretrained()"),rgt.forEach(t),hpr=r(Kz,"to load the model weights."),Kz.forEach(t),ppr=i(Kl),V_e=n(Kl,"P",{});var tgt=s(V_e);_pr=r(tgt,"Examples:"),tgt.forEach(t),upr=i(Kl),m(E6.$$.fragment,Kl),Kl.forEach(t),bpr=i(Yl),Mo=n(Yl,"DIV",{class:!0});var Ta=s(Mo);m(y6.$$.fragment,Ta),vpr=i(Ta),z_e=n(Ta,"P",{});var agt=s(z_e);Tpr=r(agt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),agt.forEach(t),Fpr=i(Ta),Tn=n(Ta,"P",{});var g4=s(Tn);Cpr=r(g4,"The model class to instantiate is selected based on the "),W_e=n(g4,"CODE",{});var ngt=s(W_e);Mpr=r(ngt,"model_type"),ngt.forEach(t),Epr=r(g4,` property of the config object (either
passed as an argument or loaded from `),Q_e=n(g4,"CODE",{});var sgt=s(Q_e);ypr=r(sgt,"pretrained_model_name_or_path"),sgt.forEach(t),wpr=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),H_e=n(g4,"CODE",{});var lgt=s(H_e);Apr=r(lgt,"pretrained_model_name_or_path"),lgt.forEach(t),Lpr=r(g4,":"),g4.forEach(t),Bpr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);t9=n(re,"LI",{});var qLe=s(t9);U_e=n(qLe,"STRONG",{});var igt=s(U_e);xpr=r(igt,"albert"),igt.forEach(t),kpr=r(qLe," \u2014 "),kO=n(qLe,"A",{href:!0});var dgt=s(kO);Rpr=r(dgt,"TFAlbertForQuestionAnswering"),dgt.forEach(t),Spr=r(qLe," (ALBERT model)"),qLe.forEach(t),Ppr=i(re),a9=n(re,"LI",{});var GLe=s(a9);J_e=n(GLe,"STRONG",{});var cgt=s(J_e);$pr=r(cgt,"bert"),cgt.forEach(t),Ipr=r(GLe," \u2014 "),RO=n(GLe,"A",{href:!0});var fgt=s(RO);Dpr=r(fgt,"TFBertForQuestionAnswering"),fgt.forEach(t),jpr=r(GLe," (BERT model)"),GLe.forEach(t),Npr=i(re),n9=n(re,"LI",{});var OLe=s(n9);Y_e=n(OLe,"STRONG",{});var mgt=s(Y_e);qpr=r(mgt,"camembert"),mgt.forEach(t),Gpr=r(OLe," \u2014 "),SO=n(OLe,"A",{href:!0});var ggt=s(SO);Opr=r(ggt,"TFCamembertForQuestionAnswering"),ggt.forEach(t),Xpr=r(OLe," (CamemBERT model)"),OLe.forEach(t),Vpr=i(re),s9=n(re,"LI",{});var XLe=s(s9);K_e=n(XLe,"STRONG",{});var hgt=s(K_e);zpr=r(hgt,"convbert"),hgt.forEach(t),Wpr=r(XLe," \u2014 "),PO=n(XLe,"A",{href:!0});var pgt=s(PO);Qpr=r(pgt,"TFConvBertForQuestionAnswering"),pgt.forEach(t),Hpr=r(XLe," (ConvBERT model)"),XLe.forEach(t),Upr=i(re),l9=n(re,"LI",{});var VLe=s(l9);Z_e=n(VLe,"STRONG",{});var _gt=s(Z_e);Jpr=r(_gt,"deberta"),_gt.forEach(t),Ypr=r(VLe," \u2014 "),$O=n(VLe,"A",{href:!0});var ugt=s($O);Kpr=r(ugt,"TFDebertaForQuestionAnswering"),ugt.forEach(t),Zpr=r(VLe," (DeBERTa model)"),VLe.forEach(t),e_r=i(re),i9=n(re,"LI",{});var zLe=s(i9);eue=n(zLe,"STRONG",{});var bgt=s(eue);o_r=r(bgt,"deberta-v2"),bgt.forEach(t),r_r=r(zLe," \u2014 "),IO=n(zLe,"A",{href:!0});var vgt=s(IO);t_r=r(vgt,"TFDebertaV2ForQuestionAnswering"),vgt.forEach(t),a_r=r(zLe," (DeBERTa-v2 model)"),zLe.forEach(t),n_r=i(re),d9=n(re,"LI",{});var WLe=s(d9);oue=n(WLe,"STRONG",{});var Tgt=s(oue);s_r=r(Tgt,"distilbert"),Tgt.forEach(t),l_r=r(WLe," \u2014 "),DO=n(WLe,"A",{href:!0});var Fgt=s(DO);i_r=r(Fgt,"TFDistilBertForQuestionAnswering"),Fgt.forEach(t),d_r=r(WLe," (DistilBERT model)"),WLe.forEach(t),c_r=i(re),c9=n(re,"LI",{});var QLe=s(c9);rue=n(QLe,"STRONG",{});var Cgt=s(rue);f_r=r(Cgt,"electra"),Cgt.forEach(t),m_r=r(QLe," \u2014 "),jO=n(QLe,"A",{href:!0});var Mgt=s(jO);g_r=r(Mgt,"TFElectraForQuestionAnswering"),Mgt.forEach(t),h_r=r(QLe," (ELECTRA model)"),QLe.forEach(t),p_r=i(re),f9=n(re,"LI",{});var HLe=s(f9);tue=n(HLe,"STRONG",{});var Egt=s(tue);__r=r(Egt,"flaubert"),Egt.forEach(t),u_r=r(HLe," \u2014 "),NO=n(HLe,"A",{href:!0});var ygt=s(NO);b_r=r(ygt,"TFFlaubertForQuestionAnsweringSimple"),ygt.forEach(t),v_r=r(HLe," (FlauBERT model)"),HLe.forEach(t),T_r=i(re),m9=n(re,"LI",{});var ULe=s(m9);aue=n(ULe,"STRONG",{});var wgt=s(aue);F_r=r(wgt,"funnel"),wgt.forEach(t),C_r=r(ULe," \u2014 "),qO=n(ULe,"A",{href:!0});var Agt=s(qO);M_r=r(Agt,"TFFunnelForQuestionAnswering"),Agt.forEach(t),E_r=r(ULe," (Funnel Transformer model)"),ULe.forEach(t),y_r=i(re),g9=n(re,"LI",{});var JLe=s(g9);nue=n(JLe,"STRONG",{});var Lgt=s(nue);w_r=r(Lgt,"longformer"),Lgt.forEach(t),A_r=r(JLe," \u2014 "),GO=n(JLe,"A",{href:!0});var Bgt=s(GO);L_r=r(Bgt,"TFLongformerForQuestionAnswering"),Bgt.forEach(t),B_r=r(JLe," (Longformer model)"),JLe.forEach(t),x_r=i(re),h9=n(re,"LI",{});var YLe=s(h9);sue=n(YLe,"STRONG",{});var xgt=s(sue);k_r=r(xgt,"mobilebert"),xgt.forEach(t),R_r=r(YLe," \u2014 "),OO=n(YLe,"A",{href:!0});var kgt=s(OO);S_r=r(kgt,"TFMobileBertForQuestionAnswering"),kgt.forEach(t),P_r=r(YLe," (MobileBERT model)"),YLe.forEach(t),$_r=i(re),p9=n(re,"LI",{});var KLe=s(p9);lue=n(KLe,"STRONG",{});var Rgt=s(lue);I_r=r(Rgt,"mpnet"),Rgt.forEach(t),D_r=r(KLe," \u2014 "),XO=n(KLe,"A",{href:!0});var Sgt=s(XO);j_r=r(Sgt,"TFMPNetForQuestionAnswering"),Sgt.forEach(t),N_r=r(KLe," (MPNet model)"),KLe.forEach(t),q_r=i(re),_9=n(re,"LI",{});var ZLe=s(_9);iue=n(ZLe,"STRONG",{});var Pgt=s(iue);G_r=r(Pgt,"rembert"),Pgt.forEach(t),O_r=r(ZLe," \u2014 "),VO=n(ZLe,"A",{href:!0});var $gt=s(VO);X_r=r($gt,"TFRemBertForQuestionAnswering"),$gt.forEach(t),V_r=r(ZLe," (RemBERT model)"),ZLe.forEach(t),z_r=i(re),u9=n(re,"LI",{});var e8e=s(u9);due=n(e8e,"STRONG",{});var Igt=s(due);W_r=r(Igt,"roberta"),Igt.forEach(t),Q_r=r(e8e," \u2014 "),zO=n(e8e,"A",{href:!0});var Dgt=s(zO);H_r=r(Dgt,"TFRobertaForQuestionAnswering"),Dgt.forEach(t),U_r=r(e8e," (RoBERTa model)"),e8e.forEach(t),J_r=i(re),b9=n(re,"LI",{});var o8e=s(b9);cue=n(o8e,"STRONG",{});var jgt=s(cue);Y_r=r(jgt,"roformer"),jgt.forEach(t),K_r=r(o8e," \u2014 "),WO=n(o8e,"A",{href:!0});var Ngt=s(WO);Z_r=r(Ngt,"TFRoFormerForQuestionAnswering"),Ngt.forEach(t),eur=r(o8e," (RoFormer model)"),o8e.forEach(t),our=i(re),v9=n(re,"LI",{});var r8e=s(v9);fue=n(r8e,"STRONG",{});var qgt=s(fue);rur=r(qgt,"xlm"),qgt.forEach(t),tur=r(r8e," \u2014 "),QO=n(r8e,"A",{href:!0});var Ggt=s(QO);aur=r(Ggt,"TFXLMForQuestionAnsweringSimple"),Ggt.forEach(t),nur=r(r8e," (XLM model)"),r8e.forEach(t),sur=i(re),T9=n(re,"LI",{});var t8e=s(T9);mue=n(t8e,"STRONG",{});var Ogt=s(mue);lur=r(Ogt,"xlm-roberta"),Ogt.forEach(t),iur=r(t8e," \u2014 "),HO=n(t8e,"A",{href:!0});var Xgt=s(HO);dur=r(Xgt,"TFXLMRobertaForQuestionAnswering"),Xgt.forEach(t),cur=r(t8e," (XLM-RoBERTa model)"),t8e.forEach(t),fur=i(re),F9=n(re,"LI",{});var a8e=s(F9);gue=n(a8e,"STRONG",{});var Vgt=s(gue);mur=r(Vgt,"xlnet"),Vgt.forEach(t),gur=r(a8e," \u2014 "),UO=n(a8e,"A",{href:!0});var zgt=s(UO);hur=r(zgt,"TFXLNetForQuestionAnsweringSimple"),zgt.forEach(t),pur=r(a8e," (XLNet model)"),a8e.forEach(t),re.forEach(t),_ur=i(Ta),hue=n(Ta,"P",{});var Wgt=s(hue);uur=r(Wgt,"Examples:"),Wgt.forEach(t),bur=i(Ta),m(w6.$$.fragment,Ta),Ta.forEach(t),Yl.forEach(t),Exe=i(c),Pc=n(c,"H2",{class:!0});var PRe=s(Pc);C9=n(PRe,"A",{id:!0,class:!0,href:!0});var Qgt=s(C9);pue=n(Qgt,"SPAN",{});var Hgt=s(pue);m(A6.$$.fragment,Hgt),Hgt.forEach(t),Qgt.forEach(t),vur=i(PRe),_ue=n(PRe,"SPAN",{});var Ugt=s(_ue);Tur=r(Ugt,"TFAutoModelForVision2Seq"),Ugt.forEach(t),PRe.forEach(t),yxe=i(c),Lr=n(c,"DIV",{class:!0});var Zl=s(Lr);m(L6.$$.fragment,Zl),Fur=i(Zl),$c=n(Zl,"P",{});var Zz=s($c);Cur=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),uue=n(Zz,"CODE",{});var Jgt=s(uue);Mur=r(Jgt,"from_pretrained()"),Jgt.forEach(t),Eur=r(Zz,"class method or the "),bue=n(Zz,"CODE",{});var Ygt=s(bue);yur=r(Ygt,"from_config()"),Ygt.forEach(t),wur=r(Zz,`class
method.`),Zz.forEach(t),Aur=i(Zl),B6=n(Zl,"P",{});var $Re=s(B6);Lur=r($Re,"This class cannot be instantiated directly using "),vue=n($Re,"CODE",{});var Kgt=s(vue);Bur=r(Kgt,"__init__()"),Kgt.forEach(t),xur=r($Re," (throws an error)."),$Re.forEach(t),kur=i(Zl),Ft=n(Zl,"DIV",{class:!0});var ei=s(Ft);m(x6.$$.fragment,ei),Rur=i(ei),Tue=n(ei,"P",{});var Zgt=s(Tue);Sur=r(Zgt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Zgt.forEach(t),Pur=i(ei),Ic=n(ei,"P",{});var eW=s(Ic);$ur=r(eW,`Note:
Loading a model from its configuration file does `),Fue=n(eW,"STRONG",{});var eht=s(Fue);Iur=r(eht,"not"),eht.forEach(t),Dur=r(eW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cue=n(eW,"CODE",{});var oht=s(Cue);jur=r(oht,"from_pretrained()"),oht.forEach(t),Nur=r(eW,"to load the model weights."),eW.forEach(t),qur=i(ei),Mue=n(ei,"P",{});var rht=s(Mue);Gur=r(rht,"Examples:"),rht.forEach(t),Our=i(ei),m(k6.$$.fragment,ei),ei.forEach(t),Xur=i(Zl),Eo=n(Zl,"DIV",{class:!0});var Fa=s(Eo);m(R6.$$.fragment,Fa),Vur=i(Fa),Eue=n(Fa,"P",{});var tht=s(Eue);zur=r(tht,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),tht.forEach(t),Wur=i(Fa),Fn=n(Fa,"P",{});var h4=s(Fn);Qur=r(h4,"The model class to instantiate is selected based on the "),yue=n(h4,"CODE",{});var aht=s(yue);Hur=r(aht,"model_type"),aht.forEach(t),Uur=r(h4,` property of the config object (either
passed as an argument or loaded from `),wue=n(h4,"CODE",{});var nht=s(wue);Jur=r(nht,"pretrained_model_name_or_path"),nht.forEach(t),Yur=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aue=n(h4,"CODE",{});var sht=s(Aue);Kur=r(sht,"pretrained_model_name_or_path"),sht.forEach(t),Zur=r(h4,":"),h4.forEach(t),e0r=i(Fa),Lue=n(Fa,"UL",{});var lht=s(Lue);M9=n(lht,"LI",{});var n8e=s(M9);Bue=n(n8e,"STRONG",{});var iht=s(Bue);o0r=r(iht,"vision-encoder-decoder"),iht.forEach(t),r0r=r(n8e," \u2014 "),JO=n(n8e,"A",{href:!0});var dht=s(JO);t0r=r(dht,"TFVisionEncoderDecoderModel"),dht.forEach(t),a0r=r(n8e," (Vision Encoder decoder model)"),n8e.forEach(t),lht.forEach(t),n0r=i(Fa),xue=n(Fa,"P",{});var cht=s(xue);s0r=r(cht,"Examples:"),cht.forEach(t),l0r=i(Fa),m(S6.$$.fragment,Fa),Fa.forEach(t),Zl.forEach(t),wxe=i(c),Dc=n(c,"H2",{class:!0});var IRe=s(Dc);E9=n(IRe,"A",{id:!0,class:!0,href:!0});var fht=s(E9);kue=n(fht,"SPAN",{});var mht=s(kue);m(P6.$$.fragment,mht),mht.forEach(t),fht.forEach(t),i0r=i(IRe),Rue=n(IRe,"SPAN",{});var ght=s(Rue);d0r=r(ght,"TFAutoModelForSpeechSeq2Seq"),ght.forEach(t),IRe.forEach(t),Axe=i(c),Br=n(c,"DIV",{class:!0});var oi=s(Br);m($6.$$.fragment,oi),c0r=i(oi),jc=n(oi,"P",{});var oW=s(jc);f0r=r(oW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Sue=n(oW,"CODE",{});var hht=s(Sue);m0r=r(hht,"from_pretrained()"),hht.forEach(t),g0r=r(oW,"class method or the "),Pue=n(oW,"CODE",{});var pht=s(Pue);h0r=r(pht,"from_config()"),pht.forEach(t),p0r=r(oW,`class
method.`),oW.forEach(t),_0r=i(oi),I6=n(oi,"P",{});var DRe=s(I6);u0r=r(DRe,"This class cannot be instantiated directly using "),$ue=n(DRe,"CODE",{});var _ht=s($ue);b0r=r(_ht,"__init__()"),_ht.forEach(t),v0r=r(DRe," (throws an error)."),DRe.forEach(t),T0r=i(oi),Ct=n(oi,"DIV",{class:!0});var ri=s(Ct);m(D6.$$.fragment,ri),F0r=i(ri),Iue=n(ri,"P",{});var uht=s(Iue);C0r=r(uht,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),uht.forEach(t),M0r=i(ri),Nc=n(ri,"P",{});var rW=s(Nc);E0r=r(rW,`Note:
Loading a model from its configuration file does `),Due=n(rW,"STRONG",{});var bht=s(Due);y0r=r(bht,"not"),bht.forEach(t),w0r=r(rW,` load the model weights. It only affects the
model\u2019s configuration. Use `),jue=n(rW,"CODE",{});var vht=s(jue);A0r=r(vht,"from_pretrained()"),vht.forEach(t),L0r=r(rW,"to load the model weights."),rW.forEach(t),B0r=i(ri),Nue=n(ri,"P",{});var Tht=s(Nue);x0r=r(Tht,"Examples:"),Tht.forEach(t),k0r=i(ri),m(j6.$$.fragment,ri),ri.forEach(t),R0r=i(oi),yo=n(oi,"DIV",{class:!0});var Ca=s(yo);m(N6.$$.fragment,Ca),S0r=i(Ca),que=n(Ca,"P",{});var Fht=s(que);P0r=r(Fht,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Fht.forEach(t),$0r=i(Ca),Cn=n(Ca,"P",{});var p4=s(Cn);I0r=r(p4,"The model class to instantiate is selected based on the "),Gue=n(p4,"CODE",{});var Cht=s(Gue);D0r=r(Cht,"model_type"),Cht.forEach(t),j0r=r(p4,` property of the config object (either
passed as an argument or loaded from `),Oue=n(p4,"CODE",{});var Mht=s(Oue);N0r=r(Mht,"pretrained_model_name_or_path"),Mht.forEach(t),q0r=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xue=n(p4,"CODE",{});var Eht=s(Xue);G0r=r(Eht,"pretrained_model_name_or_path"),Eht.forEach(t),O0r=r(p4,":"),p4.forEach(t),X0r=i(Ca),Vue=n(Ca,"UL",{});var yht=s(Vue);y9=n(yht,"LI",{});var s8e=s(y9);zue=n(s8e,"STRONG",{});var wht=s(zue);V0r=r(wht,"speech_to_text"),wht.forEach(t),z0r=r(s8e," \u2014 "),YO=n(s8e,"A",{href:!0});var Aht=s(YO);W0r=r(Aht,"TFSpeech2TextForConditionalGeneration"),Aht.forEach(t),Q0r=r(s8e," (Speech2Text model)"),s8e.forEach(t),yht.forEach(t),H0r=i(Ca),Wue=n(Ca,"P",{});var Lht=s(Wue);U0r=r(Lht,"Examples:"),Lht.forEach(t),J0r=i(Ca),m(q6.$$.fragment,Ca),Ca.forEach(t),oi.forEach(t),Lxe=i(c),qc=n(c,"H2",{class:!0});var jRe=s(qc);w9=n(jRe,"A",{id:!0,class:!0,href:!0});var Bht=s(w9);Que=n(Bht,"SPAN",{});var xht=s(Que);m(G6.$$.fragment,xht),xht.forEach(t),Bht.forEach(t),Y0r=i(jRe),Hue=n(jRe,"SPAN",{});var kht=s(Hue);K0r=r(kht,"FlaxAutoModel"),kht.forEach(t),jRe.forEach(t),Bxe=i(c),xr=n(c,"DIV",{class:!0});var ti=s(xr);m(O6.$$.fragment,ti),Z0r=i(ti),Gc=n(ti,"P",{});var tW=s(Gc);e1r=r(tW,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Uue=n(tW,"CODE",{});var Rht=s(Uue);o1r=r(Rht,"from_pretrained()"),Rht.forEach(t),r1r=r(tW,"class method or the "),Jue=n(tW,"CODE",{});var Sht=s(Jue);t1r=r(Sht,"from_config()"),Sht.forEach(t),a1r=r(tW,`class
method.`),tW.forEach(t),n1r=i(ti),X6=n(ti,"P",{});var NRe=s(X6);s1r=r(NRe,"This class cannot be instantiated directly using "),Yue=n(NRe,"CODE",{});var Pht=s(Yue);l1r=r(Pht,"__init__()"),Pht.forEach(t),i1r=r(NRe," (throws an error)."),NRe.forEach(t),d1r=i(ti),Mt=n(ti,"DIV",{class:!0});var ai=s(Mt);m(V6.$$.fragment,ai),c1r=i(ai),Kue=n(ai,"P",{});var $ht=s(Kue);f1r=r($ht,"Instantiates one of the base model classes of the library from a configuration."),$ht.forEach(t),m1r=i(ai),Oc=n(ai,"P",{});var aW=s(Oc);g1r=r(aW,`Note:
Loading a model from its configuration file does `),Zue=n(aW,"STRONG",{});var Iht=s(Zue);h1r=r(Iht,"not"),Iht.forEach(t),p1r=r(aW,` load the model weights. It only affects the
model\u2019s configuration. Use `),e0e=n(aW,"CODE",{});var Dht=s(e0e);_1r=r(Dht,"from_pretrained()"),Dht.forEach(t),u1r=r(aW,"to load the model weights."),aW.forEach(t),b1r=i(ai),o0e=n(ai,"P",{});var jht=s(o0e);v1r=r(jht,"Examples:"),jht.forEach(t),T1r=i(ai),m(z6.$$.fragment,ai),ai.forEach(t),F1r=i(ti),wo=n(ti,"DIV",{class:!0});var Ma=s(wo);m(W6.$$.fragment,Ma),C1r=i(Ma),r0e=n(Ma,"P",{});var Nht=s(r0e);M1r=r(Nht,"Instantiate one of the base model classes of the library from a pretrained model."),Nht.forEach(t),E1r=i(Ma),Mn=n(Ma,"P",{});var _4=s(Mn);y1r=r(_4,"The model class to instantiate is selected based on the "),t0e=n(_4,"CODE",{});var qht=s(t0e);w1r=r(qht,"model_type"),qht.forEach(t),A1r=r(_4,` property of the config object (either
passed as an argument or loaded from `),a0e=n(_4,"CODE",{});var Ght=s(a0e);L1r=r(Ght,"pretrained_model_name_or_path"),Ght.forEach(t),B1r=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),n0e=n(_4,"CODE",{});var Oht=s(n0e);x1r=r(Oht,"pretrained_model_name_or_path"),Oht.forEach(t),k1r=r(_4,":"),_4.forEach(t),R1r=i(Ma),V=n(Ma,"UL",{});var Q=s(V);A9=n(Q,"LI",{});var l8e=s(A9);s0e=n(l8e,"STRONG",{});var Xht=s(s0e);S1r=r(Xht,"albert"),Xht.forEach(t),P1r=r(l8e," \u2014 "),KO=n(l8e,"A",{href:!0});var Vht=s(KO);$1r=r(Vht,"FlaxAlbertModel"),Vht.forEach(t),I1r=r(l8e," (ALBERT model)"),l8e.forEach(t),D1r=i(Q),L9=n(Q,"LI",{});var i8e=s(L9);l0e=n(i8e,"STRONG",{});var zht=s(l0e);j1r=r(zht,"bart"),zht.forEach(t),N1r=r(i8e," \u2014 "),ZO=n(i8e,"A",{href:!0});var Wht=s(ZO);q1r=r(Wht,"FlaxBartModel"),Wht.forEach(t),G1r=r(i8e," (BART model)"),i8e.forEach(t),O1r=i(Q),B9=n(Q,"LI",{});var d8e=s(B9);i0e=n(d8e,"STRONG",{});var Qht=s(i0e);X1r=r(Qht,"beit"),Qht.forEach(t),V1r=r(d8e," \u2014 "),eX=n(d8e,"A",{href:!0});var Hht=s(eX);z1r=r(Hht,"FlaxBeitModel"),Hht.forEach(t),W1r=r(d8e," (BEiT model)"),d8e.forEach(t),Q1r=i(Q),x9=n(Q,"LI",{});var c8e=s(x9);d0e=n(c8e,"STRONG",{});var Uht=s(d0e);H1r=r(Uht,"bert"),Uht.forEach(t),U1r=r(c8e," \u2014 "),oX=n(c8e,"A",{href:!0});var Jht=s(oX);J1r=r(Jht,"FlaxBertModel"),Jht.forEach(t),Y1r=r(c8e," (BERT model)"),c8e.forEach(t),K1r=i(Q),k9=n(Q,"LI",{});var f8e=s(k9);c0e=n(f8e,"STRONG",{});var Yht=s(c0e);Z1r=r(Yht,"big_bird"),Yht.forEach(t),ebr=r(f8e," \u2014 "),rX=n(f8e,"A",{href:!0});var Kht=s(rX);obr=r(Kht,"FlaxBigBirdModel"),Kht.forEach(t),rbr=r(f8e," (BigBird model)"),f8e.forEach(t),tbr=i(Q),R9=n(Q,"LI",{});var m8e=s(R9);f0e=n(m8e,"STRONG",{});var Zht=s(f0e);abr=r(Zht,"blenderbot"),Zht.forEach(t),nbr=r(m8e," \u2014 "),tX=n(m8e,"A",{href:!0});var ept=s(tX);sbr=r(ept,"FlaxBlenderbotModel"),ept.forEach(t),lbr=r(m8e," (Blenderbot model)"),m8e.forEach(t),ibr=i(Q),S9=n(Q,"LI",{});var g8e=s(S9);m0e=n(g8e,"STRONG",{});var opt=s(m0e);dbr=r(opt,"blenderbot-small"),opt.forEach(t),cbr=r(g8e," \u2014 "),aX=n(g8e,"A",{href:!0});var rpt=s(aX);fbr=r(rpt,"FlaxBlenderbotSmallModel"),rpt.forEach(t),mbr=r(g8e," (BlenderbotSmall model)"),g8e.forEach(t),gbr=i(Q),P9=n(Q,"LI",{});var h8e=s(P9);g0e=n(h8e,"STRONG",{});var tpt=s(g0e);hbr=r(tpt,"clip"),tpt.forEach(t),pbr=r(h8e," \u2014 "),nX=n(h8e,"A",{href:!0});var apt=s(nX);_br=r(apt,"FlaxCLIPModel"),apt.forEach(t),ubr=r(h8e," (CLIP model)"),h8e.forEach(t),bbr=i(Q),$9=n(Q,"LI",{});var p8e=s($9);h0e=n(p8e,"STRONG",{});var npt=s(h0e);vbr=r(npt,"distilbert"),npt.forEach(t),Tbr=r(p8e," \u2014 "),sX=n(p8e,"A",{href:!0});var spt=s(sX);Fbr=r(spt,"FlaxDistilBertModel"),spt.forEach(t),Cbr=r(p8e," (DistilBERT model)"),p8e.forEach(t),Mbr=i(Q),I9=n(Q,"LI",{});var _8e=s(I9);p0e=n(_8e,"STRONG",{});var lpt=s(p0e);Ebr=r(lpt,"electra"),lpt.forEach(t),ybr=r(_8e," \u2014 "),lX=n(_8e,"A",{href:!0});var ipt=s(lX);wbr=r(ipt,"FlaxElectraModel"),ipt.forEach(t),Abr=r(_8e," (ELECTRA model)"),_8e.forEach(t),Lbr=i(Q),D9=n(Q,"LI",{});var u8e=s(D9);_0e=n(u8e,"STRONG",{});var dpt=s(_0e);Bbr=r(dpt,"gpt2"),dpt.forEach(t),xbr=r(u8e," \u2014 "),iX=n(u8e,"A",{href:!0});var cpt=s(iX);kbr=r(cpt,"FlaxGPT2Model"),cpt.forEach(t),Rbr=r(u8e," (OpenAI GPT-2 model)"),u8e.forEach(t),Sbr=i(Q),j9=n(Q,"LI",{});var b8e=s(j9);u0e=n(b8e,"STRONG",{});var fpt=s(u0e);Pbr=r(fpt,"gpt_neo"),fpt.forEach(t),$br=r(b8e," \u2014 "),dX=n(b8e,"A",{href:!0});var mpt=s(dX);Ibr=r(mpt,"FlaxGPTNeoModel"),mpt.forEach(t),Dbr=r(b8e," (GPT Neo model)"),b8e.forEach(t),jbr=i(Q),N9=n(Q,"LI",{});var v8e=s(N9);b0e=n(v8e,"STRONG",{});var gpt=s(b0e);Nbr=r(gpt,"gptj"),gpt.forEach(t),qbr=r(v8e," \u2014 "),cX=n(v8e,"A",{href:!0});var hpt=s(cX);Gbr=r(hpt,"FlaxGPTJModel"),hpt.forEach(t),Obr=r(v8e," (GPT-J model)"),v8e.forEach(t),Xbr=i(Q),q9=n(Q,"LI",{});var T8e=s(q9);v0e=n(T8e,"STRONG",{});var ppt=s(v0e);Vbr=r(ppt,"marian"),ppt.forEach(t),zbr=r(T8e," \u2014 "),fX=n(T8e,"A",{href:!0});var _pt=s(fX);Wbr=r(_pt,"FlaxMarianModel"),_pt.forEach(t),Qbr=r(T8e," (Marian model)"),T8e.forEach(t),Hbr=i(Q),G9=n(Q,"LI",{});var F8e=s(G9);T0e=n(F8e,"STRONG",{});var upt=s(T0e);Ubr=r(upt,"mbart"),upt.forEach(t),Jbr=r(F8e," \u2014 "),mX=n(F8e,"A",{href:!0});var bpt=s(mX);Ybr=r(bpt,"FlaxMBartModel"),bpt.forEach(t),Kbr=r(F8e," (mBART model)"),F8e.forEach(t),Zbr=i(Q),O9=n(Q,"LI",{});var C8e=s(O9);F0e=n(C8e,"STRONG",{});var vpt=s(F0e);e5r=r(vpt,"mt5"),vpt.forEach(t),o5r=r(C8e," \u2014 "),gX=n(C8e,"A",{href:!0});var Tpt=s(gX);r5r=r(Tpt,"FlaxMT5Model"),Tpt.forEach(t),t5r=r(C8e," (mT5 model)"),C8e.forEach(t),a5r=i(Q),X9=n(Q,"LI",{});var M8e=s(X9);C0e=n(M8e,"STRONG",{});var Fpt=s(C0e);n5r=r(Fpt,"pegasus"),Fpt.forEach(t),s5r=r(M8e," \u2014 "),hX=n(M8e,"A",{href:!0});var Cpt=s(hX);l5r=r(Cpt,"FlaxPegasusModel"),Cpt.forEach(t),i5r=r(M8e," (Pegasus model)"),M8e.forEach(t),d5r=i(Q),V9=n(Q,"LI",{});var E8e=s(V9);M0e=n(E8e,"STRONG",{});var Mpt=s(M0e);c5r=r(Mpt,"roberta"),Mpt.forEach(t),f5r=r(E8e," \u2014 "),pX=n(E8e,"A",{href:!0});var Ept=s(pX);m5r=r(Ept,"FlaxRobertaModel"),Ept.forEach(t),g5r=r(E8e," (RoBERTa model)"),E8e.forEach(t),h5r=i(Q),z9=n(Q,"LI",{});var y8e=s(z9);E0e=n(y8e,"STRONG",{});var ypt=s(E0e);p5r=r(ypt,"roformer"),ypt.forEach(t),_5r=r(y8e," \u2014 "),_X=n(y8e,"A",{href:!0});var wpt=s(_X);u5r=r(wpt,"FlaxRoFormerModel"),wpt.forEach(t),b5r=r(y8e," (RoFormer model)"),y8e.forEach(t),v5r=i(Q),W9=n(Q,"LI",{});var w8e=s(W9);y0e=n(w8e,"STRONG",{});var Apt=s(y0e);T5r=r(Apt,"t5"),Apt.forEach(t),F5r=r(w8e," \u2014 "),uX=n(w8e,"A",{href:!0});var Lpt=s(uX);C5r=r(Lpt,"FlaxT5Model"),Lpt.forEach(t),M5r=r(w8e," (T5 model)"),w8e.forEach(t),E5r=i(Q),Q9=n(Q,"LI",{});var A8e=s(Q9);w0e=n(A8e,"STRONG",{});var Bpt=s(w0e);y5r=r(Bpt,"vision-text-dual-encoder"),Bpt.forEach(t),w5r=r(A8e," \u2014 "),bX=n(A8e,"A",{href:!0});var xpt=s(bX);A5r=r(xpt,"FlaxVisionTextDualEncoderModel"),xpt.forEach(t),L5r=r(A8e," (VisionTextDualEncoder model)"),A8e.forEach(t),B5r=i(Q),H9=n(Q,"LI",{});var L8e=s(H9);A0e=n(L8e,"STRONG",{});var kpt=s(A0e);x5r=r(kpt,"vit"),kpt.forEach(t),k5r=r(L8e," \u2014 "),vX=n(L8e,"A",{href:!0});var Rpt=s(vX);R5r=r(Rpt,"FlaxViTModel"),Rpt.forEach(t),S5r=r(L8e," (ViT model)"),L8e.forEach(t),P5r=i(Q),U9=n(Q,"LI",{});var B8e=s(U9);L0e=n(B8e,"STRONG",{});var Spt=s(L0e);$5r=r(Spt,"wav2vec2"),Spt.forEach(t),I5r=r(B8e," \u2014 "),TX=n(B8e,"A",{href:!0});var Ppt=s(TX);D5r=r(Ppt,"FlaxWav2Vec2Model"),Ppt.forEach(t),j5r=r(B8e," (Wav2Vec2 model)"),B8e.forEach(t),N5r=i(Q),J9=n(Q,"LI",{});var x8e=s(J9);B0e=n(x8e,"STRONG",{});var $pt=s(B0e);q5r=r($pt,"xglm"),$pt.forEach(t),G5r=r(x8e," \u2014 "),FX=n(x8e,"A",{href:!0});var Ipt=s(FX);O5r=r(Ipt,"FlaxXGLMModel"),Ipt.forEach(t),X5r=r(x8e," (XGLM model)"),x8e.forEach(t),V5r=i(Q),Y9=n(Q,"LI",{});var k8e=s(Y9);x0e=n(k8e,"STRONG",{});var Dpt=s(x0e);z5r=r(Dpt,"xlm-roberta"),Dpt.forEach(t),W5r=r(k8e," \u2014 "),k0e=n(k8e,"CODE",{});var jpt=s(k0e);Q5r=r(jpt,"FlaxXLMRobertaModel"),jpt.forEach(t),H5r=r(k8e,"(XLM-RoBERTa model)"),k8e.forEach(t),Q.forEach(t),U5r=i(Ma),R0e=n(Ma,"P",{});var Npt=s(R0e);J5r=r(Npt,"Examples:"),Npt.forEach(t),Y5r=i(Ma),m(Q6.$$.fragment,Ma),Ma.forEach(t),ti.forEach(t),xxe=i(c),Xc=n(c,"H2",{class:!0});var qRe=s(Xc);K9=n(qRe,"A",{id:!0,class:!0,href:!0});var qpt=s(K9);S0e=n(qpt,"SPAN",{});var Gpt=s(S0e);m(H6.$$.fragment,Gpt),Gpt.forEach(t),qpt.forEach(t),K5r=i(qRe),P0e=n(qRe,"SPAN",{});var Opt=s(P0e);Z5r=r(Opt,"FlaxAutoModelForCausalLM"),Opt.forEach(t),qRe.forEach(t),kxe=i(c),kr=n(c,"DIV",{class:!0});var ni=s(kr);m(U6.$$.fragment,ni),e2r=i(ni),Vc=n(ni,"P",{});var nW=s(Vc);o2r=r(nW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),$0e=n(nW,"CODE",{});var Xpt=s($0e);r2r=r(Xpt,"from_pretrained()"),Xpt.forEach(t),t2r=r(nW,"class method or the "),I0e=n(nW,"CODE",{});var Vpt=s(I0e);a2r=r(Vpt,"from_config()"),Vpt.forEach(t),n2r=r(nW,`class
method.`),nW.forEach(t),s2r=i(ni),J6=n(ni,"P",{});var GRe=s(J6);l2r=r(GRe,"This class cannot be instantiated directly using "),D0e=n(GRe,"CODE",{});var zpt=s(D0e);i2r=r(zpt,"__init__()"),zpt.forEach(t),d2r=r(GRe," (throws an error)."),GRe.forEach(t),c2r=i(ni),Et=n(ni,"DIV",{class:!0});var si=s(Et);m(Y6.$$.fragment,si),f2r=i(si),j0e=n(si,"P",{});var Wpt=s(j0e);m2r=r(Wpt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Wpt.forEach(t),g2r=i(si),zc=n(si,"P",{});var sW=s(zc);h2r=r(sW,`Note:
Loading a model from its configuration file does `),N0e=n(sW,"STRONG",{});var Qpt=s(N0e);p2r=r(Qpt,"not"),Qpt.forEach(t),_2r=r(sW,` load the model weights. It only affects the
model\u2019s configuration. Use `),q0e=n(sW,"CODE",{});var Hpt=s(q0e);u2r=r(Hpt,"from_pretrained()"),Hpt.forEach(t),b2r=r(sW,"to load the model weights."),sW.forEach(t),v2r=i(si),G0e=n(si,"P",{});var Upt=s(G0e);T2r=r(Upt,"Examples:"),Upt.forEach(t),F2r=i(si),m(K6.$$.fragment,si),si.forEach(t),C2r=i(ni),Ao=n(ni,"DIV",{class:!0});var Ea=s(Ao);m(Z6.$$.fragment,Ea),M2r=i(Ea),O0e=n(Ea,"P",{});var Jpt=s(O0e);E2r=r(Jpt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Jpt.forEach(t),y2r=i(Ea),En=n(Ea,"P",{});var u4=s(En);w2r=r(u4,"The model class to instantiate is selected based on the "),X0e=n(u4,"CODE",{});var Ypt=s(X0e);A2r=r(Ypt,"model_type"),Ypt.forEach(t),L2r=r(u4,` property of the config object (either
passed as an argument or loaded from `),V0e=n(u4,"CODE",{});var Kpt=s(V0e);B2r=r(Kpt,"pretrained_model_name_or_path"),Kpt.forEach(t),x2r=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),z0e=n(u4,"CODE",{});var Zpt=s(z0e);k2r=r(Zpt,"pretrained_model_name_or_path"),Zpt.forEach(t),R2r=r(u4,":"),u4.forEach(t),S2r=i(Ea),yn=n(Ea,"UL",{});var b4=s(yn);Z9=n(b4,"LI",{});var R8e=s(Z9);W0e=n(R8e,"STRONG",{});var e_t=s(W0e);P2r=r(e_t,"gpt2"),e_t.forEach(t),$2r=r(R8e," \u2014 "),CX=n(R8e,"A",{href:!0});var o_t=s(CX);I2r=r(o_t,"FlaxGPT2LMHeadModel"),o_t.forEach(t),D2r=r(R8e," (OpenAI GPT-2 model)"),R8e.forEach(t),j2r=i(b4),eC=n(b4,"LI",{});var S8e=s(eC);Q0e=n(S8e,"STRONG",{});var r_t=s(Q0e);N2r=r(r_t,"gpt_neo"),r_t.forEach(t),q2r=r(S8e," \u2014 "),MX=n(S8e,"A",{href:!0});var t_t=s(MX);G2r=r(t_t,"FlaxGPTNeoForCausalLM"),t_t.forEach(t),O2r=r(S8e," (GPT Neo model)"),S8e.forEach(t),X2r=i(b4),oC=n(b4,"LI",{});var P8e=s(oC);H0e=n(P8e,"STRONG",{});var a_t=s(H0e);V2r=r(a_t,"gptj"),a_t.forEach(t),z2r=r(P8e," \u2014 "),EX=n(P8e,"A",{href:!0});var n_t=s(EX);W2r=r(n_t,"FlaxGPTJForCausalLM"),n_t.forEach(t),Q2r=r(P8e," (GPT-J model)"),P8e.forEach(t),H2r=i(b4),rC=n(b4,"LI",{});var $8e=s(rC);U0e=n($8e,"STRONG",{});var s_t=s(U0e);U2r=r(s_t,"xglm"),s_t.forEach(t),J2r=r($8e," \u2014 "),yX=n($8e,"A",{href:!0});var l_t=s(yX);Y2r=r(l_t,"FlaxXGLMForCausalLM"),l_t.forEach(t),K2r=r($8e," (XGLM model)"),$8e.forEach(t),b4.forEach(t),Z2r=i(Ea),J0e=n(Ea,"P",{});var i_t=s(J0e);evr=r(i_t,"Examples:"),i_t.forEach(t),ovr=i(Ea),m(eA.$$.fragment,Ea),Ea.forEach(t),ni.forEach(t),Rxe=i(c),Wc=n(c,"H2",{class:!0});var ORe=s(Wc);tC=n(ORe,"A",{id:!0,class:!0,href:!0});var d_t=s(tC);Y0e=n(d_t,"SPAN",{});var c_t=s(Y0e);m(oA.$$.fragment,c_t),c_t.forEach(t),d_t.forEach(t),rvr=i(ORe),K0e=n(ORe,"SPAN",{});var f_t=s(K0e);tvr=r(f_t,"FlaxAutoModelForPreTraining"),f_t.forEach(t),ORe.forEach(t),Sxe=i(c),Rr=n(c,"DIV",{class:!0});var li=s(Rr);m(rA.$$.fragment,li),avr=i(li),Qc=n(li,"P",{});var lW=s(Qc);nvr=r(lW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Z0e=n(lW,"CODE",{});var m_t=s(Z0e);svr=r(m_t,"from_pretrained()"),m_t.forEach(t),lvr=r(lW,"class method or the "),e1e=n(lW,"CODE",{});var g_t=s(e1e);ivr=r(g_t,"from_config()"),g_t.forEach(t),dvr=r(lW,`class
method.`),lW.forEach(t),cvr=i(li),tA=n(li,"P",{});var XRe=s(tA);fvr=r(XRe,"This class cannot be instantiated directly using "),o1e=n(XRe,"CODE",{});var h_t=s(o1e);mvr=r(h_t,"__init__()"),h_t.forEach(t),gvr=r(XRe," (throws an error)."),XRe.forEach(t),hvr=i(li),yt=n(li,"DIV",{class:!0});var ii=s(yt);m(aA.$$.fragment,ii),pvr=i(ii),r1e=n(ii,"P",{});var p_t=s(r1e);_vr=r(p_t,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),p_t.forEach(t),uvr=i(ii),Hc=n(ii,"P",{});var iW=s(Hc);bvr=r(iW,`Note:
Loading a model from its configuration file does `),t1e=n(iW,"STRONG",{});var __t=s(t1e);vvr=r(__t,"not"),__t.forEach(t),Tvr=r(iW,` load the model weights. It only affects the
model\u2019s configuration. Use `),a1e=n(iW,"CODE",{});var u_t=s(a1e);Fvr=r(u_t,"from_pretrained()"),u_t.forEach(t),Cvr=r(iW,"to load the model weights."),iW.forEach(t),Mvr=i(ii),n1e=n(ii,"P",{});var b_t=s(n1e);Evr=r(b_t,"Examples:"),b_t.forEach(t),yvr=i(ii),m(nA.$$.fragment,ii),ii.forEach(t),wvr=i(li),Lo=n(li,"DIV",{class:!0});var ya=s(Lo);m(sA.$$.fragment,ya),Avr=i(ya),s1e=n(ya,"P",{});var v_t=s(s1e);Lvr=r(v_t,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),v_t.forEach(t),Bvr=i(ya),wn=n(ya,"P",{});var v4=s(wn);xvr=r(v4,"The model class to instantiate is selected based on the "),l1e=n(v4,"CODE",{});var T_t=s(l1e);kvr=r(T_t,"model_type"),T_t.forEach(t),Rvr=r(v4,` property of the config object (either
passed as an argument or loaded from `),i1e=n(v4,"CODE",{});var F_t=s(i1e);Svr=r(F_t,"pretrained_model_name_or_path"),F_t.forEach(t),Pvr=r(v4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d1e=n(v4,"CODE",{});var C_t=s(d1e);$vr=r(C_t,"pretrained_model_name_or_path"),C_t.forEach(t),Ivr=r(v4,":"),v4.forEach(t),Dvr=i(ya),ce=n(ya,"UL",{});var me=s(ce);aC=n(me,"LI",{});var I8e=s(aC);c1e=n(I8e,"STRONG",{});var M_t=s(c1e);jvr=r(M_t,"albert"),M_t.forEach(t),Nvr=r(I8e," \u2014 "),wX=n(I8e,"A",{href:!0});var E_t=s(wX);qvr=r(E_t,"FlaxAlbertForPreTraining"),E_t.forEach(t),Gvr=r(I8e," (ALBERT model)"),I8e.forEach(t),Ovr=i(me),nC=n(me,"LI",{});var D8e=s(nC);f1e=n(D8e,"STRONG",{});var y_t=s(f1e);Xvr=r(y_t,"bart"),y_t.forEach(t),Vvr=r(D8e," \u2014 "),AX=n(D8e,"A",{href:!0});var w_t=s(AX);zvr=r(w_t,"FlaxBartForConditionalGeneration"),w_t.forEach(t),Wvr=r(D8e," (BART model)"),D8e.forEach(t),Qvr=i(me),sC=n(me,"LI",{});var j8e=s(sC);m1e=n(j8e,"STRONG",{});var A_t=s(m1e);Hvr=r(A_t,"bert"),A_t.forEach(t),Uvr=r(j8e," \u2014 "),LX=n(j8e,"A",{href:!0});var L_t=s(LX);Jvr=r(L_t,"FlaxBertForPreTraining"),L_t.forEach(t),Yvr=r(j8e," (BERT model)"),j8e.forEach(t),Kvr=i(me),lC=n(me,"LI",{});var N8e=s(lC);g1e=n(N8e,"STRONG",{});var B_t=s(g1e);Zvr=r(B_t,"big_bird"),B_t.forEach(t),eTr=r(N8e," \u2014 "),BX=n(N8e,"A",{href:!0});var x_t=s(BX);oTr=r(x_t,"FlaxBigBirdForPreTraining"),x_t.forEach(t),rTr=r(N8e," (BigBird model)"),N8e.forEach(t),tTr=i(me),iC=n(me,"LI",{});var q8e=s(iC);h1e=n(q8e,"STRONG",{});var k_t=s(h1e);aTr=r(k_t,"electra"),k_t.forEach(t),nTr=r(q8e," \u2014 "),xX=n(q8e,"A",{href:!0});var R_t=s(xX);sTr=r(R_t,"FlaxElectraForPreTraining"),R_t.forEach(t),lTr=r(q8e," (ELECTRA model)"),q8e.forEach(t),iTr=i(me),dC=n(me,"LI",{});var G8e=s(dC);p1e=n(G8e,"STRONG",{});var S_t=s(p1e);dTr=r(S_t,"mbart"),S_t.forEach(t),cTr=r(G8e," \u2014 "),kX=n(G8e,"A",{href:!0});var P_t=s(kX);fTr=r(P_t,"FlaxMBartForConditionalGeneration"),P_t.forEach(t),mTr=r(G8e," (mBART model)"),G8e.forEach(t),gTr=i(me),cC=n(me,"LI",{});var O8e=s(cC);_1e=n(O8e,"STRONG",{});var $_t=s(_1e);hTr=r($_t,"mt5"),$_t.forEach(t),pTr=r(O8e," \u2014 "),RX=n(O8e,"A",{href:!0});var I_t=s(RX);_Tr=r(I_t,"FlaxMT5ForConditionalGeneration"),I_t.forEach(t),uTr=r(O8e," (mT5 model)"),O8e.forEach(t),bTr=i(me),fC=n(me,"LI",{});var X8e=s(fC);u1e=n(X8e,"STRONG",{});var D_t=s(u1e);vTr=r(D_t,"roberta"),D_t.forEach(t),TTr=r(X8e," \u2014 "),SX=n(X8e,"A",{href:!0});var j_t=s(SX);FTr=r(j_t,"FlaxRobertaForMaskedLM"),j_t.forEach(t),CTr=r(X8e," (RoBERTa model)"),X8e.forEach(t),MTr=i(me),mC=n(me,"LI",{});var V8e=s(mC);b1e=n(V8e,"STRONG",{});var N_t=s(b1e);ETr=r(N_t,"roformer"),N_t.forEach(t),yTr=r(V8e," \u2014 "),PX=n(V8e,"A",{href:!0});var q_t=s(PX);wTr=r(q_t,"FlaxRoFormerForMaskedLM"),q_t.forEach(t),ATr=r(V8e," (RoFormer model)"),V8e.forEach(t),LTr=i(me),gC=n(me,"LI",{});var z8e=s(gC);v1e=n(z8e,"STRONG",{});var G_t=s(v1e);BTr=r(G_t,"t5"),G_t.forEach(t),xTr=r(z8e," \u2014 "),$X=n(z8e,"A",{href:!0});var O_t=s($X);kTr=r(O_t,"FlaxT5ForConditionalGeneration"),O_t.forEach(t),RTr=r(z8e," (T5 model)"),z8e.forEach(t),STr=i(me),hC=n(me,"LI",{});var W8e=s(hC);T1e=n(W8e,"STRONG",{});var X_t=s(T1e);PTr=r(X_t,"wav2vec2"),X_t.forEach(t),$Tr=r(W8e," \u2014 "),IX=n(W8e,"A",{href:!0});var V_t=s(IX);ITr=r(V_t,"FlaxWav2Vec2ForPreTraining"),V_t.forEach(t),DTr=r(W8e," (Wav2Vec2 model)"),W8e.forEach(t),jTr=i(me),pC=n(me,"LI",{});var Q8e=s(pC);F1e=n(Q8e,"STRONG",{});var z_t=s(F1e);NTr=r(z_t,"xlm-roberta"),z_t.forEach(t),qTr=r(Q8e," \u2014 "),C1e=n(Q8e,"CODE",{});var W_t=s(C1e);GTr=r(W_t,"FlaxXLMRobertaForMaskedLM"),W_t.forEach(t),OTr=r(Q8e,"(XLM-RoBERTa model)"),Q8e.forEach(t),me.forEach(t),XTr=i(ya),M1e=n(ya,"P",{});var Q_t=s(M1e);VTr=r(Q_t,"Examples:"),Q_t.forEach(t),zTr=i(ya),m(lA.$$.fragment,ya),ya.forEach(t),li.forEach(t),Pxe=i(c),Uc=n(c,"H2",{class:!0});var VRe=s(Uc);_C=n(VRe,"A",{id:!0,class:!0,href:!0});var H_t=s(_C);E1e=n(H_t,"SPAN",{});var U_t=s(E1e);m(iA.$$.fragment,U_t),U_t.forEach(t),H_t.forEach(t),WTr=i(VRe),y1e=n(VRe,"SPAN",{});var J_t=s(y1e);QTr=r(J_t,"FlaxAutoModelForMaskedLM"),J_t.forEach(t),VRe.forEach(t),$xe=i(c),Sr=n(c,"DIV",{class:!0});var di=s(Sr);m(dA.$$.fragment,di),HTr=i(di),Jc=n(di,"P",{});var dW=s(Jc);UTr=r(dW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),w1e=n(dW,"CODE",{});var Y_t=s(w1e);JTr=r(Y_t,"from_pretrained()"),Y_t.forEach(t),YTr=r(dW,"class method or the "),A1e=n(dW,"CODE",{});var K_t=s(A1e);KTr=r(K_t,"from_config()"),K_t.forEach(t),ZTr=r(dW,`class
method.`),dW.forEach(t),eFr=i(di),cA=n(di,"P",{});var zRe=s(cA);oFr=r(zRe,"This class cannot be instantiated directly using "),L1e=n(zRe,"CODE",{});var Z_t=s(L1e);rFr=r(Z_t,"__init__()"),Z_t.forEach(t),tFr=r(zRe," (throws an error)."),zRe.forEach(t),aFr=i(di),wt=n(di,"DIV",{class:!0});var ci=s(wt);m(fA.$$.fragment,ci),nFr=i(ci),B1e=n(ci,"P",{});var eut=s(B1e);sFr=r(eut,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),eut.forEach(t),lFr=i(ci),Yc=n(ci,"P",{});var cW=s(Yc);iFr=r(cW,`Note:
Loading a model from its configuration file does `),x1e=n(cW,"STRONG",{});var out=s(x1e);dFr=r(out,"not"),out.forEach(t),cFr=r(cW,` load the model weights. It only affects the
model\u2019s configuration. Use `),k1e=n(cW,"CODE",{});var rut=s(k1e);fFr=r(rut,"from_pretrained()"),rut.forEach(t),mFr=r(cW,"to load the model weights."),cW.forEach(t),gFr=i(ci),R1e=n(ci,"P",{});var tut=s(R1e);hFr=r(tut,"Examples:"),tut.forEach(t),pFr=i(ci),m(mA.$$.fragment,ci),ci.forEach(t),_Fr=i(di),Bo=n(di,"DIV",{class:!0});var wa=s(Bo);m(gA.$$.fragment,wa),uFr=i(wa),S1e=n(wa,"P",{});var aut=s(S1e);bFr=r(aut,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),aut.forEach(t),vFr=i(wa),An=n(wa,"P",{});var T4=s(An);TFr=r(T4,"The model class to instantiate is selected based on the "),P1e=n(T4,"CODE",{});var nut=s(P1e);FFr=r(nut,"model_type"),nut.forEach(t),CFr=r(T4,` property of the config object (either
passed as an argument or loaded from `),$1e=n(T4,"CODE",{});var sut=s($1e);MFr=r(sut,"pretrained_model_name_or_path"),sut.forEach(t),EFr=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),I1e=n(T4,"CODE",{});var lut=s(I1e);yFr=r(lut,"pretrained_model_name_or_path"),lut.forEach(t),wFr=r(T4,":"),T4.forEach(t),AFr=i(wa),ue=n(wa,"UL",{});var ye=s(ue);uC=n(ye,"LI",{});var H8e=s(uC);D1e=n(H8e,"STRONG",{});var iut=s(D1e);LFr=r(iut,"albert"),iut.forEach(t),BFr=r(H8e," \u2014 "),DX=n(H8e,"A",{href:!0});var dut=s(DX);xFr=r(dut,"FlaxAlbertForMaskedLM"),dut.forEach(t),kFr=r(H8e," (ALBERT model)"),H8e.forEach(t),RFr=i(ye),bC=n(ye,"LI",{});var U8e=s(bC);j1e=n(U8e,"STRONG",{});var cut=s(j1e);SFr=r(cut,"bart"),cut.forEach(t),PFr=r(U8e," \u2014 "),jX=n(U8e,"A",{href:!0});var fut=s(jX);$Fr=r(fut,"FlaxBartForConditionalGeneration"),fut.forEach(t),IFr=r(U8e," (BART model)"),U8e.forEach(t),DFr=i(ye),vC=n(ye,"LI",{});var J8e=s(vC);N1e=n(J8e,"STRONG",{});var mut=s(N1e);jFr=r(mut,"bert"),mut.forEach(t),NFr=r(J8e," \u2014 "),NX=n(J8e,"A",{href:!0});var gut=s(NX);qFr=r(gut,"FlaxBertForMaskedLM"),gut.forEach(t),GFr=r(J8e," (BERT model)"),J8e.forEach(t),OFr=i(ye),TC=n(ye,"LI",{});var Y8e=s(TC);q1e=n(Y8e,"STRONG",{});var hut=s(q1e);XFr=r(hut,"big_bird"),hut.forEach(t),VFr=r(Y8e," \u2014 "),qX=n(Y8e,"A",{href:!0});var put=s(qX);zFr=r(put,"FlaxBigBirdForMaskedLM"),put.forEach(t),WFr=r(Y8e," (BigBird model)"),Y8e.forEach(t),QFr=i(ye),FC=n(ye,"LI",{});var K8e=s(FC);G1e=n(K8e,"STRONG",{});var _ut=s(G1e);HFr=r(_ut,"distilbert"),_ut.forEach(t),UFr=r(K8e," \u2014 "),GX=n(K8e,"A",{href:!0});var uut=s(GX);JFr=r(uut,"FlaxDistilBertForMaskedLM"),uut.forEach(t),YFr=r(K8e," (DistilBERT model)"),K8e.forEach(t),KFr=i(ye),CC=n(ye,"LI",{});var Z8e=s(CC);O1e=n(Z8e,"STRONG",{});var but=s(O1e);ZFr=r(but,"electra"),but.forEach(t),e9r=r(Z8e," \u2014 "),OX=n(Z8e,"A",{href:!0});var vut=s(OX);o9r=r(vut,"FlaxElectraForMaskedLM"),vut.forEach(t),r9r=r(Z8e," (ELECTRA model)"),Z8e.forEach(t),t9r=i(ye),MC=n(ye,"LI",{});var e7e=s(MC);X1e=n(e7e,"STRONG",{});var Tut=s(X1e);a9r=r(Tut,"mbart"),Tut.forEach(t),n9r=r(e7e," \u2014 "),XX=n(e7e,"A",{href:!0});var Fut=s(XX);s9r=r(Fut,"FlaxMBartForConditionalGeneration"),Fut.forEach(t),l9r=r(e7e," (mBART model)"),e7e.forEach(t),i9r=i(ye),EC=n(ye,"LI",{});var o7e=s(EC);V1e=n(o7e,"STRONG",{});var Cut=s(V1e);d9r=r(Cut,"roberta"),Cut.forEach(t),c9r=r(o7e," \u2014 "),VX=n(o7e,"A",{href:!0});var Mut=s(VX);f9r=r(Mut,"FlaxRobertaForMaskedLM"),Mut.forEach(t),m9r=r(o7e," (RoBERTa model)"),o7e.forEach(t),g9r=i(ye),yC=n(ye,"LI",{});var r7e=s(yC);z1e=n(r7e,"STRONG",{});var Eut=s(z1e);h9r=r(Eut,"roformer"),Eut.forEach(t),p9r=r(r7e," \u2014 "),zX=n(r7e,"A",{href:!0});var yut=s(zX);_9r=r(yut,"FlaxRoFormerForMaskedLM"),yut.forEach(t),u9r=r(r7e," (RoFormer model)"),r7e.forEach(t),b9r=i(ye),wC=n(ye,"LI",{});var t7e=s(wC);W1e=n(t7e,"STRONG",{});var wut=s(W1e);v9r=r(wut,"xlm-roberta"),wut.forEach(t),T9r=r(t7e," \u2014 "),Q1e=n(t7e,"CODE",{});var Aut=s(Q1e);F9r=r(Aut,"FlaxXLMRobertaForMaskedLM"),Aut.forEach(t),C9r=r(t7e,"(XLM-RoBERTa model)"),t7e.forEach(t),ye.forEach(t),M9r=i(wa),H1e=n(wa,"P",{});var Lut=s(H1e);E9r=r(Lut,"Examples:"),Lut.forEach(t),y9r=i(wa),m(hA.$$.fragment,wa),wa.forEach(t),di.forEach(t),Ixe=i(c),Kc=n(c,"H2",{class:!0});var WRe=s(Kc);AC=n(WRe,"A",{id:!0,class:!0,href:!0});var But=s(AC);U1e=n(But,"SPAN",{});var xut=s(U1e);m(pA.$$.fragment,xut),xut.forEach(t),But.forEach(t),w9r=i(WRe),J1e=n(WRe,"SPAN",{});var kut=s(J1e);A9r=r(kut,"FlaxAutoModelForSeq2SeqLM"),kut.forEach(t),WRe.forEach(t),Dxe=i(c),Pr=n(c,"DIV",{class:!0});var fi=s(Pr);m(_A.$$.fragment,fi),L9r=i(fi),Zc=n(fi,"P",{});var fW=s(Zc);B9r=r(fW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Y1e=n(fW,"CODE",{});var Rut=s(Y1e);x9r=r(Rut,"from_pretrained()"),Rut.forEach(t),k9r=r(fW,"class method or the "),K1e=n(fW,"CODE",{});var Sut=s(K1e);R9r=r(Sut,"from_config()"),Sut.forEach(t),S9r=r(fW,`class
method.`),fW.forEach(t),P9r=i(fi),uA=n(fi,"P",{});var QRe=s(uA);$9r=r(QRe,"This class cannot be instantiated directly using "),Z1e=n(QRe,"CODE",{});var Put=s(Z1e);I9r=r(Put,"__init__()"),Put.forEach(t),D9r=r(QRe," (throws an error)."),QRe.forEach(t),j9r=i(fi),At=n(fi,"DIV",{class:!0});var mi=s(At);m(bA.$$.fragment,mi),N9r=i(mi),ebe=n(mi,"P",{});var $ut=s(ebe);q9r=r($ut,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$ut.forEach(t),G9r=i(mi),ef=n(mi,"P",{});var mW=s(ef);O9r=r(mW,`Note:
Loading a model from its configuration file does `),obe=n(mW,"STRONG",{});var Iut=s(obe);X9r=r(Iut,"not"),Iut.forEach(t),V9r=r(mW,` load the model weights. It only affects the
model\u2019s configuration. Use `),rbe=n(mW,"CODE",{});var Dut=s(rbe);z9r=r(Dut,"from_pretrained()"),Dut.forEach(t),W9r=r(mW,"to load the model weights."),mW.forEach(t),Q9r=i(mi),tbe=n(mi,"P",{});var jut=s(tbe);H9r=r(jut,"Examples:"),jut.forEach(t),U9r=i(mi),m(vA.$$.fragment,mi),mi.forEach(t),J9r=i(fi),xo=n(fi,"DIV",{class:!0});var Aa=s(xo);m(TA.$$.fragment,Aa),Y9r=i(Aa),abe=n(Aa,"P",{});var Nut=s(abe);K9r=r(Nut,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Nut.forEach(t),Z9r=i(Aa),Ln=n(Aa,"P",{});var F4=s(Ln);eCr=r(F4,"The model class to instantiate is selected based on the "),nbe=n(F4,"CODE",{});var qut=s(nbe);oCr=r(qut,"model_type"),qut.forEach(t),rCr=r(F4,` property of the config object (either
passed as an argument or loaded from `),sbe=n(F4,"CODE",{});var Gut=s(sbe);tCr=r(Gut,"pretrained_model_name_or_path"),Gut.forEach(t),aCr=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lbe=n(F4,"CODE",{});var Out=s(lbe);nCr=r(Out,"pretrained_model_name_or_path"),Out.forEach(t),sCr=r(F4,":"),F4.forEach(t),lCr=i(Aa),Ce=n(Aa,"UL",{});var so=s(Ce);LC=n(so,"LI",{});var a7e=s(LC);ibe=n(a7e,"STRONG",{});var Xut=s(ibe);iCr=r(Xut,"bart"),Xut.forEach(t),dCr=r(a7e," \u2014 "),WX=n(a7e,"A",{href:!0});var Vut=s(WX);cCr=r(Vut,"FlaxBartForConditionalGeneration"),Vut.forEach(t),fCr=r(a7e," (BART model)"),a7e.forEach(t),mCr=i(so),BC=n(so,"LI",{});var n7e=s(BC);dbe=n(n7e,"STRONG",{});var zut=s(dbe);gCr=r(zut,"blenderbot"),zut.forEach(t),hCr=r(n7e," \u2014 "),QX=n(n7e,"A",{href:!0});var Wut=s(QX);pCr=r(Wut,"FlaxBlenderbotForConditionalGeneration"),Wut.forEach(t),_Cr=r(n7e," (Blenderbot model)"),n7e.forEach(t),uCr=i(so),xC=n(so,"LI",{});var s7e=s(xC);cbe=n(s7e,"STRONG",{});var Qut=s(cbe);bCr=r(Qut,"blenderbot-small"),Qut.forEach(t),vCr=r(s7e," \u2014 "),HX=n(s7e,"A",{href:!0});var Hut=s(HX);TCr=r(Hut,"FlaxBlenderbotSmallForConditionalGeneration"),Hut.forEach(t),FCr=r(s7e," (BlenderbotSmall model)"),s7e.forEach(t),CCr=i(so),kC=n(so,"LI",{});var l7e=s(kC);fbe=n(l7e,"STRONG",{});var Uut=s(fbe);MCr=r(Uut,"encoder-decoder"),Uut.forEach(t),ECr=r(l7e," \u2014 "),UX=n(l7e,"A",{href:!0});var Jut=s(UX);yCr=r(Jut,"FlaxEncoderDecoderModel"),Jut.forEach(t),wCr=r(l7e," (Encoder decoder model)"),l7e.forEach(t),ACr=i(so),RC=n(so,"LI",{});var i7e=s(RC);mbe=n(i7e,"STRONG",{});var Yut=s(mbe);LCr=r(Yut,"marian"),Yut.forEach(t),BCr=r(i7e," \u2014 "),JX=n(i7e,"A",{href:!0});var Kut=s(JX);xCr=r(Kut,"FlaxMarianMTModel"),Kut.forEach(t),kCr=r(i7e," (Marian model)"),i7e.forEach(t),RCr=i(so),SC=n(so,"LI",{});var d7e=s(SC);gbe=n(d7e,"STRONG",{});var Zut=s(gbe);SCr=r(Zut,"mbart"),Zut.forEach(t),PCr=r(d7e," \u2014 "),YX=n(d7e,"A",{href:!0});var e0t=s(YX);$Cr=r(e0t,"FlaxMBartForConditionalGeneration"),e0t.forEach(t),ICr=r(d7e," (mBART model)"),d7e.forEach(t),DCr=i(so),PC=n(so,"LI",{});var c7e=s(PC);hbe=n(c7e,"STRONG",{});var o0t=s(hbe);jCr=r(o0t,"mt5"),o0t.forEach(t),NCr=r(c7e," \u2014 "),KX=n(c7e,"A",{href:!0});var r0t=s(KX);qCr=r(r0t,"FlaxMT5ForConditionalGeneration"),r0t.forEach(t),GCr=r(c7e," (mT5 model)"),c7e.forEach(t),OCr=i(so),$C=n(so,"LI",{});var f7e=s($C);pbe=n(f7e,"STRONG",{});var t0t=s(pbe);XCr=r(t0t,"pegasus"),t0t.forEach(t),VCr=r(f7e," \u2014 "),ZX=n(f7e,"A",{href:!0});var a0t=s(ZX);zCr=r(a0t,"FlaxPegasusForConditionalGeneration"),a0t.forEach(t),WCr=r(f7e," (Pegasus model)"),f7e.forEach(t),QCr=i(so),IC=n(so,"LI",{});var m7e=s(IC);_be=n(m7e,"STRONG",{});var n0t=s(_be);HCr=r(n0t,"t5"),n0t.forEach(t),UCr=r(m7e," \u2014 "),eV=n(m7e,"A",{href:!0});var s0t=s(eV);JCr=r(s0t,"FlaxT5ForConditionalGeneration"),s0t.forEach(t),YCr=r(m7e," (T5 model)"),m7e.forEach(t),so.forEach(t),KCr=i(Aa),ube=n(Aa,"P",{});var l0t=s(ube);ZCr=r(l0t,"Examples:"),l0t.forEach(t),eMr=i(Aa),m(FA.$$.fragment,Aa),Aa.forEach(t),fi.forEach(t),jxe=i(c),of=n(c,"H2",{class:!0});var HRe=s(of);DC=n(HRe,"A",{id:!0,class:!0,href:!0});var i0t=s(DC);bbe=n(i0t,"SPAN",{});var d0t=s(bbe);m(CA.$$.fragment,d0t),d0t.forEach(t),i0t.forEach(t),oMr=i(HRe),vbe=n(HRe,"SPAN",{});var c0t=s(vbe);rMr=r(c0t,"FlaxAutoModelForSequenceClassification"),c0t.forEach(t),HRe.forEach(t),Nxe=i(c),$r=n(c,"DIV",{class:!0});var gi=s($r);m(MA.$$.fragment,gi),tMr=i(gi),rf=n(gi,"P",{});var gW=s(rf);aMr=r(gW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Tbe=n(gW,"CODE",{});var f0t=s(Tbe);nMr=r(f0t,"from_pretrained()"),f0t.forEach(t),sMr=r(gW,"class method or the "),Fbe=n(gW,"CODE",{});var m0t=s(Fbe);lMr=r(m0t,"from_config()"),m0t.forEach(t),iMr=r(gW,`class
method.`),gW.forEach(t),dMr=i(gi),EA=n(gi,"P",{});var URe=s(EA);cMr=r(URe,"This class cannot be instantiated directly using "),Cbe=n(URe,"CODE",{});var g0t=s(Cbe);fMr=r(g0t,"__init__()"),g0t.forEach(t),mMr=r(URe," (throws an error)."),URe.forEach(t),gMr=i(gi),Lt=n(gi,"DIV",{class:!0});var hi=s(Lt);m(yA.$$.fragment,hi),hMr=i(hi),Mbe=n(hi,"P",{});var h0t=s(Mbe);pMr=r(h0t,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),h0t.forEach(t),_Mr=i(hi),tf=n(hi,"P",{});var hW=s(tf);uMr=r(hW,`Note:
Loading a model from its configuration file does `),Ebe=n(hW,"STRONG",{});var p0t=s(Ebe);bMr=r(p0t,"not"),p0t.forEach(t),vMr=r(hW,` load the model weights. It only affects the
model\u2019s configuration. Use `),ybe=n(hW,"CODE",{});var _0t=s(ybe);TMr=r(_0t,"from_pretrained()"),_0t.forEach(t),FMr=r(hW,"to load the model weights."),hW.forEach(t),CMr=i(hi),wbe=n(hi,"P",{});var u0t=s(wbe);MMr=r(u0t,"Examples:"),u0t.forEach(t),EMr=i(hi),m(wA.$$.fragment,hi),hi.forEach(t),yMr=i(gi),ko=n(gi,"DIV",{class:!0});var La=s(ko);m(AA.$$.fragment,La),wMr=i(La),Abe=n(La,"P",{});var b0t=s(Abe);AMr=r(b0t,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),b0t.forEach(t),LMr=i(La),Bn=n(La,"P",{});var C4=s(Bn);BMr=r(C4,"The model class to instantiate is selected based on the "),Lbe=n(C4,"CODE",{});var v0t=s(Lbe);xMr=r(v0t,"model_type"),v0t.forEach(t),kMr=r(C4,` property of the config object (either
passed as an argument or loaded from `),Bbe=n(C4,"CODE",{});var T0t=s(Bbe);RMr=r(T0t,"pretrained_model_name_or_path"),T0t.forEach(t),SMr=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xbe=n(C4,"CODE",{});var F0t=s(xbe);PMr=r(F0t,"pretrained_model_name_or_path"),F0t.forEach(t),$Mr=r(C4,":"),C4.forEach(t),IMr=i(La),be=n(La,"UL",{});var we=s(be);jC=n(we,"LI",{});var g7e=s(jC);kbe=n(g7e,"STRONG",{});var C0t=s(kbe);DMr=r(C0t,"albert"),C0t.forEach(t),jMr=r(g7e," \u2014 "),oV=n(g7e,"A",{href:!0});var M0t=s(oV);NMr=r(M0t,"FlaxAlbertForSequenceClassification"),M0t.forEach(t),qMr=r(g7e," (ALBERT model)"),g7e.forEach(t),GMr=i(we),NC=n(we,"LI",{});var h7e=s(NC);Rbe=n(h7e,"STRONG",{});var E0t=s(Rbe);OMr=r(E0t,"bart"),E0t.forEach(t),XMr=r(h7e," \u2014 "),rV=n(h7e,"A",{href:!0});var y0t=s(rV);VMr=r(y0t,"FlaxBartForSequenceClassification"),y0t.forEach(t),zMr=r(h7e," (BART model)"),h7e.forEach(t),WMr=i(we),qC=n(we,"LI",{});var p7e=s(qC);Sbe=n(p7e,"STRONG",{});var w0t=s(Sbe);QMr=r(w0t,"bert"),w0t.forEach(t),HMr=r(p7e," \u2014 "),tV=n(p7e,"A",{href:!0});var A0t=s(tV);UMr=r(A0t,"FlaxBertForSequenceClassification"),A0t.forEach(t),JMr=r(p7e," (BERT model)"),p7e.forEach(t),YMr=i(we),GC=n(we,"LI",{});var _7e=s(GC);Pbe=n(_7e,"STRONG",{});var L0t=s(Pbe);KMr=r(L0t,"big_bird"),L0t.forEach(t),ZMr=r(_7e," \u2014 "),aV=n(_7e,"A",{href:!0});var B0t=s(aV);e4r=r(B0t,"FlaxBigBirdForSequenceClassification"),B0t.forEach(t),o4r=r(_7e," (BigBird model)"),_7e.forEach(t),r4r=i(we),OC=n(we,"LI",{});var u7e=s(OC);$be=n(u7e,"STRONG",{});var x0t=s($be);t4r=r(x0t,"distilbert"),x0t.forEach(t),a4r=r(u7e," \u2014 "),nV=n(u7e,"A",{href:!0});var k0t=s(nV);n4r=r(k0t,"FlaxDistilBertForSequenceClassification"),k0t.forEach(t),s4r=r(u7e," (DistilBERT model)"),u7e.forEach(t),l4r=i(we),XC=n(we,"LI",{});var b7e=s(XC);Ibe=n(b7e,"STRONG",{});var R0t=s(Ibe);i4r=r(R0t,"electra"),R0t.forEach(t),d4r=r(b7e," \u2014 "),sV=n(b7e,"A",{href:!0});var S0t=s(sV);c4r=r(S0t,"FlaxElectraForSequenceClassification"),S0t.forEach(t),f4r=r(b7e," (ELECTRA model)"),b7e.forEach(t),m4r=i(we),VC=n(we,"LI",{});var v7e=s(VC);Dbe=n(v7e,"STRONG",{});var P0t=s(Dbe);g4r=r(P0t,"mbart"),P0t.forEach(t),h4r=r(v7e," \u2014 "),lV=n(v7e,"A",{href:!0});var $0t=s(lV);p4r=r($0t,"FlaxMBartForSequenceClassification"),$0t.forEach(t),_4r=r(v7e," (mBART model)"),v7e.forEach(t),u4r=i(we),zC=n(we,"LI",{});var T7e=s(zC);jbe=n(T7e,"STRONG",{});var I0t=s(jbe);b4r=r(I0t,"roberta"),I0t.forEach(t),v4r=r(T7e," \u2014 "),iV=n(T7e,"A",{href:!0});var D0t=s(iV);T4r=r(D0t,"FlaxRobertaForSequenceClassification"),D0t.forEach(t),F4r=r(T7e," (RoBERTa model)"),T7e.forEach(t),C4r=i(we),WC=n(we,"LI",{});var F7e=s(WC);Nbe=n(F7e,"STRONG",{});var j0t=s(Nbe);M4r=r(j0t,"roformer"),j0t.forEach(t),E4r=r(F7e," \u2014 "),dV=n(F7e,"A",{href:!0});var N0t=s(dV);y4r=r(N0t,"FlaxRoFormerForSequenceClassification"),N0t.forEach(t),w4r=r(F7e," (RoFormer model)"),F7e.forEach(t),A4r=i(we),QC=n(we,"LI",{});var C7e=s(QC);qbe=n(C7e,"STRONG",{});var q0t=s(qbe);L4r=r(q0t,"xlm-roberta"),q0t.forEach(t),B4r=r(C7e," \u2014 "),Gbe=n(C7e,"CODE",{});var G0t=s(Gbe);x4r=r(G0t,"FlaxXLMRobertaForSequenceClassification"),G0t.forEach(t),k4r=r(C7e,"(XLM-RoBERTa model)"),C7e.forEach(t),we.forEach(t),R4r=i(La),Obe=n(La,"P",{});var O0t=s(Obe);S4r=r(O0t,"Examples:"),O0t.forEach(t),P4r=i(La),m(LA.$$.fragment,La),La.forEach(t),gi.forEach(t),qxe=i(c),af=n(c,"H2",{class:!0});var JRe=s(af);HC=n(JRe,"A",{id:!0,class:!0,href:!0});var X0t=s(HC);Xbe=n(X0t,"SPAN",{});var V0t=s(Xbe);m(BA.$$.fragment,V0t),V0t.forEach(t),X0t.forEach(t),$4r=i(JRe),Vbe=n(JRe,"SPAN",{});var z0t=s(Vbe);I4r=r(z0t,"FlaxAutoModelForQuestionAnswering"),z0t.forEach(t),JRe.forEach(t),Gxe=i(c),Ir=n(c,"DIV",{class:!0});var pi=s(Ir);m(xA.$$.fragment,pi),D4r=i(pi),nf=n(pi,"P",{});var pW=s(nf);j4r=r(pW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),zbe=n(pW,"CODE",{});var W0t=s(zbe);N4r=r(W0t,"from_pretrained()"),W0t.forEach(t),q4r=r(pW,"class method or the "),Wbe=n(pW,"CODE",{});var Q0t=s(Wbe);G4r=r(Q0t,"from_config()"),Q0t.forEach(t),O4r=r(pW,`class
method.`),pW.forEach(t),X4r=i(pi),kA=n(pi,"P",{});var YRe=s(kA);V4r=r(YRe,"This class cannot be instantiated directly using "),Qbe=n(YRe,"CODE",{});var H0t=s(Qbe);z4r=r(H0t,"__init__()"),H0t.forEach(t),W4r=r(YRe," (throws an error)."),YRe.forEach(t),Q4r=i(pi),Bt=n(pi,"DIV",{class:!0});var _i=s(Bt);m(RA.$$.fragment,_i),H4r=i(_i),Hbe=n(_i,"P",{});var U0t=s(Hbe);U4r=r(U0t,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),U0t.forEach(t),J4r=i(_i),sf=n(_i,"P",{});var _W=s(sf);Y4r=r(_W,`Note:
Loading a model from its configuration file does `),Ube=n(_W,"STRONG",{});var J0t=s(Ube);K4r=r(J0t,"not"),J0t.forEach(t),Z4r=r(_W,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jbe=n(_W,"CODE",{});var Y0t=s(Jbe);eEr=r(Y0t,"from_pretrained()"),Y0t.forEach(t),oEr=r(_W,"to load the model weights."),_W.forEach(t),rEr=i(_i),Ybe=n(_i,"P",{});var K0t=s(Ybe);tEr=r(K0t,"Examples:"),K0t.forEach(t),aEr=i(_i),m(SA.$$.fragment,_i),_i.forEach(t),nEr=i(pi),Ro=n(pi,"DIV",{class:!0});var Ba=s(Ro);m(PA.$$.fragment,Ba),sEr=i(Ba),Kbe=n(Ba,"P",{});var Z0t=s(Kbe);lEr=r(Z0t,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Z0t.forEach(t),iEr=i(Ba),xn=n(Ba,"P",{});var M4=s(xn);dEr=r(M4,"The model class to instantiate is selected based on the "),Zbe=n(M4,"CODE",{});var e1t=s(Zbe);cEr=r(e1t,"model_type"),e1t.forEach(t),fEr=r(M4,` property of the config object (either
passed as an argument or loaded from `),e5e=n(M4,"CODE",{});var o1t=s(e5e);mEr=r(o1t,"pretrained_model_name_or_path"),o1t.forEach(t),gEr=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o5e=n(M4,"CODE",{});var r1t=s(o5e);hEr=r(r1t,"pretrained_model_name_or_path"),r1t.forEach(t),pEr=r(M4,":"),M4.forEach(t),_Er=i(Ba),ve=n(Ba,"UL",{});var Ae=s(ve);UC=n(Ae,"LI",{});var M7e=s(UC);r5e=n(M7e,"STRONG",{});var t1t=s(r5e);uEr=r(t1t,"albert"),t1t.forEach(t),bEr=r(M7e," \u2014 "),cV=n(M7e,"A",{href:!0});var a1t=s(cV);vEr=r(a1t,"FlaxAlbertForQuestionAnswering"),a1t.forEach(t),TEr=r(M7e," (ALBERT model)"),M7e.forEach(t),FEr=i(Ae),JC=n(Ae,"LI",{});var E7e=s(JC);t5e=n(E7e,"STRONG",{});var n1t=s(t5e);CEr=r(n1t,"bart"),n1t.forEach(t),MEr=r(E7e," \u2014 "),fV=n(E7e,"A",{href:!0});var s1t=s(fV);EEr=r(s1t,"FlaxBartForQuestionAnswering"),s1t.forEach(t),yEr=r(E7e," (BART model)"),E7e.forEach(t),wEr=i(Ae),YC=n(Ae,"LI",{});var y7e=s(YC);a5e=n(y7e,"STRONG",{});var l1t=s(a5e);AEr=r(l1t,"bert"),l1t.forEach(t),LEr=r(y7e," \u2014 "),mV=n(y7e,"A",{href:!0});var i1t=s(mV);BEr=r(i1t,"FlaxBertForQuestionAnswering"),i1t.forEach(t),xEr=r(y7e," (BERT model)"),y7e.forEach(t),kEr=i(Ae),KC=n(Ae,"LI",{});var w7e=s(KC);n5e=n(w7e,"STRONG",{});var d1t=s(n5e);REr=r(d1t,"big_bird"),d1t.forEach(t),SEr=r(w7e," \u2014 "),gV=n(w7e,"A",{href:!0});var c1t=s(gV);PEr=r(c1t,"FlaxBigBirdForQuestionAnswering"),c1t.forEach(t),$Er=r(w7e," (BigBird model)"),w7e.forEach(t),IEr=i(Ae),ZC=n(Ae,"LI",{});var A7e=s(ZC);s5e=n(A7e,"STRONG",{});var f1t=s(s5e);DEr=r(f1t,"distilbert"),f1t.forEach(t),jEr=r(A7e," \u2014 "),hV=n(A7e,"A",{href:!0});var m1t=s(hV);NEr=r(m1t,"FlaxDistilBertForQuestionAnswering"),m1t.forEach(t),qEr=r(A7e," (DistilBERT model)"),A7e.forEach(t),GEr=i(Ae),eM=n(Ae,"LI",{});var L7e=s(eM);l5e=n(L7e,"STRONG",{});var g1t=s(l5e);OEr=r(g1t,"electra"),g1t.forEach(t),XEr=r(L7e," \u2014 "),pV=n(L7e,"A",{href:!0});var h1t=s(pV);VEr=r(h1t,"FlaxElectraForQuestionAnswering"),h1t.forEach(t),zEr=r(L7e," (ELECTRA model)"),L7e.forEach(t),WEr=i(Ae),oM=n(Ae,"LI",{});var B7e=s(oM);i5e=n(B7e,"STRONG",{});var p1t=s(i5e);QEr=r(p1t,"mbart"),p1t.forEach(t),HEr=r(B7e," \u2014 "),_V=n(B7e,"A",{href:!0});var _1t=s(_V);UEr=r(_1t,"FlaxMBartForQuestionAnswering"),_1t.forEach(t),JEr=r(B7e," (mBART model)"),B7e.forEach(t),YEr=i(Ae),rM=n(Ae,"LI",{});var x7e=s(rM);d5e=n(x7e,"STRONG",{});var u1t=s(d5e);KEr=r(u1t,"roberta"),u1t.forEach(t),ZEr=r(x7e," \u2014 "),uV=n(x7e,"A",{href:!0});var b1t=s(uV);e3r=r(b1t,"FlaxRobertaForQuestionAnswering"),b1t.forEach(t),o3r=r(x7e," (RoBERTa model)"),x7e.forEach(t),r3r=i(Ae),tM=n(Ae,"LI",{});var k7e=s(tM);c5e=n(k7e,"STRONG",{});var v1t=s(c5e);t3r=r(v1t,"roformer"),v1t.forEach(t),a3r=r(k7e," \u2014 "),bV=n(k7e,"A",{href:!0});var T1t=s(bV);n3r=r(T1t,"FlaxRoFormerForQuestionAnswering"),T1t.forEach(t),s3r=r(k7e," (RoFormer model)"),k7e.forEach(t),l3r=i(Ae),aM=n(Ae,"LI",{});var R7e=s(aM);f5e=n(R7e,"STRONG",{});var F1t=s(f5e);i3r=r(F1t,"xlm-roberta"),F1t.forEach(t),d3r=r(R7e," \u2014 "),m5e=n(R7e,"CODE",{});var C1t=s(m5e);c3r=r(C1t,"FlaxXLMRobertaForQuestionAnswering"),C1t.forEach(t),f3r=r(R7e,"(XLM-RoBERTa model)"),R7e.forEach(t),Ae.forEach(t),m3r=i(Ba),g5e=n(Ba,"P",{});var M1t=s(g5e);g3r=r(M1t,"Examples:"),M1t.forEach(t),h3r=i(Ba),m($A.$$.fragment,Ba),Ba.forEach(t),pi.forEach(t),Oxe=i(c),lf=n(c,"H2",{class:!0});var KRe=s(lf);nM=n(KRe,"A",{id:!0,class:!0,href:!0});var E1t=s(nM);h5e=n(E1t,"SPAN",{});var y1t=s(h5e);m(IA.$$.fragment,y1t),y1t.forEach(t),E1t.forEach(t),p3r=i(KRe),p5e=n(KRe,"SPAN",{});var w1t=s(p5e);_3r=r(w1t,"FlaxAutoModelForTokenClassification"),w1t.forEach(t),KRe.forEach(t),Xxe=i(c),Dr=n(c,"DIV",{class:!0});var ui=s(Dr);m(DA.$$.fragment,ui),u3r=i(ui),df=n(ui,"P",{});var uW=s(df);b3r=r(uW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),_5e=n(uW,"CODE",{});var A1t=s(_5e);v3r=r(A1t,"from_pretrained()"),A1t.forEach(t),T3r=r(uW,"class method or the "),u5e=n(uW,"CODE",{});var L1t=s(u5e);F3r=r(L1t,"from_config()"),L1t.forEach(t),C3r=r(uW,`class
method.`),uW.forEach(t),M3r=i(ui),jA=n(ui,"P",{});var ZRe=s(jA);E3r=r(ZRe,"This class cannot be instantiated directly using "),b5e=n(ZRe,"CODE",{});var B1t=s(b5e);y3r=r(B1t,"__init__()"),B1t.forEach(t),w3r=r(ZRe," (throws an error)."),ZRe.forEach(t),A3r=i(ui),xt=n(ui,"DIV",{class:!0});var bi=s(xt);m(NA.$$.fragment,bi),L3r=i(bi),v5e=n(bi,"P",{});var x1t=s(v5e);B3r=r(x1t,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),x1t.forEach(t),x3r=i(bi),cf=n(bi,"P",{});var bW=s(cf);k3r=r(bW,`Note:
Loading a model from its configuration file does `),T5e=n(bW,"STRONG",{});var k1t=s(T5e);R3r=r(k1t,"not"),k1t.forEach(t),S3r=r(bW,` load the model weights. It only affects the
model\u2019s configuration. Use `),F5e=n(bW,"CODE",{});var R1t=s(F5e);P3r=r(R1t,"from_pretrained()"),R1t.forEach(t),$3r=r(bW,"to load the model weights."),bW.forEach(t),I3r=i(bi),C5e=n(bi,"P",{});var S1t=s(C5e);D3r=r(S1t,"Examples:"),S1t.forEach(t),j3r=i(bi),m(qA.$$.fragment,bi),bi.forEach(t),N3r=i(ui),So=n(ui,"DIV",{class:!0});var xa=s(So);m(GA.$$.fragment,xa),q3r=i(xa),M5e=n(xa,"P",{});var P1t=s(M5e);G3r=r(P1t,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),P1t.forEach(t),O3r=i(xa),kn=n(xa,"P",{});var E4=s(kn);X3r=r(E4,"The model class to instantiate is selected based on the "),E5e=n(E4,"CODE",{});var $1t=s(E5e);V3r=r($1t,"model_type"),$1t.forEach(t),z3r=r(E4,` property of the config object (either
passed as an argument or loaded from `),y5e=n(E4,"CODE",{});var I1t=s(y5e);W3r=r(I1t,"pretrained_model_name_or_path"),I1t.forEach(t),Q3r=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w5e=n(E4,"CODE",{});var D1t=s(w5e);H3r=r(D1t,"pretrained_model_name_or_path"),D1t.forEach(t),U3r=r(E4,":"),E4.forEach(t),J3r=i(xa),Re=n(xa,"UL",{});var Go=s(Re);sM=n(Go,"LI",{});var S7e=s(sM);A5e=n(S7e,"STRONG",{});var j1t=s(A5e);Y3r=r(j1t,"albert"),j1t.forEach(t),K3r=r(S7e," \u2014 "),vV=n(S7e,"A",{href:!0});var N1t=s(vV);Z3r=r(N1t,"FlaxAlbertForTokenClassification"),N1t.forEach(t),eyr=r(S7e," (ALBERT model)"),S7e.forEach(t),oyr=i(Go),lM=n(Go,"LI",{});var P7e=s(lM);L5e=n(P7e,"STRONG",{});var q1t=s(L5e);ryr=r(q1t,"bert"),q1t.forEach(t),tyr=r(P7e," \u2014 "),TV=n(P7e,"A",{href:!0});var G1t=s(TV);ayr=r(G1t,"FlaxBertForTokenClassification"),G1t.forEach(t),nyr=r(P7e," (BERT model)"),P7e.forEach(t),syr=i(Go),iM=n(Go,"LI",{});var $7e=s(iM);B5e=n($7e,"STRONG",{});var O1t=s(B5e);lyr=r(O1t,"big_bird"),O1t.forEach(t),iyr=r($7e," \u2014 "),FV=n($7e,"A",{href:!0});var X1t=s(FV);dyr=r(X1t,"FlaxBigBirdForTokenClassification"),X1t.forEach(t),cyr=r($7e," (BigBird model)"),$7e.forEach(t),fyr=i(Go),dM=n(Go,"LI",{});var I7e=s(dM);x5e=n(I7e,"STRONG",{});var V1t=s(x5e);myr=r(V1t,"distilbert"),V1t.forEach(t),gyr=r(I7e," \u2014 "),CV=n(I7e,"A",{href:!0});var z1t=s(CV);hyr=r(z1t,"FlaxDistilBertForTokenClassification"),z1t.forEach(t),pyr=r(I7e," (DistilBERT model)"),I7e.forEach(t),_yr=i(Go),cM=n(Go,"LI",{});var D7e=s(cM);k5e=n(D7e,"STRONG",{});var W1t=s(k5e);uyr=r(W1t,"electra"),W1t.forEach(t),byr=r(D7e," \u2014 "),MV=n(D7e,"A",{href:!0});var Q1t=s(MV);vyr=r(Q1t,"FlaxElectraForTokenClassification"),Q1t.forEach(t),Tyr=r(D7e," (ELECTRA model)"),D7e.forEach(t),Fyr=i(Go),fM=n(Go,"LI",{});var j7e=s(fM);R5e=n(j7e,"STRONG",{});var H1t=s(R5e);Cyr=r(H1t,"roberta"),H1t.forEach(t),Myr=r(j7e," \u2014 "),EV=n(j7e,"A",{href:!0});var U1t=s(EV);Eyr=r(U1t,"FlaxRobertaForTokenClassification"),U1t.forEach(t),yyr=r(j7e," (RoBERTa model)"),j7e.forEach(t),wyr=i(Go),mM=n(Go,"LI",{});var N7e=s(mM);S5e=n(N7e,"STRONG",{});var J1t=s(S5e);Ayr=r(J1t,"roformer"),J1t.forEach(t),Lyr=r(N7e," \u2014 "),yV=n(N7e,"A",{href:!0});var Y1t=s(yV);Byr=r(Y1t,"FlaxRoFormerForTokenClassification"),Y1t.forEach(t),xyr=r(N7e," (RoFormer model)"),N7e.forEach(t),kyr=i(Go),gM=n(Go,"LI",{});var q7e=s(gM);P5e=n(q7e,"STRONG",{});var K1t=s(P5e);Ryr=r(K1t,"xlm-roberta"),K1t.forEach(t),Syr=r(q7e," \u2014 "),$5e=n(q7e,"CODE",{});var Z1t=s($5e);Pyr=r(Z1t,"FlaxXLMRobertaForTokenClassification"),Z1t.forEach(t),$yr=r(q7e,"(XLM-RoBERTa model)"),q7e.forEach(t),Go.forEach(t),Iyr=i(xa),I5e=n(xa,"P",{});var ebt=s(I5e);Dyr=r(ebt,"Examples:"),ebt.forEach(t),jyr=i(xa),m(OA.$$.fragment,xa),xa.forEach(t),ui.forEach(t),Vxe=i(c),ff=n(c,"H2",{class:!0});var eSe=s(ff);hM=n(eSe,"A",{id:!0,class:!0,href:!0});var obt=s(hM);D5e=n(obt,"SPAN",{});var rbt=s(D5e);m(XA.$$.fragment,rbt),rbt.forEach(t),obt.forEach(t),Nyr=i(eSe),j5e=n(eSe,"SPAN",{});var tbt=s(j5e);qyr=r(tbt,"FlaxAutoModelForMultipleChoice"),tbt.forEach(t),eSe.forEach(t),zxe=i(c),jr=n(c,"DIV",{class:!0});var vi=s(jr);m(VA.$$.fragment,vi),Gyr=i(vi),mf=n(vi,"P",{});var vW=s(mf);Oyr=r(vW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),N5e=n(vW,"CODE",{});var abt=s(N5e);Xyr=r(abt,"from_pretrained()"),abt.forEach(t),Vyr=r(vW,"class method or the "),q5e=n(vW,"CODE",{});var nbt=s(q5e);zyr=r(nbt,"from_config()"),nbt.forEach(t),Wyr=r(vW,`class
method.`),vW.forEach(t),Qyr=i(vi),zA=n(vi,"P",{});var oSe=s(zA);Hyr=r(oSe,"This class cannot be instantiated directly using "),G5e=n(oSe,"CODE",{});var sbt=s(G5e);Uyr=r(sbt,"__init__()"),sbt.forEach(t),Jyr=r(oSe," (throws an error)."),oSe.forEach(t),Yyr=i(vi),kt=n(vi,"DIV",{class:!0});var Ti=s(kt);m(WA.$$.fragment,Ti),Kyr=i(Ti),O5e=n(Ti,"P",{});var lbt=s(O5e);Zyr=r(lbt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),lbt.forEach(t),ewr=i(Ti),gf=n(Ti,"P",{});var TW=s(gf);owr=r(TW,`Note:
Loading a model from its configuration file does `),X5e=n(TW,"STRONG",{});var ibt=s(X5e);rwr=r(ibt,"not"),ibt.forEach(t),twr=r(TW,` load the model weights. It only affects the
model\u2019s configuration. Use `),V5e=n(TW,"CODE",{});var dbt=s(V5e);awr=r(dbt,"from_pretrained()"),dbt.forEach(t),nwr=r(TW,"to load the model weights."),TW.forEach(t),swr=i(Ti),z5e=n(Ti,"P",{});var cbt=s(z5e);lwr=r(cbt,"Examples:"),cbt.forEach(t),iwr=i(Ti),m(QA.$$.fragment,Ti),Ti.forEach(t),dwr=i(vi),Po=n(vi,"DIV",{class:!0});var ka=s(Po);m(HA.$$.fragment,ka),cwr=i(ka),W5e=n(ka,"P",{});var fbt=s(W5e);fwr=r(fbt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),fbt.forEach(t),mwr=i(ka),Rn=n(ka,"P",{});var y4=s(Rn);gwr=r(y4,"The model class to instantiate is selected based on the "),Q5e=n(y4,"CODE",{});var mbt=s(Q5e);hwr=r(mbt,"model_type"),mbt.forEach(t),pwr=r(y4,` property of the config object (either
passed as an argument or loaded from `),H5e=n(y4,"CODE",{});var gbt=s(H5e);_wr=r(gbt,"pretrained_model_name_or_path"),gbt.forEach(t),uwr=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U5e=n(y4,"CODE",{});var hbt=s(U5e);bwr=r(hbt,"pretrained_model_name_or_path"),hbt.forEach(t),vwr=r(y4,":"),y4.forEach(t),Twr=i(ka),Se=n(ka,"UL",{});var Oo=s(Se);pM=n(Oo,"LI",{});var G7e=s(pM);J5e=n(G7e,"STRONG",{});var pbt=s(J5e);Fwr=r(pbt,"albert"),pbt.forEach(t),Cwr=r(G7e," \u2014 "),wV=n(G7e,"A",{href:!0});var _bt=s(wV);Mwr=r(_bt,"FlaxAlbertForMultipleChoice"),_bt.forEach(t),Ewr=r(G7e," (ALBERT model)"),G7e.forEach(t),ywr=i(Oo),_M=n(Oo,"LI",{});var O7e=s(_M);Y5e=n(O7e,"STRONG",{});var ubt=s(Y5e);wwr=r(ubt,"bert"),ubt.forEach(t),Awr=r(O7e," \u2014 "),AV=n(O7e,"A",{href:!0});var bbt=s(AV);Lwr=r(bbt,"FlaxBertForMultipleChoice"),bbt.forEach(t),Bwr=r(O7e," (BERT model)"),O7e.forEach(t),xwr=i(Oo),uM=n(Oo,"LI",{});var X7e=s(uM);K5e=n(X7e,"STRONG",{});var vbt=s(K5e);kwr=r(vbt,"big_bird"),vbt.forEach(t),Rwr=r(X7e," \u2014 "),LV=n(X7e,"A",{href:!0});var Tbt=s(LV);Swr=r(Tbt,"FlaxBigBirdForMultipleChoice"),Tbt.forEach(t),Pwr=r(X7e," (BigBird model)"),X7e.forEach(t),$wr=i(Oo),bM=n(Oo,"LI",{});var V7e=s(bM);Z5e=n(V7e,"STRONG",{});var Fbt=s(Z5e);Iwr=r(Fbt,"distilbert"),Fbt.forEach(t),Dwr=r(V7e," \u2014 "),BV=n(V7e,"A",{href:!0});var Cbt=s(BV);jwr=r(Cbt,"FlaxDistilBertForMultipleChoice"),Cbt.forEach(t),Nwr=r(V7e," (DistilBERT model)"),V7e.forEach(t),qwr=i(Oo),vM=n(Oo,"LI",{});var z7e=s(vM);e2e=n(z7e,"STRONG",{});var Mbt=s(e2e);Gwr=r(Mbt,"electra"),Mbt.forEach(t),Owr=r(z7e," \u2014 "),xV=n(z7e,"A",{href:!0});var Ebt=s(xV);Xwr=r(Ebt,"FlaxElectraForMultipleChoice"),Ebt.forEach(t),Vwr=r(z7e," (ELECTRA model)"),z7e.forEach(t),zwr=i(Oo),TM=n(Oo,"LI",{});var W7e=s(TM);o2e=n(W7e,"STRONG",{});var ybt=s(o2e);Wwr=r(ybt,"roberta"),ybt.forEach(t),Qwr=r(W7e," \u2014 "),kV=n(W7e,"A",{href:!0});var wbt=s(kV);Hwr=r(wbt,"FlaxRobertaForMultipleChoice"),wbt.forEach(t),Uwr=r(W7e," (RoBERTa model)"),W7e.forEach(t),Jwr=i(Oo),FM=n(Oo,"LI",{});var Q7e=s(FM);r2e=n(Q7e,"STRONG",{});var Abt=s(r2e);Ywr=r(Abt,"roformer"),Abt.forEach(t),Kwr=r(Q7e," \u2014 "),RV=n(Q7e,"A",{href:!0});var Lbt=s(RV);Zwr=r(Lbt,"FlaxRoFormerForMultipleChoice"),Lbt.forEach(t),e6r=r(Q7e," (RoFormer model)"),Q7e.forEach(t),o6r=i(Oo),CM=n(Oo,"LI",{});var H7e=s(CM);t2e=n(H7e,"STRONG",{});var Bbt=s(t2e);r6r=r(Bbt,"xlm-roberta"),Bbt.forEach(t),t6r=r(H7e," \u2014 "),a2e=n(H7e,"CODE",{});var xbt=s(a2e);a6r=r(xbt,"FlaxXLMRobertaForMultipleChoice"),xbt.forEach(t),n6r=r(H7e,"(XLM-RoBERTa model)"),H7e.forEach(t),Oo.forEach(t),s6r=i(ka),n2e=n(ka,"P",{});var kbt=s(n2e);l6r=r(kbt,"Examples:"),kbt.forEach(t),i6r=i(ka),m(UA.$$.fragment,ka),ka.forEach(t),vi.forEach(t),Wxe=i(c),hf=n(c,"H2",{class:!0});var rSe=s(hf);MM=n(rSe,"A",{id:!0,class:!0,href:!0});var Rbt=s(MM);s2e=n(Rbt,"SPAN",{});var Sbt=s(s2e);m(JA.$$.fragment,Sbt),Sbt.forEach(t),Rbt.forEach(t),d6r=i(rSe),l2e=n(rSe,"SPAN",{});var Pbt=s(l2e);c6r=r(Pbt,"FlaxAutoModelForNextSentencePrediction"),Pbt.forEach(t),rSe.forEach(t),Qxe=i(c),Nr=n(c,"DIV",{class:!0});var Fi=s(Nr);m(YA.$$.fragment,Fi),f6r=i(Fi),pf=n(Fi,"P",{});var FW=s(pf);m6r=r(FW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),i2e=n(FW,"CODE",{});var $bt=s(i2e);g6r=r($bt,"from_pretrained()"),$bt.forEach(t),h6r=r(FW,"class method or the "),d2e=n(FW,"CODE",{});var Ibt=s(d2e);p6r=r(Ibt,"from_config()"),Ibt.forEach(t),_6r=r(FW,`class
method.`),FW.forEach(t),u6r=i(Fi),KA=n(Fi,"P",{});var tSe=s(KA);b6r=r(tSe,"This class cannot be instantiated directly using "),c2e=n(tSe,"CODE",{});var Dbt=s(c2e);v6r=r(Dbt,"__init__()"),Dbt.forEach(t),T6r=r(tSe," (throws an error)."),tSe.forEach(t),F6r=i(Fi),Rt=n(Fi,"DIV",{class:!0});var Ci=s(Rt);m(ZA.$$.fragment,Ci),C6r=i(Ci),f2e=n(Ci,"P",{});var jbt=s(f2e);M6r=r(jbt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),jbt.forEach(t),E6r=i(Ci),_f=n(Ci,"P",{});var CW=s(_f);y6r=r(CW,`Note:
Loading a model from its configuration file does `),m2e=n(CW,"STRONG",{});var Nbt=s(m2e);w6r=r(Nbt,"not"),Nbt.forEach(t),A6r=r(CW,` load the model weights. It only affects the
model\u2019s configuration. Use `),g2e=n(CW,"CODE",{});var qbt=s(g2e);L6r=r(qbt,"from_pretrained()"),qbt.forEach(t),B6r=r(CW,"to load the model weights."),CW.forEach(t),x6r=i(Ci),h2e=n(Ci,"P",{});var Gbt=s(h2e);k6r=r(Gbt,"Examples:"),Gbt.forEach(t),R6r=i(Ci),m(eL.$$.fragment,Ci),Ci.forEach(t),S6r=i(Fi),$o=n(Fi,"DIV",{class:!0});var Ra=s($o);m(oL.$$.fragment,Ra),P6r=i(Ra),p2e=n(Ra,"P",{});var Obt=s(p2e);$6r=r(Obt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Obt.forEach(t),I6r=i(Ra),Sn=n(Ra,"P",{});var w4=s(Sn);D6r=r(w4,"The model class to instantiate is selected based on the "),_2e=n(w4,"CODE",{});var Xbt=s(_2e);j6r=r(Xbt,"model_type"),Xbt.forEach(t),N6r=r(w4,` property of the config object (either
passed as an argument or loaded from `),u2e=n(w4,"CODE",{});var Vbt=s(u2e);q6r=r(Vbt,"pretrained_model_name_or_path"),Vbt.forEach(t),G6r=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),b2e=n(w4,"CODE",{});var zbt=s(b2e);O6r=r(zbt,"pretrained_model_name_or_path"),zbt.forEach(t),X6r=r(w4,":"),w4.forEach(t),V6r=i(Ra),v2e=n(Ra,"UL",{});var Wbt=s(v2e);EM=n(Wbt,"LI",{});var U7e=s(EM);T2e=n(U7e,"STRONG",{});var Qbt=s(T2e);z6r=r(Qbt,"bert"),Qbt.forEach(t),W6r=r(U7e," \u2014 "),SV=n(U7e,"A",{href:!0});var Hbt=s(SV);Q6r=r(Hbt,"FlaxBertForNextSentencePrediction"),Hbt.forEach(t),H6r=r(U7e," (BERT model)"),U7e.forEach(t),Wbt.forEach(t),U6r=i(Ra),F2e=n(Ra,"P",{});var Ubt=s(F2e);J6r=r(Ubt,"Examples:"),Ubt.forEach(t),Y6r=i(Ra),m(rL.$$.fragment,Ra),Ra.forEach(t),Fi.forEach(t),Hxe=i(c),uf=n(c,"H2",{class:!0});var aSe=s(uf);yM=n(aSe,"A",{id:!0,class:!0,href:!0});var Jbt=s(yM);C2e=n(Jbt,"SPAN",{});var Ybt=s(C2e);m(tL.$$.fragment,Ybt),Ybt.forEach(t),Jbt.forEach(t),K6r=i(aSe),M2e=n(aSe,"SPAN",{});var Kbt=s(M2e);Z6r=r(Kbt,"FlaxAutoModelForImageClassification"),Kbt.forEach(t),aSe.forEach(t),Uxe=i(c),qr=n(c,"DIV",{class:!0});var Mi=s(qr);m(aL.$$.fragment,Mi),eAr=i(Mi),bf=n(Mi,"P",{});var MW=s(bf);oAr=r(MW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),E2e=n(MW,"CODE",{});var Zbt=s(E2e);rAr=r(Zbt,"from_pretrained()"),Zbt.forEach(t),tAr=r(MW,"class method or the "),y2e=n(MW,"CODE",{});var e5t=s(y2e);aAr=r(e5t,"from_config()"),e5t.forEach(t),nAr=r(MW,`class
method.`),MW.forEach(t),sAr=i(Mi),nL=n(Mi,"P",{});var nSe=s(nL);lAr=r(nSe,"This class cannot be instantiated directly using "),w2e=n(nSe,"CODE",{});var o5t=s(w2e);iAr=r(o5t,"__init__()"),o5t.forEach(t),dAr=r(nSe," (throws an error)."),nSe.forEach(t),cAr=i(Mi),St=n(Mi,"DIV",{class:!0});var Ei=s(St);m(sL.$$.fragment,Ei),fAr=i(Ei),A2e=n(Ei,"P",{});var r5t=s(A2e);mAr=r(r5t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),r5t.forEach(t),gAr=i(Ei),vf=n(Ei,"P",{});var EW=s(vf);hAr=r(EW,`Note:
Loading a model from its configuration file does `),L2e=n(EW,"STRONG",{});var t5t=s(L2e);pAr=r(t5t,"not"),t5t.forEach(t),_Ar=r(EW,` load the model weights. It only affects the
model\u2019s configuration. Use `),B2e=n(EW,"CODE",{});var a5t=s(B2e);uAr=r(a5t,"from_pretrained()"),a5t.forEach(t),bAr=r(EW,"to load the model weights."),EW.forEach(t),vAr=i(Ei),x2e=n(Ei,"P",{});var n5t=s(x2e);TAr=r(n5t,"Examples:"),n5t.forEach(t),FAr=i(Ei),m(lL.$$.fragment,Ei),Ei.forEach(t),CAr=i(Mi),Io=n(Mi,"DIV",{class:!0});var Sa=s(Io);m(iL.$$.fragment,Sa),MAr=i(Sa),k2e=n(Sa,"P",{});var s5t=s(k2e);EAr=r(s5t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),s5t.forEach(t),yAr=i(Sa),Pn=n(Sa,"P",{});var A4=s(Pn);wAr=r(A4,"The model class to instantiate is selected based on the "),R2e=n(A4,"CODE",{});var l5t=s(R2e);AAr=r(l5t,"model_type"),l5t.forEach(t),LAr=r(A4,` property of the config object (either
passed as an argument or loaded from `),S2e=n(A4,"CODE",{});var i5t=s(S2e);BAr=r(i5t,"pretrained_model_name_or_path"),i5t.forEach(t),xAr=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P2e=n(A4,"CODE",{});var d5t=s(P2e);kAr=r(d5t,"pretrained_model_name_or_path"),d5t.forEach(t),RAr=r(A4,":"),A4.forEach(t),SAr=i(Sa),dL=n(Sa,"UL",{});var sSe=s(dL);wM=n(sSe,"LI",{});var J7e=s(wM);$2e=n(J7e,"STRONG",{});var c5t=s($2e);PAr=r(c5t,"beit"),c5t.forEach(t),$Ar=r(J7e," \u2014 "),PV=n(J7e,"A",{href:!0});var f5t=s(PV);IAr=r(f5t,"FlaxBeitForImageClassification"),f5t.forEach(t),DAr=r(J7e," (BEiT model)"),J7e.forEach(t),jAr=i(sSe),AM=n(sSe,"LI",{});var Y7e=s(AM);I2e=n(Y7e,"STRONG",{});var m5t=s(I2e);NAr=r(m5t,"vit"),m5t.forEach(t),qAr=r(Y7e," \u2014 "),$V=n(Y7e,"A",{href:!0});var g5t=s($V);GAr=r(g5t,"FlaxViTForImageClassification"),g5t.forEach(t),OAr=r(Y7e," (ViT model)"),Y7e.forEach(t),sSe.forEach(t),XAr=i(Sa),D2e=n(Sa,"P",{});var h5t=s(D2e);VAr=r(h5t,"Examples:"),h5t.forEach(t),zAr=i(Sa),m(cL.$$.fragment,Sa),Sa.forEach(t),Mi.forEach(t),Jxe=i(c),Tf=n(c,"H2",{class:!0});var lSe=s(Tf);LM=n(lSe,"A",{id:!0,class:!0,href:!0});var p5t=s(LM);j2e=n(p5t,"SPAN",{});var _5t=s(j2e);m(fL.$$.fragment,_5t),_5t.forEach(t),p5t.forEach(t),WAr=i(lSe),N2e=n(lSe,"SPAN",{});var u5t=s(N2e);QAr=r(u5t,"FlaxAutoModelForVision2Seq"),u5t.forEach(t),lSe.forEach(t),Yxe=i(c),Gr=n(c,"DIV",{class:!0});var yi=s(Gr);m(mL.$$.fragment,yi),HAr=i(yi),Ff=n(yi,"P",{});var yW=s(Ff);UAr=r(yW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),q2e=n(yW,"CODE",{});var b5t=s(q2e);JAr=r(b5t,"from_pretrained()"),b5t.forEach(t),YAr=r(yW,"class method or the "),G2e=n(yW,"CODE",{});var v5t=s(G2e);KAr=r(v5t,"from_config()"),v5t.forEach(t),ZAr=r(yW,`class
method.`),yW.forEach(t),eLr=i(yi),gL=n(yi,"P",{});var iSe=s(gL);oLr=r(iSe,"This class cannot be instantiated directly using "),O2e=n(iSe,"CODE",{});var T5t=s(O2e);rLr=r(T5t,"__init__()"),T5t.forEach(t),tLr=r(iSe," (throws an error)."),iSe.forEach(t),aLr=i(yi),Pt=n(yi,"DIV",{class:!0});var wi=s(Pt);m(hL.$$.fragment,wi),nLr=i(wi),X2e=n(wi,"P",{});var F5t=s(X2e);sLr=r(F5t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),F5t.forEach(t),lLr=i(wi),Cf=n(wi,"P",{});var wW=s(Cf);iLr=r(wW,`Note:
Loading a model from its configuration file does `),V2e=n(wW,"STRONG",{});var C5t=s(V2e);dLr=r(C5t,"not"),C5t.forEach(t),cLr=r(wW,` load the model weights. It only affects the
model\u2019s configuration. Use `),z2e=n(wW,"CODE",{});var M5t=s(z2e);fLr=r(M5t,"from_pretrained()"),M5t.forEach(t),mLr=r(wW,"to load the model weights."),wW.forEach(t),gLr=i(wi),W2e=n(wi,"P",{});var E5t=s(W2e);hLr=r(E5t,"Examples:"),E5t.forEach(t),pLr=i(wi),m(pL.$$.fragment,wi),wi.forEach(t),_Lr=i(yi),Do=n(yi,"DIV",{class:!0});var Pa=s(Do);m(_L.$$.fragment,Pa),uLr=i(Pa),Q2e=n(Pa,"P",{});var y5t=s(Q2e);bLr=r(y5t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),y5t.forEach(t),vLr=i(Pa),$n=n(Pa,"P",{});var L4=s($n);TLr=r(L4,"The model class to instantiate is selected based on the "),H2e=n(L4,"CODE",{});var w5t=s(H2e);FLr=r(w5t,"model_type"),w5t.forEach(t),CLr=r(L4,` property of the config object (either
passed as an argument or loaded from `),U2e=n(L4,"CODE",{});var A5t=s(U2e);MLr=r(A5t,"pretrained_model_name_or_path"),A5t.forEach(t),ELr=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),J2e=n(L4,"CODE",{});var L5t=s(J2e);yLr=r(L5t,"pretrained_model_name_or_path"),L5t.forEach(t),wLr=r(L4,":"),L4.forEach(t),ALr=i(Pa),Y2e=n(Pa,"UL",{});var B5t=s(Y2e);BM=n(B5t,"LI",{});var K7e=s(BM);K2e=n(K7e,"STRONG",{});var x5t=s(K2e);LLr=r(x5t,"vision-encoder-decoder"),x5t.forEach(t),BLr=r(K7e," \u2014 "),IV=n(K7e,"A",{href:!0});var k5t=s(IV);xLr=r(k5t,"FlaxVisionEncoderDecoderModel"),k5t.forEach(t),kLr=r(K7e," (Vision Encoder decoder model)"),K7e.forEach(t),B5t.forEach(t),RLr=i(Pa),Z2e=n(Pa,"P",{});var R5t=s(Z2e);SLr=r(R5t,"Examples:"),R5t.forEach(t),PLr=i(Pa),m(uL.$$.fragment,Pa),Pa.forEach(t),yi.forEach(t),this.h()},h(){d(J,"name","hf:doc:metadata"),d(J,"content",JSON.stringify(G5t)),d(ge,"id","auto-classes"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#auto-classes"),d(ie,"class","relative group"),d(In,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoConfig"),d(jn,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoModel"),d(Nn,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoTokenizer"),d(Pi,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertModel"),d(Lf,"id","extending-the-auto-classes"),d(Lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lf,"href","#extending-the-auto-classes"),d($i,"class","relative group"),d(xf,"id","transformers.AutoConfig"),d(xf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xf,"href","#transformers.AutoConfig"),d(Ii,"class","relative group"),d(T8,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),d(F8,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig"),d(C8,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig"),d(M8,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig"),d(E8,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig"),d(y8,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationConfig"),d(w8,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig"),d(A8,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),d(L8,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig"),d(B8,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),d(x8,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig"),d(k8,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig"),d(R8,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig"),d(S8,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig"),d(P8,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig"),d($8,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig"),d(I8,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig"),d(D8,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig"),d(j8,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig"),d(N8,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config"),d(q8,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig"),d(G8,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig"),d(O8,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig"),d(X8,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRConfig"),d(V8,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig"),d(z8,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),d(W8,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig"),d(Q8,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig"),d(H8,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig"),d(U8,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig"),d(J8,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config"),d(Y8,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),d(K8,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig"),d(Z8,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig"),d(e7,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig"),d(o7,"href","/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTConfig"),d(r7,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig"),d(t7,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),d(a7,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig"),d(n7,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig"),d(s7,"href","/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeConfig"),d(l7,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig"),d(i7,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Config"),d(d7,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig"),d(c7,"href","/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerConfig"),d(f7,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig"),d(m7,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),d(g7,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig"),d(h7,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig"),d(p7,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config"),d(_7,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig"),d(u7,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),d(b7,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig"),d(v7,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig"),d(T7,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig"),d(F7,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerConfig"),d(C7,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig"),d(M7,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig"),d(E7,"href","/docs/transformers/pr_15900/en/model_doc/rag#transformers.RagConfig"),d(y7,"href","/docs/transformers/pr_15900/en/model_doc/realm#transformers.RealmConfig"),d(w7,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig"),d(A7,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig"),d(L7,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertConfig"),d(B7,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig"),d(x7,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig"),d(k7,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig"),d(R7,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig"),d(S7,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig"),d(P7,"href","/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),d($7,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),d(I7,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),d(D7,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterConfig"),d(j7,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),d(N7,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig"),d(q7,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config"),d(G7,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig"),d(O7,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),d(X7,"href","/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRConfig"),d(V7,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig"),d(z7,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),d(W7,"href","/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltConfig"),d(Q7,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),d(H7,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),d(U7,"href","/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertConfig"),d(J7,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig"),d(Y7,"href","/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEConfig"),d(K7,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),d(Z7,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig"),d(eB,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig"),d(oB,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig"),d(rB,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),d(tB,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),d(aB,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),d(nB,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig"),d(sB,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig"),d(fo,"class","docstring"),d(ug,"class","docstring"),d(zo,"class","docstring"),d(bg,"id","transformers.AutoTokenizer"),d(bg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bg,"href","#transformers.AutoTokenizer"),d(ji,"class","relative group"),d(lB,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(iB,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertTokenizer"),d(dB,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertTokenizerFast"),d(cB,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartTokenizer"),d(fB,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartTokenizerFast"),d(mB,"href","/docs/transformers/pr_15900/en/model_doc/barthez#transformers.BarthezTokenizer"),d(gB,"href","/docs/transformers/pr_15900/en/model_doc/barthez#transformers.BarthezTokenizerFast"),d(hB,"href","/docs/transformers/pr_15900/en/model_doc/bartpho#transformers.BartphoTokenizer"),d(pB,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizer"),d(_B,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizerFast"),d(uB,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),d(bB,"href","/docs/transformers/pr_15900/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),d(vB,"href","/docs/transformers/pr_15900/en/model_doc/bertweet#transformers.BertweetTokenizer"),d(TB,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdTokenizer"),d(FB,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),d(CB,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(MB,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(EB,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),d(yB,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),d(wB,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),d(AB,"href","/docs/transformers/pr_15900/en/model_doc/byt5#transformers.ByT5Tokenizer"),d(LB,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertTokenizer"),d(BB,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertTokenizerFast"),d(xB,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineTokenizer"),d(kB,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPTokenizer"),d(RB,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(SB,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertTokenizer"),d(PB,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),d($B,"href","/docs/transformers/pr_15900/en/model_doc/cpm#transformers.CpmTokenizer"),d(IB,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLTokenizer"),d(DB,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaTokenizer"),d(jB,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaTokenizerFast"),d(NB,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),d(qB,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertTokenizer"),d(GB,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),d(OB,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(XB,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(VB,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraTokenizer"),d(zB,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraTokenizerFast"),d(WB,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertTokenizer"),d(QB,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetTokenizer"),d(HB,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetTokenizerFast"),d(UB,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTTokenizer"),d(JB,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelTokenizer"),d(YB,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelTokenizerFast"),d(KB,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(ZB,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(ex,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(ox,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(rx,"href","/docs/transformers/pr_15900/en/model_doc/herbert#transformers.HerbertTokenizer"),d(tx,"href","/docs/transformers/pr_15900/en/model_doc/herbert#transformers.HerbertTokenizerFast"),d(ax,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(nx,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizer"),d(sx,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(lx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),d(ix,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),d(dx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),d(cx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),d(fx,"href","/docs/transformers/pr_15900/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),d(mx,"href","/docs/transformers/pr_15900/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),d(gx,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDTokenizer"),d(hx,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDTokenizerFast"),d(px,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerTokenizer"),d(_x,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerTokenizerFast"),d(ux,"href","/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeTokenizer"),d(bx,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertTokenizer"),d(vx,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),d(Tx,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),d(Fx,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianTokenizer"),d(Cx,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartTokenizer"),d(Mx,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartTokenizerFast"),d(Ex,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBart50Tokenizer"),d(yx,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBart50TokenizerFast"),d(wx,"href","/docs/transformers/pr_15900/en/model_doc/mluke#transformers.MLukeTokenizer"),d(Ax,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),d(Lx,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),d(Bx,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetTokenizer"),d(xx,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),d(kx,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5Tokenizer"),d(Rx,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5TokenizerFast"),d(Sx,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),d(Px,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),d($x,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(Ix,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(Dx,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverTokenizer"),d(jx,"href","/docs/transformers/pr_15900/en/model_doc/phobert#transformers.PhobertTokenizer"),d(Nx,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartTokenizer"),d(qx,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),d(Gx,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizer"),d(Ox,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizerFast"),d(Xx,"href","/docs/transformers/pr_15900/en/model_doc/rag#transformers.RagTokenizer"),d(Vx,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerTokenizer"),d(zx,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerTokenizerFast"),d(Wx,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertTokenizer"),d(Qx,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertTokenizerFast"),d(Hx,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertTokenizer"),d(Ux,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),d(Jx,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizer"),d(Yx,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(Kx,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerTokenizer"),d(Zx,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),d(ek,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),d(ok,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),d(rk,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterTokenizer"),d(tk,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterTokenizerFast"),d(ak,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),d(nk,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),d(sk,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5Tokenizer"),d(lk,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5TokenizerFast"),d(ik,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasTokenizer"),d(dk,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),d(ck,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(fk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),d(mk,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMTokenizer"),d(gk,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMTokenizerFast"),d(hk,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMTokenizer"),d(pk,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),d(_k,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),d(uk,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),d(bk,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetTokenizer"),d(vk,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),d(mo,"class","docstring"),d(Hg,"class","docstring"),d(Wo,"class","docstring"),d(Ug,"id","transformers.AutoFeatureExtractor"),d(Ug,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ug,"href","#transformers.AutoFeatureExtractor"),d(Ni,"class","relative group"),d(Tk,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(Fk,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitFeatureExtractor"),d(Ck,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPFeatureExtractor"),d(Mk,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(Ek,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTFeatureExtractor"),d(yk,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrFeatureExtractor"),d(wk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(Ak,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),d(Lk,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),d(Bk,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),d(xk,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),d(kk,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),d(Rk,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Sk,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Pk,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTFeatureExtractor"),d($k,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d($e,"class","docstring"),d(mh,"class","docstring"),d(Qo,"class","docstring"),d(gh,"id","transformers.AutoProcessor"),d(gh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(gh,"href","#transformers.AutoProcessor"),d(qi,"class","relative group"),d(Ik,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(Dk,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPProcessor"),d(jk,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),d(Nk,"href","/docs/transformers/pr_15900/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),d(qk,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),d(Gk,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),d(Ok,"href","/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRProcessor"),d(Xk,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),d(Vk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),d(Ie,"class","docstring"),d(Mh,"class","docstring"),d(Ho,"class","docstring"),d(Eh,"id","transformers.AutoModel"),d(Eh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Eh,"href","#transformers.AutoModel"),d(Oi,"class","relative group"),d(Or,"class","docstring"),d(zk,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertModel"),d(Wk,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartModel"),d(Qk,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitModel"),d(Hk,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertModel"),d(Uk,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),d(Jk,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdModel"),d(Yk,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),d(Kk,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotModel"),d(Zk,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),d(eR,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertModel"),d(oR,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineModel"),d(rR,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPModel"),d(tR,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertModel"),d(aR,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextModel"),d(nR,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLModel"),d(sR,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioModel"),d(lR,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextModel"),d(iR,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaModel"),d(dR,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Model"),d(cR,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTModel"),d(fR,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrModel"),d(mR,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertModel"),d(gR,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(hR,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraModel"),d(pR,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertModel"),d(_R,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetModel"),d(uR,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTModel"),d(bR,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelModel"),d(vR,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelBaseModel"),d(TR,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Model"),d(FR,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoModel"),d(CR,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJModel"),d(MR,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertModel"),d(ER,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertModel"),d(yR,"href","/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTModel"),d(wR,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMModel"),d(AR,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),d(LR,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDModel"),d(BR,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerModel"),d(xR,"href","/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeModel"),d(kR,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertModel"),d(RR,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Model"),d(SR,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianModel"),d(PR,"href","/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerModel"),d($R,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartModel"),d(IR,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertModel"),d(DR,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertModel"),d(jR,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetModel"),d(NR,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Model"),d(qR,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerModel"),d(GR,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),d(OR,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusModel"),d(XR,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverModel"),d(VR,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartModel"),d(zR,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerModel"),d(WR,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetModel"),d(QR,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertModel"),d(HR,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModel"),d(UR,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertModel"),d(JR,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel"),d(YR,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaModel"),d(KR,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerModel"),d(ZR,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerModel"),d(eS,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWModel"),d(oS,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDModel"),d(rS,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextModel"),d(tS,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterModel"),d(aS,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertModel"),d(nS,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinModel"),d(sS,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Model"),d(lS,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasModel"),d(iS,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLModel"),d(dS,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechModel"),d(cS,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),d(fS,"href","/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltModel"),d(mS,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),d(gS,"href","/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertModel"),d(hS,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTModel"),d(pS,"href","/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEModel"),d(_S,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),d(uS,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMModel"),d(bS,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMModel"),d(vS,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMModel"),d(TS,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),d(FS,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),d(CS,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),d(MS,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetModel"),d(ES,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoModel"),d(De,"class","docstring"),d(Uo,"class","docstring"),d(a_,"id","transformers.AutoModelForPreTraining"),d(a_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(a_,"href","#transformers.AutoModelForPreTraining"),d(zi,"class","relative group"),d(Xr,"class","docstring"),d(yS,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForPreTraining"),d(wS,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(AS,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForPreTraining"),d(LS,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),d(BS,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(xS,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(kS,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(RS,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(SS,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(PS,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d($S,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForPreTraining"),d(IS,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(DS,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForPreTraining"),d(jS,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(NS,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForPreTraining"),d(qS,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(GS,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(OS,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(XS,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(VS,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForPreTraining"),d(zS,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),d(WS,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),d(QS,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(HS,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(US,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel"),d(JS,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(YS,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(KS,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(ZS,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(eP,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(oP,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),d(rP,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),d(tP,"href","/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),d(aP,"href","/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),d(nP,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),d(sP,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(lP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(iP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(dP,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(je,"class","docstring"),d(Jo,"class","docstring"),d(z_,"id","transformers.AutoModelForCausalLM"),d(z_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(z_,"href","#transformers.AutoModelForCausalLM"),d(Hi,"class","relative group"),d(Vr,"class","docstring"),d(cP,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForCausalLM"),d(fP,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertLMHeadModel"),d(mP,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),d(gP,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),d(hP,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),d(pP,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),d(_P,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),d(uP,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForCausalLM"),d(bP,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(vP,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM"),d(TP,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForCausalLM"),d(FP,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(CP,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),d(MP,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForCausalLM"),d(EP,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianForCausalLM"),d(yP,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForCausalLM"),d(wP,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),d(AP,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(LP,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForCausalLM"),d(BP,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForCausalLM"),d(xP,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),d(kP,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),d(RP,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),d(SP,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForCausalLM"),d(PP,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForCausalLM"),d($P,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForCausalLM"),d(IP,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),d(DP,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(jP,"href","/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRForCausalLM"),d(NP,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMForCausalLM"),d(qP,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(GP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),d(OP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),d(XP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),d(VP,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Ne,"class","docstring"),d(Yo,"class","docstring"),d(Lu,"id","transformers.AutoModelForMaskedLM"),d(Lu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lu,"href","#transformers.AutoModelForMaskedLM"),d(Yi,"class","relative group"),d(zr,"class","docstring"),d(zP,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMaskedLM"),d(WP,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(QP,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMaskedLM"),d(HP,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),d(UP,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(JP,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),d(YP,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(KP,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(ZP,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(e$,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(o$,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMaskedLM"),d(r$,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(t$,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMaskedLM"),d(a$,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMaskedLM"),d(n$,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(s$,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(l$,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(i$,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(d$,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),d(c$,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),d(f$,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(m$,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),d(g$,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(h$,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),d(p$,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForMaskedLM"),d(_$,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMaskedLM"),d(u$,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(b$,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),d(v$,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(T$,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(F$,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(C$,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(M$,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(E$,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMaskedLM"),d(qe,"class","docstring"),d(Ko,"class","docstring"),d(f0,"id","transformers.AutoModelForSeq2SeqLM"),d(f0,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(f0,"href","#transformers.AutoModelForSeq2SeqLM"),d(ed,"class","relative group"),d(Wr,"class","docstring"),d(y$,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(w$,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),d(A$,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),d(L$,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),d(B$,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),d(x$,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(k$,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForConditionalGeneration"),d(R$,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),d(S$,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianMTModel"),d(P$,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d($$,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),d(I$,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),d(D$,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),d(j$,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),d(N$,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(q$,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),d(Ge,"class","docstring"),d(Zo,"class","docstring"),d(B0,"id","transformers.AutoModelForSequenceClassification"),d(B0,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B0,"href","#transformers.AutoModelForSequenceClassification"),d(td,"class","relative group"),d(Qr,"class","docstring"),d(G$,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForSequenceClassification"),d(O$,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForSequenceClassification"),d(X$,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForSequenceClassification"),d(V$,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),d(z$,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),d(W$,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),d(Q$,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForSequenceClassification"),d(H$,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),d(U$,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),d(J$,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification"),d(Y$,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),d(K$,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),d(Z$,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),d(eI,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForSequenceClassification"),d(oI,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),d(rI,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForSequenceClassification"),d(tI,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),d(aI,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),d(nI,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),d(sI,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),d(lI,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForSequenceClassification"),d(iI,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),d(dI,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),d(cI,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForSequenceClassification"),d(fI,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),d(mI,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForSequenceClassification"),d(gI,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),d(hI,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),d(pI,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),d(_I,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),d(uI,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),d(bI,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(vI,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),d(TI,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),d(FI,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),d(CI,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),d(MI,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),d(EI,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),d(yI,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),d(wI,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForSequenceClassification"),d(AI,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),d(LI,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForSequenceClassification"),d(BI,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),d(xI,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),d(kI,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),d(RI,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForSequenceClassification"),d(Oe,"class","docstring"),d(er,"class","docstring"),d(M1,"id","transformers.AutoModelForMultipleChoice"),d(M1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(M1,"href","#transformers.AutoModelForMultipleChoice"),d(sd,"class","relative group"),d(Hr,"class","docstring"),d(SI,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMultipleChoice"),d(PI,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMultipleChoice"),d($I,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),d(II,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),d(DI,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForMultipleChoice"),d(jI,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),d(NI,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice"),d(qI,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),d(GI,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMultipleChoice"),d(OI,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),d(XI,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMultipleChoice"),d(VI,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),d(zI,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMultipleChoice"),d(WI,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),d(QI,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),d(HI,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),d(UI,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),d(JI,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),d(YI,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),d(KI,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),d(ZI,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),d(eD,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),d(oD,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),d(rD,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForMultipleChoice"),d(tD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),d(aD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),d(nD,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),d(sD,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMultipleChoice"),d(Xe,"class","docstring"),d(or,"class","docstring"),d(Z1,"id","transformers.AutoModelForNextSentencePrediction"),d(Z1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Z1,"href","#transformers.AutoModelForNextSentencePrediction"),d(dd,"class","relative group"),d(Ur,"class","docstring"),d(lD,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForNextSentencePrediction"),d(iD,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),d(dD,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),d(cD,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),d(fD,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),d(Ve,"class","docstring"),d(rr,"class","docstring"),d(sb,"id","transformers.AutoModelForTokenClassification"),d(sb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(sb,"href","#transformers.AutoModelForTokenClassification"),d(md,"class","relative group"),d(Jr,"class","docstring"),d(mD,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForTokenClassification"),d(gD,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForTokenClassification"),d(hD,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),d(pD,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForTokenClassification"),d(_D,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForTokenClassification"),d(uD,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),d(bD,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification"),d(vD,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForTokenClassification"),d(TD,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),d(FD,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),d(CD,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForTokenClassification"),d(MD,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),d(ED,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForTokenClassification"),d(yD,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForTokenClassification"),d(wD,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),d(AD,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForTokenClassification"),d(LD,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),d(BD,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),d(xD,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForTokenClassification"),d(kD,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),d(RD,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),d(SD,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),d(PD,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),d($D,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),d(ID,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForTokenClassification"),d(DD,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForTokenClassification"),d(jD,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),d(ND,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),d(qD,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForTokenClassification"),d(GD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),d(OD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),d(XD,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),d(VD,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForTokenClassification"),d(ze,"class","docstring"),d(tr,"class","docstring"),d(Gb,"id","transformers.AutoModelForQuestionAnswering"),d(Gb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Gb,"href","#transformers.AutoModelForQuestionAnswering"),d(pd,"class","relative group"),d(Yr,"class","docstring"),d(zD,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),d(WD,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForQuestionAnswering"),d(QD,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForQuestionAnswering"),d(HD,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),d(UD,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),d(JD,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),d(YD,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForQuestionAnswering"),d(KD,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),d(ZD,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering"),d(ej,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),d(oj,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),d(rj,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),d(tj,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),d(aj,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),d(nj,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),d(sj,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),d(lj,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),d(ij,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),d(dj,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),d(cj,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForQuestionAnswering"),d(fj,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),d(mj,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),d(gj,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),d(hj,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),d(pj,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),d(_j,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),d(uj,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),d(bj,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),d(vj,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),d(Tj,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),d(Fj,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),d(Cj,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),d(Mj,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),d(Ej,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),d(yj,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),d(wj,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),d(Aj,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),d(Lj,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),d(Bj,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),d(We,"class","docstring"),d(ar,"class","docstring"),d(L5,"id","transformers.AutoModelForTableQuestionAnswering"),d(L5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(L5,"href","#transformers.AutoModelForTableQuestionAnswering"),d(bd,"class","relative group"),d(Kr,"class","docstring"),d(xj,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),d(Qe,"class","docstring"),d(nr,"class","docstring"),d(k5,"id","transformers.AutoModelForImageClassification"),d(k5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(k5,"href","#transformers.AutoModelForImageClassification"),d(Fd,"class","relative group"),d(Zr,"class","docstring"),d(kj,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForImageClassification"),d(Rj,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextForImageClassification"),d(Sj,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassification"),d(Pj,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),d($j,"href","/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),d(Ij,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(Dj,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d(jj,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(Nj,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),d(qj,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForImageClassification"),d(Gj,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForImageClassification"),d(Oj,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForImageClassification"),d(He,"class","docstring"),d(sr,"class","docstring"),d(q5,"id","transformers.AutoModelForVision2Seq"),d(q5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(q5,"href","#transformers.AutoModelForVision2Seq"),d(Ed,"class","relative group"),d(et,"class","docstring"),d(Xj,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),d(Ue,"class","docstring"),d(lr,"class","docstring"),d(X5,"id","transformers.AutoModelForAudioClassification"),d(X5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(X5,"href","#transformers.AutoModelForAudioClassification"),d(Ad,"class","relative group"),d(ot,"class","docstring"),d(Vj,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification"),d(zj,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForSequenceClassification"),d(Wj,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForSequenceClassification"),d(Qj,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),d(Hj,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),d(Uj,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),d(Jj,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),d(Yj,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),d(Je,"class","docstring"),d(ir,"class","docstring"),d(Z5,"id","transformers.AutoModelForAudioFrameClassification"),d(Z5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Z5,"href","#transformers.AutoModelForAudioFrameClassification"),d(xd,"class","relative group"),d(rt,"class","docstring"),d(Kj,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification"),d(Zj,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),d(eN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),d(oN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),d(Ye,"class","docstring"),d(dr,"class","docstring"),d(n2,"id","transformers.AutoModelForCTC"),d(n2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(n2,"href","#transformers.AutoModelForCTC"),d(Sd,"class","relative group"),d(tt,"class","docstring"),d(rN,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForCTC"),d(tN,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForCTC"),d(aN,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForCTC"),d(nN,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForCTC"),d(sN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForCTC"),d(lN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),d(iN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),d(dN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForCTC"),d(Ke,"class","docstring"),d(cr,"class","docstring"),d(p2,"id","transformers.AutoModelForSpeechSeq2Seq"),d(p2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p2,"href","#transformers.AutoModelForSpeechSeq2Seq"),d(Id,"class","relative group"),d(at,"class","docstring"),d(cN,"href","/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),d(fN,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),d(Ze,"class","docstring"),d(fr,"class","docstring"),d(v2,"id","transformers.AutoModelForAudioXVector"),d(v2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(v2,"href","#transformers.AutoModelForAudioXVector"),d(Nd,"class","relative group"),d(nt,"class","docstring"),d(mN,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForXVector"),d(gN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),d(hN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),d(pN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForXVector"),d(eo,"class","docstring"),d(mr,"class","docstring"),d(y2,"id","transformers.AutoModelForMaskedImageModeling"),d(y2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(y2,"href","#transformers.AutoModelForMaskedImageModeling"),d(Od,"class","relative group"),d(st,"class","docstring"),d(_N,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),d(uN,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),d(bN,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),d(oo,"class","docstring"),d(gr,"class","docstring"),d(x2,"id","transformers.AutoModelForObjectDetection"),d(x2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(x2,"href","#transformers.AutoModelForObjectDetection"),d(Wd,"class","relative group"),d(lt,"class","docstring"),d(vN,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForObjectDetection"),d(ro,"class","docstring"),d(hr,"class","docstring"),d(S2,"id","transformers.AutoModelForImageSegmentation"),d(S2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(S2,"href","#transformers.AutoModelForImageSegmentation"),d(Ud,"class","relative group"),d(it,"class","docstring"),d(TN,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForSegmentation"),d(to,"class","docstring"),d(pr,"class","docstring"),d(I2,"id","transformers.AutoModelForSemanticSegmentation"),d(I2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(I2,"href","#transformers.AutoModelForSemanticSegmentation"),d(Kd,"class","relative group"),d(dt,"class","docstring"),d(FN,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),d(CN,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),d(ao,"class","docstring"),d(_r,"class","docstring"),d(q2,"id","transformers.TFAutoModel"),d(q2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(q2,"href","#transformers.TFAutoModel"),d(oc,"class","relative group"),d(ct,"class","docstring"),d(MN,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertModel"),d(EN,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartModel"),d(yN,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertModel"),d(wN,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),d(AN,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),d(LN,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertModel"),d(BN,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.TFCLIPModel"),d(xN,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertModel"),d(kN,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextModel"),d(RN,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLModel"),d(SN,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaModel"),d(PN,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),d($N,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertModel"),d(IN,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(DN,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraModel"),d(jN,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertModel"),d(NN,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelModel"),d(qN,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelBaseModel"),d(GN,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2Model"),d(ON,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.TFHubertModel"),d(XN,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),d(VN,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDModel"),d(zN,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerModel"),d(WN,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertModel"),d(QN,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianModel"),d(HN,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartModel"),d(UN,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertModel"),d(JN,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetModel"),d(YN,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5Model"),d(KN,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),d(ZN,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusModel"),d(eq,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertModel"),d(oq,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaModel"),d(rq,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerModel"),d(tq,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),d(aq,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5Model"),d(nq,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasModel"),d(sq,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),d(lq,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTModel"),d(iq,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),d(dq,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMModel"),d(cq,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),d(fq,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetModel"),d(go,"class","docstring"),d(ur,"class","docstring"),d(Lv,"id","transformers.TFAutoModelForPreTraining"),d(Lv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lv,"href","#transformers.TFAutoModelForPreTraining"),d(ac,"class","relative group"),d(ft,"class","docstring"),d(mq,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForPreTraining"),d(gq,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(hq,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForPreTraining"),d(pq,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(_q,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(uq,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(bq,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForPreTraining"),d(vq,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(Tq,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),d(Fq,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(Cq,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(Mq,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),d(Eq,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),d(yq,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(wq,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(Aq,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(Lq,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(Bq,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(xq,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(kq,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Rq,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(Sq,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(ho,"class","docstring"),d(br,"class","docstring"),d(Yv,"id","transformers.TFAutoModelForCausalLM"),d(Yv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Yv,"href","#transformers.TFAutoModelForCausalLM"),d(lc,"class","relative group"),d(mt,"class","docstring"),d(Pq,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertLMHeadModel"),d($q,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(Iq,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(Dq,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(jq,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),d(Nq,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),d(qq,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),d(Gq,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(Oq,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Xq,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(po,"class","docstring"),d(vr,"class","docstring"),d(iT,"id","transformers.TFAutoModelForImageClassification"),d(iT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(iT,"href","#transformers.TFAutoModelForImageClassification"),d(cc,"class","relative group"),d(gt,"class","docstring"),d(Vq,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),d(zq,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTForImageClassification"),d(_o,"class","docstring"),d(Tr,"class","docstring"),d(fT,"id","transformers.TFAutoModelForMaskedLM"),d(fT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fT,"href","#transformers.TFAutoModelForMaskedLM"),d(gc,"class","relative group"),d(ht,"class","docstring"),d(Wq,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),d(Qq,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMaskedLM"),d(Hq,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(Uq,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),d(Jq,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),d(Yq,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),d(Kq,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(Zq,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMaskedLM"),d(eG,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(oG,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),d(rG,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(tG,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),d(aG,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),d(nG,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(sG,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),d(lG,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(iG,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),d(dG,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(cG,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(fG,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(uo,"class","docstring"),d(Fr,"class","docstring"),d(RT,"id","transformers.TFAutoModelForSeq2SeqLM"),d(RT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(RT,"href","#transformers.TFAutoModelForSeq2SeqLM"),d(_c,"class","relative group"),d(pt,"class","docstring"),d(mG,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(gG,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),d(hG,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),d(pG,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),d(_G,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),d(uG,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianMTModel"),d(bG,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),d(vG,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),d(TG,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),d(FG,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(bo,"class","docstring"),d(Cr,"class","docstring"),d(XT,"id","transformers.TFAutoModelForSequenceClassification"),d(XT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(XT,"href","#transformers.TFAutoModelForSequenceClassification"),d(vc,"class","relative group"),d(_t,"class","docstring"),d(CG,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),d(MG,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForSequenceClassification"),d(EG,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),d(yG,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),d(wG,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),d(AG,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),d(LG,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),d(BG,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),d(xG,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),d(kG,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),d(RG,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),d(SG,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),d(PG,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),d($G,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),d(IG,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),d(DG,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),d(jG,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),d(NG,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),d(qG,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),d(GG,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),d(OG,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),d(XG,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),d(VG,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),d(zG,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),d(WG,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),d(vo,"class","docstring"),d(Mr,"class","docstring"),d(pF,"id","transformers.TFAutoModelForMultipleChoice"),d(pF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pF,"href","#transformers.TFAutoModelForMultipleChoice"),d(Cc,"class","relative group"),d(ut,"class","docstring"),d(QG,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),d(HG,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMultipleChoice"),d(UG,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),d(JG,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),d(YG,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),d(KG,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),d(ZG,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),d(eO,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),d(oO,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),d(rO,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),d(tO,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),d(aO,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),d(nO,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),d(sO,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),d(lO,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),d(iO,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),d(dO,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),d(To,"class","docstring"),d(Er,"class","docstring"),d(SF,"id","transformers.TFAutoModelForTableQuestionAnswering"),d(SF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(SF,"href","#transformers.TFAutoModelForTableQuestionAnswering"),d(yc,"class","relative group"),d(bt,"class","docstring"),d(cO,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),d(Fo,"class","docstring"),d(yr,"class","docstring"),d($F,"id","transformers.TFAutoModelForTokenClassification"),d($F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($F,"href","#transformers.TFAutoModelForTokenClassification"),d(Lc,"class","relative group"),d(vt,"class","docstring"),d(fO,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),d(mO,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForTokenClassification"),d(gO,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),d(hO,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),d(pO,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),d(_O,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),d(uO,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),d(bO,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForTokenClassification"),d(vO,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),d(TO,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),d(FO,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),d(CO,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),d(MO,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),d(EO,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),d(yO,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),d(wO,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),d(AO,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),d(LO,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),d(BO,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),d(xO,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),d(Co,"class","docstring"),d(wr,"class","docstring"),d(r9,"id","transformers.TFAutoModelForQuestionAnswering"),d(r9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(r9,"href","#transformers.TFAutoModelForQuestionAnswering"),d(kc,"class","relative group"),d(Tt,"class","docstring"),d(kO,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),d(RO,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),d(SO,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),d(PO,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),d($O,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),d(IO,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),d(DO,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),d(jO,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),d(NO,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),d(qO,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),d(GO,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),d(OO,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),d(XO,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),d(VO,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),d(zO,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),d(WO,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),d(QO,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),d(HO,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),d(UO,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),d(Mo,"class","docstring"),d(Ar,"class","docstring"),d(C9,"id","transformers.TFAutoModelForVision2Seq"),d(C9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(C9,"href","#transformers.TFAutoModelForVision2Seq"),d(Pc,"class","relative group"),d(Ft,"class","docstring"),d(JO,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),d(Eo,"class","docstring"),d(Lr,"class","docstring"),d(E9,"id","transformers.TFAutoModelForSpeechSeq2Seq"),d(E9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(E9,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),d(Dc,"class","relative group"),d(Ct,"class","docstring"),d(YO,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),d(yo,"class","docstring"),d(Br,"class","docstring"),d(w9,"id","transformers.FlaxAutoModel"),d(w9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w9,"href","#transformers.FlaxAutoModel"),d(qc,"class","relative group"),d(Mt,"class","docstring"),d(KO,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertModel"),d(ZO,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartModel"),d(eX,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitModel"),d(oX,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertModel"),d(rX,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),d(tX,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),d(aX,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),d(nX,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.FlaxCLIPModel"),d(sX,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),d(lX,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraModel"),d(iX,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2Model"),d(dX,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),d(cX,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJModel"),d(fX,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianModel"),d(mX,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartModel"),d(gX,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5Model"),d(hX,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusModel"),d(pX,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaModel"),d(_X,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerModel"),d(uX,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5Model"),d(bX,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),d(vX,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTModel"),d(TX,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),d(FX,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMModel"),d(wo,"class","docstring"),d(xr,"class","docstring"),d(K9,"id","transformers.FlaxAutoModelForCausalLM"),d(K9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(K9,"href","#transformers.FlaxAutoModelForCausalLM"),d(Xc,"class","relative group"),d(Et,"class","docstring"),d(CX,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),d(MX,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),d(EX,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),d(yX,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),d(Ao,"class","docstring"),d(kr,"class","docstring"),d(tC,"id","transformers.FlaxAutoModelForPreTraining"),d(tC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tC,"href","#transformers.FlaxAutoModelForPreTraining"),d(Wc,"class","relative group"),d(yt,"class","docstring"),d(wX,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),d(AX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(LX,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForPreTraining"),d(BX,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),d(xX,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),d(kX,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(RX,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(SX,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(PX,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d($X,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(IX,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),d(Lo,"class","docstring"),d(Rr,"class","docstring"),d(_C,"id","transformers.FlaxAutoModelForMaskedLM"),d(_C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_C,"href","#transformers.FlaxAutoModelForMaskedLM"),d(Uc,"class","relative group"),d(wt,"class","docstring"),d(DX,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),d(jX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(NX,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),d(qX,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),d(GX,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),d(OX,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),d(XX,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(VX,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(zX,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(Bo,"class","docstring"),d(Sr,"class","docstring"),d(AC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),d(AC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(AC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),d(Kc,"class","relative group"),d(At,"class","docstring"),d(WX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(QX,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),d(HX,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),d(UX,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),d(JX,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianMTModel"),d(YX,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(KX,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(ZX,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),d(eV,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(xo,"class","docstring"),d(Pr,"class","docstring"),d(DC,"id","transformers.FlaxAutoModelForSequenceClassification"),d(DC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(DC,"href","#transformers.FlaxAutoModelForSequenceClassification"),d(of,"class","relative group"),d(Lt,"class","docstring"),d(oV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),d(rV,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),d(tV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),d(aV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),d(nV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),d(sV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),d(lV,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),d(iV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),d(dV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),d(ko,"class","docstring"),d($r,"class","docstring"),d(HC,"id","transformers.FlaxAutoModelForQuestionAnswering"),d(HC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(HC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),d(af,"class","relative group"),d(Bt,"class","docstring"),d(cV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),d(fV,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),d(mV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),d(gV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),d(hV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),d(pV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),d(_V,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),d(uV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),d(bV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),d(Ro,"class","docstring"),d(Ir,"class","docstring"),d(nM,"id","transformers.FlaxAutoModelForTokenClassification"),d(nM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(nM,"href","#transformers.FlaxAutoModelForTokenClassification"),d(lf,"class","relative group"),d(xt,"class","docstring"),d(vV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),d(TV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),d(FV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),d(CV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),d(MV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),d(EV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),d(yV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),d(So,"class","docstring"),d(Dr,"class","docstring"),d(hM,"id","transformers.FlaxAutoModelForMultipleChoice"),d(hM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hM,"href","#transformers.FlaxAutoModelForMultipleChoice"),d(ff,"class","relative group"),d(kt,"class","docstring"),d(wV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),d(AV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),d(LV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),d(BV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),d(xV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),d(kV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),d(RV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),d(Po,"class","docstring"),d(jr,"class","docstring"),d(MM,"id","transformers.FlaxAutoModelForNextSentencePrediction"),d(MM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(MM,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),d(hf,"class","relative group"),d(Rt,"class","docstring"),d(SV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),d($o,"class","docstring"),d(Nr,"class","docstring"),d(yM,"id","transformers.FlaxAutoModelForImageClassification"),d(yM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yM,"href","#transformers.FlaxAutoModelForImageClassification"),d(uf,"class","relative group"),d(St,"class","docstring"),d(PV,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),d($V,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTForImageClassification"),d(Io,"class","docstring"),d(qr,"class","docstring"),d(LM,"id","transformers.FlaxAutoModelForVision2Seq"),d(LM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(LM,"href","#transformers.FlaxAutoModelForVision2Seq"),d(Tf,"class","relative group"),d(Pt,"class","docstring"),d(IV,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),d(Do,"class","docstring"),d(Gr,"class","docstring")},m(c,u){e(document.head,J),b(c,Pe,u),b(c,ie,u),e(ie,ge),e(ge,lo),g(fe,lo,null),e(ie,Te),e(ie,Xo),e(Xo,Li),b(c,Ef,u),b(c,sa,u),e(sa,Bi),e(sa,xi),e(xi,B4),e(sa,yf),b(c,Le,u),b(c,io,u),e(io,ki),e(io,In),e(In,x4),e(io,Dn),e(io,jn),e(jn,k4),e(io,Ri),e(io,Nn),e(Nn,R4),e(io,Si),b(c,wf,u),g($a,c,u),b(c,co,u),b(c,he,u),e(he,h8),e(he,Pi),e(Pi,p8),e(he,_8),b(c,Vo,u),b(c,Ia,u),e(Ia,u8),e(Ia,Af),e(Af,b8),e(Ia,dSe),b(c,Z7e,u),b(c,$i,u),e($i,Lf),e(Lf,AW),g(S4,AW,null),e($i,cSe),e($i,LW),e(LW,fSe),b(c,eBe,u),b(c,qn,u),e(qn,mSe),e(qn,BW),e(BW,gSe),e(qn,hSe),e(qn,xW),e(xW,pSe),e(qn,_Se),b(c,oBe,u),g(P4,c,u),b(c,rBe,u),b(c,v8,u),e(v8,uSe),b(c,tBe,u),g(Bf,c,u),b(c,aBe,u),b(c,Ii,u),e(Ii,xf),e(xf,kW),g($4,kW,null),e(Ii,bSe),e(Ii,RW),e(RW,vSe),b(c,nBe,u),b(c,zo,u),g(I4,zo,null),e(zo,TSe),e(zo,D4),e(D4,FSe),e(D4,T8),e(T8,CSe),e(D4,MSe),e(zo,ESe),e(zo,j4),e(j4,ySe),e(j4,SW),e(SW,wSe),e(j4,ASe),e(zo,LSe),e(zo,fo),g(N4,fo,null),e(fo,BSe),e(fo,PW),e(PW,xSe),e(fo,kSe),e(fo,Di),e(Di,RSe),e(Di,$W),e($W,SSe),e(Di,PSe),e(Di,IW),e(IW,$Se),e(Di,ISe),e(fo,DSe),e(fo,v),e(v,kf),e(kf,DW),e(DW,jSe),e(kf,NSe),e(kf,F8),e(F8,qSe),e(kf,GSe),e(v,OSe),e(v,Rf),e(Rf,jW),e(jW,XSe),e(Rf,VSe),e(Rf,C8),e(C8,zSe),e(Rf,WSe),e(v,QSe),e(v,Sf),e(Sf,NW),e(NW,HSe),e(Sf,USe),e(Sf,M8),e(M8,JSe),e(Sf,YSe),e(v,KSe),e(v,Pf),e(Pf,qW),e(qW,ZSe),e(Pf,ePe),e(Pf,E8),e(E8,oPe),e(Pf,rPe),e(v,tPe),e(v,$f),e($f,GW),e(GW,aPe),e($f,nPe),e($f,y8),e(y8,sPe),e($f,lPe),e(v,iPe),e(v,If),e(If,OW),e(OW,dPe),e(If,cPe),e(If,w8),e(w8,fPe),e(If,mPe),e(v,gPe),e(v,Df),e(Df,XW),e(XW,hPe),e(Df,pPe),e(Df,A8),e(A8,_Pe),e(Df,uPe),e(v,bPe),e(v,jf),e(jf,VW),e(VW,vPe),e(jf,TPe),e(jf,L8),e(L8,FPe),e(jf,CPe),e(v,MPe),e(v,Nf),e(Nf,zW),e(zW,EPe),e(Nf,yPe),e(Nf,B8),e(B8,wPe),e(Nf,APe),e(v,LPe),e(v,qf),e(qf,WW),e(WW,BPe),e(qf,xPe),e(qf,x8),e(x8,kPe),e(qf,RPe),e(v,SPe),e(v,Gf),e(Gf,QW),e(QW,PPe),e(Gf,$Pe),e(Gf,k8),e(k8,IPe),e(Gf,DPe),e(v,jPe),e(v,Of),e(Of,HW),e(HW,NPe),e(Of,qPe),e(Of,R8),e(R8,GPe),e(Of,OPe),e(v,XPe),e(v,Xf),e(Xf,UW),e(UW,VPe),e(Xf,zPe),e(Xf,S8),e(S8,WPe),e(Xf,QPe),e(v,HPe),e(v,Vf),e(Vf,JW),e(JW,UPe),e(Vf,JPe),e(Vf,P8),e(P8,YPe),e(Vf,KPe),e(v,ZPe),e(v,zf),e(zf,YW),e(YW,e$e),e(zf,o$e),e(zf,$8),e($8,r$e),e(zf,t$e),e(v,a$e),e(v,Wf),e(Wf,KW),e(KW,n$e),e(Wf,s$e),e(Wf,I8),e(I8,l$e),e(Wf,i$e),e(v,d$e),e(v,Qf),e(Qf,ZW),e(ZW,c$e),e(Qf,f$e),e(Qf,D8),e(D8,m$e),e(Qf,g$e),e(v,h$e),e(v,Hf),e(Hf,eQ),e(eQ,p$e),e(Hf,_$e),e(Hf,j8),e(j8,u$e),e(Hf,b$e),e(v,v$e),e(v,Uf),e(Uf,oQ),e(oQ,T$e),e(Uf,F$e),e(Uf,N8),e(N8,C$e),e(Uf,M$e),e(v,E$e),e(v,Jf),e(Jf,rQ),e(rQ,y$e),e(Jf,w$e),e(Jf,q8),e(q8,A$e),e(Jf,L$e),e(v,B$e),e(v,Yf),e(Yf,tQ),e(tQ,x$e),e(Yf,k$e),e(Yf,G8),e(G8,R$e),e(Yf,S$e),e(v,P$e),e(v,Kf),e(Kf,aQ),e(aQ,$$e),e(Kf,I$e),e(Kf,O8),e(O8,D$e),e(Kf,j$e),e(v,N$e),e(v,Zf),e(Zf,nQ),e(nQ,q$e),e(Zf,G$e),e(Zf,X8),e(X8,O$e),e(Zf,X$e),e(v,V$e),e(v,em),e(em,sQ),e(sQ,z$e),e(em,W$e),e(em,V8),e(V8,Q$e),e(em,H$e),e(v,U$e),e(v,om),e(om,lQ),e(lQ,J$e),e(om,Y$e),e(om,z8),e(z8,K$e),e(om,Z$e),e(v,eIe),e(v,rm),e(rm,iQ),e(iQ,oIe),e(rm,rIe),e(rm,W8),e(W8,tIe),e(rm,aIe),e(v,nIe),e(v,tm),e(tm,dQ),e(dQ,sIe),e(tm,lIe),e(tm,Q8),e(Q8,iIe),e(tm,dIe),e(v,cIe),e(v,am),e(am,cQ),e(cQ,fIe),e(am,mIe),e(am,H8),e(H8,gIe),e(am,hIe),e(v,pIe),e(v,nm),e(nm,fQ),e(fQ,_Ie),e(nm,uIe),e(nm,U8),e(U8,bIe),e(nm,vIe),e(v,TIe),e(v,sm),e(sm,mQ),e(mQ,FIe),e(sm,CIe),e(sm,J8),e(J8,MIe),e(sm,EIe),e(v,yIe),e(v,lm),e(lm,gQ),e(gQ,wIe),e(lm,AIe),e(lm,Y8),e(Y8,LIe),e(lm,BIe),e(v,xIe),e(v,im),e(im,hQ),e(hQ,kIe),e(im,RIe),e(im,K8),e(K8,SIe),e(im,PIe),e(v,$Ie),e(v,dm),e(dm,pQ),e(pQ,IIe),e(dm,DIe),e(dm,Z8),e(Z8,jIe),e(dm,NIe),e(v,qIe),e(v,cm),e(cm,_Q),e(_Q,GIe),e(cm,OIe),e(cm,e7),e(e7,XIe),e(cm,VIe),e(v,zIe),e(v,fm),e(fm,uQ),e(uQ,WIe),e(fm,QIe),e(fm,o7),e(o7,HIe),e(fm,UIe),e(v,JIe),e(v,mm),e(mm,bQ),e(bQ,YIe),e(mm,KIe),e(mm,r7),e(r7,ZIe),e(mm,eDe),e(v,oDe),e(v,gm),e(gm,vQ),e(vQ,rDe),e(gm,tDe),e(gm,t7),e(t7,aDe),e(gm,nDe),e(v,sDe),e(v,hm),e(hm,TQ),e(TQ,lDe),e(hm,iDe),e(hm,a7),e(a7,dDe),e(hm,cDe),e(v,fDe),e(v,pm),e(pm,FQ),e(FQ,mDe),e(pm,gDe),e(pm,n7),e(n7,hDe),e(pm,pDe),e(v,_De),e(v,_m),e(_m,CQ),e(CQ,uDe),e(_m,bDe),e(_m,s7),e(s7,vDe),e(_m,TDe),e(v,FDe),e(v,um),e(um,MQ),e(MQ,CDe),e(um,MDe),e(um,l7),e(l7,EDe),e(um,yDe),e(v,wDe),e(v,bm),e(bm,EQ),e(EQ,ADe),e(bm,LDe),e(bm,i7),e(i7,BDe),e(bm,xDe),e(v,kDe),e(v,vm),e(vm,yQ),e(yQ,RDe),e(vm,SDe),e(vm,d7),e(d7,PDe),e(vm,$De),e(v,IDe),e(v,Tm),e(Tm,wQ),e(wQ,DDe),e(Tm,jDe),e(Tm,c7),e(c7,NDe),e(Tm,qDe),e(v,GDe),e(v,Fm),e(Fm,AQ),e(AQ,ODe),e(Fm,XDe),e(Fm,f7),e(f7,VDe),e(Fm,zDe),e(v,WDe),e(v,Cm),e(Cm,LQ),e(LQ,QDe),e(Cm,HDe),e(Cm,m7),e(m7,UDe),e(Cm,JDe),e(v,YDe),e(v,Mm),e(Mm,BQ),e(BQ,KDe),e(Mm,ZDe),e(Mm,g7),e(g7,eje),e(Mm,oje),e(v,rje),e(v,Em),e(Em,xQ),e(xQ,tje),e(Em,aje),e(Em,h7),e(h7,nje),e(Em,sje),e(v,lje),e(v,ym),e(ym,kQ),e(kQ,ije),e(ym,dje),e(ym,p7),e(p7,cje),e(ym,fje),e(v,mje),e(v,wm),e(wm,RQ),e(RQ,gje),e(wm,hje),e(wm,_7),e(_7,pje),e(wm,_je),e(v,uje),e(v,Am),e(Am,SQ),e(SQ,bje),e(Am,vje),e(Am,u7),e(u7,Tje),e(Am,Fje),e(v,Cje),e(v,Lm),e(Lm,PQ),e(PQ,Mje),e(Lm,Eje),e(Lm,b7),e(b7,yje),e(Lm,wje),e(v,Aje),e(v,Bm),e(Bm,$Q),e($Q,Lje),e(Bm,Bje),e(Bm,v7),e(v7,xje),e(Bm,kje),e(v,Rje),e(v,xm),e(xm,IQ),e(IQ,Sje),e(xm,Pje),e(xm,T7),e(T7,$je),e(xm,Ije),e(v,Dje),e(v,km),e(km,DQ),e(DQ,jje),e(km,Nje),e(km,F7),e(F7,qje),e(km,Gje),e(v,Oje),e(v,Rm),e(Rm,jQ),e(jQ,Xje),e(Rm,Vje),e(Rm,C7),e(C7,zje),e(Rm,Wje),e(v,Qje),e(v,Sm),e(Sm,NQ),e(NQ,Hje),e(Sm,Uje),e(Sm,M7),e(M7,Jje),e(Sm,Yje),e(v,Kje),e(v,Pm),e(Pm,qQ),e(qQ,Zje),e(Pm,eNe),e(Pm,E7),e(E7,oNe),e(Pm,rNe),e(v,tNe),e(v,$m),e($m,GQ),e(GQ,aNe),e($m,nNe),e($m,y7),e(y7,sNe),e($m,lNe),e(v,iNe),e(v,Im),e(Im,OQ),e(OQ,dNe),e(Im,cNe),e(Im,w7),e(w7,fNe),e(Im,mNe),e(v,gNe),e(v,Dm),e(Dm,XQ),e(XQ,hNe),e(Dm,pNe),e(Dm,A7),e(A7,_Ne),e(Dm,uNe),e(v,bNe),e(v,jm),e(jm,VQ),e(VQ,vNe),e(jm,TNe),e(jm,L7),e(L7,FNe),e(jm,CNe),e(v,MNe),e(v,Nm),e(Nm,zQ),e(zQ,ENe),e(Nm,yNe),e(Nm,B7),e(B7,wNe),e(Nm,ANe),e(v,LNe),e(v,qm),e(qm,WQ),e(WQ,BNe),e(qm,xNe),e(qm,x7),e(x7,kNe),e(qm,RNe),e(v,SNe),e(v,Gm),e(Gm,QQ),e(QQ,PNe),e(Gm,$Ne),e(Gm,k7),e(k7,INe),e(Gm,DNe),e(v,jNe),e(v,Om),e(Om,HQ),e(HQ,NNe),e(Om,qNe),e(Om,R7),e(R7,GNe),e(Om,ONe),e(v,XNe),e(v,Xm),e(Xm,UQ),e(UQ,VNe),e(Xm,zNe),e(Xm,S7),e(S7,WNe),e(Xm,QNe),e(v,HNe),e(v,Vm),e(Vm,JQ),e(JQ,UNe),e(Vm,JNe),e(Vm,P7),e(P7,YNe),e(Vm,KNe),e(v,ZNe),e(v,zm),e(zm,YQ),e(YQ,eqe),e(zm,oqe),e(zm,$7),e($7,rqe),e(zm,tqe),e(v,aqe),e(v,Wm),e(Wm,KQ),e(KQ,nqe),e(Wm,sqe),e(Wm,I7),e(I7,lqe),e(Wm,iqe),e(v,dqe),e(v,Qm),e(Qm,ZQ),e(ZQ,cqe),e(Qm,fqe),e(Qm,D7),e(D7,mqe),e(Qm,gqe),e(v,hqe),e(v,Hm),e(Hm,eH),e(eH,pqe),e(Hm,_qe),e(Hm,j7),e(j7,uqe),e(Hm,bqe),e(v,vqe),e(v,Um),e(Um,oH),e(oH,Tqe),e(Um,Fqe),e(Um,N7),e(N7,Cqe),e(Um,Mqe),e(v,Eqe),e(v,Jm),e(Jm,rH),e(rH,yqe),e(Jm,wqe),e(Jm,q7),e(q7,Aqe),e(Jm,Lqe),e(v,Bqe),e(v,Ym),e(Ym,tH),e(tH,xqe),e(Ym,kqe),e(Ym,G7),e(G7,Rqe),e(Ym,Sqe),e(v,Pqe),e(v,Km),e(Km,aH),e(aH,$qe),e(Km,Iqe),e(Km,O7),e(O7,Dqe),e(Km,jqe),e(v,Nqe),e(v,Zm),e(Zm,nH),e(nH,qqe),e(Zm,Gqe),e(Zm,X7),e(X7,Oqe),e(Zm,Xqe),e(v,Vqe),e(v,eg),e(eg,sH),e(sH,zqe),e(eg,Wqe),e(eg,V7),e(V7,Qqe),e(eg,Hqe),e(v,Uqe),e(v,og),e(og,lH),e(lH,Jqe),e(og,Yqe),e(og,z7),e(z7,Kqe),e(og,Zqe),e(v,eGe),e(v,rg),e(rg,iH),e(iH,oGe),e(rg,rGe),e(rg,W7),e(W7,tGe),e(rg,aGe),e(v,nGe),e(v,tg),e(tg,dH),e(dH,sGe),e(tg,lGe),e(tg,Q7),e(Q7,iGe),e(tg,dGe),e(v,cGe),e(v,ag),e(ag,cH),e(cH,fGe),e(ag,mGe),e(ag,H7),e(H7,gGe),e(ag,hGe),e(v,pGe),e(v,ng),e(ng,fH),e(fH,_Ge),e(ng,uGe),e(ng,U7),e(U7,bGe),e(ng,vGe),e(v,TGe),e(v,sg),e(sg,mH),e(mH,FGe),e(sg,CGe),e(sg,J7),e(J7,MGe),e(sg,EGe),e(v,yGe),e(v,lg),e(lg,gH),e(gH,wGe),e(lg,AGe),e(lg,Y7),e(Y7,LGe),e(lg,BGe),e(v,xGe),e(v,ig),e(ig,hH),e(hH,kGe),e(ig,RGe),e(ig,K7),e(K7,SGe),e(ig,PGe),e(v,$Ge),e(v,dg),e(dg,pH),e(pH,IGe),e(dg,DGe),e(dg,Z7),e(Z7,jGe),e(dg,NGe),e(v,qGe),e(v,cg),e(cg,_H),e(_H,GGe),e(cg,OGe),e(cg,eB),e(eB,XGe),e(cg,VGe),e(v,zGe),e(v,fg),e(fg,uH),e(uH,WGe),e(fg,QGe),e(fg,oB),e(oB,HGe),e(fg,UGe),e(v,JGe),e(v,mg),e(mg,bH),e(bH,YGe),e(mg,KGe),e(mg,rB),e(rB,ZGe),e(mg,eOe),e(v,oOe),e(v,gg),e(gg,vH),e(vH,rOe),e(gg,tOe),e(gg,tB),e(tB,aOe),e(gg,nOe),e(v,sOe),e(v,hg),e(hg,TH),e(TH,lOe),e(hg,iOe),e(hg,aB),e(aB,dOe),e(hg,cOe),e(v,fOe),e(v,pg),e(pg,FH),e(FH,mOe),e(pg,gOe),e(pg,nB),e(nB,hOe),e(pg,pOe),e(v,_Oe),e(v,_g),e(_g,CH),e(CH,uOe),e(_g,bOe),e(_g,sB),e(sB,vOe),e(_g,TOe),e(fo,FOe),e(fo,MH),e(MH,COe),e(fo,MOe),g(q4,fo,null),e(zo,EOe),e(zo,ug),g(G4,ug,null),e(ug,yOe),e(ug,EH),e(EH,wOe),b(c,sBe,u),b(c,ji,u),e(ji,bg),e(bg,yH),g(O4,yH,null),e(ji,AOe),e(ji,wH),e(wH,LOe),b(c,lBe,u),b(c,Wo,u),g(X4,Wo,null),e(Wo,BOe),e(Wo,V4),e(V4,xOe),e(V4,lB),e(lB,kOe),e(V4,ROe),e(Wo,SOe),e(Wo,z4),e(z4,POe),e(z4,AH),e(AH,$Oe),e(z4,IOe),e(Wo,DOe),e(Wo,mo),g(W4,mo,null),e(mo,jOe),e(mo,LH),e(LH,NOe),e(mo,qOe),e(mo,Da),e(Da,GOe),e(Da,BH),e(BH,OOe),e(Da,XOe),e(Da,xH),e(xH,VOe),e(Da,zOe),e(Da,kH),e(kH,WOe),e(Da,QOe),e(mo,HOe),e(mo,M),e(M,Gn),e(Gn,RH),e(RH,UOe),e(Gn,JOe),e(Gn,iB),e(iB,YOe),e(Gn,KOe),e(Gn,dB),e(dB,ZOe),e(Gn,eXe),e(M,oXe),e(M,On),e(On,SH),e(SH,rXe),e(On,tXe),e(On,cB),e(cB,aXe),e(On,nXe),e(On,fB),e(fB,sXe),e(On,lXe),e(M,iXe),e(M,Xn),e(Xn,PH),e(PH,dXe),e(Xn,cXe),e(Xn,mB),e(mB,fXe),e(Xn,mXe),e(Xn,gB),e(gB,gXe),e(Xn,hXe),e(M,pXe),e(M,vg),e(vg,$H),e($H,_Xe),e(vg,uXe),e(vg,hB),e(hB,bXe),e(vg,vXe),e(M,TXe),e(M,Vn),e(Vn,IH),e(IH,FXe),e(Vn,CXe),e(Vn,pB),e(pB,MXe),e(Vn,EXe),e(Vn,_B),e(_B,yXe),e(Vn,wXe),e(M,AXe),e(M,Tg),e(Tg,DH),e(DH,LXe),e(Tg,BXe),e(Tg,uB),e(uB,xXe),e(Tg,kXe),e(M,RXe),e(M,Fg),e(Fg,jH),e(jH,SXe),e(Fg,PXe),e(Fg,bB),e(bB,$Xe),e(Fg,IXe),e(M,DXe),e(M,Cg),e(Cg,NH),e(NH,jXe),e(Cg,NXe),e(Cg,vB),e(vB,qXe),e(Cg,GXe),e(M,OXe),e(M,zn),e(zn,qH),e(qH,XXe),e(zn,VXe),e(zn,TB),e(TB,zXe),e(zn,WXe),e(zn,FB),e(FB,QXe),e(zn,HXe),e(M,UXe),e(M,Wn),e(Wn,GH),e(GH,JXe),e(Wn,YXe),e(Wn,CB),e(CB,KXe),e(Wn,ZXe),e(Wn,MB),e(MB,eVe),e(Wn,oVe),e(M,rVe),e(M,Qn),e(Qn,OH),e(OH,tVe),e(Qn,aVe),e(Qn,EB),e(EB,nVe),e(Qn,sVe),e(Qn,yB),e(yB,lVe),e(Qn,iVe),e(M,dVe),e(M,Mg),e(Mg,XH),e(XH,cVe),e(Mg,fVe),e(Mg,wB),e(wB,mVe),e(Mg,gVe),e(M,hVe),e(M,Eg),e(Eg,VH),e(VH,pVe),e(Eg,_Ve),e(Eg,AB),e(AB,uVe),e(Eg,bVe),e(M,vVe),e(M,Hn),e(Hn,zH),e(zH,TVe),e(Hn,FVe),e(Hn,LB),e(LB,CVe),e(Hn,MVe),e(Hn,BB),e(BB,EVe),e(Hn,yVe),e(M,wVe),e(M,yg),e(yg,WH),e(WH,AVe),e(yg,LVe),e(yg,xB),e(xB,BVe),e(yg,xVe),e(M,kVe),e(M,Un),e(Un,QH),e(QH,RVe),e(Un,SVe),e(Un,kB),e(kB,PVe),e(Un,$Ve),e(Un,RB),e(RB,IVe),e(Un,DVe),e(M,jVe),e(M,Jn),e(Jn,HH),e(HH,NVe),e(Jn,qVe),e(Jn,SB),e(SB,GVe),e(Jn,OVe),e(Jn,PB),e(PB,XVe),e(Jn,VVe),e(M,zVe),e(M,Yn),e(Yn,UH),e(UH,WVe),e(Yn,QVe),e(Yn,$B),e($B,HVe),e(Yn,UVe),e(Yn,JH),e(JH,JVe),e(Yn,YVe),e(M,KVe),e(M,wg),e(wg,YH),e(YH,ZVe),e(wg,eze),e(wg,IB),e(IB,oze),e(wg,rze),e(M,tze),e(M,Kn),e(Kn,KH),e(KH,aze),e(Kn,nze),e(Kn,DB),e(DB,sze),e(Kn,lze),e(Kn,jB),e(jB,ize),e(Kn,dze),e(M,cze),e(M,Ag),e(Ag,ZH),e(ZH,fze),e(Ag,mze),e(Ag,NB),e(NB,gze),e(Ag,hze),e(M,pze),e(M,Zn),e(Zn,eU),e(eU,_ze),e(Zn,uze),e(Zn,qB),e(qB,bze),e(Zn,vze),e(Zn,GB),e(GB,Tze),e(Zn,Fze),e(M,Cze),e(M,es),e(es,oU),e(oU,Mze),e(es,Eze),e(es,OB),e(OB,yze),e(es,wze),e(es,XB),e(XB,Aze),e(es,Lze),e(M,Bze),e(M,os),e(os,rU),e(rU,xze),e(os,kze),e(os,VB),e(VB,Rze),e(os,Sze),e(os,zB),e(zB,Pze),e(os,$ze),e(M,Ize),e(M,Lg),e(Lg,tU),e(tU,Dze),e(Lg,jze),e(Lg,WB),e(WB,Nze),e(Lg,qze),e(M,Gze),e(M,rs),e(rs,aU),e(aU,Oze),e(rs,Xze),e(rs,QB),e(QB,Vze),e(rs,zze),e(rs,HB),e(HB,Wze),e(rs,Qze),e(M,Hze),e(M,Bg),e(Bg,nU),e(nU,Uze),e(Bg,Jze),e(Bg,UB),e(UB,Yze),e(Bg,Kze),e(M,Zze),e(M,ts),e(ts,sU),e(sU,eWe),e(ts,oWe),e(ts,JB),e(JB,rWe),e(ts,tWe),e(ts,YB),e(YB,aWe),e(ts,nWe),e(M,sWe),e(M,as),e(as,lU),e(lU,lWe),e(as,iWe),e(as,KB),e(KB,dWe),e(as,cWe),e(as,ZB),e(ZB,fWe),e(as,mWe),e(M,gWe),e(M,ns),e(ns,iU),e(iU,hWe),e(ns,pWe),e(ns,ex),e(ex,_We),e(ns,uWe),e(ns,ox),e(ox,bWe),e(ns,vWe),e(M,TWe),e(M,ss),e(ss,dU),e(dU,FWe),e(ss,CWe),e(ss,rx),e(rx,MWe),e(ss,EWe),e(ss,tx),e(tx,yWe),e(ss,wWe),e(M,AWe),e(M,xg),e(xg,cU),e(cU,LWe),e(xg,BWe),e(xg,ax),e(ax,xWe),e(xg,kWe),e(M,RWe),e(M,ls),e(ls,fU),e(fU,SWe),e(ls,PWe),e(ls,nx),e(nx,$We),e(ls,IWe),e(ls,sx),e(sx,DWe),e(ls,jWe),e(M,NWe),e(M,is),e(is,mU),e(mU,qWe),e(is,GWe),e(is,lx),e(lx,OWe),e(is,XWe),e(is,ix),e(ix,VWe),e(is,zWe),e(M,WWe),e(M,ds),e(ds,gU),e(gU,QWe),e(ds,HWe),e(ds,dx),e(dx,UWe),e(ds,JWe),e(ds,cx),e(cx,YWe),e(ds,KWe),e(M,ZWe),e(M,cs),e(cs,hU),e(hU,eQe),e(cs,oQe),e(cs,fx),e(fx,rQe),e(cs,tQe),e(cs,mx),e(mx,aQe),e(cs,nQe),e(M,sQe),e(M,fs),e(fs,pU),e(pU,lQe),e(fs,iQe),e(fs,gx),e(gx,dQe),e(fs,cQe),e(fs,hx),e(hx,fQe),e(fs,mQe),e(M,gQe),e(M,ms),e(ms,_U),e(_U,hQe),e(ms,pQe),e(ms,px),e(px,_Qe),e(ms,uQe),e(ms,_x),e(_x,bQe),e(ms,vQe),e(M,TQe),e(M,kg),e(kg,uU),e(uU,FQe),e(kg,CQe),e(kg,ux),e(ux,MQe),e(kg,EQe),e(M,yQe),e(M,gs),e(gs,bU),e(bU,wQe),e(gs,AQe),e(gs,bx),e(bx,LQe),e(gs,BQe),e(gs,vx),e(vx,xQe),e(gs,kQe),e(M,RQe),e(M,Rg),e(Rg,vU),e(vU,SQe),e(Rg,PQe),e(Rg,Tx),e(Tx,$Qe),e(Rg,IQe),e(M,DQe),e(M,Sg),e(Sg,TU),e(TU,jQe),e(Sg,NQe),e(Sg,Fx),e(Fx,qQe),e(Sg,GQe),e(M,OQe),e(M,hs),e(hs,FU),e(FU,XQe),e(hs,VQe),e(hs,Cx),e(Cx,zQe),e(hs,WQe),e(hs,Mx),e(Mx,QQe),e(hs,HQe),e(M,UQe),e(M,ps),e(ps,CU),e(CU,JQe),e(ps,YQe),e(ps,Ex),e(Ex,KQe),e(ps,ZQe),e(ps,yx),e(yx,eHe),e(ps,oHe),e(M,rHe),e(M,Pg),e(Pg,MU),e(MU,tHe),e(Pg,aHe),e(Pg,wx),e(wx,nHe),e(Pg,sHe),e(M,lHe),e(M,_s),e(_s,EU),e(EU,iHe),e(_s,dHe),e(_s,Ax),e(Ax,cHe),e(_s,fHe),e(_s,Lx),e(Lx,mHe),e(_s,gHe),e(M,hHe),e(M,us),e(us,yU),e(yU,pHe),e(us,_He),e(us,Bx),e(Bx,uHe),e(us,bHe),e(us,xx),e(xx,vHe),e(us,THe),e(M,FHe),e(M,bs),e(bs,wU),e(wU,CHe),e(bs,MHe),e(bs,kx),e(kx,EHe),e(bs,yHe),e(bs,Rx),e(Rx,wHe),e(bs,AHe),e(M,LHe),e(M,vs),e(vs,AU),e(AU,BHe),e(vs,xHe),e(vs,Sx),e(Sx,kHe),e(vs,RHe),e(vs,Px),e(Px,SHe),e(vs,PHe),e(M,$He),e(M,Ts),e(Ts,LU),e(LU,IHe),e(Ts,DHe),e(Ts,$x),e($x,jHe),e(Ts,NHe),e(Ts,Ix),e(Ix,qHe),e(Ts,GHe),e(M,OHe),e(M,$g),e($g,BU),e(BU,XHe),e($g,VHe),e($g,Dx),e(Dx,zHe),e($g,WHe),e(M,QHe),e(M,Ig),e(Ig,xU),e(xU,HHe),e(Ig,UHe),e(Ig,jx),e(jx,JHe),e(Ig,YHe),e(M,KHe),e(M,Dg),e(Dg,kU),e(kU,ZHe),e(Dg,eUe),e(Dg,Nx),e(Nx,oUe),e(Dg,rUe),e(M,tUe),e(M,jg),e(jg,RU),e(RU,aUe),e(jg,nUe),e(jg,qx),e(qx,sUe),e(jg,lUe),e(M,iUe),e(M,Fs),e(Fs,SU),e(SU,dUe),e(Fs,cUe),e(Fs,Gx),e(Gx,fUe),e(Fs,mUe),e(Fs,Ox),e(Ox,gUe),e(Fs,hUe),e(M,pUe),e(M,Ng),e(Ng,PU),e(PU,_Ue),e(Ng,uUe),e(Ng,Xx),e(Xx,bUe),e(Ng,vUe),e(M,TUe),e(M,Cs),e(Cs,$U),e($U,FUe),e(Cs,CUe),e(Cs,Vx),e(Vx,MUe),e(Cs,EUe),e(Cs,zx),e(zx,yUe),e(Cs,wUe),e(M,AUe),e(M,Ms),e(Ms,IU),e(IU,LUe),e(Ms,BUe),e(Ms,Wx),e(Wx,xUe),e(Ms,kUe),e(Ms,Qx),e(Qx,RUe),e(Ms,SUe),e(M,PUe),e(M,Es),e(Es,DU),e(DU,$Ue),e(Es,IUe),e(Es,Hx),e(Hx,DUe),e(Es,jUe),e(Es,Ux),e(Ux,NUe),e(Es,qUe),e(M,GUe),e(M,ys),e(ys,jU),e(jU,OUe),e(ys,XUe),e(ys,Jx),e(Jx,VUe),e(ys,zUe),e(ys,Yx),e(Yx,WUe),e(ys,QUe),e(M,HUe),e(M,ws),e(ws,NU),e(NU,UUe),e(ws,JUe),e(ws,Kx),e(Kx,YUe),e(ws,KUe),e(ws,Zx),e(Zx,ZUe),e(ws,eJe),e(M,oJe),e(M,qg),e(qg,qU),e(qU,rJe),e(qg,tJe),e(qg,ek),e(ek,aJe),e(qg,nJe),e(M,sJe),e(M,Gg),e(Gg,GU),e(GU,lJe),e(Gg,iJe),e(Gg,ok),e(ok,dJe),e(Gg,cJe),e(M,fJe),e(M,As),e(As,OU),e(OU,mJe),e(As,gJe),e(As,rk),e(rk,hJe),e(As,pJe),e(As,tk),e(tk,_Je),e(As,uJe),e(M,bJe),e(M,Ls),e(Ls,XU),e(XU,vJe),e(Ls,TJe),e(Ls,ak),e(ak,FJe),e(Ls,CJe),e(Ls,nk),e(nk,MJe),e(Ls,EJe),e(M,yJe),e(M,Bs),e(Bs,VU),e(VU,wJe),e(Bs,AJe),e(Bs,sk),e(sk,LJe),e(Bs,BJe),e(Bs,lk),e(lk,xJe),e(Bs,kJe),e(M,RJe),e(M,Og),e(Og,zU),e(zU,SJe),e(Og,PJe),e(Og,ik),e(ik,$Je),e(Og,IJe),e(M,DJe),e(M,Xg),e(Xg,WU),e(WU,jJe),e(Xg,NJe),e(Xg,dk),e(dk,qJe),e(Xg,GJe),e(M,OJe),e(M,Vg),e(Vg,QU),e(QU,XJe),e(Vg,VJe),e(Vg,ck),e(ck,zJe),e(Vg,WJe),e(M,QJe),e(M,zg),e(zg,HU),e(HU,HJe),e(zg,UJe),e(zg,fk),e(fk,JJe),e(zg,YJe),e(M,KJe),e(M,xs),e(xs,UU),e(UU,ZJe),e(xs,eYe),e(xs,mk),e(mk,oYe),e(xs,rYe),e(xs,gk),e(gk,tYe),e(xs,aYe),e(M,nYe),e(M,Wg),e(Wg,JU),e(JU,sYe),e(Wg,lYe),e(Wg,hk),e(hk,iYe),e(Wg,dYe),e(M,cYe),e(M,Qg),e(Qg,YU),e(YU,fYe),e(Qg,mYe),e(Qg,pk),e(pk,gYe),e(Qg,hYe),e(M,pYe),e(M,ks),e(ks,KU),e(KU,_Ye),e(ks,uYe),e(ks,_k),e(_k,bYe),e(ks,vYe),e(ks,uk),e(uk,TYe),e(ks,FYe),e(M,CYe),e(M,Rs),e(Rs,ZU),e(ZU,MYe),e(Rs,EYe),e(Rs,bk),e(bk,yYe),e(Rs,wYe),e(Rs,vk),e(vk,AYe),e(Rs,LYe),e(mo,BYe),e(mo,eJ),e(eJ,xYe),e(mo,kYe),g(Q4,mo,null),e(Wo,RYe),e(Wo,Hg),g(H4,Hg,null),e(Hg,SYe),e(Hg,oJ),e(oJ,PYe),b(c,iBe,u),b(c,Ni,u),e(Ni,Ug),e(Ug,rJ),g(U4,rJ,null),e(Ni,$Ye),e(Ni,tJ),e(tJ,IYe),b(c,dBe,u),b(c,Qo,u),g(J4,Qo,null),e(Qo,DYe),e(Qo,Y4),e(Y4,jYe),e(Y4,Tk),e(Tk,NYe),e(Y4,qYe),e(Qo,GYe),e(Qo,K4),e(K4,OYe),e(K4,aJ),e(aJ,XYe),e(K4,VYe),e(Qo,zYe),e(Qo,$e),g(Z4,$e,null),e($e,WYe),e($e,nJ),e(nJ,QYe),e($e,HYe),e($e,ja),e(ja,UYe),e(ja,sJ),e(sJ,JYe),e(ja,YYe),e(ja,lJ),e(lJ,KYe),e(ja,ZYe),e(ja,iJ),e(iJ,eKe),e(ja,oKe),e($e,rKe),e($e,se),e(se,Jg),e(Jg,dJ),e(dJ,tKe),e(Jg,aKe),e(Jg,Fk),e(Fk,nKe),e(Jg,sKe),e(se,lKe),e(se,Yg),e(Yg,cJ),e(cJ,iKe),e(Yg,dKe),e(Yg,Ck),e(Ck,cKe),e(Yg,fKe),e(se,mKe),e(se,Kg),e(Kg,fJ),e(fJ,gKe),e(Kg,hKe),e(Kg,Mk),e(Mk,pKe),e(Kg,_Ke),e(se,uKe),e(se,Zg),e(Zg,mJ),e(mJ,bKe),e(Zg,vKe),e(Zg,Ek),e(Ek,TKe),e(Zg,FKe),e(se,CKe),e(se,eh),e(eh,gJ),e(gJ,MKe),e(eh,EKe),e(eh,yk),e(yk,yKe),e(eh,wKe),e(se,AKe),e(se,oh),e(oh,hJ),e(hJ,LKe),e(oh,BKe),e(oh,wk),e(wk,xKe),e(oh,kKe),e(se,RKe),e(se,rh),e(rh,pJ),e(pJ,SKe),e(rh,PKe),e(rh,Ak),e(Ak,$Ke),e(rh,IKe),e(se,DKe),e(se,th),e(th,_J),e(_J,jKe),e(th,NKe),e(th,Lk),e(Lk,qKe),e(th,GKe),e(se,OKe),e(se,ah),e(ah,uJ),e(uJ,XKe),e(ah,VKe),e(ah,Bk),e(Bk,zKe),e(ah,WKe),e(se,QKe),e(se,nh),e(nh,bJ),e(bJ,HKe),e(nh,UKe),e(nh,xk),e(xk,JKe),e(nh,YKe),e(se,KKe),e(se,sh),e(sh,vJ),e(vJ,ZKe),e(sh,eZe),e(sh,kk),e(kk,oZe),e(sh,rZe),e(se,tZe),e(se,lh),e(lh,TJ),e(TJ,aZe),e(lh,nZe),e(lh,Rk),e(Rk,sZe),e(lh,lZe),e(se,iZe),e(se,ih),e(ih,FJ),e(FJ,dZe),e(ih,cZe),e(ih,Sk),e(Sk,fZe),e(ih,mZe),e(se,gZe),e(se,dh),e(dh,CJ),e(CJ,hZe),e(dh,pZe),e(dh,Pk),e(Pk,_Ze),e(dh,uZe),e(se,bZe),e(se,ch),e(ch,MJ),e(MJ,vZe),e(ch,TZe),e(ch,$k),e($k,FZe),e(ch,CZe),e($e,MZe),g(fh,$e,null),e($e,EZe),e($e,EJ),e(EJ,yZe),e($e,wZe),g(eE,$e,null),e(Qo,AZe),e(Qo,mh),g(oE,mh,null),e(mh,LZe),e(mh,yJ),e(yJ,BZe),b(c,cBe,u),b(c,qi,u),e(qi,gh),e(gh,wJ),g(rE,wJ,null),e(qi,xZe),e(qi,AJ),e(AJ,kZe),b(c,fBe,u),b(c,Ho,u),g(tE,Ho,null),e(Ho,RZe),e(Ho,aE),e(aE,SZe),e(aE,Ik),e(Ik,PZe),e(aE,$Ze),e(Ho,IZe),e(Ho,nE),e(nE,DZe),e(nE,LJ),e(LJ,jZe),e(nE,NZe),e(Ho,qZe),e(Ho,Ie),g(sE,Ie,null),e(Ie,GZe),e(Ie,BJ),e(BJ,OZe),e(Ie,XZe),e(Ie,Gi),e(Gi,VZe),e(Gi,xJ),e(xJ,zZe),e(Gi,WZe),e(Gi,kJ),e(kJ,QZe),e(Gi,HZe),e(Ie,UZe),e(Ie,Be),e(Be,hh),e(hh,RJ),e(RJ,JZe),e(hh,YZe),e(hh,Dk),e(Dk,KZe),e(hh,ZZe),e(Be,eeo),e(Be,ph),e(ph,SJ),e(SJ,oeo),e(ph,reo),e(ph,jk),e(jk,teo),e(ph,aeo),e(Be,neo),e(Be,_h),e(_h,PJ),e(PJ,seo),e(_h,leo),e(_h,Nk),e(Nk,ieo),e(_h,deo),e(Be,ceo),e(Be,uh),e(uh,$J),e($J,feo),e(uh,meo),e(uh,qk),e(qk,geo),e(uh,heo),e(Be,peo),e(Be,bh),e(bh,IJ),e(IJ,_eo),e(bh,ueo),e(bh,Gk),e(Gk,beo),e(bh,veo),e(Be,Teo),e(Be,vh),e(vh,DJ),e(DJ,Feo),e(vh,Ceo),e(vh,Ok),e(Ok,Meo),e(vh,Eeo),e(Be,yeo),e(Be,Th),e(Th,jJ),e(jJ,weo),e(Th,Aeo),e(Th,Xk),e(Xk,Leo),e(Th,Beo),e(Be,xeo),e(Be,Fh),e(Fh,NJ),e(NJ,keo),e(Fh,Reo),e(Fh,Vk),e(Vk,Seo),e(Fh,Peo),e(Ie,$eo),g(Ch,Ie,null),e(Ie,Ieo),e(Ie,qJ),e(qJ,Deo),e(Ie,jeo),g(lE,Ie,null),e(Ho,Neo),e(Ho,Mh),g(iE,Mh,null),e(Mh,qeo),e(Mh,GJ),e(GJ,Geo),b(c,mBe,u),b(c,Oi,u),e(Oi,Eh),e(Eh,OJ),g(dE,OJ,null),e(Oi,Oeo),e(Oi,XJ),e(XJ,Xeo),b(c,gBe,u),b(c,Uo,u),g(cE,Uo,null),e(Uo,Veo),e(Uo,Xi),e(Xi,zeo),e(Xi,VJ),e(VJ,Weo),e(Xi,Qeo),e(Xi,zJ),e(zJ,Heo),e(Xi,Ueo),e(Uo,Jeo),e(Uo,fE),e(fE,Yeo),e(fE,WJ),e(WJ,Keo),e(fE,Zeo),e(Uo,eoo),e(Uo,Or),g(mE,Or,null),e(Or,ooo),e(Or,QJ),e(QJ,roo),e(Or,too),e(Or,Vi),e(Vi,aoo),e(Vi,HJ),e(HJ,noo),e(Vi,soo),e(Vi,UJ),e(UJ,loo),e(Vi,ioo),e(Or,doo),e(Or,JJ),e(JJ,coo),e(Or,foo),g(gE,Or,null),e(Uo,moo),e(Uo,De),g(hE,De,null),e(De,goo),e(De,YJ),e(YJ,hoo),e(De,poo),e(De,Na),e(Na,_oo),e(Na,KJ),e(KJ,uoo),e(Na,boo),e(Na,ZJ),e(ZJ,voo),e(Na,Too),e(Na,eY),e(eY,Foo),e(Na,Coo),e(De,Moo),e(De,F),e(F,yh),e(yh,oY),e(oY,Eoo),e(yh,yoo),e(yh,zk),e(zk,woo),e(yh,Aoo),e(F,Loo),e(F,wh),e(wh,rY),e(rY,Boo),e(wh,xoo),e(wh,Wk),e(Wk,koo),e(wh,Roo),e(F,Soo),e(F,Ah),e(Ah,tY),e(tY,Poo),e(Ah,$oo),e(Ah,Qk),e(Qk,Ioo),e(Ah,Doo),e(F,joo),e(F,Lh),e(Lh,aY),e(aY,Noo),e(Lh,qoo),e(Lh,Hk),e(Hk,Goo),e(Lh,Ooo),e(F,Xoo),e(F,Bh),e(Bh,nY),e(nY,Voo),e(Bh,zoo),e(Bh,Uk),e(Uk,Woo),e(Bh,Qoo),e(F,Hoo),e(F,xh),e(xh,sY),e(sY,Uoo),e(xh,Joo),e(xh,Jk),e(Jk,Yoo),e(xh,Koo),e(F,Zoo),e(F,kh),e(kh,lY),e(lY,ero),e(kh,oro),e(kh,Yk),e(Yk,rro),e(kh,tro),e(F,aro),e(F,Rh),e(Rh,iY),e(iY,nro),e(Rh,sro),e(Rh,Kk),e(Kk,lro),e(Rh,iro),e(F,dro),e(F,Sh),e(Sh,dY),e(dY,cro),e(Sh,fro),e(Sh,Zk),e(Zk,mro),e(Sh,gro),e(F,hro),e(F,Ph),e(Ph,cY),e(cY,pro),e(Ph,_ro),e(Ph,eR),e(eR,uro),e(Ph,bro),e(F,vro),e(F,$h),e($h,fY),e(fY,Tro),e($h,Fro),e($h,oR),e(oR,Cro),e($h,Mro),e(F,Ero),e(F,Ih),e(Ih,mY),e(mY,yro),e(Ih,wro),e(Ih,rR),e(rR,Aro),e(Ih,Lro),e(F,Bro),e(F,Dh),e(Dh,gY),e(gY,xro),e(Dh,kro),e(Dh,tR),e(tR,Rro),e(Dh,Sro),e(F,Pro),e(F,jh),e(jh,hY),e(hY,$ro),e(jh,Iro),e(jh,aR),e(aR,Dro),e(jh,jro),e(F,Nro),e(F,Nh),e(Nh,pY),e(pY,qro),e(Nh,Gro),e(Nh,nR),e(nR,Oro),e(Nh,Xro),e(F,Vro),e(F,qh),e(qh,_Y),e(_Y,zro),e(qh,Wro),e(qh,sR),e(sR,Qro),e(qh,Hro),e(F,Uro),e(F,Gh),e(Gh,uY),e(uY,Jro),e(Gh,Yro),e(Gh,lR),e(lR,Kro),e(Gh,Zro),e(F,eto),e(F,Oh),e(Oh,bY),e(bY,oto),e(Oh,rto),e(Oh,iR),e(iR,tto),e(Oh,ato),e(F,nto),e(F,Xh),e(Xh,vY),e(vY,sto),e(Xh,lto),e(Xh,dR),e(dR,ito),e(Xh,dto),e(F,cto),e(F,Vh),e(Vh,TY),e(TY,fto),e(Vh,mto),e(Vh,cR),e(cR,gto),e(Vh,hto),e(F,pto),e(F,zh),e(zh,FY),e(FY,_to),e(zh,uto),e(zh,fR),e(fR,bto),e(zh,vto),e(F,Tto),e(F,Wh),e(Wh,CY),e(CY,Fto),e(Wh,Cto),e(Wh,mR),e(mR,Mto),e(Wh,Eto),e(F,yto),e(F,Qh),e(Qh,MY),e(MY,wto),e(Qh,Ato),e(Qh,gR),e(gR,Lto),e(Qh,Bto),e(F,xto),e(F,Hh),e(Hh,EY),e(EY,kto),e(Hh,Rto),e(Hh,hR),e(hR,Sto),e(Hh,Pto),e(F,$to),e(F,Uh),e(Uh,yY),e(yY,Ito),e(Uh,Dto),e(Uh,pR),e(pR,jto),e(Uh,Nto),e(F,qto),e(F,Jh),e(Jh,wY),e(wY,Gto),e(Jh,Oto),e(Jh,_R),e(_R,Xto),e(Jh,Vto),e(F,zto),e(F,Yh),e(Yh,AY),e(AY,Wto),e(Yh,Qto),e(Yh,uR),e(uR,Hto),e(Yh,Uto),e(F,Jto),e(F,Ss),e(Ss,LY),e(LY,Yto),e(Ss,Kto),e(Ss,bR),e(bR,Zto),e(Ss,eao),e(Ss,vR),e(vR,oao),e(Ss,rao),e(F,tao),e(F,Kh),e(Kh,BY),e(BY,aao),e(Kh,nao),e(Kh,TR),e(TR,sao),e(Kh,lao),e(F,iao),e(F,Zh),e(Zh,xY),e(xY,dao),e(Zh,cao),e(Zh,FR),e(FR,fao),e(Zh,mao),e(F,gao),e(F,ep),e(ep,kY),e(kY,hao),e(ep,pao),e(ep,CR),e(CR,_ao),e(ep,uao),e(F,bao),e(F,op),e(op,RY),e(RY,vao),e(op,Tao),e(op,MR),e(MR,Fao),e(op,Cao),e(F,Mao),e(F,rp),e(rp,SY),e(SY,Eao),e(rp,yao),e(rp,ER),e(ER,wao),e(rp,Aao),e(F,Lao),e(F,tp),e(tp,PY),e(PY,Bao),e(tp,xao),e(tp,yR),e(yR,kao),e(tp,Rao),e(F,Sao),e(F,ap),e(ap,$Y),e($Y,Pao),e(ap,$ao),e(ap,wR),e(wR,Iao),e(ap,Dao),e(F,jao),e(F,np),e(np,IY),e(IY,Nao),e(np,qao),e(np,AR),e(AR,Gao),e(np,Oao),e(F,Xao),e(F,sp),e(sp,DY),e(DY,Vao),e(sp,zao),e(sp,LR),e(LR,Wao),e(sp,Qao),e(F,Hao),e(F,lp),e(lp,jY),e(jY,Uao),e(lp,Jao),e(lp,BR),e(BR,Yao),e(lp,Kao),e(F,Zao),e(F,ip),e(ip,NY),e(NY,eno),e(ip,ono),e(ip,xR),e(xR,rno),e(ip,tno),e(F,ano),e(F,dp),e(dp,qY),e(qY,nno),e(dp,sno),e(dp,kR),e(kR,lno),e(dp,ino),e(F,dno),e(F,cp),e(cp,GY),e(GY,cno),e(cp,fno),e(cp,RR),e(RR,mno),e(cp,gno),e(F,hno),e(F,fp),e(fp,OY),e(OY,pno),e(fp,_no),e(fp,SR),e(SR,uno),e(fp,bno),e(F,vno),e(F,mp),e(mp,XY),e(XY,Tno),e(mp,Fno),e(mp,PR),e(PR,Cno),e(mp,Mno),e(F,Eno),e(F,gp),e(gp,VY),e(VY,yno),e(gp,wno),e(gp,$R),e($R,Ano),e(gp,Lno),e(F,Bno),e(F,hp),e(hp,zY),e(zY,xno),e(hp,kno),e(hp,IR),e(IR,Rno),e(hp,Sno),e(F,Pno),e(F,pp),e(pp,WY),e(WY,$no),e(pp,Ino),e(pp,DR),e(DR,Dno),e(pp,jno),e(F,Nno),e(F,_p),e(_p,QY),e(QY,qno),e(_p,Gno),e(_p,jR),e(jR,Ono),e(_p,Xno),e(F,Vno),e(F,up),e(up,HY),e(HY,zno),e(up,Wno),e(up,NR),e(NR,Qno),e(up,Hno),e(F,Uno),e(F,bp),e(bp,UY),e(UY,Jno),e(bp,Yno),e(bp,qR),e(qR,Kno),e(bp,Zno),e(F,eso),e(F,vp),e(vp,JY),e(JY,oso),e(vp,rso),e(vp,GR),e(GR,tso),e(vp,aso),e(F,nso),e(F,Tp),e(Tp,YY),e(YY,sso),e(Tp,lso),e(Tp,OR),e(OR,iso),e(Tp,dso),e(F,cso),e(F,Fp),e(Fp,KY),e(KY,fso),e(Fp,mso),e(Fp,XR),e(XR,gso),e(Fp,hso),e(F,pso),e(F,Cp),e(Cp,ZY),e(ZY,_so),e(Cp,uso),e(Cp,VR),e(VR,bso),e(Cp,vso),e(F,Tso),e(F,Mp),e(Mp,eK),e(eK,Fso),e(Mp,Cso),e(Mp,zR),e(zR,Mso),e(Mp,Eso),e(F,yso),e(F,Ep),e(Ep,oK),e(oK,wso),e(Ep,Aso),e(Ep,WR),e(WR,Lso),e(Ep,Bso),e(F,xso),e(F,yp),e(yp,rK),e(rK,kso),e(yp,Rso),e(yp,QR),e(QR,Sso),e(yp,Pso),e(F,$so),e(F,wp),e(wp,tK),e(tK,Iso),e(wp,Dso),e(wp,HR),e(HR,jso),e(wp,Nso),e(F,qso),e(F,Ap),e(Ap,aK),e(aK,Gso),e(Ap,Oso),e(Ap,UR),e(UR,Xso),e(Ap,Vso),e(F,zso),e(F,Lp),e(Lp,nK),e(nK,Wso),e(Lp,Qso),e(Lp,JR),e(JR,Hso),e(Lp,Uso),e(F,Jso),e(F,Bp),e(Bp,sK),e(sK,Yso),e(Bp,Kso),e(Bp,YR),e(YR,Zso),e(Bp,elo),e(F,olo),e(F,xp),e(xp,lK),e(lK,rlo),e(xp,tlo),e(xp,KR),e(KR,alo),e(xp,nlo),e(F,slo),e(F,kp),e(kp,iK),e(iK,llo),e(kp,ilo),e(kp,ZR),e(ZR,dlo),e(kp,clo),e(F,flo),e(F,Rp),e(Rp,dK),e(dK,mlo),e(Rp,glo),e(Rp,eS),e(eS,hlo),e(Rp,plo),e(F,_lo),e(F,Sp),e(Sp,cK),e(cK,ulo),e(Sp,blo),e(Sp,oS),e(oS,vlo),e(Sp,Tlo),e(F,Flo),e(F,Pp),e(Pp,fK),e(fK,Clo),e(Pp,Mlo),e(Pp,rS),e(rS,Elo),e(Pp,ylo),e(F,wlo),e(F,$p),e($p,mK),e(mK,Alo),e($p,Llo),e($p,tS),e(tS,Blo),e($p,xlo),e(F,klo),e(F,Ip),e(Ip,gK),e(gK,Rlo),e(Ip,Slo),e(Ip,aS),e(aS,Plo),e(Ip,$lo),e(F,Ilo),e(F,Dp),e(Dp,hK),e(hK,Dlo),e(Dp,jlo),e(Dp,nS),e(nS,Nlo),e(Dp,qlo),e(F,Glo),e(F,jp),e(jp,pK),e(pK,Olo),e(jp,Xlo),e(jp,sS),e(sS,Vlo),e(jp,zlo),e(F,Wlo),e(F,Np),e(Np,_K),e(_K,Qlo),e(Np,Hlo),e(Np,lS),e(lS,Ulo),e(Np,Jlo),e(F,Ylo),e(F,qp),e(qp,uK),e(uK,Klo),e(qp,Zlo),e(qp,iS),e(iS,eio),e(qp,oio),e(F,rio),e(F,Gp),e(Gp,bK),e(bK,tio),e(Gp,aio),e(Gp,dS),e(dS,nio),e(Gp,sio),e(F,lio),e(F,Op),e(Op,vK),e(vK,iio),e(Op,dio),e(Op,cS),e(cS,cio),e(Op,fio),e(F,mio),e(F,Xp),e(Xp,TK),e(TK,gio),e(Xp,hio),e(Xp,fS),e(fS,pio),e(Xp,_io),e(F,uio),e(F,Vp),e(Vp,FK),e(FK,bio),e(Vp,vio),e(Vp,mS),e(mS,Tio),e(Vp,Fio),e(F,Cio),e(F,zp),e(zp,CK),e(CK,Mio),e(zp,Eio),e(zp,gS),e(gS,yio),e(zp,wio),e(F,Aio),e(F,Wp),e(Wp,MK),e(MK,Lio),e(Wp,Bio),e(Wp,hS),e(hS,xio),e(Wp,kio),e(F,Rio),e(F,Qp),e(Qp,EK),e(EK,Sio),e(Qp,Pio),e(Qp,pS),e(pS,$io),e(Qp,Iio),e(F,Dio),e(F,Hp),e(Hp,yK),e(yK,jio),e(Hp,Nio),e(Hp,_S),e(_S,qio),e(Hp,Gio),e(F,Oio),e(F,Up),e(Up,wK),e(wK,Xio),e(Up,Vio),e(Up,uS),e(uS,zio),e(Up,Wio),e(F,Qio),e(F,Jp),e(Jp,AK),e(AK,Hio),e(Jp,Uio),e(Jp,bS),e(bS,Jio),e(Jp,Yio),e(F,Kio),e(F,Yp),e(Yp,LK),e(LK,Zio),e(Yp,edo),e(Yp,vS),e(vS,odo),e(Yp,rdo),e(F,tdo),e(F,Kp),e(Kp,BK),e(BK,ado),e(Kp,ndo),e(Kp,TS),e(TS,sdo),e(Kp,ldo),e(F,ido),e(F,Zp),e(Zp,xK),e(xK,ddo),e(Zp,cdo),e(Zp,FS),e(FS,fdo),e(Zp,mdo),e(F,gdo),e(F,e_),e(e_,kK),e(kK,hdo),e(e_,pdo),e(e_,CS),e(CS,_do),e(e_,udo),e(F,bdo),e(F,o_),e(o_,RK),e(RK,vdo),e(o_,Tdo),e(o_,MS),e(MS,Fdo),e(o_,Cdo),e(F,Mdo),e(F,r_),e(r_,SK),e(SK,Edo),e(r_,ydo),e(r_,ES),e(ES,wdo),e(r_,Ado),e(De,Ldo),e(De,t_),e(t_,Bdo),e(t_,PK),e(PK,xdo),e(t_,kdo),e(t_,$K),e($K,Rdo),e(De,Sdo),e(De,IK),e(IK,Pdo),e(De,$do),g(pE,De,null),b(c,hBe,u),b(c,zi,u),e(zi,a_),e(a_,DK),g(_E,DK,null),e(zi,Ido),e(zi,jK),e(jK,Ddo),b(c,pBe,u),b(c,Jo,u),g(uE,Jo,null),e(Jo,jdo),e(Jo,Wi),e(Wi,Ndo),e(Wi,NK),e(NK,qdo),e(Wi,Gdo),e(Wi,qK),e(qK,Odo),e(Wi,Xdo),e(Jo,Vdo),e(Jo,bE),e(bE,zdo),e(bE,GK),e(GK,Wdo),e(bE,Qdo),e(Jo,Hdo),e(Jo,Xr),g(vE,Xr,null),e(Xr,Udo),e(Xr,OK),e(OK,Jdo),e(Xr,Ydo),e(Xr,Qi),e(Qi,Kdo),e(Qi,XK),e(XK,Zdo),e(Qi,eco),e(Qi,VK),e(VK,oco),e(Qi,rco),e(Xr,tco),e(Xr,zK),e(zK,aco),e(Xr,nco),g(TE,Xr,null),e(Jo,sco),e(Jo,je),g(FE,je,null),e(je,lco),e(je,WK),e(WK,ico),e(je,dco),e(je,qa),e(qa,cco),e(qa,QK),e(QK,fco),e(qa,mco),e(qa,HK),e(HK,gco),e(qa,hco),e(qa,UK),e(UK,pco),e(qa,_co),e(je,uco),e(je,k),e(k,n_),e(n_,JK),e(JK,bco),e(n_,vco),e(n_,yS),e(yS,Tco),e(n_,Fco),e(k,Cco),e(k,s_),e(s_,YK),e(YK,Mco),e(s_,Eco),e(s_,wS),e(wS,yco),e(s_,wco),e(k,Aco),e(k,l_),e(l_,KK),e(KK,Lco),e(l_,Bco),e(l_,AS),e(AS,xco),e(l_,kco),e(k,Rco),e(k,i_),e(i_,ZK),e(ZK,Sco),e(i_,Pco),e(i_,LS),e(LS,$co),e(i_,Ico),e(k,Dco),e(k,d_),e(d_,eZ),e(eZ,jco),e(d_,Nco),e(d_,BS),e(BS,qco),e(d_,Gco),e(k,Oco),e(k,c_),e(c_,oZ),e(oZ,Xco),e(c_,Vco),e(c_,xS),e(xS,zco),e(c_,Wco),e(k,Qco),e(k,f_),e(f_,rZ),e(rZ,Hco),e(f_,Uco),e(f_,kS),e(kS,Jco),e(f_,Yco),e(k,Kco),e(k,m_),e(m_,tZ),e(tZ,Zco),e(m_,efo),e(m_,RS),e(RS,ofo),e(m_,rfo),e(k,tfo),e(k,g_),e(g_,aZ),e(aZ,afo),e(g_,nfo),e(g_,SS),e(SS,sfo),e(g_,lfo),e(k,ifo),e(k,h_),e(h_,nZ),e(nZ,dfo),e(h_,cfo),e(h_,PS),e(PS,ffo),e(h_,mfo),e(k,gfo),e(k,p_),e(p_,sZ),e(sZ,hfo),e(p_,pfo),e(p_,$S),e($S,_fo),e(p_,ufo),e(k,bfo),e(k,__),e(__,lZ),e(lZ,vfo),e(__,Tfo),e(__,IS),e(IS,Ffo),e(__,Cfo),e(k,Mfo),e(k,u_),e(u_,iZ),e(iZ,Efo),e(u_,yfo),e(u_,DS),e(DS,wfo),e(u_,Afo),e(k,Lfo),e(k,b_),e(b_,dZ),e(dZ,Bfo),e(b_,xfo),e(b_,jS),e(jS,kfo),e(b_,Rfo),e(k,Sfo),e(k,v_),e(v_,cZ),e(cZ,Pfo),e(v_,$fo),e(v_,NS),e(NS,Ifo),e(v_,Dfo),e(k,jfo),e(k,T_),e(T_,fZ),e(fZ,Nfo),e(T_,qfo),e(T_,qS),e(qS,Gfo),e(T_,Ofo),e(k,Xfo),e(k,F_),e(F_,mZ),e(mZ,Vfo),e(F_,zfo),e(F_,GS),e(GS,Wfo),e(F_,Qfo),e(k,Hfo),e(k,C_),e(C_,gZ),e(gZ,Ufo),e(C_,Jfo),e(C_,OS),e(OS,Yfo),e(C_,Kfo),e(k,Zfo),e(k,M_),e(M_,hZ),e(hZ,emo),e(M_,omo),e(M_,XS),e(XS,rmo),e(M_,tmo),e(k,amo),e(k,E_),e(E_,pZ),e(pZ,nmo),e(E_,smo),e(E_,VS),e(VS,lmo),e(E_,imo),e(k,dmo),e(k,y_),e(y_,_Z),e(_Z,cmo),e(y_,fmo),e(y_,zS),e(zS,mmo),e(y_,gmo),e(k,hmo),e(k,w_),e(w_,uZ),e(uZ,pmo),e(w_,_mo),e(w_,WS),e(WS,umo),e(w_,bmo),e(k,vmo),e(k,A_),e(A_,bZ),e(bZ,Tmo),e(A_,Fmo),e(A_,QS),e(QS,Cmo),e(A_,Mmo),e(k,Emo),e(k,L_),e(L_,vZ),e(vZ,ymo),e(L_,wmo),e(L_,HS),e(HS,Amo),e(L_,Lmo),e(k,Bmo),e(k,B_),e(B_,TZ),e(TZ,xmo),e(B_,kmo),e(B_,US),e(US,Rmo),e(B_,Smo),e(k,Pmo),e(k,x_),e(x_,FZ),e(FZ,$mo),e(x_,Imo),e(x_,JS),e(JS,Dmo),e(x_,jmo),e(k,Nmo),e(k,k_),e(k_,CZ),e(CZ,qmo),e(k_,Gmo),e(k_,YS),e(YS,Omo),e(k_,Xmo),e(k,Vmo),e(k,R_),e(R_,MZ),e(MZ,zmo),e(R_,Wmo),e(R_,KS),e(KS,Qmo),e(R_,Hmo),e(k,Umo),e(k,S_),e(S_,EZ),e(EZ,Jmo),e(S_,Ymo),e(S_,ZS),e(ZS,Kmo),e(S_,Zmo),e(k,ego),e(k,P_),e(P_,yZ),e(yZ,ogo),e(P_,rgo),e(P_,eP),e(eP,tgo),e(P_,ago),e(k,ngo),e(k,$_),e($_,wZ),e(wZ,sgo),e($_,lgo),e($_,oP),e(oP,igo),e($_,dgo),e(k,cgo),e(k,I_),e(I_,AZ),e(AZ,fgo),e(I_,mgo),e(I_,rP),e(rP,ggo),e(I_,hgo),e(k,pgo),e(k,D_),e(D_,LZ),e(LZ,_go),e(D_,ugo),e(D_,tP),e(tP,bgo),e(D_,vgo),e(k,Tgo),e(k,j_),e(j_,BZ),e(BZ,Fgo),e(j_,Cgo),e(j_,aP),e(aP,Mgo),e(j_,Ego),e(k,ygo),e(k,N_),e(N_,xZ),e(xZ,wgo),e(N_,Ago),e(N_,nP),e(nP,Lgo),e(N_,Bgo),e(k,xgo),e(k,q_),e(q_,kZ),e(kZ,kgo),e(q_,Rgo),e(q_,sP),e(sP,Sgo),e(q_,Pgo),e(k,$go),e(k,G_),e(G_,RZ),e(RZ,Igo),e(G_,Dgo),e(G_,lP),e(lP,jgo),e(G_,Ngo),e(k,qgo),e(k,O_),e(O_,SZ),e(SZ,Ggo),e(O_,Ogo),e(O_,iP),e(iP,Xgo),e(O_,Vgo),e(k,zgo),e(k,X_),e(X_,PZ),e(PZ,Wgo),e(X_,Qgo),e(X_,dP),e(dP,Hgo),e(X_,Ugo),e(je,Jgo),e(je,V_),e(V_,Ygo),e(V_,$Z),e($Z,Kgo),e(V_,Zgo),e(V_,IZ),e(IZ,eho),e(je,oho),e(je,DZ),e(DZ,rho),e(je,tho),g(CE,je,null),b(c,_Be,u),b(c,Hi,u),e(Hi,z_),e(z_,jZ),g(ME,jZ,null),e(Hi,aho),e(Hi,NZ),e(NZ,nho),b(c,uBe,u),b(c,Yo,u),g(EE,Yo,null),e(Yo,sho),e(Yo,Ui),e(Ui,lho),e(Ui,qZ),e(qZ,iho),e(Ui,dho),e(Ui,GZ),e(GZ,cho),e(Ui,fho),e(Yo,mho),e(Yo,yE),e(yE,gho),e(yE,OZ),e(OZ,hho),e(yE,pho),e(Yo,_ho),e(Yo,Vr),g(wE,Vr,null),e(Vr,uho),e(Vr,XZ),e(XZ,bho),e(Vr,vho),e(Vr,Ji),e(Ji,Tho),e(Ji,VZ),e(VZ,Fho),e(Ji,Cho),e(Ji,zZ),e(zZ,Mho),e(Ji,Eho),e(Vr,yho),e(Vr,WZ),e(WZ,who),e(Vr,Aho),g(AE,Vr,null),e(Yo,Lho),e(Yo,Ne),g(LE,Ne,null),e(Ne,Bho),e(Ne,QZ),e(QZ,xho),e(Ne,kho),e(Ne,Ga),e(Ga,Rho),e(Ga,HZ),e(HZ,Sho),e(Ga,Pho),e(Ga,UZ),e(UZ,$ho),e(Ga,Iho),e(Ga,JZ),e(JZ,Dho),e(Ga,jho),e(Ne,Nho),e(Ne,$),e($,W_),e(W_,YZ),e(YZ,qho),e(W_,Gho),e(W_,cP),e(cP,Oho),e(W_,Xho),e($,Vho),e($,Q_),e(Q_,KZ),e(KZ,zho),e(Q_,Who),e(Q_,fP),e(fP,Qho),e(Q_,Hho),e($,Uho),e($,H_),e(H_,ZZ),e(ZZ,Jho),e(H_,Yho),e(H_,mP),e(mP,Kho),e(H_,Zho),e($,epo),e($,U_),e(U_,eee),e(eee,opo),e(U_,rpo),e(U_,gP),e(gP,tpo),e(U_,apo),e($,npo),e($,J_),e(J_,oee),e(oee,spo),e(J_,lpo),e(J_,hP),e(hP,ipo),e(J_,dpo),e($,cpo),e($,Y_),e(Y_,ree),e(ree,fpo),e(Y_,mpo),e(Y_,pP),e(pP,gpo),e(Y_,hpo),e($,ppo),e($,K_),e(K_,tee),e(tee,_po),e(K_,upo),e(K_,_P),e(_P,bpo),e(K_,vpo),e($,Tpo),e($,Z_),e(Z_,aee),e(aee,Fpo),e(Z_,Cpo),e(Z_,uP),e(uP,Mpo),e(Z_,Epo),e($,ypo),e($,eu),e(eu,nee),e(nee,wpo),e(eu,Apo),e(eu,bP),e(bP,Lpo),e(eu,Bpo),e($,xpo),e($,ou),e(ou,see),e(see,kpo),e(ou,Rpo),e(ou,vP),e(vP,Spo),e(ou,Ppo),e($,$po),e($,ru),e(ru,lee),e(lee,Ipo),e(ru,Dpo),e(ru,TP),e(TP,jpo),e(ru,Npo),e($,qpo),e($,tu),e(tu,iee),e(iee,Gpo),e(tu,Opo),e(tu,FP),e(FP,Xpo),e(tu,Vpo),e($,zpo),e($,au),e(au,dee),e(dee,Wpo),e(au,Qpo),e(au,CP),e(CP,Hpo),e(au,Upo),e($,Jpo),e($,nu),e(nu,cee),e(cee,Ypo),e(nu,Kpo),e(nu,MP),e(MP,Zpo),e(nu,e_o),e($,o_o),e($,su),e(su,fee),e(fee,r_o),e(su,t_o),e(su,EP),e(EP,a_o),e(su,n_o),e($,s_o),e($,lu),e(lu,mee),e(mee,l_o),e(lu,i_o),e(lu,yP),e(yP,d_o),e(lu,c_o),e($,f_o),e($,iu),e(iu,gee),e(gee,m_o),e(iu,g_o),e(iu,wP),e(wP,h_o),e(iu,p_o),e($,__o),e($,du),e(du,hee),e(hee,u_o),e(du,b_o),e(du,AP),e(AP,v_o),e(du,T_o),e($,F_o),e($,cu),e(cu,pee),e(pee,C_o),e(cu,M_o),e(cu,LP),e(LP,E_o),e(cu,y_o),e($,w_o),e($,fu),e(fu,_ee),e(_ee,A_o),e(fu,L_o),e(fu,BP),e(BP,B_o),e(fu,x_o),e($,k_o),e($,mu),e(mu,uee),e(uee,R_o),e(mu,S_o),e(mu,xP),e(xP,P_o),e(mu,$_o),e($,I_o),e($,gu),e(gu,bee),e(bee,D_o),e(gu,j_o),e(gu,kP),e(kP,N_o),e(gu,q_o),e($,G_o),e($,hu),e(hu,vee),e(vee,O_o),e(hu,X_o),e(hu,RP),e(RP,V_o),e(hu,z_o),e($,W_o),e($,pu),e(pu,Tee),e(Tee,Q_o),e(pu,H_o),e(pu,SP),e(SP,U_o),e(pu,J_o),e($,Y_o),e($,_u),e(_u,Fee),e(Fee,K_o),e(_u,Z_o),e(_u,PP),e(PP,euo),e(_u,ouo),e($,ruo),e($,uu),e(uu,Cee),e(Cee,tuo),e(uu,auo),e(uu,$P),e($P,nuo),e(uu,suo),e($,luo),e($,bu),e(bu,Mee),e(Mee,iuo),e(bu,duo),e(bu,IP),e(IP,cuo),e(bu,fuo),e($,muo),e($,vu),e(vu,Eee),e(Eee,guo),e(vu,huo),e(vu,DP),e(DP,puo),e(vu,_uo),e($,uuo),e($,Tu),e(Tu,yee),e(yee,buo),e(Tu,vuo),e(Tu,jP),e(jP,Tuo),e(Tu,Fuo),e($,Cuo),e($,Fu),e(Fu,wee),e(wee,Muo),e(Fu,Euo),e(Fu,NP),e(NP,yuo),e(Fu,wuo),e($,Auo),e($,Cu),e(Cu,Aee),e(Aee,Luo),e(Cu,Buo),e(Cu,qP),e(qP,xuo),e(Cu,kuo),e($,Ruo),e($,Mu),e(Mu,Lee),e(Lee,Suo),e(Mu,Puo),e(Mu,GP),e(GP,$uo),e(Mu,Iuo),e($,Duo),e($,Eu),e(Eu,Bee),e(Bee,juo),e(Eu,Nuo),e(Eu,OP),e(OP,quo),e(Eu,Guo),e($,Ouo),e($,yu),e(yu,xee),e(xee,Xuo),e(yu,Vuo),e(yu,XP),e(XP,zuo),e(yu,Wuo),e($,Quo),e($,wu),e(wu,kee),e(kee,Huo),e(wu,Uuo),e(wu,VP),e(VP,Juo),e(wu,Yuo),e(Ne,Kuo),e(Ne,Au),e(Au,Zuo),e(Au,Ree),e(Ree,e0o),e(Au,o0o),e(Au,See),e(See,r0o),e(Ne,t0o),e(Ne,Pee),e(Pee,a0o),e(Ne,n0o),g(BE,Ne,null),b(c,bBe,u),b(c,Yi,u),e(Yi,Lu),e(Lu,$ee),g(xE,$ee,null),e(Yi,s0o),e(Yi,Iee),e(Iee,l0o),b(c,vBe,u),b(c,Ko,u),g(kE,Ko,null),e(Ko,i0o),e(Ko,Ki),e(Ki,d0o),e(Ki,Dee),e(Dee,c0o),e(Ki,f0o),e(Ki,jee),e(jee,m0o),e(Ki,g0o),e(Ko,h0o),e(Ko,RE),e(RE,p0o),e(RE,Nee),e(Nee,_0o),e(RE,u0o),e(Ko,b0o),e(Ko,zr),g(SE,zr,null),e(zr,v0o),e(zr,qee),e(qee,T0o),e(zr,F0o),e(zr,Zi),e(Zi,C0o),e(Zi,Gee),e(Gee,M0o),e(Zi,E0o),e(Zi,Oee),e(Oee,y0o),e(Zi,w0o),e(zr,A0o),e(zr,Xee),e(Xee,L0o),e(zr,B0o),g(PE,zr,null),e(Ko,x0o),e(Ko,qe),g($E,qe,null),e(qe,k0o),e(qe,Vee),e(Vee,R0o),e(qe,S0o),e(qe,Oa),e(Oa,P0o),e(Oa,zee),e(zee,$0o),e(Oa,I0o),e(Oa,Wee),e(Wee,D0o),e(Oa,j0o),e(Oa,Qee),e(Qee,N0o),e(Oa,q0o),e(qe,G0o),e(qe,I),e(I,Bu),e(Bu,Hee),e(Hee,O0o),e(Bu,X0o),e(Bu,zP),e(zP,V0o),e(Bu,z0o),e(I,W0o),e(I,xu),e(xu,Uee),e(Uee,Q0o),e(xu,H0o),e(xu,WP),e(WP,U0o),e(xu,J0o),e(I,Y0o),e(I,ku),e(ku,Jee),e(Jee,K0o),e(ku,Z0o),e(ku,QP),e(QP,e1o),e(ku,o1o),e(I,r1o),e(I,Ru),e(Ru,Yee),e(Yee,t1o),e(Ru,a1o),e(Ru,HP),e(HP,n1o),e(Ru,s1o),e(I,l1o),e(I,Su),e(Su,Kee),e(Kee,i1o),e(Su,d1o),e(Su,UP),e(UP,c1o),e(Su,f1o),e(I,m1o),e(I,Pu),e(Pu,Zee),e(Zee,g1o),e(Pu,h1o),e(Pu,JP),e(JP,p1o),e(Pu,_1o),e(I,u1o),e(I,$u),e($u,eoe),e(eoe,b1o),e($u,v1o),e($u,YP),e(YP,T1o),e($u,F1o),e(I,C1o),e(I,Iu),e(Iu,ooe),e(ooe,M1o),e(Iu,E1o),e(Iu,KP),e(KP,y1o),e(Iu,w1o),e(I,A1o),e(I,Du),e(Du,roe),e(roe,L1o),e(Du,B1o),e(Du,ZP),e(ZP,x1o),e(Du,k1o),e(I,R1o),e(I,ju),e(ju,toe),e(toe,S1o),e(ju,P1o),e(ju,e$),e(e$,$1o),e(ju,I1o),e(I,D1o),e(I,Nu),e(Nu,aoe),e(aoe,j1o),e(Nu,N1o),e(Nu,o$),e(o$,q1o),e(Nu,G1o),e(I,O1o),e(I,qu),e(qu,noe),e(noe,X1o),e(qu,V1o),e(qu,r$),e(r$,z1o),e(qu,W1o),e(I,Q1o),e(I,Gu),e(Gu,soe),e(soe,H1o),e(Gu,U1o),e(Gu,t$),e(t$,J1o),e(Gu,Y1o),e(I,K1o),e(I,Ou),e(Ou,loe),e(loe,Z1o),e(Ou,ebo),e(Ou,a$),e(a$,obo),e(Ou,rbo),e(I,tbo),e(I,Xu),e(Xu,ioe),e(ioe,abo),e(Xu,nbo),e(Xu,n$),e(n$,sbo),e(Xu,lbo),e(I,ibo),e(I,Vu),e(Vu,doe),e(doe,dbo),e(Vu,cbo),e(Vu,s$),e(s$,fbo),e(Vu,mbo),e(I,gbo),e(I,zu),e(zu,coe),e(coe,hbo),e(zu,pbo),e(zu,l$),e(l$,_bo),e(zu,ubo),e(I,bbo),e(I,Wu),e(Wu,foe),e(foe,vbo),e(Wu,Tbo),e(Wu,i$),e(i$,Fbo),e(Wu,Cbo),e(I,Mbo),e(I,Qu),e(Qu,moe),e(moe,Ebo),e(Qu,ybo),e(Qu,d$),e(d$,wbo),e(Qu,Abo),e(I,Lbo),e(I,Hu),e(Hu,goe),e(goe,Bbo),e(Hu,xbo),e(Hu,c$),e(c$,kbo),e(Hu,Rbo),e(I,Sbo),e(I,Uu),e(Uu,hoe),e(hoe,Pbo),e(Uu,$bo),e(Uu,f$),e(f$,Ibo),e(Uu,Dbo),e(I,jbo),e(I,Ju),e(Ju,poe),e(poe,Nbo),e(Ju,qbo),e(Ju,m$),e(m$,Gbo),e(Ju,Obo),e(I,Xbo),e(I,Yu),e(Yu,_oe),e(_oe,Vbo),e(Yu,zbo),e(Yu,g$),e(g$,Wbo),e(Yu,Qbo),e(I,Hbo),e(I,Ku),e(Ku,uoe),e(uoe,Ubo),e(Ku,Jbo),e(Ku,h$),e(h$,Ybo),e(Ku,Kbo),e(I,Zbo),e(I,Zu),e(Zu,boe),e(boe,e5o),e(Zu,o5o),e(Zu,p$),e(p$,r5o),e(Zu,t5o),e(I,a5o),e(I,e0),e(e0,voe),e(voe,n5o),e(e0,s5o),e(e0,_$),e(_$,l5o),e(e0,i5o),e(I,d5o),e(I,o0),e(o0,Toe),e(Toe,c5o),e(o0,f5o),e(o0,u$),e(u$,m5o),e(o0,g5o),e(I,h5o),e(I,r0),e(r0,Foe),e(Foe,p5o),e(r0,_5o),e(r0,b$),e(b$,u5o),e(r0,b5o),e(I,v5o),e(I,t0),e(t0,Coe),e(Coe,T5o),e(t0,F5o),e(t0,v$),e(v$,C5o),e(t0,M5o),e(I,E5o),e(I,a0),e(a0,Moe),e(Moe,y5o),e(a0,w5o),e(a0,T$),e(T$,A5o),e(a0,L5o),e(I,B5o),e(I,n0),e(n0,Eoe),e(Eoe,x5o),e(n0,k5o),e(n0,yoe),e(yoe,R5o),e(n0,S5o),e(I,P5o),e(I,s0),e(s0,woe),e(woe,$5o),e(s0,I5o),e(s0,F$),e(F$,D5o),e(s0,j5o),e(I,N5o),e(I,l0),e(l0,Aoe),e(Aoe,q5o),e(l0,G5o),e(l0,C$),e(C$,O5o),e(l0,X5o),e(I,V5o),e(I,i0),e(i0,Loe),e(Loe,z5o),e(i0,W5o),e(i0,M$),e(M$,Q5o),e(i0,H5o),e(I,U5o),e(I,d0),e(d0,Boe),e(Boe,J5o),e(d0,Y5o),e(d0,E$),e(E$,K5o),e(d0,Z5o),e(qe,e2o),e(qe,c0),e(c0,o2o),e(c0,xoe),e(xoe,r2o),e(c0,t2o),e(c0,koe),e(koe,a2o),e(qe,n2o),e(qe,Roe),e(Roe,s2o),e(qe,l2o),g(IE,qe,null),b(c,TBe,u),b(c,ed,u),e(ed,f0),e(f0,Soe),g(DE,Soe,null),e(ed,i2o),e(ed,Poe),e(Poe,d2o),b(c,FBe,u),b(c,Zo,u),g(jE,Zo,null),e(Zo,c2o),e(Zo,od),e(od,f2o),e(od,$oe),e($oe,m2o),e(od,g2o),e(od,Ioe),e(Ioe,h2o),e(od,p2o),e(Zo,_2o),e(Zo,NE),e(NE,u2o),e(NE,Doe),e(Doe,b2o),e(NE,v2o),e(Zo,T2o),e(Zo,Wr),g(qE,Wr,null),e(Wr,F2o),e(Wr,joe),e(joe,C2o),e(Wr,M2o),e(Wr,rd),e(rd,E2o),e(rd,Noe),e(Noe,y2o),e(rd,w2o),e(rd,qoe),e(qoe,A2o),e(rd,L2o),e(Wr,B2o),e(Wr,Goe),e(Goe,x2o),e(Wr,k2o),g(GE,Wr,null),e(Zo,R2o),e(Zo,Ge),g(OE,Ge,null),e(Ge,S2o),e(Ge,Ooe),e(Ooe,P2o),e(Ge,$2o),e(Ge,Xa),e(Xa,I2o),e(Xa,Xoe),e(Xoe,D2o),e(Xa,j2o),e(Xa,Voe),e(Voe,N2o),e(Xa,q2o),e(Xa,zoe),e(zoe,G2o),e(Xa,O2o),e(Ge,X2o),e(Ge,ae),e(ae,m0),e(m0,Woe),e(Woe,V2o),e(m0,z2o),e(m0,y$),e(y$,W2o),e(m0,Q2o),e(ae,H2o),e(ae,g0),e(g0,Qoe),e(Qoe,U2o),e(g0,J2o),e(g0,w$),e(w$,Y2o),e(g0,K2o),e(ae,Z2o),e(ae,h0),e(h0,Hoe),e(Hoe,evo),e(h0,ovo),e(h0,A$),e(A$,rvo),e(h0,tvo),e(ae,avo),e(ae,p0),e(p0,Uoe),e(Uoe,nvo),e(p0,svo),e(p0,L$),e(L$,lvo),e(p0,ivo),e(ae,dvo),e(ae,_0),e(_0,Joe),e(Joe,cvo),e(_0,fvo),e(_0,B$),e(B$,mvo),e(_0,gvo),e(ae,hvo),e(ae,u0),e(u0,Yoe),e(Yoe,pvo),e(u0,_vo),e(u0,x$),e(x$,uvo),e(u0,bvo),e(ae,vvo),e(ae,b0),e(b0,Koe),e(Koe,Tvo),e(b0,Fvo),e(b0,k$),e(k$,Cvo),e(b0,Mvo),e(ae,Evo),e(ae,v0),e(v0,Zoe),e(Zoe,yvo),e(v0,wvo),e(v0,R$),e(R$,Avo),e(v0,Lvo),e(ae,Bvo),e(ae,T0),e(T0,ere),e(ere,xvo),e(T0,kvo),e(T0,S$),e(S$,Rvo),e(T0,Svo),e(ae,Pvo),e(ae,F0),e(F0,ore),e(ore,$vo),e(F0,Ivo),e(F0,P$),e(P$,Dvo),e(F0,jvo),e(ae,Nvo),e(ae,C0),e(C0,rre),e(rre,qvo),e(C0,Gvo),e(C0,$$),e($$,Ovo),e(C0,Xvo),e(ae,Vvo),e(ae,M0),e(M0,tre),e(tre,zvo),e(M0,Wvo),e(M0,I$),e(I$,Qvo),e(M0,Hvo),e(ae,Uvo),e(ae,E0),e(E0,are),e(are,Jvo),e(E0,Yvo),e(E0,D$),e(D$,Kvo),e(E0,Zvo),e(ae,eTo),e(ae,y0),e(y0,nre),e(nre,oTo),e(y0,rTo),e(y0,j$),e(j$,tTo),e(y0,aTo),e(ae,nTo),e(ae,w0),e(w0,sre),e(sre,sTo),e(w0,lTo),e(w0,N$),e(N$,iTo),e(w0,dTo),e(ae,cTo),e(ae,A0),e(A0,lre),e(lre,fTo),e(A0,mTo),e(A0,q$),e(q$,gTo),e(A0,hTo),e(Ge,pTo),e(Ge,L0),e(L0,_To),e(L0,ire),e(ire,uTo),e(L0,bTo),e(L0,dre),e(dre,vTo),e(Ge,TTo),e(Ge,cre),e(cre,FTo),e(Ge,CTo),g(XE,Ge,null),b(c,CBe,u),b(c,td,u),e(td,B0),e(B0,fre),g(VE,fre,null),e(td,MTo),e(td,mre),e(mre,ETo),b(c,MBe,u),b(c,er,u),g(zE,er,null),e(er,yTo),e(er,ad),e(ad,wTo),e(ad,gre),e(gre,ATo),e(ad,LTo),e(ad,hre),e(hre,BTo),e(ad,xTo),e(er,kTo),e(er,WE),e(WE,RTo),e(WE,pre),e(pre,STo),e(WE,PTo),e(er,$To),e(er,Qr),g(QE,Qr,null),e(Qr,ITo),e(Qr,_re),e(_re,DTo),e(Qr,jTo),e(Qr,nd),e(nd,NTo),e(nd,ure),e(ure,qTo),e(nd,GTo),e(nd,bre),e(bre,OTo),e(nd,XTo),e(Qr,VTo),e(Qr,vre),e(vre,zTo),e(Qr,WTo),g(HE,Qr,null),e(er,QTo),e(er,Oe),g(UE,Oe,null),e(Oe,HTo),e(Oe,Tre),e(Tre,UTo),e(Oe,JTo),e(Oe,Va),e(Va,YTo),e(Va,Fre),e(Fre,KTo),e(Va,ZTo),e(Va,Cre),e(Cre,eFo),e(Va,oFo),e(Va,Mre),e(Mre,rFo),e(Va,tFo),e(Oe,aFo),e(Oe,A),e(A,x0),e(x0,Ere),e(Ere,nFo),e(x0,sFo),e(x0,G$),e(G$,lFo),e(x0,iFo),e(A,dFo),e(A,k0),e(k0,yre),e(yre,cFo),e(k0,fFo),e(k0,O$),e(O$,mFo),e(k0,gFo),e(A,hFo),e(A,R0),e(R0,wre),e(wre,pFo),e(R0,_Fo),e(R0,X$),e(X$,uFo),e(R0,bFo),e(A,vFo),e(A,S0),e(S0,Are),e(Are,TFo),e(S0,FFo),e(S0,V$),e(V$,CFo),e(S0,MFo),e(A,EFo),e(A,P0),e(P0,Lre),e(Lre,yFo),e(P0,wFo),e(P0,z$),e(z$,AFo),e(P0,LFo),e(A,BFo),e(A,$0),e($0,Bre),e(Bre,xFo),e($0,kFo),e($0,W$),e(W$,RFo),e($0,SFo),e(A,PFo),e(A,I0),e(I0,xre),e(xre,$Fo),e(I0,IFo),e(I0,Q$),e(Q$,DFo),e(I0,jFo),e(A,NFo),e(A,D0),e(D0,kre),e(kre,qFo),e(D0,GFo),e(D0,H$),e(H$,OFo),e(D0,XFo),e(A,VFo),e(A,j0),e(j0,Rre),e(Rre,zFo),e(j0,WFo),e(j0,U$),e(U$,QFo),e(j0,HFo),e(A,UFo),e(A,N0),e(N0,Sre),e(Sre,JFo),e(N0,YFo),e(N0,J$),e(J$,KFo),e(N0,ZFo),e(A,e9o),e(A,q0),e(q0,Pre),e(Pre,o9o),e(q0,r9o),e(q0,Y$),e(Y$,t9o),e(q0,a9o),e(A,n9o),e(A,G0),e(G0,$re),e($re,s9o),e(G0,l9o),e(G0,K$),e(K$,i9o),e(G0,d9o),e(A,c9o),e(A,O0),e(O0,Ire),e(Ire,f9o),e(O0,m9o),e(O0,Z$),e(Z$,g9o),e(O0,h9o),e(A,p9o),e(A,X0),e(X0,Dre),e(Dre,_9o),e(X0,u9o),e(X0,eI),e(eI,b9o),e(X0,v9o),e(A,T9o),e(A,V0),e(V0,jre),e(jre,F9o),e(V0,C9o),e(V0,oI),e(oI,M9o),e(V0,E9o),e(A,y9o),e(A,z0),e(z0,Nre),e(Nre,w9o),e(z0,A9o),e(z0,rI),e(rI,L9o),e(z0,B9o),e(A,x9o),e(A,W0),e(W0,qre),e(qre,k9o),e(W0,R9o),e(W0,tI),e(tI,S9o),e(W0,P9o),e(A,$9o),e(A,Q0),e(Q0,Gre),e(Gre,I9o),e(Q0,D9o),e(Q0,aI),e(aI,j9o),e(Q0,N9o),e(A,q9o),e(A,H0),e(H0,Ore),e(Ore,G9o),e(H0,O9o),e(H0,nI),e(nI,X9o),e(H0,V9o),e(A,z9o),e(A,U0),e(U0,Xre),e(Xre,W9o),e(U0,Q9o),e(U0,sI),e(sI,H9o),e(U0,U9o),e(A,J9o),e(A,J0),e(J0,Vre),e(Vre,Y9o),e(J0,K9o),e(J0,lI),e(lI,Z9o),e(J0,eCo),e(A,oCo),e(A,Y0),e(Y0,zre),e(zre,rCo),e(Y0,tCo),e(Y0,iI),e(iI,aCo),e(Y0,nCo),e(A,sCo),e(A,K0),e(K0,Wre),e(Wre,lCo),e(K0,iCo),e(K0,dI),e(dI,dCo),e(K0,cCo),e(A,fCo),e(A,Z0),e(Z0,Qre),e(Qre,mCo),e(Z0,gCo),e(Z0,cI),e(cI,hCo),e(Z0,pCo),e(A,_Co),e(A,e1),e(e1,Hre),e(Hre,uCo),e(e1,bCo),e(e1,fI),e(fI,vCo),e(e1,TCo),e(A,FCo),e(A,o1),e(o1,Ure),e(Ure,CCo),e(o1,MCo),e(o1,mI),e(mI,ECo),e(o1,yCo),e(A,wCo),e(A,r1),e(r1,Jre),e(Jre,ACo),e(r1,LCo),e(r1,gI),e(gI,BCo),e(r1,xCo),e(A,kCo),e(A,t1),e(t1,Yre),e(Yre,RCo),e(t1,SCo),e(t1,hI),e(hI,PCo),e(t1,$Co),e(A,ICo),e(A,a1),e(a1,Kre),e(Kre,DCo),e(a1,jCo),e(a1,pI),e(pI,NCo),e(a1,qCo),e(A,GCo),e(A,n1),e(n1,Zre),e(Zre,OCo),e(n1,XCo),e(n1,_I),e(_I,VCo),e(n1,zCo),e(A,WCo),e(A,s1),e(s1,ete),e(ete,QCo),e(s1,HCo),e(s1,uI),e(uI,UCo),e(s1,JCo),e(A,YCo),e(A,l1),e(l1,ote),e(ote,KCo),e(l1,ZCo),e(l1,bI),e(bI,eMo),e(l1,oMo),e(A,rMo),e(A,i1),e(i1,rte),e(rte,tMo),e(i1,aMo),e(i1,vI),e(vI,nMo),e(i1,sMo),e(A,lMo),e(A,d1),e(d1,tte),e(tte,iMo),e(d1,dMo),e(d1,TI),e(TI,cMo),e(d1,fMo),e(A,mMo),e(A,c1),e(c1,ate),e(ate,gMo),e(c1,hMo),e(c1,FI),e(FI,pMo),e(c1,_Mo),e(A,uMo),e(A,f1),e(f1,nte),e(nte,bMo),e(f1,vMo),e(f1,CI),e(CI,TMo),e(f1,FMo),e(A,CMo),e(A,m1),e(m1,ste),e(ste,MMo),e(m1,EMo),e(m1,MI),e(MI,yMo),e(m1,wMo),e(A,AMo),e(A,g1),e(g1,lte),e(lte,LMo),e(g1,BMo),e(g1,EI),e(EI,xMo),e(g1,kMo),e(A,RMo),e(A,h1),e(h1,ite),e(ite,SMo),e(h1,PMo),e(h1,yI),e(yI,$Mo),e(h1,IMo),e(A,DMo),e(A,p1),e(p1,dte),e(dte,jMo),e(p1,NMo),e(p1,wI),e(wI,qMo),e(p1,GMo),e(A,OMo),e(A,_1),e(_1,cte),e(cte,XMo),e(_1,VMo),e(_1,AI),e(AI,zMo),e(_1,WMo),e(A,QMo),e(A,u1),e(u1,fte),e(fte,HMo),e(u1,UMo),e(u1,LI),e(LI,JMo),e(u1,YMo),e(A,KMo),e(A,b1),e(b1,mte),e(mte,ZMo),e(b1,e4o),e(b1,BI),e(BI,o4o),e(b1,r4o),e(A,t4o),e(A,v1),e(v1,gte),e(gte,a4o),e(v1,n4o),e(v1,xI),e(xI,s4o),e(v1,l4o),e(A,i4o),e(A,T1),e(T1,hte),e(hte,d4o),e(T1,c4o),e(T1,kI),e(kI,f4o),e(T1,m4o),e(A,g4o),e(A,F1),e(F1,pte),e(pte,h4o),e(F1,p4o),e(F1,RI),e(RI,_4o),e(F1,u4o),e(Oe,b4o),e(Oe,C1),e(C1,v4o),e(C1,_te),e(_te,T4o),e(C1,F4o),e(C1,ute),e(ute,C4o),e(Oe,M4o),e(Oe,bte),e(bte,E4o),e(Oe,y4o),g(JE,Oe,null),b(c,EBe,u),b(c,sd,u),e(sd,M1),e(M1,vte),g(YE,vte,null),e(sd,w4o),e(sd,Tte),e(Tte,A4o),b(c,yBe,u),b(c,or,u),g(KE,or,null),e(or,L4o),e(or,ld),e(ld,B4o),e(ld,Fte),e(Fte,x4o),e(ld,k4o),e(ld,Cte),e(Cte,R4o),e(ld,S4o),e(or,P4o),e(or,ZE),e(ZE,$4o),e(ZE,Mte),e(Mte,I4o),e(ZE,D4o),e(or,j4o),e(or,Hr),g(e3,Hr,null),e(Hr,N4o),e(Hr,Ete),e(Ete,q4o),e(Hr,G4o),e(Hr,id),e(id,O4o),e(id,yte),e(yte,X4o),e(id,V4o),e(id,wte),e(wte,z4o),e(id,W4o),e(Hr,Q4o),e(Hr,Ate),e(Ate,H4o),e(Hr,U4o),g(o3,Hr,null),e(or,J4o),e(or,Xe),g(r3,Xe,null),e(Xe,Y4o),e(Xe,Lte),e(Lte,K4o),e(Xe,Z4o),e(Xe,za),e(za,eEo),e(za,Bte),e(Bte,oEo),e(za,rEo),e(za,xte),e(xte,tEo),e(za,aEo),e(za,kte),e(kte,nEo),e(za,sEo),e(Xe,lEo),e(Xe,G),e(G,E1),e(E1,Rte),e(Rte,iEo),e(E1,dEo),e(E1,SI),e(SI,cEo),e(E1,fEo),e(G,mEo),e(G,y1),e(y1,Ste),e(Ste,gEo),e(y1,hEo),e(y1,PI),e(PI,pEo),e(y1,_Eo),e(G,uEo),e(G,w1),e(w1,Pte),e(Pte,bEo),e(w1,vEo),e(w1,$I),e($I,TEo),e(w1,FEo),e(G,CEo),e(G,A1),e(A1,$te),e($te,MEo),e(A1,EEo),e(A1,II),e(II,yEo),e(A1,wEo),e(G,AEo),e(G,L1),e(L1,Ite),e(Ite,LEo),e(L1,BEo),e(L1,DI),e(DI,xEo),e(L1,kEo),e(G,REo),e(G,B1),e(B1,Dte),e(Dte,SEo),e(B1,PEo),e(B1,jI),e(jI,$Eo),e(B1,IEo),e(G,DEo),e(G,x1),e(x1,jte),e(jte,jEo),e(x1,NEo),e(x1,NI),e(NI,qEo),e(x1,GEo),e(G,OEo),e(G,k1),e(k1,Nte),e(Nte,XEo),e(k1,VEo),e(k1,qI),e(qI,zEo),e(k1,WEo),e(G,QEo),e(G,R1),e(R1,qte),e(qte,HEo),e(R1,UEo),e(R1,GI),e(GI,JEo),e(R1,YEo),e(G,KEo),e(G,S1),e(S1,Gte),e(Gte,ZEo),e(S1,e3o),e(S1,OI),e(OI,o3o),e(S1,r3o),e(G,t3o),e(G,P1),e(P1,Ote),e(Ote,a3o),e(P1,n3o),e(P1,XI),e(XI,s3o),e(P1,l3o),e(G,i3o),e(G,$1),e($1,Xte),e(Xte,d3o),e($1,c3o),e($1,VI),e(VI,f3o),e($1,m3o),e(G,g3o),e(G,I1),e(I1,Vte),e(Vte,h3o),e(I1,p3o),e(I1,zI),e(zI,_3o),e(I1,u3o),e(G,b3o),e(G,D1),e(D1,zte),e(zte,v3o),e(D1,T3o),e(D1,WI),e(WI,F3o),e(D1,C3o),e(G,M3o),e(G,j1),e(j1,Wte),e(Wte,E3o),e(j1,y3o),e(j1,QI),e(QI,w3o),e(j1,A3o),e(G,L3o),e(G,N1),e(N1,Qte),e(Qte,B3o),e(N1,x3o),e(N1,HI),e(HI,k3o),e(N1,R3o),e(G,S3o),e(G,q1),e(q1,Hte),e(Hte,P3o),e(q1,$3o),e(q1,UI),e(UI,I3o),e(q1,D3o),e(G,j3o),e(G,G1),e(G1,Ute),e(Ute,N3o),e(G1,q3o),e(G1,JI),e(JI,G3o),e(G1,O3o),e(G,X3o),e(G,O1),e(O1,Jte),e(Jte,V3o),e(O1,z3o),e(O1,YI),e(YI,W3o),e(O1,Q3o),e(G,H3o),e(G,X1),e(X1,Yte),e(Yte,U3o),e(X1,J3o),e(X1,KI),e(KI,Y3o),e(X1,K3o),e(G,Z3o),e(G,V1),e(V1,Kte),e(Kte,eyo),e(V1,oyo),e(V1,ZI),e(ZI,ryo),e(V1,tyo),e(G,ayo),e(G,z1),e(z1,Zte),e(Zte,nyo),e(z1,syo),e(z1,eD),e(eD,lyo),e(z1,iyo),e(G,dyo),e(G,W1),e(W1,eae),e(eae,cyo),e(W1,fyo),e(W1,oD),e(oD,myo),e(W1,gyo),e(G,hyo),e(G,Q1),e(Q1,oae),e(oae,pyo),e(Q1,_yo),e(Q1,rD),e(rD,uyo),e(Q1,byo),e(G,vyo),e(G,H1),e(H1,rae),e(rae,Tyo),e(H1,Fyo),e(H1,tD),e(tD,Cyo),e(H1,Myo),e(G,Eyo),e(G,U1),e(U1,tae),e(tae,yyo),e(U1,wyo),e(U1,aD),e(aD,Ayo),e(U1,Lyo),e(G,Byo),e(G,J1),e(J1,aae),e(aae,xyo),e(J1,kyo),e(J1,nD),e(nD,Ryo),e(J1,Syo),e(G,Pyo),e(G,Y1),e(Y1,nae),e(nae,$yo),e(Y1,Iyo),e(Y1,sD),e(sD,Dyo),e(Y1,jyo),e(Xe,Nyo),e(Xe,K1),e(K1,qyo),e(K1,sae),e(sae,Gyo),e(K1,Oyo),e(K1,lae),e(lae,Xyo),e(Xe,Vyo),e(Xe,iae),e(iae,zyo),e(Xe,Wyo),g(t3,Xe,null),b(c,wBe,u),b(c,dd,u),e(dd,Z1),e(Z1,dae),g(a3,dae,null),e(dd,Qyo),e(dd,cae),e(cae,Hyo),b(c,ABe,u),b(c,rr,u),g(n3,rr,null),e(rr,Uyo),e(rr,cd),e(cd,Jyo),e(cd,fae),e(fae,Yyo),e(cd,Kyo),e(cd,mae),e(mae,Zyo),e(cd,ewo),e(rr,owo),e(rr,s3),e(s3,rwo),e(s3,gae),e(gae,two),e(s3,awo),e(rr,nwo),e(rr,Ur),g(l3,Ur,null),e(Ur,swo),e(Ur,hae),e(hae,lwo),e(Ur,iwo),e(Ur,fd),e(fd,dwo),e(fd,pae),e(pae,cwo),e(fd,fwo),e(fd,_ae),e(_ae,mwo),e(fd,gwo),e(Ur,hwo),e(Ur,uae),e(uae,pwo),e(Ur,_wo),g(i3,Ur,null),e(rr,uwo),e(rr,Ve),g(d3,Ve,null),e(Ve,bwo),e(Ve,bae),e(bae,vwo),e(Ve,Two),e(Ve,Wa),e(Wa,Fwo),e(Wa,vae),e(vae,Cwo),e(Wa,Mwo),e(Wa,Tae),e(Tae,Ewo),e(Wa,ywo),e(Wa,Fae),e(Fae,wwo),e(Wa,Awo),e(Ve,Lwo),e(Ve,na),e(na,eb),e(eb,Cae),e(Cae,Bwo),e(eb,xwo),e(eb,lD),e(lD,kwo),e(eb,Rwo),e(na,Swo),e(na,ob),e(ob,Mae),e(Mae,Pwo),e(ob,$wo),e(ob,iD),e(iD,Iwo),e(ob,Dwo),e(na,jwo),e(na,rb),e(rb,Eae),e(Eae,Nwo),e(rb,qwo),e(rb,dD),e(dD,Gwo),e(rb,Owo),e(na,Xwo),e(na,tb),e(tb,yae),e(yae,Vwo),e(tb,zwo),e(tb,cD),e(cD,Wwo),e(tb,Qwo),e(na,Hwo),e(na,ab),e(ab,wae),e(wae,Uwo),e(ab,Jwo),e(ab,fD),e(fD,Ywo),e(ab,Kwo),e(Ve,Zwo),e(Ve,nb),e(nb,e6o),e(nb,Aae),e(Aae,o6o),e(nb,r6o),e(nb,Lae),e(Lae,t6o),e(Ve,a6o),e(Ve,Bae),e(Bae,n6o),e(Ve,s6o),g(c3,Ve,null),b(c,LBe,u),b(c,md,u),e(md,sb),e(sb,xae),g(f3,xae,null),e(md,l6o),e(md,kae),e(kae,i6o),b(c,BBe,u),b(c,tr,u),g(m3,tr,null),e(tr,d6o),e(tr,gd),e(gd,c6o),e(gd,Rae),e(Rae,f6o),e(gd,m6o),e(gd,Sae),e(Sae,g6o),e(gd,h6o),e(tr,p6o),e(tr,g3),e(g3,_6o),e(g3,Pae),e(Pae,u6o),e(g3,b6o),e(tr,v6o),e(tr,Jr),g(h3,Jr,null),e(Jr,T6o),e(Jr,$ae),e($ae,F6o),e(Jr,C6o),e(Jr,hd),e(hd,M6o),e(hd,Iae),e(Iae,E6o),e(hd,y6o),e(hd,Dae),e(Dae,w6o),e(hd,A6o),e(Jr,L6o),e(Jr,jae),e(jae,B6o),e(Jr,x6o),g(p3,Jr,null),e(tr,k6o),e(tr,ze),g(_3,ze,null),e(ze,R6o),e(ze,Nae),e(Nae,S6o),e(ze,P6o),e(ze,Qa),e(Qa,$6o),e(Qa,qae),e(qae,I6o),e(Qa,D6o),e(Qa,Gae),e(Gae,j6o),e(Qa,N6o),e(Qa,Oae),e(Oae,q6o),e(Qa,G6o),e(ze,O6o),e(ze,N),e(N,lb),e(lb,Xae),e(Xae,X6o),e(lb,V6o),e(lb,mD),e(mD,z6o),e(lb,W6o),e(N,Q6o),e(N,ib),e(ib,Vae),e(Vae,H6o),e(ib,U6o),e(ib,gD),e(gD,J6o),e(ib,Y6o),e(N,K6o),e(N,db),e(db,zae),e(zae,Z6o),e(db,eAo),e(db,hD),e(hD,oAo),e(db,rAo),e(N,tAo),e(N,cb),e(cb,Wae),e(Wae,aAo),e(cb,nAo),e(cb,pD),e(pD,sAo),e(cb,lAo),e(N,iAo),e(N,fb),e(fb,Qae),e(Qae,dAo),e(fb,cAo),e(fb,_D),e(_D,fAo),e(fb,mAo),e(N,gAo),e(N,mb),e(mb,Hae),e(Hae,hAo),e(mb,pAo),e(mb,uD),e(uD,_Ao),e(mb,uAo),e(N,bAo),e(N,gb),e(gb,Uae),e(Uae,vAo),e(gb,TAo),e(gb,bD),e(bD,FAo),e(gb,CAo),e(N,MAo),e(N,hb),e(hb,Jae),e(Jae,EAo),e(hb,yAo),e(hb,vD),e(vD,wAo),e(hb,AAo),e(N,LAo),e(N,pb),e(pb,Yae),e(Yae,BAo),e(pb,xAo),e(pb,TD),e(TD,kAo),e(pb,RAo),e(N,SAo),e(N,_b),e(_b,Kae),e(Kae,PAo),e(_b,$Ao),e(_b,FD),e(FD,IAo),e(_b,DAo),e(N,jAo),e(N,ub),e(ub,Zae),e(Zae,NAo),e(ub,qAo),e(ub,CD),e(CD,GAo),e(ub,OAo),e(N,XAo),e(N,bb),e(bb,ene),e(ene,VAo),e(bb,zAo),e(bb,MD),e(MD,WAo),e(bb,QAo),e(N,HAo),e(N,vb),e(vb,one),e(one,UAo),e(vb,JAo),e(vb,ED),e(ED,YAo),e(vb,KAo),e(N,ZAo),e(N,Tb),e(Tb,rne),e(rne,eLo),e(Tb,oLo),e(Tb,yD),e(yD,rLo),e(Tb,tLo),e(N,aLo),e(N,Fb),e(Fb,tne),e(tne,nLo),e(Fb,sLo),e(Fb,wD),e(wD,lLo),e(Fb,iLo),e(N,dLo),e(N,Cb),e(Cb,ane),e(ane,cLo),e(Cb,fLo),e(Cb,AD),e(AD,mLo),e(Cb,gLo),e(N,hLo),e(N,Mb),e(Mb,nne),e(nne,pLo),e(Mb,_Lo),e(Mb,LD),e(LD,uLo),e(Mb,bLo),e(N,vLo),e(N,Eb),e(Eb,sne),e(sne,TLo),e(Eb,FLo),e(Eb,BD),e(BD,CLo),e(Eb,MLo),e(N,ELo),e(N,yb),e(yb,lne),e(lne,yLo),e(yb,wLo),e(yb,xD),e(xD,ALo),e(yb,LLo),e(N,BLo),e(N,wb),e(wb,ine),e(ine,xLo),e(wb,kLo),e(wb,kD),e(kD,RLo),e(wb,SLo),e(N,PLo),e(N,Ab),e(Ab,dne),e(dne,$Lo),e(Ab,ILo),e(Ab,RD),e(RD,DLo),e(Ab,jLo),e(N,NLo),e(N,Lb),e(Lb,cne),e(cne,qLo),e(Lb,GLo),e(Lb,SD),e(SD,OLo),e(Lb,XLo),e(N,VLo),e(N,Bb),e(Bb,fne),e(fne,zLo),e(Bb,WLo),e(Bb,PD),e(PD,QLo),e(Bb,HLo),e(N,ULo),e(N,xb),e(xb,mne),e(mne,JLo),e(xb,YLo),e(xb,$D),e($D,KLo),e(xb,ZLo),e(N,e8o),e(N,kb),e(kb,gne),e(gne,o8o),e(kb,r8o),e(kb,ID),e(ID,t8o),e(kb,a8o),e(N,n8o),e(N,Rb),e(Rb,hne),e(hne,s8o),e(Rb,l8o),e(Rb,DD),e(DD,i8o),e(Rb,d8o),e(N,c8o),e(N,Sb),e(Sb,pne),e(pne,f8o),e(Sb,m8o),e(Sb,jD),e(jD,g8o),e(Sb,h8o),e(N,p8o),e(N,Pb),e(Pb,_ne),e(_ne,_8o),e(Pb,u8o),e(Pb,ND),e(ND,b8o),e(Pb,v8o),e(N,T8o),e(N,$b),e($b,une),e(une,F8o),e($b,C8o),e($b,qD),e(qD,M8o),e($b,E8o),e(N,y8o),e(N,Ib),e(Ib,bne),e(bne,w8o),e(Ib,A8o),e(Ib,GD),e(GD,L8o),e(Ib,B8o),e(N,x8o),e(N,Db),e(Db,vne),e(vne,k8o),e(Db,R8o),e(Db,OD),e(OD,S8o),e(Db,P8o),e(N,$8o),e(N,jb),e(jb,Tne),e(Tne,I8o),e(jb,D8o),e(jb,XD),e(XD,j8o),e(jb,N8o),e(N,q8o),e(N,Nb),e(Nb,Fne),e(Fne,G8o),e(Nb,O8o),e(Nb,VD),e(VD,X8o),e(Nb,V8o),e(ze,z8o),e(ze,qb),e(qb,W8o),e(qb,Cne),e(Cne,Q8o),e(qb,H8o),e(qb,Mne),e(Mne,U8o),e(ze,J8o),e(ze,Ene),e(Ene,Y8o),e(ze,K8o),g(u3,ze,null),b(c,xBe,u),b(c,pd,u),e(pd,Gb),e(Gb,yne),g(b3,yne,null),e(pd,Z8o),e(pd,wne),e(wne,e7o),b(c,kBe,u),b(c,ar,u),g(v3,ar,null),e(ar,o7o),e(ar,_d),e(_d,r7o),e(_d,Ane),e(Ane,t7o),e(_d,a7o),e(_d,Lne),e(Lne,n7o),e(_d,s7o),e(ar,l7o),e(ar,T3),e(T3,i7o),e(T3,Bne),e(Bne,d7o),e(T3,c7o),e(ar,f7o),e(ar,Yr),g(F3,Yr,null),e(Yr,m7o),e(Yr,xne),e(xne,g7o),e(Yr,h7o),e(Yr,ud),e(ud,p7o),e(ud,kne),e(kne,_7o),e(ud,u7o),e(ud,Rne),e(Rne,b7o),e(ud,v7o),e(Yr,T7o),e(Yr,Sne),e(Sne,F7o),e(Yr,C7o),g(C3,Yr,null),e(ar,M7o),e(ar,We),g(M3,We,null),e(We,E7o),e(We,Pne),e(Pne,y7o),e(We,w7o),e(We,Ha),e(Ha,A7o),e(Ha,$ne),e($ne,L7o),e(Ha,B7o),e(Ha,Ine),e(Ine,x7o),e(Ha,k7o),e(Ha,Dne),e(Dne,R7o),e(Ha,S7o),e(We,P7o),e(We,R),e(R,Ob),e(Ob,jne),e(jne,$7o),e(Ob,I7o),e(Ob,zD),e(zD,D7o),e(Ob,j7o),e(R,N7o),e(R,Xb),e(Xb,Nne),e(Nne,q7o),e(Xb,G7o),e(Xb,WD),e(WD,O7o),e(Xb,X7o),e(R,V7o),e(R,Vb),e(Vb,qne),e(qne,z7o),e(Vb,W7o),e(Vb,QD),e(QD,Q7o),e(Vb,H7o),e(R,U7o),e(R,zb),e(zb,Gne),e(Gne,J7o),e(zb,Y7o),e(zb,HD),e(HD,K7o),e(zb,Z7o),e(R,eBo),e(R,Wb),e(Wb,One),e(One,oBo),e(Wb,rBo),e(Wb,UD),e(UD,tBo),e(Wb,aBo),e(R,nBo),e(R,Qb),e(Qb,Xne),e(Xne,sBo),e(Qb,lBo),e(Qb,JD),e(JD,iBo),e(Qb,dBo),e(R,cBo),e(R,Hb),e(Hb,Vne),e(Vne,fBo),e(Hb,mBo),e(Hb,YD),e(YD,gBo),e(Hb,hBo),e(R,pBo),e(R,Ub),e(Ub,zne),e(zne,_Bo),e(Ub,uBo),e(Ub,KD),e(KD,bBo),e(Ub,vBo),e(R,TBo),e(R,Jb),e(Jb,Wne),e(Wne,FBo),e(Jb,CBo),e(Jb,ZD),e(ZD,MBo),e(Jb,EBo),e(R,yBo),e(R,Yb),e(Yb,Qne),e(Qne,wBo),e(Yb,ABo),e(Yb,ej),e(ej,LBo),e(Yb,BBo),e(R,xBo),e(R,Kb),e(Kb,Hne),e(Hne,kBo),e(Kb,RBo),e(Kb,oj),e(oj,SBo),e(Kb,PBo),e(R,$Bo),e(R,Zb),e(Zb,Une),e(Une,IBo),e(Zb,DBo),e(Zb,rj),e(rj,jBo),e(Zb,NBo),e(R,qBo),e(R,e5),e(e5,Jne),e(Jne,GBo),e(e5,OBo),e(e5,tj),e(tj,XBo),e(e5,VBo),e(R,zBo),e(R,o5),e(o5,Yne),e(Yne,WBo),e(o5,QBo),e(o5,aj),e(aj,HBo),e(o5,UBo),e(R,JBo),e(R,r5),e(r5,Kne),e(Kne,YBo),e(r5,KBo),e(r5,nj),e(nj,ZBo),e(r5,exo),e(R,oxo),e(R,t5),e(t5,Zne),e(Zne,rxo),e(t5,txo),e(t5,sj),e(sj,axo),e(t5,nxo),e(R,sxo),e(R,a5),e(a5,ese),e(ese,lxo),e(a5,ixo),e(a5,lj),e(lj,dxo),e(a5,cxo),e(R,fxo),e(R,n5),e(n5,ose),e(ose,mxo),e(n5,gxo),e(n5,ij),e(ij,hxo),e(n5,pxo),e(R,_xo),e(R,s5),e(s5,rse),e(rse,uxo),e(s5,bxo),e(s5,dj),e(dj,vxo),e(s5,Txo),e(R,Fxo),e(R,l5),e(l5,tse),e(tse,Cxo),e(l5,Mxo),e(l5,cj),e(cj,Exo),e(l5,yxo),e(R,wxo),e(R,i5),e(i5,ase),e(ase,Axo),e(i5,Lxo),e(i5,fj),e(fj,Bxo),e(i5,xxo),e(R,kxo),e(R,d5),e(d5,nse),e(nse,Rxo),e(d5,Sxo),e(d5,mj),e(mj,Pxo),e(d5,$xo),e(R,Ixo),e(R,c5),e(c5,sse),e(sse,Dxo),e(c5,jxo),e(c5,gj),e(gj,Nxo),e(c5,qxo),e(R,Gxo),e(R,f5),e(f5,lse),e(lse,Oxo),e(f5,Xxo),e(f5,hj),e(hj,Vxo),e(f5,zxo),e(R,Wxo),e(R,m5),e(m5,ise),e(ise,Qxo),e(m5,Hxo),e(m5,pj),e(pj,Uxo),e(m5,Jxo),e(R,Yxo),e(R,g5),e(g5,dse),e(dse,Kxo),e(g5,Zxo),e(g5,_j),e(_j,eko),e(g5,oko),e(R,rko),e(R,h5),e(h5,cse),e(cse,tko),e(h5,ako),e(h5,uj),e(uj,nko),e(h5,sko),e(R,lko),e(R,p5),e(p5,fse),e(fse,iko),e(p5,dko),e(p5,bj),e(bj,cko),e(p5,fko),e(R,mko),e(R,_5),e(_5,mse),e(mse,gko),e(_5,hko),e(_5,vj),e(vj,pko),e(_5,_ko),e(R,uko),e(R,u5),e(u5,gse),e(gse,bko),e(u5,vko),e(u5,Tj),e(Tj,Tko),e(u5,Fko),e(R,Cko),e(R,b5),e(b5,hse),e(hse,Mko),e(b5,Eko),e(b5,Fj),e(Fj,yko),e(b5,wko),e(R,Ako),e(R,v5),e(v5,pse),e(pse,Lko),e(v5,Bko),e(v5,Cj),e(Cj,xko),e(v5,kko),e(R,Rko),e(R,T5),e(T5,_se),e(_se,Sko),e(T5,Pko),e(T5,Mj),e(Mj,$ko),e(T5,Iko),e(R,Dko),e(R,F5),e(F5,use),e(use,jko),e(F5,Nko),e(F5,Ej),e(Ej,qko),e(F5,Gko),e(R,Oko),e(R,C5),e(C5,bse),e(bse,Xko),e(C5,Vko),e(C5,yj),e(yj,zko),e(C5,Wko),e(R,Qko),e(R,M5),e(M5,vse),e(vse,Hko),e(M5,Uko),e(M5,wj),e(wj,Jko),e(M5,Yko),e(R,Kko),e(R,E5),e(E5,Tse),e(Tse,Zko),e(E5,eRo),e(E5,Aj),e(Aj,oRo),e(E5,rRo),e(R,tRo),e(R,y5),e(y5,Fse),e(Fse,aRo),e(y5,nRo),e(y5,Lj),e(Lj,sRo),e(y5,lRo),e(R,iRo),e(R,w5),e(w5,Cse),e(Cse,dRo),e(w5,cRo),e(w5,Bj),e(Bj,fRo),e(w5,mRo),e(We,gRo),e(We,A5),e(A5,hRo),e(A5,Mse),e(Mse,pRo),e(A5,_Ro),e(A5,Ese),e(Ese,uRo),e(We,bRo),e(We,yse),e(yse,vRo),e(We,TRo),g(E3,We,null),b(c,RBe,u),b(c,bd,u),e(bd,L5),e(L5,wse),g(y3,wse,null),e(bd,FRo),e(bd,Ase),e(Ase,CRo),b(c,SBe,u),b(c,nr,u),g(w3,nr,null),e(nr,MRo),e(nr,vd),e(vd,ERo),e(vd,Lse),e(Lse,yRo),e(vd,wRo),e(vd,Bse),e(Bse,ARo),e(vd,LRo),e(nr,BRo),e(nr,A3),e(A3,xRo),e(A3,xse),e(xse,kRo),e(A3,RRo),e(nr,SRo),e(nr,Kr),g(L3,Kr,null),e(Kr,PRo),e(Kr,kse),e(kse,$Ro),e(Kr,IRo),e(Kr,Td),e(Td,DRo),e(Td,Rse),e(Rse,jRo),e(Td,NRo),e(Td,Sse),e(Sse,qRo),e(Td,GRo),e(Kr,ORo),e(Kr,Pse),e(Pse,XRo),e(Kr,VRo),g(B3,Kr,null),e(nr,zRo),e(nr,Qe),g(x3,Qe,null),e(Qe,WRo),e(Qe,$se),e($se,QRo),e(Qe,HRo),e(Qe,Ua),e(Ua,URo),e(Ua,Ise),e(Ise,JRo),e(Ua,YRo),e(Ua,Dse),e(Dse,KRo),e(Ua,ZRo),e(Ua,jse),e(jse,eSo),e(Ua,oSo),e(Qe,rSo),e(Qe,Nse),e(Nse,B5),e(B5,qse),e(qse,tSo),e(B5,aSo),e(B5,xj),e(xj,nSo),e(B5,sSo),e(Qe,lSo),e(Qe,x5),e(x5,iSo),e(x5,Gse),e(Gse,dSo),e(x5,cSo),e(x5,Ose),e(Ose,fSo),e(Qe,mSo),e(Qe,Xse),e(Xse,gSo),e(Qe,hSo),g(k3,Qe,null),b(c,PBe,u),b(c,Fd,u),e(Fd,k5),e(k5,Vse),g(R3,Vse,null),e(Fd,pSo),e(Fd,zse),e(zse,_So),b(c,$Be,u),b(c,sr,u),g(S3,sr,null),e(sr,uSo),e(sr,Cd),e(Cd,bSo),e(Cd,Wse),e(Wse,vSo),e(Cd,TSo),e(Cd,Qse),e(Qse,FSo),e(Cd,CSo),e(sr,MSo),e(sr,P3),e(P3,ESo),e(P3,Hse),e(Hse,ySo),e(P3,wSo),e(sr,ASo),e(sr,Zr),g($3,Zr,null),e(Zr,LSo),e(Zr,Use),e(Use,BSo),e(Zr,xSo),e(Zr,Md),e(Md,kSo),e(Md,Jse),e(Jse,RSo),e(Md,SSo),e(Md,Yse),e(Yse,PSo),e(Md,$So),e(Zr,ISo),e(Zr,Kse),e(Kse,DSo),e(Zr,jSo),g(I3,Zr,null),e(sr,NSo),e(sr,He),g(D3,He,null),e(He,qSo),e(He,Zse),e(Zse,GSo),e(He,OSo),e(He,Ja),e(Ja,XSo),e(Ja,ele),e(ele,VSo),e(Ja,zSo),e(Ja,ole),e(ole,WSo),e(Ja,QSo),e(Ja,rle),e(rle,HSo),e(Ja,USo),e(He,JSo),e(He,Fe),e(Fe,R5),e(R5,tle),e(tle,YSo),e(R5,KSo),e(R5,kj),e(kj,ZSo),e(R5,ePo),e(Fe,oPo),e(Fe,S5),e(S5,ale),e(ale,rPo),e(S5,tPo),e(S5,Rj),e(Rj,aPo),e(S5,nPo),e(Fe,sPo),e(Fe,Ps),e(Ps,nle),e(nle,lPo),e(Ps,iPo),e(Ps,Sj),e(Sj,dPo),e(Ps,cPo),e(Ps,Pj),e(Pj,fPo),e(Ps,mPo),e(Fe,gPo),e(Fe,P5),e(P5,sle),e(sle,hPo),e(P5,pPo),e(P5,$j),e($j,_Po),e(P5,uPo),e(Fe,bPo),e(Fe,la),e(la,lle),e(lle,vPo),e(la,TPo),e(la,Ij),e(Ij,FPo),e(la,CPo),e(la,Dj),e(Dj,MPo),e(la,EPo),e(la,jj),e(jj,yPo),e(la,wPo),e(Fe,APo),e(Fe,$5),e($5,ile),e(ile,LPo),e($5,BPo),e($5,Nj),e(Nj,xPo),e($5,kPo),e(Fe,RPo),e(Fe,I5),e(I5,dle),e(dle,SPo),e(I5,PPo),e(I5,qj),e(qj,$Po),e(I5,IPo),e(Fe,DPo),e(Fe,D5),e(D5,cle),e(cle,jPo),e(D5,NPo),e(D5,Gj),e(Gj,qPo),e(D5,GPo),e(Fe,OPo),e(Fe,j5),e(j5,fle),e(fle,XPo),e(j5,VPo),e(j5,Oj),e(Oj,zPo),e(j5,WPo),e(He,QPo),e(He,N5),e(N5,HPo),e(N5,mle),e(mle,UPo),e(N5,JPo),e(N5,gle),e(gle,YPo),e(He,KPo),e(He,hle),e(hle,ZPo),e(He,e$o),g(j3,He,null),b(c,IBe,u),b(c,Ed,u),e(Ed,q5),e(q5,ple),g(N3,ple,null),e(Ed,o$o),e(Ed,_le),e(_le,r$o),b(c,DBe,u),b(c,lr,u),g(q3,lr,null),e(lr,t$o),e(lr,yd),e(yd,a$o),e(yd,ule),e(ule,n$o),e(yd,s$o),e(yd,ble),e(ble,l$o),e(yd,i$o),e(lr,d$o),e(lr,G3),e(G3,c$o),e(G3,vle),e(vle,f$o),e(G3,m$o),e(lr,g$o),e(lr,et),g(O3,et,null),e(et,h$o),e(et,Tle),e(Tle,p$o),e(et,_$o),e(et,wd),e(wd,u$o),e(wd,Fle),e(Fle,b$o),e(wd,v$o),e(wd,Cle),e(Cle,T$o),e(wd,F$o),e(et,C$o),e(et,Mle),e(Mle,M$o),e(et,E$o),g(X3,et,null),e(lr,y$o),e(lr,Ue),g(V3,Ue,null),e(Ue,w$o),e(Ue,Ele),e(Ele,A$o),e(Ue,L$o),e(Ue,Ya),e(Ya,B$o),e(Ya,yle),e(yle,x$o),e(Ya,k$o),e(Ya,wle),e(wle,R$o),e(Ya,S$o),e(Ya,Ale),e(Ale,P$o),e(Ya,$$o),e(Ue,I$o),e(Ue,Lle),e(Lle,G5),e(G5,Ble),e(Ble,D$o),e(G5,j$o),e(G5,Xj),e(Xj,N$o),e(G5,q$o),e(Ue,G$o),e(Ue,O5),e(O5,O$o),e(O5,xle),e(xle,X$o),e(O5,V$o),e(O5,kle),e(kle,z$o),e(Ue,W$o),e(Ue,Rle),e(Rle,Q$o),e(Ue,H$o),g(z3,Ue,null),b(c,jBe,u),b(c,Ad,u),e(Ad,X5),e(X5,Sle),g(W3,Sle,null),e(Ad,U$o),e(Ad,Ple),e(Ple,J$o),b(c,NBe,u),b(c,ir,u),g(Q3,ir,null),e(ir,Y$o),e(ir,Ld),e(Ld,K$o),e(Ld,$le),e($le,Z$o),e(Ld,eIo),e(Ld,Ile),e(Ile,oIo),e(Ld,rIo),e(ir,tIo),e(ir,H3),e(H3,aIo),e(H3,Dle),e(Dle,nIo),e(H3,sIo),e(ir,lIo),e(ir,ot),g(U3,ot,null),e(ot,iIo),e(ot,jle),e(jle,dIo),e(ot,cIo),e(ot,Bd),e(Bd,fIo),e(Bd,Nle),e(Nle,mIo),e(Bd,gIo),e(Bd,qle),e(qle,hIo),e(Bd,pIo),e(ot,_Io),e(ot,Gle),e(Gle,uIo),e(ot,bIo),g(J3,ot,null),e(ir,vIo),e(ir,Je),g(Y3,Je,null),e(Je,TIo),e(Je,Ole),e(Ole,FIo),e(Je,CIo),e(Je,Ka),e(Ka,MIo),e(Ka,Xle),e(Xle,EIo),e(Ka,yIo),e(Ka,Vle),e(Vle,wIo),e(Ka,AIo),e(Ka,zle),e(zle,LIo),e(Ka,BIo),e(Je,xIo),e(Je,xe),e(xe,V5),e(V5,Wle),e(Wle,kIo),e(V5,RIo),e(V5,Vj),e(Vj,SIo),e(V5,PIo),e(xe,$Io),e(xe,z5),e(z5,Qle),e(Qle,IIo),e(z5,DIo),e(z5,zj),e(zj,jIo),e(z5,NIo),e(xe,qIo),e(xe,W5),e(W5,Hle),e(Hle,GIo),e(W5,OIo),e(W5,Wj),e(Wj,XIo),e(W5,VIo),e(xe,zIo),e(xe,Q5),e(Q5,Ule),e(Ule,WIo),e(Q5,QIo),e(Q5,Qj),e(Qj,HIo),e(Q5,UIo),e(xe,JIo),e(xe,H5),e(H5,Jle),e(Jle,YIo),e(H5,KIo),e(H5,Hj),e(Hj,ZIo),e(H5,eDo),e(xe,oDo),e(xe,U5),e(U5,Yle),e(Yle,rDo),e(U5,tDo),e(U5,Uj),e(Uj,aDo),e(U5,nDo),e(xe,sDo),e(xe,J5),e(J5,Kle),e(Kle,lDo),e(J5,iDo),e(J5,Jj),e(Jj,dDo),e(J5,cDo),e(xe,fDo),e(xe,Y5),e(Y5,Zle),e(Zle,mDo),e(Y5,gDo),e(Y5,Yj),e(Yj,hDo),e(Y5,pDo),e(Je,_Do),e(Je,K5),e(K5,uDo),e(K5,eie),e(eie,bDo),e(K5,vDo),e(K5,oie),e(oie,TDo),e(Je,FDo),e(Je,rie),e(rie,CDo),e(Je,MDo),g(K3,Je,null),b(c,qBe,u),b(c,xd,u),e(xd,Z5),e(Z5,tie),g(Z3,tie,null),e(xd,EDo),e(xd,aie),e(aie,yDo),b(c,GBe,u),b(c,dr,u),g(ey,dr,null),e(dr,wDo),e(dr,kd),e(kd,ADo),e(kd,nie),e(nie,LDo),e(kd,BDo),e(kd,sie),e(sie,xDo),e(kd,kDo),e(dr,RDo),e(dr,oy),e(oy,SDo),e(oy,lie),e(lie,PDo),e(oy,$Do),e(dr,IDo),e(dr,rt),g(ry,rt,null),e(rt,DDo),e(rt,iie),e(iie,jDo),e(rt,NDo),e(rt,Rd),e(Rd,qDo),e(Rd,die),e(die,GDo),e(Rd,ODo),e(Rd,cie),e(cie,XDo),e(Rd,VDo),e(rt,zDo),e(rt,fie),e(fie,WDo),e(rt,QDo),g(ty,rt,null),e(dr,HDo),e(dr,Ye),g(ay,Ye,null),e(Ye,UDo),e(Ye,mie),e(mie,JDo),e(Ye,YDo),e(Ye,Za),e(Za,KDo),e(Za,gie),e(gie,ZDo),e(Za,ejo),e(Za,hie),e(hie,ojo),e(Za,rjo),e(Za,pie),e(pie,tjo),e(Za,ajo),e(Ye,njo),e(Ye,en),e(en,e2),e(e2,_ie),e(_ie,sjo),e(e2,ljo),e(e2,Kj),e(Kj,ijo),e(e2,djo),e(en,cjo),e(en,o2),e(o2,uie),e(uie,fjo),e(o2,mjo),e(o2,Zj),e(Zj,gjo),e(o2,hjo),e(en,pjo),e(en,r2),e(r2,bie),e(bie,_jo),e(r2,ujo),e(r2,eN),e(eN,bjo),e(r2,vjo),e(en,Tjo),e(en,t2),e(t2,vie),e(vie,Fjo),e(t2,Cjo),e(t2,oN),e(oN,Mjo),e(t2,Ejo),e(Ye,yjo),e(Ye,a2),e(a2,wjo),e(a2,Tie),e(Tie,Ajo),e(a2,Ljo),e(a2,Fie),e(Fie,Bjo),e(Ye,xjo),e(Ye,Cie),e(Cie,kjo),e(Ye,Rjo),g(ny,Ye,null),b(c,OBe,u),b(c,Sd,u),e(Sd,n2),e(n2,Mie),g(sy,Mie,null),e(Sd,Sjo),e(Sd,Eie),e(Eie,Pjo),b(c,XBe,u),b(c,cr,u),g(ly,cr,null),e(cr,$jo),e(cr,Pd),e(Pd,Ijo),e(Pd,yie),e(yie,Djo),e(Pd,jjo),e(Pd,wie),e(wie,Njo),e(Pd,qjo),e(cr,Gjo),e(cr,iy),e(iy,Ojo),e(iy,Aie),e(Aie,Xjo),e(iy,Vjo),e(cr,zjo),e(cr,tt),g(dy,tt,null),e(tt,Wjo),e(tt,Lie),e(Lie,Qjo),e(tt,Hjo),e(tt,$d),e($d,Ujo),e($d,Bie),e(Bie,Jjo),e($d,Yjo),e($d,xie),e(xie,Kjo),e($d,Zjo),e(tt,eNo),e(tt,kie),e(kie,oNo),e(tt,rNo),g(cy,tt,null),e(cr,tNo),e(cr,Ke),g(fy,Ke,null),e(Ke,aNo),e(Ke,Rie),e(Rie,nNo),e(Ke,sNo),e(Ke,on),e(on,lNo),e(on,Sie),e(Sie,iNo),e(on,dNo),e(on,Pie),e(Pie,cNo),e(on,fNo),e(on,$ie),e($ie,mNo),e(on,gNo),e(Ke,hNo),e(Ke,ke),e(ke,s2),e(s2,Iie),e(Iie,pNo),e(s2,_No),e(s2,rN),e(rN,uNo),e(s2,bNo),e(ke,vNo),e(ke,l2),e(l2,Die),e(Die,TNo),e(l2,FNo),e(l2,tN),e(tN,CNo),e(l2,MNo),e(ke,ENo),e(ke,i2),e(i2,jie),e(jie,yNo),e(i2,wNo),e(i2,aN),e(aN,ANo),e(i2,LNo),e(ke,BNo),e(ke,d2),e(d2,Nie),e(Nie,xNo),e(d2,kNo),e(d2,nN),e(nN,RNo),e(d2,SNo),e(ke,PNo),e(ke,c2),e(c2,qie),e(qie,$No),e(c2,INo),e(c2,sN),e(sN,DNo),e(c2,jNo),e(ke,NNo),e(ke,f2),e(f2,Gie),e(Gie,qNo),e(f2,GNo),e(f2,lN),e(lN,ONo),e(f2,XNo),e(ke,VNo),e(ke,m2),e(m2,Oie),e(Oie,zNo),e(m2,WNo),e(m2,iN),e(iN,QNo),e(m2,HNo),e(ke,UNo),e(ke,g2),e(g2,Xie),e(Xie,JNo),e(g2,YNo),e(g2,dN),e(dN,KNo),e(g2,ZNo),e(Ke,eqo),e(Ke,h2),e(h2,oqo),e(h2,Vie),e(Vie,rqo),e(h2,tqo),e(h2,zie),e(zie,aqo),e(Ke,nqo),e(Ke,Wie),e(Wie,sqo),e(Ke,lqo),g(my,Ke,null),b(c,VBe,u),b(c,Id,u),e(Id,p2),e(p2,Qie),g(gy,Qie,null),e(Id,iqo),e(Id,Hie),e(Hie,dqo),b(c,zBe,u),b(c,fr,u),g(hy,fr,null),e(fr,cqo),e(fr,Dd),e(Dd,fqo),e(Dd,Uie),e(Uie,mqo),e(Dd,gqo),e(Dd,Jie),e(Jie,hqo),e(Dd,pqo),e(fr,_qo),e(fr,py),e(py,uqo),e(py,Yie),e(Yie,bqo),e(py,vqo),e(fr,Tqo),e(fr,at),g(_y,at,null),e(at,Fqo),e(at,Kie),e(Kie,Cqo),e(at,Mqo),e(at,jd),e(jd,Eqo),e(jd,Zie),e(Zie,yqo),e(jd,wqo),e(jd,ede),e(ede,Aqo),e(jd,Lqo),e(at,Bqo),e(at,ode),e(ode,xqo),e(at,kqo),g(uy,at,null),e(fr,Rqo),e(fr,Ze),g(by,Ze,null),e(Ze,Sqo),e(Ze,rde),e(rde,Pqo),e(Ze,$qo),e(Ze,rn),e(rn,Iqo),e(rn,tde),e(tde,Dqo),e(rn,jqo),e(rn,ade),e(ade,Nqo),e(rn,qqo),e(rn,nde),e(nde,Gqo),e(rn,Oqo),e(Ze,Xqo),e(Ze,vy),e(vy,_2),e(_2,sde),e(sde,Vqo),e(_2,zqo),e(_2,cN),e(cN,Wqo),e(_2,Qqo),e(vy,Hqo),e(vy,u2),e(u2,lde),e(lde,Uqo),e(u2,Jqo),e(u2,fN),e(fN,Yqo),e(u2,Kqo),e(Ze,Zqo),e(Ze,b2),e(b2,eGo),e(b2,ide),e(ide,oGo),e(b2,rGo),e(b2,dde),e(dde,tGo),e(Ze,aGo),e(Ze,cde),e(cde,nGo),e(Ze,sGo),g(Ty,Ze,null),b(c,WBe,u),b(c,Nd,u),e(Nd,v2),e(v2,fde),g(Fy,fde,null),e(Nd,lGo),e(Nd,mde),e(mde,iGo),b(c,QBe,u),b(c,mr,u),g(Cy,mr,null),e(mr,dGo),e(mr,qd),e(qd,cGo),e(qd,gde),e(gde,fGo),e(qd,mGo),e(qd,hde),e(hde,gGo),e(qd,hGo),e(mr,pGo),e(mr,My),e(My,_Go),e(My,pde),e(pde,uGo),e(My,bGo),e(mr,vGo),e(mr,nt),g(Ey,nt,null),e(nt,TGo),e(nt,_de),e(_de,FGo),e(nt,CGo),e(nt,Gd),e(Gd,MGo),e(Gd,ude),e(ude,EGo),e(Gd,yGo),e(Gd,bde),e(bde,wGo),e(Gd,AGo),e(nt,LGo),e(nt,vde),e(vde,BGo),e(nt,xGo),g(yy,nt,null),e(mr,kGo),e(mr,eo),g(wy,eo,null),e(eo,RGo),e(eo,Tde),e(Tde,SGo),e(eo,PGo),e(eo,tn),e(tn,$Go),e(tn,Fde),e(Fde,IGo),e(tn,DGo),e(tn,Cde),e(Cde,jGo),e(tn,NGo),e(tn,Mde),e(Mde,qGo),e(tn,GGo),e(eo,OGo),e(eo,an),e(an,T2),e(T2,Ede),e(Ede,XGo),e(T2,VGo),e(T2,mN),e(mN,zGo),e(T2,WGo),e(an,QGo),e(an,F2),e(F2,yde),e(yde,HGo),e(F2,UGo),e(F2,gN),e(gN,JGo),e(F2,YGo),e(an,KGo),e(an,C2),e(C2,wde),e(wde,ZGo),e(C2,eOo),e(C2,hN),e(hN,oOo),e(C2,rOo),e(an,tOo),e(an,M2),e(M2,Ade),e(Ade,aOo),e(M2,nOo),e(M2,pN),e(pN,sOo),e(M2,lOo),e(eo,iOo),e(eo,E2),e(E2,dOo),e(E2,Lde),e(Lde,cOo),e(E2,fOo),e(E2,Bde),e(Bde,mOo),e(eo,gOo),e(eo,xde),e(xde,hOo),e(eo,pOo),g(Ay,eo,null),b(c,HBe,u),b(c,Od,u),e(Od,y2),e(y2,kde),g(Ly,kde,null),e(Od,_Oo),e(Od,Rde),e(Rde,uOo),b(c,UBe,u),b(c,gr,u),g(By,gr,null),e(gr,bOo),e(gr,Xd),e(Xd,vOo),e(Xd,Sde),e(Sde,TOo),e(Xd,FOo),e(Xd,Pde),e(Pde,COo),e(Xd,MOo),e(gr,EOo),e(gr,xy),e(xy,yOo),e(xy,$de),e($de,wOo),e(xy,AOo),e(gr,LOo),e(gr,st),g(ky,st,null),e(st,BOo),e(st,Ide),e(Ide,xOo),e(st,kOo),e(st,Vd),e(Vd,ROo),e(Vd,Dde),e(Dde,SOo),e(Vd,POo),e(Vd,jde),e(jde,$Oo),e(Vd,IOo),e(st,DOo),e(st,Nde),e(Nde,jOo),e(st,NOo),g(Ry,st,null),e(gr,qOo),e(gr,oo),g(Sy,oo,null),e(oo,GOo),e(oo,qde),e(qde,OOo),e(oo,XOo),e(oo,nn),e(nn,VOo),e(nn,Gde),e(Gde,zOo),e(nn,WOo),e(nn,Ode),e(Ode,QOo),e(nn,HOo),e(nn,Xde),e(Xde,UOo),e(nn,JOo),e(oo,YOo),e(oo,zd),e(zd,w2),e(w2,Vde),e(Vde,KOo),e(w2,ZOo),e(w2,_N),e(_N,eXo),e(w2,oXo),e(zd,rXo),e(zd,A2),e(A2,zde),e(zde,tXo),e(A2,aXo),e(A2,uN),e(uN,nXo),e(A2,sXo),e(zd,lXo),e(zd,L2),e(L2,Wde),e(Wde,iXo),e(L2,dXo),e(L2,bN),e(bN,cXo),e(L2,fXo),e(oo,mXo),e(oo,B2),e(B2,gXo),e(B2,Qde),e(Qde,hXo),e(B2,pXo),e(B2,Hde),e(Hde,_Xo),e(oo,uXo),e(oo,Ude),e(Ude,bXo),e(oo,vXo),g(Py,oo,null),b(c,JBe,u),b(c,Wd,u),e(Wd,x2),e(x2,Jde),g($y,Jde,null),e(Wd,TXo),e(Wd,Yde),e(Yde,FXo),b(c,YBe,u),b(c,hr,u),g(Iy,hr,null),e(hr,CXo),e(hr,Qd),e(Qd,MXo),e(Qd,Kde),e(Kde,EXo),e(Qd,yXo),e(Qd,Zde),e(Zde,wXo),e(Qd,AXo),e(hr,LXo),e(hr,Dy),e(Dy,BXo),e(Dy,ece),e(ece,xXo),e(Dy,kXo),e(hr,RXo),e(hr,lt),g(jy,lt,null),e(lt,SXo),e(lt,oce),e(oce,PXo),e(lt,$Xo),e(lt,Hd),e(Hd,IXo),e(Hd,rce),e(rce,DXo),e(Hd,jXo),e(Hd,tce),e(tce,NXo),e(Hd,qXo),e(lt,GXo),e(lt,ace),e(ace,OXo),e(lt,XXo),g(Ny,lt,null),e(hr,VXo),e(hr,ro),g(qy,ro,null),e(ro,zXo),e(ro,nce),e(nce,WXo),e(ro,QXo),e(ro,sn),e(sn,HXo),e(sn,sce),e(sce,UXo),e(sn,JXo),e(sn,lce),e(lce,YXo),e(sn,KXo),e(sn,ice),e(ice,ZXo),e(sn,eVo),e(ro,oVo),e(ro,dce),e(dce,k2),e(k2,cce),e(cce,rVo),e(k2,tVo),e(k2,vN),e(vN,aVo),e(k2,nVo),e(ro,sVo),e(ro,R2),e(R2,lVo),e(R2,fce),e(fce,iVo),e(R2,dVo),e(R2,mce),e(mce,cVo),e(ro,fVo),e(ro,gce),e(gce,mVo),e(ro,gVo),g(Gy,ro,null),b(c,KBe,u),b(c,Ud,u),e(Ud,S2),e(S2,hce),g(Oy,hce,null),e(Ud,hVo),e(Ud,pce),e(pce,pVo),b(c,ZBe,u),b(c,pr,u),g(Xy,pr,null),e(pr,_Vo),e(pr,Jd),e(Jd,uVo),e(Jd,_ce),e(_ce,bVo),e(Jd,vVo),e(Jd,uce),e(uce,TVo),e(Jd,FVo),e(pr,CVo),e(pr,Vy),e(Vy,MVo),e(Vy,bce),e(bce,EVo),e(Vy,yVo),e(pr,wVo),e(pr,it),g(zy,it,null),e(it,AVo),e(it,vce),e(vce,LVo),e(it,BVo),e(it,Yd),e(Yd,xVo),e(Yd,Tce),e(Tce,kVo),e(Yd,RVo),e(Yd,Fce),e(Fce,SVo),e(Yd,PVo),e(it,$Vo),e(it,Cce),e(Cce,IVo),e(it,DVo),g(Wy,it,null),e(pr,jVo),e(pr,to),g(Qy,to,null),e(to,NVo),e(to,Mce),e(Mce,qVo),e(to,GVo),e(to,ln),e(ln,OVo),e(ln,Ece),e(Ece,XVo),e(ln,VVo),e(ln,yce),e(yce,zVo),e(ln,WVo),e(ln,wce),e(wce,QVo),e(ln,HVo),e(to,UVo),e(to,Ace),e(Ace,P2),e(P2,Lce),e(Lce,JVo),e(P2,YVo),e(P2,TN),e(TN,KVo),e(P2,ZVo),e(to,ezo),e(to,$2),e($2,ozo),e($2,Bce),e(Bce,rzo),e($2,tzo),e($2,xce),e(xce,azo),e(to,nzo),e(to,kce),e(kce,szo),e(to,lzo),g(Hy,to,null),b(c,exe,u),b(c,Kd,u),e(Kd,I2),e(I2,Rce),g(Uy,Rce,null),e(Kd,izo),e(Kd,Sce),e(Sce,dzo),b(c,oxe,u),b(c,_r,u),g(Jy,_r,null),e(_r,czo),e(_r,Zd),e(Zd,fzo),e(Zd,Pce),e(Pce,mzo),e(Zd,gzo),e(Zd,$ce),e($ce,hzo),e(Zd,pzo),e(_r,_zo),e(_r,Yy),e(Yy,uzo),e(Yy,Ice),e(Ice,bzo),e(Yy,vzo),e(_r,Tzo),e(_r,dt),g(Ky,dt,null),e(dt,Fzo),e(dt,Dce),e(Dce,Czo),e(dt,Mzo),e(dt,ec),e(ec,Ezo),e(ec,jce),e(jce,yzo),e(ec,wzo),e(ec,Nce),e(Nce,Azo),e(ec,Lzo),e(dt,Bzo),e(dt,qce),e(qce,xzo),e(dt,kzo),g(Zy,dt,null),e(_r,Rzo),e(_r,ao),g(ew,ao,null),e(ao,Szo),e(ao,Gce),e(Gce,Pzo),e(ao,$zo),e(ao,dn),e(dn,Izo),e(dn,Oce),e(Oce,Dzo),e(dn,jzo),e(dn,Xce),e(Xce,Nzo),e(dn,qzo),e(dn,Vce),e(Vce,Gzo),e(dn,Ozo),e(ao,Xzo),e(ao,ow),e(ow,D2),e(D2,zce),e(zce,Vzo),e(D2,zzo),e(D2,FN),e(FN,Wzo),e(D2,Qzo),e(ow,Hzo),e(ow,j2),e(j2,Wce),e(Wce,Uzo),e(j2,Jzo),e(j2,CN),e(CN,Yzo),e(j2,Kzo),e(ao,Zzo),e(ao,N2),e(N2,eWo),e(N2,Qce),e(Qce,oWo),e(N2,rWo),e(N2,Hce),e(Hce,tWo),e(ao,aWo),e(ao,Uce),e(Uce,nWo),e(ao,sWo),g(rw,ao,null),b(c,rxe,u),b(c,oc,u),e(oc,q2),e(q2,Jce),g(tw,Jce,null),e(oc,lWo),e(oc,Yce),e(Yce,iWo),b(c,txe,u),b(c,ur,u),g(aw,ur,null),e(ur,dWo),e(ur,rc),e(rc,cWo),e(rc,Kce),e(Kce,fWo),e(rc,mWo),e(rc,Zce),e(Zce,gWo),e(rc,hWo),e(ur,pWo),e(ur,nw),e(nw,_Wo),e(nw,efe),e(efe,uWo),e(nw,bWo),e(ur,vWo),e(ur,ct),g(sw,ct,null),e(ct,TWo),e(ct,ofe),e(ofe,FWo),e(ct,CWo),e(ct,tc),e(tc,MWo),e(tc,rfe),e(rfe,EWo),e(tc,yWo),e(tc,tfe),e(tfe,wWo),e(tc,AWo),e(ct,LWo),e(ct,afe),e(afe,BWo),e(ct,xWo),g(lw,ct,null),e(ur,kWo),e(ur,go),g(iw,go,null),e(go,RWo),e(go,nfe),e(nfe,SWo),e(go,PWo),e(go,cn),e(cn,$Wo),e(cn,sfe),e(sfe,IWo),e(cn,DWo),e(cn,lfe),e(lfe,jWo),e(cn,NWo),e(cn,ife),e(ife,qWo),e(cn,GWo),e(go,OWo),e(go,B),e(B,G2),e(G2,dfe),e(dfe,XWo),e(G2,VWo),e(G2,MN),e(MN,zWo),e(G2,WWo),e(B,QWo),e(B,O2),e(O2,cfe),e(cfe,HWo),e(O2,UWo),e(O2,EN),e(EN,JWo),e(O2,YWo),e(B,KWo),e(B,X2),e(X2,ffe),e(ffe,ZWo),e(X2,eQo),e(X2,yN),e(yN,oQo),e(X2,rQo),e(B,tQo),e(B,V2),e(V2,mfe),e(mfe,aQo),e(V2,nQo),e(V2,wN),e(wN,sQo),e(V2,lQo),e(B,iQo),e(B,z2),e(z2,gfe),e(gfe,dQo),e(z2,cQo),e(z2,AN),e(AN,fQo),e(z2,mQo),e(B,gQo),e(B,W2),e(W2,hfe),e(hfe,hQo),e(W2,pQo),e(W2,LN),e(LN,_Qo),e(W2,uQo),e(B,bQo),e(B,Q2),e(Q2,pfe),e(pfe,vQo),e(Q2,TQo),e(Q2,BN),e(BN,FQo),e(Q2,CQo),e(B,MQo),e(B,H2),e(H2,_fe),e(_fe,EQo),e(H2,yQo),e(H2,xN),e(xN,wQo),e(H2,AQo),e(B,LQo),e(B,U2),e(U2,ufe),e(ufe,BQo),e(U2,xQo),e(U2,kN),e(kN,kQo),e(U2,RQo),e(B,SQo),e(B,J2),e(J2,bfe),e(bfe,PQo),e(J2,$Qo),e(J2,RN),e(RN,IQo),e(J2,DQo),e(B,jQo),e(B,Y2),e(Y2,vfe),e(vfe,NQo),e(Y2,qQo),e(Y2,SN),e(SN,GQo),e(Y2,OQo),e(B,XQo),e(B,K2),e(K2,Tfe),e(Tfe,VQo),e(K2,zQo),e(K2,PN),e(PN,WQo),e(K2,QQo),e(B,HQo),e(B,Z2),e(Z2,Ffe),e(Ffe,UQo),e(Z2,JQo),e(Z2,$N),e($N,YQo),e(Z2,KQo),e(B,ZQo),e(B,ev),e(ev,Cfe),e(Cfe,eHo),e(ev,oHo),e(ev,IN),e(IN,rHo),e(ev,tHo),e(B,aHo),e(B,ov),e(ov,Mfe),e(Mfe,nHo),e(ov,sHo),e(ov,DN),e(DN,lHo),e(ov,iHo),e(B,dHo),e(B,rv),e(rv,Efe),e(Efe,cHo),e(rv,fHo),e(rv,jN),e(jN,mHo),e(rv,gHo),e(B,hHo),e(B,$s),e($s,yfe),e(yfe,pHo),e($s,_Ho),e($s,NN),e(NN,uHo),e($s,bHo),e($s,qN),e(qN,vHo),e($s,THo),e(B,FHo),e(B,tv),e(tv,wfe),e(wfe,CHo),e(tv,MHo),e(tv,GN),e(GN,EHo),e(tv,yHo),e(B,wHo),e(B,av),e(av,Afe),e(Afe,AHo),e(av,LHo),e(av,ON),e(ON,BHo),e(av,xHo),e(B,kHo),e(B,nv),e(nv,Lfe),e(Lfe,RHo),e(nv,SHo),e(nv,XN),e(XN,PHo),e(nv,$Ho),e(B,IHo),e(B,sv),e(sv,Bfe),e(Bfe,DHo),e(sv,jHo),e(sv,VN),e(VN,NHo),e(sv,qHo),e(B,GHo),e(B,lv),e(lv,xfe),e(xfe,OHo),e(lv,XHo),e(lv,zN),e(zN,VHo),e(lv,zHo),e(B,WHo),e(B,iv),e(iv,kfe),e(kfe,QHo),e(iv,HHo),e(iv,WN),e(WN,UHo),e(iv,JHo),e(B,YHo),e(B,dv),e(dv,Rfe),e(Rfe,KHo),e(dv,ZHo),e(dv,QN),e(QN,eUo),e(dv,oUo),e(B,rUo),e(B,cv),e(cv,Sfe),e(Sfe,tUo),e(cv,aUo),e(cv,HN),e(HN,nUo),e(cv,sUo),e(B,lUo),e(B,fv),e(fv,Pfe),e(Pfe,iUo),e(fv,dUo),e(fv,UN),e(UN,cUo),e(fv,fUo),e(B,mUo),e(B,mv),e(mv,$fe),e($fe,gUo),e(mv,hUo),e(mv,JN),e(JN,pUo),e(mv,_Uo),e(B,uUo),e(B,gv),e(gv,Ife),e(Ife,bUo),e(gv,vUo),e(gv,YN),e(YN,TUo),e(gv,FUo),e(B,CUo),e(B,hv),e(hv,Dfe),e(Dfe,MUo),e(hv,EUo),e(hv,KN),e(KN,yUo),e(hv,wUo),e(B,AUo),e(B,pv),e(pv,jfe),e(jfe,LUo),e(pv,BUo),e(pv,ZN),e(ZN,xUo),e(pv,kUo),e(B,RUo),e(B,_v),e(_v,Nfe),e(Nfe,SUo),e(_v,PUo),e(_v,eq),e(eq,$Uo),e(_v,IUo),e(B,DUo),e(B,uv),e(uv,qfe),e(qfe,jUo),e(uv,NUo),e(uv,oq),e(oq,qUo),e(uv,GUo),e(B,OUo),e(B,bv),e(bv,Gfe),e(Gfe,XUo),e(bv,VUo),e(bv,rq),e(rq,zUo),e(bv,WUo),e(B,QUo),e(B,vv),e(vv,Ofe),e(Ofe,HUo),e(vv,UUo),e(vv,tq),e(tq,JUo),e(vv,YUo),e(B,KUo),e(B,Tv),e(Tv,Xfe),e(Xfe,ZUo),e(Tv,eJo),e(Tv,aq),e(aq,oJo),e(Tv,rJo),e(B,tJo),e(B,Fv),e(Fv,Vfe),e(Vfe,aJo),e(Fv,nJo),e(Fv,nq),e(nq,sJo),e(Fv,lJo),e(B,iJo),e(B,Cv),e(Cv,zfe),e(zfe,dJo),e(Cv,cJo),e(Cv,sq),e(sq,fJo),e(Cv,mJo),e(B,gJo),e(B,Mv),e(Mv,Wfe),e(Wfe,hJo),e(Mv,pJo),e(Mv,lq),e(lq,_Jo),e(Mv,uJo),e(B,bJo),e(B,Ev),e(Ev,Qfe),e(Qfe,vJo),e(Ev,TJo),e(Ev,iq),e(iq,FJo),e(Ev,CJo),e(B,MJo),e(B,yv),e(yv,Hfe),e(Hfe,EJo),e(yv,yJo),e(yv,dq),e(dq,wJo),e(yv,AJo),e(B,LJo),e(B,wv),e(wv,Ufe),e(Ufe,BJo),e(wv,xJo),e(wv,cq),e(cq,kJo),e(wv,RJo),e(B,SJo),e(B,Av),e(Av,Jfe),e(Jfe,PJo),e(Av,$Jo),e(Av,fq),e(fq,IJo),e(Av,DJo),e(go,jJo),e(go,Yfe),e(Yfe,NJo),e(go,qJo),g(dw,go,null),b(c,axe,u),b(c,ac,u),e(ac,Lv),e(Lv,Kfe),g(cw,Kfe,null),e(ac,GJo),e(ac,Zfe),e(Zfe,OJo),b(c,nxe,u),b(c,br,u),g(fw,br,null),e(br,XJo),e(br,nc),e(nc,VJo),e(nc,eme),e(eme,zJo),e(nc,WJo),e(nc,ome),e(ome,QJo),e(nc,HJo),e(br,UJo),e(br,mw),e(mw,JJo),e(mw,rme),e(rme,YJo),e(mw,KJo),e(br,ZJo),e(br,ft),g(gw,ft,null),e(ft,eYo),e(ft,tme),e(tme,oYo),e(ft,rYo),e(ft,sc),e(sc,tYo),e(sc,ame),e(ame,aYo),e(sc,nYo),e(sc,nme),e(nme,sYo),e(sc,lYo),e(ft,iYo),e(ft,sme),e(sme,dYo),e(ft,cYo),g(hw,ft,null),e(br,fYo),e(br,ho),g(pw,ho,null),e(ho,mYo),e(ho,lme),e(lme,gYo),e(ho,hYo),e(ho,fn),e(fn,pYo),e(fn,ime),e(ime,_Yo),e(fn,uYo),e(fn,dme),e(dme,bYo),e(fn,vYo),e(fn,cme),e(cme,TYo),e(fn,FYo),e(ho,CYo),e(ho,H),e(H,Bv),e(Bv,fme),e(fme,MYo),e(Bv,EYo),e(Bv,mq),e(mq,yYo),e(Bv,wYo),e(H,AYo),e(H,xv),e(xv,mme),e(mme,LYo),e(xv,BYo),e(xv,gq),e(gq,xYo),e(xv,kYo),e(H,RYo),e(H,kv),e(kv,gme),e(gme,SYo),e(kv,PYo),e(kv,hq),e(hq,$Yo),e(kv,IYo),e(H,DYo),e(H,Rv),e(Rv,hme),e(hme,jYo),e(Rv,NYo),e(Rv,pq),e(pq,qYo),e(Rv,GYo),e(H,OYo),e(H,Sv),e(Sv,pme),e(pme,XYo),e(Sv,VYo),e(Sv,_q),e(_q,zYo),e(Sv,WYo),e(H,QYo),e(H,Pv),e(Pv,_me),e(_me,HYo),e(Pv,UYo),e(Pv,uq),e(uq,JYo),e(Pv,YYo),e(H,KYo),e(H,$v),e($v,ume),e(ume,ZYo),e($v,eKo),e($v,bq),e(bq,oKo),e($v,rKo),e(H,tKo),e(H,Iv),e(Iv,bme),e(bme,aKo),e(Iv,nKo),e(Iv,vq),e(vq,sKo),e(Iv,lKo),e(H,iKo),e(H,Dv),e(Dv,vme),e(vme,dKo),e(Dv,cKo),e(Dv,Tq),e(Tq,fKo),e(Dv,mKo),e(H,gKo),e(H,jv),e(jv,Tme),e(Tme,hKo),e(jv,pKo),e(jv,Fq),e(Fq,_Ko),e(jv,uKo),e(H,bKo),e(H,Nv),e(Nv,Fme),e(Fme,vKo),e(Nv,TKo),e(Nv,Cq),e(Cq,FKo),e(Nv,CKo),e(H,MKo),e(H,qv),e(qv,Cme),e(Cme,EKo),e(qv,yKo),e(qv,Mq),e(Mq,wKo),e(qv,AKo),e(H,LKo),e(H,Gv),e(Gv,Mme),e(Mme,BKo),e(Gv,xKo),e(Gv,Eq),e(Eq,kKo),e(Gv,RKo),e(H,SKo),e(H,Ov),e(Ov,Eme),e(Eme,PKo),e(Ov,$Ko),e(Ov,yq),e(yq,IKo),e(Ov,DKo),e(H,jKo),e(H,Xv),e(Xv,yme),e(yme,NKo),e(Xv,qKo),e(Xv,wq),e(wq,GKo),e(Xv,OKo),e(H,XKo),e(H,Vv),e(Vv,wme),e(wme,VKo),e(Vv,zKo),e(Vv,Aq),e(Aq,WKo),e(Vv,QKo),e(H,HKo),e(H,zv),e(zv,Ame),e(Ame,UKo),e(zv,JKo),e(zv,Lq),e(Lq,YKo),e(zv,KKo),e(H,ZKo),e(H,Wv),e(Wv,Lme),e(Lme,eZo),e(Wv,oZo),e(Wv,Bq),e(Bq,rZo),e(Wv,tZo),e(H,aZo),e(H,Qv),e(Qv,Bme),e(Bme,nZo),e(Qv,sZo),e(Qv,xq),e(xq,lZo),e(Qv,iZo),e(H,dZo),e(H,Hv),e(Hv,xme),e(xme,cZo),e(Hv,fZo),e(Hv,kq),e(kq,mZo),e(Hv,gZo),e(H,hZo),e(H,Uv),e(Uv,kme),e(kme,pZo),e(Uv,_Zo),e(Uv,Rq),e(Rq,uZo),e(Uv,bZo),e(H,vZo),e(H,Jv),e(Jv,Rme),e(Rme,TZo),e(Jv,FZo),e(Jv,Sq),e(Sq,CZo),e(Jv,MZo),e(ho,EZo),e(ho,Sme),e(Sme,yZo),e(ho,wZo),g(_w,ho,null),b(c,sxe,u),b(c,lc,u),e(lc,Yv),e(Yv,Pme),g(uw,Pme,null),e(lc,AZo),e(lc,$me),e($me,LZo),b(c,lxe,u),b(c,vr,u),g(bw,vr,null),e(vr,BZo),e(vr,ic),e(ic,xZo),e(ic,Ime),e(Ime,kZo),e(ic,RZo),e(ic,Dme),e(Dme,SZo),e(ic,PZo),e(vr,$Zo),e(vr,vw),e(vw,IZo),e(vw,jme),e(jme,DZo),e(vw,jZo),e(vr,NZo),e(vr,mt),g(Tw,mt,null),e(mt,qZo),e(mt,Nme),e(Nme,GZo),e(mt,OZo),e(mt,dc),e(dc,XZo),e(dc,qme),e(qme,VZo),e(dc,zZo),e(dc,Gme),e(Gme,WZo),e(dc,QZo),e(mt,HZo),e(mt,Ome),e(Ome,UZo),e(mt,JZo),g(Fw,mt,null),e(vr,YZo),e(vr,po),g(Cw,po,null),e(po,KZo),e(po,Xme),e(Xme,ZZo),e(po,eer),e(po,mn),e(mn,oer),e(mn,Vme),e(Vme,rer),e(mn,ter),e(mn,zme),e(zme,aer),e(mn,ner),e(mn,Wme),e(Wme,ser),e(mn,ler),e(po,ier),e(po,pe),e(pe,Kv),e(Kv,Qme),e(Qme,der),e(Kv,cer),e(Kv,Pq),e(Pq,fer),e(Kv,mer),e(pe,ger),e(pe,Zv),e(Zv,Hme),e(Hme,her),e(Zv,per),e(Zv,$q),e($q,_er),e(Zv,uer),e(pe,ber),e(pe,eT),e(eT,Ume),e(Ume,ver),e(eT,Ter),e(eT,Iq),e(Iq,Fer),e(eT,Cer),e(pe,Mer),e(pe,oT),e(oT,Jme),e(Jme,Eer),e(oT,yer),e(oT,Dq),e(Dq,wer),e(oT,Aer),e(pe,Ler),e(pe,rT),e(rT,Yme),e(Yme,Ber),e(rT,xer),e(rT,jq),e(jq,ker),e(rT,Rer),e(pe,Ser),e(pe,tT),e(tT,Kme),e(Kme,Per),e(tT,$er),e(tT,Nq),e(Nq,Ier),e(tT,Der),e(pe,jer),e(pe,aT),e(aT,Zme),e(Zme,Ner),e(aT,qer),e(aT,qq),e(qq,Ger),e(aT,Oer),e(pe,Xer),e(pe,nT),e(nT,ege),e(ege,Ver),e(nT,zer),e(nT,Gq),e(Gq,Wer),e(nT,Qer),e(pe,Her),e(pe,sT),e(sT,oge),e(oge,Uer),e(sT,Jer),e(sT,Oq),e(Oq,Yer),e(sT,Ker),e(pe,Zer),e(pe,lT),e(lT,rge),e(rge,eor),e(lT,oor),e(lT,Xq),e(Xq,ror),e(lT,tor),e(po,aor),e(po,tge),e(tge,nor),e(po,sor),g(Mw,po,null),b(c,ixe,u),b(c,cc,u),e(cc,iT),e(iT,age),g(Ew,age,null),e(cc,lor),e(cc,nge),e(nge,ior),b(c,dxe,u),b(c,Tr,u),g(yw,Tr,null),e(Tr,dor),e(Tr,fc),e(fc,cor),e(fc,sge),e(sge,mor),e(fc,gor),e(fc,lge),e(lge,hor),e(fc,por),e(Tr,_or),e(Tr,ww),e(ww,uor),e(ww,ige),e(ige,bor),e(ww,vor),e(Tr,Tor),e(Tr,gt),g(Aw,gt,null),e(gt,For),e(gt,dge),e(dge,Cor),e(gt,Mor),e(gt,mc),e(mc,Eor),e(mc,cge),e(cge,yor),e(mc,wor),e(mc,fge),e(fge,Aor),e(mc,Lor),e(gt,Bor),e(gt,mge),e(mge,xor),e(gt,kor),g(Lw,gt,null),e(Tr,Ror),e(Tr,_o),g(Bw,_o,null),e(_o,Sor),e(_o,gge),e(gge,Por),e(_o,$or),e(_o,gn),e(gn,Ior),e(gn,hge),e(hge,Dor),e(gn,jor),e(gn,pge),e(pge,Nor),e(gn,qor),e(gn,_ge),e(_ge,Gor),e(gn,Oor),e(_o,Xor),e(_o,xw),e(xw,dT),e(dT,uge),e(uge,Vor),e(dT,zor),e(dT,Vq),e(Vq,Wor),e(dT,Qor),e(xw,Hor),e(xw,cT),e(cT,bge),e(bge,Uor),e(cT,Jor),e(cT,zq),e(zq,Yor),e(cT,Kor),e(_o,Zor),e(_o,vge),e(vge,err),e(_o,orr),g(kw,_o,null),b(c,cxe,u),b(c,gc,u),e(gc,fT),e(fT,Tge),g(Rw,Tge,null),e(gc,rrr),e(gc,Fge),e(Fge,trr),b(c,fxe,u),b(c,Fr,u),g(Sw,Fr,null),e(Fr,arr),e(Fr,hc),e(hc,nrr),e(hc,Cge),e(Cge,srr),e(hc,lrr),e(hc,Mge),e(Mge,irr),e(hc,drr),e(Fr,crr),e(Fr,Pw),e(Pw,frr),e(Pw,Ege),e(Ege,mrr),e(Pw,grr),e(Fr,hrr),e(Fr,ht),g($w,ht,null),e(ht,prr),e(ht,yge),e(yge,_rr),e(ht,urr),e(ht,pc),e(pc,brr),e(pc,wge),e(wge,vrr),e(pc,Trr),e(pc,Age),e(Age,Frr),e(pc,Crr),e(ht,Mrr),e(ht,Lge),e(Lge,Err),e(ht,yrr),g(Iw,ht,null),e(Fr,wrr),e(Fr,uo),g(Dw,uo,null),e(uo,Arr),e(uo,Bge),e(Bge,Lrr),e(uo,Brr),e(uo,hn),e(hn,xrr),e(hn,xge),e(xge,krr),e(hn,Rrr),e(hn,kge),e(kge,Srr),e(hn,Prr),e(hn,Rge),e(Rge,$rr),e(hn,Irr),e(uo,Drr),e(uo,Y),e(Y,mT),e(mT,Sge),e(Sge,jrr),e(mT,Nrr),e(mT,Wq),e(Wq,qrr),e(mT,Grr),e(Y,Orr),e(Y,gT),e(gT,Pge),e(Pge,Xrr),e(gT,Vrr),e(gT,Qq),e(Qq,zrr),e(gT,Wrr),e(Y,Qrr),e(Y,hT),e(hT,$ge),e($ge,Hrr),e(hT,Urr),e(hT,Hq),e(Hq,Jrr),e(hT,Yrr),e(Y,Krr),e(Y,pT),e(pT,Ige),e(Ige,Zrr),e(pT,etr),e(pT,Uq),e(Uq,otr),e(pT,rtr),e(Y,ttr),e(Y,_T),e(_T,Dge),e(Dge,atr),e(_T,ntr),e(_T,Jq),e(Jq,str),e(_T,ltr),e(Y,itr),e(Y,uT),e(uT,jge),e(jge,dtr),e(uT,ctr),e(uT,Yq),e(Yq,ftr),e(uT,mtr),e(Y,gtr),e(Y,bT),e(bT,Nge),e(Nge,htr),e(bT,ptr),e(bT,Kq),e(Kq,_tr),e(bT,utr),e(Y,btr),e(Y,vT),e(vT,qge),e(qge,vtr),e(vT,Ttr),e(vT,Zq),e(Zq,Ftr),e(vT,Ctr),e(Y,Mtr),e(Y,TT),e(TT,Gge),e(Gge,Etr),e(TT,ytr),e(TT,eG),e(eG,wtr),e(TT,Atr),e(Y,Ltr),e(Y,FT),e(FT,Oge),e(Oge,Btr),e(FT,xtr),e(FT,oG),e(oG,ktr),e(FT,Rtr),e(Y,Str),e(Y,CT),e(CT,Xge),e(Xge,Ptr),e(CT,$tr),e(CT,rG),e(rG,Itr),e(CT,Dtr),e(Y,jtr),e(Y,MT),e(MT,Vge),e(Vge,Ntr),e(MT,qtr),e(MT,tG),e(tG,Gtr),e(MT,Otr),e(Y,Xtr),e(Y,ET),e(ET,zge),e(zge,Vtr),e(ET,ztr),e(ET,aG),e(aG,Wtr),e(ET,Qtr),e(Y,Htr),e(Y,yT),e(yT,Wge),e(Wge,Utr),e(yT,Jtr),e(yT,nG),e(nG,Ytr),e(yT,Ktr),e(Y,Ztr),e(Y,wT),e(wT,Qge),e(Qge,ear),e(wT,oar),e(wT,sG),e(sG,rar),e(wT,tar),e(Y,aar),e(Y,AT),e(AT,Hge),e(Hge,nar),e(AT,sar),e(AT,lG),e(lG,lar),e(AT,iar),e(Y,dar),e(Y,LT),e(LT,Uge),e(Uge,car),e(LT,far),e(LT,iG),e(iG,mar),e(LT,gar),e(Y,har),e(Y,BT),e(BT,Jge),e(Jge,par),e(BT,_ar),e(BT,dG),e(dG,uar),e(BT,bar),e(Y,Tar),e(Y,xT),e(xT,Yge),e(Yge,Far),e(xT,Car),e(xT,cG),e(cG,Mar),e(xT,Ear),e(Y,yar),e(Y,kT),e(kT,Kge),e(Kge,war),e(kT,Aar),e(kT,fG),e(fG,Lar),e(kT,Bar),e(uo,xar),e(uo,Zge),e(Zge,kar),e(uo,Rar),g(jw,uo,null),b(c,mxe,u),b(c,_c,u),e(_c,RT),e(RT,ehe),g(Nw,ehe,null),e(_c,Sar),e(_c,ohe),e(ohe,Par),b(c,gxe,u),b(c,Cr,u),g(qw,Cr,null),e(Cr,$ar),e(Cr,uc),e(uc,Iar),e(uc,rhe),e(rhe,Dar),e(uc,jar),e(uc,the),e(the,Nar),e(uc,qar),e(Cr,Gar),e(Cr,Gw),e(Gw,Oar),e(Gw,ahe),e(ahe,Xar),e(Gw,Var),e(Cr,zar),e(Cr,pt),g(Ow,pt,null),e(pt,War),e(pt,nhe),e(nhe,Qar),e(pt,Har),e(pt,bc),e(bc,Uar),e(bc,she),e(she,Jar),e(bc,Yar),e(bc,lhe),e(lhe,Kar),e(bc,Zar),e(pt,enr),e(pt,ihe),e(ihe,onr),e(pt,rnr),g(Xw,pt,null),e(Cr,tnr),e(Cr,bo),g(Vw,bo,null),e(bo,anr),e(bo,dhe),e(dhe,nnr),e(bo,snr),e(bo,pn),e(pn,lnr),e(pn,che),e(che,inr),e(pn,dnr),e(pn,fhe),e(fhe,cnr),e(pn,fnr),e(pn,mhe),e(mhe,mnr),e(pn,gnr),e(bo,hnr),e(bo,_e),e(_e,ST),e(ST,ghe),e(ghe,pnr),e(ST,_nr),e(ST,mG),e(mG,unr),e(ST,bnr),e(_e,vnr),e(_e,PT),e(PT,hhe),e(hhe,Tnr),e(PT,Fnr),e(PT,gG),e(gG,Cnr),e(PT,Mnr),e(_e,Enr),e(_e,$T),e($T,phe),e(phe,ynr),e($T,wnr),e($T,hG),e(hG,Anr),e($T,Lnr),e(_e,Bnr),e(_e,IT),e(IT,_he),e(_he,xnr),e(IT,knr),e(IT,pG),e(pG,Rnr),e(IT,Snr),e(_e,Pnr),e(_e,DT),e(DT,uhe),e(uhe,$nr),e(DT,Inr),e(DT,_G),e(_G,Dnr),e(DT,jnr),e(_e,Nnr),e(_e,jT),e(jT,bhe),e(bhe,qnr),e(jT,Gnr),e(jT,uG),e(uG,Onr),e(jT,Xnr),e(_e,Vnr),e(_e,NT),e(NT,vhe),e(vhe,znr),e(NT,Wnr),e(NT,bG),e(bG,Qnr),e(NT,Hnr),e(_e,Unr),e(_e,qT),e(qT,The),e(The,Jnr),e(qT,Ynr),e(qT,vG),e(vG,Knr),e(qT,Znr),e(_e,esr),e(_e,GT),e(GT,Fhe),e(Fhe,osr),e(GT,rsr),e(GT,TG),e(TG,tsr),e(GT,asr),e(_e,nsr),e(_e,OT),e(OT,Che),e(Che,ssr),e(OT,lsr),e(OT,FG),e(FG,isr),e(OT,dsr),e(bo,csr),e(bo,Mhe),e(Mhe,fsr),e(bo,msr),g(zw,bo,null),b(c,hxe,u),b(c,vc,u),e(vc,XT),e(XT,Ehe),g(Ww,Ehe,null),e(vc,gsr),e(vc,yhe),e(yhe,hsr),b(c,pxe,u),b(c,Mr,u),g(Qw,Mr,null),e(Mr,psr),e(Mr,Tc),e(Tc,_sr),e(Tc,whe),e(whe,usr),e(Tc,bsr),e(Tc,Ahe),e(Ahe,vsr),e(Tc,Tsr),e(Mr,Fsr),e(Mr,Hw),e(Hw,Csr),e(Hw,Lhe),e(Lhe,Msr),e(Hw,Esr),e(Mr,ysr),e(Mr,_t),g(Uw,_t,null),e(_t,wsr),e(_t,Bhe),e(Bhe,Asr),e(_t,Lsr),e(_t,Fc),e(Fc,Bsr),e(Fc,xhe),e(xhe,xsr),e(Fc,ksr),e(Fc,khe),e(khe,Rsr),e(Fc,Ssr),e(_t,Psr),e(_t,Rhe),e(Rhe,$sr),e(_t,Isr),g(Jw,_t,null),e(Mr,Dsr),e(Mr,vo),g(Yw,vo,null),e(vo,jsr),e(vo,She),e(She,Nsr),e(vo,qsr),e(vo,_n),e(_n,Gsr),e(_n,Phe),e(Phe,Osr),e(_n,Xsr),e(_n,$he),e($he,Vsr),e(_n,zsr),e(_n,Ihe),e(Ihe,Wsr),e(_n,Qsr),e(vo,Hsr),e(vo,X),e(X,VT),e(VT,Dhe),e(Dhe,Usr),e(VT,Jsr),e(VT,CG),e(CG,Ysr),e(VT,Ksr),e(X,Zsr),e(X,zT),e(zT,jhe),e(jhe,elr),e(zT,olr),e(zT,MG),e(MG,rlr),e(zT,tlr),e(X,alr),e(X,WT),e(WT,Nhe),e(Nhe,nlr),e(WT,slr),e(WT,EG),e(EG,llr),e(WT,ilr),e(X,dlr),e(X,QT),e(QT,qhe),e(qhe,clr),e(QT,flr),e(QT,yG),e(yG,mlr),e(QT,glr),e(X,hlr),e(X,HT),e(HT,Ghe),e(Ghe,plr),e(HT,_lr),e(HT,wG),e(wG,ulr),e(HT,blr),e(X,vlr),e(X,UT),e(UT,Ohe),e(Ohe,Tlr),e(UT,Flr),e(UT,AG),e(AG,Clr),e(UT,Mlr),e(X,Elr),e(X,JT),e(JT,Xhe),e(Xhe,ylr),e(JT,wlr),e(JT,LG),e(LG,Alr),e(JT,Llr),e(X,Blr),e(X,YT),e(YT,Vhe),e(Vhe,xlr),e(YT,klr),e(YT,BG),e(BG,Rlr),e(YT,Slr),e(X,Plr),e(X,KT),e(KT,zhe),e(zhe,$lr),e(KT,Ilr),e(KT,xG),e(xG,Dlr),e(KT,jlr),e(X,Nlr),e(X,ZT),e(ZT,Whe),e(Whe,qlr),e(ZT,Glr),e(ZT,kG),e(kG,Olr),e(ZT,Xlr),e(X,Vlr),e(X,eF),e(eF,Qhe),e(Qhe,zlr),e(eF,Wlr),e(eF,RG),e(RG,Qlr),e(eF,Hlr),e(X,Ulr),e(X,oF),e(oF,Hhe),e(Hhe,Jlr),e(oF,Ylr),e(oF,SG),e(SG,Klr),e(oF,Zlr),e(X,eir),e(X,rF),e(rF,Uhe),e(Uhe,oir),e(rF,rir),e(rF,PG),e(PG,tir),e(rF,air),e(X,nir),e(X,tF),e(tF,Jhe),e(Jhe,sir),e(tF,lir),e(tF,$G),e($G,iir),e(tF,dir),e(X,cir),e(X,aF),e(aF,Yhe),e(Yhe,fir),e(aF,mir),e(aF,IG),e(IG,gir),e(aF,hir),e(X,pir),e(X,nF),e(nF,Khe),e(Khe,_ir),e(nF,uir),e(nF,DG),e(DG,bir),e(nF,vir),e(X,Tir),e(X,sF),e(sF,Zhe),e(Zhe,Fir),e(sF,Cir),e(sF,jG),e(jG,Mir),e(sF,Eir),e(X,yir),e(X,lF),e(lF,epe),e(epe,wir),e(lF,Air),e(lF,NG),e(NG,Lir),e(lF,Bir),e(X,xir),e(X,iF),e(iF,ope),e(ope,kir),e(iF,Rir),e(iF,qG),e(qG,Sir),e(iF,Pir),e(X,$ir),e(X,dF),e(dF,rpe),e(rpe,Iir),e(dF,Dir),e(dF,GG),e(GG,jir),e(dF,Nir),e(X,qir),e(X,cF),e(cF,tpe),e(tpe,Gir),e(cF,Oir),e(cF,OG),e(OG,Xir),e(cF,Vir),e(X,zir),e(X,fF),e(fF,ape),e(ape,Wir),e(fF,Qir),e(fF,XG),e(XG,Hir),e(fF,Uir),e(X,Jir),e(X,mF),e(mF,npe),e(npe,Yir),e(mF,Kir),e(mF,VG),e(VG,Zir),e(mF,edr),e(X,odr),e(X,gF),e(gF,spe),e(spe,rdr),e(gF,tdr),e(gF,zG),e(zG,adr),e(gF,ndr),e(X,sdr),e(X,hF),e(hF,lpe),e(lpe,ldr),e(hF,idr),e(hF,WG),e(WG,ddr),e(hF,cdr),e(vo,fdr),e(vo,ipe),e(ipe,mdr),e(vo,gdr),g(Kw,vo,null),b(c,_xe,u),b(c,Cc,u),e(Cc,pF),e(pF,dpe),g(Zw,dpe,null),e(Cc,hdr),e(Cc,cpe),e(cpe,pdr),b(c,uxe,u),b(c,Er,u),g(e6,Er,null),e(Er,_dr),e(Er,Mc),e(Mc,udr),e(Mc,fpe),e(fpe,bdr),e(Mc,vdr),e(Mc,mpe),e(mpe,Tdr),e(Mc,Fdr),e(Er,Cdr),e(Er,o6),e(o6,Mdr),e(o6,gpe),e(gpe,Edr),e(o6,ydr),e(Er,wdr),e(Er,ut),g(r6,ut,null),e(ut,Adr),e(ut,hpe),e(hpe,Ldr),e(ut,Bdr),e(ut,Ec),e(Ec,xdr),e(Ec,ppe),e(ppe,kdr),e(Ec,Rdr),e(Ec,_pe),e(_pe,Sdr),e(Ec,Pdr),e(ut,$dr),e(ut,upe),e(upe,Idr),e(ut,Ddr),g(t6,ut,null),e(Er,jdr),e(Er,To),g(a6,To,null),e(To,Ndr),e(To,bpe),e(bpe,qdr),e(To,Gdr),e(To,un),e(un,Odr),e(un,vpe),e(vpe,Xdr),e(un,Vdr),e(un,Tpe),e(Tpe,zdr),e(un,Wdr),e(un,Fpe),e(Fpe,Qdr),e(un,Hdr),e(To,Udr),e(To,te),e(te,_F),e(_F,Cpe),e(Cpe,Jdr),e(_F,Ydr),e(_F,QG),e(QG,Kdr),e(_F,Zdr),e(te,ecr),e(te,uF),e(uF,Mpe),e(Mpe,ocr),e(uF,rcr),e(uF,HG),e(HG,tcr),e(uF,acr),e(te,ncr),e(te,bF),e(bF,Epe),e(Epe,scr),e(bF,lcr),e(bF,UG),e(UG,icr),e(bF,dcr),e(te,ccr),e(te,vF),e(vF,ype),e(ype,fcr),e(vF,mcr),e(vF,JG),e(JG,gcr),e(vF,hcr),e(te,pcr),e(te,TF),e(TF,wpe),e(wpe,_cr),e(TF,ucr),e(TF,YG),e(YG,bcr),e(TF,vcr),e(te,Tcr),e(te,FF),e(FF,Ape),e(Ape,Fcr),e(FF,Ccr),e(FF,KG),e(KG,Mcr),e(FF,Ecr),e(te,ycr),e(te,CF),e(CF,Lpe),e(Lpe,wcr),e(CF,Acr),e(CF,ZG),e(ZG,Lcr),e(CF,Bcr),e(te,xcr),e(te,MF),e(MF,Bpe),e(Bpe,kcr),e(MF,Rcr),e(MF,eO),e(eO,Scr),e(MF,Pcr),e(te,$cr),e(te,EF),e(EF,xpe),e(xpe,Icr),e(EF,Dcr),e(EF,oO),e(oO,jcr),e(EF,Ncr),e(te,qcr),e(te,yF),e(yF,kpe),e(kpe,Gcr),e(yF,Ocr),e(yF,rO),e(rO,Xcr),e(yF,Vcr),e(te,zcr),e(te,wF),e(wF,Rpe),e(Rpe,Wcr),e(wF,Qcr),e(wF,tO),e(tO,Hcr),e(wF,Ucr),e(te,Jcr),e(te,AF),e(AF,Spe),e(Spe,Ycr),e(AF,Kcr),e(AF,aO),e(aO,Zcr),e(AF,efr),e(te,ofr),e(te,LF),e(LF,Ppe),e(Ppe,rfr),e(LF,tfr),e(LF,nO),e(nO,afr),e(LF,nfr),e(te,sfr),e(te,BF),e(BF,$pe),e($pe,lfr),e(BF,ifr),e(BF,sO),e(sO,dfr),e(BF,cfr),e(te,ffr),e(te,xF),e(xF,Ipe),e(Ipe,mfr),e(xF,gfr),e(xF,lO),e(lO,hfr),e(xF,pfr),e(te,_fr),e(te,kF),e(kF,Dpe),e(Dpe,ufr),e(kF,bfr),e(kF,iO),e(iO,vfr),e(kF,Tfr),e(te,Ffr),e(te,RF),e(RF,jpe),e(jpe,Cfr),e(RF,Mfr),e(RF,dO),e(dO,Efr),e(RF,yfr),e(To,wfr),e(To,Npe),e(Npe,Afr),e(To,Lfr),g(n6,To,null),b(c,bxe,u),b(c,yc,u),e(yc,SF),e(SF,qpe),g(s6,qpe,null),e(yc,Bfr),e(yc,Gpe),e(Gpe,xfr),b(c,vxe,u),b(c,yr,u),g(l6,yr,null),e(yr,kfr),e(yr,wc),e(wc,Rfr),e(wc,Ope),e(Ope,Sfr),e(wc,Pfr),e(wc,Xpe),e(Xpe,$fr),e(wc,Ifr),e(yr,Dfr),e(yr,i6),e(i6,jfr),e(i6,Vpe),e(Vpe,Nfr),e(i6,qfr),e(yr,Gfr),e(yr,bt),g(d6,bt,null),e(bt,Ofr),e(bt,zpe),e(zpe,Xfr),e(bt,Vfr),e(bt,Ac),e(Ac,zfr),e(Ac,Wpe),e(Wpe,Wfr),e(Ac,Qfr),e(Ac,Qpe),e(Qpe,Hfr),e(Ac,Ufr),e(bt,Jfr),e(bt,Hpe),e(Hpe,Yfr),e(bt,Kfr),g(c6,bt,null),e(yr,Zfr),e(yr,Fo),g(f6,Fo,null),e(Fo,emr),e(Fo,Upe),e(Upe,omr),e(Fo,rmr),e(Fo,bn),e(bn,tmr),e(bn,Jpe),e(Jpe,amr),e(bn,nmr),e(bn,Ype),e(Ype,smr),e(bn,lmr),e(bn,Kpe),e(Kpe,imr),e(bn,dmr),e(Fo,cmr),e(Fo,Zpe),e(Zpe,PF),e(PF,e_e),e(e_e,fmr),e(PF,mmr),e(PF,cO),e(cO,gmr),e(PF,hmr),e(Fo,pmr),e(Fo,o_e),e(o_e,_mr),e(Fo,umr),g(m6,Fo,null),b(c,Txe,u),b(c,Lc,u),e(Lc,$F),e($F,r_e),g(g6,r_e,null),e(Lc,bmr),e(Lc,t_e),e(t_e,vmr),b(c,Fxe,u),b(c,wr,u),g(h6,wr,null),e(wr,Tmr),e(wr,Bc),e(Bc,Fmr),e(Bc,a_e),e(a_e,Cmr),e(Bc,Mmr),e(Bc,n_e),e(n_e,Emr),e(Bc,ymr),e(wr,wmr),e(wr,p6),e(p6,Amr),e(p6,s_e),e(s_e,Lmr),e(p6,Bmr),e(wr,xmr),e(wr,vt),g(_6,vt,null),e(vt,kmr),e(vt,l_e),e(l_e,Rmr),e(vt,Smr),e(vt,xc),e(xc,Pmr),e(xc,i_e),e(i_e,$mr),e(xc,Imr),e(xc,d_e),e(d_e,Dmr),e(xc,jmr),e(vt,Nmr),e(vt,c_e),e(c_e,qmr),e(vt,Gmr),g(u6,vt,null),e(wr,Omr),e(wr,Co),g(b6,Co,null),e(Co,Xmr),e(Co,f_e),e(f_e,Vmr),e(Co,zmr),e(Co,vn),e(vn,Wmr),e(vn,m_e),e(m_e,Qmr),e(vn,Hmr),e(vn,g_e),e(g_e,Umr),e(vn,Jmr),e(vn,h_e),e(h_e,Ymr),e(vn,Kmr),e(Co,Zmr),e(Co,K),e(K,IF),e(IF,p_e),e(p_e,egr),e(IF,ogr),e(IF,fO),e(fO,rgr),e(IF,tgr),e(K,agr),e(K,DF),e(DF,__e),e(__e,ngr),e(DF,sgr),e(DF,mO),e(mO,lgr),e(DF,igr),e(K,dgr),e(K,jF),e(jF,u_e),e(u_e,cgr),e(jF,fgr),e(jF,gO),e(gO,mgr),e(jF,ggr),e(K,hgr),e(K,NF),e(NF,b_e),e(b_e,pgr),e(NF,_gr),e(NF,hO),e(hO,ugr),e(NF,bgr),e(K,vgr),e(K,qF),e(qF,v_e),e(v_e,Tgr),e(qF,Fgr),e(qF,pO),e(pO,Cgr),e(qF,Mgr),e(K,Egr),e(K,GF),e(GF,T_e),e(T_e,ygr),e(GF,wgr),e(GF,_O),e(_O,Agr),e(GF,Lgr),e(K,Bgr),e(K,OF),e(OF,F_e),e(F_e,xgr),e(OF,kgr),e(OF,uO),e(uO,Rgr),e(OF,Sgr),e(K,Pgr),e(K,XF),e(XF,C_e),e(C_e,$gr),e(XF,Igr),e(XF,bO),e(bO,Dgr),e(XF,jgr),e(K,Ngr),e(K,VF),e(VF,M_e),e(M_e,qgr),e(VF,Ggr),e(VF,vO),e(vO,Ogr),e(VF,Xgr),e(K,Vgr),e(K,zF),e(zF,E_e),e(E_e,zgr),e(zF,Wgr),e(zF,TO),e(TO,Qgr),e(zF,Hgr),e(K,Ugr),e(K,WF),e(WF,y_e),e(y_e,Jgr),e(WF,Ygr),e(WF,FO),e(FO,Kgr),e(WF,Zgr),e(K,ehr),e(K,QF),e(QF,w_e),e(w_e,ohr),e(QF,rhr),e(QF,CO),e(CO,thr),e(QF,ahr),e(K,nhr),e(K,HF),e(HF,A_e),e(A_e,shr),e(HF,lhr),e(HF,MO),e(MO,ihr),e(HF,dhr),e(K,chr),e(K,UF),e(UF,L_e),e(L_e,fhr),e(UF,mhr),e(UF,EO),e(EO,ghr),e(UF,hhr),e(K,phr),e(K,JF),e(JF,B_e),e(B_e,_hr),e(JF,uhr),e(JF,yO),e(yO,bhr),e(JF,vhr),e(K,Thr),e(K,YF),e(YF,x_e),e(x_e,Fhr),e(YF,Chr),e(YF,wO),e(wO,Mhr),e(YF,Ehr),e(K,yhr),e(K,KF),e(KF,k_e),e(k_e,whr),e(KF,Ahr),e(KF,AO),e(AO,Lhr),e(KF,Bhr),e(K,xhr),e(K,ZF),e(ZF,R_e),e(R_e,khr),e(ZF,Rhr),e(ZF,LO),e(LO,Shr),e(ZF,Phr),e(K,$hr),e(K,e9),e(e9,S_e),e(S_e,Ihr),e(e9,Dhr),e(e9,BO),e(BO,jhr),e(e9,Nhr),e(K,qhr),e(K,o9),e(o9,P_e),e(P_e,Ghr),e(o9,Ohr),e(o9,xO),e(xO,Xhr),e(o9,Vhr),e(Co,zhr),e(Co,$_e),e($_e,Whr),e(Co,Qhr),g(v6,Co,null),b(c,Cxe,u),b(c,kc,u),e(kc,r9),e(r9,I_e),g(T6,I_e,null),e(kc,Hhr),e(kc,D_e),e(D_e,Uhr),b(c,Mxe,u),b(c,Ar,u),g(F6,Ar,null),e(Ar,Jhr),e(Ar,Rc),e(Rc,Yhr),e(Rc,j_e),e(j_e,Khr),e(Rc,Zhr),e(Rc,N_e),e(N_e,epr),e(Rc,opr),e(Ar,rpr),e(Ar,C6),e(C6,tpr),e(C6,q_e),e(q_e,apr),e(C6,npr),e(Ar,spr),e(Ar,Tt),g(M6,Tt,null),e(Tt,lpr),e(Tt,G_e),e(G_e,ipr),e(Tt,dpr),e(Tt,Sc),e(Sc,cpr),e(Sc,O_e),e(O_e,fpr),e(Sc,mpr),e(Sc,X_e),e(X_e,gpr),e(Sc,hpr),e(Tt,ppr),e(Tt,V_e),e(V_e,_pr),e(Tt,upr),g(E6,Tt,null),e(Ar,bpr),e(Ar,Mo),g(y6,Mo,null),e(Mo,vpr),e(Mo,z_e),e(z_e,Tpr),e(Mo,Fpr),e(Mo,Tn),e(Tn,Cpr),e(Tn,W_e),e(W_e,Mpr),e(Tn,Epr),e(Tn,Q_e),e(Q_e,ypr),e(Tn,wpr),e(Tn,H_e),e(H_e,Apr),e(Tn,Lpr),e(Mo,Bpr),e(Mo,Z),e(Z,t9),e(t9,U_e),e(U_e,xpr),e(t9,kpr),e(t9,kO),e(kO,Rpr),e(t9,Spr),e(Z,Ppr),e(Z,a9),e(a9,J_e),e(J_e,$pr),e(a9,Ipr),e(a9,RO),e(RO,Dpr),e(a9,jpr),e(Z,Npr),e(Z,n9),e(n9,Y_e),e(Y_e,qpr),e(n9,Gpr),e(n9,SO),e(SO,Opr),e(n9,Xpr),e(Z,Vpr),e(Z,s9),e(s9,K_e),e(K_e,zpr),e(s9,Wpr),e(s9,PO),e(PO,Qpr),e(s9,Hpr),e(Z,Upr),e(Z,l9),e(l9,Z_e),e(Z_e,Jpr),e(l9,Ypr),e(l9,$O),e($O,Kpr),e(l9,Zpr),e(Z,e_r),e(Z,i9),e(i9,eue),e(eue,o_r),e(i9,r_r),e(i9,IO),e(IO,t_r),e(i9,a_r),e(Z,n_r),e(Z,d9),e(d9,oue),e(oue,s_r),e(d9,l_r),e(d9,DO),e(DO,i_r),e(d9,d_r),e(Z,c_r),e(Z,c9),e(c9,rue),e(rue,f_r),e(c9,m_r),e(c9,jO),e(jO,g_r),e(c9,h_r),e(Z,p_r),e(Z,f9),e(f9,tue),e(tue,__r),e(f9,u_r),e(f9,NO),e(NO,b_r),e(f9,v_r),e(Z,T_r),e(Z,m9),e(m9,aue),e(aue,F_r),e(m9,C_r),e(m9,qO),e(qO,M_r),e(m9,E_r),e(Z,y_r),e(Z,g9),e(g9,nue),e(nue,w_r),e(g9,A_r),e(g9,GO),e(GO,L_r),e(g9,B_r),e(Z,x_r),e(Z,h9),e(h9,sue),e(sue,k_r),e(h9,R_r),e(h9,OO),e(OO,S_r),e(h9,P_r),e(Z,$_r),e(Z,p9),e(p9,lue),e(lue,I_r),e(p9,D_r),e(p9,XO),e(XO,j_r),e(p9,N_r),e(Z,q_r),e(Z,_9),e(_9,iue),e(iue,G_r),e(_9,O_r),e(_9,VO),e(VO,X_r),e(_9,V_r),e(Z,z_r),e(Z,u9),e(u9,due),e(due,W_r),e(u9,Q_r),e(u9,zO),e(zO,H_r),e(u9,U_r),e(Z,J_r),e(Z,b9),e(b9,cue),e(cue,Y_r),e(b9,K_r),e(b9,WO),e(WO,Z_r),e(b9,eur),e(Z,our),e(Z,v9),e(v9,fue),e(fue,rur),e(v9,tur),e(v9,QO),e(QO,aur),e(v9,nur),e(Z,sur),e(Z,T9),e(T9,mue),e(mue,lur),e(T9,iur),e(T9,HO),e(HO,dur),e(T9,cur),e(Z,fur),e(Z,F9),e(F9,gue),e(gue,mur),e(F9,gur),e(F9,UO),e(UO,hur),e(F9,pur),e(Mo,_ur),e(Mo,hue),e(hue,uur),e(Mo,bur),g(w6,Mo,null),b(c,Exe,u),b(c,Pc,u),e(Pc,C9),e(C9,pue),g(A6,pue,null),e(Pc,vur),e(Pc,_ue),e(_ue,Tur),b(c,yxe,u),b(c,Lr,u),g(L6,Lr,null),e(Lr,Fur),e(Lr,$c),e($c,Cur),e($c,uue),e(uue,Mur),e($c,Eur),e($c,bue),e(bue,yur),e($c,wur),e(Lr,Aur),e(Lr,B6),e(B6,Lur),e(B6,vue),e(vue,Bur),e(B6,xur),e(Lr,kur),e(Lr,Ft),g(x6,Ft,null),e(Ft,Rur),e(Ft,Tue),e(Tue,Sur),e(Ft,Pur),e(Ft,Ic),e(Ic,$ur),e(Ic,Fue),e(Fue,Iur),e(Ic,Dur),e(Ic,Cue),e(Cue,jur),e(Ic,Nur),e(Ft,qur),e(Ft,Mue),e(Mue,Gur),e(Ft,Our),g(k6,Ft,null),e(Lr,Xur),e(Lr,Eo),g(R6,Eo,null),e(Eo,Vur),e(Eo,Eue),e(Eue,zur),e(Eo,Wur),e(Eo,Fn),e(Fn,Qur),e(Fn,yue),e(yue,Hur),e(Fn,Uur),e(Fn,wue),e(wue,Jur),e(Fn,Yur),e(Fn,Aue),e(Aue,Kur),e(Fn,Zur),e(Eo,e0r),e(Eo,Lue),e(Lue,M9),e(M9,Bue),e(Bue,o0r),e(M9,r0r),e(M9,JO),e(JO,t0r),e(M9,a0r),e(Eo,n0r),e(Eo,xue),e(xue,s0r),e(Eo,l0r),g(S6,Eo,null),b(c,wxe,u),b(c,Dc,u),e(Dc,E9),e(E9,kue),g(P6,kue,null),e(Dc,i0r),e(Dc,Rue),e(Rue,d0r),b(c,Axe,u),b(c,Br,u),g($6,Br,null),e(Br,c0r),e(Br,jc),e(jc,f0r),e(jc,Sue),e(Sue,m0r),e(jc,g0r),e(jc,Pue),e(Pue,h0r),e(jc,p0r),e(Br,_0r),e(Br,I6),e(I6,u0r),e(I6,$ue),e($ue,b0r),e(I6,v0r),e(Br,T0r),e(Br,Ct),g(D6,Ct,null),e(Ct,F0r),e(Ct,Iue),e(Iue,C0r),e(Ct,M0r),e(Ct,Nc),e(Nc,E0r),e(Nc,Due),e(Due,y0r),e(Nc,w0r),e(Nc,jue),e(jue,A0r),e(Nc,L0r),e(Ct,B0r),e(Ct,Nue),e(Nue,x0r),e(Ct,k0r),g(j6,Ct,null),e(Br,R0r),e(Br,yo),g(N6,yo,null),e(yo,S0r),e(yo,que),e(que,P0r),e(yo,$0r),e(yo,Cn),e(Cn,I0r),e(Cn,Gue),e(Gue,D0r),e(Cn,j0r),e(Cn,Oue),e(Oue,N0r),e(Cn,q0r),e(Cn,Xue),e(Xue,G0r),e(Cn,O0r),e(yo,X0r),e(yo,Vue),e(Vue,y9),e(y9,zue),e(zue,V0r),e(y9,z0r),e(y9,YO),e(YO,W0r),e(y9,Q0r),e(yo,H0r),e(yo,Wue),e(Wue,U0r),e(yo,J0r),g(q6,yo,null),b(c,Lxe,u),b(c,qc,u),e(qc,w9),e(w9,Que),g(G6,Que,null),e(qc,Y0r),e(qc,Hue),e(Hue,K0r),b(c,Bxe,u),b(c,xr,u),g(O6,xr,null),e(xr,Z0r),e(xr,Gc),e(Gc,e1r),e(Gc,Uue),e(Uue,o1r),e(Gc,r1r),e(Gc,Jue),e(Jue,t1r),e(Gc,a1r),e(xr,n1r),e(xr,X6),e(X6,s1r),e(X6,Yue),e(Yue,l1r),e(X6,i1r),e(xr,d1r),e(xr,Mt),g(V6,Mt,null),e(Mt,c1r),e(Mt,Kue),e(Kue,f1r),e(Mt,m1r),e(Mt,Oc),e(Oc,g1r),e(Oc,Zue),e(Zue,h1r),e(Oc,p1r),e(Oc,e0e),e(e0e,_1r),e(Oc,u1r),e(Mt,b1r),e(Mt,o0e),e(o0e,v1r),e(Mt,T1r),g(z6,Mt,null),e(xr,F1r),e(xr,wo),g(W6,wo,null),e(wo,C1r),e(wo,r0e),e(r0e,M1r),e(wo,E1r),e(wo,Mn),e(Mn,y1r),e(Mn,t0e),e(t0e,w1r),e(Mn,A1r),e(Mn,a0e),e(a0e,L1r),e(Mn,B1r),e(Mn,n0e),e(n0e,x1r),e(Mn,k1r),e(wo,R1r),e(wo,V),e(V,A9),e(A9,s0e),e(s0e,S1r),e(A9,P1r),e(A9,KO),e(KO,$1r),e(A9,I1r),e(V,D1r),e(V,L9),e(L9,l0e),e(l0e,j1r),e(L9,N1r),e(L9,ZO),e(ZO,q1r),e(L9,G1r),e(V,O1r),e(V,B9),e(B9,i0e),e(i0e,X1r),e(B9,V1r),e(B9,eX),e(eX,z1r),e(B9,W1r),e(V,Q1r),e(V,x9),e(x9,d0e),e(d0e,H1r),e(x9,U1r),e(x9,oX),e(oX,J1r),e(x9,Y1r),e(V,K1r),e(V,k9),e(k9,c0e),e(c0e,Z1r),e(k9,ebr),e(k9,rX),e(rX,obr),e(k9,rbr),e(V,tbr),e(V,R9),e(R9,f0e),e(f0e,abr),e(R9,nbr),e(R9,tX),e(tX,sbr),e(R9,lbr),e(V,ibr),e(V,S9),e(S9,m0e),e(m0e,dbr),e(S9,cbr),e(S9,aX),e(aX,fbr),e(S9,mbr),e(V,gbr),e(V,P9),e(P9,g0e),e(g0e,hbr),e(P9,pbr),e(P9,nX),e(nX,_br),e(P9,ubr),e(V,bbr),e(V,$9),e($9,h0e),e(h0e,vbr),e($9,Tbr),e($9,sX),e(sX,Fbr),e($9,Cbr),e(V,Mbr),e(V,I9),e(I9,p0e),e(p0e,Ebr),e(I9,ybr),e(I9,lX),e(lX,wbr),e(I9,Abr),e(V,Lbr),e(V,D9),e(D9,_0e),e(_0e,Bbr),e(D9,xbr),e(D9,iX),e(iX,kbr),e(D9,Rbr),e(V,Sbr),e(V,j9),e(j9,u0e),e(u0e,Pbr),e(j9,$br),e(j9,dX),e(dX,Ibr),e(j9,Dbr),e(V,jbr),e(V,N9),e(N9,b0e),e(b0e,Nbr),e(N9,qbr),e(N9,cX),e(cX,Gbr),e(N9,Obr),e(V,Xbr),e(V,q9),e(q9,v0e),e(v0e,Vbr),e(q9,zbr),e(q9,fX),e(fX,Wbr),e(q9,Qbr),e(V,Hbr),e(V,G9),e(G9,T0e),e(T0e,Ubr),e(G9,Jbr),e(G9,mX),e(mX,Ybr),e(G9,Kbr),e(V,Zbr),e(V,O9),e(O9,F0e),e(F0e,e5r),e(O9,o5r),e(O9,gX),e(gX,r5r),e(O9,t5r),e(V,a5r),e(V,X9),e(X9,C0e),e(C0e,n5r),e(X9,s5r),e(X9,hX),e(hX,l5r),e(X9,i5r),e(V,d5r),e(V,V9),e(V9,M0e),e(M0e,c5r),e(V9,f5r),e(V9,pX),e(pX,m5r),e(V9,g5r),e(V,h5r),e(V,z9),e(z9,E0e),e(E0e,p5r),e(z9,_5r),e(z9,_X),e(_X,u5r),e(z9,b5r),e(V,v5r),e(V,W9),e(W9,y0e),e(y0e,T5r),e(W9,F5r),e(W9,uX),e(uX,C5r),e(W9,M5r),e(V,E5r),e(V,Q9),e(Q9,w0e),e(w0e,y5r),e(Q9,w5r),e(Q9,bX),e(bX,A5r),e(Q9,L5r),e(V,B5r),e(V,H9),e(H9,A0e),e(A0e,x5r),e(H9,k5r),e(H9,vX),e(vX,R5r),e(H9,S5r),e(V,P5r),e(V,U9),e(U9,L0e),e(L0e,$5r),e(U9,I5r),e(U9,TX),e(TX,D5r),e(U9,j5r),e(V,N5r),e(V,J9),e(J9,B0e),e(B0e,q5r),e(J9,G5r),e(J9,FX),e(FX,O5r),e(J9,X5r),e(V,V5r),e(V,Y9),e(Y9,x0e),e(x0e,z5r),e(Y9,W5r),e(Y9,k0e),e(k0e,Q5r),e(Y9,H5r),e(wo,U5r),e(wo,R0e),e(R0e,J5r),e(wo,Y5r),g(Q6,wo,null),b(c,xxe,u),b(c,Xc,u),e(Xc,K9),e(K9,S0e),g(H6,S0e,null),e(Xc,K5r),e(Xc,P0e),e(P0e,Z5r),b(c,kxe,u),b(c,kr,u),g(U6,kr,null),e(kr,e2r),e(kr,Vc),e(Vc,o2r),e(Vc,$0e),e($0e,r2r),e(Vc,t2r),e(Vc,I0e),e(I0e,a2r),e(Vc,n2r),e(kr,s2r),e(kr,J6),e(J6,l2r),e(J6,D0e),e(D0e,i2r),e(J6,d2r),e(kr,c2r),e(kr,Et),g(Y6,Et,null),e(Et,f2r),e(Et,j0e),e(j0e,m2r),e(Et,g2r),e(Et,zc),e(zc,h2r),e(zc,N0e),e(N0e,p2r),e(zc,_2r),e(zc,q0e),e(q0e,u2r),e(zc,b2r),e(Et,v2r),e(Et,G0e),e(G0e,T2r),e(Et,F2r),g(K6,Et,null),e(kr,C2r),e(kr,Ao),g(Z6,Ao,null),e(Ao,M2r),e(Ao,O0e),e(O0e,E2r),e(Ao,y2r),e(Ao,En),e(En,w2r),e(En,X0e),e(X0e,A2r),e(En,L2r),e(En,V0e),e(V0e,B2r),e(En,x2r),e(En,z0e),e(z0e,k2r),e(En,R2r),e(Ao,S2r),e(Ao,yn),e(yn,Z9),e(Z9,W0e),e(W0e,P2r),e(Z9,$2r),e(Z9,CX),e(CX,I2r),e(Z9,D2r),e(yn,j2r),e(yn,eC),e(eC,Q0e),e(Q0e,N2r),e(eC,q2r),e(eC,MX),e(MX,G2r),e(eC,O2r),e(yn,X2r),e(yn,oC),e(oC,H0e),e(H0e,V2r),e(oC,z2r),e(oC,EX),e(EX,W2r),e(oC,Q2r),e(yn,H2r),e(yn,rC),e(rC,U0e),e(U0e,U2r),e(rC,J2r),e(rC,yX),e(yX,Y2r),e(rC,K2r),e(Ao,Z2r),e(Ao,J0e),e(J0e,evr),e(Ao,ovr),g(eA,Ao,null),b(c,Rxe,u),b(c,Wc,u),e(Wc,tC),e(tC,Y0e),g(oA,Y0e,null),e(Wc,rvr),e(Wc,K0e),e(K0e,tvr),b(c,Sxe,u),b(c,Rr,u),g(rA,Rr,null),e(Rr,avr),e(Rr,Qc),e(Qc,nvr),e(Qc,Z0e),e(Z0e,svr),e(Qc,lvr),e(Qc,e1e),e(e1e,ivr),e(Qc,dvr),e(Rr,cvr),e(Rr,tA),e(tA,fvr),e(tA,o1e),e(o1e,mvr),e(tA,gvr),e(Rr,hvr),e(Rr,yt),g(aA,yt,null),e(yt,pvr),e(yt,r1e),e(r1e,_vr),e(yt,uvr),e(yt,Hc),e(Hc,bvr),e(Hc,t1e),e(t1e,vvr),e(Hc,Tvr),e(Hc,a1e),e(a1e,Fvr),e(Hc,Cvr),e(yt,Mvr),e(yt,n1e),e(n1e,Evr),e(yt,yvr),g(nA,yt,null),e(Rr,wvr),e(Rr,Lo),g(sA,Lo,null),e(Lo,Avr),e(Lo,s1e),e(s1e,Lvr),e(Lo,Bvr),e(Lo,wn),e(wn,xvr),e(wn,l1e),e(l1e,kvr),e(wn,Rvr),e(wn,i1e),e(i1e,Svr),e(wn,Pvr),e(wn,d1e),e(d1e,$vr),e(wn,Ivr),e(Lo,Dvr),e(Lo,ce),e(ce,aC),e(aC,c1e),e(c1e,jvr),e(aC,Nvr),e(aC,wX),e(wX,qvr),e(aC,Gvr),e(ce,Ovr),e(ce,nC),e(nC,f1e),e(f1e,Xvr),e(nC,Vvr),e(nC,AX),e(AX,zvr),e(nC,Wvr),e(ce,Qvr),e(ce,sC),e(sC,m1e),e(m1e,Hvr),e(sC,Uvr),e(sC,LX),e(LX,Jvr),e(sC,Yvr),e(ce,Kvr),e(ce,lC),e(lC,g1e),e(g1e,Zvr),e(lC,eTr),e(lC,BX),e(BX,oTr),e(lC,rTr),e(ce,tTr),e(ce,iC),e(iC,h1e),e(h1e,aTr),e(iC,nTr),e(iC,xX),e(xX,sTr),e(iC,lTr),e(ce,iTr),e(ce,dC),e(dC,p1e),e(p1e,dTr),e(dC,cTr),e(dC,kX),e(kX,fTr),e(dC,mTr),e(ce,gTr),e(ce,cC),e(cC,_1e),e(_1e,hTr),e(cC,pTr),e(cC,RX),e(RX,_Tr),e(cC,uTr),e(ce,bTr),e(ce,fC),e(fC,u1e),e(u1e,vTr),e(fC,TTr),e(fC,SX),e(SX,FTr),e(fC,CTr),e(ce,MTr),e(ce,mC),e(mC,b1e),e(b1e,ETr),e(mC,yTr),e(mC,PX),e(PX,wTr),e(mC,ATr),e(ce,LTr),e(ce,gC),e(gC,v1e),e(v1e,BTr),e(gC,xTr),e(gC,$X),e($X,kTr),e(gC,RTr),e(ce,STr),e(ce,hC),e(hC,T1e),e(T1e,PTr),e(hC,$Tr),e(hC,IX),e(IX,ITr),e(hC,DTr),e(ce,jTr),e(ce,pC),e(pC,F1e),e(F1e,NTr),e(pC,qTr),e(pC,C1e),e(C1e,GTr),e(pC,OTr),e(Lo,XTr),e(Lo,M1e),e(M1e,VTr),e(Lo,zTr),g(lA,Lo,null),b(c,Pxe,u),b(c,Uc,u),e(Uc,_C),e(_C,E1e),g(iA,E1e,null),e(Uc,WTr),e(Uc,y1e),e(y1e,QTr),b(c,$xe,u),b(c,Sr,u),g(dA,Sr,null),e(Sr,HTr),e(Sr,Jc),e(Jc,UTr),e(Jc,w1e),e(w1e,JTr),e(Jc,YTr),e(Jc,A1e),e(A1e,KTr),e(Jc,ZTr),e(Sr,eFr),e(Sr,cA),e(cA,oFr),e(cA,L1e),e(L1e,rFr),e(cA,tFr),e(Sr,aFr),e(Sr,wt),g(fA,wt,null),e(wt,nFr),e(wt,B1e),e(B1e,sFr),e(wt,lFr),e(wt,Yc),e(Yc,iFr),e(Yc,x1e),e(x1e,dFr),e(Yc,cFr),e(Yc,k1e),e(k1e,fFr),e(Yc,mFr),e(wt,gFr),e(wt,R1e),e(R1e,hFr),e(wt,pFr),g(mA,wt,null),e(Sr,_Fr),e(Sr,Bo),g(gA,Bo,null),e(Bo,uFr),e(Bo,S1e),e(S1e,bFr),e(Bo,vFr),e(Bo,An),e(An,TFr),e(An,P1e),e(P1e,FFr),e(An,CFr),e(An,$1e),e($1e,MFr),e(An,EFr),e(An,I1e),e(I1e,yFr),e(An,wFr),e(Bo,AFr),e(Bo,ue),e(ue,uC),e(uC,D1e),e(D1e,LFr),e(uC,BFr),e(uC,DX),e(DX,xFr),e(uC,kFr),e(ue,RFr),e(ue,bC),e(bC,j1e),e(j1e,SFr),e(bC,PFr),e(bC,jX),e(jX,$Fr),e(bC,IFr),e(ue,DFr),e(ue,vC),e(vC,N1e),e(N1e,jFr),e(vC,NFr),e(vC,NX),e(NX,qFr),e(vC,GFr),e(ue,OFr),e(ue,TC),e(TC,q1e),e(q1e,XFr),e(TC,VFr),e(TC,qX),e(qX,zFr),e(TC,WFr),e(ue,QFr),e(ue,FC),e(FC,G1e),e(G1e,HFr),e(FC,UFr),e(FC,GX),e(GX,JFr),e(FC,YFr),e(ue,KFr),e(ue,CC),e(CC,O1e),e(O1e,ZFr),e(CC,e9r),e(CC,OX),e(OX,o9r),e(CC,r9r),e(ue,t9r),e(ue,MC),e(MC,X1e),e(X1e,a9r),e(MC,n9r),e(MC,XX),e(XX,s9r),e(MC,l9r),e(ue,i9r),e(ue,EC),e(EC,V1e),e(V1e,d9r),e(EC,c9r),e(EC,VX),e(VX,f9r),e(EC,m9r),e(ue,g9r),e(ue,yC),e(yC,z1e),e(z1e,h9r),e(yC,p9r),e(yC,zX),e(zX,_9r),e(yC,u9r),e(ue,b9r),e(ue,wC),e(wC,W1e),e(W1e,v9r),e(wC,T9r),e(wC,Q1e),e(Q1e,F9r),e(wC,C9r),e(Bo,M9r),e(Bo,H1e),e(H1e,E9r),e(Bo,y9r),g(hA,Bo,null),b(c,Ixe,u),b(c,Kc,u),e(Kc,AC),e(AC,U1e),g(pA,U1e,null),e(Kc,w9r),e(Kc,J1e),e(J1e,A9r),b(c,Dxe,u),b(c,Pr,u),g(_A,Pr,null),e(Pr,L9r),e(Pr,Zc),e(Zc,B9r),e(Zc,Y1e),e(Y1e,x9r),e(Zc,k9r),e(Zc,K1e),e(K1e,R9r),e(Zc,S9r),e(Pr,P9r),e(Pr,uA),e(uA,$9r),e(uA,Z1e),e(Z1e,I9r),e(uA,D9r),e(Pr,j9r),e(Pr,At),g(bA,At,null),e(At,N9r),e(At,ebe),e(ebe,q9r),e(At,G9r),e(At,ef),e(ef,O9r),e(ef,obe),e(obe,X9r),e(ef,V9r),e(ef,rbe),e(rbe,z9r),e(ef,W9r),e(At,Q9r),e(At,tbe),e(tbe,H9r),e(At,U9r),g(vA,At,null),e(Pr,J9r),e(Pr,xo),g(TA,xo,null),e(xo,Y9r),e(xo,abe),e(abe,K9r),e(xo,Z9r),e(xo,Ln),e(Ln,eCr),e(Ln,nbe),e(nbe,oCr),e(Ln,rCr),e(Ln,sbe),e(sbe,tCr),e(Ln,aCr),e(Ln,lbe),e(lbe,nCr),e(Ln,sCr),e(xo,lCr),e(xo,Ce),e(Ce,LC),e(LC,ibe),e(ibe,iCr),e(LC,dCr),e(LC,WX),e(WX,cCr),e(LC,fCr),e(Ce,mCr),e(Ce,BC),e(BC,dbe),e(dbe,gCr),e(BC,hCr),e(BC,QX),e(QX,pCr),e(BC,_Cr),e(Ce,uCr),e(Ce,xC),e(xC,cbe),e(cbe,bCr),e(xC,vCr),e(xC,HX),e(HX,TCr),e(xC,FCr),e(Ce,CCr),e(Ce,kC),e(kC,fbe),e(fbe,MCr),e(kC,ECr),e(kC,UX),e(UX,yCr),e(kC,wCr),e(Ce,ACr),e(Ce,RC),e(RC,mbe),e(mbe,LCr),e(RC,BCr),e(RC,JX),e(JX,xCr),e(RC,kCr),e(Ce,RCr),e(Ce,SC),e(SC,gbe),e(gbe,SCr),e(SC,PCr),e(SC,YX),e(YX,$Cr),e(SC,ICr),e(Ce,DCr),e(Ce,PC),e(PC,hbe),e(hbe,jCr),e(PC,NCr),e(PC,KX),e(KX,qCr),e(PC,GCr),e(Ce,OCr),e(Ce,$C),e($C,pbe),e(pbe,XCr),e($C,VCr),e($C,ZX),e(ZX,zCr),e($C,WCr),e(Ce,QCr),e(Ce,IC),e(IC,_be),e(_be,HCr),e(IC,UCr),e(IC,eV),e(eV,JCr),e(IC,YCr),e(xo,KCr),e(xo,ube),e(ube,ZCr),e(xo,eMr),g(FA,xo,null),b(c,jxe,u),b(c,of,u),e(of,DC),e(DC,bbe),g(CA,bbe,null),e(of,oMr),e(of,vbe),e(vbe,rMr),b(c,Nxe,u),b(c,$r,u),g(MA,$r,null),e($r,tMr),e($r,rf),e(rf,aMr),e(rf,Tbe),e(Tbe,nMr),e(rf,sMr),e(rf,Fbe),e(Fbe,lMr),e(rf,iMr),e($r,dMr),e($r,EA),e(EA,cMr),e(EA,Cbe),e(Cbe,fMr),e(EA,mMr),e($r,gMr),e($r,Lt),g(yA,Lt,null),e(Lt,hMr),e(Lt,Mbe),e(Mbe,pMr),e(Lt,_Mr),e(Lt,tf),e(tf,uMr),e(tf,Ebe),e(Ebe,bMr),e(tf,vMr),e(tf,ybe),e(ybe,TMr),e(tf,FMr),e(Lt,CMr),e(Lt,wbe),e(wbe,MMr),e(Lt,EMr),g(wA,Lt,null),e($r,yMr),e($r,ko),g(AA,ko,null),e(ko,wMr),e(ko,Abe),e(Abe,AMr),e(ko,LMr),e(ko,Bn),e(Bn,BMr),e(Bn,Lbe),e(Lbe,xMr),e(Bn,kMr),e(Bn,Bbe),e(Bbe,RMr),e(Bn,SMr),e(Bn,xbe),e(xbe,PMr),e(Bn,$Mr),e(ko,IMr),e(ko,be),e(be,jC),e(jC,kbe),e(kbe,DMr),e(jC,jMr),e(jC,oV),e(oV,NMr),e(jC,qMr),e(be,GMr),e(be,NC),e(NC,Rbe),e(Rbe,OMr),e(NC,XMr),e(NC,rV),e(rV,VMr),e(NC,zMr),e(be,WMr),e(be,qC),e(qC,Sbe),e(Sbe,QMr),e(qC,HMr),e(qC,tV),e(tV,UMr),e(qC,JMr),e(be,YMr),e(be,GC),e(GC,Pbe),e(Pbe,KMr),e(GC,ZMr),e(GC,aV),e(aV,e4r),e(GC,o4r),e(be,r4r),e(be,OC),e(OC,$be),e($be,t4r),e(OC,a4r),e(OC,nV),e(nV,n4r),e(OC,s4r),e(be,l4r),e(be,XC),e(XC,Ibe),e(Ibe,i4r),e(XC,d4r),e(XC,sV),e(sV,c4r),e(XC,f4r),e(be,m4r),e(be,VC),e(VC,Dbe),e(Dbe,g4r),e(VC,h4r),e(VC,lV),e(lV,p4r),e(VC,_4r),e(be,u4r),e(be,zC),e(zC,jbe),e(jbe,b4r),e(zC,v4r),e(zC,iV),e(iV,T4r),e(zC,F4r),e(be,C4r),e(be,WC),e(WC,Nbe),e(Nbe,M4r),e(WC,E4r),e(WC,dV),e(dV,y4r),e(WC,w4r),e(be,A4r),e(be,QC),e(QC,qbe),e(qbe,L4r),e(QC,B4r),e(QC,Gbe),e(Gbe,x4r),e(QC,k4r),e(ko,R4r),e(ko,Obe),e(Obe,S4r),e(ko,P4r),g(LA,ko,null),b(c,qxe,u),b(c,af,u),e(af,HC),e(HC,Xbe),g(BA,Xbe,null),e(af,$4r),e(af,Vbe),e(Vbe,I4r),b(c,Gxe,u),b(c,Ir,u),g(xA,Ir,null),e(Ir,D4r),e(Ir,nf),e(nf,j4r),e(nf,zbe),e(zbe,N4r),e(nf,q4r),e(nf,Wbe),e(Wbe,G4r),e(nf,O4r),e(Ir,X4r),e(Ir,kA),e(kA,V4r),e(kA,Qbe),e(Qbe,z4r),e(kA,W4r),e(Ir,Q4r),e(Ir,Bt),g(RA,Bt,null),e(Bt,H4r),e(Bt,Hbe),e(Hbe,U4r),e(Bt,J4r),e(Bt,sf),e(sf,Y4r),e(sf,Ube),e(Ube,K4r),e(sf,Z4r),e(sf,Jbe),e(Jbe,eEr),e(sf,oEr),e(Bt,rEr),e(Bt,Ybe),e(Ybe,tEr),e(Bt,aEr),g(SA,Bt,null),e(Ir,nEr),e(Ir,Ro),g(PA,Ro,null),e(Ro,sEr),e(Ro,Kbe),e(Kbe,lEr),e(Ro,iEr),e(Ro,xn),e(xn,dEr),e(xn,Zbe),e(Zbe,cEr),e(xn,fEr),e(xn,e5e),e(e5e,mEr),e(xn,gEr),e(xn,o5e),e(o5e,hEr),e(xn,pEr),e(Ro,_Er),e(Ro,ve),e(ve,UC),e(UC,r5e),e(r5e,uEr),e(UC,bEr),e(UC,cV),e(cV,vEr),e(UC,TEr),e(ve,FEr),e(ve,JC),e(JC,t5e),e(t5e,CEr),e(JC,MEr),e(JC,fV),e(fV,EEr),e(JC,yEr),e(ve,wEr),e(ve,YC),e(YC,a5e),e(a5e,AEr),e(YC,LEr),e(YC,mV),e(mV,BEr),e(YC,xEr),e(ve,kEr),e(ve,KC),e(KC,n5e),e(n5e,REr),e(KC,SEr),e(KC,gV),e(gV,PEr),e(KC,$Er),e(ve,IEr),e(ve,ZC),e(ZC,s5e),e(s5e,DEr),e(ZC,jEr),e(ZC,hV),e(hV,NEr),e(ZC,qEr),e(ve,GEr),e(ve,eM),e(eM,l5e),e(l5e,OEr),e(eM,XEr),e(eM,pV),e(pV,VEr),e(eM,zEr),e(ve,WEr),e(ve,oM),e(oM,i5e),e(i5e,QEr),e(oM,HEr),e(oM,_V),e(_V,UEr),e(oM,JEr),e(ve,YEr),e(ve,rM),e(rM,d5e),e(d5e,KEr),e(rM,ZEr),e(rM,uV),e(uV,e3r),e(rM,o3r),e(ve,r3r),e(ve,tM),e(tM,c5e),e(c5e,t3r),e(tM,a3r),e(tM,bV),e(bV,n3r),e(tM,s3r),e(ve,l3r),e(ve,aM),e(aM,f5e),e(f5e,i3r),e(aM,d3r),e(aM,m5e),e(m5e,c3r),e(aM,f3r),e(Ro,m3r),e(Ro,g5e),e(g5e,g3r),e(Ro,h3r),g($A,Ro,null),b(c,Oxe,u),b(c,lf,u),e(lf,nM),e(nM,h5e),g(IA,h5e,null),e(lf,p3r),e(lf,p5e),e(p5e,_3r),b(c,Xxe,u),b(c,Dr,u),g(DA,Dr,null),e(Dr,u3r),e(Dr,df),e(df,b3r),e(df,_5e),e(_5e,v3r),e(df,T3r),e(df,u5e),e(u5e,F3r),e(df,C3r),e(Dr,M3r),e(Dr,jA),e(jA,E3r),e(jA,b5e),e(b5e,y3r),e(jA,w3r),e(Dr,A3r),e(Dr,xt),g(NA,xt,null),e(xt,L3r),e(xt,v5e),e(v5e,B3r),e(xt,x3r),e(xt,cf),e(cf,k3r),e(cf,T5e),e(T5e,R3r),e(cf,S3r),e(cf,F5e),e(F5e,P3r),e(cf,$3r),e(xt,I3r),e(xt,C5e),e(C5e,D3r),e(xt,j3r),g(qA,xt,null),e(Dr,N3r),e(Dr,So),g(GA,So,null),e(So,q3r),e(So,M5e),e(M5e,G3r),e(So,O3r),e(So,kn),e(kn,X3r),e(kn,E5e),e(E5e,V3r),e(kn,z3r),e(kn,y5e),e(y5e,W3r),e(kn,Q3r),e(kn,w5e),e(w5e,H3r),e(kn,U3r),e(So,J3r),e(So,Re),e(Re,sM),e(sM,A5e),e(A5e,Y3r),e(sM,K3r),e(sM,vV),e(vV,Z3r),e(sM,eyr),e(Re,oyr),e(Re,lM),e(lM,L5e),e(L5e,ryr),e(lM,tyr),e(lM,TV),e(TV,ayr),e(lM,nyr),e(Re,syr),e(Re,iM),e(iM,B5e),e(B5e,lyr),e(iM,iyr),e(iM,FV),e(FV,dyr),e(iM,cyr),e(Re,fyr),e(Re,dM),e(dM,x5e),e(x5e,myr),e(dM,gyr),e(dM,CV),e(CV,hyr),e(dM,pyr),e(Re,_yr),e(Re,cM),e(cM,k5e),e(k5e,uyr),e(cM,byr),e(cM,MV),e(MV,vyr),e(cM,Tyr),e(Re,Fyr),e(Re,fM),e(fM,R5e),e(R5e,Cyr),e(fM,Myr),e(fM,EV),e(EV,Eyr),e(fM,yyr),e(Re,wyr),e(Re,mM),e(mM,S5e),e(S5e,Ayr),e(mM,Lyr),e(mM,yV),e(yV,Byr),e(mM,xyr),e(Re,kyr),e(Re,gM),e(gM,P5e),e(P5e,Ryr),e(gM,Syr),e(gM,$5e),e($5e,Pyr),e(gM,$yr),e(So,Iyr),e(So,I5e),e(I5e,Dyr),e(So,jyr),g(OA,So,null),b(c,Vxe,u),b(c,ff,u),e(ff,hM),e(hM,D5e),g(XA,D5e,null),e(ff,Nyr),e(ff,j5e),e(j5e,qyr),b(c,zxe,u),b(c,jr,u),g(VA,jr,null),e(jr,Gyr),e(jr,mf),e(mf,Oyr),e(mf,N5e),e(N5e,Xyr),e(mf,Vyr),e(mf,q5e),e(q5e,zyr),e(mf,Wyr),e(jr,Qyr),e(jr,zA),e(zA,Hyr),e(zA,G5e),e(G5e,Uyr),e(zA,Jyr),e(jr,Yyr),e(jr,kt),g(WA,kt,null),e(kt,Kyr),e(kt,O5e),e(O5e,Zyr),e(kt,ewr),e(kt,gf),e(gf,owr),e(gf,X5e),e(X5e,rwr),e(gf,twr),e(gf,V5e),e(V5e,awr),e(gf,nwr),e(kt,swr),e(kt,z5e),e(z5e,lwr),e(kt,iwr),g(QA,kt,null),e(jr,dwr),e(jr,Po),g(HA,Po,null),e(Po,cwr),e(Po,W5e),e(W5e,fwr),e(Po,mwr),e(Po,Rn),e(Rn,gwr),e(Rn,Q5e),e(Q5e,hwr),e(Rn,pwr),e(Rn,H5e),e(H5e,_wr),e(Rn,uwr),e(Rn,U5e),e(U5e,bwr),e(Rn,vwr),e(Po,Twr),e(Po,Se),e(Se,pM),e(pM,J5e),e(J5e,Fwr),e(pM,Cwr),e(pM,wV),e(wV,Mwr),e(pM,Ewr),e(Se,ywr),e(Se,_M),e(_M,Y5e),e(Y5e,wwr),e(_M,Awr),e(_M,AV),e(AV,Lwr),e(_M,Bwr),e(Se,xwr),e(Se,uM),e(uM,K5e),e(K5e,kwr),e(uM,Rwr),e(uM,LV),e(LV,Swr),e(uM,Pwr),e(Se,$wr),e(Se,bM),e(bM,Z5e),e(Z5e,Iwr),e(bM,Dwr),e(bM,BV),e(BV,jwr),e(bM,Nwr),e(Se,qwr),e(Se,vM),e(vM,e2e),e(e2e,Gwr),e(vM,Owr),e(vM,xV),e(xV,Xwr),e(vM,Vwr),e(Se,zwr),e(Se,TM),e(TM,o2e),e(o2e,Wwr),e(TM,Qwr),e(TM,kV),e(kV,Hwr),e(TM,Uwr),e(Se,Jwr),e(Se,FM),e(FM,r2e),e(r2e,Ywr),e(FM,Kwr),e(FM,RV),e(RV,Zwr),e(FM,e6r),e(Se,o6r),e(Se,CM),e(CM,t2e),e(t2e,r6r),e(CM,t6r),e(CM,a2e),e(a2e,a6r),e(CM,n6r),e(Po,s6r),e(Po,n2e),e(n2e,l6r),e(Po,i6r),g(UA,Po,null),b(c,Wxe,u),b(c,hf,u),e(hf,MM),e(MM,s2e),g(JA,s2e,null),e(hf,d6r),e(hf,l2e),e(l2e,c6r),b(c,Qxe,u),b(c,Nr,u),g(YA,Nr,null),e(Nr,f6r),e(Nr,pf),e(pf,m6r),e(pf,i2e),e(i2e,g6r),e(pf,h6r),e(pf,d2e),e(d2e,p6r),e(pf,_6r),e(Nr,u6r),e(Nr,KA),e(KA,b6r),e(KA,c2e),e(c2e,v6r),e(KA,T6r),e(Nr,F6r),e(Nr,Rt),g(ZA,Rt,null),e(Rt,C6r),e(Rt,f2e),e(f2e,M6r),e(Rt,E6r),e(Rt,_f),e(_f,y6r),e(_f,m2e),e(m2e,w6r),e(_f,A6r),e(_f,g2e),e(g2e,L6r),e(_f,B6r),e(Rt,x6r),e(Rt,h2e),e(h2e,k6r),e(Rt,R6r),g(eL,Rt,null),e(Nr,S6r),e(Nr,$o),g(oL,$o,null),e($o,P6r),e($o,p2e),e(p2e,$6r),e($o,I6r),e($o,Sn),e(Sn,D6r),e(Sn,_2e),e(_2e,j6r),e(Sn,N6r),e(Sn,u2e),e(u2e,q6r),e(Sn,G6r),e(Sn,b2e),e(b2e,O6r),e(Sn,X6r),e($o,V6r),e($o,v2e),e(v2e,EM),e(EM,T2e),e(T2e,z6r),e(EM,W6r),e(EM,SV),e(SV,Q6r),e(EM,H6r),e($o,U6r),e($o,F2e),e(F2e,J6r),e($o,Y6r),g(rL,$o,null),b(c,Hxe,u),b(c,uf,u),e(uf,yM),e(yM,C2e),g(tL,C2e,null),e(uf,K6r),e(uf,M2e),e(M2e,Z6r),b(c,Uxe,u),b(c,qr,u),g(aL,qr,null),e(qr,eAr),e(qr,bf),e(bf,oAr),e(bf,E2e),e(E2e,rAr),e(bf,tAr),e(bf,y2e),e(y2e,aAr),e(bf,nAr),e(qr,sAr),e(qr,nL),e(nL,lAr),e(nL,w2e),e(w2e,iAr),e(nL,dAr),e(qr,cAr),e(qr,St),g(sL,St,null),e(St,fAr),e(St,A2e),e(A2e,mAr),e(St,gAr),e(St,vf),e(vf,hAr),e(vf,L2e),e(L2e,pAr),e(vf,_Ar),e(vf,B2e),e(B2e,uAr),e(vf,bAr),e(St,vAr),e(St,x2e),e(x2e,TAr),e(St,FAr),g(lL,St,null),e(qr,CAr),e(qr,Io),g(iL,Io,null),e(Io,MAr),e(Io,k2e),e(k2e,EAr),e(Io,yAr),e(Io,Pn),e(Pn,wAr),e(Pn,R2e),e(R2e,AAr),e(Pn,LAr),e(Pn,S2e),e(S2e,BAr),e(Pn,xAr),e(Pn,P2e),e(P2e,kAr),e(Pn,RAr),e(Io,SAr),e(Io,dL),e(dL,wM),e(wM,$2e),e($2e,PAr),e(wM,$Ar),e(wM,PV),e(PV,IAr),e(wM,DAr),e(dL,jAr),e(dL,AM),e(AM,I2e),e(I2e,NAr),e(AM,qAr),e(AM,$V),e($V,GAr),e(AM,OAr),e(Io,XAr),e(Io,D2e),e(D2e,VAr),e(Io,zAr),g(cL,Io,null),b(c,Jxe,u),b(c,Tf,u),e(Tf,LM),e(LM,j2e),g(fL,j2e,null),e(Tf,WAr),e(Tf,N2e),e(N2e,QAr),b(c,Yxe,u),b(c,Gr,u),g(mL,Gr,null),e(Gr,HAr),e(Gr,Ff),e(Ff,UAr),e(Ff,q2e),e(q2e,JAr),e(Ff,YAr),e(Ff,G2e),e(G2e,KAr),e(Ff,ZAr),e(Gr,eLr),e(Gr,gL),e(gL,oLr),e(gL,O2e),e(O2e,rLr),e(gL,tLr),e(Gr,aLr),e(Gr,Pt),g(hL,Pt,null),e(Pt,nLr),e(Pt,X2e),e(X2e,sLr),e(Pt,lLr),e(Pt,Cf),e(Cf,iLr),e(Cf,V2e),e(V2e,dLr),e(Cf,cLr),e(Cf,z2e),e(z2e,fLr),e(Cf,mLr),e(Pt,gLr),e(Pt,W2e),e(W2e,hLr),e(Pt,pLr),g(pL,Pt,null),e(Gr,_Lr),e(Gr,Do),g(_L,Do,null),e(Do,uLr),e(Do,Q2e),e(Q2e,bLr),e(Do,vLr),e(Do,$n),e($n,TLr),e($n,H2e),e(H2e,FLr),e($n,CLr),e($n,U2e),e(U2e,MLr),e($n,ELr),e($n,J2e),e(J2e,yLr),e($n,wLr),e(Do,ALr),e(Do,Y2e),e(Y2e,BM),e(BM,K2e),e(K2e,LLr),e(BM,BLr),e(BM,IV),e(IV,xLr),e(BM,kLr),e(Do,RLr),e(Do,Z2e),e(Z2e,SLr),e(Do,PLr),g(uL,Do,null),Kxe=!0},p(c,[u]){const bL={};u&2&&(bL.$$scope={dirty:u,ctx:c}),Bf.$set(bL);const eve={};u&2&&(eve.$$scope={dirty:u,ctx:c}),fh.$set(eve);const ove={};u&2&&(ove.$$scope={dirty:u,ctx:c}),Ch.$set(ove)},i(c){Kxe||(h(fe.$$.fragment,c),h($a.$$.fragment,c),h(S4.$$.fragment,c),h(P4.$$.fragment,c),h(Bf.$$.fragment,c),h($4.$$.fragment,c),h(I4.$$.fragment,c),h(N4.$$.fragment,c),h(q4.$$.fragment,c),h(G4.$$.fragment,c),h(O4.$$.fragment,c),h(X4.$$.fragment,c),h(W4.$$.fragment,c),h(Q4.$$.fragment,c),h(H4.$$.fragment,c),h(U4.$$.fragment,c),h(J4.$$.fragment,c),h(Z4.$$.fragment,c),h(fh.$$.fragment,c),h(eE.$$.fragment,c),h(oE.$$.fragment,c),h(rE.$$.fragment,c),h(tE.$$.fragment,c),h(sE.$$.fragment,c),h(Ch.$$.fragment,c),h(lE.$$.fragment,c),h(iE.$$.fragment,c),h(dE.$$.fragment,c),h(cE.$$.fragment,c),h(mE.$$.fragment,c),h(gE.$$.fragment,c),h(hE.$$.fragment,c),h(pE.$$.fragment,c),h(_E.$$.fragment,c),h(uE.$$.fragment,c),h(vE.$$.fragment,c),h(TE.$$.fragment,c),h(FE.$$.fragment,c),h(CE.$$.fragment,c),h(ME.$$.fragment,c),h(EE.$$.fragment,c),h(wE.$$.fragment,c),h(AE.$$.fragment,c),h(LE.$$.fragment,c),h(BE.$$.fragment,c),h(xE.$$.fragment,c),h(kE.$$.fragment,c),h(SE.$$.fragment,c),h(PE.$$.fragment,c),h($E.$$.fragment,c),h(IE.$$.fragment,c),h(DE.$$.fragment,c),h(jE.$$.fragment,c),h(qE.$$.fragment,c),h(GE.$$.fragment,c),h(OE.$$.fragment,c),h(XE.$$.fragment,c),h(VE.$$.fragment,c),h(zE.$$.fragment,c),h(QE.$$.fragment,c),h(HE.$$.fragment,c),h(UE.$$.fragment,c),h(JE.$$.fragment,c),h(YE.$$.fragment,c),h(KE.$$.fragment,c),h(e3.$$.fragment,c),h(o3.$$.fragment,c),h(r3.$$.fragment,c),h(t3.$$.fragment,c),h(a3.$$.fragment,c),h(n3.$$.fragment,c),h(l3.$$.fragment,c),h(i3.$$.fragment,c),h(d3.$$.fragment,c),h(c3.$$.fragment,c),h(f3.$$.fragment,c),h(m3.$$.fragment,c),h(h3.$$.fragment,c),h(p3.$$.fragment,c),h(_3.$$.fragment,c),h(u3.$$.fragment,c),h(b3.$$.fragment,c),h(v3.$$.fragment,c),h(F3.$$.fragment,c),h(C3.$$.fragment,c),h(M3.$$.fragment,c),h(E3.$$.fragment,c),h(y3.$$.fragment,c),h(w3.$$.fragment,c),h(L3.$$.fragment,c),h(B3.$$.fragment,c),h(x3.$$.fragment,c),h(k3.$$.fragment,c),h(R3.$$.fragment,c),h(S3.$$.fragment,c),h($3.$$.fragment,c),h(I3.$$.fragment,c),h(D3.$$.fragment,c),h(j3.$$.fragment,c),h(N3.$$.fragment,c),h(q3.$$.fragment,c),h(O3.$$.fragment,c),h(X3.$$.fragment,c),h(V3.$$.fragment,c),h(z3.$$.fragment,c),h(W3.$$.fragment,c),h(Q3.$$.fragment,c),h(U3.$$.fragment,c),h(J3.$$.fragment,c),h(Y3.$$.fragment,c),h(K3.$$.fragment,c),h(Z3.$$.fragment,c),h(ey.$$.fragment,c),h(ry.$$.fragment,c),h(ty.$$.fragment,c),h(ay.$$.fragment,c),h(ny.$$.fragment,c),h(sy.$$.fragment,c),h(ly.$$.fragment,c),h(dy.$$.fragment,c),h(cy.$$.fragment,c),h(fy.$$.fragment,c),h(my.$$.fragment,c),h(gy.$$.fragment,c),h(hy.$$.fragment,c),h(_y.$$.fragment,c),h(uy.$$.fragment,c),h(by.$$.fragment,c),h(Ty.$$.fragment,c),h(Fy.$$.fragment,c),h(Cy.$$.fragment,c),h(Ey.$$.fragment,c),h(yy.$$.fragment,c),h(wy.$$.fragment,c),h(Ay.$$.fragment,c),h(Ly.$$.fragment,c),h(By.$$.fragment,c),h(ky.$$.fragment,c),h(Ry.$$.fragment,c),h(Sy.$$.fragment,c),h(Py.$$.fragment,c),h($y.$$.fragment,c),h(Iy.$$.fragment,c),h(jy.$$.fragment,c),h(Ny.$$.fragment,c),h(qy.$$.fragment,c),h(Gy.$$.fragment,c),h(Oy.$$.fragment,c),h(Xy.$$.fragment,c),h(zy.$$.fragment,c),h(Wy.$$.fragment,c),h(Qy.$$.fragment,c),h(Hy.$$.fragment,c),h(Uy.$$.fragment,c),h(Jy.$$.fragment,c),h(Ky.$$.fragment,c),h(Zy.$$.fragment,c),h(ew.$$.fragment,c),h(rw.$$.fragment,c),h(tw.$$.fragment,c),h(aw.$$.fragment,c),h(sw.$$.fragment,c),h(lw.$$.fragment,c),h(iw.$$.fragment,c),h(dw.$$.fragment,c),h(cw.$$.fragment,c),h(fw.$$.fragment,c),h(gw.$$.fragment,c),h(hw.$$.fragment,c),h(pw.$$.fragment,c),h(_w.$$.fragment,c),h(uw.$$.fragment,c),h(bw.$$.fragment,c),h(Tw.$$.fragment,c),h(Fw.$$.fragment,c),h(Cw.$$.fragment,c),h(Mw.$$.fragment,c),h(Ew.$$.fragment,c),h(yw.$$.fragment,c),h(Aw.$$.fragment,c),h(Lw.$$.fragment,c),h(Bw.$$.fragment,c),h(kw.$$.fragment,c),h(Rw.$$.fragment,c),h(Sw.$$.fragment,c),h($w.$$.fragment,c),h(Iw.$$.fragment,c),h(Dw.$$.fragment,c),h(jw.$$.fragment,c),h(Nw.$$.fragment,c),h(qw.$$.fragment,c),h(Ow.$$.fragment,c),h(Xw.$$.fragment,c),h(Vw.$$.fragment,c),h(zw.$$.fragment,c),h(Ww.$$.fragment,c),h(Qw.$$.fragment,c),h(Uw.$$.fragment,c),h(Jw.$$.fragment,c),h(Yw.$$.fragment,c),h(Kw.$$.fragment,c),h(Zw.$$.fragment,c),h(e6.$$.fragment,c),h(r6.$$.fragment,c),h(t6.$$.fragment,c),h(a6.$$.fragment,c),h(n6.$$.fragment,c),h(s6.$$.fragment,c),h(l6.$$.fragment,c),h(d6.$$.fragment,c),h(c6.$$.fragment,c),h(f6.$$.fragment,c),h(m6.$$.fragment,c),h(g6.$$.fragment,c),h(h6.$$.fragment,c),h(_6.$$.fragment,c),h(u6.$$.fragment,c),h(b6.$$.fragment,c),h(v6.$$.fragment,c),h(T6.$$.fragment,c),h(F6.$$.fragment,c),h(M6.$$.fragment,c),h(E6.$$.fragment,c),h(y6.$$.fragment,c),h(w6.$$.fragment,c),h(A6.$$.fragment,c),h(L6.$$.fragment,c),h(x6.$$.fragment,c),h(k6.$$.fragment,c),h(R6.$$.fragment,c),h(S6.$$.fragment,c),h(P6.$$.fragment,c),h($6.$$.fragment,c),h(D6.$$.fragment,c),h(j6.$$.fragment,c),h(N6.$$.fragment,c),h(q6.$$.fragment,c),h(G6.$$.fragment,c),h(O6.$$.fragment,c),h(V6.$$.fragment,c),h(z6.$$.fragment,c),h(W6.$$.fragment,c),h(Q6.$$.fragment,c),h(H6.$$.fragment,c),h(U6.$$.fragment,c),h(Y6.$$.fragment,c),h(K6.$$.fragment,c),h(Z6.$$.fragment,c),h(eA.$$.fragment,c),h(oA.$$.fragment,c),h(rA.$$.fragment,c),h(aA.$$.fragment,c),h(nA.$$.fragment,c),h(sA.$$.fragment,c),h(lA.$$.fragment,c),h(iA.$$.fragment,c),h(dA.$$.fragment,c),h(fA.$$.fragment,c),h(mA.$$.fragment,c),h(gA.$$.fragment,c),h(hA.$$.fragment,c),h(pA.$$.fragment,c),h(_A.$$.fragment,c),h(bA.$$.fragment,c),h(vA.$$.fragment,c),h(TA.$$.fragment,c),h(FA.$$.fragment,c),h(CA.$$.fragment,c),h(MA.$$.fragment,c),h(yA.$$.fragment,c),h(wA.$$.fragment,c),h(AA.$$.fragment,c),h(LA.$$.fragment,c),h(BA.$$.fragment,c),h(xA.$$.fragment,c),h(RA.$$.fragment,c),h(SA.$$.fragment,c),h(PA.$$.fragment,c),h($A.$$.fragment,c),h(IA.$$.fragment,c),h(DA.$$.fragment,c),h(NA.$$.fragment,c),h(qA.$$.fragment,c),h(GA.$$.fragment,c),h(OA.$$.fragment,c),h(XA.$$.fragment,c),h(VA.$$.fragment,c),h(WA.$$.fragment,c),h(QA.$$.fragment,c),h(HA.$$.fragment,c),h(UA.$$.fragment,c),h(JA.$$.fragment,c),h(YA.$$.fragment,c),h(ZA.$$.fragment,c),h(eL.$$.fragment,c),h(oL.$$.fragment,c),h(rL.$$.fragment,c),h(tL.$$.fragment,c),h(aL.$$.fragment,c),h(sL.$$.fragment,c),h(lL.$$.fragment,c),h(iL.$$.fragment,c),h(cL.$$.fragment,c),h(fL.$$.fragment,c),h(mL.$$.fragment,c),h(hL.$$.fragment,c),h(pL.$$.fragment,c),h(_L.$$.fragment,c),h(uL.$$.fragment,c),Kxe=!0)},o(c){p(fe.$$.fragment,c),p($a.$$.fragment,c),p(S4.$$.fragment,c),p(P4.$$.fragment,c),p(Bf.$$.fragment,c),p($4.$$.fragment,c),p(I4.$$.fragment,c),p(N4.$$.fragment,c),p(q4.$$.fragment,c),p(G4.$$.fragment,c),p(O4.$$.fragment,c),p(X4.$$.fragment,c),p(W4.$$.fragment,c),p(Q4.$$.fragment,c),p(H4.$$.fragment,c),p(U4.$$.fragment,c),p(J4.$$.fragment,c),p(Z4.$$.fragment,c),p(fh.$$.fragment,c),p(eE.$$.fragment,c),p(oE.$$.fragment,c),p(rE.$$.fragment,c),p(tE.$$.fragment,c),p(sE.$$.fragment,c),p(Ch.$$.fragment,c),p(lE.$$.fragment,c),p(iE.$$.fragment,c),p(dE.$$.fragment,c),p(cE.$$.fragment,c),p(mE.$$.fragment,c),p(gE.$$.fragment,c),p(hE.$$.fragment,c),p(pE.$$.fragment,c),p(_E.$$.fragment,c),p(uE.$$.fragment,c),p(vE.$$.fragment,c),p(TE.$$.fragment,c),p(FE.$$.fragment,c),p(CE.$$.fragment,c),p(ME.$$.fragment,c),p(EE.$$.fragment,c),p(wE.$$.fragment,c),p(AE.$$.fragment,c),p(LE.$$.fragment,c),p(BE.$$.fragment,c),p(xE.$$.fragment,c),p(kE.$$.fragment,c),p(SE.$$.fragment,c),p(PE.$$.fragment,c),p($E.$$.fragment,c),p(IE.$$.fragment,c),p(DE.$$.fragment,c),p(jE.$$.fragment,c),p(qE.$$.fragment,c),p(GE.$$.fragment,c),p(OE.$$.fragment,c),p(XE.$$.fragment,c),p(VE.$$.fragment,c),p(zE.$$.fragment,c),p(QE.$$.fragment,c),p(HE.$$.fragment,c),p(UE.$$.fragment,c),p(JE.$$.fragment,c),p(YE.$$.fragment,c),p(KE.$$.fragment,c),p(e3.$$.fragment,c),p(o3.$$.fragment,c),p(r3.$$.fragment,c),p(t3.$$.fragment,c),p(a3.$$.fragment,c),p(n3.$$.fragment,c),p(l3.$$.fragment,c),p(i3.$$.fragment,c),p(d3.$$.fragment,c),p(c3.$$.fragment,c),p(f3.$$.fragment,c),p(m3.$$.fragment,c),p(h3.$$.fragment,c),p(p3.$$.fragment,c),p(_3.$$.fragment,c),p(u3.$$.fragment,c),p(b3.$$.fragment,c),p(v3.$$.fragment,c),p(F3.$$.fragment,c),p(C3.$$.fragment,c),p(M3.$$.fragment,c),p(E3.$$.fragment,c),p(y3.$$.fragment,c),p(w3.$$.fragment,c),p(L3.$$.fragment,c),p(B3.$$.fragment,c),p(x3.$$.fragment,c),p(k3.$$.fragment,c),p(R3.$$.fragment,c),p(S3.$$.fragment,c),p($3.$$.fragment,c),p(I3.$$.fragment,c),p(D3.$$.fragment,c),p(j3.$$.fragment,c),p(N3.$$.fragment,c),p(q3.$$.fragment,c),p(O3.$$.fragment,c),p(X3.$$.fragment,c),p(V3.$$.fragment,c),p(z3.$$.fragment,c),p(W3.$$.fragment,c),p(Q3.$$.fragment,c),p(U3.$$.fragment,c),p(J3.$$.fragment,c),p(Y3.$$.fragment,c),p(K3.$$.fragment,c),p(Z3.$$.fragment,c),p(ey.$$.fragment,c),p(ry.$$.fragment,c),p(ty.$$.fragment,c),p(ay.$$.fragment,c),p(ny.$$.fragment,c),p(sy.$$.fragment,c),p(ly.$$.fragment,c),p(dy.$$.fragment,c),p(cy.$$.fragment,c),p(fy.$$.fragment,c),p(my.$$.fragment,c),p(gy.$$.fragment,c),p(hy.$$.fragment,c),p(_y.$$.fragment,c),p(uy.$$.fragment,c),p(by.$$.fragment,c),p(Ty.$$.fragment,c),p(Fy.$$.fragment,c),p(Cy.$$.fragment,c),p(Ey.$$.fragment,c),p(yy.$$.fragment,c),p(wy.$$.fragment,c),p(Ay.$$.fragment,c),p(Ly.$$.fragment,c),p(By.$$.fragment,c),p(ky.$$.fragment,c),p(Ry.$$.fragment,c),p(Sy.$$.fragment,c),p(Py.$$.fragment,c),p($y.$$.fragment,c),p(Iy.$$.fragment,c),p(jy.$$.fragment,c),p(Ny.$$.fragment,c),p(qy.$$.fragment,c),p(Gy.$$.fragment,c),p(Oy.$$.fragment,c),p(Xy.$$.fragment,c),p(zy.$$.fragment,c),p(Wy.$$.fragment,c),p(Qy.$$.fragment,c),p(Hy.$$.fragment,c),p(Uy.$$.fragment,c),p(Jy.$$.fragment,c),p(Ky.$$.fragment,c),p(Zy.$$.fragment,c),p(ew.$$.fragment,c),p(rw.$$.fragment,c),p(tw.$$.fragment,c),p(aw.$$.fragment,c),p(sw.$$.fragment,c),p(lw.$$.fragment,c),p(iw.$$.fragment,c),p(dw.$$.fragment,c),p(cw.$$.fragment,c),p(fw.$$.fragment,c),p(gw.$$.fragment,c),p(hw.$$.fragment,c),p(pw.$$.fragment,c),p(_w.$$.fragment,c),p(uw.$$.fragment,c),p(bw.$$.fragment,c),p(Tw.$$.fragment,c),p(Fw.$$.fragment,c),p(Cw.$$.fragment,c),p(Mw.$$.fragment,c),p(Ew.$$.fragment,c),p(yw.$$.fragment,c),p(Aw.$$.fragment,c),p(Lw.$$.fragment,c),p(Bw.$$.fragment,c),p(kw.$$.fragment,c),p(Rw.$$.fragment,c),p(Sw.$$.fragment,c),p($w.$$.fragment,c),p(Iw.$$.fragment,c),p(Dw.$$.fragment,c),p(jw.$$.fragment,c),p(Nw.$$.fragment,c),p(qw.$$.fragment,c),p(Ow.$$.fragment,c),p(Xw.$$.fragment,c),p(Vw.$$.fragment,c),p(zw.$$.fragment,c),p(Ww.$$.fragment,c),p(Qw.$$.fragment,c),p(Uw.$$.fragment,c),p(Jw.$$.fragment,c),p(Yw.$$.fragment,c),p(Kw.$$.fragment,c),p(Zw.$$.fragment,c),p(e6.$$.fragment,c),p(r6.$$.fragment,c),p(t6.$$.fragment,c),p(a6.$$.fragment,c),p(n6.$$.fragment,c),p(s6.$$.fragment,c),p(l6.$$.fragment,c),p(d6.$$.fragment,c),p(c6.$$.fragment,c),p(f6.$$.fragment,c),p(m6.$$.fragment,c),p(g6.$$.fragment,c),p(h6.$$.fragment,c),p(_6.$$.fragment,c),p(u6.$$.fragment,c),p(b6.$$.fragment,c),p(v6.$$.fragment,c),p(T6.$$.fragment,c),p(F6.$$.fragment,c),p(M6.$$.fragment,c),p(E6.$$.fragment,c),p(y6.$$.fragment,c),p(w6.$$.fragment,c),p(A6.$$.fragment,c),p(L6.$$.fragment,c),p(x6.$$.fragment,c),p(k6.$$.fragment,c),p(R6.$$.fragment,c),p(S6.$$.fragment,c),p(P6.$$.fragment,c),p($6.$$.fragment,c),p(D6.$$.fragment,c),p(j6.$$.fragment,c),p(N6.$$.fragment,c),p(q6.$$.fragment,c),p(G6.$$.fragment,c),p(O6.$$.fragment,c),p(V6.$$.fragment,c),p(z6.$$.fragment,c),p(W6.$$.fragment,c),p(Q6.$$.fragment,c),p(H6.$$.fragment,c),p(U6.$$.fragment,c),p(Y6.$$.fragment,c),p(K6.$$.fragment,c),p(Z6.$$.fragment,c),p(eA.$$.fragment,c),p(oA.$$.fragment,c),p(rA.$$.fragment,c),p(aA.$$.fragment,c),p(nA.$$.fragment,c),p(sA.$$.fragment,c),p(lA.$$.fragment,c),p(iA.$$.fragment,c),p(dA.$$.fragment,c),p(fA.$$.fragment,c),p(mA.$$.fragment,c),p(gA.$$.fragment,c),p(hA.$$.fragment,c),p(pA.$$.fragment,c),p(_A.$$.fragment,c),p(bA.$$.fragment,c),p(vA.$$.fragment,c),p(TA.$$.fragment,c),p(FA.$$.fragment,c),p(CA.$$.fragment,c),p(MA.$$.fragment,c),p(yA.$$.fragment,c),p(wA.$$.fragment,c),p(AA.$$.fragment,c),p(LA.$$.fragment,c),p(BA.$$.fragment,c),p(xA.$$.fragment,c),p(RA.$$.fragment,c),p(SA.$$.fragment,c),p(PA.$$.fragment,c),p($A.$$.fragment,c),p(IA.$$.fragment,c),p(DA.$$.fragment,c),p(NA.$$.fragment,c),p(qA.$$.fragment,c),p(GA.$$.fragment,c),p(OA.$$.fragment,c),p(XA.$$.fragment,c),p(VA.$$.fragment,c),p(WA.$$.fragment,c),p(QA.$$.fragment,c),p(HA.$$.fragment,c),p(UA.$$.fragment,c),p(JA.$$.fragment,c),p(YA.$$.fragment,c),p(ZA.$$.fragment,c),p(eL.$$.fragment,c),p(oL.$$.fragment,c),p(rL.$$.fragment,c),p(tL.$$.fragment,c),p(aL.$$.fragment,c),p(sL.$$.fragment,c),p(lL.$$.fragment,c),p(iL.$$.fragment,c),p(cL.$$.fragment,c),p(fL.$$.fragment,c),p(mL.$$.fragment,c),p(hL.$$.fragment,c),p(pL.$$.fragment,c),p(_L.$$.fragment,c),p(uL.$$.fragment,c),Kxe=!1},d(c){t(J),c&&t(Pe),c&&t(ie),_(fe),c&&t(Ef),c&&t(sa),c&&t(Le),c&&t(io),c&&t(wf),_($a,c),c&&t(co),c&&t(he),c&&t(Vo),c&&t(Ia),c&&t(Z7e),c&&t($i),_(S4),c&&t(eBe),c&&t(qn),c&&t(oBe),_(P4,c),c&&t(rBe),c&&t(v8),c&&t(tBe),_(Bf,c),c&&t(aBe),c&&t(Ii),_($4),c&&t(nBe),c&&t(zo),_(I4),_(N4),_(q4),_(G4),c&&t(sBe),c&&t(ji),_(O4),c&&t(lBe),c&&t(Wo),_(X4),_(W4),_(Q4),_(H4),c&&t(iBe),c&&t(Ni),_(U4),c&&t(dBe),c&&t(Qo),_(J4),_(Z4),_(fh),_(eE),_(oE),c&&t(cBe),c&&t(qi),_(rE),c&&t(fBe),c&&t(Ho),_(tE),_(sE),_(Ch),_(lE),_(iE),c&&t(mBe),c&&t(Oi),_(dE),c&&t(gBe),c&&t(Uo),_(cE),_(mE),_(gE),_(hE),_(pE),c&&t(hBe),c&&t(zi),_(_E),c&&t(pBe),c&&t(Jo),_(uE),_(vE),_(TE),_(FE),_(CE),c&&t(_Be),c&&t(Hi),_(ME),c&&t(uBe),c&&t(Yo),_(EE),_(wE),_(AE),_(LE),_(BE),c&&t(bBe),c&&t(Yi),_(xE),c&&t(vBe),c&&t(Ko),_(kE),_(SE),_(PE),_($E),_(IE),c&&t(TBe),c&&t(ed),_(DE),c&&t(FBe),c&&t(Zo),_(jE),_(qE),_(GE),_(OE),_(XE),c&&t(CBe),c&&t(td),_(VE),c&&t(MBe),c&&t(er),_(zE),_(QE),_(HE),_(UE),_(JE),c&&t(EBe),c&&t(sd),_(YE),c&&t(yBe),c&&t(or),_(KE),_(e3),_(o3),_(r3),_(t3),c&&t(wBe),c&&t(dd),_(a3),c&&t(ABe),c&&t(rr),_(n3),_(l3),_(i3),_(d3),_(c3),c&&t(LBe),c&&t(md),_(f3),c&&t(BBe),c&&t(tr),_(m3),_(h3),_(p3),_(_3),_(u3),c&&t(xBe),c&&t(pd),_(b3),c&&t(kBe),c&&t(ar),_(v3),_(F3),_(C3),_(M3),_(E3),c&&t(RBe),c&&t(bd),_(y3),c&&t(SBe),c&&t(nr),_(w3),_(L3),_(B3),_(x3),_(k3),c&&t(PBe),c&&t(Fd),_(R3),c&&t($Be),c&&t(sr),_(S3),_($3),_(I3),_(D3),_(j3),c&&t(IBe),c&&t(Ed),_(N3),c&&t(DBe),c&&t(lr),_(q3),_(O3),_(X3),_(V3),_(z3),c&&t(jBe),c&&t(Ad),_(W3),c&&t(NBe),c&&t(ir),_(Q3),_(U3),_(J3),_(Y3),_(K3),c&&t(qBe),c&&t(xd),_(Z3),c&&t(GBe),c&&t(dr),_(ey),_(ry),_(ty),_(ay),_(ny),c&&t(OBe),c&&t(Sd),_(sy),c&&t(XBe),c&&t(cr),_(ly),_(dy),_(cy),_(fy),_(my),c&&t(VBe),c&&t(Id),_(gy),c&&t(zBe),c&&t(fr),_(hy),_(_y),_(uy),_(by),_(Ty),c&&t(WBe),c&&t(Nd),_(Fy),c&&t(QBe),c&&t(mr),_(Cy),_(Ey),_(yy),_(wy),_(Ay),c&&t(HBe),c&&t(Od),_(Ly),c&&t(UBe),c&&t(gr),_(By),_(ky),_(Ry),_(Sy),_(Py),c&&t(JBe),c&&t(Wd),_($y),c&&t(YBe),c&&t(hr),_(Iy),_(jy),_(Ny),_(qy),_(Gy),c&&t(KBe),c&&t(Ud),_(Oy),c&&t(ZBe),c&&t(pr),_(Xy),_(zy),_(Wy),_(Qy),_(Hy),c&&t(exe),c&&t(Kd),_(Uy),c&&t(oxe),c&&t(_r),_(Jy),_(Ky),_(Zy),_(ew),_(rw),c&&t(rxe),c&&t(oc),_(tw),c&&t(txe),c&&t(ur),_(aw),_(sw),_(lw),_(iw),_(dw),c&&t(axe),c&&t(ac),_(cw),c&&t(nxe),c&&t(br),_(fw),_(gw),_(hw),_(pw),_(_w),c&&t(sxe),c&&t(lc),_(uw),c&&t(lxe),c&&t(vr),_(bw),_(Tw),_(Fw),_(Cw),_(Mw),c&&t(ixe),c&&t(cc),_(Ew),c&&t(dxe),c&&t(Tr),_(yw),_(Aw),_(Lw),_(Bw),_(kw),c&&t(cxe),c&&t(gc),_(Rw),c&&t(fxe),c&&t(Fr),_(Sw),_($w),_(Iw),_(Dw),_(jw),c&&t(mxe),c&&t(_c),_(Nw),c&&t(gxe),c&&t(Cr),_(qw),_(Ow),_(Xw),_(Vw),_(zw),c&&t(hxe),c&&t(vc),_(Ww),c&&t(pxe),c&&t(Mr),_(Qw),_(Uw),_(Jw),_(Yw),_(Kw),c&&t(_xe),c&&t(Cc),_(Zw),c&&t(uxe),c&&t(Er),_(e6),_(r6),_(t6),_(a6),_(n6),c&&t(bxe),c&&t(yc),_(s6),c&&t(vxe),c&&t(yr),_(l6),_(d6),_(c6),_(f6),_(m6),c&&t(Txe),c&&t(Lc),_(g6),c&&t(Fxe),c&&t(wr),_(h6),_(_6),_(u6),_(b6),_(v6),c&&t(Cxe),c&&t(kc),_(T6),c&&t(Mxe),c&&t(Ar),_(F6),_(M6),_(E6),_(y6),_(w6),c&&t(Exe),c&&t(Pc),_(A6),c&&t(yxe),c&&t(Lr),_(L6),_(x6),_(k6),_(R6),_(S6),c&&t(wxe),c&&t(Dc),_(P6),c&&t(Axe),c&&t(Br),_($6),_(D6),_(j6),_(N6),_(q6),c&&t(Lxe),c&&t(qc),_(G6),c&&t(Bxe),c&&t(xr),_(O6),_(V6),_(z6),_(W6),_(Q6),c&&t(xxe),c&&t(Xc),_(H6),c&&t(kxe),c&&t(kr),_(U6),_(Y6),_(K6),_(Z6),_(eA),c&&t(Rxe),c&&t(Wc),_(oA),c&&t(Sxe),c&&t(Rr),_(rA),_(aA),_(nA),_(sA),_(lA),c&&t(Pxe),c&&t(Uc),_(iA),c&&t($xe),c&&t(Sr),_(dA),_(fA),_(mA),_(gA),_(hA),c&&t(Ixe),c&&t(Kc),_(pA),c&&t(Dxe),c&&t(Pr),_(_A),_(bA),_(vA),_(TA),_(FA),c&&t(jxe),c&&t(of),_(CA),c&&t(Nxe),c&&t($r),_(MA),_(yA),_(wA),_(AA),_(LA),c&&t(qxe),c&&t(af),_(BA),c&&t(Gxe),c&&t(Ir),_(xA),_(RA),_(SA),_(PA),_($A),c&&t(Oxe),c&&t(lf),_(IA),c&&t(Xxe),c&&t(Dr),_(DA),_(NA),_(qA),_(GA),_(OA),c&&t(Vxe),c&&t(ff),_(XA),c&&t(zxe),c&&t(jr),_(VA),_(WA),_(QA),_(HA),_(UA),c&&t(Wxe),c&&t(hf),_(JA),c&&t(Qxe),c&&t(Nr),_(YA),_(ZA),_(eL),_(oL),_(rL),c&&t(Hxe),c&&t(uf),_(tL),c&&t(Uxe),c&&t(qr),_(aL),_(sL),_(lL),_(iL),_(cL),c&&t(Jxe),c&&t(Tf),_(fL),c&&t(Yxe),c&&t(Gr),_(mL),_(hL),_(pL),_(_L),_(uL)}}}const G5t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function O5t(Ai,J,Pe){let{fw:ie}=J;return Ai.$$set=ge=>{"fw"in ge&&Pe(0,ie=ge.fw)},[ie]}class U5t extends S5t{constructor(J){super();P5t(this,J,O5t,q5t,$5t,{fw:0})}}export{U5t as default,G5t as metadata};
