import{S as H5t,i as U5t,s as J5t,e as a,k as l,w as f,t as o,M as Y5t,c as n,d as t,m as i,a as s,x as m,h as r,b as d,F as e,g as b,y as g,q as h,o as p,B as _}from"../../chunks/vendor-4833417e.js";import{T as QLr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-7b52c3d4.js";import{C as w}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as z}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function K5t(Li){let J,Pe,ie,ge,lo,fe,Te,Xo,Bi,yf,sa,xi,ki,x4,wf,Le,io,Ri,In,k4,Dn,jn,R4,Si,Nn,S4,Pi,Af,$a;return{c(){J=a("p"),Pe=o("If your "),ie=a("code"),ge=o("NewModelConfig"),lo=o(" is a subclass of "),fe=a("code"),Te=o("PretrainedConfig"),Xo=o(`, make sure its
`),Bi=a("code"),yf=o("model_type"),sa=o(" attribute is set to the same key you use when registering the config (here "),xi=a("code"),ki=o('"new-model"'),x4=o(")."),wf=l(),Le=a("p"),io=o("Likewise, if your "),Ri=a("code"),In=o("NewModel"),k4=o(" is a subclass of "),Dn=a("a"),jn=o("PreTrainedModel"),R4=o(`, make sure its
`),Si=a("code"),Nn=o("config_class"),S4=o(` attribute is set to the same class you use when registering the model (here
`),Pi=a("code"),Af=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=n(co,"P",{});var he=s(J);Pe=r(he,"If your "),ie=n(he,"CODE",{});var _8=s(ie);ge=r(_8,"NewModelConfig"),_8.forEach(t),lo=r(he," is a subclass of "),fe=n(he,"CODE",{});var $i=s(fe);Te=r($i,"PretrainedConfig"),$i.forEach(t),Xo=r(he,`, make sure its
`),Bi=n(he,"CODE",{});var u8=s(Bi);yf=r(u8,"model_type"),u8.forEach(t),sa=r(he," attribute is set to the same key you use when registering the config (here "),xi=n(he,"CODE",{});var b8=s(xi);ki=r(b8,'"new-model"'),b8.forEach(t),x4=r(he,")."),he.forEach(t),wf=i(co),Le=n(co,"P",{});var Vo=s(Le);io=r(Vo,"Likewise, if your "),Ri=n(Vo,"CODE",{});var Ia=s(Ri);In=r(Ia,"NewModel"),Ia.forEach(t),k4=r(Vo," is a subclass of "),Dn=n(Vo,"A",{href:!0});var v8=s(Dn);jn=r(v8,"PreTrainedModel"),v8.forEach(t),R4=r(Vo,`, make sure its
`),Si=n(Vo,"CODE",{});var Lf=s(Si);Nn=r(Lf,"config_class"),Lf.forEach(t),S4=r(Vo,` attribute is set to the same class you use when registering the model (here
`),Pi=n(Vo,"CODE",{});var T8=s(Pi);Af=r(T8,"NewModelConfig"),T8.forEach(t),$a=r(Vo,")."),Vo.forEach(t),this.h()},h(){d(Dn,"href","/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel")},m(co,he){b(co,J,he),e(J,Pe),e(J,ie),e(ie,ge),e(J,lo),e(J,fe),e(fe,Te),e(J,Xo),e(J,Bi),e(Bi,yf),e(J,sa),e(J,xi),e(xi,ki),e(J,x4),b(co,wf,he),b(co,Le,he),e(Le,io),e(Le,Ri),e(Ri,In),e(Le,k4),e(Le,Dn),e(Dn,jn),e(Le,R4),e(Le,Si),e(Si,Nn),e(Le,S4),e(Le,Pi),e(Pi,Af),e(Le,$a)},d(co){co&&t(J),co&&t(wf),co&&t(Le)}}}function Z5t(Li){let J,Pe,ie,ge,lo;return{c(){J=a("p"),Pe=o("Passing "),ie=a("code"),ge=o("use_auth_token=True"),lo=o(" is required when you want to use a private model.")},l(fe){J=n(fe,"P",{});var Te=s(J);Pe=r(Te,"Passing "),ie=n(Te,"CODE",{});var Xo=s(ie);ge=r(Xo,"use_auth_token=True"),Xo.forEach(t),lo=r(Te," is required when you want to use a private model."),Te.forEach(t)},m(fe,Te){b(fe,J,Te),e(J,Pe),e(J,ie),e(ie,ge),e(J,lo)},d(fe){fe&&t(J)}}}function e2t(Li){let J,Pe,ie,ge,lo;return{c(){J=a("p"),Pe=o("Passing "),ie=a("code"),ge=o("use_auth_token=True"),lo=o(" is required when you want to use a private model.")},l(fe){J=n(fe,"P",{});var Te=s(J);Pe=r(Te,"Passing "),ie=n(Te,"CODE",{});var Xo=s(ie);ge=r(Xo,"use_auth_token=True"),Xo.forEach(t),lo=r(Te," is required when you want to use a private model."),Te.forEach(t)},m(fe,Te){b(fe,J,Te),e(J,Pe),e(J,ie),e(ie,ge),e(J,lo)},d(fe){fe&&t(J)}}}function o2t(Li){let J,Pe,ie,ge,lo,fe,Te,Xo,Bi,yf,sa,xi,ki,x4,wf,Le,io,Ri,In,k4,Dn,jn,R4,Si,Nn,S4,Pi,Af,$a,co,he,_8,$i,u8,b8,Vo,Ia,v8,Lf,T8,hSe,aBe,Ii,Bf,jW,P4,pSe,NW,_Se,nBe,qn,uSe,qW,bSe,vSe,GW,TSe,FSe,sBe,$4,lBe,F8,CSe,iBe,xf,dBe,Di,kf,OW,I4,MSe,XW,ESe,cBe,zo,D4,ySe,j4,wSe,C8,ASe,LSe,BSe,N4,xSe,VW,kSe,RSe,SSe,fo,q4,PSe,zW,$Se,ISe,ji,DSe,WW,jSe,NSe,QW,qSe,GSe,OSe,v,Rf,HW,XSe,VSe,M8,zSe,WSe,QSe,Sf,UW,HSe,USe,E8,JSe,YSe,KSe,Pf,JW,ZSe,ePe,y8,oPe,rPe,tPe,$f,YW,aPe,nPe,w8,sPe,lPe,iPe,If,KW,dPe,cPe,A8,fPe,mPe,gPe,Df,ZW,hPe,pPe,L8,_Pe,uPe,bPe,jf,eQ,vPe,TPe,B8,FPe,CPe,MPe,Nf,oQ,EPe,yPe,x8,wPe,APe,LPe,qf,rQ,BPe,xPe,k8,kPe,RPe,SPe,Gf,tQ,PPe,$Pe,R8,IPe,DPe,jPe,Of,aQ,NPe,qPe,S8,GPe,OPe,XPe,Xf,nQ,VPe,zPe,P8,WPe,QPe,HPe,Vf,sQ,UPe,JPe,$8,YPe,KPe,ZPe,zf,lQ,e$e,o$e,I8,r$e,t$e,a$e,Wf,iQ,n$e,s$e,D8,l$e,i$e,d$e,Qf,dQ,c$e,f$e,j8,m$e,g$e,h$e,Hf,cQ,p$e,_$e,N8,u$e,b$e,v$e,Uf,fQ,T$e,F$e,q8,C$e,M$e,E$e,Jf,mQ,y$e,w$e,G8,A$e,L$e,B$e,Yf,gQ,x$e,k$e,O8,R$e,S$e,P$e,Kf,hQ,$$e,I$e,X8,D$e,j$e,N$e,Zf,pQ,q$e,G$e,V8,O$e,X$e,V$e,em,_Q,z$e,W$e,z8,Q$e,H$e,U$e,om,uQ,J$e,Y$e,W8,K$e,Z$e,eIe,rm,bQ,oIe,rIe,Q8,tIe,aIe,nIe,tm,vQ,sIe,lIe,H8,iIe,dIe,cIe,am,TQ,fIe,mIe,U8,gIe,hIe,pIe,nm,FQ,_Ie,uIe,J8,bIe,vIe,TIe,sm,CQ,FIe,CIe,Y8,MIe,EIe,yIe,lm,MQ,wIe,AIe,K8,LIe,BIe,xIe,im,EQ,kIe,RIe,Z8,SIe,PIe,$Ie,dm,yQ,IIe,DIe,e7,jIe,NIe,qIe,cm,wQ,GIe,OIe,o7,XIe,VIe,zIe,fm,AQ,WIe,QIe,r7,HIe,UIe,JIe,mm,LQ,YIe,KIe,t7,ZIe,eDe,oDe,gm,BQ,rDe,tDe,a7,aDe,nDe,sDe,hm,xQ,lDe,iDe,n7,dDe,cDe,fDe,pm,kQ,mDe,gDe,s7,hDe,pDe,_De,_m,RQ,uDe,bDe,l7,vDe,TDe,FDe,um,SQ,CDe,MDe,i7,EDe,yDe,wDe,bm,PQ,ADe,LDe,d7,BDe,xDe,kDe,vm,$Q,RDe,SDe,c7,PDe,$De,IDe,Tm,IQ,DDe,jDe,f7,NDe,qDe,GDe,Fm,DQ,ODe,XDe,m7,VDe,zDe,WDe,Cm,jQ,QDe,HDe,g7,UDe,JDe,YDe,Mm,NQ,KDe,ZDe,h7,eje,oje,rje,Em,qQ,tje,aje,p7,nje,sje,lje,ym,GQ,ije,dje,_7,cje,fje,mje,wm,OQ,gje,hje,u7,pje,_je,uje,Am,XQ,bje,vje,b7,Tje,Fje,Cje,Lm,VQ,Mje,Eje,v7,yje,wje,Aje,Bm,zQ,Lje,Bje,T7,xje,kje,Rje,xm,WQ,Sje,Pje,F7,$je,Ije,Dje,km,QQ,jje,Nje,C7,qje,Gje,Oje,Rm,HQ,Xje,Vje,M7,zje,Wje,Qje,Sm,UQ,Hje,Uje,E7,Jje,Yje,Kje,Pm,JQ,Zje,eNe,y7,oNe,rNe,tNe,$m,YQ,aNe,nNe,w7,sNe,lNe,iNe,Im,KQ,dNe,cNe,A7,fNe,mNe,gNe,Dm,ZQ,hNe,pNe,L7,_Ne,uNe,bNe,jm,eH,vNe,TNe,B7,FNe,CNe,MNe,Nm,oH,ENe,yNe,x7,wNe,ANe,LNe,qm,rH,BNe,xNe,k7,kNe,RNe,SNe,Gm,tH,PNe,$Ne,R7,INe,DNe,jNe,Om,aH,NNe,qNe,S7,GNe,ONe,XNe,Xm,nH,VNe,zNe,P7,WNe,QNe,HNe,Vm,sH,UNe,JNe,$7,YNe,KNe,ZNe,zm,lH,eqe,oqe,I7,rqe,tqe,aqe,Wm,iH,nqe,sqe,D7,lqe,iqe,dqe,Qm,dH,cqe,fqe,j7,mqe,gqe,hqe,Hm,cH,pqe,_qe,N7,uqe,bqe,vqe,Um,fH,Tqe,Fqe,q7,Cqe,Mqe,Eqe,Jm,mH,yqe,wqe,G7,Aqe,Lqe,Bqe,Ym,gH,xqe,kqe,O7,Rqe,Sqe,Pqe,Km,hH,$qe,Iqe,X7,Dqe,jqe,Nqe,Zm,pH,qqe,Gqe,V7,Oqe,Xqe,Vqe,eg,_H,zqe,Wqe,z7,Qqe,Hqe,Uqe,og,uH,Jqe,Yqe,W7,Kqe,Zqe,eGe,rg,bH,oGe,rGe,Q7,tGe,aGe,nGe,tg,vH,sGe,lGe,H7,iGe,dGe,cGe,ag,TH,fGe,mGe,U7,gGe,hGe,pGe,ng,FH,_Ge,uGe,J7,bGe,vGe,TGe,sg,CH,FGe,CGe,Y7,MGe,EGe,yGe,lg,MH,wGe,AGe,K7,LGe,BGe,xGe,ig,EH,kGe,RGe,Z7,SGe,PGe,$Ge,dg,yH,IGe,DGe,eB,jGe,NGe,qGe,cg,wH,GGe,OGe,oB,XGe,VGe,zGe,fg,AH,WGe,QGe,rB,HGe,UGe,JGe,mg,LH,YGe,KGe,tB,ZGe,eOe,oOe,gg,BH,rOe,tOe,aB,aOe,nOe,sOe,hg,xH,lOe,iOe,nB,dOe,cOe,fOe,pg,kH,mOe,gOe,sB,hOe,pOe,_Oe,_g,RH,uOe,bOe,lB,vOe,TOe,FOe,ug,SH,COe,MOe,iB,EOe,yOe,wOe,PH,AOe,LOe,G4,BOe,bg,O4,xOe,$H,kOe,fBe,Ni,vg,IH,X4,ROe,DH,SOe,mBe,Wo,V4,POe,z4,$Oe,dB,IOe,DOe,jOe,W4,NOe,jH,qOe,GOe,OOe,mo,Q4,XOe,NH,VOe,zOe,Da,WOe,qH,QOe,HOe,GH,UOe,JOe,OH,YOe,KOe,ZOe,M,Gn,XH,eXe,oXe,cB,rXe,tXe,fB,aXe,nXe,sXe,On,VH,lXe,iXe,mB,dXe,cXe,gB,fXe,mXe,gXe,Xn,zH,hXe,pXe,hB,_Xe,uXe,pB,bXe,vXe,TXe,Tg,WH,FXe,CXe,_B,MXe,EXe,yXe,Vn,QH,wXe,AXe,uB,LXe,BXe,bB,xXe,kXe,RXe,Fg,HH,SXe,PXe,vB,$Xe,IXe,DXe,Cg,UH,jXe,NXe,TB,qXe,GXe,OXe,Mg,JH,XXe,VXe,FB,zXe,WXe,QXe,zn,YH,HXe,UXe,CB,JXe,YXe,MB,KXe,ZXe,eVe,Wn,KH,oVe,rVe,EB,tVe,aVe,yB,nVe,sVe,lVe,Qn,ZH,iVe,dVe,wB,cVe,fVe,AB,mVe,gVe,hVe,Eg,eU,pVe,_Ve,LB,uVe,bVe,vVe,yg,oU,TVe,FVe,BB,CVe,MVe,EVe,Hn,rU,yVe,wVe,xB,AVe,LVe,kB,BVe,xVe,kVe,wg,tU,RVe,SVe,RB,PVe,$Ve,IVe,Un,aU,DVe,jVe,SB,NVe,qVe,PB,GVe,OVe,XVe,Jn,nU,VVe,zVe,$B,WVe,QVe,IB,HVe,UVe,JVe,Yn,sU,YVe,KVe,DB,ZVe,eze,lU,oze,rze,tze,Ag,iU,aze,nze,jB,sze,lze,ize,Kn,dU,dze,cze,NB,fze,mze,qB,gze,hze,pze,Lg,cU,_ze,uze,GB,bze,vze,Tze,Zn,fU,Fze,Cze,OB,Mze,Eze,XB,yze,wze,Aze,es,mU,Lze,Bze,VB,xze,kze,zB,Rze,Sze,Pze,os,gU,$ze,Ize,WB,Dze,jze,QB,Nze,qze,Gze,Bg,hU,Oze,Xze,HB,Vze,zze,Wze,rs,pU,Qze,Hze,UB,Uze,Jze,JB,Yze,Kze,Zze,xg,_U,eWe,oWe,YB,rWe,tWe,aWe,ts,uU,nWe,sWe,KB,lWe,iWe,ZB,dWe,cWe,fWe,as,bU,mWe,gWe,ex,hWe,pWe,ox,_We,uWe,bWe,ns,vU,vWe,TWe,rx,FWe,CWe,tx,MWe,EWe,yWe,ss,TU,wWe,AWe,ax,LWe,BWe,nx,xWe,kWe,RWe,kg,FU,SWe,PWe,sx,$We,IWe,DWe,ls,CU,jWe,NWe,lx,qWe,GWe,ix,OWe,XWe,VWe,is,MU,zWe,WWe,dx,QWe,HWe,cx,UWe,JWe,YWe,ds,EU,KWe,ZWe,fx,eQe,oQe,mx,rQe,tQe,aQe,cs,yU,nQe,sQe,gx,lQe,iQe,hx,dQe,cQe,fQe,fs,wU,mQe,gQe,px,hQe,pQe,_x,_Qe,uQe,bQe,ms,AU,vQe,TQe,ux,FQe,CQe,bx,MQe,EQe,yQe,Rg,LU,wQe,AQe,vx,LQe,BQe,xQe,gs,BU,kQe,RQe,Tx,SQe,PQe,Fx,$Qe,IQe,DQe,Sg,xU,jQe,NQe,Cx,qQe,GQe,OQe,Pg,kU,XQe,VQe,Mx,zQe,WQe,QQe,hs,RU,HQe,UQe,Ex,JQe,YQe,yx,KQe,ZQe,eHe,ps,SU,oHe,rHe,wx,tHe,aHe,Ax,nHe,sHe,lHe,$g,PU,iHe,dHe,Lx,cHe,fHe,mHe,_s,$U,gHe,hHe,Bx,pHe,_He,xx,uHe,bHe,vHe,us,IU,THe,FHe,kx,CHe,MHe,Rx,EHe,yHe,wHe,bs,DU,AHe,LHe,Sx,BHe,xHe,Px,kHe,RHe,SHe,vs,jU,PHe,$He,$x,IHe,DHe,Ix,jHe,NHe,qHe,Ts,NU,GHe,OHe,Dx,XHe,VHe,jx,zHe,WHe,QHe,Ig,qU,HHe,UHe,Nx,JHe,YHe,KHe,Dg,GU,ZHe,eUe,qx,oUe,rUe,tUe,jg,OU,aUe,nUe,Gx,sUe,lUe,iUe,Ng,XU,dUe,cUe,Ox,fUe,mUe,gUe,Fs,VU,hUe,pUe,Xx,_Ue,uUe,Vx,bUe,vUe,TUe,qg,zU,FUe,CUe,zx,MUe,EUe,yUe,Cs,WU,wUe,AUe,Wx,LUe,BUe,Qx,xUe,kUe,RUe,Ms,QU,SUe,PUe,Hx,$Ue,IUe,Ux,DUe,jUe,NUe,Es,HU,qUe,GUe,Jx,OUe,XUe,Yx,VUe,zUe,WUe,ys,UU,QUe,HUe,Kx,UUe,JUe,Zx,YUe,KUe,ZUe,ws,JU,eJe,oJe,ek,rJe,tJe,ok,aJe,nJe,sJe,As,YU,lJe,iJe,rk,dJe,cJe,tk,fJe,mJe,gJe,Gg,KU,hJe,pJe,ak,_Je,uJe,bJe,Og,ZU,vJe,TJe,nk,FJe,CJe,MJe,Ls,eJ,EJe,yJe,sk,wJe,AJe,lk,LJe,BJe,xJe,Bs,oJ,kJe,RJe,ik,SJe,PJe,dk,$Je,IJe,DJe,xs,rJ,jJe,NJe,ck,qJe,GJe,fk,OJe,XJe,VJe,Xg,tJ,zJe,WJe,mk,QJe,HJe,UJe,Vg,aJ,JJe,YJe,gk,KJe,ZJe,eYe,zg,nJ,oYe,rYe,hk,tYe,aYe,nYe,Wg,sJ,sYe,lYe,pk,iYe,dYe,cYe,ks,lJ,fYe,mYe,_k,gYe,hYe,uk,pYe,_Ye,uYe,Qg,iJ,bYe,vYe,bk,TYe,FYe,CYe,Hg,dJ,MYe,EYe,vk,yYe,wYe,AYe,Rs,cJ,LYe,BYe,Tk,xYe,kYe,Fk,RYe,SYe,PYe,Ss,fJ,$Ye,IYe,Ck,DYe,jYe,Mk,NYe,qYe,GYe,mJ,OYe,XYe,H4,VYe,Ug,U4,zYe,gJ,WYe,gBe,qi,Jg,hJ,J4,QYe,pJ,HYe,hBe,Qo,Y4,UYe,K4,JYe,Ek,YYe,KYe,ZYe,Z4,eKe,_J,oKe,rKe,tKe,$e,eE,aKe,uJ,nKe,sKe,ja,lKe,bJ,iKe,dKe,vJ,cKe,fKe,TJ,mKe,gKe,hKe,se,Yg,FJ,pKe,_Ke,yk,uKe,bKe,vKe,Kg,CJ,TKe,FKe,wk,CKe,MKe,EKe,Zg,MJ,yKe,wKe,Ak,AKe,LKe,BKe,eh,EJ,xKe,kKe,Lk,RKe,SKe,PKe,oh,yJ,$Ke,IKe,Bk,DKe,jKe,NKe,rh,wJ,qKe,GKe,xk,OKe,XKe,VKe,th,AJ,zKe,WKe,kk,QKe,HKe,UKe,ah,LJ,JKe,YKe,Rk,KKe,ZKe,eZe,nh,BJ,oZe,rZe,Sk,tZe,aZe,nZe,sh,xJ,sZe,lZe,Pk,iZe,dZe,cZe,lh,kJ,fZe,mZe,$k,gZe,hZe,pZe,ih,RJ,_Ze,uZe,Ik,bZe,vZe,TZe,dh,SJ,FZe,CZe,Dk,MZe,EZe,yZe,ch,PJ,wZe,AZe,jk,LZe,BZe,xZe,fh,$J,kZe,RZe,Nk,SZe,PZe,$Ze,mh,IZe,IJ,DZe,jZe,oE,NZe,gh,rE,qZe,DJ,GZe,pBe,Gi,hh,jJ,tE,OZe,NJ,XZe,_Be,Ho,aE,VZe,nE,zZe,qk,WZe,QZe,HZe,sE,UZe,qJ,JZe,YZe,KZe,Ie,lE,ZZe,GJ,eeo,oeo,Oi,reo,OJ,teo,aeo,XJ,neo,seo,leo,Be,ph,VJ,ieo,deo,Gk,ceo,feo,meo,_h,zJ,geo,heo,Ok,peo,_eo,ueo,uh,WJ,beo,veo,Xk,Teo,Feo,Ceo,bh,QJ,Meo,Eeo,Vk,yeo,weo,Aeo,vh,HJ,Leo,Beo,zk,xeo,keo,Reo,Th,UJ,Seo,Peo,Wk,$eo,Ieo,Deo,Fh,JJ,jeo,Neo,Qk,qeo,Geo,Oeo,Ch,YJ,Xeo,Veo,Hk,zeo,Weo,Qeo,Mh,Heo,KJ,Ueo,Jeo,iE,Yeo,Eh,dE,Keo,ZJ,Zeo,uBe,Xi,yh,eY,cE,eoo,oY,ooo,bBe,Uo,fE,roo,Vi,too,rY,aoo,noo,tY,soo,loo,ioo,mE,doo,aY,coo,foo,moo,Or,gE,goo,nY,hoo,poo,zi,_oo,sY,uoo,boo,lY,voo,Too,Foo,iY,Coo,Moo,hE,Eoo,De,pE,yoo,dY,woo,Aoo,Na,Loo,cY,Boo,xoo,fY,koo,Roo,mY,Soo,Poo,$oo,F,wh,gY,Ioo,Doo,Uk,joo,Noo,qoo,Ah,hY,Goo,Ooo,Jk,Xoo,Voo,zoo,Lh,pY,Woo,Qoo,Yk,Hoo,Uoo,Joo,Bh,_Y,Yoo,Koo,Kk,Zoo,ero,oro,xh,uY,rro,tro,Zk,aro,nro,sro,kh,bY,lro,iro,eR,dro,cro,fro,Rh,vY,mro,gro,oR,hro,pro,_ro,Sh,TY,uro,bro,rR,vro,Tro,Fro,Ph,FY,Cro,Mro,tR,Ero,yro,wro,$h,CY,Aro,Lro,aR,Bro,xro,kro,Ih,MY,Rro,Sro,nR,Pro,$ro,Iro,Dh,EY,Dro,jro,sR,Nro,qro,Gro,jh,yY,Oro,Xro,lR,Vro,zro,Wro,Nh,wY,Qro,Hro,iR,Uro,Jro,Yro,qh,AY,Kro,Zro,dR,eto,oto,rto,Gh,LY,tto,ato,cR,nto,sto,lto,Oh,BY,ito,dto,fR,cto,fto,mto,Xh,xY,gto,hto,mR,pto,_to,uto,Vh,kY,bto,vto,gR,Tto,Fto,Cto,zh,RY,Mto,Eto,hR,yto,wto,Ato,Wh,SY,Lto,Bto,pR,xto,kto,Rto,Qh,PY,Sto,Pto,_R,$to,Ito,Dto,Hh,$Y,jto,Nto,uR,qto,Gto,Oto,Uh,IY,Xto,Vto,bR,zto,Wto,Qto,Jh,DY,Hto,Uto,vR,Jto,Yto,Kto,Yh,jY,Zto,eao,TR,oao,rao,tao,Kh,NY,aao,nao,FR,sao,lao,iao,Ps,qY,dao,cao,CR,fao,mao,MR,gao,hao,pao,Zh,GY,_ao,uao,ER,bao,vao,Tao,ep,OY,Fao,Cao,yR,Mao,Eao,yao,op,XY,wao,Aao,wR,Lao,Bao,xao,rp,VY,kao,Rao,AR,Sao,Pao,$ao,tp,zY,Iao,Dao,LR,jao,Nao,qao,ap,WY,Gao,Oao,BR,Xao,Vao,zao,np,QY,Wao,Qao,xR,Hao,Uao,Jao,sp,HY,Yao,Kao,kR,Zao,eno,ono,lp,UY,rno,tno,RR,ano,nno,sno,ip,JY,lno,ino,SR,dno,cno,fno,dp,YY,mno,gno,PR,hno,pno,_no,cp,KY,uno,bno,$R,vno,Tno,Fno,fp,ZY,Cno,Mno,IR,Eno,yno,wno,mp,eK,Ano,Lno,DR,Bno,xno,kno,gp,oK,Rno,Sno,jR,Pno,$no,Ino,hp,rK,Dno,jno,NR,Nno,qno,Gno,pp,tK,Ono,Xno,qR,Vno,zno,Wno,_p,aK,Qno,Hno,GR,Uno,Jno,Yno,up,nK,Kno,Zno,OR,eso,oso,rso,bp,sK,tso,aso,XR,nso,sso,lso,vp,lK,iso,dso,VR,cso,fso,mso,Tp,iK,gso,hso,zR,pso,_so,uso,Fp,dK,bso,vso,WR,Tso,Fso,Cso,Cp,cK,Mso,Eso,QR,yso,wso,Aso,Mp,fK,Lso,Bso,HR,xso,kso,Rso,Ep,mK,Sso,Pso,UR,$so,Iso,Dso,yp,gK,jso,Nso,JR,qso,Gso,Oso,wp,hK,Xso,Vso,YR,zso,Wso,Qso,Ap,pK,Hso,Uso,KR,Jso,Yso,Kso,Lp,_K,Zso,elo,ZR,olo,rlo,tlo,Bp,uK,alo,nlo,eS,slo,llo,ilo,xp,bK,dlo,clo,oS,flo,mlo,glo,kp,vK,hlo,plo,rS,_lo,ulo,blo,Rp,TK,vlo,Tlo,tS,Flo,Clo,Mlo,Sp,FK,Elo,ylo,aS,wlo,Alo,Llo,Pp,CK,Blo,xlo,nS,klo,Rlo,Slo,$p,MK,Plo,$lo,sS,Ilo,Dlo,jlo,Ip,EK,Nlo,qlo,lS,Glo,Olo,Xlo,Dp,yK,Vlo,zlo,iS,Wlo,Qlo,Hlo,jp,wK,Ulo,Jlo,dS,Ylo,Klo,Zlo,Np,AK,eio,oio,cS,rio,tio,aio,qp,LK,nio,sio,fS,lio,iio,dio,Gp,BK,cio,fio,mS,mio,gio,hio,Op,xK,pio,_io,gS,uio,bio,vio,Xp,kK,Tio,Fio,hS,Cio,Mio,Eio,Vp,RK,yio,wio,pS,Aio,Lio,Bio,zp,SK,xio,kio,_S,Rio,Sio,Pio,Wp,PK,$io,Iio,uS,Dio,jio,Nio,Qp,$K,qio,Gio,bS,Oio,Xio,Vio,Hp,IK,zio,Wio,vS,Qio,Hio,Uio,Up,DK,Jio,Yio,TS,Kio,Zio,edo,Jp,jK,odo,rdo,FS,tdo,ado,ndo,Yp,NK,sdo,ldo,CS,ido,ddo,cdo,Kp,qK,fdo,mdo,MS,gdo,hdo,pdo,Zp,GK,_do,udo,ES,bdo,vdo,Tdo,e_,OK,Fdo,Cdo,yS,Mdo,Edo,ydo,o_,XK,wdo,Ado,wS,Ldo,Bdo,xdo,r_,VK,kdo,Rdo,AS,Sdo,Pdo,$do,t_,zK,Ido,Ddo,LS,jdo,Ndo,qdo,a_,Gdo,WK,Odo,Xdo,QK,Vdo,zdo,HK,Wdo,Qdo,_E,vBe,Wi,n_,UK,uE,Hdo,JK,Udo,TBe,Jo,bE,Jdo,Qi,Ydo,YK,Kdo,Zdo,KK,eco,oco,rco,vE,tco,ZK,aco,nco,sco,Xr,TE,lco,eZ,ico,dco,Hi,cco,oZ,fco,mco,rZ,gco,hco,pco,tZ,_co,uco,FE,bco,je,CE,vco,aZ,Tco,Fco,qa,Cco,nZ,Mco,Eco,sZ,yco,wco,lZ,Aco,Lco,Bco,k,s_,iZ,xco,kco,BS,Rco,Sco,Pco,l_,dZ,$co,Ico,xS,Dco,jco,Nco,i_,cZ,qco,Gco,kS,Oco,Xco,Vco,d_,fZ,zco,Wco,RS,Qco,Hco,Uco,c_,mZ,Jco,Yco,SS,Kco,Zco,efo,f_,gZ,ofo,rfo,PS,tfo,afo,nfo,m_,hZ,sfo,lfo,$S,ifo,dfo,cfo,g_,pZ,ffo,mfo,IS,gfo,hfo,pfo,h_,_Z,_fo,ufo,DS,bfo,vfo,Tfo,p_,uZ,Ffo,Cfo,jS,Mfo,Efo,yfo,__,bZ,wfo,Afo,NS,Lfo,Bfo,xfo,u_,vZ,kfo,Rfo,qS,Sfo,Pfo,$fo,b_,TZ,Ifo,Dfo,GS,jfo,Nfo,qfo,v_,FZ,Gfo,Ofo,OS,Xfo,Vfo,zfo,T_,CZ,Wfo,Qfo,XS,Hfo,Ufo,Jfo,F_,MZ,Yfo,Kfo,VS,Zfo,emo,omo,C_,EZ,rmo,tmo,zS,amo,nmo,smo,M_,yZ,lmo,imo,WS,dmo,cmo,fmo,E_,wZ,mmo,gmo,QS,hmo,pmo,_mo,y_,AZ,umo,bmo,HS,vmo,Tmo,Fmo,w_,LZ,Cmo,Mmo,US,Emo,ymo,wmo,A_,BZ,Amo,Lmo,JS,Bmo,xmo,kmo,L_,xZ,Rmo,Smo,YS,Pmo,$mo,Imo,B_,kZ,Dmo,jmo,KS,Nmo,qmo,Gmo,x_,RZ,Omo,Xmo,ZS,Vmo,zmo,Wmo,k_,SZ,Qmo,Hmo,eP,Umo,Jmo,Ymo,R_,PZ,Kmo,Zmo,oP,ego,ogo,rgo,S_,$Z,tgo,ago,rP,ngo,sgo,lgo,P_,IZ,igo,dgo,tP,cgo,fgo,mgo,$_,DZ,ggo,hgo,aP,pgo,_go,ugo,I_,jZ,bgo,vgo,nP,Tgo,Fgo,Cgo,D_,NZ,Mgo,Ego,sP,ygo,wgo,Ago,j_,qZ,Lgo,Bgo,lP,xgo,kgo,Rgo,N_,GZ,Sgo,Pgo,iP,$go,Igo,Dgo,q_,OZ,jgo,Ngo,dP,qgo,Ggo,Ogo,G_,XZ,Xgo,Vgo,cP,zgo,Wgo,Qgo,O_,VZ,Hgo,Ugo,fP,Jgo,Ygo,Kgo,X_,zZ,Zgo,eho,mP,oho,rho,tho,V_,WZ,aho,nho,gP,sho,lho,iho,z_,dho,QZ,cho,fho,HZ,mho,gho,UZ,hho,pho,ME,FBe,Ui,W_,JZ,EE,_ho,YZ,uho,CBe,Yo,yE,bho,Ji,vho,KZ,Tho,Fho,ZZ,Cho,Mho,Eho,wE,yho,eee,who,Aho,Lho,Vr,AE,Bho,oee,xho,kho,Yi,Rho,ree,Sho,Pho,tee,$ho,Iho,Dho,aee,jho,Nho,LE,qho,Ne,BE,Gho,nee,Oho,Xho,Ga,Vho,see,zho,Who,lee,Qho,Hho,iee,Uho,Jho,Yho,$,Q_,dee,Kho,Zho,hP,epo,opo,rpo,H_,cee,tpo,apo,pP,npo,spo,lpo,U_,fee,ipo,dpo,_P,cpo,fpo,mpo,J_,mee,gpo,hpo,uP,ppo,_po,upo,Y_,gee,bpo,vpo,bP,Tpo,Fpo,Cpo,K_,hee,Mpo,Epo,vP,ypo,wpo,Apo,Z_,pee,Lpo,Bpo,TP,xpo,kpo,Rpo,eu,_ee,Spo,Ppo,FP,$po,Ipo,Dpo,ou,uee,jpo,Npo,CP,qpo,Gpo,Opo,ru,bee,Xpo,Vpo,MP,zpo,Wpo,Qpo,tu,vee,Hpo,Upo,EP,Jpo,Ypo,Kpo,au,Tee,Zpo,e_o,yP,o_o,r_o,t_o,nu,Fee,a_o,n_o,wP,s_o,l_o,i_o,su,Cee,d_o,c_o,AP,f_o,m_o,g_o,lu,Mee,h_o,p_o,LP,__o,u_o,b_o,iu,Eee,v_o,T_o,BP,F_o,C_o,M_o,du,yee,E_o,y_o,xP,w_o,A_o,L_o,cu,wee,B_o,x_o,kP,k_o,R_o,S_o,fu,Aee,P_o,$_o,RP,I_o,D_o,j_o,mu,Lee,N_o,q_o,SP,G_o,O_o,X_o,gu,Bee,V_o,z_o,PP,W_o,Q_o,H_o,hu,xee,U_o,J_o,$P,Y_o,K_o,Z_o,pu,kee,euo,ouo,IP,ruo,tuo,auo,_u,Ree,nuo,suo,DP,luo,iuo,duo,uu,See,cuo,fuo,jP,muo,guo,huo,bu,Pee,puo,_uo,NP,uuo,buo,vuo,vu,$ee,Tuo,Fuo,qP,Cuo,Muo,Euo,Tu,Iee,yuo,wuo,GP,Auo,Luo,Buo,Fu,Dee,xuo,kuo,OP,Ruo,Suo,Puo,Cu,jee,$uo,Iuo,XP,Duo,juo,Nuo,Mu,Nee,quo,Guo,VP,Ouo,Xuo,Vuo,Eu,qee,zuo,Wuo,zP,Quo,Huo,Uuo,yu,Gee,Juo,Yuo,WP,Kuo,Zuo,e0o,wu,Oee,o0o,r0o,QP,t0o,a0o,n0o,Au,Xee,s0o,l0o,HP,i0o,d0o,c0o,Lu,f0o,Vee,m0o,g0o,zee,h0o,p0o,Wee,_0o,u0o,xE,MBe,Ki,Bu,Qee,kE,b0o,Hee,v0o,EBe,Ko,RE,T0o,Zi,F0o,Uee,C0o,M0o,Jee,E0o,y0o,w0o,SE,A0o,Yee,L0o,B0o,x0o,zr,PE,k0o,Kee,R0o,S0o,ed,P0o,Zee,$0o,I0o,eoe,D0o,j0o,N0o,ooe,q0o,G0o,$E,O0o,qe,IE,X0o,roe,V0o,z0o,Oa,W0o,toe,Q0o,H0o,aoe,U0o,J0o,noe,Y0o,K0o,Z0o,I,xu,soe,e1o,o1o,UP,r1o,t1o,a1o,ku,loe,n1o,s1o,JP,l1o,i1o,d1o,Ru,ioe,c1o,f1o,YP,m1o,g1o,h1o,Su,doe,p1o,_1o,KP,u1o,b1o,v1o,Pu,coe,T1o,F1o,ZP,C1o,M1o,E1o,$u,foe,y1o,w1o,e$,A1o,L1o,B1o,Iu,moe,x1o,k1o,o$,R1o,S1o,P1o,Du,goe,$1o,I1o,r$,D1o,j1o,N1o,ju,hoe,q1o,G1o,t$,O1o,X1o,V1o,Nu,poe,z1o,W1o,a$,Q1o,H1o,U1o,qu,_oe,J1o,Y1o,n$,K1o,Z1o,ebo,Gu,uoe,obo,rbo,s$,tbo,abo,nbo,Ou,boe,sbo,lbo,l$,ibo,dbo,cbo,Xu,voe,fbo,mbo,i$,gbo,hbo,pbo,Vu,Toe,_bo,ubo,d$,bbo,vbo,Tbo,zu,Foe,Fbo,Cbo,c$,Mbo,Ebo,ybo,Wu,Coe,wbo,Abo,f$,Lbo,Bbo,xbo,Qu,Moe,kbo,Rbo,m$,Sbo,Pbo,$bo,Hu,Eoe,Ibo,Dbo,g$,jbo,Nbo,qbo,Uu,yoe,Gbo,Obo,h$,Xbo,Vbo,zbo,Ju,woe,Wbo,Qbo,p$,Hbo,Ubo,Jbo,Yu,Aoe,Ybo,Kbo,_$,Zbo,e5o,o5o,Ku,Loe,r5o,t5o,u$,a5o,n5o,s5o,Zu,Boe,l5o,i5o,b$,d5o,c5o,f5o,e0,xoe,m5o,g5o,v$,h5o,p5o,_5o,o0,koe,u5o,b5o,T$,v5o,T5o,F5o,r0,Roe,C5o,M5o,F$,E5o,y5o,w5o,t0,Soe,A5o,L5o,C$,B5o,x5o,k5o,a0,Poe,R5o,S5o,M$,P5o,$5o,I5o,n0,$oe,D5o,j5o,E$,N5o,q5o,G5o,s0,Ioe,O5o,X5o,Doe,V5o,z5o,W5o,l0,joe,Q5o,H5o,y$,U5o,J5o,Y5o,i0,Noe,K5o,Z5o,w$,e2o,o2o,r2o,d0,qoe,t2o,a2o,A$,n2o,s2o,l2o,c0,Goe,i2o,d2o,L$,c2o,f2o,m2o,f0,g2o,Ooe,h2o,p2o,Xoe,_2o,u2o,Voe,b2o,v2o,DE,yBe,od,m0,zoe,jE,T2o,Woe,F2o,wBe,Zo,NE,C2o,rd,M2o,Qoe,E2o,y2o,Hoe,w2o,A2o,L2o,qE,B2o,Uoe,x2o,k2o,R2o,Wr,GE,S2o,Joe,P2o,$2o,td,I2o,Yoe,D2o,j2o,Koe,N2o,q2o,G2o,Zoe,O2o,X2o,OE,V2o,Ge,XE,z2o,ere,W2o,Q2o,Xa,H2o,ore,U2o,J2o,rre,Y2o,K2o,tre,Z2o,evo,ovo,ae,g0,are,rvo,tvo,B$,avo,nvo,svo,h0,nre,lvo,ivo,x$,dvo,cvo,fvo,p0,sre,mvo,gvo,k$,hvo,pvo,_vo,_0,lre,uvo,bvo,R$,vvo,Tvo,Fvo,u0,ire,Cvo,Mvo,S$,Evo,yvo,wvo,b0,dre,Avo,Lvo,P$,Bvo,xvo,kvo,v0,cre,Rvo,Svo,$$,Pvo,$vo,Ivo,T0,fre,Dvo,jvo,I$,Nvo,qvo,Gvo,F0,mre,Ovo,Xvo,D$,Vvo,zvo,Wvo,C0,gre,Qvo,Hvo,j$,Uvo,Jvo,Yvo,M0,hre,Kvo,Zvo,N$,eTo,oTo,rTo,E0,pre,tTo,aTo,q$,nTo,sTo,lTo,y0,_re,iTo,dTo,G$,cTo,fTo,mTo,w0,ure,gTo,hTo,O$,pTo,_To,uTo,A0,bre,bTo,vTo,X$,TTo,FTo,CTo,L0,vre,MTo,ETo,V$,yTo,wTo,ATo,B0,LTo,Tre,BTo,xTo,Fre,kTo,RTo,Cre,STo,PTo,VE,ABe,ad,x0,Mre,zE,$To,Ere,ITo,LBe,er,WE,DTo,nd,jTo,yre,NTo,qTo,wre,GTo,OTo,XTo,QE,VTo,Are,zTo,WTo,QTo,Qr,HE,HTo,Lre,UTo,JTo,sd,YTo,Bre,KTo,ZTo,xre,eFo,oFo,rFo,kre,tFo,aFo,UE,nFo,Oe,JE,sFo,Rre,lFo,iFo,Va,dFo,Sre,cFo,fFo,Pre,mFo,gFo,$re,hFo,pFo,_Fo,A,k0,Ire,uFo,bFo,z$,vFo,TFo,FFo,R0,Dre,CFo,MFo,W$,EFo,yFo,wFo,S0,jre,AFo,LFo,Q$,BFo,xFo,kFo,P0,Nre,RFo,SFo,H$,PFo,$Fo,IFo,$0,qre,DFo,jFo,U$,NFo,qFo,GFo,I0,Gre,OFo,XFo,J$,VFo,zFo,WFo,D0,Ore,QFo,HFo,Y$,UFo,JFo,YFo,j0,Xre,KFo,ZFo,K$,e9o,o9o,r9o,N0,Vre,t9o,a9o,Z$,n9o,s9o,l9o,q0,zre,i9o,d9o,eI,c9o,f9o,m9o,G0,Wre,g9o,h9o,oI,p9o,_9o,u9o,O0,Qre,b9o,v9o,rI,T9o,F9o,C9o,X0,Hre,M9o,E9o,tI,y9o,w9o,A9o,V0,Ure,L9o,B9o,aI,x9o,k9o,R9o,z0,Jre,S9o,P9o,nI,$9o,I9o,D9o,W0,Yre,j9o,N9o,sI,q9o,G9o,O9o,Q0,Kre,X9o,V9o,lI,z9o,W9o,Q9o,H0,Zre,H9o,U9o,iI,J9o,Y9o,K9o,U0,ete,Z9o,eCo,dI,oCo,rCo,tCo,J0,ote,aCo,nCo,cI,sCo,lCo,iCo,Y0,rte,dCo,cCo,fI,fCo,mCo,gCo,K0,tte,hCo,pCo,mI,_Co,uCo,bCo,Z0,ate,vCo,TCo,gI,FCo,CCo,MCo,e1,nte,ECo,yCo,hI,wCo,ACo,LCo,o1,ste,BCo,xCo,pI,kCo,RCo,SCo,r1,lte,PCo,$Co,_I,ICo,DCo,jCo,t1,ite,NCo,qCo,uI,GCo,OCo,XCo,a1,dte,VCo,zCo,bI,WCo,QCo,HCo,n1,cte,UCo,JCo,vI,YCo,KCo,ZCo,s1,fte,eMo,oMo,TI,rMo,tMo,aMo,l1,mte,nMo,sMo,FI,lMo,iMo,dMo,i1,gte,cMo,fMo,CI,mMo,gMo,hMo,d1,hte,pMo,_Mo,MI,uMo,bMo,vMo,c1,pte,TMo,FMo,EI,CMo,MMo,EMo,f1,_te,yMo,wMo,yI,AMo,LMo,BMo,m1,ute,xMo,kMo,wI,RMo,SMo,PMo,g1,bte,$Mo,IMo,AI,DMo,jMo,NMo,h1,vte,qMo,GMo,LI,OMo,XMo,VMo,p1,Tte,zMo,WMo,BI,QMo,HMo,UMo,_1,Fte,JMo,YMo,xI,KMo,ZMo,e4o,u1,Cte,o4o,r4o,kI,t4o,a4o,n4o,b1,Mte,s4o,l4o,RI,i4o,d4o,c4o,v1,Ete,f4o,m4o,SI,g4o,h4o,p4o,T1,yte,_4o,u4o,PI,b4o,v4o,T4o,F1,wte,F4o,C4o,$I,M4o,E4o,y4o,C1,Ate,w4o,A4o,II,L4o,B4o,x4o,M1,k4o,Lte,R4o,S4o,Bte,P4o,$4o,xte,I4o,D4o,YE,BBe,ld,E1,kte,KE,j4o,Rte,N4o,xBe,or,ZE,q4o,id,G4o,Ste,O4o,X4o,Pte,V4o,z4o,W4o,e3,Q4o,$te,H4o,U4o,J4o,Hr,o3,Y4o,Ite,K4o,Z4o,dd,eEo,Dte,oEo,rEo,jte,tEo,aEo,nEo,Nte,sEo,lEo,r3,iEo,Xe,t3,dEo,qte,cEo,fEo,za,mEo,Gte,gEo,hEo,Ote,pEo,_Eo,Xte,uEo,bEo,vEo,G,y1,Vte,TEo,FEo,DI,CEo,MEo,EEo,w1,zte,yEo,wEo,jI,AEo,LEo,BEo,A1,Wte,xEo,kEo,NI,REo,SEo,PEo,L1,Qte,$Eo,IEo,qI,DEo,jEo,NEo,B1,Hte,qEo,GEo,GI,OEo,XEo,VEo,x1,Ute,zEo,WEo,OI,QEo,HEo,UEo,k1,Jte,JEo,YEo,XI,KEo,ZEo,e3o,R1,Yte,o3o,r3o,VI,t3o,a3o,n3o,S1,Kte,s3o,l3o,zI,i3o,d3o,c3o,P1,Zte,f3o,m3o,WI,g3o,h3o,p3o,$1,eae,_3o,u3o,QI,b3o,v3o,T3o,I1,oae,F3o,C3o,HI,M3o,E3o,y3o,D1,rae,w3o,A3o,UI,L3o,B3o,x3o,j1,tae,k3o,R3o,JI,S3o,P3o,$3o,N1,aae,I3o,D3o,YI,j3o,N3o,q3o,q1,nae,G3o,O3o,KI,X3o,V3o,z3o,G1,sae,W3o,Q3o,ZI,H3o,U3o,J3o,O1,lae,Y3o,K3o,eD,Z3o,eyo,oyo,X1,iae,ryo,tyo,oD,ayo,nyo,syo,V1,dae,lyo,iyo,rD,dyo,cyo,fyo,z1,cae,myo,gyo,tD,hyo,pyo,_yo,W1,fae,uyo,byo,aD,vyo,Tyo,Fyo,Q1,mae,Cyo,Myo,nD,Eyo,yyo,wyo,H1,gae,Ayo,Lyo,sD,Byo,xyo,kyo,U1,hae,Ryo,Syo,lD,Pyo,$yo,Iyo,J1,pae,Dyo,jyo,iD,Nyo,qyo,Gyo,Y1,_ae,Oyo,Xyo,dD,Vyo,zyo,Wyo,K1,uae,Qyo,Hyo,cD,Uyo,Jyo,Yyo,Z1,Kyo,bae,Zyo,ewo,vae,owo,rwo,Tae,two,awo,a3,kBe,cd,eb,Fae,n3,nwo,Cae,swo,RBe,rr,s3,lwo,fd,iwo,Mae,dwo,cwo,Eae,fwo,mwo,gwo,l3,hwo,yae,pwo,_wo,uwo,Ur,i3,bwo,wae,vwo,Two,md,Fwo,Aae,Cwo,Mwo,Lae,Ewo,ywo,wwo,Bae,Awo,Lwo,d3,Bwo,Ve,c3,xwo,xae,kwo,Rwo,Wa,Swo,kae,Pwo,$wo,Rae,Iwo,Dwo,Sae,jwo,Nwo,qwo,na,ob,Pae,Gwo,Owo,fD,Xwo,Vwo,zwo,rb,$ae,Wwo,Qwo,mD,Hwo,Uwo,Jwo,tb,Iae,Ywo,Kwo,gD,Zwo,e6o,o6o,ab,Dae,r6o,t6o,hD,a6o,n6o,s6o,nb,jae,l6o,i6o,pD,d6o,c6o,f6o,sb,m6o,Nae,g6o,h6o,qae,p6o,_6o,Gae,u6o,b6o,f3,SBe,gd,lb,Oae,m3,v6o,Xae,T6o,PBe,tr,g3,F6o,hd,C6o,Vae,M6o,E6o,zae,y6o,w6o,A6o,h3,L6o,Wae,B6o,x6o,k6o,Jr,p3,R6o,Qae,S6o,P6o,pd,$6o,Hae,I6o,D6o,Uae,j6o,N6o,q6o,Jae,G6o,O6o,_3,X6o,ze,u3,V6o,Yae,z6o,W6o,Qa,Q6o,Kae,H6o,U6o,Zae,J6o,Y6o,ene,K6o,Z6o,eAo,N,ib,one,oAo,rAo,_D,tAo,aAo,nAo,db,rne,sAo,lAo,uD,iAo,dAo,cAo,cb,tne,fAo,mAo,bD,gAo,hAo,pAo,fb,ane,_Ao,uAo,vD,bAo,vAo,TAo,mb,nne,FAo,CAo,TD,MAo,EAo,yAo,gb,sne,wAo,AAo,FD,LAo,BAo,xAo,hb,lne,kAo,RAo,CD,SAo,PAo,$Ao,pb,ine,IAo,DAo,MD,jAo,NAo,qAo,_b,dne,GAo,OAo,ED,XAo,VAo,zAo,ub,cne,WAo,QAo,yD,HAo,UAo,JAo,bb,fne,YAo,KAo,wD,ZAo,eLo,oLo,vb,mne,rLo,tLo,AD,aLo,nLo,sLo,Tb,gne,lLo,iLo,LD,dLo,cLo,fLo,Fb,hne,mLo,gLo,BD,hLo,pLo,_Lo,Cb,pne,uLo,bLo,xD,vLo,TLo,FLo,Mb,_ne,CLo,MLo,kD,ELo,yLo,wLo,Eb,une,ALo,LLo,RD,BLo,xLo,kLo,yb,bne,RLo,SLo,SD,PLo,$Lo,ILo,wb,vne,DLo,jLo,PD,NLo,qLo,GLo,Ab,Tne,OLo,XLo,$D,VLo,zLo,WLo,Lb,Fne,QLo,HLo,ID,ULo,JLo,YLo,Bb,Cne,KLo,ZLo,DD,e8o,o8o,r8o,xb,Mne,t8o,a8o,jD,n8o,s8o,l8o,kb,Ene,i8o,d8o,ND,c8o,f8o,m8o,Rb,yne,g8o,h8o,qD,p8o,_8o,u8o,Sb,wne,b8o,v8o,GD,T8o,F8o,C8o,Pb,Ane,M8o,E8o,OD,y8o,w8o,A8o,$b,Lne,L8o,B8o,XD,x8o,k8o,R8o,Ib,Bne,S8o,P8o,VD,$8o,I8o,D8o,Db,xne,j8o,N8o,zD,q8o,G8o,O8o,jb,kne,X8o,V8o,WD,z8o,W8o,Q8o,Nb,Rne,H8o,U8o,QD,J8o,Y8o,K8o,qb,Sne,Z8o,e7o,HD,o7o,r7o,t7o,Gb,a7o,Pne,n7o,s7o,$ne,l7o,i7o,Ine,d7o,c7o,b3,$Be,_d,Ob,Dne,v3,f7o,jne,m7o,IBe,ar,T3,g7o,ud,h7o,Nne,p7o,_7o,qne,u7o,b7o,v7o,F3,T7o,Gne,F7o,C7o,M7o,Yr,C3,E7o,One,y7o,w7o,bd,A7o,Xne,L7o,B7o,Vne,x7o,k7o,R7o,zne,S7o,P7o,M3,$7o,We,E3,I7o,Wne,D7o,j7o,Ha,N7o,Qne,q7o,G7o,Hne,O7o,X7o,Une,V7o,z7o,W7o,R,Xb,Jne,Q7o,H7o,UD,U7o,J7o,Y7o,Vb,Yne,K7o,Z7o,JD,eBo,oBo,rBo,zb,Kne,tBo,aBo,YD,nBo,sBo,lBo,Wb,Zne,iBo,dBo,KD,cBo,fBo,mBo,Qb,ese,gBo,hBo,ZD,pBo,_Bo,uBo,Hb,ose,bBo,vBo,ej,TBo,FBo,CBo,Ub,rse,MBo,EBo,oj,yBo,wBo,ABo,Jb,tse,LBo,BBo,rj,xBo,kBo,RBo,Yb,ase,SBo,PBo,tj,$Bo,IBo,DBo,Kb,nse,jBo,NBo,aj,qBo,GBo,OBo,Zb,sse,XBo,VBo,nj,zBo,WBo,QBo,e5,lse,HBo,UBo,sj,JBo,YBo,KBo,o5,ise,ZBo,exo,lj,oxo,rxo,txo,r5,dse,axo,nxo,ij,sxo,lxo,ixo,t5,cse,dxo,cxo,dj,fxo,mxo,gxo,a5,fse,hxo,pxo,cj,_xo,uxo,bxo,n5,mse,vxo,Txo,fj,Fxo,Cxo,Mxo,s5,gse,Exo,yxo,mj,wxo,Axo,Lxo,l5,hse,Bxo,xxo,gj,kxo,Rxo,Sxo,i5,pse,Pxo,$xo,hj,Ixo,Dxo,jxo,d5,_se,Nxo,qxo,pj,Gxo,Oxo,Xxo,c5,use,Vxo,zxo,_j,Wxo,Qxo,Hxo,f5,bse,Uxo,Jxo,uj,Yxo,Kxo,Zxo,m5,vse,eko,oko,bj,rko,tko,ako,g5,Tse,nko,sko,vj,lko,iko,dko,h5,Fse,cko,fko,Tj,mko,gko,hko,p5,Cse,pko,_ko,Fj,uko,bko,vko,_5,Mse,Tko,Fko,Cj,Cko,Mko,Eko,u5,Ese,yko,wko,Mj,Ako,Lko,Bko,b5,yse,xko,kko,Ej,Rko,Sko,Pko,v5,wse,$ko,Iko,yj,Dko,jko,Nko,T5,Ase,qko,Gko,wj,Oko,Xko,Vko,F5,Lse,zko,Wko,Aj,Qko,Hko,Uko,C5,Bse,Jko,Yko,Lj,Kko,Zko,eRo,M5,xse,oRo,rRo,Bj,tRo,aRo,nRo,E5,kse,sRo,lRo,xj,iRo,dRo,cRo,y5,Rse,fRo,mRo,kj,gRo,hRo,pRo,w5,Sse,_Ro,uRo,Rj,bRo,vRo,TRo,A5,Pse,FRo,CRo,Sj,MRo,ERo,yRo,L5,wRo,$se,ARo,LRo,Ise,BRo,xRo,Dse,kRo,RRo,y3,DBe,vd,B5,jse,w3,SRo,Nse,PRo,jBe,nr,A3,$Ro,Td,IRo,qse,DRo,jRo,Gse,NRo,qRo,GRo,L3,ORo,Ose,XRo,VRo,zRo,Kr,B3,WRo,Xse,QRo,HRo,Fd,URo,Vse,JRo,YRo,zse,KRo,ZRo,eSo,Wse,oSo,rSo,x3,tSo,Qe,k3,aSo,Qse,nSo,sSo,Ua,lSo,Hse,iSo,dSo,Use,cSo,fSo,Jse,mSo,gSo,hSo,Yse,x5,Kse,pSo,_So,Pj,uSo,bSo,vSo,k5,TSo,Zse,FSo,CSo,ele,MSo,ESo,ole,ySo,wSo,R3,NBe,Cd,R5,rle,S3,ASo,tle,LSo,qBe,sr,P3,BSo,Md,xSo,ale,kSo,RSo,nle,SSo,PSo,$So,$3,ISo,sle,DSo,jSo,NSo,Zr,I3,qSo,lle,GSo,OSo,Ed,XSo,ile,VSo,zSo,dle,WSo,QSo,HSo,cle,USo,JSo,D3,YSo,He,j3,KSo,fle,ZSo,ePo,Ja,oPo,mle,rPo,tPo,gle,aPo,nPo,hle,sPo,lPo,iPo,Fe,S5,ple,dPo,cPo,$j,fPo,mPo,gPo,P5,_le,hPo,pPo,Ij,_Po,uPo,bPo,$s,ule,vPo,TPo,Dj,FPo,CPo,jj,MPo,EPo,yPo,$5,ble,wPo,APo,Nj,LPo,BPo,xPo,la,vle,kPo,RPo,qj,SPo,PPo,Gj,$Po,IPo,Oj,DPo,jPo,NPo,I5,Tle,qPo,GPo,Xj,OPo,XPo,VPo,D5,Fle,zPo,WPo,Vj,QPo,HPo,UPo,j5,Cle,JPo,YPo,zj,KPo,ZPo,e$o,N5,Mle,o$o,r$o,Wj,t$o,a$o,n$o,q5,s$o,Ele,l$o,i$o,yle,d$o,c$o,wle,f$o,m$o,N3,GBe,yd,G5,Ale,q3,g$o,Lle,h$o,OBe,lr,G3,p$o,wd,_$o,Ble,u$o,b$o,xle,v$o,T$o,F$o,O3,C$o,kle,M$o,E$o,y$o,et,X3,w$o,Rle,A$o,L$o,Ad,B$o,Sle,x$o,k$o,Ple,R$o,S$o,P$o,$le,$$o,I$o,V3,D$o,Ue,z3,j$o,Ile,N$o,q$o,Ya,G$o,Dle,O$o,X$o,jle,V$o,z$o,Nle,W$o,Q$o,H$o,qle,O5,Gle,U$o,J$o,Qj,Y$o,K$o,Z$o,X5,eIo,Ole,oIo,rIo,Xle,tIo,aIo,Vle,nIo,sIo,W3,XBe,Ld,V5,zle,Q3,lIo,Wle,iIo,VBe,ir,H3,dIo,Bd,cIo,Qle,fIo,mIo,Hle,gIo,hIo,pIo,U3,_Io,Ule,uIo,bIo,vIo,ot,J3,TIo,Jle,FIo,CIo,xd,MIo,Yle,EIo,yIo,Kle,wIo,AIo,LIo,Zle,BIo,xIo,Y3,kIo,Je,K3,RIo,eie,SIo,PIo,Ka,$Io,oie,IIo,DIo,rie,jIo,NIo,tie,qIo,GIo,OIo,xe,z5,aie,XIo,VIo,Hj,zIo,WIo,QIo,W5,nie,HIo,UIo,Uj,JIo,YIo,KIo,Q5,sie,ZIo,eDo,Jj,oDo,rDo,tDo,H5,lie,aDo,nDo,Yj,sDo,lDo,iDo,U5,iie,dDo,cDo,Kj,fDo,mDo,gDo,J5,die,hDo,pDo,Zj,_Do,uDo,bDo,Y5,cie,vDo,TDo,eN,FDo,CDo,MDo,K5,fie,EDo,yDo,oN,wDo,ADo,LDo,Z5,BDo,mie,xDo,kDo,gie,RDo,SDo,hie,PDo,$Do,Z3,zBe,kd,e2,pie,ey,IDo,_ie,DDo,WBe,dr,oy,jDo,Rd,NDo,uie,qDo,GDo,bie,ODo,XDo,VDo,ry,zDo,vie,WDo,QDo,HDo,rt,ty,UDo,Tie,JDo,YDo,Sd,KDo,Fie,ZDo,ejo,Cie,ojo,rjo,tjo,Mie,ajo,njo,ay,sjo,Ye,ny,ljo,Eie,ijo,djo,Za,cjo,yie,fjo,mjo,wie,gjo,hjo,Aie,pjo,_jo,ujo,en,o2,Lie,bjo,vjo,rN,Tjo,Fjo,Cjo,r2,Bie,Mjo,Ejo,tN,yjo,wjo,Ajo,t2,xie,Ljo,Bjo,aN,xjo,kjo,Rjo,a2,kie,Sjo,Pjo,nN,$jo,Ijo,Djo,n2,jjo,Rie,Njo,qjo,Sie,Gjo,Ojo,Pie,Xjo,Vjo,sy,QBe,Pd,s2,$ie,ly,zjo,Iie,Wjo,HBe,cr,iy,Qjo,$d,Hjo,Die,Ujo,Jjo,jie,Yjo,Kjo,Zjo,dy,eNo,Nie,oNo,rNo,tNo,tt,cy,aNo,qie,nNo,sNo,Id,lNo,Gie,iNo,dNo,Oie,cNo,fNo,mNo,Xie,gNo,hNo,fy,pNo,Ke,my,_No,Vie,uNo,bNo,on,vNo,zie,TNo,FNo,Wie,CNo,MNo,Qie,ENo,yNo,wNo,ke,l2,Hie,ANo,LNo,sN,BNo,xNo,kNo,i2,Uie,RNo,SNo,lN,PNo,$No,INo,d2,Jie,DNo,jNo,iN,NNo,qNo,GNo,c2,Yie,ONo,XNo,dN,VNo,zNo,WNo,f2,Kie,QNo,HNo,cN,UNo,JNo,YNo,m2,Zie,KNo,ZNo,fN,eqo,oqo,rqo,g2,ede,tqo,aqo,mN,nqo,sqo,lqo,h2,ode,iqo,dqo,gN,cqo,fqo,mqo,p2,gqo,rde,hqo,pqo,tde,_qo,uqo,ade,bqo,vqo,gy,UBe,Dd,_2,nde,hy,Tqo,sde,Fqo,JBe,fr,py,Cqo,jd,Mqo,lde,Eqo,yqo,ide,wqo,Aqo,Lqo,_y,Bqo,dde,xqo,kqo,Rqo,at,uy,Sqo,cde,Pqo,$qo,Nd,Iqo,fde,Dqo,jqo,mde,Nqo,qqo,Gqo,gde,Oqo,Xqo,by,Vqo,Ze,vy,zqo,hde,Wqo,Qqo,rn,Hqo,pde,Uqo,Jqo,_de,Yqo,Kqo,ude,Zqo,eGo,oGo,Ty,u2,bde,rGo,tGo,hN,aGo,nGo,sGo,b2,vde,lGo,iGo,pN,dGo,cGo,fGo,v2,mGo,Tde,gGo,hGo,Fde,pGo,_Go,Cde,uGo,bGo,Fy,YBe,qd,T2,Mde,Cy,vGo,Ede,TGo,KBe,mr,My,FGo,Gd,CGo,yde,MGo,EGo,wde,yGo,wGo,AGo,Ey,LGo,Ade,BGo,xGo,kGo,nt,yy,RGo,Lde,SGo,PGo,Od,$Go,Bde,IGo,DGo,xde,jGo,NGo,qGo,kde,GGo,OGo,wy,XGo,eo,Ay,VGo,Rde,zGo,WGo,tn,QGo,Sde,HGo,UGo,Pde,JGo,YGo,$de,KGo,ZGo,eOo,an,F2,Ide,oOo,rOo,_N,tOo,aOo,nOo,C2,Dde,sOo,lOo,uN,iOo,dOo,cOo,M2,jde,fOo,mOo,bN,gOo,hOo,pOo,E2,Nde,_Oo,uOo,vN,bOo,vOo,TOo,y2,FOo,qde,COo,MOo,Gde,EOo,yOo,Ode,wOo,AOo,Ly,ZBe,Xd,w2,Xde,By,LOo,Vde,BOo,exe,gr,xy,xOo,Vd,kOo,zde,ROo,SOo,Wde,POo,$Oo,IOo,ky,DOo,Qde,jOo,NOo,qOo,st,Ry,GOo,Hde,OOo,XOo,zd,VOo,Ude,zOo,WOo,Jde,QOo,HOo,UOo,Yde,JOo,YOo,Sy,KOo,oo,Py,ZOo,Kde,eXo,oXo,nn,rXo,Zde,tXo,aXo,ece,nXo,sXo,oce,lXo,iXo,dXo,Wd,A2,rce,cXo,fXo,TN,mXo,gXo,hXo,L2,tce,pXo,_Xo,FN,uXo,bXo,vXo,B2,ace,TXo,FXo,CN,CXo,MXo,EXo,x2,yXo,nce,wXo,AXo,sce,LXo,BXo,lce,xXo,kXo,$y,oxe,Qd,k2,ice,Iy,RXo,dce,SXo,rxe,hr,Dy,PXo,Hd,$Xo,cce,IXo,DXo,fce,jXo,NXo,qXo,jy,GXo,mce,OXo,XXo,VXo,lt,Ny,zXo,gce,WXo,QXo,Ud,HXo,hce,UXo,JXo,pce,YXo,KXo,ZXo,_ce,eVo,oVo,qy,rVo,ro,Gy,tVo,uce,aVo,nVo,sn,sVo,bce,lVo,iVo,vce,dVo,cVo,Tce,fVo,mVo,gVo,Fce,R2,Cce,hVo,pVo,MN,_Vo,uVo,bVo,S2,vVo,Mce,TVo,FVo,Ece,CVo,MVo,yce,EVo,yVo,Oy,txe,Jd,P2,wce,Xy,wVo,Ace,AVo,axe,pr,Vy,LVo,Yd,BVo,Lce,xVo,kVo,Bce,RVo,SVo,PVo,zy,$Vo,xce,IVo,DVo,jVo,it,Wy,NVo,kce,qVo,GVo,Kd,OVo,Rce,XVo,VVo,Sce,zVo,WVo,QVo,Pce,HVo,UVo,Qy,JVo,to,Hy,YVo,$ce,KVo,ZVo,ln,ezo,Ice,ozo,rzo,Dce,tzo,azo,jce,nzo,szo,lzo,Nce,$2,qce,izo,dzo,EN,czo,fzo,mzo,I2,gzo,Gce,hzo,pzo,Oce,_zo,uzo,Xce,bzo,vzo,Uy,nxe,Zd,D2,Vce,Jy,Tzo,zce,Fzo,sxe,_r,Yy,Czo,ec,Mzo,Wce,Ezo,yzo,Qce,wzo,Azo,Lzo,Ky,Bzo,Hce,xzo,kzo,Rzo,dt,Zy,Szo,Uce,Pzo,$zo,oc,Izo,Jce,Dzo,jzo,Yce,Nzo,qzo,Gzo,Kce,Ozo,Xzo,ew,Vzo,ao,ow,zzo,Zce,Wzo,Qzo,dn,Hzo,efe,Uzo,Jzo,ofe,Yzo,Kzo,rfe,Zzo,eWo,oWo,rw,j2,tfe,rWo,tWo,yN,aWo,nWo,sWo,N2,afe,lWo,iWo,wN,dWo,cWo,fWo,q2,mWo,nfe,gWo,hWo,sfe,pWo,_Wo,lfe,uWo,bWo,tw,lxe,rc,G2,ife,aw,vWo,dfe,TWo,ixe,ur,nw,FWo,tc,CWo,cfe,MWo,EWo,ffe,yWo,wWo,AWo,sw,LWo,mfe,BWo,xWo,kWo,ct,lw,RWo,gfe,SWo,PWo,ac,$Wo,hfe,IWo,DWo,pfe,jWo,NWo,qWo,_fe,GWo,OWo,iw,XWo,go,dw,VWo,ufe,zWo,WWo,cn,QWo,bfe,HWo,UWo,vfe,JWo,YWo,Tfe,KWo,ZWo,eQo,B,O2,Ffe,oQo,rQo,AN,tQo,aQo,nQo,X2,Cfe,sQo,lQo,LN,iQo,dQo,cQo,V2,Mfe,fQo,mQo,BN,gQo,hQo,pQo,z2,Efe,_Qo,uQo,xN,bQo,vQo,TQo,W2,yfe,FQo,CQo,kN,MQo,EQo,yQo,Q2,wfe,wQo,AQo,RN,LQo,BQo,xQo,H2,Afe,kQo,RQo,SN,SQo,PQo,$Qo,U2,Lfe,IQo,DQo,PN,jQo,NQo,qQo,J2,Bfe,GQo,OQo,$N,XQo,VQo,zQo,Y2,xfe,WQo,QQo,IN,HQo,UQo,JQo,K2,kfe,YQo,KQo,DN,ZQo,eHo,oHo,Z2,Rfe,rHo,tHo,jN,aHo,nHo,sHo,ev,Sfe,lHo,iHo,NN,dHo,cHo,fHo,ov,Pfe,mHo,gHo,qN,hHo,pHo,_Ho,rv,$fe,uHo,bHo,GN,vHo,THo,FHo,tv,Ife,CHo,MHo,ON,EHo,yHo,wHo,Is,Dfe,AHo,LHo,XN,BHo,xHo,VN,kHo,RHo,SHo,av,jfe,PHo,$Ho,zN,IHo,DHo,jHo,nv,Nfe,NHo,qHo,WN,GHo,OHo,XHo,sv,qfe,VHo,zHo,QN,WHo,QHo,HHo,lv,Gfe,UHo,JHo,HN,YHo,KHo,ZHo,iv,Ofe,eUo,oUo,UN,rUo,tUo,aUo,dv,Xfe,nUo,sUo,JN,lUo,iUo,dUo,cv,Vfe,cUo,fUo,YN,mUo,gUo,hUo,fv,zfe,pUo,_Uo,KN,uUo,bUo,vUo,mv,Wfe,TUo,FUo,ZN,CUo,MUo,EUo,gv,Qfe,yUo,wUo,eq,AUo,LUo,BUo,hv,Hfe,xUo,kUo,oq,RUo,SUo,PUo,pv,Ufe,$Uo,IUo,rq,DUo,jUo,NUo,_v,Jfe,qUo,GUo,tq,OUo,XUo,VUo,uv,Yfe,zUo,WUo,aq,QUo,HUo,UUo,bv,Kfe,JUo,YUo,nq,KUo,ZUo,eJo,vv,Zfe,oJo,rJo,sq,tJo,aJo,nJo,Tv,eme,sJo,lJo,lq,iJo,dJo,cJo,Fv,ome,fJo,mJo,iq,gJo,hJo,pJo,Cv,rme,_Jo,uJo,dq,bJo,vJo,TJo,Mv,tme,FJo,CJo,cq,MJo,EJo,yJo,Ev,ame,wJo,AJo,fq,LJo,BJo,xJo,yv,nme,kJo,RJo,mq,SJo,PJo,$Jo,wv,sme,IJo,DJo,gq,jJo,NJo,qJo,Av,lme,GJo,OJo,hq,XJo,VJo,zJo,Lv,ime,WJo,QJo,pq,HJo,UJo,JJo,dme,YJo,KJo,cw,dxe,nc,Bv,cme,fw,ZJo,fme,eYo,cxe,br,mw,oYo,sc,rYo,mme,tYo,aYo,gme,nYo,sYo,lYo,gw,iYo,hme,dYo,cYo,fYo,ft,hw,mYo,pme,gYo,hYo,lc,pYo,_me,_Yo,uYo,ume,bYo,vYo,TYo,bme,FYo,CYo,pw,MYo,ho,_w,EYo,vme,yYo,wYo,fn,AYo,Tme,LYo,BYo,Fme,xYo,kYo,Cme,RYo,SYo,PYo,H,xv,Mme,$Yo,IYo,_q,DYo,jYo,NYo,kv,Eme,qYo,GYo,uq,OYo,XYo,VYo,Rv,yme,zYo,WYo,bq,QYo,HYo,UYo,Sv,wme,JYo,YYo,vq,KYo,ZYo,eKo,Pv,Ame,oKo,rKo,Tq,tKo,aKo,nKo,$v,Lme,sKo,lKo,Fq,iKo,dKo,cKo,Iv,Bme,fKo,mKo,Cq,gKo,hKo,pKo,Dv,xme,_Ko,uKo,Mq,bKo,vKo,TKo,jv,kme,FKo,CKo,Eq,MKo,EKo,yKo,Nv,Rme,wKo,AKo,yq,LKo,BKo,xKo,qv,Sme,kKo,RKo,wq,SKo,PKo,$Ko,Gv,Pme,IKo,DKo,Aq,jKo,NKo,qKo,Ov,$me,GKo,OKo,Lq,XKo,VKo,zKo,Xv,Ime,WKo,QKo,Bq,HKo,UKo,JKo,Vv,Dme,YKo,KKo,xq,ZKo,eZo,oZo,zv,jme,rZo,tZo,kq,aZo,nZo,sZo,Wv,Nme,lZo,iZo,Rq,dZo,cZo,fZo,Qv,qme,mZo,gZo,Sq,hZo,pZo,_Zo,Hv,Gme,uZo,bZo,Pq,vZo,TZo,FZo,Uv,Ome,CZo,MZo,$q,EZo,yZo,wZo,Jv,Xme,AZo,LZo,Iq,BZo,xZo,kZo,Yv,Vme,RZo,SZo,Dq,PZo,$Zo,IZo,zme,DZo,jZo,uw,fxe,ic,Kv,Wme,bw,NZo,Qme,qZo,mxe,vr,vw,GZo,dc,OZo,Hme,XZo,VZo,Ume,zZo,WZo,QZo,Tw,HZo,Jme,UZo,JZo,YZo,mt,Fw,KZo,Yme,ZZo,eer,cc,oer,Kme,rer,ter,Zme,aer,ner,ser,ege,ler,ier,Cw,der,po,Mw,cer,oge,fer,mer,mn,ger,rge,her,per,tge,_er,uer,age,ber,ver,Ter,pe,Zv,nge,Fer,Cer,jq,Mer,Eer,yer,eT,sge,wer,Aer,Nq,Ler,Ber,xer,oT,lge,ker,Rer,qq,Ser,Per,$er,rT,ige,Ier,Der,Gq,jer,Ner,qer,tT,dge,Ger,Oer,Oq,Xer,Ver,zer,aT,cge,Wer,Qer,Xq,Her,Uer,Jer,nT,fge,Yer,Ker,Vq,Zer,eor,oor,sT,mge,ror,tor,zq,aor,nor,sor,lT,gge,lor,ior,Wq,dor,cor,mor,iT,hge,gor,hor,Qq,por,_or,uor,pge,bor,vor,Ew,gxe,fc,dT,_ge,yw,Tor,uge,For,hxe,Tr,ww,Cor,mc,Mor,bge,Eor,yor,vge,wor,Aor,Lor,Aw,Bor,Tge,xor,kor,Ror,gt,Lw,Sor,Fge,Por,$or,gc,Ior,Cge,Dor,jor,Mge,Nor,qor,Gor,Ege,Oor,Xor,Bw,Vor,_o,xw,zor,yge,Wor,Qor,gn,Hor,wge,Uor,Jor,Age,Yor,Kor,Lge,Zor,err,orr,kw,cT,Bge,rrr,trr,Hq,arr,nrr,srr,fT,xge,lrr,irr,Uq,drr,crr,frr,kge,mrr,grr,Rw,pxe,hc,mT,Rge,Sw,hrr,Sge,prr,_xe,Fr,Pw,_rr,pc,urr,Pge,brr,vrr,$ge,Trr,Frr,Crr,$w,Mrr,Ige,Err,yrr,wrr,ht,Iw,Arr,Dge,Lrr,Brr,_c,xrr,jge,krr,Rrr,Nge,Srr,Prr,$rr,qge,Irr,Drr,Dw,jrr,uo,jw,Nrr,Gge,qrr,Grr,hn,Orr,Oge,Xrr,Vrr,Xge,zrr,Wrr,Vge,Qrr,Hrr,Urr,Y,gT,zge,Jrr,Yrr,Jq,Krr,Zrr,etr,hT,Wge,otr,rtr,Yq,ttr,atr,ntr,pT,Qge,str,ltr,Kq,itr,dtr,ctr,_T,Hge,ftr,mtr,Zq,gtr,htr,ptr,uT,Uge,_tr,utr,eG,btr,vtr,Ttr,bT,Jge,Ftr,Ctr,oG,Mtr,Etr,ytr,vT,Yge,wtr,Atr,rG,Ltr,Btr,xtr,TT,Kge,ktr,Rtr,tG,Str,Ptr,$tr,FT,Zge,Itr,Dtr,aG,jtr,Ntr,qtr,CT,ehe,Gtr,Otr,nG,Xtr,Vtr,ztr,MT,ohe,Wtr,Qtr,sG,Htr,Utr,Jtr,ET,rhe,Ytr,Ktr,lG,Ztr,ear,oar,yT,the,rar,tar,iG,aar,nar,sar,wT,ahe,lar,iar,dG,dar,car,far,AT,nhe,mar,gar,cG,har,par,_ar,LT,she,uar,bar,fG,Tar,Far,Car,BT,lhe,Mar,Ear,mG,yar,war,Aar,xT,ihe,Lar,Bar,gG,xar,kar,Rar,kT,dhe,Sar,Par,hG,$ar,Iar,Dar,RT,che,jar,Nar,pG,qar,Gar,Oar,fhe,Xar,Var,Nw,uxe,uc,ST,mhe,qw,zar,ghe,War,bxe,Cr,Gw,Qar,bc,Har,hhe,Uar,Jar,phe,Yar,Kar,Zar,Ow,enr,_he,onr,rnr,tnr,pt,Xw,anr,uhe,nnr,snr,vc,lnr,bhe,inr,dnr,vhe,cnr,fnr,mnr,The,gnr,hnr,Vw,pnr,bo,zw,_nr,Fhe,unr,bnr,pn,vnr,Che,Tnr,Fnr,Mhe,Cnr,Mnr,Ehe,Enr,ynr,wnr,_e,PT,yhe,Anr,Lnr,_G,Bnr,xnr,knr,$T,whe,Rnr,Snr,uG,Pnr,$nr,Inr,IT,Ahe,Dnr,jnr,bG,Nnr,qnr,Gnr,DT,Lhe,Onr,Xnr,vG,Vnr,znr,Wnr,jT,Bhe,Qnr,Hnr,TG,Unr,Jnr,Ynr,NT,xhe,Knr,Znr,FG,esr,osr,rsr,qT,khe,tsr,asr,CG,nsr,ssr,lsr,GT,Rhe,isr,dsr,MG,csr,fsr,msr,OT,She,gsr,hsr,EG,psr,_sr,usr,XT,Phe,bsr,vsr,yG,Tsr,Fsr,Csr,$he,Msr,Esr,Ww,vxe,Tc,VT,Ihe,Qw,ysr,Dhe,wsr,Txe,Mr,Hw,Asr,Fc,Lsr,jhe,Bsr,xsr,Nhe,ksr,Rsr,Ssr,Uw,Psr,qhe,$sr,Isr,Dsr,_t,Jw,jsr,Ghe,Nsr,qsr,Cc,Gsr,Ohe,Osr,Xsr,Xhe,Vsr,zsr,Wsr,Vhe,Qsr,Hsr,Yw,Usr,vo,Kw,Jsr,zhe,Ysr,Ksr,_n,Zsr,Whe,elr,olr,Qhe,rlr,tlr,Hhe,alr,nlr,slr,X,zT,Uhe,llr,ilr,wG,dlr,clr,flr,WT,Jhe,mlr,glr,AG,hlr,plr,_lr,QT,Yhe,ulr,blr,LG,vlr,Tlr,Flr,HT,Khe,Clr,Mlr,BG,Elr,ylr,wlr,UT,Zhe,Alr,Llr,xG,Blr,xlr,klr,JT,epe,Rlr,Slr,kG,Plr,$lr,Ilr,YT,ope,Dlr,jlr,RG,Nlr,qlr,Glr,KT,rpe,Olr,Xlr,SG,Vlr,zlr,Wlr,ZT,tpe,Qlr,Hlr,PG,Ulr,Jlr,Ylr,eF,ape,Klr,Zlr,$G,eir,oir,rir,oF,npe,tir,air,IG,nir,sir,lir,rF,spe,iir,dir,DG,cir,fir,mir,tF,lpe,gir,hir,jG,pir,_ir,uir,aF,ipe,bir,vir,NG,Tir,Fir,Cir,nF,dpe,Mir,Eir,qG,yir,wir,Air,sF,cpe,Lir,Bir,GG,xir,kir,Rir,lF,fpe,Sir,Pir,OG,$ir,Iir,Dir,iF,mpe,jir,Nir,XG,qir,Gir,Oir,dF,gpe,Xir,Vir,VG,zir,Wir,Qir,cF,hpe,Hir,Uir,zG,Jir,Yir,Kir,fF,ppe,Zir,edr,WG,odr,rdr,tdr,mF,_pe,adr,ndr,QG,sdr,ldr,idr,gF,upe,ddr,cdr,HG,fdr,mdr,gdr,hF,bpe,hdr,pdr,UG,_dr,udr,bdr,pF,vpe,vdr,Tdr,JG,Fdr,Cdr,Mdr,Tpe,Edr,ydr,Zw,Fxe,Mc,_F,Fpe,e6,wdr,Cpe,Adr,Cxe,Er,o6,Ldr,Ec,Bdr,Mpe,xdr,kdr,Epe,Rdr,Sdr,Pdr,r6,$dr,ype,Idr,Ddr,jdr,ut,t6,Ndr,wpe,qdr,Gdr,yc,Odr,Ape,Xdr,Vdr,Lpe,zdr,Wdr,Qdr,Bpe,Hdr,Udr,a6,Jdr,To,n6,Ydr,xpe,Kdr,Zdr,un,ecr,kpe,ocr,rcr,Rpe,tcr,acr,Spe,ncr,scr,lcr,te,uF,Ppe,icr,dcr,YG,ccr,fcr,mcr,bF,$pe,gcr,hcr,KG,pcr,_cr,ucr,vF,Ipe,bcr,vcr,ZG,Tcr,Fcr,Ccr,TF,Dpe,Mcr,Ecr,eO,ycr,wcr,Acr,FF,jpe,Lcr,Bcr,oO,xcr,kcr,Rcr,CF,Npe,Scr,Pcr,rO,$cr,Icr,Dcr,MF,qpe,jcr,Ncr,tO,qcr,Gcr,Ocr,EF,Gpe,Xcr,Vcr,aO,zcr,Wcr,Qcr,yF,Ope,Hcr,Ucr,nO,Jcr,Ycr,Kcr,wF,Xpe,Zcr,efr,sO,ofr,rfr,tfr,AF,Vpe,afr,nfr,lO,sfr,lfr,ifr,LF,zpe,dfr,cfr,iO,ffr,mfr,gfr,BF,Wpe,hfr,pfr,dO,_fr,ufr,bfr,xF,Qpe,vfr,Tfr,cO,Ffr,Cfr,Mfr,kF,Hpe,Efr,yfr,fO,wfr,Afr,Lfr,RF,Upe,Bfr,xfr,mO,kfr,Rfr,Sfr,SF,Jpe,Pfr,$fr,gO,Ifr,Dfr,jfr,Ype,Nfr,qfr,s6,Mxe,wc,PF,Kpe,l6,Gfr,Zpe,Ofr,Exe,yr,i6,Xfr,Ac,Vfr,e_e,zfr,Wfr,o_e,Qfr,Hfr,Ufr,d6,Jfr,r_e,Yfr,Kfr,Zfr,bt,c6,emr,t_e,omr,rmr,Lc,tmr,a_e,amr,nmr,n_e,smr,lmr,imr,s_e,dmr,cmr,f6,fmr,Fo,m6,mmr,l_e,gmr,hmr,bn,pmr,i_e,_mr,umr,d_e,bmr,vmr,c_e,Tmr,Fmr,Cmr,f_e,$F,m_e,Mmr,Emr,hO,ymr,wmr,Amr,g_e,Lmr,Bmr,g6,yxe,Bc,IF,h_e,h6,xmr,p_e,kmr,wxe,wr,p6,Rmr,xc,Smr,__e,Pmr,$mr,u_e,Imr,Dmr,jmr,_6,Nmr,b_e,qmr,Gmr,Omr,vt,u6,Xmr,v_e,Vmr,zmr,kc,Wmr,T_e,Qmr,Hmr,F_e,Umr,Jmr,Ymr,C_e,Kmr,Zmr,b6,egr,Co,v6,ogr,M_e,rgr,tgr,vn,agr,E_e,ngr,sgr,y_e,lgr,igr,w_e,dgr,cgr,fgr,K,DF,A_e,mgr,ggr,pO,hgr,pgr,_gr,jF,L_e,ugr,bgr,_O,vgr,Tgr,Fgr,NF,B_e,Cgr,Mgr,uO,Egr,ygr,wgr,qF,x_e,Agr,Lgr,bO,Bgr,xgr,kgr,GF,k_e,Rgr,Sgr,vO,Pgr,$gr,Igr,OF,R_e,Dgr,jgr,TO,Ngr,qgr,Ggr,XF,S_e,Ogr,Xgr,FO,Vgr,zgr,Wgr,VF,P_e,Qgr,Hgr,CO,Ugr,Jgr,Ygr,zF,$_e,Kgr,Zgr,MO,ehr,ohr,rhr,WF,I_e,thr,ahr,EO,nhr,shr,lhr,QF,D_e,ihr,dhr,yO,chr,fhr,mhr,HF,j_e,ghr,hhr,wO,phr,_hr,uhr,UF,N_e,bhr,vhr,AO,Thr,Fhr,Chr,JF,q_e,Mhr,Ehr,LO,yhr,whr,Ahr,YF,G_e,Lhr,Bhr,BO,xhr,khr,Rhr,KF,O_e,Shr,Phr,xO,$hr,Ihr,Dhr,ZF,X_e,jhr,Nhr,kO,qhr,Ghr,Ohr,e9,V_e,Xhr,Vhr,RO,zhr,Whr,Qhr,o9,z_e,Hhr,Uhr,SO,Jhr,Yhr,Khr,r9,W_e,Zhr,epr,PO,opr,rpr,tpr,Q_e,apr,npr,T6,Axe,Rc,t9,H_e,F6,spr,U_e,lpr,Lxe,Ar,C6,ipr,Sc,dpr,J_e,cpr,fpr,Y_e,mpr,gpr,hpr,M6,ppr,K_e,_pr,upr,bpr,Tt,E6,vpr,Z_e,Tpr,Fpr,Pc,Cpr,eue,Mpr,Epr,oue,ypr,wpr,Apr,rue,Lpr,Bpr,y6,xpr,Mo,w6,kpr,tue,Rpr,Spr,Tn,Ppr,aue,$pr,Ipr,nue,Dpr,jpr,sue,Npr,qpr,Gpr,Z,a9,lue,Opr,Xpr,$O,Vpr,zpr,Wpr,n9,iue,Qpr,Hpr,IO,Upr,Jpr,Ypr,s9,due,Kpr,Zpr,DO,e_r,o_r,r_r,l9,cue,t_r,a_r,jO,n_r,s_r,l_r,i9,fue,i_r,d_r,NO,c_r,f_r,m_r,d9,mue,g_r,h_r,qO,p_r,__r,u_r,c9,gue,b_r,v_r,GO,T_r,F_r,C_r,f9,hue,M_r,E_r,OO,y_r,w_r,A_r,m9,pue,L_r,B_r,XO,x_r,k_r,R_r,g9,_ue,S_r,P_r,VO,$_r,I_r,D_r,h9,uue,j_r,N_r,zO,q_r,G_r,O_r,p9,bue,X_r,V_r,WO,z_r,W_r,Q_r,_9,vue,H_r,U_r,QO,J_r,Y_r,K_r,u9,Tue,Z_r,eur,HO,our,rur,tur,b9,Fue,aur,nur,UO,sur,lur,iur,v9,Cue,dur,cur,JO,fur,mur,gur,T9,Mue,hur,pur,YO,_ur,uur,bur,F9,Eue,vur,Tur,KO,Fur,Cur,Mur,C9,yue,Eur,yur,ZO,wur,Aur,Lur,wue,Bur,xur,A6,Bxe,$c,M9,Aue,L6,kur,Lue,Rur,xxe,Lr,B6,Sur,Ic,Pur,Bue,$ur,Iur,xue,Dur,jur,Nur,x6,qur,kue,Gur,Our,Xur,Ft,k6,Vur,Rue,zur,Wur,Dc,Qur,Sue,Hur,Uur,Pue,Jur,Yur,Kur,$ue,Zur,e0r,R6,o0r,Eo,S6,r0r,Iue,t0r,a0r,Fn,n0r,Due,s0r,l0r,jue,i0r,d0r,Nue,c0r,f0r,m0r,que,E9,Gue,g0r,h0r,eX,p0r,_0r,u0r,Oue,b0r,v0r,P6,kxe,jc,y9,Xue,$6,T0r,Vue,F0r,Rxe,Br,I6,C0r,Nc,M0r,zue,E0r,y0r,Wue,w0r,A0r,L0r,D6,B0r,Que,x0r,k0r,R0r,Ct,j6,S0r,Hue,P0r,$0r,qc,I0r,Uue,D0r,j0r,Jue,N0r,q0r,G0r,Yue,O0r,X0r,N6,V0r,yo,q6,z0r,Kue,W0r,Q0r,Cn,H0r,Zue,U0r,J0r,e0e,Y0r,K0r,o0e,Z0r,e1r,o1r,r0e,w9,t0e,r1r,t1r,oX,a1r,n1r,s1r,a0e,l1r,i1r,G6,Sxe,Gc,A9,n0e,O6,d1r,s0e,c1r,Pxe,xr,X6,f1r,Oc,m1r,l0e,g1r,h1r,i0e,p1r,_1r,u1r,V6,b1r,d0e,v1r,T1r,F1r,Mt,z6,C1r,c0e,M1r,E1r,Xc,y1r,f0e,w1r,A1r,m0e,L1r,B1r,x1r,g0e,k1r,R1r,W6,S1r,wo,Q6,P1r,h0e,$1r,I1r,Mn,D1r,p0e,j1r,N1r,_0e,q1r,G1r,u0e,O1r,X1r,V1r,V,L9,b0e,z1r,W1r,rX,Q1r,H1r,U1r,B9,v0e,J1r,Y1r,tX,K1r,Z1r,ebr,x9,T0e,obr,rbr,aX,tbr,abr,nbr,k9,F0e,sbr,lbr,nX,ibr,dbr,cbr,R9,C0e,fbr,mbr,sX,gbr,hbr,pbr,S9,M0e,_br,ubr,lX,bbr,vbr,Tbr,P9,E0e,Fbr,Cbr,iX,Mbr,Ebr,ybr,$9,y0e,wbr,Abr,dX,Lbr,Bbr,xbr,I9,w0e,kbr,Rbr,cX,Sbr,Pbr,$br,D9,A0e,Ibr,Dbr,fX,jbr,Nbr,qbr,j9,L0e,Gbr,Obr,mX,Xbr,Vbr,zbr,N9,B0e,Wbr,Qbr,gX,Hbr,Ubr,Jbr,q9,x0e,Ybr,Kbr,hX,Zbr,e5r,o5r,G9,k0e,r5r,t5r,pX,a5r,n5r,s5r,O9,R0e,l5r,i5r,_X,d5r,c5r,f5r,X9,S0e,m5r,g5r,uX,h5r,p5r,_5r,V9,P0e,u5r,b5r,bX,v5r,T5r,F5r,z9,$0e,C5r,M5r,vX,E5r,y5r,w5r,W9,I0e,A5r,L5r,TX,B5r,x5r,k5r,Q9,D0e,R5r,S5r,FX,P5r,$5r,I5r,H9,j0e,D5r,j5r,CX,N5r,q5r,G5r,U9,N0e,O5r,X5r,MX,V5r,z5r,W5r,J9,q0e,Q5r,H5r,EX,U5r,J5r,Y5r,Y9,G0e,K5r,Z5r,yX,e2r,o2r,r2r,K9,O0e,t2r,a2r,wX,n2r,s2r,l2r,X0e,i2r,d2r,H6,$xe,Vc,Z9,V0e,U6,c2r,z0e,f2r,Ixe,kr,J6,m2r,zc,g2r,W0e,h2r,p2r,Q0e,_2r,u2r,b2r,Y6,v2r,H0e,T2r,F2r,C2r,Et,K6,M2r,U0e,E2r,y2r,Wc,w2r,J0e,A2r,L2r,Y0e,B2r,x2r,k2r,K0e,R2r,S2r,Z6,P2r,Ao,eA,$2r,Z0e,I2r,D2r,En,j2r,e1e,N2r,q2r,o1e,G2r,O2r,r1e,X2r,V2r,z2r,yn,eC,t1e,W2r,Q2r,AX,H2r,U2r,J2r,oC,a1e,Y2r,K2r,LX,Z2r,evr,ovr,rC,n1e,rvr,tvr,BX,avr,nvr,svr,tC,s1e,lvr,ivr,xX,dvr,cvr,fvr,l1e,mvr,gvr,oA,Dxe,Qc,aC,i1e,rA,hvr,d1e,pvr,jxe,Rr,tA,_vr,Hc,uvr,c1e,bvr,vvr,f1e,Tvr,Fvr,Cvr,aA,Mvr,m1e,Evr,yvr,wvr,yt,nA,Avr,g1e,Lvr,Bvr,Uc,xvr,h1e,kvr,Rvr,p1e,Svr,Pvr,$vr,_1e,Ivr,Dvr,sA,jvr,Lo,lA,Nvr,u1e,qvr,Gvr,wn,Ovr,b1e,Xvr,Vvr,v1e,zvr,Wvr,T1e,Qvr,Hvr,Uvr,ce,nC,F1e,Jvr,Yvr,kX,Kvr,Zvr,eTr,sC,C1e,oTr,rTr,RX,tTr,aTr,nTr,lC,M1e,sTr,lTr,SX,iTr,dTr,cTr,iC,E1e,fTr,mTr,PX,gTr,hTr,pTr,dC,y1e,_Tr,uTr,$X,bTr,vTr,TTr,cC,w1e,FTr,CTr,IX,MTr,ETr,yTr,fC,A1e,wTr,ATr,DX,LTr,BTr,xTr,mC,L1e,kTr,RTr,jX,STr,PTr,$Tr,gC,B1e,ITr,DTr,NX,jTr,NTr,qTr,hC,x1e,GTr,OTr,qX,XTr,VTr,zTr,pC,k1e,WTr,QTr,GX,HTr,UTr,JTr,_C,R1e,YTr,KTr,OX,ZTr,eFr,oFr,S1e,rFr,tFr,iA,Nxe,Jc,uC,P1e,dA,aFr,$1e,nFr,qxe,Sr,cA,sFr,Yc,lFr,I1e,iFr,dFr,D1e,cFr,fFr,mFr,fA,gFr,j1e,hFr,pFr,_Fr,wt,mA,uFr,N1e,bFr,vFr,Kc,TFr,q1e,FFr,CFr,G1e,MFr,EFr,yFr,O1e,wFr,AFr,gA,LFr,Bo,hA,BFr,X1e,xFr,kFr,An,RFr,V1e,SFr,PFr,z1e,$Fr,IFr,W1e,DFr,jFr,NFr,ue,bC,Q1e,qFr,GFr,XX,OFr,XFr,VFr,vC,H1e,zFr,WFr,VX,QFr,HFr,UFr,TC,U1e,JFr,YFr,zX,KFr,ZFr,e9r,FC,J1e,o9r,r9r,WX,t9r,a9r,n9r,CC,Y1e,s9r,l9r,QX,i9r,d9r,c9r,MC,K1e,f9r,m9r,HX,g9r,h9r,p9r,EC,Z1e,_9r,u9r,UX,b9r,v9r,T9r,yC,ebe,F9r,C9r,JX,M9r,E9r,y9r,wC,obe,w9r,A9r,YX,L9r,B9r,x9r,AC,rbe,k9r,R9r,KX,S9r,P9r,$9r,tbe,I9r,D9r,pA,Gxe,Zc,LC,abe,_A,j9r,nbe,N9r,Oxe,Pr,uA,q9r,ef,G9r,sbe,O9r,X9r,lbe,V9r,z9r,W9r,bA,Q9r,ibe,H9r,U9r,J9r,At,vA,Y9r,dbe,K9r,Z9r,of,eCr,cbe,oCr,rCr,fbe,tCr,aCr,nCr,mbe,sCr,lCr,TA,iCr,xo,FA,dCr,gbe,cCr,fCr,Ln,mCr,hbe,gCr,hCr,pbe,pCr,_Cr,_be,uCr,bCr,vCr,Ce,BC,ube,TCr,FCr,ZX,CCr,MCr,ECr,xC,bbe,yCr,wCr,eV,ACr,LCr,BCr,kC,vbe,xCr,kCr,oV,RCr,SCr,PCr,RC,Tbe,$Cr,ICr,rV,DCr,jCr,NCr,SC,Fbe,qCr,GCr,tV,OCr,XCr,VCr,PC,Cbe,zCr,WCr,aV,QCr,HCr,UCr,$C,Mbe,JCr,YCr,nV,KCr,ZCr,eMr,IC,Ebe,oMr,rMr,sV,tMr,aMr,nMr,DC,ybe,sMr,lMr,lV,iMr,dMr,cMr,wbe,fMr,mMr,CA,Xxe,rf,jC,Abe,MA,gMr,Lbe,hMr,Vxe,$r,EA,pMr,tf,_Mr,Bbe,uMr,bMr,xbe,vMr,TMr,FMr,yA,CMr,kbe,MMr,EMr,yMr,Lt,wA,wMr,Rbe,AMr,LMr,af,BMr,Sbe,xMr,kMr,Pbe,RMr,SMr,PMr,$be,$Mr,IMr,AA,DMr,ko,LA,jMr,Ibe,NMr,qMr,Bn,GMr,Dbe,OMr,XMr,jbe,VMr,zMr,Nbe,WMr,QMr,HMr,be,NC,qbe,UMr,JMr,iV,YMr,KMr,ZMr,qC,Gbe,e4r,o4r,dV,r4r,t4r,a4r,GC,Obe,n4r,s4r,cV,l4r,i4r,d4r,OC,Xbe,c4r,f4r,fV,m4r,g4r,h4r,XC,Vbe,p4r,_4r,mV,u4r,b4r,v4r,VC,zbe,T4r,F4r,gV,C4r,M4r,E4r,zC,Wbe,y4r,w4r,hV,A4r,L4r,B4r,WC,Qbe,x4r,k4r,pV,R4r,S4r,P4r,QC,Hbe,$4r,I4r,_V,D4r,j4r,N4r,HC,Ube,q4r,G4r,uV,O4r,X4r,V4r,Jbe,z4r,W4r,BA,zxe,nf,UC,Ybe,xA,Q4r,Kbe,H4r,Wxe,Ir,kA,U4r,sf,J4r,Zbe,Y4r,K4r,e5e,Z4r,eEr,oEr,RA,rEr,o5e,tEr,aEr,nEr,Bt,SA,sEr,r5e,lEr,iEr,lf,dEr,t5e,cEr,fEr,a5e,mEr,gEr,hEr,n5e,pEr,_Er,PA,uEr,Ro,$A,bEr,s5e,vEr,TEr,xn,FEr,l5e,CEr,MEr,i5e,EEr,yEr,d5e,wEr,AEr,LEr,ve,JC,c5e,BEr,xEr,bV,kEr,REr,SEr,YC,f5e,PEr,$Er,vV,IEr,DEr,jEr,KC,m5e,NEr,qEr,TV,GEr,OEr,XEr,ZC,g5e,VEr,zEr,FV,WEr,QEr,HEr,eM,h5e,UEr,JEr,CV,YEr,KEr,ZEr,oM,p5e,e3r,o3r,MV,r3r,t3r,a3r,rM,_5e,n3r,s3r,EV,l3r,i3r,d3r,tM,u5e,c3r,f3r,yV,m3r,g3r,h3r,aM,b5e,p3r,_3r,wV,u3r,b3r,v3r,nM,v5e,T3r,F3r,AV,C3r,M3r,E3r,T5e,y3r,w3r,IA,Qxe,df,sM,F5e,DA,A3r,C5e,L3r,Hxe,Dr,jA,B3r,cf,x3r,M5e,k3r,R3r,E5e,S3r,P3r,$3r,NA,I3r,y5e,D3r,j3r,N3r,xt,qA,q3r,w5e,G3r,O3r,ff,X3r,A5e,V3r,z3r,L5e,W3r,Q3r,H3r,B5e,U3r,J3r,GA,Y3r,So,OA,K3r,x5e,Z3r,eyr,kn,oyr,k5e,ryr,tyr,R5e,ayr,nyr,S5e,syr,lyr,iyr,Re,lM,P5e,dyr,cyr,LV,fyr,myr,gyr,iM,$5e,hyr,pyr,BV,_yr,uyr,byr,dM,I5e,vyr,Tyr,xV,Fyr,Cyr,Myr,cM,D5e,Eyr,yyr,kV,wyr,Ayr,Lyr,fM,j5e,Byr,xyr,RV,kyr,Ryr,Syr,mM,N5e,Pyr,$yr,SV,Iyr,Dyr,jyr,gM,q5e,Nyr,qyr,PV,Gyr,Oyr,Xyr,hM,G5e,Vyr,zyr,$V,Wyr,Qyr,Hyr,O5e,Uyr,Jyr,XA,Uxe,mf,pM,X5e,VA,Yyr,V5e,Kyr,Jxe,jr,zA,Zyr,gf,ewr,z5e,owr,rwr,W5e,twr,awr,nwr,WA,swr,Q5e,lwr,iwr,dwr,kt,QA,cwr,H5e,fwr,mwr,hf,gwr,U5e,hwr,pwr,J5e,_wr,uwr,bwr,Y5e,vwr,Twr,HA,Fwr,Po,UA,Cwr,K5e,Mwr,Ewr,Rn,ywr,Z5e,wwr,Awr,e2e,Lwr,Bwr,o2e,xwr,kwr,Rwr,Se,_M,r2e,Swr,Pwr,IV,$wr,Iwr,Dwr,uM,t2e,jwr,Nwr,DV,qwr,Gwr,Owr,bM,a2e,Xwr,Vwr,jV,zwr,Wwr,Qwr,vM,n2e,Hwr,Uwr,NV,Jwr,Ywr,Kwr,TM,s2e,Zwr,e6r,qV,o6r,r6r,t6r,FM,l2e,a6r,n6r,GV,s6r,l6r,i6r,CM,i2e,d6r,c6r,OV,f6r,m6r,g6r,MM,d2e,h6r,p6r,XV,_6r,u6r,b6r,c2e,v6r,T6r,JA,Yxe,pf,EM,f2e,YA,F6r,m2e,C6r,Kxe,Nr,KA,M6r,_f,E6r,g2e,y6r,w6r,h2e,A6r,L6r,B6r,ZA,x6r,p2e,k6r,R6r,S6r,Rt,eL,P6r,_2e,$6r,I6r,uf,D6r,u2e,j6r,N6r,b2e,q6r,G6r,O6r,v2e,X6r,V6r,oL,z6r,$o,rL,W6r,T2e,Q6r,H6r,Sn,U6r,F2e,J6r,Y6r,C2e,K6r,Z6r,M2e,eAr,oAr,rAr,E2e,yM,y2e,tAr,aAr,VV,nAr,sAr,lAr,w2e,iAr,dAr,tL,Zxe,bf,wM,A2e,aL,cAr,L2e,fAr,eke,qr,nL,mAr,vf,gAr,B2e,hAr,pAr,x2e,_Ar,uAr,bAr,sL,vAr,k2e,TAr,FAr,CAr,St,lL,MAr,R2e,EAr,yAr,Tf,wAr,S2e,AAr,LAr,P2e,BAr,xAr,kAr,$2e,RAr,SAr,iL,PAr,Io,dL,$Ar,I2e,IAr,DAr,Pn,jAr,D2e,NAr,qAr,j2e,GAr,OAr,N2e,XAr,VAr,zAr,cL,AM,q2e,WAr,QAr,zV,HAr,UAr,JAr,LM,G2e,YAr,KAr,WV,ZAr,eLr,oLr,O2e,rLr,tLr,fL,oke,Ff,BM,X2e,mL,aLr,V2e,nLr,rke,Gr,gL,sLr,Cf,lLr,z2e,iLr,dLr,W2e,cLr,fLr,mLr,hL,gLr,Q2e,hLr,pLr,_Lr,Pt,pL,uLr,H2e,bLr,vLr,Mf,TLr,U2e,FLr,CLr,J2e,MLr,ELr,yLr,Y2e,wLr,ALr,_L,LLr,Do,uL,BLr,K2e,xLr,kLr,$n,RLr,Z2e,SLr,PLr,eve,$Lr,ILr,ove,DLr,jLr,NLr,rve,xM,tve,qLr,GLr,QV,OLr,XLr,VLr,ave,zLr,WLr,bL,tke;return fe=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased")',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),P4=new z({}),$4=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),xf=new QLr({props:{warning:"&lcub;true}",$$slots:{default:[K5t]},$$scope:{ctx:Li}}}),I4=new z({}),D4=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/configuration_auto.py#L526"}}),q4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/configuration_auto.py#L549",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),G4=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),O4=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/configuration_auto.py#L671",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),X4=new z({}),V4=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/tokenization_auto.py#L352"}}),Q4=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/tokenization_auto.py#L366",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/pr_15900/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),H4=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),U4=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/tokenization_auto.py#L562",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),J4=new z({}),Y4=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),eE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/pr_15900/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),mh=new QLr({props:{$$slots:{default:[Z5t]},$$scope:{ctx:Li}}}),oE=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),rE=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),tE=new z({}),aE=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/processing_auto.py#L71"}}),lE=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Mh=new QLr({props:{$$slots:{default:[e2t]},$$scope:{ctx:Li}}}),iE=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),dE=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),cE=new z({}),fE=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L687"}}),gE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),hE=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),pE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_E=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uE=new z({}),bE=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L694"}}),TE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),FE=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),CE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ME=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),EE=new z({}),yE=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L709"}}),AE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),LE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),BE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xE=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),kE=new z({}),RE=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L716"}}),PE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),$E=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),IE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),DE=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jE=new z({}),NE=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L723"}}),GE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),OE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),XE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),VE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zE=new z({}),WE=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L732"}}),HE=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),UE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),JE=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),YE=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),KE=new z({}),ZE=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L766"}}),o3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),r3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),t3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),n3=new z({}),s3=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L773"}}),i3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),d3=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),c3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),f3=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m3=new z({}),g3=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L759"}}),p3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),_3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),u3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v3=new z({}),T3=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L741"}}),C3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),M3=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),E3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),y3=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),w3=new z({}),A3=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L748"}}),B3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),x3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),k3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),R3=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),S3=new z({}),P3=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L782"}}),I3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),D3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),j3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),N3=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),q3=new z({}),G3=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L812"}}),X3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),V3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),z3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),W3=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Q3=new z({}),H3=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L819"}}),J3=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Y3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),K3=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Z3=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ey=new z({}),oy=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L842"}}),ty=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),ay=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),ny=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ly=new z({}),iy=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L826"}}),cy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),fy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),my=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hy=new z({}),py=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L833"}}),uy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),by=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),vy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Fy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Cy=new z({}),My=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L851"}}),yy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),wy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),Ay=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ly=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),By=new z({}),xy=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L858"}}),Ry=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Sy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),Py=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Iy=new z({}),Dy=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L805"}}),Ny=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),qy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),Gy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Xy=new z({}),Vy=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L789"}}),Wy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Qy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),Hy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Jy=new z({}),Yy=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_auto.py#L796"}}),Zy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),ew=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),ow=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aw=new z({}),nw=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L373"}}),lw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),iw=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),dw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cw=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fw=new z({}),mw=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L380"}}),hw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),pw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),_w=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bw=new z({}),vw=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L395"}}),Fw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),Mw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ew=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yw=new z({}),ww=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L402"}}),Lw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Bw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),xw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Rw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Sw=new z({}),Pw=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L416"}}),Iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),jw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Nw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qw=new z({}),Gw=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L423"}}),Xw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),Vw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),zw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ww=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qw=new z({}),Hw=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),Jw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e6=new z({}),o6=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L468"}}),t6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),a6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),n6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),s6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l6=new z({}),i6=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L448"}}),c6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),f6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),m6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),h6=new z({}),p6=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L459"}}),u6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),b6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),v6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),T6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),F6=new z({}),C6=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),E6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),y6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),w6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),A6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),L6=new z({}),B6=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L409"}}),k6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),R6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),S6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),P6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$6=new z({}),I6=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_tf_auto.py#L484"}}),j6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),N6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),q6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),G6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),O6=new z({}),X6=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L236"}}),z6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaModel">FlaxXLMRobertaModel</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),W6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),Q6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),H6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),U6=new z({}),J6=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L250"}}),K6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),Z6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),eA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rA=new z({}),tA=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L243"}}),nA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),sA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),lA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dA=new z({}),cA=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),mA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),gA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),hA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_A=new z({}),uA=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L264"}}),vA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),TA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),FA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),MA=new z({}),EA=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),wA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForSequenceClassification">FlaxXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),AA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),LA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),BA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xA=new z({}),kA=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),SA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForQuestionAnswering">FlaxXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),PA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),$A=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),IA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DA=new z({}),jA=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),qA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForTokenClassification">FlaxXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),GA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),OA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VA=new z({}),zA=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),QA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMultipleChoice">FlaxXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),HA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),UA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JA=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YA=new z({}),KA=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L305"}}),eL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),oL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),rL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aL=new z({}),nL=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L314"}}),lL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),iL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),dL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mL=new z({}),gL=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/modeling_flax_auto.py#L323"}}),pL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),_L=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),uL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15900/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/pr_15900/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/pr_15900/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Pe=l(),ie=a("h1"),ge=a("a"),lo=a("span"),f(fe.$$.fragment),Te=l(),Xo=a("span"),Bi=o("Auto Classes"),yf=l(),sa=a("p"),xi=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),ki=a("code"),x4=o("from_pretrained()"),wf=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Le=l(),io=a("p"),Ri=o("Instantiating one of "),In=a("a"),k4=o("AutoConfig"),Dn=o(", "),jn=a("a"),R4=o("AutoModel"),Si=o(`, and
`),Nn=a("a"),S4=o("AutoTokenizer"),Pi=o(" will directly create a class of the relevant architecture. For instance"),Af=l(),f($a.$$.fragment),co=l(),he=a("p"),_8=o("will create a model that is an instance of "),$i=a("a"),u8=o("BertModel"),b8=o("."),Vo=l(),Ia=a("p"),v8=o("There is one class of "),Lf=a("code"),T8=o("AutoModel"),hSe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),aBe=l(),Ii=a("h2"),Bf=a("a"),jW=a("span"),f(P4.$$.fragment),pSe=l(),NW=a("span"),_Se=o("Extending the Auto Classes"),nBe=l(),qn=a("p"),uSe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),qW=a("code"),bSe=o("NewModel"),vSe=o(", make sure you have a "),GW=a("code"),TSe=o("NewModelConfig"),FSe=o(` then you can add those to the auto
classes like this:`),sBe=l(),f($4.$$.fragment),lBe=l(),F8=a("p"),CSe=o("You will then be able to use the auto classes like you would usually do!"),iBe=l(),f(xf.$$.fragment),dBe=l(),Di=a("h2"),kf=a("a"),OW=a("span"),f(I4.$$.fragment),MSe=l(),XW=a("span"),ESe=o("AutoConfig"),cBe=l(),zo=a("div"),f(D4.$$.fragment),ySe=l(),j4=a("p"),wSe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),C8=a("a"),ASe=o("from_pretrained()"),LSe=o(" class method."),BSe=l(),N4=a("p"),xSe=o("This class cannot be instantiated directly using "),VW=a("code"),kSe=o("__init__()"),RSe=o(" (throws an error)."),SSe=l(),fo=a("div"),f(q4.$$.fragment),PSe=l(),zW=a("p"),$Se=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),ISe=l(),ji=a("p"),DSe=o("The configuration class to instantiate is selected based on the "),WW=a("code"),jSe=o("model_type"),NSe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),QW=a("code"),qSe=o("pretrained_model_name_or_path"),GSe=o(":"),OSe=l(),v=a("ul"),Rf=a("li"),HW=a("strong"),XSe=o("albert"),VSe=o(" \u2014 "),M8=a("a"),zSe=o("AlbertConfig"),WSe=o(" (ALBERT model)"),QSe=l(),Sf=a("li"),UW=a("strong"),HSe=o("bart"),USe=o(" \u2014 "),E8=a("a"),JSe=o("BartConfig"),YSe=o(" (BART model)"),KSe=l(),Pf=a("li"),JW=a("strong"),ZSe=o("beit"),ePe=o(" \u2014 "),y8=a("a"),oPe=o("BeitConfig"),rPe=o(" (BEiT model)"),tPe=l(),$f=a("li"),YW=a("strong"),aPe=o("bert"),nPe=o(" \u2014 "),w8=a("a"),sPe=o("BertConfig"),lPe=o(" (BERT model)"),iPe=l(),If=a("li"),KW=a("strong"),dPe=o("bert-generation"),cPe=o(" \u2014 "),A8=a("a"),fPe=o("BertGenerationConfig"),mPe=o(" (Bert Generation model)"),gPe=l(),Df=a("li"),ZW=a("strong"),hPe=o("big_bird"),pPe=o(" \u2014 "),L8=a("a"),_Pe=o("BigBirdConfig"),uPe=o(" (BigBird model)"),bPe=l(),jf=a("li"),eQ=a("strong"),vPe=o("bigbird_pegasus"),TPe=o(" \u2014 "),B8=a("a"),FPe=o("BigBirdPegasusConfig"),CPe=o(" (BigBirdPegasus model)"),MPe=l(),Nf=a("li"),oQ=a("strong"),EPe=o("blenderbot"),yPe=o(" \u2014 "),x8=a("a"),wPe=o("BlenderbotConfig"),APe=o(" (Blenderbot model)"),LPe=l(),qf=a("li"),rQ=a("strong"),BPe=o("blenderbot-small"),xPe=o(" \u2014 "),k8=a("a"),kPe=o("BlenderbotSmallConfig"),RPe=o(" (BlenderbotSmall model)"),SPe=l(),Gf=a("li"),tQ=a("strong"),PPe=o("camembert"),$Pe=o(" \u2014 "),R8=a("a"),IPe=o("CamembertConfig"),DPe=o(" (CamemBERT model)"),jPe=l(),Of=a("li"),aQ=a("strong"),NPe=o("canine"),qPe=o(" \u2014 "),S8=a("a"),GPe=o("CanineConfig"),OPe=o(" (Canine model)"),XPe=l(),Xf=a("li"),nQ=a("strong"),VPe=o("clip"),zPe=o(" \u2014 "),P8=a("a"),WPe=o("CLIPConfig"),QPe=o(" (CLIP model)"),HPe=l(),Vf=a("li"),sQ=a("strong"),UPe=o("convbert"),JPe=o(" \u2014 "),$8=a("a"),YPe=o("ConvBertConfig"),KPe=o(" (ConvBERT model)"),ZPe=l(),zf=a("li"),lQ=a("strong"),e$e=o("convnext"),o$e=o(" \u2014 "),I8=a("a"),r$e=o("ConvNextConfig"),t$e=o(" (ConvNext model)"),a$e=l(),Wf=a("li"),iQ=a("strong"),n$e=o("ctrl"),s$e=o(" \u2014 "),D8=a("a"),l$e=o("CTRLConfig"),i$e=o(" (CTRL model)"),d$e=l(),Qf=a("li"),dQ=a("strong"),c$e=o("data2vec-audio"),f$e=o(" \u2014 "),j8=a("a"),m$e=o("Data2VecAudioConfig"),g$e=o(" (Data2VecAudio model)"),h$e=l(),Hf=a("li"),cQ=a("strong"),p$e=o("data2vec-text"),_$e=o(" \u2014 "),N8=a("a"),u$e=o("Data2VecTextConfig"),b$e=o(" (Data2VecText model)"),v$e=l(),Uf=a("li"),fQ=a("strong"),T$e=o("deberta"),F$e=o(" \u2014 "),q8=a("a"),C$e=o("DebertaConfig"),M$e=o(" (DeBERTa model)"),E$e=l(),Jf=a("li"),mQ=a("strong"),y$e=o("deberta-v2"),w$e=o(" \u2014 "),G8=a("a"),A$e=o("DebertaV2Config"),L$e=o(" (DeBERTa-v2 model)"),B$e=l(),Yf=a("li"),gQ=a("strong"),x$e=o("deit"),k$e=o(" \u2014 "),O8=a("a"),R$e=o("DeiTConfig"),S$e=o(" (DeiT model)"),P$e=l(),Kf=a("li"),hQ=a("strong"),$$e=o("detr"),I$e=o(" \u2014 "),X8=a("a"),D$e=o("DetrConfig"),j$e=o(" (DETR model)"),N$e=l(),Zf=a("li"),pQ=a("strong"),q$e=o("distilbert"),G$e=o(" \u2014 "),V8=a("a"),O$e=o("DistilBertConfig"),X$e=o(" (DistilBERT model)"),V$e=l(),em=a("li"),_Q=a("strong"),z$e=o("dpr"),W$e=o(" \u2014 "),z8=a("a"),Q$e=o("DPRConfig"),H$e=o(" (DPR model)"),U$e=l(),om=a("li"),uQ=a("strong"),J$e=o("electra"),Y$e=o(" \u2014 "),W8=a("a"),K$e=o("ElectraConfig"),Z$e=o(" (ELECTRA model)"),eIe=l(),rm=a("li"),bQ=a("strong"),oIe=o("encoder-decoder"),rIe=o(" \u2014 "),Q8=a("a"),tIe=o("EncoderDecoderConfig"),aIe=o(" (Encoder decoder model)"),nIe=l(),tm=a("li"),vQ=a("strong"),sIe=o("flaubert"),lIe=o(" \u2014 "),H8=a("a"),iIe=o("FlaubertConfig"),dIe=o(" (FlauBERT model)"),cIe=l(),am=a("li"),TQ=a("strong"),fIe=o("fnet"),mIe=o(" \u2014 "),U8=a("a"),gIe=o("FNetConfig"),hIe=o(" (FNet model)"),pIe=l(),nm=a("li"),FQ=a("strong"),_Ie=o("fsmt"),uIe=o(" \u2014 "),J8=a("a"),bIe=o("FSMTConfig"),vIe=o(" (FairSeq Machine-Translation model)"),TIe=l(),sm=a("li"),CQ=a("strong"),FIe=o("funnel"),CIe=o(" \u2014 "),Y8=a("a"),MIe=o("FunnelConfig"),EIe=o(" (Funnel Transformer model)"),yIe=l(),lm=a("li"),MQ=a("strong"),wIe=o("gpt2"),AIe=o(" \u2014 "),K8=a("a"),LIe=o("GPT2Config"),BIe=o(" (OpenAI GPT-2 model)"),xIe=l(),im=a("li"),EQ=a("strong"),kIe=o("gpt_neo"),RIe=o(" \u2014 "),Z8=a("a"),SIe=o("GPTNeoConfig"),PIe=o(" (GPT Neo model)"),$Ie=l(),dm=a("li"),yQ=a("strong"),IIe=o("gptj"),DIe=o(" \u2014 "),e7=a("a"),jIe=o("GPTJConfig"),NIe=o(" (GPT-J model)"),qIe=l(),cm=a("li"),wQ=a("strong"),GIe=o("hubert"),OIe=o(" \u2014 "),o7=a("a"),XIe=o("HubertConfig"),VIe=o(" (Hubert model)"),zIe=l(),fm=a("li"),AQ=a("strong"),WIe=o("ibert"),QIe=o(" \u2014 "),r7=a("a"),HIe=o("IBertConfig"),UIe=o(" (I-BERT model)"),JIe=l(),mm=a("li"),LQ=a("strong"),YIe=o("imagegpt"),KIe=o(" \u2014 "),t7=a("a"),ZIe=o("ImageGPTConfig"),eDe=o(" (ImageGPT model)"),oDe=l(),gm=a("li"),BQ=a("strong"),rDe=o("layoutlm"),tDe=o(" \u2014 "),a7=a("a"),aDe=o("LayoutLMConfig"),nDe=o(" (LayoutLM model)"),sDe=l(),hm=a("li"),xQ=a("strong"),lDe=o("layoutlmv2"),iDe=o(" \u2014 "),n7=a("a"),dDe=o("LayoutLMv2Config"),cDe=o(" (LayoutLMv2 model)"),fDe=l(),pm=a("li"),kQ=a("strong"),mDe=o("led"),gDe=o(" \u2014 "),s7=a("a"),hDe=o("LEDConfig"),pDe=o(" (LED model)"),_De=l(),_m=a("li"),RQ=a("strong"),uDe=o("longformer"),bDe=o(" \u2014 "),l7=a("a"),vDe=o("LongformerConfig"),TDe=o(" (Longformer model)"),FDe=l(),um=a("li"),SQ=a("strong"),CDe=o("luke"),MDe=o(" \u2014 "),i7=a("a"),EDe=o("LukeConfig"),yDe=o(" (LUKE model)"),wDe=l(),bm=a("li"),PQ=a("strong"),ADe=o("lxmert"),LDe=o(" \u2014 "),d7=a("a"),BDe=o("LxmertConfig"),xDe=o(" (LXMERT model)"),kDe=l(),vm=a("li"),$Q=a("strong"),RDe=o("m2m_100"),SDe=o(" \u2014 "),c7=a("a"),PDe=o("M2M100Config"),$De=o(" (M2M100 model)"),IDe=l(),Tm=a("li"),IQ=a("strong"),DDe=o("marian"),jDe=o(" \u2014 "),f7=a("a"),NDe=o("MarianConfig"),qDe=o(" (Marian model)"),GDe=l(),Fm=a("li"),DQ=a("strong"),ODe=o("maskformer"),XDe=o(" \u2014 "),m7=a("a"),VDe=o("MaskFormerConfig"),zDe=o(" (MaskFormer model)"),WDe=l(),Cm=a("li"),jQ=a("strong"),QDe=o("mbart"),HDe=o(" \u2014 "),g7=a("a"),UDe=o("MBartConfig"),JDe=o(" (mBART model)"),YDe=l(),Mm=a("li"),NQ=a("strong"),KDe=o("megatron-bert"),ZDe=o(" \u2014 "),h7=a("a"),eje=o("MegatronBertConfig"),oje=o(" (MegatronBert model)"),rje=l(),Em=a("li"),qQ=a("strong"),tje=o("mobilebert"),aje=o(" \u2014 "),p7=a("a"),nje=o("MobileBertConfig"),sje=o(" (MobileBERT model)"),lje=l(),ym=a("li"),GQ=a("strong"),ije=o("mpnet"),dje=o(" \u2014 "),_7=a("a"),cje=o("MPNetConfig"),fje=o(" (MPNet model)"),mje=l(),wm=a("li"),OQ=a("strong"),gje=o("mt5"),hje=o(" \u2014 "),u7=a("a"),pje=o("MT5Config"),_je=o(" (mT5 model)"),uje=l(),Am=a("li"),XQ=a("strong"),bje=o("nystromformer"),vje=o(" \u2014 "),b7=a("a"),Tje=o("NystromformerConfig"),Fje=o(" (Nystromformer model)"),Cje=l(),Lm=a("li"),VQ=a("strong"),Mje=o("openai-gpt"),Eje=o(" \u2014 "),v7=a("a"),yje=o("OpenAIGPTConfig"),wje=o(" (OpenAI GPT model)"),Aje=l(),Bm=a("li"),zQ=a("strong"),Lje=o("pegasus"),Bje=o(" \u2014 "),T7=a("a"),xje=o("PegasusConfig"),kje=o(" (Pegasus model)"),Rje=l(),xm=a("li"),WQ=a("strong"),Sje=o("perceiver"),Pje=o(" \u2014 "),F7=a("a"),$je=o("PerceiverConfig"),Ije=o(" (Perceiver model)"),Dje=l(),km=a("li"),QQ=a("strong"),jje=o("plbart"),Nje=o(" \u2014 "),C7=a("a"),qje=o("PLBartConfig"),Gje=o(" (PLBart model)"),Oje=l(),Rm=a("li"),HQ=a("strong"),Xje=o("poolformer"),Vje=o(" \u2014 "),M7=a("a"),zje=o("PoolFormerConfig"),Wje=o(" (PoolFormer model)"),Qje=l(),Sm=a("li"),UQ=a("strong"),Hje=o("prophetnet"),Uje=o(" \u2014 "),E7=a("a"),Jje=o("ProphetNetConfig"),Yje=o(" (ProphetNet model)"),Kje=l(),Pm=a("li"),JQ=a("strong"),Zje=o("qdqbert"),eNe=o(" \u2014 "),y7=a("a"),oNe=o("QDQBertConfig"),rNe=o(" (QDQBert model)"),tNe=l(),$m=a("li"),YQ=a("strong"),aNe=o("rag"),nNe=o(" \u2014 "),w7=a("a"),sNe=o("RagConfig"),lNe=o(" (RAG model)"),iNe=l(),Im=a("li"),KQ=a("strong"),dNe=o("realm"),cNe=o(" \u2014 "),A7=a("a"),fNe=o("RealmConfig"),mNe=o(" (Realm model)"),gNe=l(),Dm=a("li"),ZQ=a("strong"),hNe=o("reformer"),pNe=o(" \u2014 "),L7=a("a"),_Ne=o("ReformerConfig"),uNe=o(" (Reformer model)"),bNe=l(),jm=a("li"),eH=a("strong"),vNe=o("rembert"),TNe=o(" \u2014 "),B7=a("a"),FNe=o("RemBertConfig"),CNe=o(" (RemBERT model)"),MNe=l(),Nm=a("li"),oH=a("strong"),ENe=o("retribert"),yNe=o(" \u2014 "),x7=a("a"),wNe=o("RetriBertConfig"),ANe=o(" (RetriBERT model)"),LNe=l(),qm=a("li"),rH=a("strong"),BNe=o("roberta"),xNe=o(" \u2014 "),k7=a("a"),kNe=o("RobertaConfig"),RNe=o(" (RoBERTa model)"),SNe=l(),Gm=a("li"),tH=a("strong"),PNe=o("roformer"),$Ne=o(" \u2014 "),R7=a("a"),INe=o("RoFormerConfig"),DNe=o(" (RoFormer model)"),jNe=l(),Om=a("li"),aH=a("strong"),NNe=o("segformer"),qNe=o(" \u2014 "),S7=a("a"),GNe=o("SegformerConfig"),ONe=o(" (SegFormer model)"),XNe=l(),Xm=a("li"),nH=a("strong"),VNe=o("sew"),zNe=o(" \u2014 "),P7=a("a"),WNe=o("SEWConfig"),QNe=o(" (SEW model)"),HNe=l(),Vm=a("li"),sH=a("strong"),UNe=o("sew-d"),JNe=o(" \u2014 "),$7=a("a"),YNe=o("SEWDConfig"),KNe=o(" (SEW-D model)"),ZNe=l(),zm=a("li"),lH=a("strong"),eqe=o("speech-encoder-decoder"),oqe=o(" \u2014 "),I7=a("a"),rqe=o("SpeechEncoderDecoderConfig"),tqe=o(" (Speech Encoder decoder model)"),aqe=l(),Wm=a("li"),iH=a("strong"),nqe=o("speech_to_text"),sqe=o(" \u2014 "),D7=a("a"),lqe=o("Speech2TextConfig"),iqe=o(" (Speech2Text model)"),dqe=l(),Qm=a("li"),dH=a("strong"),cqe=o("speech_to_text_2"),fqe=o(" \u2014 "),j7=a("a"),mqe=o("Speech2Text2Config"),gqe=o(" (Speech2Text2 model)"),hqe=l(),Hm=a("li"),cH=a("strong"),pqe=o("splinter"),_qe=o(" \u2014 "),N7=a("a"),uqe=o("SplinterConfig"),bqe=o(" (Splinter model)"),vqe=l(),Um=a("li"),fH=a("strong"),Tqe=o("squeezebert"),Fqe=o(" \u2014 "),q7=a("a"),Cqe=o("SqueezeBertConfig"),Mqe=o(" (SqueezeBERT model)"),Eqe=l(),Jm=a("li"),mH=a("strong"),yqe=o("swin"),wqe=o(" \u2014 "),G7=a("a"),Aqe=o("SwinConfig"),Lqe=o(" (Swin model)"),Bqe=l(),Ym=a("li"),gH=a("strong"),xqe=o("t5"),kqe=o(" \u2014 "),O7=a("a"),Rqe=o("T5Config"),Sqe=o(" (T5 model)"),Pqe=l(),Km=a("li"),hH=a("strong"),$qe=o("tapas"),Iqe=o(" \u2014 "),X7=a("a"),Dqe=o("TapasConfig"),jqe=o(" (TAPAS model)"),Nqe=l(),Zm=a("li"),pH=a("strong"),qqe=o("transfo-xl"),Gqe=o(" \u2014 "),V7=a("a"),Oqe=o("TransfoXLConfig"),Xqe=o(" (Transformer-XL model)"),Vqe=l(),eg=a("li"),_H=a("strong"),zqe=o("trocr"),Wqe=o(" \u2014 "),z7=a("a"),Qqe=o("TrOCRConfig"),Hqe=o(" (TrOCR model)"),Uqe=l(),og=a("li"),uH=a("strong"),Jqe=o("unispeech"),Yqe=o(" \u2014 "),W7=a("a"),Kqe=o("UniSpeechConfig"),Zqe=o(" (UniSpeech model)"),eGe=l(),rg=a("li"),bH=a("strong"),oGe=o("unispeech-sat"),rGe=o(" \u2014 "),Q7=a("a"),tGe=o("UniSpeechSatConfig"),aGe=o(" (UniSpeechSat model)"),nGe=l(),tg=a("li"),vH=a("strong"),sGe=o("vilt"),lGe=o(" \u2014 "),H7=a("a"),iGe=o("ViltConfig"),dGe=o(" (ViLT model)"),cGe=l(),ag=a("li"),TH=a("strong"),fGe=o("vision-encoder-decoder"),mGe=o(" \u2014 "),U7=a("a"),gGe=o("VisionEncoderDecoderConfig"),hGe=o(" (Vision Encoder decoder model)"),pGe=l(),ng=a("li"),FH=a("strong"),_Ge=o("vision-text-dual-encoder"),uGe=o(" \u2014 "),J7=a("a"),bGe=o("VisionTextDualEncoderConfig"),vGe=o(" (VisionTextDualEncoder model)"),TGe=l(),sg=a("li"),CH=a("strong"),FGe=o("visual_bert"),CGe=o(" \u2014 "),Y7=a("a"),MGe=o("VisualBertConfig"),EGe=o(" (VisualBert model)"),yGe=l(),lg=a("li"),MH=a("strong"),wGe=o("vit"),AGe=o(" \u2014 "),K7=a("a"),LGe=o("ViTConfig"),BGe=o(" (ViT model)"),xGe=l(),ig=a("li"),EH=a("strong"),kGe=o("vit_mae"),RGe=o(" \u2014 "),Z7=a("a"),SGe=o("ViTMAEConfig"),PGe=o(" (ViTMAE model)"),$Ge=l(),dg=a("li"),yH=a("strong"),IGe=o("wav2vec2"),DGe=o(" \u2014 "),eB=a("a"),jGe=o("Wav2Vec2Config"),NGe=o(" (Wav2Vec2 model)"),qGe=l(),cg=a("li"),wH=a("strong"),GGe=o("wavlm"),OGe=o(" \u2014 "),oB=a("a"),XGe=o("WavLMConfig"),VGe=o(" (WavLM model)"),zGe=l(),fg=a("li"),AH=a("strong"),WGe=o("xglm"),QGe=o(" \u2014 "),rB=a("a"),HGe=o("XGLMConfig"),UGe=o(" (XGLM model)"),JGe=l(),mg=a("li"),LH=a("strong"),YGe=o("xlm"),KGe=o(" \u2014 "),tB=a("a"),ZGe=o("XLMConfig"),eOe=o(" (XLM model)"),oOe=l(),gg=a("li"),BH=a("strong"),rOe=o("xlm-prophetnet"),tOe=o(" \u2014 "),aB=a("a"),aOe=o("XLMProphetNetConfig"),nOe=o(" (XLMProphetNet model)"),sOe=l(),hg=a("li"),xH=a("strong"),lOe=o("xlm-roberta"),iOe=o(" \u2014 "),nB=a("a"),dOe=o("XLMRobertaConfig"),cOe=o(" (XLM-RoBERTa model)"),fOe=l(),pg=a("li"),kH=a("strong"),mOe=o("xlm-roberta-xl"),gOe=o(" \u2014 "),sB=a("a"),hOe=o("XLMRobertaXLConfig"),pOe=o(" (XLM-RoBERTa-XL model)"),_Oe=l(),_g=a("li"),RH=a("strong"),uOe=o("xlnet"),bOe=o(" \u2014 "),lB=a("a"),vOe=o("XLNetConfig"),TOe=o(" (XLNet model)"),FOe=l(),ug=a("li"),SH=a("strong"),COe=o("yoso"),MOe=o(" \u2014 "),iB=a("a"),EOe=o("YosoConfig"),yOe=o(" (YOSO model)"),wOe=l(),PH=a("p"),AOe=o("Examples:"),LOe=l(),f(G4.$$.fragment),BOe=l(),bg=a("div"),f(O4.$$.fragment),xOe=l(),$H=a("p"),kOe=o("Register a new configuration for this class."),fBe=l(),Ni=a("h2"),vg=a("a"),IH=a("span"),f(X4.$$.fragment),ROe=l(),DH=a("span"),SOe=o("AutoTokenizer"),mBe=l(),Wo=a("div"),f(V4.$$.fragment),POe=l(),z4=a("p"),$Oe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),dB=a("a"),IOe=o("AutoTokenizer.from_pretrained()"),DOe=o(" class method."),jOe=l(),W4=a("p"),NOe=o("This class cannot be instantiated directly using "),jH=a("code"),qOe=o("__init__()"),GOe=o(" (throws an error)."),OOe=l(),mo=a("div"),f(Q4.$$.fragment),XOe=l(),NH=a("p"),VOe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),zOe=l(),Da=a("p"),WOe=o("The tokenizer class to instantiate is selected based on the "),qH=a("code"),QOe=o("model_type"),HOe=o(` property of the config object (either
passed as an argument or loaded from `),GH=a("code"),UOe=o("pretrained_model_name_or_path"),JOe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),OH=a("code"),YOe=o("pretrained_model_name_or_path"),KOe=o(":"),ZOe=l(),M=a("ul"),Gn=a("li"),XH=a("strong"),eXe=o("albert"),oXe=o(" \u2014 "),cB=a("a"),rXe=o("AlbertTokenizer"),tXe=o(" or "),fB=a("a"),aXe=o("AlbertTokenizerFast"),nXe=o(" (ALBERT model)"),sXe=l(),On=a("li"),VH=a("strong"),lXe=o("bart"),iXe=o(" \u2014 "),mB=a("a"),dXe=o("BartTokenizer"),cXe=o(" or "),gB=a("a"),fXe=o("BartTokenizerFast"),mXe=o(" (BART model)"),gXe=l(),Xn=a("li"),zH=a("strong"),hXe=o("barthez"),pXe=o(" \u2014 "),hB=a("a"),_Xe=o("BarthezTokenizer"),uXe=o(" or "),pB=a("a"),bXe=o("BarthezTokenizerFast"),vXe=o(" (BARThez model)"),TXe=l(),Tg=a("li"),WH=a("strong"),FXe=o("bartpho"),CXe=o(" \u2014 "),_B=a("a"),MXe=o("BartphoTokenizer"),EXe=o(" (BARTpho model)"),yXe=l(),Vn=a("li"),QH=a("strong"),wXe=o("bert"),AXe=o(" \u2014 "),uB=a("a"),LXe=o("BertTokenizer"),BXe=o(" or "),bB=a("a"),xXe=o("BertTokenizerFast"),kXe=o(" (BERT model)"),RXe=l(),Fg=a("li"),HH=a("strong"),SXe=o("bert-generation"),PXe=o(" \u2014 "),vB=a("a"),$Xe=o("BertGenerationTokenizer"),IXe=o(" (Bert Generation model)"),DXe=l(),Cg=a("li"),UH=a("strong"),jXe=o("bert-japanese"),NXe=o(" \u2014 "),TB=a("a"),qXe=o("BertJapaneseTokenizer"),GXe=o(" (BertJapanese model)"),OXe=l(),Mg=a("li"),JH=a("strong"),XXe=o("bertweet"),VXe=o(" \u2014 "),FB=a("a"),zXe=o("BertweetTokenizer"),WXe=o(" (Bertweet model)"),QXe=l(),zn=a("li"),YH=a("strong"),HXe=o("big_bird"),UXe=o(" \u2014 "),CB=a("a"),JXe=o("BigBirdTokenizer"),YXe=o(" or "),MB=a("a"),KXe=o("BigBirdTokenizerFast"),ZXe=o(" (BigBird model)"),eVe=l(),Wn=a("li"),KH=a("strong"),oVe=o("bigbird_pegasus"),rVe=o(" \u2014 "),EB=a("a"),tVe=o("PegasusTokenizer"),aVe=o(" or "),yB=a("a"),nVe=o("PegasusTokenizerFast"),sVe=o(" (BigBirdPegasus model)"),lVe=l(),Qn=a("li"),ZH=a("strong"),iVe=o("blenderbot"),dVe=o(" \u2014 "),wB=a("a"),cVe=o("BlenderbotTokenizer"),fVe=o(" or "),AB=a("a"),mVe=o("BlenderbotTokenizerFast"),gVe=o(" (Blenderbot model)"),hVe=l(),Eg=a("li"),eU=a("strong"),pVe=o("blenderbot-small"),_Ve=o(" \u2014 "),LB=a("a"),uVe=o("BlenderbotSmallTokenizer"),bVe=o(" (BlenderbotSmall model)"),vVe=l(),yg=a("li"),oU=a("strong"),TVe=o("byt5"),FVe=o(" \u2014 "),BB=a("a"),CVe=o("ByT5Tokenizer"),MVe=o(" (ByT5 model)"),EVe=l(),Hn=a("li"),rU=a("strong"),yVe=o("camembert"),wVe=o(" \u2014 "),xB=a("a"),AVe=o("CamembertTokenizer"),LVe=o(" or "),kB=a("a"),BVe=o("CamembertTokenizerFast"),xVe=o(" (CamemBERT model)"),kVe=l(),wg=a("li"),tU=a("strong"),RVe=o("canine"),SVe=o(" \u2014 "),RB=a("a"),PVe=o("CanineTokenizer"),$Ve=o(" (Canine model)"),IVe=l(),Un=a("li"),aU=a("strong"),DVe=o("clip"),jVe=o(" \u2014 "),SB=a("a"),NVe=o("CLIPTokenizer"),qVe=o(" or "),PB=a("a"),GVe=o("CLIPTokenizerFast"),OVe=o(" (CLIP model)"),XVe=l(),Jn=a("li"),nU=a("strong"),VVe=o("convbert"),zVe=o(" \u2014 "),$B=a("a"),WVe=o("ConvBertTokenizer"),QVe=o(" or "),IB=a("a"),HVe=o("ConvBertTokenizerFast"),UVe=o(" (ConvBERT model)"),JVe=l(),Yn=a("li"),sU=a("strong"),YVe=o("cpm"),KVe=o(" \u2014 "),DB=a("a"),ZVe=o("CpmTokenizer"),eze=o(" or "),lU=a("code"),oze=o("CpmTokenizerFast"),rze=o(" (CPM model)"),tze=l(),Ag=a("li"),iU=a("strong"),aze=o("ctrl"),nze=o(" \u2014 "),jB=a("a"),sze=o("CTRLTokenizer"),lze=o(" (CTRL model)"),ize=l(),Kn=a("li"),dU=a("strong"),dze=o("deberta"),cze=o(" \u2014 "),NB=a("a"),fze=o("DebertaTokenizer"),mze=o(" or "),qB=a("a"),gze=o("DebertaTokenizerFast"),hze=o(" (DeBERTa model)"),pze=l(),Lg=a("li"),cU=a("strong"),_ze=o("deberta-v2"),uze=o(" \u2014 "),GB=a("a"),bze=o("DebertaV2Tokenizer"),vze=o(" (DeBERTa-v2 model)"),Tze=l(),Zn=a("li"),fU=a("strong"),Fze=o("distilbert"),Cze=o(" \u2014 "),OB=a("a"),Mze=o("DistilBertTokenizer"),Eze=o(" or "),XB=a("a"),yze=o("DistilBertTokenizerFast"),wze=o(" (DistilBERT model)"),Aze=l(),es=a("li"),mU=a("strong"),Lze=o("dpr"),Bze=o(" \u2014 "),VB=a("a"),xze=o("DPRQuestionEncoderTokenizer"),kze=o(" or "),zB=a("a"),Rze=o("DPRQuestionEncoderTokenizerFast"),Sze=o(" (DPR model)"),Pze=l(),os=a("li"),gU=a("strong"),$ze=o("electra"),Ize=o(" \u2014 "),WB=a("a"),Dze=o("ElectraTokenizer"),jze=o(" or "),QB=a("a"),Nze=o("ElectraTokenizerFast"),qze=o(" (ELECTRA model)"),Gze=l(),Bg=a("li"),hU=a("strong"),Oze=o("flaubert"),Xze=o(" \u2014 "),HB=a("a"),Vze=o("FlaubertTokenizer"),zze=o(" (FlauBERT model)"),Wze=l(),rs=a("li"),pU=a("strong"),Qze=o("fnet"),Hze=o(" \u2014 "),UB=a("a"),Uze=o("FNetTokenizer"),Jze=o(" or "),JB=a("a"),Yze=o("FNetTokenizerFast"),Kze=o(" (FNet model)"),Zze=l(),xg=a("li"),_U=a("strong"),eWe=o("fsmt"),oWe=o(" \u2014 "),YB=a("a"),rWe=o("FSMTTokenizer"),tWe=o(" (FairSeq Machine-Translation model)"),aWe=l(),ts=a("li"),uU=a("strong"),nWe=o("funnel"),sWe=o(" \u2014 "),KB=a("a"),lWe=o("FunnelTokenizer"),iWe=o(" or "),ZB=a("a"),dWe=o("FunnelTokenizerFast"),cWe=o(" (Funnel Transformer model)"),fWe=l(),as=a("li"),bU=a("strong"),mWe=o("gpt2"),gWe=o(" \u2014 "),ex=a("a"),hWe=o("GPT2Tokenizer"),pWe=o(" or "),ox=a("a"),_We=o("GPT2TokenizerFast"),uWe=o(" (OpenAI GPT-2 model)"),bWe=l(),ns=a("li"),vU=a("strong"),vWe=o("gpt_neo"),TWe=o(" \u2014 "),rx=a("a"),FWe=o("GPT2Tokenizer"),CWe=o(" or "),tx=a("a"),MWe=o("GPT2TokenizerFast"),EWe=o(" (GPT Neo model)"),yWe=l(),ss=a("li"),TU=a("strong"),wWe=o("herbert"),AWe=o(" \u2014 "),ax=a("a"),LWe=o("HerbertTokenizer"),BWe=o(" or "),nx=a("a"),xWe=o("HerbertTokenizerFast"),kWe=o(" (HerBERT model)"),RWe=l(),kg=a("li"),FU=a("strong"),SWe=o("hubert"),PWe=o(" \u2014 "),sx=a("a"),$We=o("Wav2Vec2CTCTokenizer"),IWe=o(" (Hubert model)"),DWe=l(),ls=a("li"),CU=a("strong"),jWe=o("ibert"),NWe=o(" \u2014 "),lx=a("a"),qWe=o("RobertaTokenizer"),GWe=o(" or "),ix=a("a"),OWe=o("RobertaTokenizerFast"),XWe=o(" (I-BERT model)"),VWe=l(),is=a("li"),MU=a("strong"),zWe=o("layoutlm"),WWe=o(" \u2014 "),dx=a("a"),QWe=o("LayoutLMTokenizer"),HWe=o(" or "),cx=a("a"),UWe=o("LayoutLMTokenizerFast"),JWe=o(" (LayoutLM model)"),YWe=l(),ds=a("li"),EU=a("strong"),KWe=o("layoutlmv2"),ZWe=o(" \u2014 "),fx=a("a"),eQe=o("LayoutLMv2Tokenizer"),oQe=o(" or "),mx=a("a"),rQe=o("LayoutLMv2TokenizerFast"),tQe=o(" (LayoutLMv2 model)"),aQe=l(),cs=a("li"),yU=a("strong"),nQe=o("layoutxlm"),sQe=o(" \u2014 "),gx=a("a"),lQe=o("LayoutXLMTokenizer"),iQe=o(" or "),hx=a("a"),dQe=o("LayoutXLMTokenizerFast"),cQe=o(" (LayoutXLM model)"),fQe=l(),fs=a("li"),wU=a("strong"),mQe=o("led"),gQe=o(" \u2014 "),px=a("a"),hQe=o("LEDTokenizer"),pQe=o(" or "),_x=a("a"),_Qe=o("LEDTokenizerFast"),uQe=o(" (LED model)"),bQe=l(),ms=a("li"),AU=a("strong"),vQe=o("longformer"),TQe=o(" \u2014 "),ux=a("a"),FQe=o("LongformerTokenizer"),CQe=o(" or "),bx=a("a"),MQe=o("LongformerTokenizerFast"),EQe=o(" (Longformer model)"),yQe=l(),Rg=a("li"),LU=a("strong"),wQe=o("luke"),AQe=o(" \u2014 "),vx=a("a"),LQe=o("LukeTokenizer"),BQe=o(" (LUKE model)"),xQe=l(),gs=a("li"),BU=a("strong"),kQe=o("lxmert"),RQe=o(" \u2014 "),Tx=a("a"),SQe=o("LxmertTokenizer"),PQe=o(" or "),Fx=a("a"),$Qe=o("LxmertTokenizerFast"),IQe=o(" (LXMERT model)"),DQe=l(),Sg=a("li"),xU=a("strong"),jQe=o("m2m_100"),NQe=o(" \u2014 "),Cx=a("a"),qQe=o("M2M100Tokenizer"),GQe=o(" (M2M100 model)"),OQe=l(),Pg=a("li"),kU=a("strong"),XQe=o("marian"),VQe=o(" \u2014 "),Mx=a("a"),zQe=o("MarianTokenizer"),WQe=o(" (Marian model)"),QQe=l(),hs=a("li"),RU=a("strong"),HQe=o("mbart"),UQe=o(" \u2014 "),Ex=a("a"),JQe=o("MBartTokenizer"),YQe=o(" or "),yx=a("a"),KQe=o("MBartTokenizerFast"),ZQe=o(" (mBART model)"),eHe=l(),ps=a("li"),SU=a("strong"),oHe=o("mbart50"),rHe=o(" \u2014 "),wx=a("a"),tHe=o("MBart50Tokenizer"),aHe=o(" or "),Ax=a("a"),nHe=o("MBart50TokenizerFast"),sHe=o(" (mBART-50 model)"),lHe=l(),$g=a("li"),PU=a("strong"),iHe=o("mluke"),dHe=o(" \u2014 "),Lx=a("a"),cHe=o("MLukeTokenizer"),fHe=o(" (mLUKE model)"),mHe=l(),_s=a("li"),$U=a("strong"),gHe=o("mobilebert"),hHe=o(" \u2014 "),Bx=a("a"),pHe=o("MobileBertTokenizer"),_He=o(" or "),xx=a("a"),uHe=o("MobileBertTokenizerFast"),bHe=o(" (MobileBERT model)"),vHe=l(),us=a("li"),IU=a("strong"),THe=o("mpnet"),FHe=o(" \u2014 "),kx=a("a"),CHe=o("MPNetTokenizer"),MHe=o(" or "),Rx=a("a"),EHe=o("MPNetTokenizerFast"),yHe=o(" (MPNet model)"),wHe=l(),bs=a("li"),DU=a("strong"),AHe=o("mt5"),LHe=o(" \u2014 "),Sx=a("a"),BHe=o("MT5Tokenizer"),xHe=o(" or "),Px=a("a"),kHe=o("MT5TokenizerFast"),RHe=o(" (mT5 model)"),SHe=l(),vs=a("li"),jU=a("strong"),PHe=o("openai-gpt"),$He=o(" \u2014 "),$x=a("a"),IHe=o("OpenAIGPTTokenizer"),DHe=o(" or "),Ix=a("a"),jHe=o("OpenAIGPTTokenizerFast"),NHe=o(" (OpenAI GPT model)"),qHe=l(),Ts=a("li"),NU=a("strong"),GHe=o("pegasus"),OHe=o(" \u2014 "),Dx=a("a"),XHe=o("PegasusTokenizer"),VHe=o(" or "),jx=a("a"),zHe=o("PegasusTokenizerFast"),WHe=o(" (Pegasus model)"),QHe=l(),Ig=a("li"),qU=a("strong"),HHe=o("perceiver"),UHe=o(" \u2014 "),Nx=a("a"),JHe=o("PerceiverTokenizer"),YHe=o(" (Perceiver model)"),KHe=l(),Dg=a("li"),GU=a("strong"),ZHe=o("phobert"),eUe=o(" \u2014 "),qx=a("a"),oUe=o("PhobertTokenizer"),rUe=o(" (PhoBERT model)"),tUe=l(),jg=a("li"),OU=a("strong"),aUe=o("plbart"),nUe=o(" \u2014 "),Gx=a("a"),sUe=o("PLBartTokenizer"),lUe=o(" (PLBart model)"),iUe=l(),Ng=a("li"),XU=a("strong"),dUe=o("prophetnet"),cUe=o(" \u2014 "),Ox=a("a"),fUe=o("ProphetNetTokenizer"),mUe=o(" (ProphetNet model)"),gUe=l(),Fs=a("li"),VU=a("strong"),hUe=o("qdqbert"),pUe=o(" \u2014 "),Xx=a("a"),_Ue=o("BertTokenizer"),uUe=o(" or "),Vx=a("a"),bUe=o("BertTokenizerFast"),vUe=o(" (QDQBert model)"),TUe=l(),qg=a("li"),zU=a("strong"),FUe=o("rag"),CUe=o(" \u2014 "),zx=a("a"),MUe=o("RagTokenizer"),EUe=o(" (RAG model)"),yUe=l(),Cs=a("li"),WU=a("strong"),wUe=o("realm"),AUe=o(" \u2014 "),Wx=a("a"),LUe=o("RealmTokenizer"),BUe=o(" or "),Qx=a("a"),xUe=o("RealmTokenizerFast"),kUe=o(" (Realm model)"),RUe=l(),Ms=a("li"),QU=a("strong"),SUe=o("reformer"),PUe=o(" \u2014 "),Hx=a("a"),$Ue=o("ReformerTokenizer"),IUe=o(" or "),Ux=a("a"),DUe=o("ReformerTokenizerFast"),jUe=o(" (Reformer model)"),NUe=l(),Es=a("li"),HU=a("strong"),qUe=o("rembert"),GUe=o(" \u2014 "),Jx=a("a"),OUe=o("RemBertTokenizer"),XUe=o(" or "),Yx=a("a"),VUe=o("RemBertTokenizerFast"),zUe=o(" (RemBERT model)"),WUe=l(),ys=a("li"),UU=a("strong"),QUe=o("retribert"),HUe=o(" \u2014 "),Kx=a("a"),UUe=o("RetriBertTokenizer"),JUe=o(" or "),Zx=a("a"),YUe=o("RetriBertTokenizerFast"),KUe=o(" (RetriBERT model)"),ZUe=l(),ws=a("li"),JU=a("strong"),eJe=o("roberta"),oJe=o(" \u2014 "),ek=a("a"),rJe=o("RobertaTokenizer"),tJe=o(" or "),ok=a("a"),aJe=o("RobertaTokenizerFast"),nJe=o(" (RoBERTa model)"),sJe=l(),As=a("li"),YU=a("strong"),lJe=o("roformer"),iJe=o(" \u2014 "),rk=a("a"),dJe=o("RoFormerTokenizer"),cJe=o(" or "),tk=a("a"),fJe=o("RoFormerTokenizerFast"),mJe=o(" (RoFormer model)"),gJe=l(),Gg=a("li"),KU=a("strong"),hJe=o("speech_to_text"),pJe=o(" \u2014 "),ak=a("a"),_Je=o("Speech2TextTokenizer"),uJe=o(" (Speech2Text model)"),bJe=l(),Og=a("li"),ZU=a("strong"),vJe=o("speech_to_text_2"),TJe=o(" \u2014 "),nk=a("a"),FJe=o("Speech2Text2Tokenizer"),CJe=o(" (Speech2Text2 model)"),MJe=l(),Ls=a("li"),eJ=a("strong"),EJe=o("splinter"),yJe=o(" \u2014 "),sk=a("a"),wJe=o("SplinterTokenizer"),AJe=o(" or "),lk=a("a"),LJe=o("SplinterTokenizerFast"),BJe=o(" (Splinter model)"),xJe=l(),Bs=a("li"),oJ=a("strong"),kJe=o("squeezebert"),RJe=o(" \u2014 "),ik=a("a"),SJe=o("SqueezeBertTokenizer"),PJe=o(" or "),dk=a("a"),$Je=o("SqueezeBertTokenizerFast"),IJe=o(" (SqueezeBERT model)"),DJe=l(),xs=a("li"),rJ=a("strong"),jJe=o("t5"),NJe=o(" \u2014 "),ck=a("a"),qJe=o("T5Tokenizer"),GJe=o(" or "),fk=a("a"),OJe=o("T5TokenizerFast"),XJe=o(" (T5 model)"),VJe=l(),Xg=a("li"),tJ=a("strong"),zJe=o("tapas"),WJe=o(" \u2014 "),mk=a("a"),QJe=o("TapasTokenizer"),HJe=o(" (TAPAS model)"),UJe=l(),Vg=a("li"),aJ=a("strong"),JJe=o("transfo-xl"),YJe=o(" \u2014 "),gk=a("a"),KJe=o("TransfoXLTokenizer"),ZJe=o(" (Transformer-XL model)"),eYe=l(),zg=a("li"),nJ=a("strong"),oYe=o("wav2vec2"),rYe=o(" \u2014 "),hk=a("a"),tYe=o("Wav2Vec2CTCTokenizer"),aYe=o(" (Wav2Vec2 model)"),nYe=l(),Wg=a("li"),sJ=a("strong"),sYe=o("wav2vec2_phoneme"),lYe=o(" \u2014 "),pk=a("a"),iYe=o("Wav2Vec2PhonemeCTCTokenizer"),dYe=o(" (Wav2Vec2Phoneme model)"),cYe=l(),ks=a("li"),lJ=a("strong"),fYe=o("xglm"),mYe=o(" \u2014 "),_k=a("a"),gYe=o("XGLMTokenizer"),hYe=o(" or "),uk=a("a"),pYe=o("XGLMTokenizerFast"),_Ye=o(" (XGLM model)"),uYe=l(),Qg=a("li"),iJ=a("strong"),bYe=o("xlm"),vYe=o(" \u2014 "),bk=a("a"),TYe=o("XLMTokenizer"),FYe=o(" (XLM model)"),CYe=l(),Hg=a("li"),dJ=a("strong"),MYe=o("xlm-prophetnet"),EYe=o(" \u2014 "),vk=a("a"),yYe=o("XLMProphetNetTokenizer"),wYe=o(" (XLMProphetNet model)"),AYe=l(),Rs=a("li"),cJ=a("strong"),LYe=o("xlm-roberta"),BYe=o(" \u2014 "),Tk=a("a"),xYe=o("XLMRobertaTokenizer"),kYe=o(" or "),Fk=a("a"),RYe=o("XLMRobertaTokenizerFast"),SYe=o(" (XLM-RoBERTa model)"),PYe=l(),Ss=a("li"),fJ=a("strong"),$Ye=o("xlnet"),IYe=o(" \u2014 "),Ck=a("a"),DYe=o("XLNetTokenizer"),jYe=o(" or "),Mk=a("a"),NYe=o("XLNetTokenizerFast"),qYe=o(" (XLNet model)"),GYe=l(),mJ=a("p"),OYe=o("Examples:"),XYe=l(),f(H4.$$.fragment),VYe=l(),Ug=a("div"),f(U4.$$.fragment),zYe=l(),gJ=a("p"),WYe=o("Register a new tokenizer in this mapping."),gBe=l(),qi=a("h2"),Jg=a("a"),hJ=a("span"),f(J4.$$.fragment),QYe=l(),pJ=a("span"),HYe=o("AutoFeatureExtractor"),hBe=l(),Qo=a("div"),f(Y4.$$.fragment),UYe=l(),K4=a("p"),JYe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ek=a("a"),YYe=o("AutoFeatureExtractor.from_pretrained()"),KYe=o(" class method."),ZYe=l(),Z4=a("p"),eKe=o("This class cannot be instantiated directly using "),_J=a("code"),oKe=o("__init__()"),rKe=o(" (throws an error)."),tKe=l(),$e=a("div"),f(eE.$$.fragment),aKe=l(),uJ=a("p"),nKe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),sKe=l(),ja=a("p"),lKe=o("The feature extractor class to instantiate is selected based on the "),bJ=a("code"),iKe=o("model_type"),dKe=o(` property of the config object
(either passed as an argument or loaded from `),vJ=a("code"),cKe=o("pretrained_model_name_or_path"),fKe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),TJ=a("code"),mKe=o("pretrained_model_name_or_path"),gKe=o(":"),hKe=l(),se=a("ul"),Yg=a("li"),FJ=a("strong"),pKe=o("beit"),_Ke=o(" \u2014 "),yk=a("a"),uKe=o("BeitFeatureExtractor"),bKe=o(" (BEiT model)"),vKe=l(),Kg=a("li"),CJ=a("strong"),TKe=o("clip"),FKe=o(" \u2014 "),wk=a("a"),CKe=o("CLIPFeatureExtractor"),MKe=o(" (CLIP model)"),EKe=l(),Zg=a("li"),MJ=a("strong"),yKe=o("convnext"),wKe=o(" \u2014 "),Ak=a("a"),AKe=o("ConvNextFeatureExtractor"),LKe=o(" (ConvNext model)"),BKe=l(),eh=a("li"),EJ=a("strong"),xKe=o("deit"),kKe=o(" \u2014 "),Lk=a("a"),RKe=o("DeiTFeatureExtractor"),SKe=o(" (DeiT model)"),PKe=l(),oh=a("li"),yJ=a("strong"),$Ke=o("detr"),IKe=o(" \u2014 "),Bk=a("a"),DKe=o("DetrFeatureExtractor"),jKe=o(" (DETR model)"),NKe=l(),rh=a("li"),wJ=a("strong"),qKe=o("hubert"),GKe=o(" \u2014 "),xk=a("a"),OKe=o("Wav2Vec2FeatureExtractor"),XKe=o(" (Hubert model)"),VKe=l(),th=a("li"),AJ=a("strong"),zKe=o("layoutlmv2"),WKe=o(" \u2014 "),kk=a("a"),QKe=o("LayoutLMv2FeatureExtractor"),HKe=o(" (LayoutLMv2 model)"),UKe=l(),ah=a("li"),LJ=a("strong"),JKe=o("perceiver"),YKe=o(" \u2014 "),Rk=a("a"),KKe=o("PerceiverFeatureExtractor"),ZKe=o(" (Perceiver model)"),eZe=l(),nh=a("li"),BJ=a("strong"),oZe=o("poolformer"),rZe=o(" \u2014 "),Sk=a("a"),tZe=o("PoolFormerFeatureExtractor"),aZe=o(" (PoolFormer model)"),nZe=l(),sh=a("li"),xJ=a("strong"),sZe=o("segformer"),lZe=o(" \u2014 "),Pk=a("a"),iZe=o("SegformerFeatureExtractor"),dZe=o(" (SegFormer model)"),cZe=l(),lh=a("li"),kJ=a("strong"),fZe=o("speech_to_text"),mZe=o(" \u2014 "),$k=a("a"),gZe=o("Speech2TextFeatureExtractor"),hZe=o(" (Speech2Text model)"),pZe=l(),ih=a("li"),RJ=a("strong"),_Ze=o("swin"),uZe=o(" \u2014 "),Ik=a("a"),bZe=o("ViTFeatureExtractor"),vZe=o(" (Swin model)"),TZe=l(),dh=a("li"),SJ=a("strong"),FZe=o("vit"),CZe=o(" \u2014 "),Dk=a("a"),MZe=o("ViTFeatureExtractor"),EZe=o(" (ViT model)"),yZe=l(),ch=a("li"),PJ=a("strong"),wZe=o("vit_mae"),AZe=o(" \u2014 "),jk=a("a"),LZe=o("ViTFeatureExtractor"),BZe=o(" (ViTMAE model)"),xZe=l(),fh=a("li"),$J=a("strong"),kZe=o("wav2vec2"),RZe=o(" \u2014 "),Nk=a("a"),SZe=o("Wav2Vec2FeatureExtractor"),PZe=o(" (Wav2Vec2 model)"),$Ze=l(),f(mh.$$.fragment),IZe=l(),IJ=a("p"),DZe=o("Examples:"),jZe=l(),f(oE.$$.fragment),NZe=l(),gh=a("div"),f(rE.$$.fragment),qZe=l(),DJ=a("p"),GZe=o("Register a new feature extractor for this class."),pBe=l(),Gi=a("h2"),hh=a("a"),jJ=a("span"),f(tE.$$.fragment),OZe=l(),NJ=a("span"),XZe=o("AutoProcessor"),_Be=l(),Ho=a("div"),f(aE.$$.fragment),VZe=l(),nE=a("p"),zZe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),qk=a("a"),WZe=o("AutoProcessor.from_pretrained()"),QZe=o(" class method."),HZe=l(),sE=a("p"),UZe=o("This class cannot be instantiated directly using "),qJ=a("code"),JZe=o("__init__()"),YZe=o(" (throws an error)."),KZe=l(),Ie=a("div"),f(lE.$$.fragment),ZZe=l(),GJ=a("p"),eeo=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),oeo=l(),Oi=a("p"),reo=o("The processor class to instantiate is selected based on the "),OJ=a("code"),teo=o("model_type"),aeo=o(` property of the config object (either
passed as an argument or loaded from `),XJ=a("code"),neo=o("pretrained_model_name_or_path"),seo=o(" if possible):"),leo=l(),Be=a("ul"),ph=a("li"),VJ=a("strong"),ieo=o("clip"),deo=o(" \u2014 "),Gk=a("a"),ceo=o("CLIPProcessor"),feo=o(" (CLIP model)"),meo=l(),_h=a("li"),zJ=a("strong"),geo=o("layoutlmv2"),heo=o(" \u2014 "),Ok=a("a"),peo=o("LayoutLMv2Processor"),_eo=o(" (LayoutLMv2 model)"),ueo=l(),uh=a("li"),WJ=a("strong"),beo=o("layoutxlm"),veo=o(" \u2014 "),Xk=a("a"),Teo=o("LayoutXLMProcessor"),Feo=o(" (LayoutXLM model)"),Ceo=l(),bh=a("li"),QJ=a("strong"),Meo=o("speech_to_text"),Eeo=o(" \u2014 "),Vk=a("a"),yeo=o("Speech2TextProcessor"),weo=o(" (Speech2Text model)"),Aeo=l(),vh=a("li"),HJ=a("strong"),Leo=o("speech_to_text_2"),Beo=o(" \u2014 "),zk=a("a"),xeo=o("Speech2Text2Processor"),keo=o(" (Speech2Text2 model)"),Reo=l(),Th=a("li"),UJ=a("strong"),Seo=o("trocr"),Peo=o(" \u2014 "),Wk=a("a"),$eo=o("TrOCRProcessor"),Ieo=o(" (TrOCR model)"),Deo=l(),Fh=a("li"),JJ=a("strong"),jeo=o("vision-text-dual-encoder"),Neo=o(" \u2014 "),Qk=a("a"),qeo=o("VisionTextDualEncoderProcessor"),Geo=o(" (VisionTextDualEncoder model)"),Oeo=l(),Ch=a("li"),YJ=a("strong"),Xeo=o("wav2vec2"),Veo=o(" \u2014 "),Hk=a("a"),zeo=o("Wav2Vec2Processor"),Weo=o(" (Wav2Vec2 model)"),Qeo=l(),f(Mh.$$.fragment),Heo=l(),KJ=a("p"),Ueo=o("Examples:"),Jeo=l(),f(iE.$$.fragment),Yeo=l(),Eh=a("div"),f(dE.$$.fragment),Keo=l(),ZJ=a("p"),Zeo=o("Register a new processor for this class."),uBe=l(),Xi=a("h2"),yh=a("a"),eY=a("span"),f(cE.$$.fragment),eoo=l(),oY=a("span"),ooo=o("AutoModel"),bBe=l(),Uo=a("div"),f(fE.$$.fragment),roo=l(),Vi=a("p"),too=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),rY=a("code"),aoo=o("from_pretrained()"),noo=o("class method or the "),tY=a("code"),soo=o("from_config()"),loo=o(`class
method.`),ioo=l(),mE=a("p"),doo=o("This class cannot be instantiated directly using "),aY=a("code"),coo=o("__init__()"),foo=o(" (throws an error)."),moo=l(),Or=a("div"),f(gE.$$.fragment),goo=l(),nY=a("p"),hoo=o("Instantiates one of the base model classes of the library from a configuration."),poo=l(),zi=a("p"),_oo=o(`Note:
Loading a model from its configuration file does `),sY=a("strong"),uoo=o("not"),boo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lY=a("code"),voo=o("from_pretrained()"),Too=o("to load the model weights."),Foo=l(),iY=a("p"),Coo=o("Examples:"),Moo=l(),f(hE.$$.fragment),Eoo=l(),De=a("div"),f(pE.$$.fragment),yoo=l(),dY=a("p"),woo=o("Instantiate one of the base model classes of the library from a pretrained model."),Aoo=l(),Na=a("p"),Loo=o("The model class to instantiate is selected based on the "),cY=a("code"),Boo=o("model_type"),xoo=o(` property of the config object (either
passed as an argument or loaded from `),fY=a("code"),koo=o("pretrained_model_name_or_path"),Roo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mY=a("code"),Soo=o("pretrained_model_name_or_path"),Poo=o(":"),$oo=l(),F=a("ul"),wh=a("li"),gY=a("strong"),Ioo=o("albert"),Doo=o(" \u2014 "),Uk=a("a"),joo=o("AlbertModel"),Noo=o(" (ALBERT model)"),qoo=l(),Ah=a("li"),hY=a("strong"),Goo=o("bart"),Ooo=o(" \u2014 "),Jk=a("a"),Xoo=o("BartModel"),Voo=o(" (BART model)"),zoo=l(),Lh=a("li"),pY=a("strong"),Woo=o("beit"),Qoo=o(" \u2014 "),Yk=a("a"),Hoo=o("BeitModel"),Uoo=o(" (BEiT model)"),Joo=l(),Bh=a("li"),_Y=a("strong"),Yoo=o("bert"),Koo=o(" \u2014 "),Kk=a("a"),Zoo=o("BertModel"),ero=o(" (BERT model)"),oro=l(),xh=a("li"),uY=a("strong"),rro=o("bert-generation"),tro=o(" \u2014 "),Zk=a("a"),aro=o("BertGenerationEncoder"),nro=o(" (Bert Generation model)"),sro=l(),kh=a("li"),bY=a("strong"),lro=o("big_bird"),iro=o(" \u2014 "),eR=a("a"),dro=o("BigBirdModel"),cro=o(" (BigBird model)"),fro=l(),Rh=a("li"),vY=a("strong"),mro=o("bigbird_pegasus"),gro=o(" \u2014 "),oR=a("a"),hro=o("BigBirdPegasusModel"),pro=o(" (BigBirdPegasus model)"),_ro=l(),Sh=a("li"),TY=a("strong"),uro=o("blenderbot"),bro=o(" \u2014 "),rR=a("a"),vro=o("BlenderbotModel"),Tro=o(" (Blenderbot model)"),Fro=l(),Ph=a("li"),FY=a("strong"),Cro=o("blenderbot-small"),Mro=o(" \u2014 "),tR=a("a"),Ero=o("BlenderbotSmallModel"),yro=o(" (BlenderbotSmall model)"),wro=l(),$h=a("li"),CY=a("strong"),Aro=o("camembert"),Lro=o(" \u2014 "),aR=a("a"),Bro=o("CamembertModel"),xro=o(" (CamemBERT model)"),kro=l(),Ih=a("li"),MY=a("strong"),Rro=o("canine"),Sro=o(" \u2014 "),nR=a("a"),Pro=o("CanineModel"),$ro=o(" (Canine model)"),Iro=l(),Dh=a("li"),EY=a("strong"),Dro=o("clip"),jro=o(" \u2014 "),sR=a("a"),Nro=o("CLIPModel"),qro=o(" (CLIP model)"),Gro=l(),jh=a("li"),yY=a("strong"),Oro=o("convbert"),Xro=o(" \u2014 "),lR=a("a"),Vro=o("ConvBertModel"),zro=o(" (ConvBERT model)"),Wro=l(),Nh=a("li"),wY=a("strong"),Qro=o("convnext"),Hro=o(" \u2014 "),iR=a("a"),Uro=o("ConvNextModel"),Jro=o(" (ConvNext model)"),Yro=l(),qh=a("li"),AY=a("strong"),Kro=o("ctrl"),Zro=o(" \u2014 "),dR=a("a"),eto=o("CTRLModel"),oto=o(" (CTRL model)"),rto=l(),Gh=a("li"),LY=a("strong"),tto=o("data2vec-audio"),ato=o(" \u2014 "),cR=a("a"),nto=o("Data2VecAudioModel"),sto=o(" (Data2VecAudio model)"),lto=l(),Oh=a("li"),BY=a("strong"),ito=o("data2vec-text"),dto=o(" \u2014 "),fR=a("a"),cto=o("Data2VecTextModel"),fto=o(" (Data2VecText model)"),mto=l(),Xh=a("li"),xY=a("strong"),gto=o("deberta"),hto=o(" \u2014 "),mR=a("a"),pto=o("DebertaModel"),_to=o(" (DeBERTa model)"),uto=l(),Vh=a("li"),kY=a("strong"),bto=o("deberta-v2"),vto=o(" \u2014 "),gR=a("a"),Tto=o("DebertaV2Model"),Fto=o(" (DeBERTa-v2 model)"),Cto=l(),zh=a("li"),RY=a("strong"),Mto=o("deit"),Eto=o(" \u2014 "),hR=a("a"),yto=o("DeiTModel"),wto=o(" (DeiT model)"),Ato=l(),Wh=a("li"),SY=a("strong"),Lto=o("detr"),Bto=o(" \u2014 "),pR=a("a"),xto=o("DetrModel"),kto=o(" (DETR model)"),Rto=l(),Qh=a("li"),PY=a("strong"),Sto=o("distilbert"),Pto=o(" \u2014 "),_R=a("a"),$to=o("DistilBertModel"),Ito=o(" (DistilBERT model)"),Dto=l(),Hh=a("li"),$Y=a("strong"),jto=o("dpr"),Nto=o(" \u2014 "),uR=a("a"),qto=o("DPRQuestionEncoder"),Gto=o(" (DPR model)"),Oto=l(),Uh=a("li"),IY=a("strong"),Xto=o("electra"),Vto=o(" \u2014 "),bR=a("a"),zto=o("ElectraModel"),Wto=o(" (ELECTRA model)"),Qto=l(),Jh=a("li"),DY=a("strong"),Hto=o("flaubert"),Uto=o(" \u2014 "),vR=a("a"),Jto=o("FlaubertModel"),Yto=o(" (FlauBERT model)"),Kto=l(),Yh=a("li"),jY=a("strong"),Zto=o("fnet"),eao=o(" \u2014 "),TR=a("a"),oao=o("FNetModel"),rao=o(" (FNet model)"),tao=l(),Kh=a("li"),NY=a("strong"),aao=o("fsmt"),nao=o(" \u2014 "),FR=a("a"),sao=o("FSMTModel"),lao=o(" (FairSeq Machine-Translation model)"),iao=l(),Ps=a("li"),qY=a("strong"),dao=o("funnel"),cao=o(" \u2014 "),CR=a("a"),fao=o("FunnelModel"),mao=o(" or "),MR=a("a"),gao=o("FunnelBaseModel"),hao=o(" (Funnel Transformer model)"),pao=l(),Zh=a("li"),GY=a("strong"),_ao=o("gpt2"),uao=o(" \u2014 "),ER=a("a"),bao=o("GPT2Model"),vao=o(" (OpenAI GPT-2 model)"),Tao=l(),ep=a("li"),OY=a("strong"),Fao=o("gpt_neo"),Cao=o(" \u2014 "),yR=a("a"),Mao=o("GPTNeoModel"),Eao=o(" (GPT Neo model)"),yao=l(),op=a("li"),XY=a("strong"),wao=o("gptj"),Aao=o(" \u2014 "),wR=a("a"),Lao=o("GPTJModel"),Bao=o(" (GPT-J model)"),xao=l(),rp=a("li"),VY=a("strong"),kao=o("hubert"),Rao=o(" \u2014 "),AR=a("a"),Sao=o("HubertModel"),Pao=o(" (Hubert model)"),$ao=l(),tp=a("li"),zY=a("strong"),Iao=o("ibert"),Dao=o(" \u2014 "),LR=a("a"),jao=o("IBertModel"),Nao=o(" (I-BERT model)"),qao=l(),ap=a("li"),WY=a("strong"),Gao=o("imagegpt"),Oao=o(" \u2014 "),BR=a("a"),Xao=o("ImageGPTModel"),Vao=o(" (ImageGPT model)"),zao=l(),np=a("li"),QY=a("strong"),Wao=o("layoutlm"),Qao=o(" \u2014 "),xR=a("a"),Hao=o("LayoutLMModel"),Uao=o(" (LayoutLM model)"),Jao=l(),sp=a("li"),HY=a("strong"),Yao=o("layoutlmv2"),Kao=o(" \u2014 "),kR=a("a"),Zao=o("LayoutLMv2Model"),eno=o(" (LayoutLMv2 model)"),ono=l(),lp=a("li"),UY=a("strong"),rno=o("led"),tno=o(" \u2014 "),RR=a("a"),ano=o("LEDModel"),nno=o(" (LED model)"),sno=l(),ip=a("li"),JY=a("strong"),lno=o("longformer"),ino=o(" \u2014 "),SR=a("a"),dno=o("LongformerModel"),cno=o(" (Longformer model)"),fno=l(),dp=a("li"),YY=a("strong"),mno=o("luke"),gno=o(" \u2014 "),PR=a("a"),hno=o("LukeModel"),pno=o(" (LUKE model)"),_no=l(),cp=a("li"),KY=a("strong"),uno=o("lxmert"),bno=o(" \u2014 "),$R=a("a"),vno=o("LxmertModel"),Tno=o(" (LXMERT model)"),Fno=l(),fp=a("li"),ZY=a("strong"),Cno=o("m2m_100"),Mno=o(" \u2014 "),IR=a("a"),Eno=o("M2M100Model"),yno=o(" (M2M100 model)"),wno=l(),mp=a("li"),eK=a("strong"),Ano=o("marian"),Lno=o(" \u2014 "),DR=a("a"),Bno=o("MarianModel"),xno=o(" (Marian model)"),kno=l(),gp=a("li"),oK=a("strong"),Rno=o("maskformer"),Sno=o(" \u2014 "),jR=a("a"),Pno=o("MaskFormerModel"),$no=o(" (MaskFormer model)"),Ino=l(),hp=a("li"),rK=a("strong"),Dno=o("mbart"),jno=o(" \u2014 "),NR=a("a"),Nno=o("MBartModel"),qno=o(" (mBART model)"),Gno=l(),pp=a("li"),tK=a("strong"),Ono=o("megatron-bert"),Xno=o(" \u2014 "),qR=a("a"),Vno=o("MegatronBertModel"),zno=o(" (MegatronBert model)"),Wno=l(),_p=a("li"),aK=a("strong"),Qno=o("mobilebert"),Hno=o(" \u2014 "),GR=a("a"),Uno=o("MobileBertModel"),Jno=o(" (MobileBERT model)"),Yno=l(),up=a("li"),nK=a("strong"),Kno=o("mpnet"),Zno=o(" \u2014 "),OR=a("a"),eso=o("MPNetModel"),oso=o(" (MPNet model)"),rso=l(),bp=a("li"),sK=a("strong"),tso=o("mt5"),aso=o(" \u2014 "),XR=a("a"),nso=o("MT5Model"),sso=o(" (mT5 model)"),lso=l(),vp=a("li"),lK=a("strong"),iso=o("nystromformer"),dso=o(" \u2014 "),VR=a("a"),cso=o("NystromformerModel"),fso=o(" (Nystromformer model)"),mso=l(),Tp=a("li"),iK=a("strong"),gso=o("openai-gpt"),hso=o(" \u2014 "),zR=a("a"),pso=o("OpenAIGPTModel"),_so=o(" (OpenAI GPT model)"),uso=l(),Fp=a("li"),dK=a("strong"),bso=o("pegasus"),vso=o(" \u2014 "),WR=a("a"),Tso=o("PegasusModel"),Fso=o(" (Pegasus model)"),Cso=l(),Cp=a("li"),cK=a("strong"),Mso=o("perceiver"),Eso=o(" \u2014 "),QR=a("a"),yso=o("PerceiverModel"),wso=o(" (Perceiver model)"),Aso=l(),Mp=a("li"),fK=a("strong"),Lso=o("plbart"),Bso=o(" \u2014 "),HR=a("a"),xso=o("PLBartModel"),kso=o(" (PLBart model)"),Rso=l(),Ep=a("li"),mK=a("strong"),Sso=o("poolformer"),Pso=o(" \u2014 "),UR=a("a"),$so=o("PoolFormerModel"),Iso=o(" (PoolFormer model)"),Dso=l(),yp=a("li"),gK=a("strong"),jso=o("prophetnet"),Nso=o(" \u2014 "),JR=a("a"),qso=o("ProphetNetModel"),Gso=o(" (ProphetNet model)"),Oso=l(),wp=a("li"),hK=a("strong"),Xso=o("qdqbert"),Vso=o(" \u2014 "),YR=a("a"),zso=o("QDQBertModel"),Wso=o(" (QDQBert model)"),Qso=l(),Ap=a("li"),pK=a("strong"),Hso=o("reformer"),Uso=o(" \u2014 "),KR=a("a"),Jso=o("ReformerModel"),Yso=o(" (Reformer model)"),Kso=l(),Lp=a("li"),_K=a("strong"),Zso=o("rembert"),elo=o(" \u2014 "),ZR=a("a"),olo=o("RemBertModel"),rlo=o(" (RemBERT model)"),tlo=l(),Bp=a("li"),uK=a("strong"),alo=o("retribert"),nlo=o(" \u2014 "),eS=a("a"),slo=o("RetriBertModel"),llo=o(" (RetriBERT model)"),ilo=l(),xp=a("li"),bK=a("strong"),dlo=o("roberta"),clo=o(" \u2014 "),oS=a("a"),flo=o("RobertaModel"),mlo=o(" (RoBERTa model)"),glo=l(),kp=a("li"),vK=a("strong"),hlo=o("roformer"),plo=o(" \u2014 "),rS=a("a"),_lo=o("RoFormerModel"),ulo=o(" (RoFormer model)"),blo=l(),Rp=a("li"),TK=a("strong"),vlo=o("segformer"),Tlo=o(" \u2014 "),tS=a("a"),Flo=o("SegformerModel"),Clo=o(" (SegFormer model)"),Mlo=l(),Sp=a("li"),FK=a("strong"),Elo=o("sew"),ylo=o(" \u2014 "),aS=a("a"),wlo=o("SEWModel"),Alo=o(" (SEW model)"),Llo=l(),Pp=a("li"),CK=a("strong"),Blo=o("sew-d"),xlo=o(" \u2014 "),nS=a("a"),klo=o("SEWDModel"),Rlo=o(" (SEW-D model)"),Slo=l(),$p=a("li"),MK=a("strong"),Plo=o("speech_to_text"),$lo=o(" \u2014 "),sS=a("a"),Ilo=o("Speech2TextModel"),Dlo=o(" (Speech2Text model)"),jlo=l(),Ip=a("li"),EK=a("strong"),Nlo=o("splinter"),qlo=o(" \u2014 "),lS=a("a"),Glo=o("SplinterModel"),Olo=o(" (Splinter model)"),Xlo=l(),Dp=a("li"),yK=a("strong"),Vlo=o("squeezebert"),zlo=o(" \u2014 "),iS=a("a"),Wlo=o("SqueezeBertModel"),Qlo=o(" (SqueezeBERT model)"),Hlo=l(),jp=a("li"),wK=a("strong"),Ulo=o("swin"),Jlo=o(" \u2014 "),dS=a("a"),Ylo=o("SwinModel"),Klo=o(" (Swin model)"),Zlo=l(),Np=a("li"),AK=a("strong"),eio=o("t5"),oio=o(" \u2014 "),cS=a("a"),rio=o("T5Model"),tio=o(" (T5 model)"),aio=l(),qp=a("li"),LK=a("strong"),nio=o("tapas"),sio=o(" \u2014 "),fS=a("a"),lio=o("TapasModel"),iio=o(" (TAPAS model)"),dio=l(),Gp=a("li"),BK=a("strong"),cio=o("transfo-xl"),fio=o(" \u2014 "),mS=a("a"),mio=o("TransfoXLModel"),gio=o(" (Transformer-XL model)"),hio=l(),Op=a("li"),xK=a("strong"),pio=o("unispeech"),_io=o(" \u2014 "),gS=a("a"),uio=o("UniSpeechModel"),bio=o(" (UniSpeech model)"),vio=l(),Xp=a("li"),kK=a("strong"),Tio=o("unispeech-sat"),Fio=o(" \u2014 "),hS=a("a"),Cio=o("UniSpeechSatModel"),Mio=o(" (UniSpeechSat model)"),Eio=l(),Vp=a("li"),RK=a("strong"),yio=o("vilt"),wio=o(" \u2014 "),pS=a("a"),Aio=o("ViltModel"),Lio=o(" (ViLT model)"),Bio=l(),zp=a("li"),SK=a("strong"),xio=o("vision-text-dual-encoder"),kio=o(" \u2014 "),_S=a("a"),Rio=o("VisionTextDualEncoderModel"),Sio=o(" (VisionTextDualEncoder model)"),Pio=l(),Wp=a("li"),PK=a("strong"),$io=o("visual_bert"),Iio=o(" \u2014 "),uS=a("a"),Dio=o("VisualBertModel"),jio=o(" (VisualBert model)"),Nio=l(),Qp=a("li"),$K=a("strong"),qio=o("vit"),Gio=o(" \u2014 "),bS=a("a"),Oio=o("ViTModel"),Xio=o(" (ViT model)"),Vio=l(),Hp=a("li"),IK=a("strong"),zio=o("vit_mae"),Wio=o(" \u2014 "),vS=a("a"),Qio=o("ViTMAEModel"),Hio=o(" (ViTMAE model)"),Uio=l(),Up=a("li"),DK=a("strong"),Jio=o("wav2vec2"),Yio=o(" \u2014 "),TS=a("a"),Kio=o("Wav2Vec2Model"),Zio=o(" (Wav2Vec2 model)"),edo=l(),Jp=a("li"),jK=a("strong"),odo=o("wavlm"),rdo=o(" \u2014 "),FS=a("a"),tdo=o("WavLMModel"),ado=o(" (WavLM model)"),ndo=l(),Yp=a("li"),NK=a("strong"),sdo=o("xglm"),ldo=o(" \u2014 "),CS=a("a"),ido=o("XGLMModel"),ddo=o(" (XGLM model)"),cdo=l(),Kp=a("li"),qK=a("strong"),fdo=o("xlm"),mdo=o(" \u2014 "),MS=a("a"),gdo=o("XLMModel"),hdo=o(" (XLM model)"),pdo=l(),Zp=a("li"),GK=a("strong"),_do=o("xlm-prophetnet"),udo=o(" \u2014 "),ES=a("a"),bdo=o("XLMProphetNetModel"),vdo=o(" (XLMProphetNet model)"),Tdo=l(),e_=a("li"),OK=a("strong"),Fdo=o("xlm-roberta"),Cdo=o(" \u2014 "),yS=a("a"),Mdo=o("XLMRobertaModel"),Edo=o(" (XLM-RoBERTa model)"),ydo=l(),o_=a("li"),XK=a("strong"),wdo=o("xlm-roberta-xl"),Ado=o(" \u2014 "),wS=a("a"),Ldo=o("XLMRobertaXLModel"),Bdo=o(" (XLM-RoBERTa-XL model)"),xdo=l(),r_=a("li"),VK=a("strong"),kdo=o("xlnet"),Rdo=o(" \u2014 "),AS=a("a"),Sdo=o("XLNetModel"),Pdo=o(" (XLNet model)"),$do=l(),t_=a("li"),zK=a("strong"),Ido=o("yoso"),Ddo=o(" \u2014 "),LS=a("a"),jdo=o("YosoModel"),Ndo=o(" (YOSO model)"),qdo=l(),a_=a("p"),Gdo=o("The model is set in evaluation mode by default using "),WK=a("code"),Odo=o("model.eval()"),Xdo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),QK=a("code"),Vdo=o("model.train()"),zdo=l(),HK=a("p"),Wdo=o("Examples:"),Qdo=l(),f(_E.$$.fragment),vBe=l(),Wi=a("h2"),n_=a("a"),UK=a("span"),f(uE.$$.fragment),Hdo=l(),JK=a("span"),Udo=o("AutoModelForPreTraining"),TBe=l(),Jo=a("div"),f(bE.$$.fragment),Jdo=l(),Qi=a("p"),Ydo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),YK=a("code"),Kdo=o("from_pretrained()"),Zdo=o("class method or the "),KK=a("code"),eco=o("from_config()"),oco=o(`class
method.`),rco=l(),vE=a("p"),tco=o("This class cannot be instantiated directly using "),ZK=a("code"),aco=o("__init__()"),nco=o(" (throws an error)."),sco=l(),Xr=a("div"),f(TE.$$.fragment),lco=l(),eZ=a("p"),ico=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),dco=l(),Hi=a("p"),cco=o(`Note:
Loading a model from its configuration file does `),oZ=a("strong"),fco=o("not"),mco=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rZ=a("code"),gco=o("from_pretrained()"),hco=o("to load the model weights."),pco=l(),tZ=a("p"),_co=o("Examples:"),uco=l(),f(FE.$$.fragment),bco=l(),je=a("div"),f(CE.$$.fragment),vco=l(),aZ=a("p"),Tco=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Fco=l(),qa=a("p"),Cco=o("The model class to instantiate is selected based on the "),nZ=a("code"),Mco=o("model_type"),Eco=o(` property of the config object (either
passed as an argument or loaded from `),sZ=a("code"),yco=o("pretrained_model_name_or_path"),wco=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lZ=a("code"),Aco=o("pretrained_model_name_or_path"),Lco=o(":"),Bco=l(),k=a("ul"),s_=a("li"),iZ=a("strong"),xco=o("albert"),kco=o(" \u2014 "),BS=a("a"),Rco=o("AlbertForPreTraining"),Sco=o(" (ALBERT model)"),Pco=l(),l_=a("li"),dZ=a("strong"),$co=o("bart"),Ico=o(" \u2014 "),xS=a("a"),Dco=o("BartForConditionalGeneration"),jco=o(" (BART model)"),Nco=l(),i_=a("li"),cZ=a("strong"),qco=o("bert"),Gco=o(" \u2014 "),kS=a("a"),Oco=o("BertForPreTraining"),Xco=o(" (BERT model)"),Vco=l(),d_=a("li"),fZ=a("strong"),zco=o("big_bird"),Wco=o(" \u2014 "),RS=a("a"),Qco=o("BigBirdForPreTraining"),Hco=o(" (BigBird model)"),Uco=l(),c_=a("li"),mZ=a("strong"),Jco=o("camembert"),Yco=o(" \u2014 "),SS=a("a"),Kco=o("CamembertForMaskedLM"),Zco=o(" (CamemBERT model)"),efo=l(),f_=a("li"),gZ=a("strong"),ofo=o("ctrl"),rfo=o(" \u2014 "),PS=a("a"),tfo=o("CTRLLMHeadModel"),afo=o(" (CTRL model)"),nfo=l(),m_=a("li"),hZ=a("strong"),sfo=o("data2vec-text"),lfo=o(" \u2014 "),$S=a("a"),ifo=o("Data2VecTextForMaskedLM"),dfo=o(" (Data2VecText model)"),cfo=l(),g_=a("li"),pZ=a("strong"),ffo=o("deberta"),mfo=o(" \u2014 "),IS=a("a"),gfo=o("DebertaForMaskedLM"),hfo=o(" (DeBERTa model)"),pfo=l(),h_=a("li"),_Z=a("strong"),_fo=o("deberta-v2"),ufo=o(" \u2014 "),DS=a("a"),bfo=o("DebertaV2ForMaskedLM"),vfo=o(" (DeBERTa-v2 model)"),Tfo=l(),p_=a("li"),uZ=a("strong"),Ffo=o("distilbert"),Cfo=o(" \u2014 "),jS=a("a"),Mfo=o("DistilBertForMaskedLM"),Efo=o(" (DistilBERT model)"),yfo=l(),__=a("li"),bZ=a("strong"),wfo=o("electra"),Afo=o(" \u2014 "),NS=a("a"),Lfo=o("ElectraForPreTraining"),Bfo=o(" (ELECTRA model)"),xfo=l(),u_=a("li"),vZ=a("strong"),kfo=o("flaubert"),Rfo=o(" \u2014 "),qS=a("a"),Sfo=o("FlaubertWithLMHeadModel"),Pfo=o(" (FlauBERT model)"),$fo=l(),b_=a("li"),TZ=a("strong"),Ifo=o("fnet"),Dfo=o(" \u2014 "),GS=a("a"),jfo=o("FNetForPreTraining"),Nfo=o(" (FNet model)"),qfo=l(),v_=a("li"),FZ=a("strong"),Gfo=o("fsmt"),Ofo=o(" \u2014 "),OS=a("a"),Xfo=o("FSMTForConditionalGeneration"),Vfo=o(" (FairSeq Machine-Translation model)"),zfo=l(),T_=a("li"),CZ=a("strong"),Wfo=o("funnel"),Qfo=o(" \u2014 "),XS=a("a"),Hfo=o("FunnelForPreTraining"),Ufo=o(" (Funnel Transformer model)"),Jfo=l(),F_=a("li"),MZ=a("strong"),Yfo=o("gpt2"),Kfo=o(" \u2014 "),VS=a("a"),Zfo=o("GPT2LMHeadModel"),emo=o(" (OpenAI GPT-2 model)"),omo=l(),C_=a("li"),EZ=a("strong"),rmo=o("ibert"),tmo=o(" \u2014 "),zS=a("a"),amo=o("IBertForMaskedLM"),nmo=o(" (I-BERT model)"),smo=l(),M_=a("li"),yZ=a("strong"),lmo=o("layoutlm"),imo=o(" \u2014 "),WS=a("a"),dmo=o("LayoutLMForMaskedLM"),cmo=o(" (LayoutLM model)"),fmo=l(),E_=a("li"),wZ=a("strong"),mmo=o("longformer"),gmo=o(" \u2014 "),QS=a("a"),hmo=o("LongformerForMaskedLM"),pmo=o(" (Longformer model)"),_mo=l(),y_=a("li"),AZ=a("strong"),umo=o("lxmert"),bmo=o(" \u2014 "),HS=a("a"),vmo=o("LxmertForPreTraining"),Tmo=o(" (LXMERT model)"),Fmo=l(),w_=a("li"),LZ=a("strong"),Cmo=o("megatron-bert"),Mmo=o(" \u2014 "),US=a("a"),Emo=o("MegatronBertForPreTraining"),ymo=o(" (MegatronBert model)"),wmo=l(),A_=a("li"),BZ=a("strong"),Amo=o("mobilebert"),Lmo=o(" \u2014 "),JS=a("a"),Bmo=o("MobileBertForPreTraining"),xmo=o(" (MobileBERT model)"),kmo=l(),L_=a("li"),xZ=a("strong"),Rmo=o("mpnet"),Smo=o(" \u2014 "),YS=a("a"),Pmo=o("MPNetForMaskedLM"),$mo=o(" (MPNet model)"),Imo=l(),B_=a("li"),kZ=a("strong"),Dmo=o("openai-gpt"),jmo=o(" \u2014 "),KS=a("a"),Nmo=o("OpenAIGPTLMHeadModel"),qmo=o(" (OpenAI GPT model)"),Gmo=l(),x_=a("li"),RZ=a("strong"),Omo=o("retribert"),Xmo=o(" \u2014 "),ZS=a("a"),Vmo=o("RetriBertModel"),zmo=o(" (RetriBERT model)"),Wmo=l(),k_=a("li"),SZ=a("strong"),Qmo=o("roberta"),Hmo=o(" \u2014 "),eP=a("a"),Umo=o("RobertaForMaskedLM"),Jmo=o(" (RoBERTa model)"),Ymo=l(),R_=a("li"),PZ=a("strong"),Kmo=o("squeezebert"),Zmo=o(" \u2014 "),oP=a("a"),ego=o("SqueezeBertForMaskedLM"),ogo=o(" (SqueezeBERT model)"),rgo=l(),S_=a("li"),$Z=a("strong"),tgo=o("t5"),ago=o(" \u2014 "),rP=a("a"),ngo=o("T5ForConditionalGeneration"),sgo=o(" (T5 model)"),lgo=l(),P_=a("li"),IZ=a("strong"),igo=o("tapas"),dgo=o(" \u2014 "),tP=a("a"),cgo=o("TapasForMaskedLM"),fgo=o(" (TAPAS model)"),mgo=l(),$_=a("li"),DZ=a("strong"),ggo=o("transfo-xl"),hgo=o(" \u2014 "),aP=a("a"),pgo=o("TransfoXLLMHeadModel"),_go=o(" (Transformer-XL model)"),ugo=l(),I_=a("li"),jZ=a("strong"),bgo=o("unispeech"),vgo=o(" \u2014 "),nP=a("a"),Tgo=o("UniSpeechForPreTraining"),Fgo=o(" (UniSpeech model)"),Cgo=l(),D_=a("li"),NZ=a("strong"),Mgo=o("unispeech-sat"),Ego=o(" \u2014 "),sP=a("a"),ygo=o("UniSpeechSatForPreTraining"),wgo=o(" (UniSpeechSat model)"),Ago=l(),j_=a("li"),qZ=a("strong"),Lgo=o("visual_bert"),Bgo=o(" \u2014 "),lP=a("a"),xgo=o("VisualBertForPreTraining"),kgo=o(" (VisualBert model)"),Rgo=l(),N_=a("li"),GZ=a("strong"),Sgo=o("vit_mae"),Pgo=o(" \u2014 "),iP=a("a"),$go=o("ViTMAEForPreTraining"),Igo=o(" (ViTMAE model)"),Dgo=l(),q_=a("li"),OZ=a("strong"),jgo=o("wav2vec2"),Ngo=o(" \u2014 "),dP=a("a"),qgo=o("Wav2Vec2ForPreTraining"),Ggo=o(" (Wav2Vec2 model)"),Ogo=l(),G_=a("li"),XZ=a("strong"),Xgo=o("xlm"),Vgo=o(" \u2014 "),cP=a("a"),zgo=o("XLMWithLMHeadModel"),Wgo=o(" (XLM model)"),Qgo=l(),O_=a("li"),VZ=a("strong"),Hgo=o("xlm-roberta"),Ugo=o(" \u2014 "),fP=a("a"),Jgo=o("XLMRobertaForMaskedLM"),Ygo=o(" (XLM-RoBERTa model)"),Kgo=l(),X_=a("li"),zZ=a("strong"),Zgo=o("xlm-roberta-xl"),eho=o(" \u2014 "),mP=a("a"),oho=o("XLMRobertaXLForMaskedLM"),rho=o(" (XLM-RoBERTa-XL model)"),tho=l(),V_=a("li"),WZ=a("strong"),aho=o("xlnet"),nho=o(" \u2014 "),gP=a("a"),sho=o("XLNetLMHeadModel"),lho=o(" (XLNet model)"),iho=l(),z_=a("p"),dho=o("The model is set in evaluation mode by default using "),QZ=a("code"),cho=o("model.eval()"),fho=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HZ=a("code"),mho=o("model.train()"),gho=l(),UZ=a("p"),hho=o("Examples:"),pho=l(),f(ME.$$.fragment),FBe=l(),Ui=a("h2"),W_=a("a"),JZ=a("span"),f(EE.$$.fragment),_ho=l(),YZ=a("span"),uho=o("AutoModelForCausalLM"),CBe=l(),Yo=a("div"),f(yE.$$.fragment),bho=l(),Ji=a("p"),vho=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),KZ=a("code"),Tho=o("from_pretrained()"),Fho=o("class method or the "),ZZ=a("code"),Cho=o("from_config()"),Mho=o(`class
method.`),Eho=l(),wE=a("p"),yho=o("This class cannot be instantiated directly using "),eee=a("code"),who=o("__init__()"),Aho=o(" (throws an error)."),Lho=l(),Vr=a("div"),f(AE.$$.fragment),Bho=l(),oee=a("p"),xho=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),kho=l(),Yi=a("p"),Rho=o(`Note:
Loading a model from its configuration file does `),ree=a("strong"),Sho=o("not"),Pho=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tee=a("code"),$ho=o("from_pretrained()"),Iho=o("to load the model weights."),Dho=l(),aee=a("p"),jho=o("Examples:"),Nho=l(),f(LE.$$.fragment),qho=l(),Ne=a("div"),f(BE.$$.fragment),Gho=l(),nee=a("p"),Oho=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Xho=l(),Ga=a("p"),Vho=o("The model class to instantiate is selected based on the "),see=a("code"),zho=o("model_type"),Who=o(` property of the config object (either
passed as an argument or loaded from `),lee=a("code"),Qho=o("pretrained_model_name_or_path"),Hho=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iee=a("code"),Uho=o("pretrained_model_name_or_path"),Jho=o(":"),Yho=l(),$=a("ul"),Q_=a("li"),dee=a("strong"),Kho=o("bart"),Zho=o(" \u2014 "),hP=a("a"),epo=o("BartForCausalLM"),opo=o(" (BART model)"),rpo=l(),H_=a("li"),cee=a("strong"),tpo=o("bert"),apo=o(" \u2014 "),pP=a("a"),npo=o("BertLMHeadModel"),spo=o(" (BERT model)"),lpo=l(),U_=a("li"),fee=a("strong"),ipo=o("bert-generation"),dpo=o(" \u2014 "),_P=a("a"),cpo=o("BertGenerationDecoder"),fpo=o(" (Bert Generation model)"),mpo=l(),J_=a("li"),mee=a("strong"),gpo=o("big_bird"),hpo=o(" \u2014 "),uP=a("a"),ppo=o("BigBirdForCausalLM"),_po=o(" (BigBird model)"),upo=l(),Y_=a("li"),gee=a("strong"),bpo=o("bigbird_pegasus"),vpo=o(" \u2014 "),bP=a("a"),Tpo=o("BigBirdPegasusForCausalLM"),Fpo=o(" (BigBirdPegasus model)"),Cpo=l(),K_=a("li"),hee=a("strong"),Mpo=o("blenderbot"),Epo=o(" \u2014 "),vP=a("a"),ypo=o("BlenderbotForCausalLM"),wpo=o(" (Blenderbot model)"),Apo=l(),Z_=a("li"),pee=a("strong"),Lpo=o("blenderbot-small"),Bpo=o(" \u2014 "),TP=a("a"),xpo=o("BlenderbotSmallForCausalLM"),kpo=o(" (BlenderbotSmall model)"),Rpo=l(),eu=a("li"),_ee=a("strong"),Spo=o("camembert"),Ppo=o(" \u2014 "),FP=a("a"),$po=o("CamembertForCausalLM"),Ipo=o(" (CamemBERT model)"),Dpo=l(),ou=a("li"),uee=a("strong"),jpo=o("ctrl"),Npo=o(" \u2014 "),CP=a("a"),qpo=o("CTRLLMHeadModel"),Gpo=o(" (CTRL model)"),Opo=l(),ru=a("li"),bee=a("strong"),Xpo=o("data2vec-text"),Vpo=o(" \u2014 "),MP=a("a"),zpo=o("Data2VecTextForCausalLM"),Wpo=o(" (Data2VecText model)"),Qpo=l(),tu=a("li"),vee=a("strong"),Hpo=o("electra"),Upo=o(" \u2014 "),EP=a("a"),Jpo=o("ElectraForCausalLM"),Ypo=o(" (ELECTRA model)"),Kpo=l(),au=a("li"),Tee=a("strong"),Zpo=o("gpt2"),e_o=o(" \u2014 "),yP=a("a"),o_o=o("GPT2LMHeadModel"),r_o=o(" (OpenAI GPT-2 model)"),t_o=l(),nu=a("li"),Fee=a("strong"),a_o=o("gpt_neo"),n_o=o(" \u2014 "),wP=a("a"),s_o=o("GPTNeoForCausalLM"),l_o=o(" (GPT Neo model)"),i_o=l(),su=a("li"),Cee=a("strong"),d_o=o("gptj"),c_o=o(" \u2014 "),AP=a("a"),f_o=o("GPTJForCausalLM"),m_o=o(" (GPT-J model)"),g_o=l(),lu=a("li"),Mee=a("strong"),h_o=o("marian"),p_o=o(" \u2014 "),LP=a("a"),__o=o("MarianForCausalLM"),u_o=o(" (Marian model)"),b_o=l(),iu=a("li"),Eee=a("strong"),v_o=o("mbart"),T_o=o(" \u2014 "),BP=a("a"),F_o=o("MBartForCausalLM"),C_o=o(" (mBART model)"),M_o=l(),du=a("li"),yee=a("strong"),E_o=o("megatron-bert"),y_o=o(" \u2014 "),xP=a("a"),w_o=o("MegatronBertForCausalLM"),A_o=o(" (MegatronBert model)"),L_o=l(),cu=a("li"),wee=a("strong"),B_o=o("openai-gpt"),x_o=o(" \u2014 "),kP=a("a"),k_o=o("OpenAIGPTLMHeadModel"),R_o=o(" (OpenAI GPT model)"),S_o=l(),fu=a("li"),Aee=a("strong"),P_o=o("pegasus"),$_o=o(" \u2014 "),RP=a("a"),I_o=o("PegasusForCausalLM"),D_o=o(" (Pegasus model)"),j_o=l(),mu=a("li"),Lee=a("strong"),N_o=o("plbart"),q_o=o(" \u2014 "),SP=a("a"),G_o=o("PLBartForCausalLM"),O_o=o(" (PLBart model)"),X_o=l(),gu=a("li"),Bee=a("strong"),V_o=o("prophetnet"),z_o=o(" \u2014 "),PP=a("a"),W_o=o("ProphetNetForCausalLM"),Q_o=o(" (ProphetNet model)"),H_o=l(),hu=a("li"),xee=a("strong"),U_o=o("qdqbert"),J_o=o(" \u2014 "),$P=a("a"),Y_o=o("QDQBertLMHeadModel"),K_o=o(" (QDQBert model)"),Z_o=l(),pu=a("li"),kee=a("strong"),euo=o("reformer"),ouo=o(" \u2014 "),IP=a("a"),ruo=o("ReformerModelWithLMHead"),tuo=o(" (Reformer model)"),auo=l(),_u=a("li"),Ree=a("strong"),nuo=o("rembert"),suo=o(" \u2014 "),DP=a("a"),luo=o("RemBertForCausalLM"),iuo=o(" (RemBERT model)"),duo=l(),uu=a("li"),See=a("strong"),cuo=o("roberta"),fuo=o(" \u2014 "),jP=a("a"),muo=o("RobertaForCausalLM"),guo=o(" (RoBERTa model)"),huo=l(),bu=a("li"),Pee=a("strong"),puo=o("roformer"),_uo=o(" \u2014 "),NP=a("a"),uuo=o("RoFormerForCausalLM"),buo=o(" (RoFormer model)"),vuo=l(),vu=a("li"),$ee=a("strong"),Tuo=o("speech_to_text_2"),Fuo=o(" \u2014 "),qP=a("a"),Cuo=o("Speech2Text2ForCausalLM"),Muo=o(" (Speech2Text2 model)"),Euo=l(),Tu=a("li"),Iee=a("strong"),yuo=o("transfo-xl"),wuo=o(" \u2014 "),GP=a("a"),Auo=o("TransfoXLLMHeadModel"),Luo=o(" (Transformer-XL model)"),Buo=l(),Fu=a("li"),Dee=a("strong"),xuo=o("trocr"),kuo=o(" \u2014 "),OP=a("a"),Ruo=o("TrOCRForCausalLM"),Suo=o(" (TrOCR model)"),Puo=l(),Cu=a("li"),jee=a("strong"),$uo=o("xglm"),Iuo=o(" \u2014 "),XP=a("a"),Duo=o("XGLMForCausalLM"),juo=o(" (XGLM model)"),Nuo=l(),Mu=a("li"),Nee=a("strong"),quo=o("xlm"),Guo=o(" \u2014 "),VP=a("a"),Ouo=o("XLMWithLMHeadModel"),Xuo=o(" (XLM model)"),Vuo=l(),Eu=a("li"),qee=a("strong"),zuo=o("xlm-prophetnet"),Wuo=o(" \u2014 "),zP=a("a"),Quo=o("XLMProphetNetForCausalLM"),Huo=o(" (XLMProphetNet model)"),Uuo=l(),yu=a("li"),Gee=a("strong"),Juo=o("xlm-roberta"),Yuo=o(" \u2014 "),WP=a("a"),Kuo=o("XLMRobertaForCausalLM"),Zuo=o(" (XLM-RoBERTa model)"),e0o=l(),wu=a("li"),Oee=a("strong"),o0o=o("xlm-roberta-xl"),r0o=o(" \u2014 "),QP=a("a"),t0o=o("XLMRobertaXLForCausalLM"),a0o=o(" (XLM-RoBERTa-XL model)"),n0o=l(),Au=a("li"),Xee=a("strong"),s0o=o("xlnet"),l0o=o(" \u2014 "),HP=a("a"),i0o=o("XLNetLMHeadModel"),d0o=o(" (XLNet model)"),c0o=l(),Lu=a("p"),f0o=o("The model is set in evaluation mode by default using "),Vee=a("code"),m0o=o("model.eval()"),g0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zee=a("code"),h0o=o("model.train()"),p0o=l(),Wee=a("p"),_0o=o("Examples:"),u0o=l(),f(xE.$$.fragment),MBe=l(),Ki=a("h2"),Bu=a("a"),Qee=a("span"),f(kE.$$.fragment),b0o=l(),Hee=a("span"),v0o=o("AutoModelForMaskedLM"),EBe=l(),Ko=a("div"),f(RE.$$.fragment),T0o=l(),Zi=a("p"),F0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Uee=a("code"),C0o=o("from_pretrained()"),M0o=o("class method or the "),Jee=a("code"),E0o=o("from_config()"),y0o=o(`class
method.`),w0o=l(),SE=a("p"),A0o=o("This class cannot be instantiated directly using "),Yee=a("code"),L0o=o("__init__()"),B0o=o(" (throws an error)."),x0o=l(),zr=a("div"),f(PE.$$.fragment),k0o=l(),Kee=a("p"),R0o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),S0o=l(),ed=a("p"),P0o=o(`Note:
Loading a model from its configuration file does `),Zee=a("strong"),$0o=o("not"),I0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),eoe=a("code"),D0o=o("from_pretrained()"),j0o=o("to load the model weights."),N0o=l(),ooe=a("p"),q0o=o("Examples:"),G0o=l(),f($E.$$.fragment),O0o=l(),qe=a("div"),f(IE.$$.fragment),X0o=l(),roe=a("p"),V0o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),z0o=l(),Oa=a("p"),W0o=o("The model class to instantiate is selected based on the "),toe=a("code"),Q0o=o("model_type"),H0o=o(` property of the config object (either
passed as an argument or loaded from `),aoe=a("code"),U0o=o("pretrained_model_name_or_path"),J0o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),noe=a("code"),Y0o=o("pretrained_model_name_or_path"),K0o=o(":"),Z0o=l(),I=a("ul"),xu=a("li"),soe=a("strong"),e1o=o("albert"),o1o=o(" \u2014 "),UP=a("a"),r1o=o("AlbertForMaskedLM"),t1o=o(" (ALBERT model)"),a1o=l(),ku=a("li"),loe=a("strong"),n1o=o("bart"),s1o=o(" \u2014 "),JP=a("a"),l1o=o("BartForConditionalGeneration"),i1o=o(" (BART model)"),d1o=l(),Ru=a("li"),ioe=a("strong"),c1o=o("bert"),f1o=o(" \u2014 "),YP=a("a"),m1o=o("BertForMaskedLM"),g1o=o(" (BERT model)"),h1o=l(),Su=a("li"),doe=a("strong"),p1o=o("big_bird"),_1o=o(" \u2014 "),KP=a("a"),u1o=o("BigBirdForMaskedLM"),b1o=o(" (BigBird model)"),v1o=l(),Pu=a("li"),coe=a("strong"),T1o=o("camembert"),F1o=o(" \u2014 "),ZP=a("a"),C1o=o("CamembertForMaskedLM"),M1o=o(" (CamemBERT model)"),E1o=l(),$u=a("li"),foe=a("strong"),y1o=o("convbert"),w1o=o(" \u2014 "),e$=a("a"),A1o=o("ConvBertForMaskedLM"),L1o=o(" (ConvBERT model)"),B1o=l(),Iu=a("li"),moe=a("strong"),x1o=o("data2vec-text"),k1o=o(" \u2014 "),o$=a("a"),R1o=o("Data2VecTextForMaskedLM"),S1o=o(" (Data2VecText model)"),P1o=l(),Du=a("li"),goe=a("strong"),$1o=o("deberta"),I1o=o(" \u2014 "),r$=a("a"),D1o=o("DebertaForMaskedLM"),j1o=o(" (DeBERTa model)"),N1o=l(),ju=a("li"),hoe=a("strong"),q1o=o("deberta-v2"),G1o=o(" \u2014 "),t$=a("a"),O1o=o("DebertaV2ForMaskedLM"),X1o=o(" (DeBERTa-v2 model)"),V1o=l(),Nu=a("li"),poe=a("strong"),z1o=o("distilbert"),W1o=o(" \u2014 "),a$=a("a"),Q1o=o("DistilBertForMaskedLM"),H1o=o(" (DistilBERT model)"),U1o=l(),qu=a("li"),_oe=a("strong"),J1o=o("electra"),Y1o=o(" \u2014 "),n$=a("a"),K1o=o("ElectraForMaskedLM"),Z1o=o(" (ELECTRA model)"),ebo=l(),Gu=a("li"),uoe=a("strong"),obo=o("flaubert"),rbo=o(" \u2014 "),s$=a("a"),tbo=o("FlaubertWithLMHeadModel"),abo=o(" (FlauBERT model)"),nbo=l(),Ou=a("li"),boe=a("strong"),sbo=o("fnet"),lbo=o(" \u2014 "),l$=a("a"),ibo=o("FNetForMaskedLM"),dbo=o(" (FNet model)"),cbo=l(),Xu=a("li"),voe=a("strong"),fbo=o("funnel"),mbo=o(" \u2014 "),i$=a("a"),gbo=o("FunnelForMaskedLM"),hbo=o(" (Funnel Transformer model)"),pbo=l(),Vu=a("li"),Toe=a("strong"),_bo=o("ibert"),ubo=o(" \u2014 "),d$=a("a"),bbo=o("IBertForMaskedLM"),vbo=o(" (I-BERT model)"),Tbo=l(),zu=a("li"),Foe=a("strong"),Fbo=o("layoutlm"),Cbo=o(" \u2014 "),c$=a("a"),Mbo=o("LayoutLMForMaskedLM"),Ebo=o(" (LayoutLM model)"),ybo=l(),Wu=a("li"),Coe=a("strong"),wbo=o("longformer"),Abo=o(" \u2014 "),f$=a("a"),Lbo=o("LongformerForMaskedLM"),Bbo=o(" (Longformer model)"),xbo=l(),Qu=a("li"),Moe=a("strong"),kbo=o("mbart"),Rbo=o(" \u2014 "),m$=a("a"),Sbo=o("MBartForConditionalGeneration"),Pbo=o(" (mBART model)"),$bo=l(),Hu=a("li"),Eoe=a("strong"),Ibo=o("megatron-bert"),Dbo=o(" \u2014 "),g$=a("a"),jbo=o("MegatronBertForMaskedLM"),Nbo=o(" (MegatronBert model)"),qbo=l(),Uu=a("li"),yoe=a("strong"),Gbo=o("mobilebert"),Obo=o(" \u2014 "),h$=a("a"),Xbo=o("MobileBertForMaskedLM"),Vbo=o(" (MobileBERT model)"),zbo=l(),Ju=a("li"),woe=a("strong"),Wbo=o("mpnet"),Qbo=o(" \u2014 "),p$=a("a"),Hbo=o("MPNetForMaskedLM"),Ubo=o(" (MPNet model)"),Jbo=l(),Yu=a("li"),Aoe=a("strong"),Ybo=o("nystromformer"),Kbo=o(" \u2014 "),_$=a("a"),Zbo=o("NystromformerForMaskedLM"),e5o=o(" (Nystromformer model)"),o5o=l(),Ku=a("li"),Loe=a("strong"),r5o=o("perceiver"),t5o=o(" \u2014 "),u$=a("a"),a5o=o("PerceiverForMaskedLM"),n5o=o(" (Perceiver model)"),s5o=l(),Zu=a("li"),Boe=a("strong"),l5o=o("qdqbert"),i5o=o(" \u2014 "),b$=a("a"),d5o=o("QDQBertForMaskedLM"),c5o=o(" (QDQBert model)"),f5o=l(),e0=a("li"),xoe=a("strong"),m5o=o("reformer"),g5o=o(" \u2014 "),v$=a("a"),h5o=o("ReformerForMaskedLM"),p5o=o(" (Reformer model)"),_5o=l(),o0=a("li"),koe=a("strong"),u5o=o("rembert"),b5o=o(" \u2014 "),T$=a("a"),v5o=o("RemBertForMaskedLM"),T5o=o(" (RemBERT model)"),F5o=l(),r0=a("li"),Roe=a("strong"),C5o=o("roberta"),M5o=o(" \u2014 "),F$=a("a"),E5o=o("RobertaForMaskedLM"),y5o=o(" (RoBERTa model)"),w5o=l(),t0=a("li"),Soe=a("strong"),A5o=o("roformer"),L5o=o(" \u2014 "),C$=a("a"),B5o=o("RoFormerForMaskedLM"),x5o=o(" (RoFormer model)"),k5o=l(),a0=a("li"),Poe=a("strong"),R5o=o("squeezebert"),S5o=o(" \u2014 "),M$=a("a"),P5o=o("SqueezeBertForMaskedLM"),$5o=o(" (SqueezeBERT model)"),I5o=l(),n0=a("li"),$oe=a("strong"),D5o=o("tapas"),j5o=o(" \u2014 "),E$=a("a"),N5o=o("TapasForMaskedLM"),q5o=o(" (TAPAS model)"),G5o=l(),s0=a("li"),Ioe=a("strong"),O5o=o("wav2vec2"),X5o=o(" \u2014 "),Doe=a("code"),V5o=o("Wav2Vec2ForMaskedLM"),z5o=o("(Wav2Vec2 model)"),W5o=l(),l0=a("li"),joe=a("strong"),Q5o=o("xlm"),H5o=o(" \u2014 "),y$=a("a"),U5o=o("XLMWithLMHeadModel"),J5o=o(" (XLM model)"),Y5o=l(),i0=a("li"),Noe=a("strong"),K5o=o("xlm-roberta"),Z5o=o(" \u2014 "),w$=a("a"),e2o=o("XLMRobertaForMaskedLM"),o2o=o(" (XLM-RoBERTa model)"),r2o=l(),d0=a("li"),qoe=a("strong"),t2o=o("xlm-roberta-xl"),a2o=o(" \u2014 "),A$=a("a"),n2o=o("XLMRobertaXLForMaskedLM"),s2o=o(" (XLM-RoBERTa-XL model)"),l2o=l(),c0=a("li"),Goe=a("strong"),i2o=o("yoso"),d2o=o(" \u2014 "),L$=a("a"),c2o=o("YosoForMaskedLM"),f2o=o(" (YOSO model)"),m2o=l(),f0=a("p"),g2o=o("The model is set in evaluation mode by default using "),Ooe=a("code"),h2o=o("model.eval()"),p2o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xoe=a("code"),_2o=o("model.train()"),u2o=l(),Voe=a("p"),b2o=o("Examples:"),v2o=l(),f(DE.$$.fragment),yBe=l(),od=a("h2"),m0=a("a"),zoe=a("span"),f(jE.$$.fragment),T2o=l(),Woe=a("span"),F2o=o("AutoModelForSeq2SeqLM"),wBe=l(),Zo=a("div"),f(NE.$$.fragment),C2o=l(),rd=a("p"),M2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Qoe=a("code"),E2o=o("from_pretrained()"),y2o=o("class method or the "),Hoe=a("code"),w2o=o("from_config()"),A2o=o(`class
method.`),L2o=l(),qE=a("p"),B2o=o("This class cannot be instantiated directly using "),Uoe=a("code"),x2o=o("__init__()"),k2o=o(" (throws an error)."),R2o=l(),Wr=a("div"),f(GE.$$.fragment),S2o=l(),Joe=a("p"),P2o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$2o=l(),td=a("p"),I2o=o(`Note:
Loading a model from its configuration file does `),Yoe=a("strong"),D2o=o("not"),j2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Koe=a("code"),N2o=o("from_pretrained()"),q2o=o("to load the model weights."),G2o=l(),Zoe=a("p"),O2o=o("Examples:"),X2o=l(),f(OE.$$.fragment),V2o=l(),Ge=a("div"),f(XE.$$.fragment),z2o=l(),ere=a("p"),W2o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Q2o=l(),Xa=a("p"),H2o=o("The model class to instantiate is selected based on the "),ore=a("code"),U2o=o("model_type"),J2o=o(` property of the config object (either
passed as an argument or loaded from `),rre=a("code"),Y2o=o("pretrained_model_name_or_path"),K2o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tre=a("code"),Z2o=o("pretrained_model_name_or_path"),evo=o(":"),ovo=l(),ae=a("ul"),g0=a("li"),are=a("strong"),rvo=o("bart"),tvo=o(" \u2014 "),B$=a("a"),avo=o("BartForConditionalGeneration"),nvo=o(" (BART model)"),svo=l(),h0=a("li"),nre=a("strong"),lvo=o("bigbird_pegasus"),ivo=o(" \u2014 "),x$=a("a"),dvo=o("BigBirdPegasusForConditionalGeneration"),cvo=o(" (BigBirdPegasus model)"),fvo=l(),p0=a("li"),sre=a("strong"),mvo=o("blenderbot"),gvo=o(" \u2014 "),k$=a("a"),hvo=o("BlenderbotForConditionalGeneration"),pvo=o(" (Blenderbot model)"),_vo=l(),_0=a("li"),lre=a("strong"),uvo=o("blenderbot-small"),bvo=o(" \u2014 "),R$=a("a"),vvo=o("BlenderbotSmallForConditionalGeneration"),Tvo=o(" (BlenderbotSmall model)"),Fvo=l(),u0=a("li"),ire=a("strong"),Cvo=o("encoder-decoder"),Mvo=o(" \u2014 "),S$=a("a"),Evo=o("EncoderDecoderModel"),yvo=o(" (Encoder decoder model)"),wvo=l(),b0=a("li"),dre=a("strong"),Avo=o("fsmt"),Lvo=o(" \u2014 "),P$=a("a"),Bvo=o("FSMTForConditionalGeneration"),xvo=o(" (FairSeq Machine-Translation model)"),kvo=l(),v0=a("li"),cre=a("strong"),Rvo=o("led"),Svo=o(" \u2014 "),$$=a("a"),Pvo=o("LEDForConditionalGeneration"),$vo=o(" (LED model)"),Ivo=l(),T0=a("li"),fre=a("strong"),Dvo=o("m2m_100"),jvo=o(" \u2014 "),I$=a("a"),Nvo=o("M2M100ForConditionalGeneration"),qvo=o(" (M2M100 model)"),Gvo=l(),F0=a("li"),mre=a("strong"),Ovo=o("marian"),Xvo=o(" \u2014 "),D$=a("a"),Vvo=o("MarianMTModel"),zvo=o(" (Marian model)"),Wvo=l(),C0=a("li"),gre=a("strong"),Qvo=o("mbart"),Hvo=o(" \u2014 "),j$=a("a"),Uvo=o("MBartForConditionalGeneration"),Jvo=o(" (mBART model)"),Yvo=l(),M0=a("li"),hre=a("strong"),Kvo=o("mt5"),Zvo=o(" \u2014 "),N$=a("a"),eTo=o("MT5ForConditionalGeneration"),oTo=o(" (mT5 model)"),rTo=l(),E0=a("li"),pre=a("strong"),tTo=o("pegasus"),aTo=o(" \u2014 "),q$=a("a"),nTo=o("PegasusForConditionalGeneration"),sTo=o(" (Pegasus model)"),lTo=l(),y0=a("li"),_re=a("strong"),iTo=o("plbart"),dTo=o(" \u2014 "),G$=a("a"),cTo=o("PLBartForConditionalGeneration"),fTo=o(" (PLBart model)"),mTo=l(),w0=a("li"),ure=a("strong"),gTo=o("prophetnet"),hTo=o(" \u2014 "),O$=a("a"),pTo=o("ProphetNetForConditionalGeneration"),_To=o(" (ProphetNet model)"),uTo=l(),A0=a("li"),bre=a("strong"),bTo=o("t5"),vTo=o(" \u2014 "),X$=a("a"),TTo=o("T5ForConditionalGeneration"),FTo=o(" (T5 model)"),CTo=l(),L0=a("li"),vre=a("strong"),MTo=o("xlm-prophetnet"),ETo=o(" \u2014 "),V$=a("a"),yTo=o("XLMProphetNetForConditionalGeneration"),wTo=o(" (XLMProphetNet model)"),ATo=l(),B0=a("p"),LTo=o("The model is set in evaluation mode by default using "),Tre=a("code"),BTo=o("model.eval()"),xTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fre=a("code"),kTo=o("model.train()"),RTo=l(),Cre=a("p"),STo=o("Examples:"),PTo=l(),f(VE.$$.fragment),ABe=l(),ad=a("h2"),x0=a("a"),Mre=a("span"),f(zE.$$.fragment),$To=l(),Ere=a("span"),ITo=o("AutoModelForSequenceClassification"),LBe=l(),er=a("div"),f(WE.$$.fragment),DTo=l(),nd=a("p"),jTo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),yre=a("code"),NTo=o("from_pretrained()"),qTo=o("class method or the "),wre=a("code"),GTo=o("from_config()"),OTo=o(`class
method.`),XTo=l(),QE=a("p"),VTo=o("This class cannot be instantiated directly using "),Are=a("code"),zTo=o("__init__()"),WTo=o(" (throws an error)."),QTo=l(),Qr=a("div"),f(HE.$$.fragment),HTo=l(),Lre=a("p"),UTo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),JTo=l(),sd=a("p"),YTo=o(`Note:
Loading a model from its configuration file does `),Bre=a("strong"),KTo=o("not"),ZTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xre=a("code"),eFo=o("from_pretrained()"),oFo=o("to load the model weights."),rFo=l(),kre=a("p"),tFo=o("Examples:"),aFo=l(),f(UE.$$.fragment),nFo=l(),Oe=a("div"),f(JE.$$.fragment),sFo=l(),Rre=a("p"),lFo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),iFo=l(),Va=a("p"),dFo=o("The model class to instantiate is selected based on the "),Sre=a("code"),cFo=o("model_type"),fFo=o(` property of the config object (either
passed as an argument or loaded from `),Pre=a("code"),mFo=o("pretrained_model_name_or_path"),gFo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$re=a("code"),hFo=o("pretrained_model_name_or_path"),pFo=o(":"),_Fo=l(),A=a("ul"),k0=a("li"),Ire=a("strong"),uFo=o("albert"),bFo=o(" \u2014 "),z$=a("a"),vFo=o("AlbertForSequenceClassification"),TFo=o(" (ALBERT model)"),FFo=l(),R0=a("li"),Dre=a("strong"),CFo=o("bart"),MFo=o(" \u2014 "),W$=a("a"),EFo=o("BartForSequenceClassification"),yFo=o(" (BART model)"),wFo=l(),S0=a("li"),jre=a("strong"),AFo=o("bert"),LFo=o(" \u2014 "),Q$=a("a"),BFo=o("BertForSequenceClassification"),xFo=o(" (BERT model)"),kFo=l(),P0=a("li"),Nre=a("strong"),RFo=o("big_bird"),SFo=o(" \u2014 "),H$=a("a"),PFo=o("BigBirdForSequenceClassification"),$Fo=o(" (BigBird model)"),IFo=l(),$0=a("li"),qre=a("strong"),DFo=o("bigbird_pegasus"),jFo=o(" \u2014 "),U$=a("a"),NFo=o("BigBirdPegasusForSequenceClassification"),qFo=o(" (BigBirdPegasus model)"),GFo=l(),I0=a("li"),Gre=a("strong"),OFo=o("camembert"),XFo=o(" \u2014 "),J$=a("a"),VFo=o("CamembertForSequenceClassification"),zFo=o(" (CamemBERT model)"),WFo=l(),D0=a("li"),Ore=a("strong"),QFo=o("canine"),HFo=o(" \u2014 "),Y$=a("a"),UFo=o("CanineForSequenceClassification"),JFo=o(" (Canine model)"),YFo=l(),j0=a("li"),Xre=a("strong"),KFo=o("convbert"),ZFo=o(" \u2014 "),K$=a("a"),e9o=o("ConvBertForSequenceClassification"),o9o=o(" (ConvBERT model)"),r9o=l(),N0=a("li"),Vre=a("strong"),t9o=o("ctrl"),a9o=o(" \u2014 "),Z$=a("a"),n9o=o("CTRLForSequenceClassification"),s9o=o(" (CTRL model)"),l9o=l(),q0=a("li"),zre=a("strong"),i9o=o("data2vec-text"),d9o=o(" \u2014 "),eI=a("a"),c9o=o("Data2VecTextForSequenceClassification"),f9o=o(" (Data2VecText model)"),m9o=l(),G0=a("li"),Wre=a("strong"),g9o=o("deberta"),h9o=o(" \u2014 "),oI=a("a"),p9o=o("DebertaForSequenceClassification"),_9o=o(" (DeBERTa model)"),u9o=l(),O0=a("li"),Qre=a("strong"),b9o=o("deberta-v2"),v9o=o(" \u2014 "),rI=a("a"),T9o=o("DebertaV2ForSequenceClassification"),F9o=o(" (DeBERTa-v2 model)"),C9o=l(),X0=a("li"),Hre=a("strong"),M9o=o("distilbert"),E9o=o(" \u2014 "),tI=a("a"),y9o=o("DistilBertForSequenceClassification"),w9o=o(" (DistilBERT model)"),A9o=l(),V0=a("li"),Ure=a("strong"),L9o=o("electra"),B9o=o(" \u2014 "),aI=a("a"),x9o=o("ElectraForSequenceClassification"),k9o=o(" (ELECTRA model)"),R9o=l(),z0=a("li"),Jre=a("strong"),S9o=o("flaubert"),P9o=o(" \u2014 "),nI=a("a"),$9o=o("FlaubertForSequenceClassification"),I9o=o(" (FlauBERT model)"),D9o=l(),W0=a("li"),Yre=a("strong"),j9o=o("fnet"),N9o=o(" \u2014 "),sI=a("a"),q9o=o("FNetForSequenceClassification"),G9o=o(" (FNet model)"),O9o=l(),Q0=a("li"),Kre=a("strong"),X9o=o("funnel"),V9o=o(" \u2014 "),lI=a("a"),z9o=o("FunnelForSequenceClassification"),W9o=o(" (Funnel Transformer model)"),Q9o=l(),H0=a("li"),Zre=a("strong"),H9o=o("gpt2"),U9o=o(" \u2014 "),iI=a("a"),J9o=o("GPT2ForSequenceClassification"),Y9o=o(" (OpenAI GPT-2 model)"),K9o=l(),U0=a("li"),ete=a("strong"),Z9o=o("gpt_neo"),eCo=o(" \u2014 "),dI=a("a"),oCo=o("GPTNeoForSequenceClassification"),rCo=o(" (GPT Neo model)"),tCo=l(),J0=a("li"),ote=a("strong"),aCo=o("gptj"),nCo=o(" \u2014 "),cI=a("a"),sCo=o("GPTJForSequenceClassification"),lCo=o(" (GPT-J model)"),iCo=l(),Y0=a("li"),rte=a("strong"),dCo=o("ibert"),cCo=o(" \u2014 "),fI=a("a"),fCo=o("IBertForSequenceClassification"),mCo=o(" (I-BERT model)"),gCo=l(),K0=a("li"),tte=a("strong"),hCo=o("layoutlm"),pCo=o(" \u2014 "),mI=a("a"),_Co=o("LayoutLMForSequenceClassification"),uCo=o(" (LayoutLM model)"),bCo=l(),Z0=a("li"),ate=a("strong"),vCo=o("layoutlmv2"),TCo=o(" \u2014 "),gI=a("a"),FCo=o("LayoutLMv2ForSequenceClassification"),CCo=o(" (LayoutLMv2 model)"),MCo=l(),e1=a("li"),nte=a("strong"),ECo=o("led"),yCo=o(" \u2014 "),hI=a("a"),wCo=o("LEDForSequenceClassification"),ACo=o(" (LED model)"),LCo=l(),o1=a("li"),ste=a("strong"),BCo=o("longformer"),xCo=o(" \u2014 "),pI=a("a"),kCo=o("LongformerForSequenceClassification"),RCo=o(" (Longformer model)"),SCo=l(),r1=a("li"),lte=a("strong"),PCo=o("mbart"),$Co=o(" \u2014 "),_I=a("a"),ICo=o("MBartForSequenceClassification"),DCo=o(" (mBART model)"),jCo=l(),t1=a("li"),ite=a("strong"),NCo=o("megatron-bert"),qCo=o(" \u2014 "),uI=a("a"),GCo=o("MegatronBertForSequenceClassification"),OCo=o(" (MegatronBert model)"),XCo=l(),a1=a("li"),dte=a("strong"),VCo=o("mobilebert"),zCo=o(" \u2014 "),bI=a("a"),WCo=o("MobileBertForSequenceClassification"),QCo=o(" (MobileBERT model)"),HCo=l(),n1=a("li"),cte=a("strong"),UCo=o("mpnet"),JCo=o(" \u2014 "),vI=a("a"),YCo=o("MPNetForSequenceClassification"),KCo=o(" (MPNet model)"),ZCo=l(),s1=a("li"),fte=a("strong"),eMo=o("nystromformer"),oMo=o(" \u2014 "),TI=a("a"),rMo=o("NystromformerForSequenceClassification"),tMo=o(" (Nystromformer model)"),aMo=l(),l1=a("li"),mte=a("strong"),nMo=o("openai-gpt"),sMo=o(" \u2014 "),FI=a("a"),lMo=o("OpenAIGPTForSequenceClassification"),iMo=o(" (OpenAI GPT model)"),dMo=l(),i1=a("li"),gte=a("strong"),cMo=o("perceiver"),fMo=o(" \u2014 "),CI=a("a"),mMo=o("PerceiverForSequenceClassification"),gMo=o(" (Perceiver model)"),hMo=l(),d1=a("li"),hte=a("strong"),pMo=o("plbart"),_Mo=o(" \u2014 "),MI=a("a"),uMo=o("PLBartForSequenceClassification"),bMo=o(" (PLBart model)"),vMo=l(),c1=a("li"),pte=a("strong"),TMo=o("qdqbert"),FMo=o(" \u2014 "),EI=a("a"),CMo=o("QDQBertForSequenceClassification"),MMo=o(" (QDQBert model)"),EMo=l(),f1=a("li"),_te=a("strong"),yMo=o("reformer"),wMo=o(" \u2014 "),yI=a("a"),AMo=o("ReformerForSequenceClassification"),LMo=o(" (Reformer model)"),BMo=l(),m1=a("li"),ute=a("strong"),xMo=o("rembert"),kMo=o(" \u2014 "),wI=a("a"),RMo=o("RemBertForSequenceClassification"),SMo=o(" (RemBERT model)"),PMo=l(),g1=a("li"),bte=a("strong"),$Mo=o("roberta"),IMo=o(" \u2014 "),AI=a("a"),DMo=o("RobertaForSequenceClassification"),jMo=o(" (RoBERTa model)"),NMo=l(),h1=a("li"),vte=a("strong"),qMo=o("roformer"),GMo=o(" \u2014 "),LI=a("a"),OMo=o("RoFormerForSequenceClassification"),XMo=o(" (RoFormer model)"),VMo=l(),p1=a("li"),Tte=a("strong"),zMo=o("squeezebert"),WMo=o(" \u2014 "),BI=a("a"),QMo=o("SqueezeBertForSequenceClassification"),HMo=o(" (SqueezeBERT model)"),UMo=l(),_1=a("li"),Fte=a("strong"),JMo=o("tapas"),YMo=o(" \u2014 "),xI=a("a"),KMo=o("TapasForSequenceClassification"),ZMo=o(" (TAPAS model)"),e4o=l(),u1=a("li"),Cte=a("strong"),o4o=o("transfo-xl"),r4o=o(" \u2014 "),kI=a("a"),t4o=o("TransfoXLForSequenceClassification"),a4o=o(" (Transformer-XL model)"),n4o=l(),b1=a("li"),Mte=a("strong"),s4o=o("xlm"),l4o=o(" \u2014 "),RI=a("a"),i4o=o("XLMForSequenceClassification"),d4o=o(" (XLM model)"),c4o=l(),v1=a("li"),Ete=a("strong"),f4o=o("xlm-roberta"),m4o=o(" \u2014 "),SI=a("a"),g4o=o("XLMRobertaForSequenceClassification"),h4o=o(" (XLM-RoBERTa model)"),p4o=l(),T1=a("li"),yte=a("strong"),_4o=o("xlm-roberta-xl"),u4o=o(" \u2014 "),PI=a("a"),b4o=o("XLMRobertaXLForSequenceClassification"),v4o=o(" (XLM-RoBERTa-XL model)"),T4o=l(),F1=a("li"),wte=a("strong"),F4o=o("xlnet"),C4o=o(" \u2014 "),$I=a("a"),M4o=o("XLNetForSequenceClassification"),E4o=o(" (XLNet model)"),y4o=l(),C1=a("li"),Ate=a("strong"),w4o=o("yoso"),A4o=o(" \u2014 "),II=a("a"),L4o=o("YosoForSequenceClassification"),B4o=o(" (YOSO model)"),x4o=l(),M1=a("p"),k4o=o("The model is set in evaluation mode by default using "),Lte=a("code"),R4o=o("model.eval()"),S4o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bte=a("code"),P4o=o("model.train()"),$4o=l(),xte=a("p"),I4o=o("Examples:"),D4o=l(),f(YE.$$.fragment),BBe=l(),ld=a("h2"),E1=a("a"),kte=a("span"),f(KE.$$.fragment),j4o=l(),Rte=a("span"),N4o=o("AutoModelForMultipleChoice"),xBe=l(),or=a("div"),f(ZE.$$.fragment),q4o=l(),id=a("p"),G4o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Ste=a("code"),O4o=o("from_pretrained()"),X4o=o("class method or the "),Pte=a("code"),V4o=o("from_config()"),z4o=o(`class
method.`),W4o=l(),e3=a("p"),Q4o=o("This class cannot be instantiated directly using "),$te=a("code"),H4o=o("__init__()"),U4o=o(" (throws an error)."),J4o=l(),Hr=a("div"),f(o3.$$.fragment),Y4o=l(),Ite=a("p"),K4o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Z4o=l(),dd=a("p"),eEo=o(`Note:
Loading a model from its configuration file does `),Dte=a("strong"),oEo=o("not"),rEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jte=a("code"),tEo=o("from_pretrained()"),aEo=o("to load the model weights."),nEo=l(),Nte=a("p"),sEo=o("Examples:"),lEo=l(),f(r3.$$.fragment),iEo=l(),Xe=a("div"),f(t3.$$.fragment),dEo=l(),qte=a("p"),cEo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),fEo=l(),za=a("p"),mEo=o("The model class to instantiate is selected based on the "),Gte=a("code"),gEo=o("model_type"),hEo=o(` property of the config object (either
passed as an argument or loaded from `),Ote=a("code"),pEo=o("pretrained_model_name_or_path"),_Eo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xte=a("code"),uEo=o("pretrained_model_name_or_path"),bEo=o(":"),vEo=l(),G=a("ul"),y1=a("li"),Vte=a("strong"),TEo=o("albert"),FEo=o(" \u2014 "),DI=a("a"),CEo=o("AlbertForMultipleChoice"),MEo=o(" (ALBERT model)"),EEo=l(),w1=a("li"),zte=a("strong"),yEo=o("bert"),wEo=o(" \u2014 "),jI=a("a"),AEo=o("BertForMultipleChoice"),LEo=o(" (BERT model)"),BEo=l(),A1=a("li"),Wte=a("strong"),xEo=o("big_bird"),kEo=o(" \u2014 "),NI=a("a"),REo=o("BigBirdForMultipleChoice"),SEo=o(" (BigBird model)"),PEo=l(),L1=a("li"),Qte=a("strong"),$Eo=o("camembert"),IEo=o(" \u2014 "),qI=a("a"),DEo=o("CamembertForMultipleChoice"),jEo=o(" (CamemBERT model)"),NEo=l(),B1=a("li"),Hte=a("strong"),qEo=o("canine"),GEo=o(" \u2014 "),GI=a("a"),OEo=o("CanineForMultipleChoice"),XEo=o(" (Canine model)"),VEo=l(),x1=a("li"),Ute=a("strong"),zEo=o("convbert"),WEo=o(" \u2014 "),OI=a("a"),QEo=o("ConvBertForMultipleChoice"),HEo=o(" (ConvBERT model)"),UEo=l(),k1=a("li"),Jte=a("strong"),JEo=o("data2vec-text"),YEo=o(" \u2014 "),XI=a("a"),KEo=o("Data2VecTextForMultipleChoice"),ZEo=o(" (Data2VecText model)"),e3o=l(),R1=a("li"),Yte=a("strong"),o3o=o("distilbert"),r3o=o(" \u2014 "),VI=a("a"),t3o=o("DistilBertForMultipleChoice"),a3o=o(" (DistilBERT model)"),n3o=l(),S1=a("li"),Kte=a("strong"),s3o=o("electra"),l3o=o(" \u2014 "),zI=a("a"),i3o=o("ElectraForMultipleChoice"),d3o=o(" (ELECTRA model)"),c3o=l(),P1=a("li"),Zte=a("strong"),f3o=o("flaubert"),m3o=o(" \u2014 "),WI=a("a"),g3o=o("FlaubertForMultipleChoice"),h3o=o(" (FlauBERT model)"),p3o=l(),$1=a("li"),eae=a("strong"),_3o=o("fnet"),u3o=o(" \u2014 "),QI=a("a"),b3o=o("FNetForMultipleChoice"),v3o=o(" (FNet model)"),T3o=l(),I1=a("li"),oae=a("strong"),F3o=o("funnel"),C3o=o(" \u2014 "),HI=a("a"),M3o=o("FunnelForMultipleChoice"),E3o=o(" (Funnel Transformer model)"),y3o=l(),D1=a("li"),rae=a("strong"),w3o=o("ibert"),A3o=o(" \u2014 "),UI=a("a"),L3o=o("IBertForMultipleChoice"),B3o=o(" (I-BERT model)"),x3o=l(),j1=a("li"),tae=a("strong"),k3o=o("longformer"),R3o=o(" \u2014 "),JI=a("a"),S3o=o("LongformerForMultipleChoice"),P3o=o(" (Longformer model)"),$3o=l(),N1=a("li"),aae=a("strong"),I3o=o("megatron-bert"),D3o=o(" \u2014 "),YI=a("a"),j3o=o("MegatronBertForMultipleChoice"),N3o=o(" (MegatronBert model)"),q3o=l(),q1=a("li"),nae=a("strong"),G3o=o("mobilebert"),O3o=o(" \u2014 "),KI=a("a"),X3o=o("MobileBertForMultipleChoice"),V3o=o(" (MobileBERT model)"),z3o=l(),G1=a("li"),sae=a("strong"),W3o=o("mpnet"),Q3o=o(" \u2014 "),ZI=a("a"),H3o=o("MPNetForMultipleChoice"),U3o=o(" (MPNet model)"),J3o=l(),O1=a("li"),lae=a("strong"),Y3o=o("nystromformer"),K3o=o(" \u2014 "),eD=a("a"),Z3o=o("NystromformerForMultipleChoice"),eyo=o(" (Nystromformer model)"),oyo=l(),X1=a("li"),iae=a("strong"),ryo=o("qdqbert"),tyo=o(" \u2014 "),oD=a("a"),ayo=o("QDQBertForMultipleChoice"),nyo=o(" (QDQBert model)"),syo=l(),V1=a("li"),dae=a("strong"),lyo=o("rembert"),iyo=o(" \u2014 "),rD=a("a"),dyo=o("RemBertForMultipleChoice"),cyo=o(" (RemBERT model)"),fyo=l(),z1=a("li"),cae=a("strong"),myo=o("roberta"),gyo=o(" \u2014 "),tD=a("a"),hyo=o("RobertaForMultipleChoice"),pyo=o(" (RoBERTa model)"),_yo=l(),W1=a("li"),fae=a("strong"),uyo=o("roformer"),byo=o(" \u2014 "),aD=a("a"),vyo=o("RoFormerForMultipleChoice"),Tyo=o(" (RoFormer model)"),Fyo=l(),Q1=a("li"),mae=a("strong"),Cyo=o("squeezebert"),Myo=o(" \u2014 "),nD=a("a"),Eyo=o("SqueezeBertForMultipleChoice"),yyo=o(" (SqueezeBERT model)"),wyo=l(),H1=a("li"),gae=a("strong"),Ayo=o("xlm"),Lyo=o(" \u2014 "),sD=a("a"),Byo=o("XLMForMultipleChoice"),xyo=o(" (XLM model)"),kyo=l(),U1=a("li"),hae=a("strong"),Ryo=o("xlm-roberta"),Syo=o(" \u2014 "),lD=a("a"),Pyo=o("XLMRobertaForMultipleChoice"),$yo=o(" (XLM-RoBERTa model)"),Iyo=l(),J1=a("li"),pae=a("strong"),Dyo=o("xlm-roberta-xl"),jyo=o(" \u2014 "),iD=a("a"),Nyo=o("XLMRobertaXLForMultipleChoice"),qyo=o(" (XLM-RoBERTa-XL model)"),Gyo=l(),Y1=a("li"),_ae=a("strong"),Oyo=o("xlnet"),Xyo=o(" \u2014 "),dD=a("a"),Vyo=o("XLNetForMultipleChoice"),zyo=o(" (XLNet model)"),Wyo=l(),K1=a("li"),uae=a("strong"),Qyo=o("yoso"),Hyo=o(" \u2014 "),cD=a("a"),Uyo=o("YosoForMultipleChoice"),Jyo=o(" (YOSO model)"),Yyo=l(),Z1=a("p"),Kyo=o("The model is set in evaluation mode by default using "),bae=a("code"),Zyo=o("model.eval()"),ewo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=a("code"),owo=o("model.train()"),rwo=l(),Tae=a("p"),two=o("Examples:"),awo=l(),f(a3.$$.fragment),kBe=l(),cd=a("h2"),eb=a("a"),Fae=a("span"),f(n3.$$.fragment),nwo=l(),Cae=a("span"),swo=o("AutoModelForNextSentencePrediction"),RBe=l(),rr=a("div"),f(s3.$$.fragment),lwo=l(),fd=a("p"),iwo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Mae=a("code"),dwo=o("from_pretrained()"),cwo=o("class method or the "),Eae=a("code"),fwo=o("from_config()"),mwo=o(`class
method.`),gwo=l(),l3=a("p"),hwo=o("This class cannot be instantiated directly using "),yae=a("code"),pwo=o("__init__()"),_wo=o(" (throws an error)."),uwo=l(),Ur=a("div"),f(i3.$$.fragment),bwo=l(),wae=a("p"),vwo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Two=l(),md=a("p"),Fwo=o(`Note:
Loading a model from its configuration file does `),Aae=a("strong"),Cwo=o("not"),Mwo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=a("code"),Ewo=o("from_pretrained()"),ywo=o("to load the model weights."),wwo=l(),Bae=a("p"),Awo=o("Examples:"),Lwo=l(),f(d3.$$.fragment),Bwo=l(),Ve=a("div"),f(c3.$$.fragment),xwo=l(),xae=a("p"),kwo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Rwo=l(),Wa=a("p"),Swo=o("The model class to instantiate is selected based on the "),kae=a("code"),Pwo=o("model_type"),$wo=o(` property of the config object (either
passed as an argument or loaded from `),Rae=a("code"),Iwo=o("pretrained_model_name_or_path"),Dwo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=a("code"),jwo=o("pretrained_model_name_or_path"),Nwo=o(":"),qwo=l(),na=a("ul"),ob=a("li"),Pae=a("strong"),Gwo=o("bert"),Owo=o(" \u2014 "),fD=a("a"),Xwo=o("BertForNextSentencePrediction"),Vwo=o(" (BERT model)"),zwo=l(),rb=a("li"),$ae=a("strong"),Wwo=o("fnet"),Qwo=o(" \u2014 "),mD=a("a"),Hwo=o("FNetForNextSentencePrediction"),Uwo=o(" (FNet model)"),Jwo=l(),tb=a("li"),Iae=a("strong"),Ywo=o("megatron-bert"),Kwo=o(" \u2014 "),gD=a("a"),Zwo=o("MegatronBertForNextSentencePrediction"),e6o=o(" (MegatronBert model)"),o6o=l(),ab=a("li"),Dae=a("strong"),r6o=o("mobilebert"),t6o=o(" \u2014 "),hD=a("a"),a6o=o("MobileBertForNextSentencePrediction"),n6o=o(" (MobileBERT model)"),s6o=l(),nb=a("li"),jae=a("strong"),l6o=o("qdqbert"),i6o=o(" \u2014 "),pD=a("a"),d6o=o("QDQBertForNextSentencePrediction"),c6o=o(" (QDQBert model)"),f6o=l(),sb=a("p"),m6o=o("The model is set in evaluation mode by default using "),Nae=a("code"),g6o=o("model.eval()"),h6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),qae=a("code"),p6o=o("model.train()"),_6o=l(),Gae=a("p"),u6o=o("Examples:"),b6o=l(),f(f3.$$.fragment),SBe=l(),gd=a("h2"),lb=a("a"),Oae=a("span"),f(m3.$$.fragment),v6o=l(),Xae=a("span"),T6o=o("AutoModelForTokenClassification"),PBe=l(),tr=a("div"),f(g3.$$.fragment),F6o=l(),hd=a("p"),C6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Vae=a("code"),M6o=o("from_pretrained()"),E6o=o("class method or the "),zae=a("code"),y6o=o("from_config()"),w6o=o(`class
method.`),A6o=l(),h3=a("p"),L6o=o("This class cannot be instantiated directly using "),Wae=a("code"),B6o=o("__init__()"),x6o=o(" (throws an error)."),k6o=l(),Jr=a("div"),f(p3.$$.fragment),R6o=l(),Qae=a("p"),S6o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),P6o=l(),pd=a("p"),$6o=o(`Note:
Loading a model from its configuration file does `),Hae=a("strong"),I6o=o("not"),D6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uae=a("code"),j6o=o("from_pretrained()"),N6o=o("to load the model weights."),q6o=l(),Jae=a("p"),G6o=o("Examples:"),O6o=l(),f(_3.$$.fragment),X6o=l(),ze=a("div"),f(u3.$$.fragment),V6o=l(),Yae=a("p"),z6o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),W6o=l(),Qa=a("p"),Q6o=o("The model class to instantiate is selected based on the "),Kae=a("code"),H6o=o("model_type"),U6o=o(` property of the config object (either
passed as an argument or loaded from `),Zae=a("code"),J6o=o("pretrained_model_name_or_path"),Y6o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ene=a("code"),K6o=o("pretrained_model_name_or_path"),Z6o=o(":"),eAo=l(),N=a("ul"),ib=a("li"),one=a("strong"),oAo=o("albert"),rAo=o(" \u2014 "),_D=a("a"),tAo=o("AlbertForTokenClassification"),aAo=o(" (ALBERT model)"),nAo=l(),db=a("li"),rne=a("strong"),sAo=o("bert"),lAo=o(" \u2014 "),uD=a("a"),iAo=o("BertForTokenClassification"),dAo=o(" (BERT model)"),cAo=l(),cb=a("li"),tne=a("strong"),fAo=o("big_bird"),mAo=o(" \u2014 "),bD=a("a"),gAo=o("BigBirdForTokenClassification"),hAo=o(" (BigBird model)"),pAo=l(),fb=a("li"),ane=a("strong"),_Ao=o("camembert"),uAo=o(" \u2014 "),vD=a("a"),bAo=o("CamembertForTokenClassification"),vAo=o(" (CamemBERT model)"),TAo=l(),mb=a("li"),nne=a("strong"),FAo=o("canine"),CAo=o(" \u2014 "),TD=a("a"),MAo=o("CanineForTokenClassification"),EAo=o(" (Canine model)"),yAo=l(),gb=a("li"),sne=a("strong"),wAo=o("convbert"),AAo=o(" \u2014 "),FD=a("a"),LAo=o("ConvBertForTokenClassification"),BAo=o(" (ConvBERT model)"),xAo=l(),hb=a("li"),lne=a("strong"),kAo=o("data2vec-text"),RAo=o(" \u2014 "),CD=a("a"),SAo=o("Data2VecTextForTokenClassification"),PAo=o(" (Data2VecText model)"),$Ao=l(),pb=a("li"),ine=a("strong"),IAo=o("deberta"),DAo=o(" \u2014 "),MD=a("a"),jAo=o("DebertaForTokenClassification"),NAo=o(" (DeBERTa model)"),qAo=l(),_b=a("li"),dne=a("strong"),GAo=o("deberta-v2"),OAo=o(" \u2014 "),ED=a("a"),XAo=o("DebertaV2ForTokenClassification"),VAo=o(" (DeBERTa-v2 model)"),zAo=l(),ub=a("li"),cne=a("strong"),WAo=o("distilbert"),QAo=o(" \u2014 "),yD=a("a"),HAo=o("DistilBertForTokenClassification"),UAo=o(" (DistilBERT model)"),JAo=l(),bb=a("li"),fne=a("strong"),YAo=o("electra"),KAo=o(" \u2014 "),wD=a("a"),ZAo=o("ElectraForTokenClassification"),eLo=o(" (ELECTRA model)"),oLo=l(),vb=a("li"),mne=a("strong"),rLo=o("flaubert"),tLo=o(" \u2014 "),AD=a("a"),aLo=o("FlaubertForTokenClassification"),nLo=o(" (FlauBERT model)"),sLo=l(),Tb=a("li"),gne=a("strong"),lLo=o("fnet"),iLo=o(" \u2014 "),LD=a("a"),dLo=o("FNetForTokenClassification"),cLo=o(" (FNet model)"),fLo=l(),Fb=a("li"),hne=a("strong"),mLo=o("funnel"),gLo=o(" \u2014 "),BD=a("a"),hLo=o("FunnelForTokenClassification"),pLo=o(" (Funnel Transformer model)"),_Lo=l(),Cb=a("li"),pne=a("strong"),uLo=o("gpt2"),bLo=o(" \u2014 "),xD=a("a"),vLo=o("GPT2ForTokenClassification"),TLo=o(" (OpenAI GPT-2 model)"),FLo=l(),Mb=a("li"),_ne=a("strong"),CLo=o("ibert"),MLo=o(" \u2014 "),kD=a("a"),ELo=o("IBertForTokenClassification"),yLo=o(" (I-BERT model)"),wLo=l(),Eb=a("li"),une=a("strong"),ALo=o("layoutlm"),LLo=o(" \u2014 "),RD=a("a"),BLo=o("LayoutLMForTokenClassification"),xLo=o(" (LayoutLM model)"),kLo=l(),yb=a("li"),bne=a("strong"),RLo=o("layoutlmv2"),SLo=o(" \u2014 "),SD=a("a"),PLo=o("LayoutLMv2ForTokenClassification"),$Lo=o(" (LayoutLMv2 model)"),ILo=l(),wb=a("li"),vne=a("strong"),DLo=o("longformer"),jLo=o(" \u2014 "),PD=a("a"),NLo=o("LongformerForTokenClassification"),qLo=o(" (Longformer model)"),GLo=l(),Ab=a("li"),Tne=a("strong"),OLo=o("megatron-bert"),XLo=o(" \u2014 "),$D=a("a"),VLo=o("MegatronBertForTokenClassification"),zLo=o(" (MegatronBert model)"),WLo=l(),Lb=a("li"),Fne=a("strong"),QLo=o("mobilebert"),HLo=o(" \u2014 "),ID=a("a"),ULo=o("MobileBertForTokenClassification"),JLo=o(" (MobileBERT model)"),YLo=l(),Bb=a("li"),Cne=a("strong"),KLo=o("mpnet"),ZLo=o(" \u2014 "),DD=a("a"),e8o=o("MPNetForTokenClassification"),o8o=o(" (MPNet model)"),r8o=l(),xb=a("li"),Mne=a("strong"),t8o=o("nystromformer"),a8o=o(" \u2014 "),jD=a("a"),n8o=o("NystromformerForTokenClassification"),s8o=o(" (Nystromformer model)"),l8o=l(),kb=a("li"),Ene=a("strong"),i8o=o("qdqbert"),d8o=o(" \u2014 "),ND=a("a"),c8o=o("QDQBertForTokenClassification"),f8o=o(" (QDQBert model)"),m8o=l(),Rb=a("li"),yne=a("strong"),g8o=o("rembert"),h8o=o(" \u2014 "),qD=a("a"),p8o=o("RemBertForTokenClassification"),_8o=o(" (RemBERT model)"),u8o=l(),Sb=a("li"),wne=a("strong"),b8o=o("roberta"),v8o=o(" \u2014 "),GD=a("a"),T8o=o("RobertaForTokenClassification"),F8o=o(" (RoBERTa model)"),C8o=l(),Pb=a("li"),Ane=a("strong"),M8o=o("roformer"),E8o=o(" \u2014 "),OD=a("a"),y8o=o("RoFormerForTokenClassification"),w8o=o(" (RoFormer model)"),A8o=l(),$b=a("li"),Lne=a("strong"),L8o=o("squeezebert"),B8o=o(" \u2014 "),XD=a("a"),x8o=o("SqueezeBertForTokenClassification"),k8o=o(" (SqueezeBERT model)"),R8o=l(),Ib=a("li"),Bne=a("strong"),S8o=o("xlm"),P8o=o(" \u2014 "),VD=a("a"),$8o=o("XLMForTokenClassification"),I8o=o(" (XLM model)"),D8o=l(),Db=a("li"),xne=a("strong"),j8o=o("xlm-roberta"),N8o=o(" \u2014 "),zD=a("a"),q8o=o("XLMRobertaForTokenClassification"),G8o=o(" (XLM-RoBERTa model)"),O8o=l(),jb=a("li"),kne=a("strong"),X8o=o("xlm-roberta-xl"),V8o=o(" \u2014 "),WD=a("a"),z8o=o("XLMRobertaXLForTokenClassification"),W8o=o(" (XLM-RoBERTa-XL model)"),Q8o=l(),Nb=a("li"),Rne=a("strong"),H8o=o("xlnet"),U8o=o(" \u2014 "),QD=a("a"),J8o=o("XLNetForTokenClassification"),Y8o=o(" (XLNet model)"),K8o=l(),qb=a("li"),Sne=a("strong"),Z8o=o("yoso"),e7o=o(" \u2014 "),HD=a("a"),o7o=o("YosoForTokenClassification"),r7o=o(" (YOSO model)"),t7o=l(),Gb=a("p"),a7o=o("The model is set in evaluation mode by default using "),Pne=a("code"),n7o=o("model.eval()"),s7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$ne=a("code"),l7o=o("model.train()"),i7o=l(),Ine=a("p"),d7o=o("Examples:"),c7o=l(),f(b3.$$.fragment),$Be=l(),_d=a("h2"),Ob=a("a"),Dne=a("span"),f(v3.$$.fragment),f7o=l(),jne=a("span"),m7o=o("AutoModelForQuestionAnswering"),IBe=l(),ar=a("div"),f(T3.$$.fragment),g7o=l(),ud=a("p"),h7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Nne=a("code"),p7o=o("from_pretrained()"),_7o=o("class method or the "),qne=a("code"),u7o=o("from_config()"),b7o=o(`class
method.`),v7o=l(),F3=a("p"),T7o=o("This class cannot be instantiated directly using "),Gne=a("code"),F7o=o("__init__()"),C7o=o(" (throws an error)."),M7o=l(),Yr=a("div"),f(C3.$$.fragment),E7o=l(),One=a("p"),y7o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),w7o=l(),bd=a("p"),A7o=o(`Note:
Loading a model from its configuration file does `),Xne=a("strong"),L7o=o("not"),B7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vne=a("code"),x7o=o("from_pretrained()"),k7o=o("to load the model weights."),R7o=l(),zne=a("p"),S7o=o("Examples:"),P7o=l(),f(M3.$$.fragment),$7o=l(),We=a("div"),f(E3.$$.fragment),I7o=l(),Wne=a("p"),D7o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),j7o=l(),Ha=a("p"),N7o=o("The model class to instantiate is selected based on the "),Qne=a("code"),q7o=o("model_type"),G7o=o(` property of the config object (either
passed as an argument or loaded from `),Hne=a("code"),O7o=o("pretrained_model_name_or_path"),X7o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Une=a("code"),V7o=o("pretrained_model_name_or_path"),z7o=o(":"),W7o=l(),R=a("ul"),Xb=a("li"),Jne=a("strong"),Q7o=o("albert"),H7o=o(" \u2014 "),UD=a("a"),U7o=o("AlbertForQuestionAnswering"),J7o=o(" (ALBERT model)"),Y7o=l(),Vb=a("li"),Yne=a("strong"),K7o=o("bart"),Z7o=o(" \u2014 "),JD=a("a"),eBo=o("BartForQuestionAnswering"),oBo=o(" (BART model)"),rBo=l(),zb=a("li"),Kne=a("strong"),tBo=o("bert"),aBo=o(" \u2014 "),YD=a("a"),nBo=o("BertForQuestionAnswering"),sBo=o(" (BERT model)"),lBo=l(),Wb=a("li"),Zne=a("strong"),iBo=o("big_bird"),dBo=o(" \u2014 "),KD=a("a"),cBo=o("BigBirdForQuestionAnswering"),fBo=o(" (BigBird model)"),mBo=l(),Qb=a("li"),ese=a("strong"),gBo=o("bigbird_pegasus"),hBo=o(" \u2014 "),ZD=a("a"),pBo=o("BigBirdPegasusForQuestionAnswering"),_Bo=o(" (BigBirdPegasus model)"),uBo=l(),Hb=a("li"),ose=a("strong"),bBo=o("camembert"),vBo=o(" \u2014 "),ej=a("a"),TBo=o("CamembertForQuestionAnswering"),FBo=o(" (CamemBERT model)"),CBo=l(),Ub=a("li"),rse=a("strong"),MBo=o("canine"),EBo=o(" \u2014 "),oj=a("a"),yBo=o("CanineForQuestionAnswering"),wBo=o(" (Canine model)"),ABo=l(),Jb=a("li"),tse=a("strong"),LBo=o("convbert"),BBo=o(" \u2014 "),rj=a("a"),xBo=o("ConvBertForQuestionAnswering"),kBo=o(" (ConvBERT model)"),RBo=l(),Yb=a("li"),ase=a("strong"),SBo=o("data2vec-text"),PBo=o(" \u2014 "),tj=a("a"),$Bo=o("Data2VecTextForQuestionAnswering"),IBo=o(" (Data2VecText model)"),DBo=l(),Kb=a("li"),nse=a("strong"),jBo=o("deberta"),NBo=o(" \u2014 "),aj=a("a"),qBo=o("DebertaForQuestionAnswering"),GBo=o(" (DeBERTa model)"),OBo=l(),Zb=a("li"),sse=a("strong"),XBo=o("deberta-v2"),VBo=o(" \u2014 "),nj=a("a"),zBo=o("DebertaV2ForQuestionAnswering"),WBo=o(" (DeBERTa-v2 model)"),QBo=l(),e5=a("li"),lse=a("strong"),HBo=o("distilbert"),UBo=o(" \u2014 "),sj=a("a"),JBo=o("DistilBertForQuestionAnswering"),YBo=o(" (DistilBERT model)"),KBo=l(),o5=a("li"),ise=a("strong"),ZBo=o("electra"),exo=o(" \u2014 "),lj=a("a"),oxo=o("ElectraForQuestionAnswering"),rxo=o(" (ELECTRA model)"),txo=l(),r5=a("li"),dse=a("strong"),axo=o("flaubert"),nxo=o(" \u2014 "),ij=a("a"),sxo=o("FlaubertForQuestionAnsweringSimple"),lxo=o(" (FlauBERT model)"),ixo=l(),t5=a("li"),cse=a("strong"),dxo=o("fnet"),cxo=o(" \u2014 "),dj=a("a"),fxo=o("FNetForQuestionAnswering"),mxo=o(" (FNet model)"),gxo=l(),a5=a("li"),fse=a("strong"),hxo=o("funnel"),pxo=o(" \u2014 "),cj=a("a"),_xo=o("FunnelForQuestionAnswering"),uxo=o(" (Funnel Transformer model)"),bxo=l(),n5=a("li"),mse=a("strong"),vxo=o("gptj"),Txo=o(" \u2014 "),fj=a("a"),Fxo=o("GPTJForQuestionAnswering"),Cxo=o(" (GPT-J model)"),Mxo=l(),s5=a("li"),gse=a("strong"),Exo=o("ibert"),yxo=o(" \u2014 "),mj=a("a"),wxo=o("IBertForQuestionAnswering"),Axo=o(" (I-BERT model)"),Lxo=l(),l5=a("li"),hse=a("strong"),Bxo=o("layoutlmv2"),xxo=o(" \u2014 "),gj=a("a"),kxo=o("LayoutLMv2ForQuestionAnswering"),Rxo=o(" (LayoutLMv2 model)"),Sxo=l(),i5=a("li"),pse=a("strong"),Pxo=o("led"),$xo=o(" \u2014 "),hj=a("a"),Ixo=o("LEDForQuestionAnswering"),Dxo=o(" (LED model)"),jxo=l(),d5=a("li"),_se=a("strong"),Nxo=o("longformer"),qxo=o(" \u2014 "),pj=a("a"),Gxo=o("LongformerForQuestionAnswering"),Oxo=o(" (Longformer model)"),Xxo=l(),c5=a("li"),use=a("strong"),Vxo=o("lxmert"),zxo=o(" \u2014 "),_j=a("a"),Wxo=o("LxmertForQuestionAnswering"),Qxo=o(" (LXMERT model)"),Hxo=l(),f5=a("li"),bse=a("strong"),Uxo=o("mbart"),Jxo=o(" \u2014 "),uj=a("a"),Yxo=o("MBartForQuestionAnswering"),Kxo=o(" (mBART model)"),Zxo=l(),m5=a("li"),vse=a("strong"),eko=o("megatron-bert"),oko=o(" \u2014 "),bj=a("a"),rko=o("MegatronBertForQuestionAnswering"),tko=o(" (MegatronBert model)"),ako=l(),g5=a("li"),Tse=a("strong"),nko=o("mobilebert"),sko=o(" \u2014 "),vj=a("a"),lko=o("MobileBertForQuestionAnswering"),iko=o(" (MobileBERT model)"),dko=l(),h5=a("li"),Fse=a("strong"),cko=o("mpnet"),fko=o(" \u2014 "),Tj=a("a"),mko=o("MPNetForQuestionAnswering"),gko=o(" (MPNet model)"),hko=l(),p5=a("li"),Cse=a("strong"),pko=o("nystromformer"),_ko=o(" \u2014 "),Fj=a("a"),uko=o("NystromformerForQuestionAnswering"),bko=o(" (Nystromformer model)"),vko=l(),_5=a("li"),Mse=a("strong"),Tko=o("qdqbert"),Fko=o(" \u2014 "),Cj=a("a"),Cko=o("QDQBertForQuestionAnswering"),Mko=o(" (QDQBert model)"),Eko=l(),u5=a("li"),Ese=a("strong"),yko=o("reformer"),wko=o(" \u2014 "),Mj=a("a"),Ako=o("ReformerForQuestionAnswering"),Lko=o(" (Reformer model)"),Bko=l(),b5=a("li"),yse=a("strong"),xko=o("rembert"),kko=o(" \u2014 "),Ej=a("a"),Rko=o("RemBertForQuestionAnswering"),Sko=o(" (RemBERT model)"),Pko=l(),v5=a("li"),wse=a("strong"),$ko=o("roberta"),Iko=o(" \u2014 "),yj=a("a"),Dko=o("RobertaForQuestionAnswering"),jko=o(" (RoBERTa model)"),Nko=l(),T5=a("li"),Ase=a("strong"),qko=o("roformer"),Gko=o(" \u2014 "),wj=a("a"),Oko=o("RoFormerForQuestionAnswering"),Xko=o(" (RoFormer model)"),Vko=l(),F5=a("li"),Lse=a("strong"),zko=o("splinter"),Wko=o(" \u2014 "),Aj=a("a"),Qko=o("SplinterForQuestionAnswering"),Hko=o(" (Splinter model)"),Uko=l(),C5=a("li"),Bse=a("strong"),Jko=o("squeezebert"),Yko=o(" \u2014 "),Lj=a("a"),Kko=o("SqueezeBertForQuestionAnswering"),Zko=o(" (SqueezeBERT model)"),eRo=l(),M5=a("li"),xse=a("strong"),oRo=o("xlm"),rRo=o(" \u2014 "),Bj=a("a"),tRo=o("XLMForQuestionAnsweringSimple"),aRo=o(" (XLM model)"),nRo=l(),E5=a("li"),kse=a("strong"),sRo=o("xlm-roberta"),lRo=o(" \u2014 "),xj=a("a"),iRo=o("XLMRobertaForQuestionAnswering"),dRo=o(" (XLM-RoBERTa model)"),cRo=l(),y5=a("li"),Rse=a("strong"),fRo=o("xlm-roberta-xl"),mRo=o(" \u2014 "),kj=a("a"),gRo=o("XLMRobertaXLForQuestionAnswering"),hRo=o(" (XLM-RoBERTa-XL model)"),pRo=l(),w5=a("li"),Sse=a("strong"),_Ro=o("xlnet"),uRo=o(" \u2014 "),Rj=a("a"),bRo=o("XLNetForQuestionAnsweringSimple"),vRo=o(" (XLNet model)"),TRo=l(),A5=a("li"),Pse=a("strong"),FRo=o("yoso"),CRo=o(" \u2014 "),Sj=a("a"),MRo=o("YosoForQuestionAnswering"),ERo=o(" (YOSO model)"),yRo=l(),L5=a("p"),wRo=o("The model is set in evaluation mode by default using "),$se=a("code"),ARo=o("model.eval()"),LRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ise=a("code"),BRo=o("model.train()"),xRo=l(),Dse=a("p"),kRo=o("Examples:"),RRo=l(),f(y3.$$.fragment),DBe=l(),vd=a("h2"),B5=a("a"),jse=a("span"),f(w3.$$.fragment),SRo=l(),Nse=a("span"),PRo=o("AutoModelForTableQuestionAnswering"),jBe=l(),nr=a("div"),f(A3.$$.fragment),$Ro=l(),Td=a("p"),IRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),qse=a("code"),DRo=o("from_pretrained()"),jRo=o("class method or the "),Gse=a("code"),NRo=o("from_config()"),qRo=o(`class
method.`),GRo=l(),L3=a("p"),ORo=o("This class cannot be instantiated directly using "),Ose=a("code"),XRo=o("__init__()"),VRo=o(" (throws an error)."),zRo=l(),Kr=a("div"),f(B3.$$.fragment),WRo=l(),Xse=a("p"),QRo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),HRo=l(),Fd=a("p"),URo=o(`Note:
Loading a model from its configuration file does `),Vse=a("strong"),JRo=o("not"),YRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zse=a("code"),KRo=o("from_pretrained()"),ZRo=o("to load the model weights."),eSo=l(),Wse=a("p"),oSo=o("Examples:"),rSo=l(),f(x3.$$.fragment),tSo=l(),Qe=a("div"),f(k3.$$.fragment),aSo=l(),Qse=a("p"),nSo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),sSo=l(),Ua=a("p"),lSo=o("The model class to instantiate is selected based on the "),Hse=a("code"),iSo=o("model_type"),dSo=o(` property of the config object (either
passed as an argument or loaded from `),Use=a("code"),cSo=o("pretrained_model_name_or_path"),fSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jse=a("code"),mSo=o("pretrained_model_name_or_path"),gSo=o(":"),hSo=l(),Yse=a("ul"),x5=a("li"),Kse=a("strong"),pSo=o("tapas"),_So=o(" \u2014 "),Pj=a("a"),uSo=o("TapasForQuestionAnswering"),bSo=o(" (TAPAS model)"),vSo=l(),k5=a("p"),TSo=o("The model is set in evaluation mode by default using "),Zse=a("code"),FSo=o("model.eval()"),CSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ele=a("code"),MSo=o("model.train()"),ESo=l(),ole=a("p"),ySo=o("Examples:"),wSo=l(),f(R3.$$.fragment),NBe=l(),Cd=a("h2"),R5=a("a"),rle=a("span"),f(S3.$$.fragment),ASo=l(),tle=a("span"),LSo=o("AutoModelForImageClassification"),qBe=l(),sr=a("div"),f(P3.$$.fragment),BSo=l(),Md=a("p"),xSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ale=a("code"),kSo=o("from_pretrained()"),RSo=o("class method or the "),nle=a("code"),SSo=o("from_config()"),PSo=o(`class
method.`),$So=l(),$3=a("p"),ISo=o("This class cannot be instantiated directly using "),sle=a("code"),DSo=o("__init__()"),jSo=o(" (throws an error)."),NSo=l(),Zr=a("div"),f(I3.$$.fragment),qSo=l(),lle=a("p"),GSo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),OSo=l(),Ed=a("p"),XSo=o(`Note:
Loading a model from its configuration file does `),ile=a("strong"),VSo=o("not"),zSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dle=a("code"),WSo=o("from_pretrained()"),QSo=o("to load the model weights."),HSo=l(),cle=a("p"),USo=o("Examples:"),JSo=l(),f(D3.$$.fragment),YSo=l(),He=a("div"),f(j3.$$.fragment),KSo=l(),fle=a("p"),ZSo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ePo=l(),Ja=a("p"),oPo=o("The model class to instantiate is selected based on the "),mle=a("code"),rPo=o("model_type"),tPo=o(` property of the config object (either
passed as an argument or loaded from `),gle=a("code"),aPo=o("pretrained_model_name_or_path"),nPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hle=a("code"),sPo=o("pretrained_model_name_or_path"),lPo=o(":"),iPo=l(),Fe=a("ul"),S5=a("li"),ple=a("strong"),dPo=o("beit"),cPo=o(" \u2014 "),$j=a("a"),fPo=o("BeitForImageClassification"),mPo=o(" (BEiT model)"),gPo=l(),P5=a("li"),_le=a("strong"),hPo=o("convnext"),pPo=o(" \u2014 "),Ij=a("a"),_Po=o("ConvNextForImageClassification"),uPo=o(" (ConvNext model)"),bPo=l(),$s=a("li"),ule=a("strong"),vPo=o("deit"),TPo=o(" \u2014 "),Dj=a("a"),FPo=o("DeiTForImageClassification"),CPo=o(" or "),jj=a("a"),MPo=o("DeiTForImageClassificationWithTeacher"),EPo=o(" (DeiT model)"),yPo=l(),$5=a("li"),ble=a("strong"),wPo=o("imagegpt"),APo=o(" \u2014 "),Nj=a("a"),LPo=o("ImageGPTForImageClassification"),BPo=o(" (ImageGPT model)"),xPo=l(),la=a("li"),vle=a("strong"),kPo=o("perceiver"),RPo=o(" \u2014 "),qj=a("a"),SPo=o("PerceiverForImageClassificationLearned"),PPo=o(" or "),Gj=a("a"),$Po=o("PerceiverForImageClassificationFourier"),IPo=o(" or "),Oj=a("a"),DPo=o("PerceiverForImageClassificationConvProcessing"),jPo=o(" (Perceiver model)"),NPo=l(),I5=a("li"),Tle=a("strong"),qPo=o("poolformer"),GPo=o(" \u2014 "),Xj=a("a"),OPo=o("PoolFormerForImageClassification"),XPo=o(" (PoolFormer model)"),VPo=l(),D5=a("li"),Fle=a("strong"),zPo=o("segformer"),WPo=o(" \u2014 "),Vj=a("a"),QPo=o("SegformerForImageClassification"),HPo=o(" (SegFormer model)"),UPo=l(),j5=a("li"),Cle=a("strong"),JPo=o("swin"),YPo=o(" \u2014 "),zj=a("a"),KPo=o("SwinForImageClassification"),ZPo=o(" (Swin model)"),e$o=l(),N5=a("li"),Mle=a("strong"),o$o=o("vit"),r$o=o(" \u2014 "),Wj=a("a"),t$o=o("ViTForImageClassification"),a$o=o(" (ViT model)"),n$o=l(),q5=a("p"),s$o=o("The model is set in evaluation mode by default using "),Ele=a("code"),l$o=o("model.eval()"),i$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yle=a("code"),d$o=o("model.train()"),c$o=l(),wle=a("p"),f$o=o("Examples:"),m$o=l(),f(N3.$$.fragment),GBe=l(),yd=a("h2"),G5=a("a"),Ale=a("span"),f(q3.$$.fragment),g$o=l(),Lle=a("span"),h$o=o("AutoModelForVision2Seq"),OBe=l(),lr=a("div"),f(G3.$$.fragment),p$o=l(),wd=a("p"),_$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ble=a("code"),u$o=o("from_pretrained()"),b$o=o("class method or the "),xle=a("code"),v$o=o("from_config()"),T$o=o(`class
method.`),F$o=l(),O3=a("p"),C$o=o("This class cannot be instantiated directly using "),kle=a("code"),M$o=o("__init__()"),E$o=o(" (throws an error)."),y$o=l(),et=a("div"),f(X3.$$.fragment),w$o=l(),Rle=a("p"),A$o=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),L$o=l(),Ad=a("p"),B$o=o(`Note:
Loading a model from its configuration file does `),Sle=a("strong"),x$o=o("not"),k$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ple=a("code"),R$o=o("from_pretrained()"),S$o=o("to load the model weights."),P$o=l(),$le=a("p"),$$o=o("Examples:"),I$o=l(),f(V3.$$.fragment),D$o=l(),Ue=a("div"),f(z3.$$.fragment),j$o=l(),Ile=a("p"),N$o=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),q$o=l(),Ya=a("p"),G$o=o("The model class to instantiate is selected based on the "),Dle=a("code"),O$o=o("model_type"),X$o=o(` property of the config object (either
passed as an argument or loaded from `),jle=a("code"),V$o=o("pretrained_model_name_or_path"),z$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nle=a("code"),W$o=o("pretrained_model_name_or_path"),Q$o=o(":"),H$o=l(),qle=a("ul"),O5=a("li"),Gle=a("strong"),U$o=o("vision-encoder-decoder"),J$o=o(" \u2014 "),Qj=a("a"),Y$o=o("VisionEncoderDecoderModel"),K$o=o(" (Vision Encoder decoder model)"),Z$o=l(),X5=a("p"),eIo=o("The model is set in evaluation mode by default using "),Ole=a("code"),oIo=o("model.eval()"),rIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xle=a("code"),tIo=o("model.train()"),aIo=l(),Vle=a("p"),nIo=o("Examples:"),sIo=l(),f(W3.$$.fragment),XBe=l(),Ld=a("h2"),V5=a("a"),zle=a("span"),f(Q3.$$.fragment),lIo=l(),Wle=a("span"),iIo=o("AutoModelForAudioClassification"),VBe=l(),ir=a("div"),f(H3.$$.fragment),dIo=l(),Bd=a("p"),cIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Qle=a("code"),fIo=o("from_pretrained()"),mIo=o("class method or the "),Hle=a("code"),gIo=o("from_config()"),hIo=o(`class
method.`),pIo=l(),U3=a("p"),_Io=o("This class cannot be instantiated directly using "),Ule=a("code"),uIo=o("__init__()"),bIo=o(" (throws an error)."),vIo=l(),ot=a("div"),f(J3.$$.fragment),TIo=l(),Jle=a("p"),FIo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),CIo=l(),xd=a("p"),MIo=o(`Note:
Loading a model from its configuration file does `),Yle=a("strong"),EIo=o("not"),yIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kle=a("code"),wIo=o("from_pretrained()"),AIo=o("to load the model weights."),LIo=l(),Zle=a("p"),BIo=o("Examples:"),xIo=l(),f(Y3.$$.fragment),kIo=l(),Je=a("div"),f(K3.$$.fragment),RIo=l(),eie=a("p"),SIo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),PIo=l(),Ka=a("p"),$Io=o("The model class to instantiate is selected based on the "),oie=a("code"),IIo=o("model_type"),DIo=o(` property of the config object (either
passed as an argument or loaded from `),rie=a("code"),jIo=o("pretrained_model_name_or_path"),NIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tie=a("code"),qIo=o("pretrained_model_name_or_path"),GIo=o(":"),OIo=l(),xe=a("ul"),z5=a("li"),aie=a("strong"),XIo=o("data2vec-audio"),VIo=o(" \u2014 "),Hj=a("a"),zIo=o("Data2VecAudioForSequenceClassification"),WIo=o(" (Data2VecAudio model)"),QIo=l(),W5=a("li"),nie=a("strong"),HIo=o("hubert"),UIo=o(" \u2014 "),Uj=a("a"),JIo=o("HubertForSequenceClassification"),YIo=o(" (Hubert model)"),KIo=l(),Q5=a("li"),sie=a("strong"),ZIo=o("sew"),eDo=o(" \u2014 "),Jj=a("a"),oDo=o("SEWForSequenceClassification"),rDo=o(" (SEW model)"),tDo=l(),H5=a("li"),lie=a("strong"),aDo=o("sew-d"),nDo=o(" \u2014 "),Yj=a("a"),sDo=o("SEWDForSequenceClassification"),lDo=o(" (SEW-D model)"),iDo=l(),U5=a("li"),iie=a("strong"),dDo=o("unispeech"),cDo=o(" \u2014 "),Kj=a("a"),fDo=o("UniSpeechForSequenceClassification"),mDo=o(" (UniSpeech model)"),gDo=l(),J5=a("li"),die=a("strong"),hDo=o("unispeech-sat"),pDo=o(" \u2014 "),Zj=a("a"),_Do=o("UniSpeechSatForSequenceClassification"),uDo=o(" (UniSpeechSat model)"),bDo=l(),Y5=a("li"),cie=a("strong"),vDo=o("wav2vec2"),TDo=o(" \u2014 "),eN=a("a"),FDo=o("Wav2Vec2ForSequenceClassification"),CDo=o(" (Wav2Vec2 model)"),MDo=l(),K5=a("li"),fie=a("strong"),EDo=o("wavlm"),yDo=o(" \u2014 "),oN=a("a"),wDo=o("WavLMForSequenceClassification"),ADo=o(" (WavLM model)"),LDo=l(),Z5=a("p"),BDo=o("The model is set in evaluation mode by default using "),mie=a("code"),xDo=o("model.eval()"),kDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gie=a("code"),RDo=o("model.train()"),SDo=l(),hie=a("p"),PDo=o("Examples:"),$Do=l(),f(Z3.$$.fragment),zBe=l(),kd=a("h2"),e2=a("a"),pie=a("span"),f(ey.$$.fragment),IDo=l(),_ie=a("span"),DDo=o("AutoModelForAudioFrameClassification"),WBe=l(),dr=a("div"),f(oy.$$.fragment),jDo=l(),Rd=a("p"),NDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),uie=a("code"),qDo=o("from_pretrained()"),GDo=o("class method or the "),bie=a("code"),ODo=o("from_config()"),XDo=o(`class
method.`),VDo=l(),ry=a("p"),zDo=o("This class cannot be instantiated directly using "),vie=a("code"),WDo=o("__init__()"),QDo=o(" (throws an error)."),HDo=l(),rt=a("div"),f(ty.$$.fragment),UDo=l(),Tie=a("p"),JDo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),YDo=l(),Sd=a("p"),KDo=o(`Note:
Loading a model from its configuration file does `),Fie=a("strong"),ZDo=o("not"),ejo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cie=a("code"),ojo=o("from_pretrained()"),rjo=o("to load the model weights."),tjo=l(),Mie=a("p"),ajo=o("Examples:"),njo=l(),f(ay.$$.fragment),sjo=l(),Ye=a("div"),f(ny.$$.fragment),ljo=l(),Eie=a("p"),ijo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),djo=l(),Za=a("p"),cjo=o("The model class to instantiate is selected based on the "),yie=a("code"),fjo=o("model_type"),mjo=o(` property of the config object (either
passed as an argument or loaded from `),wie=a("code"),gjo=o("pretrained_model_name_or_path"),hjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aie=a("code"),pjo=o("pretrained_model_name_or_path"),_jo=o(":"),ujo=l(),en=a("ul"),o2=a("li"),Lie=a("strong"),bjo=o("data2vec-audio"),vjo=o(" \u2014 "),rN=a("a"),Tjo=o("Data2VecAudioForAudioFrameClassification"),Fjo=o(" (Data2VecAudio model)"),Cjo=l(),r2=a("li"),Bie=a("strong"),Mjo=o("unispeech-sat"),Ejo=o(" \u2014 "),tN=a("a"),yjo=o("UniSpeechSatForAudioFrameClassification"),wjo=o(" (UniSpeechSat model)"),Ajo=l(),t2=a("li"),xie=a("strong"),Ljo=o("wav2vec2"),Bjo=o(" \u2014 "),aN=a("a"),xjo=o("Wav2Vec2ForAudioFrameClassification"),kjo=o(" (Wav2Vec2 model)"),Rjo=l(),a2=a("li"),kie=a("strong"),Sjo=o("wavlm"),Pjo=o(" \u2014 "),nN=a("a"),$jo=o("WavLMForAudioFrameClassification"),Ijo=o(" (WavLM model)"),Djo=l(),n2=a("p"),jjo=o("The model is set in evaluation mode by default using "),Rie=a("code"),Njo=o("model.eval()"),qjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Sie=a("code"),Gjo=o("model.train()"),Ojo=l(),Pie=a("p"),Xjo=o("Examples:"),Vjo=l(),f(sy.$$.fragment),QBe=l(),Pd=a("h2"),s2=a("a"),$ie=a("span"),f(ly.$$.fragment),zjo=l(),Iie=a("span"),Wjo=o("AutoModelForCTC"),HBe=l(),cr=a("div"),f(iy.$$.fragment),Qjo=l(),$d=a("p"),Hjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Die=a("code"),Ujo=o("from_pretrained()"),Jjo=o("class method or the "),jie=a("code"),Yjo=o("from_config()"),Kjo=o(`class
method.`),Zjo=l(),dy=a("p"),eNo=o("This class cannot be instantiated directly using "),Nie=a("code"),oNo=o("__init__()"),rNo=o(" (throws an error)."),tNo=l(),tt=a("div"),f(cy.$$.fragment),aNo=l(),qie=a("p"),nNo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),sNo=l(),Id=a("p"),lNo=o(`Note:
Loading a model from its configuration file does `),Gie=a("strong"),iNo=o("not"),dNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Oie=a("code"),cNo=o("from_pretrained()"),fNo=o("to load the model weights."),mNo=l(),Xie=a("p"),gNo=o("Examples:"),hNo=l(),f(fy.$$.fragment),pNo=l(),Ke=a("div"),f(my.$$.fragment),_No=l(),Vie=a("p"),uNo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),bNo=l(),on=a("p"),vNo=o("The model class to instantiate is selected based on the "),zie=a("code"),TNo=o("model_type"),FNo=o(` property of the config object (either
passed as an argument or loaded from `),Wie=a("code"),CNo=o("pretrained_model_name_or_path"),MNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qie=a("code"),ENo=o("pretrained_model_name_or_path"),yNo=o(":"),wNo=l(),ke=a("ul"),l2=a("li"),Hie=a("strong"),ANo=o("data2vec-audio"),LNo=o(" \u2014 "),sN=a("a"),BNo=o("Data2VecAudioForCTC"),xNo=o(" (Data2VecAudio model)"),kNo=l(),i2=a("li"),Uie=a("strong"),RNo=o("hubert"),SNo=o(" \u2014 "),lN=a("a"),PNo=o("HubertForCTC"),$No=o(" (Hubert model)"),INo=l(),d2=a("li"),Jie=a("strong"),DNo=o("sew"),jNo=o(" \u2014 "),iN=a("a"),NNo=o("SEWForCTC"),qNo=o(" (SEW model)"),GNo=l(),c2=a("li"),Yie=a("strong"),ONo=o("sew-d"),XNo=o(" \u2014 "),dN=a("a"),VNo=o("SEWDForCTC"),zNo=o(" (SEW-D model)"),WNo=l(),f2=a("li"),Kie=a("strong"),QNo=o("unispeech"),HNo=o(" \u2014 "),cN=a("a"),UNo=o("UniSpeechForCTC"),JNo=o(" (UniSpeech model)"),YNo=l(),m2=a("li"),Zie=a("strong"),KNo=o("unispeech-sat"),ZNo=o(" \u2014 "),fN=a("a"),eqo=o("UniSpeechSatForCTC"),oqo=o(" (UniSpeechSat model)"),rqo=l(),g2=a("li"),ede=a("strong"),tqo=o("wav2vec2"),aqo=o(" \u2014 "),mN=a("a"),nqo=o("Wav2Vec2ForCTC"),sqo=o(" (Wav2Vec2 model)"),lqo=l(),h2=a("li"),ode=a("strong"),iqo=o("wavlm"),dqo=o(" \u2014 "),gN=a("a"),cqo=o("WavLMForCTC"),fqo=o(" (WavLM model)"),mqo=l(),p2=a("p"),gqo=o("The model is set in evaluation mode by default using "),rde=a("code"),hqo=o("model.eval()"),pqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=a("code"),_qo=o("model.train()"),uqo=l(),ade=a("p"),bqo=o("Examples:"),vqo=l(),f(gy.$$.fragment),UBe=l(),Dd=a("h2"),_2=a("a"),nde=a("span"),f(hy.$$.fragment),Tqo=l(),sde=a("span"),Fqo=o("AutoModelForSpeechSeq2Seq"),JBe=l(),fr=a("div"),f(py.$$.fragment),Cqo=l(),jd=a("p"),Mqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),lde=a("code"),Eqo=o("from_pretrained()"),yqo=o("class method or the "),ide=a("code"),wqo=o("from_config()"),Aqo=o(`class
method.`),Lqo=l(),_y=a("p"),Bqo=o("This class cannot be instantiated directly using "),dde=a("code"),xqo=o("__init__()"),kqo=o(" (throws an error)."),Rqo=l(),at=a("div"),f(uy.$$.fragment),Sqo=l(),cde=a("p"),Pqo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),$qo=l(),Nd=a("p"),Iqo=o(`Note:
Loading a model from its configuration file does `),fde=a("strong"),Dqo=o("not"),jqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=a("code"),Nqo=o("from_pretrained()"),qqo=o("to load the model weights."),Gqo=l(),gde=a("p"),Oqo=o("Examples:"),Xqo=l(),f(by.$$.fragment),Vqo=l(),Ze=a("div"),f(vy.$$.fragment),zqo=l(),hde=a("p"),Wqo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Qqo=l(),rn=a("p"),Hqo=o("The model class to instantiate is selected based on the "),pde=a("code"),Uqo=o("model_type"),Jqo=o(` property of the config object (either
passed as an argument or loaded from `),_de=a("code"),Yqo=o("pretrained_model_name_or_path"),Kqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=a("code"),Zqo=o("pretrained_model_name_or_path"),eGo=o(":"),oGo=l(),Ty=a("ul"),u2=a("li"),bde=a("strong"),rGo=o("speech-encoder-decoder"),tGo=o(" \u2014 "),hN=a("a"),aGo=o("SpeechEncoderDecoderModel"),nGo=o(" (Speech Encoder decoder model)"),sGo=l(),b2=a("li"),vde=a("strong"),lGo=o("speech_to_text"),iGo=o(" \u2014 "),pN=a("a"),dGo=o("Speech2TextForConditionalGeneration"),cGo=o(" (Speech2Text model)"),fGo=l(),v2=a("p"),mGo=o("The model is set in evaluation mode by default using "),Tde=a("code"),gGo=o("model.eval()"),hGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=a("code"),pGo=o("model.train()"),_Go=l(),Cde=a("p"),uGo=o("Examples:"),bGo=l(),f(Fy.$$.fragment),YBe=l(),qd=a("h2"),T2=a("a"),Mde=a("span"),f(Cy.$$.fragment),vGo=l(),Ede=a("span"),TGo=o("AutoModelForAudioXVector"),KBe=l(),mr=a("div"),f(My.$$.fragment),FGo=l(),Gd=a("p"),CGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),yde=a("code"),MGo=o("from_pretrained()"),EGo=o("class method or the "),wde=a("code"),yGo=o("from_config()"),wGo=o(`class
method.`),AGo=l(),Ey=a("p"),LGo=o("This class cannot be instantiated directly using "),Ade=a("code"),BGo=o("__init__()"),xGo=o(" (throws an error)."),kGo=l(),nt=a("div"),f(yy.$$.fragment),RGo=l(),Lde=a("p"),SGo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),PGo=l(),Od=a("p"),$Go=o(`Note:
Loading a model from its configuration file does `),Bde=a("strong"),IGo=o("not"),DGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xde=a("code"),jGo=o("from_pretrained()"),NGo=o("to load the model weights."),qGo=l(),kde=a("p"),GGo=o("Examples:"),OGo=l(),f(wy.$$.fragment),XGo=l(),eo=a("div"),f(Ay.$$.fragment),VGo=l(),Rde=a("p"),zGo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),WGo=l(),tn=a("p"),QGo=o("The model class to instantiate is selected based on the "),Sde=a("code"),HGo=o("model_type"),UGo=o(` property of the config object (either
passed as an argument or loaded from `),Pde=a("code"),JGo=o("pretrained_model_name_or_path"),YGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=a("code"),KGo=o("pretrained_model_name_or_path"),ZGo=o(":"),eOo=l(),an=a("ul"),F2=a("li"),Ide=a("strong"),oOo=o("data2vec-audio"),rOo=o(" \u2014 "),_N=a("a"),tOo=o("Data2VecAudioForXVector"),aOo=o(" (Data2VecAudio model)"),nOo=l(),C2=a("li"),Dde=a("strong"),sOo=o("unispeech-sat"),lOo=o(" \u2014 "),uN=a("a"),iOo=o("UniSpeechSatForXVector"),dOo=o(" (UniSpeechSat model)"),cOo=l(),M2=a("li"),jde=a("strong"),fOo=o("wav2vec2"),mOo=o(" \u2014 "),bN=a("a"),gOo=o("Wav2Vec2ForXVector"),hOo=o(" (Wav2Vec2 model)"),pOo=l(),E2=a("li"),Nde=a("strong"),_Oo=o("wavlm"),uOo=o(" \u2014 "),vN=a("a"),bOo=o("WavLMForXVector"),vOo=o(" (WavLM model)"),TOo=l(),y2=a("p"),FOo=o("The model is set in evaluation mode by default using "),qde=a("code"),COo=o("model.eval()"),MOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gde=a("code"),EOo=o("model.train()"),yOo=l(),Ode=a("p"),wOo=o("Examples:"),AOo=l(),f(Ly.$$.fragment),ZBe=l(),Xd=a("h2"),w2=a("a"),Xde=a("span"),f(By.$$.fragment),LOo=l(),Vde=a("span"),BOo=o("AutoModelForMaskedImageModeling"),exe=l(),gr=a("div"),f(xy.$$.fragment),xOo=l(),Vd=a("p"),kOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),zde=a("code"),ROo=o("from_pretrained()"),SOo=o("class method or the "),Wde=a("code"),POo=o("from_config()"),$Oo=o(`class
method.`),IOo=l(),ky=a("p"),DOo=o("This class cannot be instantiated directly using "),Qde=a("code"),jOo=o("__init__()"),NOo=o(" (throws an error)."),qOo=l(),st=a("div"),f(Ry.$$.fragment),GOo=l(),Hde=a("p"),OOo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),XOo=l(),zd=a("p"),VOo=o(`Note:
Loading a model from its configuration file does `),Ude=a("strong"),zOo=o("not"),WOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jde=a("code"),QOo=o("from_pretrained()"),HOo=o("to load the model weights."),UOo=l(),Yde=a("p"),JOo=o("Examples:"),YOo=l(),f(Sy.$$.fragment),KOo=l(),oo=a("div"),f(Py.$$.fragment),ZOo=l(),Kde=a("p"),eXo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),oXo=l(),nn=a("p"),rXo=o("The model class to instantiate is selected based on the "),Zde=a("code"),tXo=o("model_type"),aXo=o(` property of the config object (either
passed as an argument or loaded from `),ece=a("code"),nXo=o("pretrained_model_name_or_path"),sXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),oce=a("code"),lXo=o("pretrained_model_name_or_path"),iXo=o(":"),dXo=l(),Wd=a("ul"),A2=a("li"),rce=a("strong"),cXo=o("deit"),fXo=o(" \u2014 "),TN=a("a"),mXo=o("DeiTForMaskedImageModeling"),gXo=o(" (DeiT model)"),hXo=l(),L2=a("li"),tce=a("strong"),pXo=o("swin"),_Xo=o(" \u2014 "),FN=a("a"),uXo=o("SwinForMaskedImageModeling"),bXo=o(" (Swin model)"),vXo=l(),B2=a("li"),ace=a("strong"),TXo=o("vit"),FXo=o(" \u2014 "),CN=a("a"),CXo=o("ViTForMaskedImageModeling"),MXo=o(" (ViT model)"),EXo=l(),x2=a("p"),yXo=o("The model is set in evaluation mode by default using "),nce=a("code"),wXo=o("model.eval()"),AXo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sce=a("code"),LXo=o("model.train()"),BXo=l(),lce=a("p"),xXo=o("Examples:"),kXo=l(),f($y.$$.fragment),oxe=l(),Qd=a("h2"),k2=a("a"),ice=a("span"),f(Iy.$$.fragment),RXo=l(),dce=a("span"),SXo=o("AutoModelForObjectDetection"),rxe=l(),hr=a("div"),f(Dy.$$.fragment),PXo=l(),Hd=a("p"),$Xo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),cce=a("code"),IXo=o("from_pretrained()"),DXo=o("class method or the "),fce=a("code"),jXo=o("from_config()"),NXo=o(`class
method.`),qXo=l(),jy=a("p"),GXo=o("This class cannot be instantiated directly using "),mce=a("code"),OXo=o("__init__()"),XXo=o(" (throws an error)."),VXo=l(),lt=a("div"),f(Ny.$$.fragment),zXo=l(),gce=a("p"),WXo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),QXo=l(),Ud=a("p"),HXo=o(`Note:
Loading a model from its configuration file does `),hce=a("strong"),UXo=o("not"),JXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pce=a("code"),YXo=o("from_pretrained()"),KXo=o("to load the model weights."),ZXo=l(),_ce=a("p"),eVo=o("Examples:"),oVo=l(),f(qy.$$.fragment),rVo=l(),ro=a("div"),f(Gy.$$.fragment),tVo=l(),uce=a("p"),aVo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),nVo=l(),sn=a("p"),sVo=o("The model class to instantiate is selected based on the "),bce=a("code"),lVo=o("model_type"),iVo=o(` property of the config object (either
passed as an argument or loaded from `),vce=a("code"),dVo=o("pretrained_model_name_or_path"),cVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tce=a("code"),fVo=o("pretrained_model_name_or_path"),mVo=o(":"),gVo=l(),Fce=a("ul"),R2=a("li"),Cce=a("strong"),hVo=o("detr"),pVo=o(" \u2014 "),MN=a("a"),_Vo=o("DetrForObjectDetection"),uVo=o(" (DETR model)"),bVo=l(),S2=a("p"),vVo=o("The model is set in evaluation mode by default using "),Mce=a("code"),TVo=o("model.eval()"),FVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ece=a("code"),CVo=o("model.train()"),MVo=l(),yce=a("p"),EVo=o("Examples:"),yVo=l(),f(Oy.$$.fragment),txe=l(),Jd=a("h2"),P2=a("a"),wce=a("span"),f(Xy.$$.fragment),wVo=l(),Ace=a("span"),AVo=o("AutoModelForImageSegmentation"),axe=l(),pr=a("div"),f(Vy.$$.fragment),LVo=l(),Yd=a("p"),BVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Lce=a("code"),xVo=o("from_pretrained()"),kVo=o("class method or the "),Bce=a("code"),RVo=o("from_config()"),SVo=o(`class
method.`),PVo=l(),zy=a("p"),$Vo=o("This class cannot be instantiated directly using "),xce=a("code"),IVo=o("__init__()"),DVo=o(" (throws an error)."),jVo=l(),it=a("div"),f(Wy.$$.fragment),NVo=l(),kce=a("p"),qVo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),GVo=l(),Kd=a("p"),OVo=o(`Note:
Loading a model from its configuration file does `),Rce=a("strong"),XVo=o("not"),VVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sce=a("code"),zVo=o("from_pretrained()"),WVo=o("to load the model weights."),QVo=l(),Pce=a("p"),HVo=o("Examples:"),UVo=l(),f(Qy.$$.fragment),JVo=l(),to=a("div"),f(Hy.$$.fragment),YVo=l(),$ce=a("p"),KVo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),ZVo=l(),ln=a("p"),ezo=o("The model class to instantiate is selected based on the "),Ice=a("code"),ozo=o("model_type"),rzo=o(` property of the config object (either
passed as an argument or loaded from `),Dce=a("code"),tzo=o("pretrained_model_name_or_path"),azo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jce=a("code"),nzo=o("pretrained_model_name_or_path"),szo=o(":"),lzo=l(),Nce=a("ul"),$2=a("li"),qce=a("strong"),izo=o("detr"),dzo=o(" \u2014 "),EN=a("a"),czo=o("DetrForSegmentation"),fzo=o(" (DETR model)"),mzo=l(),I2=a("p"),gzo=o("The model is set in evaluation mode by default using "),Gce=a("code"),hzo=o("model.eval()"),pzo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Oce=a("code"),_zo=o("model.train()"),uzo=l(),Xce=a("p"),bzo=o("Examples:"),vzo=l(),f(Uy.$$.fragment),nxe=l(),Zd=a("h2"),D2=a("a"),Vce=a("span"),f(Jy.$$.fragment),Tzo=l(),zce=a("span"),Fzo=o("AutoModelForSemanticSegmentation"),sxe=l(),_r=a("div"),f(Yy.$$.fragment),Czo=l(),ec=a("p"),Mzo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Wce=a("code"),Ezo=o("from_pretrained()"),yzo=o("class method or the "),Qce=a("code"),wzo=o("from_config()"),Azo=o(`class
method.`),Lzo=l(),Ky=a("p"),Bzo=o("This class cannot be instantiated directly using "),Hce=a("code"),xzo=o("__init__()"),kzo=o(" (throws an error)."),Rzo=l(),dt=a("div"),f(Zy.$$.fragment),Szo=l(),Uce=a("p"),Pzo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),$zo=l(),oc=a("p"),Izo=o(`Note:
Loading a model from its configuration file does `),Jce=a("strong"),Dzo=o("not"),jzo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yce=a("code"),Nzo=o("from_pretrained()"),qzo=o("to load the model weights."),Gzo=l(),Kce=a("p"),Ozo=o("Examples:"),Xzo=l(),f(ew.$$.fragment),Vzo=l(),ao=a("div"),f(ow.$$.fragment),zzo=l(),Zce=a("p"),Wzo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),Qzo=l(),dn=a("p"),Hzo=o("The model class to instantiate is selected based on the "),efe=a("code"),Uzo=o("model_type"),Jzo=o(` property of the config object (either
passed as an argument or loaded from `),ofe=a("code"),Yzo=o("pretrained_model_name_or_path"),Kzo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rfe=a("code"),Zzo=o("pretrained_model_name_or_path"),eWo=o(":"),oWo=l(),rw=a("ul"),j2=a("li"),tfe=a("strong"),rWo=o("beit"),tWo=o(" \u2014 "),yN=a("a"),aWo=o("BeitForSemanticSegmentation"),nWo=o(" (BEiT model)"),sWo=l(),N2=a("li"),afe=a("strong"),lWo=o("segformer"),iWo=o(" \u2014 "),wN=a("a"),dWo=o("SegformerForSemanticSegmentation"),cWo=o(" (SegFormer model)"),fWo=l(),q2=a("p"),mWo=o("The model is set in evaluation mode by default using "),nfe=a("code"),gWo=o("model.eval()"),hWo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sfe=a("code"),pWo=o("model.train()"),_Wo=l(),lfe=a("p"),uWo=o("Examples:"),bWo=l(),f(tw.$$.fragment),lxe=l(),rc=a("h2"),G2=a("a"),ife=a("span"),f(aw.$$.fragment),vWo=l(),dfe=a("span"),TWo=o("TFAutoModel"),ixe=l(),ur=a("div"),f(nw.$$.fragment),FWo=l(),tc=a("p"),CWo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),cfe=a("code"),MWo=o("from_pretrained()"),EWo=o("class method or the "),ffe=a("code"),yWo=o("from_config()"),wWo=o(`class
method.`),AWo=l(),sw=a("p"),LWo=o("This class cannot be instantiated directly using "),mfe=a("code"),BWo=o("__init__()"),xWo=o(" (throws an error)."),kWo=l(),ct=a("div"),f(lw.$$.fragment),RWo=l(),gfe=a("p"),SWo=o("Instantiates one of the base model classes of the library from a configuration."),PWo=l(),ac=a("p"),$Wo=o(`Note:
Loading a model from its configuration file does `),hfe=a("strong"),IWo=o("not"),DWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pfe=a("code"),jWo=o("from_pretrained()"),NWo=o("to load the model weights."),qWo=l(),_fe=a("p"),GWo=o("Examples:"),OWo=l(),f(iw.$$.fragment),XWo=l(),go=a("div"),f(dw.$$.fragment),VWo=l(),ufe=a("p"),zWo=o("Instantiate one of the base model classes of the library from a pretrained model."),WWo=l(),cn=a("p"),QWo=o("The model class to instantiate is selected based on the "),bfe=a("code"),HWo=o("model_type"),UWo=o(` property of the config object (either
passed as an argument or loaded from `),vfe=a("code"),JWo=o("pretrained_model_name_or_path"),YWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tfe=a("code"),KWo=o("pretrained_model_name_or_path"),ZWo=o(":"),eQo=l(),B=a("ul"),O2=a("li"),Ffe=a("strong"),oQo=o("albert"),rQo=o(" \u2014 "),AN=a("a"),tQo=o("TFAlbertModel"),aQo=o(" (ALBERT model)"),nQo=l(),X2=a("li"),Cfe=a("strong"),sQo=o("bart"),lQo=o(" \u2014 "),LN=a("a"),iQo=o("TFBartModel"),dQo=o(" (BART model)"),cQo=l(),V2=a("li"),Mfe=a("strong"),fQo=o("bert"),mQo=o(" \u2014 "),BN=a("a"),gQo=o("TFBertModel"),hQo=o(" (BERT model)"),pQo=l(),z2=a("li"),Efe=a("strong"),_Qo=o("blenderbot"),uQo=o(" \u2014 "),xN=a("a"),bQo=o("TFBlenderbotModel"),vQo=o(" (Blenderbot model)"),TQo=l(),W2=a("li"),yfe=a("strong"),FQo=o("blenderbot-small"),CQo=o(" \u2014 "),kN=a("a"),MQo=o("TFBlenderbotSmallModel"),EQo=o(" (BlenderbotSmall model)"),yQo=l(),Q2=a("li"),wfe=a("strong"),wQo=o("camembert"),AQo=o(" \u2014 "),RN=a("a"),LQo=o("TFCamembertModel"),BQo=o(" (CamemBERT model)"),xQo=l(),H2=a("li"),Afe=a("strong"),kQo=o("clip"),RQo=o(" \u2014 "),SN=a("a"),SQo=o("TFCLIPModel"),PQo=o(" (CLIP model)"),$Qo=l(),U2=a("li"),Lfe=a("strong"),IQo=o("convbert"),DQo=o(" \u2014 "),PN=a("a"),jQo=o("TFConvBertModel"),NQo=o(" (ConvBERT model)"),qQo=l(),J2=a("li"),Bfe=a("strong"),GQo=o("convnext"),OQo=o(" \u2014 "),$N=a("a"),XQo=o("TFConvNextModel"),VQo=o(" (ConvNext model)"),zQo=l(),Y2=a("li"),xfe=a("strong"),WQo=o("ctrl"),QQo=o(" \u2014 "),IN=a("a"),HQo=o("TFCTRLModel"),UQo=o(" (CTRL model)"),JQo=l(),K2=a("li"),kfe=a("strong"),YQo=o("deberta"),KQo=o(" \u2014 "),DN=a("a"),ZQo=o("TFDebertaModel"),eHo=o(" (DeBERTa model)"),oHo=l(),Z2=a("li"),Rfe=a("strong"),rHo=o("deberta-v2"),tHo=o(" \u2014 "),jN=a("a"),aHo=o("TFDebertaV2Model"),nHo=o(" (DeBERTa-v2 model)"),sHo=l(),ev=a("li"),Sfe=a("strong"),lHo=o("distilbert"),iHo=o(" \u2014 "),NN=a("a"),dHo=o("TFDistilBertModel"),cHo=o(" (DistilBERT model)"),fHo=l(),ov=a("li"),Pfe=a("strong"),mHo=o("dpr"),gHo=o(" \u2014 "),qN=a("a"),hHo=o("TFDPRQuestionEncoder"),pHo=o(" (DPR model)"),_Ho=l(),rv=a("li"),$fe=a("strong"),uHo=o("electra"),bHo=o(" \u2014 "),GN=a("a"),vHo=o("TFElectraModel"),THo=o(" (ELECTRA model)"),FHo=l(),tv=a("li"),Ife=a("strong"),CHo=o("flaubert"),MHo=o(" \u2014 "),ON=a("a"),EHo=o("TFFlaubertModel"),yHo=o(" (FlauBERT model)"),wHo=l(),Is=a("li"),Dfe=a("strong"),AHo=o("funnel"),LHo=o(" \u2014 "),XN=a("a"),BHo=o("TFFunnelModel"),xHo=o(" or "),VN=a("a"),kHo=o("TFFunnelBaseModel"),RHo=o(" (Funnel Transformer model)"),SHo=l(),av=a("li"),jfe=a("strong"),PHo=o("gpt2"),$Ho=o(" \u2014 "),zN=a("a"),IHo=o("TFGPT2Model"),DHo=o(" (OpenAI GPT-2 model)"),jHo=l(),nv=a("li"),Nfe=a("strong"),NHo=o("hubert"),qHo=o(" \u2014 "),WN=a("a"),GHo=o("TFHubertModel"),OHo=o(" (Hubert model)"),XHo=l(),sv=a("li"),qfe=a("strong"),VHo=o("layoutlm"),zHo=o(" \u2014 "),QN=a("a"),WHo=o("TFLayoutLMModel"),QHo=o(" (LayoutLM model)"),HHo=l(),lv=a("li"),Gfe=a("strong"),UHo=o("led"),JHo=o(" \u2014 "),HN=a("a"),YHo=o("TFLEDModel"),KHo=o(" (LED model)"),ZHo=l(),iv=a("li"),Ofe=a("strong"),eUo=o("longformer"),oUo=o(" \u2014 "),UN=a("a"),rUo=o("TFLongformerModel"),tUo=o(" (Longformer model)"),aUo=l(),dv=a("li"),Xfe=a("strong"),nUo=o("lxmert"),sUo=o(" \u2014 "),JN=a("a"),lUo=o("TFLxmertModel"),iUo=o(" (LXMERT model)"),dUo=l(),cv=a("li"),Vfe=a("strong"),cUo=o("marian"),fUo=o(" \u2014 "),YN=a("a"),mUo=o("TFMarianModel"),gUo=o(" (Marian model)"),hUo=l(),fv=a("li"),zfe=a("strong"),pUo=o("mbart"),_Uo=o(" \u2014 "),KN=a("a"),uUo=o("TFMBartModel"),bUo=o(" (mBART model)"),vUo=l(),mv=a("li"),Wfe=a("strong"),TUo=o("mobilebert"),FUo=o(" \u2014 "),ZN=a("a"),CUo=o("TFMobileBertModel"),MUo=o(" (MobileBERT model)"),EUo=l(),gv=a("li"),Qfe=a("strong"),yUo=o("mpnet"),wUo=o(" \u2014 "),eq=a("a"),AUo=o("TFMPNetModel"),LUo=o(" (MPNet model)"),BUo=l(),hv=a("li"),Hfe=a("strong"),xUo=o("mt5"),kUo=o(" \u2014 "),oq=a("a"),RUo=o("TFMT5Model"),SUo=o(" (mT5 model)"),PUo=l(),pv=a("li"),Ufe=a("strong"),$Uo=o("openai-gpt"),IUo=o(" \u2014 "),rq=a("a"),DUo=o("TFOpenAIGPTModel"),jUo=o(" (OpenAI GPT model)"),NUo=l(),_v=a("li"),Jfe=a("strong"),qUo=o("pegasus"),GUo=o(" \u2014 "),tq=a("a"),OUo=o("TFPegasusModel"),XUo=o(" (Pegasus model)"),VUo=l(),uv=a("li"),Yfe=a("strong"),zUo=o("rembert"),WUo=o(" \u2014 "),aq=a("a"),QUo=o("TFRemBertModel"),HUo=o(" (RemBERT model)"),UUo=l(),bv=a("li"),Kfe=a("strong"),JUo=o("roberta"),YUo=o(" \u2014 "),nq=a("a"),KUo=o("TFRobertaModel"),ZUo=o(" (RoBERTa model)"),eJo=l(),vv=a("li"),Zfe=a("strong"),oJo=o("roformer"),rJo=o(" \u2014 "),sq=a("a"),tJo=o("TFRoFormerModel"),aJo=o(" (RoFormer model)"),nJo=l(),Tv=a("li"),eme=a("strong"),sJo=o("speech_to_text"),lJo=o(" \u2014 "),lq=a("a"),iJo=o("TFSpeech2TextModel"),dJo=o(" (Speech2Text model)"),cJo=l(),Fv=a("li"),ome=a("strong"),fJo=o("t5"),mJo=o(" \u2014 "),iq=a("a"),gJo=o("TFT5Model"),hJo=o(" (T5 model)"),pJo=l(),Cv=a("li"),rme=a("strong"),_Jo=o("tapas"),uJo=o(" \u2014 "),dq=a("a"),bJo=o("TFTapasModel"),vJo=o(" (TAPAS model)"),TJo=l(),Mv=a("li"),tme=a("strong"),FJo=o("transfo-xl"),CJo=o(" \u2014 "),cq=a("a"),MJo=o("TFTransfoXLModel"),EJo=o(" (Transformer-XL model)"),yJo=l(),Ev=a("li"),ame=a("strong"),wJo=o("vit"),AJo=o(" \u2014 "),fq=a("a"),LJo=o("TFViTModel"),BJo=o(" (ViT model)"),xJo=l(),yv=a("li"),nme=a("strong"),kJo=o("wav2vec2"),RJo=o(" \u2014 "),mq=a("a"),SJo=o("TFWav2Vec2Model"),PJo=o(" (Wav2Vec2 model)"),$Jo=l(),wv=a("li"),sme=a("strong"),IJo=o("xlm"),DJo=o(" \u2014 "),gq=a("a"),jJo=o("TFXLMModel"),NJo=o(" (XLM model)"),qJo=l(),Av=a("li"),lme=a("strong"),GJo=o("xlm-roberta"),OJo=o(" \u2014 "),hq=a("a"),XJo=o("TFXLMRobertaModel"),VJo=o(" (XLM-RoBERTa model)"),zJo=l(),Lv=a("li"),ime=a("strong"),WJo=o("xlnet"),QJo=o(" \u2014 "),pq=a("a"),HJo=o("TFXLNetModel"),UJo=o(" (XLNet model)"),JJo=l(),dme=a("p"),YJo=o("Examples:"),KJo=l(),f(cw.$$.fragment),dxe=l(),nc=a("h2"),Bv=a("a"),cme=a("span"),f(fw.$$.fragment),ZJo=l(),fme=a("span"),eYo=o("TFAutoModelForPreTraining"),cxe=l(),br=a("div"),f(mw.$$.fragment),oYo=l(),sc=a("p"),rYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),mme=a("code"),tYo=o("from_pretrained()"),aYo=o("class method or the "),gme=a("code"),nYo=o("from_config()"),sYo=o(`class
method.`),lYo=l(),gw=a("p"),iYo=o("This class cannot be instantiated directly using "),hme=a("code"),dYo=o("__init__()"),cYo=o(" (throws an error)."),fYo=l(),ft=a("div"),f(hw.$$.fragment),mYo=l(),pme=a("p"),gYo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),hYo=l(),lc=a("p"),pYo=o(`Note:
Loading a model from its configuration file does `),_me=a("strong"),_Yo=o("not"),uYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=a("code"),bYo=o("from_pretrained()"),vYo=o("to load the model weights."),TYo=l(),bme=a("p"),FYo=o("Examples:"),CYo=l(),f(pw.$$.fragment),MYo=l(),ho=a("div"),f(_w.$$.fragment),EYo=l(),vme=a("p"),yYo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),wYo=l(),fn=a("p"),AYo=o("The model class to instantiate is selected based on the "),Tme=a("code"),LYo=o("model_type"),BYo=o(` property of the config object (either
passed as an argument or loaded from `),Fme=a("code"),xYo=o("pretrained_model_name_or_path"),kYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=a("code"),RYo=o("pretrained_model_name_or_path"),SYo=o(":"),PYo=l(),H=a("ul"),xv=a("li"),Mme=a("strong"),$Yo=o("albert"),IYo=o(" \u2014 "),_q=a("a"),DYo=o("TFAlbertForPreTraining"),jYo=o(" (ALBERT model)"),NYo=l(),kv=a("li"),Eme=a("strong"),qYo=o("bart"),GYo=o(" \u2014 "),uq=a("a"),OYo=o("TFBartForConditionalGeneration"),XYo=o(" (BART model)"),VYo=l(),Rv=a("li"),yme=a("strong"),zYo=o("bert"),WYo=o(" \u2014 "),bq=a("a"),QYo=o("TFBertForPreTraining"),HYo=o(" (BERT model)"),UYo=l(),Sv=a("li"),wme=a("strong"),JYo=o("camembert"),YYo=o(" \u2014 "),vq=a("a"),KYo=o("TFCamembertForMaskedLM"),ZYo=o(" (CamemBERT model)"),eKo=l(),Pv=a("li"),Ame=a("strong"),oKo=o("ctrl"),rKo=o(" \u2014 "),Tq=a("a"),tKo=o("TFCTRLLMHeadModel"),aKo=o(" (CTRL model)"),nKo=l(),$v=a("li"),Lme=a("strong"),sKo=o("distilbert"),lKo=o(" \u2014 "),Fq=a("a"),iKo=o("TFDistilBertForMaskedLM"),dKo=o(" (DistilBERT model)"),cKo=l(),Iv=a("li"),Bme=a("strong"),fKo=o("electra"),mKo=o(" \u2014 "),Cq=a("a"),gKo=o("TFElectraForPreTraining"),hKo=o(" (ELECTRA model)"),pKo=l(),Dv=a("li"),xme=a("strong"),_Ko=o("flaubert"),uKo=o(" \u2014 "),Mq=a("a"),bKo=o("TFFlaubertWithLMHeadModel"),vKo=o(" (FlauBERT model)"),TKo=l(),jv=a("li"),kme=a("strong"),FKo=o("funnel"),CKo=o(" \u2014 "),Eq=a("a"),MKo=o("TFFunnelForPreTraining"),EKo=o(" (Funnel Transformer model)"),yKo=l(),Nv=a("li"),Rme=a("strong"),wKo=o("gpt2"),AKo=o(" \u2014 "),yq=a("a"),LKo=o("TFGPT2LMHeadModel"),BKo=o(" (OpenAI GPT-2 model)"),xKo=l(),qv=a("li"),Sme=a("strong"),kKo=o("layoutlm"),RKo=o(" \u2014 "),wq=a("a"),SKo=o("TFLayoutLMForMaskedLM"),PKo=o(" (LayoutLM model)"),$Ko=l(),Gv=a("li"),Pme=a("strong"),IKo=o("lxmert"),DKo=o(" \u2014 "),Aq=a("a"),jKo=o("TFLxmertForPreTraining"),NKo=o(" (LXMERT model)"),qKo=l(),Ov=a("li"),$me=a("strong"),GKo=o("mobilebert"),OKo=o(" \u2014 "),Lq=a("a"),XKo=o("TFMobileBertForPreTraining"),VKo=o(" (MobileBERT model)"),zKo=l(),Xv=a("li"),Ime=a("strong"),WKo=o("mpnet"),QKo=o(" \u2014 "),Bq=a("a"),HKo=o("TFMPNetForMaskedLM"),UKo=o(" (MPNet model)"),JKo=l(),Vv=a("li"),Dme=a("strong"),YKo=o("openai-gpt"),KKo=o(" \u2014 "),xq=a("a"),ZKo=o("TFOpenAIGPTLMHeadModel"),eZo=o(" (OpenAI GPT model)"),oZo=l(),zv=a("li"),jme=a("strong"),rZo=o("roberta"),tZo=o(" \u2014 "),kq=a("a"),aZo=o("TFRobertaForMaskedLM"),nZo=o(" (RoBERTa model)"),sZo=l(),Wv=a("li"),Nme=a("strong"),lZo=o("t5"),iZo=o(" \u2014 "),Rq=a("a"),dZo=o("TFT5ForConditionalGeneration"),cZo=o(" (T5 model)"),fZo=l(),Qv=a("li"),qme=a("strong"),mZo=o("tapas"),gZo=o(" \u2014 "),Sq=a("a"),hZo=o("TFTapasForMaskedLM"),pZo=o(" (TAPAS model)"),_Zo=l(),Hv=a("li"),Gme=a("strong"),uZo=o("transfo-xl"),bZo=o(" \u2014 "),Pq=a("a"),vZo=o("TFTransfoXLLMHeadModel"),TZo=o(" (Transformer-XL model)"),FZo=l(),Uv=a("li"),Ome=a("strong"),CZo=o("xlm"),MZo=o(" \u2014 "),$q=a("a"),EZo=o("TFXLMWithLMHeadModel"),yZo=o(" (XLM model)"),wZo=l(),Jv=a("li"),Xme=a("strong"),AZo=o("xlm-roberta"),LZo=o(" \u2014 "),Iq=a("a"),BZo=o("TFXLMRobertaForMaskedLM"),xZo=o(" (XLM-RoBERTa model)"),kZo=l(),Yv=a("li"),Vme=a("strong"),RZo=o("xlnet"),SZo=o(" \u2014 "),Dq=a("a"),PZo=o("TFXLNetLMHeadModel"),$Zo=o(" (XLNet model)"),IZo=l(),zme=a("p"),DZo=o("Examples:"),jZo=l(),f(uw.$$.fragment),fxe=l(),ic=a("h2"),Kv=a("a"),Wme=a("span"),f(bw.$$.fragment),NZo=l(),Qme=a("span"),qZo=o("TFAutoModelForCausalLM"),mxe=l(),vr=a("div"),f(vw.$$.fragment),GZo=l(),dc=a("p"),OZo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Hme=a("code"),XZo=o("from_pretrained()"),VZo=o("class method or the "),Ume=a("code"),zZo=o("from_config()"),WZo=o(`class
method.`),QZo=l(),Tw=a("p"),HZo=o("This class cannot be instantiated directly using "),Jme=a("code"),UZo=o("__init__()"),JZo=o(" (throws an error)."),YZo=l(),mt=a("div"),f(Fw.$$.fragment),KZo=l(),Yme=a("p"),ZZo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),eer=l(),cc=a("p"),oer=o(`Note:
Loading a model from its configuration file does `),Kme=a("strong"),rer=o("not"),ter=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zme=a("code"),aer=o("from_pretrained()"),ner=o("to load the model weights."),ser=l(),ege=a("p"),ler=o("Examples:"),ier=l(),f(Cw.$$.fragment),der=l(),po=a("div"),f(Mw.$$.fragment),cer=l(),oge=a("p"),fer=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),mer=l(),mn=a("p"),ger=o("The model class to instantiate is selected based on the "),rge=a("code"),her=o("model_type"),per=o(` property of the config object (either
passed as an argument or loaded from `),tge=a("code"),_er=o("pretrained_model_name_or_path"),uer=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),age=a("code"),ber=o("pretrained_model_name_or_path"),ver=o(":"),Ter=l(),pe=a("ul"),Zv=a("li"),nge=a("strong"),Fer=o("bert"),Cer=o(" \u2014 "),jq=a("a"),Mer=o("TFBertLMHeadModel"),Eer=o(" (BERT model)"),yer=l(),eT=a("li"),sge=a("strong"),wer=o("ctrl"),Aer=o(" \u2014 "),Nq=a("a"),Ler=o("TFCTRLLMHeadModel"),Ber=o(" (CTRL model)"),xer=l(),oT=a("li"),lge=a("strong"),ker=o("gpt2"),Rer=o(" \u2014 "),qq=a("a"),Ser=o("TFGPT2LMHeadModel"),Per=o(" (OpenAI GPT-2 model)"),$er=l(),rT=a("li"),ige=a("strong"),Ier=o("openai-gpt"),Der=o(" \u2014 "),Gq=a("a"),jer=o("TFOpenAIGPTLMHeadModel"),Ner=o(" (OpenAI GPT model)"),qer=l(),tT=a("li"),dge=a("strong"),Ger=o("rembert"),Oer=o(" \u2014 "),Oq=a("a"),Xer=o("TFRemBertForCausalLM"),Ver=o(" (RemBERT model)"),zer=l(),aT=a("li"),cge=a("strong"),Wer=o("roberta"),Qer=o(" \u2014 "),Xq=a("a"),Her=o("TFRobertaForCausalLM"),Uer=o(" (RoBERTa model)"),Jer=l(),nT=a("li"),fge=a("strong"),Yer=o("roformer"),Ker=o(" \u2014 "),Vq=a("a"),Zer=o("TFRoFormerForCausalLM"),eor=o(" (RoFormer model)"),oor=l(),sT=a("li"),mge=a("strong"),ror=o("transfo-xl"),tor=o(" \u2014 "),zq=a("a"),aor=o("TFTransfoXLLMHeadModel"),nor=o(" (Transformer-XL model)"),sor=l(),lT=a("li"),gge=a("strong"),lor=o("xlm"),ior=o(" \u2014 "),Wq=a("a"),dor=o("TFXLMWithLMHeadModel"),cor=o(" (XLM model)"),mor=l(),iT=a("li"),hge=a("strong"),gor=o("xlnet"),hor=o(" \u2014 "),Qq=a("a"),por=o("TFXLNetLMHeadModel"),_or=o(" (XLNet model)"),uor=l(),pge=a("p"),bor=o("Examples:"),vor=l(),f(Ew.$$.fragment),gxe=l(),fc=a("h2"),dT=a("a"),_ge=a("span"),f(yw.$$.fragment),Tor=l(),uge=a("span"),For=o("TFAutoModelForImageClassification"),hxe=l(),Tr=a("div"),f(ww.$$.fragment),Cor=l(),mc=a("p"),Mor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),bge=a("code"),Eor=o("from_pretrained()"),yor=o("class method or the "),vge=a("code"),wor=o("from_config()"),Aor=o(`class
method.`),Lor=l(),Aw=a("p"),Bor=o("This class cannot be instantiated directly using "),Tge=a("code"),xor=o("__init__()"),kor=o(" (throws an error)."),Ror=l(),gt=a("div"),f(Lw.$$.fragment),Sor=l(),Fge=a("p"),Por=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),$or=l(),gc=a("p"),Ior=o(`Note:
Loading a model from its configuration file does `),Cge=a("strong"),Dor=o("not"),jor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mge=a("code"),Nor=o("from_pretrained()"),qor=o("to load the model weights."),Gor=l(),Ege=a("p"),Oor=o("Examples:"),Xor=l(),f(Bw.$$.fragment),Vor=l(),_o=a("div"),f(xw.$$.fragment),zor=l(),yge=a("p"),Wor=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Qor=l(),gn=a("p"),Hor=o("The model class to instantiate is selected based on the "),wge=a("code"),Uor=o("model_type"),Jor=o(` property of the config object (either
passed as an argument or loaded from `),Age=a("code"),Yor=o("pretrained_model_name_or_path"),Kor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lge=a("code"),Zor=o("pretrained_model_name_or_path"),err=o(":"),orr=l(),kw=a("ul"),cT=a("li"),Bge=a("strong"),rrr=o("convnext"),trr=o(" \u2014 "),Hq=a("a"),arr=o("TFConvNextForImageClassification"),nrr=o(" (ConvNext model)"),srr=l(),fT=a("li"),xge=a("strong"),lrr=o("vit"),irr=o(" \u2014 "),Uq=a("a"),drr=o("TFViTForImageClassification"),crr=o(" (ViT model)"),frr=l(),kge=a("p"),mrr=o("Examples:"),grr=l(),f(Rw.$$.fragment),pxe=l(),hc=a("h2"),mT=a("a"),Rge=a("span"),f(Sw.$$.fragment),hrr=l(),Sge=a("span"),prr=o("TFAutoModelForMaskedLM"),_xe=l(),Fr=a("div"),f(Pw.$$.fragment),_rr=l(),pc=a("p"),urr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Pge=a("code"),brr=o("from_pretrained()"),vrr=o("class method or the "),$ge=a("code"),Trr=o("from_config()"),Frr=o(`class
method.`),Crr=l(),$w=a("p"),Mrr=o("This class cannot be instantiated directly using "),Ige=a("code"),Err=o("__init__()"),yrr=o(" (throws an error)."),wrr=l(),ht=a("div"),f(Iw.$$.fragment),Arr=l(),Dge=a("p"),Lrr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Brr=l(),_c=a("p"),xrr=o(`Note:
Loading a model from its configuration file does `),jge=a("strong"),krr=o("not"),Rrr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Nge=a("code"),Srr=o("from_pretrained()"),Prr=o("to load the model weights."),$rr=l(),qge=a("p"),Irr=o("Examples:"),Drr=l(),f(Dw.$$.fragment),jrr=l(),uo=a("div"),f(jw.$$.fragment),Nrr=l(),Gge=a("p"),qrr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Grr=l(),hn=a("p"),Orr=o("The model class to instantiate is selected based on the "),Oge=a("code"),Xrr=o("model_type"),Vrr=o(` property of the config object (either
passed as an argument or loaded from `),Xge=a("code"),zrr=o("pretrained_model_name_or_path"),Wrr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vge=a("code"),Qrr=o("pretrained_model_name_or_path"),Hrr=o(":"),Urr=l(),Y=a("ul"),gT=a("li"),zge=a("strong"),Jrr=o("albert"),Yrr=o(" \u2014 "),Jq=a("a"),Krr=o("TFAlbertForMaskedLM"),Zrr=o(" (ALBERT model)"),etr=l(),hT=a("li"),Wge=a("strong"),otr=o("bert"),rtr=o(" \u2014 "),Yq=a("a"),ttr=o("TFBertForMaskedLM"),atr=o(" (BERT model)"),ntr=l(),pT=a("li"),Qge=a("strong"),str=o("camembert"),ltr=o(" \u2014 "),Kq=a("a"),itr=o("TFCamembertForMaskedLM"),dtr=o(" (CamemBERT model)"),ctr=l(),_T=a("li"),Hge=a("strong"),ftr=o("convbert"),mtr=o(" \u2014 "),Zq=a("a"),gtr=o("TFConvBertForMaskedLM"),htr=o(" (ConvBERT model)"),ptr=l(),uT=a("li"),Uge=a("strong"),_tr=o("deberta"),utr=o(" \u2014 "),eG=a("a"),btr=o("TFDebertaForMaskedLM"),vtr=o(" (DeBERTa model)"),Ttr=l(),bT=a("li"),Jge=a("strong"),Ftr=o("deberta-v2"),Ctr=o(" \u2014 "),oG=a("a"),Mtr=o("TFDebertaV2ForMaskedLM"),Etr=o(" (DeBERTa-v2 model)"),ytr=l(),vT=a("li"),Yge=a("strong"),wtr=o("distilbert"),Atr=o(" \u2014 "),rG=a("a"),Ltr=o("TFDistilBertForMaskedLM"),Btr=o(" (DistilBERT model)"),xtr=l(),TT=a("li"),Kge=a("strong"),ktr=o("electra"),Rtr=o(" \u2014 "),tG=a("a"),Str=o("TFElectraForMaskedLM"),Ptr=o(" (ELECTRA model)"),$tr=l(),FT=a("li"),Zge=a("strong"),Itr=o("flaubert"),Dtr=o(" \u2014 "),aG=a("a"),jtr=o("TFFlaubertWithLMHeadModel"),Ntr=o(" (FlauBERT model)"),qtr=l(),CT=a("li"),ehe=a("strong"),Gtr=o("funnel"),Otr=o(" \u2014 "),nG=a("a"),Xtr=o("TFFunnelForMaskedLM"),Vtr=o(" (Funnel Transformer model)"),ztr=l(),MT=a("li"),ohe=a("strong"),Wtr=o("layoutlm"),Qtr=o(" \u2014 "),sG=a("a"),Htr=o("TFLayoutLMForMaskedLM"),Utr=o(" (LayoutLM model)"),Jtr=l(),ET=a("li"),rhe=a("strong"),Ytr=o("longformer"),Ktr=o(" \u2014 "),lG=a("a"),Ztr=o("TFLongformerForMaskedLM"),ear=o(" (Longformer model)"),oar=l(),yT=a("li"),the=a("strong"),rar=o("mobilebert"),tar=o(" \u2014 "),iG=a("a"),aar=o("TFMobileBertForMaskedLM"),nar=o(" (MobileBERT model)"),sar=l(),wT=a("li"),ahe=a("strong"),lar=o("mpnet"),iar=o(" \u2014 "),dG=a("a"),dar=o("TFMPNetForMaskedLM"),car=o(" (MPNet model)"),far=l(),AT=a("li"),nhe=a("strong"),mar=o("rembert"),gar=o(" \u2014 "),cG=a("a"),har=o("TFRemBertForMaskedLM"),par=o(" (RemBERT model)"),_ar=l(),LT=a("li"),she=a("strong"),uar=o("roberta"),bar=o(" \u2014 "),fG=a("a"),Tar=o("TFRobertaForMaskedLM"),Far=o(" (RoBERTa model)"),Car=l(),BT=a("li"),lhe=a("strong"),Mar=o("roformer"),Ear=o(" \u2014 "),mG=a("a"),yar=o("TFRoFormerForMaskedLM"),war=o(" (RoFormer model)"),Aar=l(),xT=a("li"),ihe=a("strong"),Lar=o("tapas"),Bar=o(" \u2014 "),gG=a("a"),xar=o("TFTapasForMaskedLM"),kar=o(" (TAPAS model)"),Rar=l(),kT=a("li"),dhe=a("strong"),Sar=o("xlm"),Par=o(" \u2014 "),hG=a("a"),$ar=o("TFXLMWithLMHeadModel"),Iar=o(" (XLM model)"),Dar=l(),RT=a("li"),che=a("strong"),jar=o("xlm-roberta"),Nar=o(" \u2014 "),pG=a("a"),qar=o("TFXLMRobertaForMaskedLM"),Gar=o(" (XLM-RoBERTa model)"),Oar=l(),fhe=a("p"),Xar=o("Examples:"),Var=l(),f(Nw.$$.fragment),uxe=l(),uc=a("h2"),ST=a("a"),mhe=a("span"),f(qw.$$.fragment),zar=l(),ghe=a("span"),War=o("TFAutoModelForSeq2SeqLM"),bxe=l(),Cr=a("div"),f(Gw.$$.fragment),Qar=l(),bc=a("p"),Har=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),hhe=a("code"),Uar=o("from_pretrained()"),Jar=o("class method or the "),phe=a("code"),Yar=o("from_config()"),Kar=o(`class
method.`),Zar=l(),Ow=a("p"),enr=o("This class cannot be instantiated directly using "),_he=a("code"),onr=o("__init__()"),rnr=o(" (throws an error)."),tnr=l(),pt=a("div"),f(Xw.$$.fragment),anr=l(),uhe=a("p"),nnr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),snr=l(),vc=a("p"),lnr=o(`Note:
Loading a model from its configuration file does `),bhe=a("strong"),inr=o("not"),dnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vhe=a("code"),cnr=o("from_pretrained()"),fnr=o("to load the model weights."),mnr=l(),The=a("p"),gnr=o("Examples:"),hnr=l(),f(Vw.$$.fragment),pnr=l(),bo=a("div"),f(zw.$$.fragment),_nr=l(),Fhe=a("p"),unr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),bnr=l(),pn=a("p"),vnr=o("The model class to instantiate is selected based on the "),Che=a("code"),Tnr=o("model_type"),Fnr=o(` property of the config object (either
passed as an argument or loaded from `),Mhe=a("code"),Cnr=o("pretrained_model_name_or_path"),Mnr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ehe=a("code"),Enr=o("pretrained_model_name_or_path"),ynr=o(":"),wnr=l(),_e=a("ul"),PT=a("li"),yhe=a("strong"),Anr=o("bart"),Lnr=o(" \u2014 "),_G=a("a"),Bnr=o("TFBartForConditionalGeneration"),xnr=o(" (BART model)"),knr=l(),$T=a("li"),whe=a("strong"),Rnr=o("blenderbot"),Snr=o(" \u2014 "),uG=a("a"),Pnr=o("TFBlenderbotForConditionalGeneration"),$nr=o(" (Blenderbot model)"),Inr=l(),IT=a("li"),Ahe=a("strong"),Dnr=o("blenderbot-small"),jnr=o(" \u2014 "),bG=a("a"),Nnr=o("TFBlenderbotSmallForConditionalGeneration"),qnr=o(" (BlenderbotSmall model)"),Gnr=l(),DT=a("li"),Lhe=a("strong"),Onr=o("encoder-decoder"),Xnr=o(" \u2014 "),vG=a("a"),Vnr=o("TFEncoderDecoderModel"),znr=o(" (Encoder decoder model)"),Wnr=l(),jT=a("li"),Bhe=a("strong"),Qnr=o("led"),Hnr=o(" \u2014 "),TG=a("a"),Unr=o("TFLEDForConditionalGeneration"),Jnr=o(" (LED model)"),Ynr=l(),NT=a("li"),xhe=a("strong"),Knr=o("marian"),Znr=o(" \u2014 "),FG=a("a"),esr=o("TFMarianMTModel"),osr=o(" (Marian model)"),rsr=l(),qT=a("li"),khe=a("strong"),tsr=o("mbart"),asr=o(" \u2014 "),CG=a("a"),nsr=o("TFMBartForConditionalGeneration"),ssr=o(" (mBART model)"),lsr=l(),GT=a("li"),Rhe=a("strong"),isr=o("mt5"),dsr=o(" \u2014 "),MG=a("a"),csr=o("TFMT5ForConditionalGeneration"),fsr=o(" (mT5 model)"),msr=l(),OT=a("li"),She=a("strong"),gsr=o("pegasus"),hsr=o(" \u2014 "),EG=a("a"),psr=o("TFPegasusForConditionalGeneration"),_sr=o(" (Pegasus model)"),usr=l(),XT=a("li"),Phe=a("strong"),bsr=o("t5"),vsr=o(" \u2014 "),yG=a("a"),Tsr=o("TFT5ForConditionalGeneration"),Fsr=o(" (T5 model)"),Csr=l(),$he=a("p"),Msr=o("Examples:"),Esr=l(),f(Ww.$$.fragment),vxe=l(),Tc=a("h2"),VT=a("a"),Ihe=a("span"),f(Qw.$$.fragment),ysr=l(),Dhe=a("span"),wsr=o("TFAutoModelForSequenceClassification"),Txe=l(),Mr=a("div"),f(Hw.$$.fragment),Asr=l(),Fc=a("p"),Lsr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),jhe=a("code"),Bsr=o("from_pretrained()"),xsr=o("class method or the "),Nhe=a("code"),ksr=o("from_config()"),Rsr=o(`class
method.`),Ssr=l(),Uw=a("p"),Psr=o("This class cannot be instantiated directly using "),qhe=a("code"),$sr=o("__init__()"),Isr=o(" (throws an error)."),Dsr=l(),_t=a("div"),f(Jw.$$.fragment),jsr=l(),Ghe=a("p"),Nsr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),qsr=l(),Cc=a("p"),Gsr=o(`Note:
Loading a model from its configuration file does `),Ohe=a("strong"),Osr=o("not"),Xsr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xhe=a("code"),Vsr=o("from_pretrained()"),zsr=o("to load the model weights."),Wsr=l(),Vhe=a("p"),Qsr=o("Examples:"),Hsr=l(),f(Yw.$$.fragment),Usr=l(),vo=a("div"),f(Kw.$$.fragment),Jsr=l(),zhe=a("p"),Ysr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Ksr=l(),_n=a("p"),Zsr=o("The model class to instantiate is selected based on the "),Whe=a("code"),elr=o("model_type"),olr=o(` property of the config object (either
passed as an argument or loaded from `),Qhe=a("code"),rlr=o("pretrained_model_name_or_path"),tlr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hhe=a("code"),alr=o("pretrained_model_name_or_path"),nlr=o(":"),slr=l(),X=a("ul"),zT=a("li"),Uhe=a("strong"),llr=o("albert"),ilr=o(" \u2014 "),wG=a("a"),dlr=o("TFAlbertForSequenceClassification"),clr=o(" (ALBERT model)"),flr=l(),WT=a("li"),Jhe=a("strong"),mlr=o("bert"),glr=o(" \u2014 "),AG=a("a"),hlr=o("TFBertForSequenceClassification"),plr=o(" (BERT model)"),_lr=l(),QT=a("li"),Yhe=a("strong"),ulr=o("camembert"),blr=o(" \u2014 "),LG=a("a"),vlr=o("TFCamembertForSequenceClassification"),Tlr=o(" (CamemBERT model)"),Flr=l(),HT=a("li"),Khe=a("strong"),Clr=o("convbert"),Mlr=o(" \u2014 "),BG=a("a"),Elr=o("TFConvBertForSequenceClassification"),ylr=o(" (ConvBERT model)"),wlr=l(),UT=a("li"),Zhe=a("strong"),Alr=o("ctrl"),Llr=o(" \u2014 "),xG=a("a"),Blr=o("TFCTRLForSequenceClassification"),xlr=o(" (CTRL model)"),klr=l(),JT=a("li"),epe=a("strong"),Rlr=o("deberta"),Slr=o(" \u2014 "),kG=a("a"),Plr=o("TFDebertaForSequenceClassification"),$lr=o(" (DeBERTa model)"),Ilr=l(),YT=a("li"),ope=a("strong"),Dlr=o("deberta-v2"),jlr=o(" \u2014 "),RG=a("a"),Nlr=o("TFDebertaV2ForSequenceClassification"),qlr=o(" (DeBERTa-v2 model)"),Glr=l(),KT=a("li"),rpe=a("strong"),Olr=o("distilbert"),Xlr=o(" \u2014 "),SG=a("a"),Vlr=o("TFDistilBertForSequenceClassification"),zlr=o(" (DistilBERT model)"),Wlr=l(),ZT=a("li"),tpe=a("strong"),Qlr=o("electra"),Hlr=o(" \u2014 "),PG=a("a"),Ulr=o("TFElectraForSequenceClassification"),Jlr=o(" (ELECTRA model)"),Ylr=l(),eF=a("li"),ape=a("strong"),Klr=o("flaubert"),Zlr=o(" \u2014 "),$G=a("a"),eir=o("TFFlaubertForSequenceClassification"),oir=o(" (FlauBERT model)"),rir=l(),oF=a("li"),npe=a("strong"),tir=o("funnel"),air=o(" \u2014 "),IG=a("a"),nir=o("TFFunnelForSequenceClassification"),sir=o(" (Funnel Transformer model)"),lir=l(),rF=a("li"),spe=a("strong"),iir=o("gpt2"),dir=o(" \u2014 "),DG=a("a"),cir=o("TFGPT2ForSequenceClassification"),fir=o(" (OpenAI GPT-2 model)"),mir=l(),tF=a("li"),lpe=a("strong"),gir=o("layoutlm"),hir=o(" \u2014 "),jG=a("a"),pir=o("TFLayoutLMForSequenceClassification"),_ir=o(" (LayoutLM model)"),uir=l(),aF=a("li"),ipe=a("strong"),bir=o("longformer"),vir=o(" \u2014 "),NG=a("a"),Tir=o("TFLongformerForSequenceClassification"),Fir=o(" (Longformer model)"),Cir=l(),nF=a("li"),dpe=a("strong"),Mir=o("mobilebert"),Eir=o(" \u2014 "),qG=a("a"),yir=o("TFMobileBertForSequenceClassification"),wir=o(" (MobileBERT model)"),Air=l(),sF=a("li"),cpe=a("strong"),Lir=o("mpnet"),Bir=o(" \u2014 "),GG=a("a"),xir=o("TFMPNetForSequenceClassification"),kir=o(" (MPNet model)"),Rir=l(),lF=a("li"),fpe=a("strong"),Sir=o("openai-gpt"),Pir=o(" \u2014 "),OG=a("a"),$ir=o("TFOpenAIGPTForSequenceClassification"),Iir=o(" (OpenAI GPT model)"),Dir=l(),iF=a("li"),mpe=a("strong"),jir=o("rembert"),Nir=o(" \u2014 "),XG=a("a"),qir=o("TFRemBertForSequenceClassification"),Gir=o(" (RemBERT model)"),Oir=l(),dF=a("li"),gpe=a("strong"),Xir=o("roberta"),Vir=o(" \u2014 "),VG=a("a"),zir=o("TFRobertaForSequenceClassification"),Wir=o(" (RoBERTa model)"),Qir=l(),cF=a("li"),hpe=a("strong"),Hir=o("roformer"),Uir=o(" \u2014 "),zG=a("a"),Jir=o("TFRoFormerForSequenceClassification"),Yir=o(" (RoFormer model)"),Kir=l(),fF=a("li"),ppe=a("strong"),Zir=o("tapas"),edr=o(" \u2014 "),WG=a("a"),odr=o("TFTapasForSequenceClassification"),rdr=o(" (TAPAS model)"),tdr=l(),mF=a("li"),_pe=a("strong"),adr=o("transfo-xl"),ndr=o(" \u2014 "),QG=a("a"),sdr=o("TFTransfoXLForSequenceClassification"),ldr=o(" (Transformer-XL model)"),idr=l(),gF=a("li"),upe=a("strong"),ddr=o("xlm"),cdr=o(" \u2014 "),HG=a("a"),fdr=o("TFXLMForSequenceClassification"),mdr=o(" (XLM model)"),gdr=l(),hF=a("li"),bpe=a("strong"),hdr=o("xlm-roberta"),pdr=o(" \u2014 "),UG=a("a"),_dr=o("TFXLMRobertaForSequenceClassification"),udr=o(" (XLM-RoBERTa model)"),bdr=l(),pF=a("li"),vpe=a("strong"),vdr=o("xlnet"),Tdr=o(" \u2014 "),JG=a("a"),Fdr=o("TFXLNetForSequenceClassification"),Cdr=o(" (XLNet model)"),Mdr=l(),Tpe=a("p"),Edr=o("Examples:"),ydr=l(),f(Zw.$$.fragment),Fxe=l(),Mc=a("h2"),_F=a("a"),Fpe=a("span"),f(e6.$$.fragment),wdr=l(),Cpe=a("span"),Adr=o("TFAutoModelForMultipleChoice"),Cxe=l(),Er=a("div"),f(o6.$$.fragment),Ldr=l(),Ec=a("p"),Bdr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Mpe=a("code"),xdr=o("from_pretrained()"),kdr=o("class method or the "),Epe=a("code"),Rdr=o("from_config()"),Sdr=o(`class
method.`),Pdr=l(),r6=a("p"),$dr=o("This class cannot be instantiated directly using "),ype=a("code"),Idr=o("__init__()"),Ddr=o(" (throws an error)."),jdr=l(),ut=a("div"),f(t6.$$.fragment),Ndr=l(),wpe=a("p"),qdr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Gdr=l(),yc=a("p"),Odr=o(`Note:
Loading a model from its configuration file does `),Ape=a("strong"),Xdr=o("not"),Vdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lpe=a("code"),zdr=o("from_pretrained()"),Wdr=o("to load the model weights."),Qdr=l(),Bpe=a("p"),Hdr=o("Examples:"),Udr=l(),f(a6.$$.fragment),Jdr=l(),To=a("div"),f(n6.$$.fragment),Ydr=l(),xpe=a("p"),Kdr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Zdr=l(),un=a("p"),ecr=o("The model class to instantiate is selected based on the "),kpe=a("code"),ocr=o("model_type"),rcr=o(` property of the config object (either
passed as an argument or loaded from `),Rpe=a("code"),tcr=o("pretrained_model_name_or_path"),acr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Spe=a("code"),ncr=o("pretrained_model_name_or_path"),scr=o(":"),lcr=l(),te=a("ul"),uF=a("li"),Ppe=a("strong"),icr=o("albert"),dcr=o(" \u2014 "),YG=a("a"),ccr=o("TFAlbertForMultipleChoice"),fcr=o(" (ALBERT model)"),mcr=l(),bF=a("li"),$pe=a("strong"),gcr=o("bert"),hcr=o(" \u2014 "),KG=a("a"),pcr=o("TFBertForMultipleChoice"),_cr=o(" (BERT model)"),ucr=l(),vF=a("li"),Ipe=a("strong"),bcr=o("camembert"),vcr=o(" \u2014 "),ZG=a("a"),Tcr=o("TFCamembertForMultipleChoice"),Fcr=o(" (CamemBERT model)"),Ccr=l(),TF=a("li"),Dpe=a("strong"),Mcr=o("convbert"),Ecr=o(" \u2014 "),eO=a("a"),ycr=o("TFConvBertForMultipleChoice"),wcr=o(" (ConvBERT model)"),Acr=l(),FF=a("li"),jpe=a("strong"),Lcr=o("distilbert"),Bcr=o(" \u2014 "),oO=a("a"),xcr=o("TFDistilBertForMultipleChoice"),kcr=o(" (DistilBERT model)"),Rcr=l(),CF=a("li"),Npe=a("strong"),Scr=o("electra"),Pcr=o(" \u2014 "),rO=a("a"),$cr=o("TFElectraForMultipleChoice"),Icr=o(" (ELECTRA model)"),Dcr=l(),MF=a("li"),qpe=a("strong"),jcr=o("flaubert"),Ncr=o(" \u2014 "),tO=a("a"),qcr=o("TFFlaubertForMultipleChoice"),Gcr=o(" (FlauBERT model)"),Ocr=l(),EF=a("li"),Gpe=a("strong"),Xcr=o("funnel"),Vcr=o(" \u2014 "),aO=a("a"),zcr=o("TFFunnelForMultipleChoice"),Wcr=o(" (Funnel Transformer model)"),Qcr=l(),yF=a("li"),Ope=a("strong"),Hcr=o("longformer"),Ucr=o(" \u2014 "),nO=a("a"),Jcr=o("TFLongformerForMultipleChoice"),Ycr=o(" (Longformer model)"),Kcr=l(),wF=a("li"),Xpe=a("strong"),Zcr=o("mobilebert"),efr=o(" \u2014 "),sO=a("a"),ofr=o("TFMobileBertForMultipleChoice"),rfr=o(" (MobileBERT model)"),tfr=l(),AF=a("li"),Vpe=a("strong"),afr=o("mpnet"),nfr=o(" \u2014 "),lO=a("a"),sfr=o("TFMPNetForMultipleChoice"),lfr=o(" (MPNet model)"),ifr=l(),LF=a("li"),zpe=a("strong"),dfr=o("rembert"),cfr=o(" \u2014 "),iO=a("a"),ffr=o("TFRemBertForMultipleChoice"),mfr=o(" (RemBERT model)"),gfr=l(),BF=a("li"),Wpe=a("strong"),hfr=o("roberta"),pfr=o(" \u2014 "),dO=a("a"),_fr=o("TFRobertaForMultipleChoice"),ufr=o(" (RoBERTa model)"),bfr=l(),xF=a("li"),Qpe=a("strong"),vfr=o("roformer"),Tfr=o(" \u2014 "),cO=a("a"),Ffr=o("TFRoFormerForMultipleChoice"),Cfr=o(" (RoFormer model)"),Mfr=l(),kF=a("li"),Hpe=a("strong"),Efr=o("xlm"),yfr=o(" \u2014 "),fO=a("a"),wfr=o("TFXLMForMultipleChoice"),Afr=o(" (XLM model)"),Lfr=l(),RF=a("li"),Upe=a("strong"),Bfr=o("xlm-roberta"),xfr=o(" \u2014 "),mO=a("a"),kfr=o("TFXLMRobertaForMultipleChoice"),Rfr=o(" (XLM-RoBERTa model)"),Sfr=l(),SF=a("li"),Jpe=a("strong"),Pfr=o("xlnet"),$fr=o(" \u2014 "),gO=a("a"),Ifr=o("TFXLNetForMultipleChoice"),Dfr=o(" (XLNet model)"),jfr=l(),Ype=a("p"),Nfr=o("Examples:"),qfr=l(),f(s6.$$.fragment),Mxe=l(),wc=a("h2"),PF=a("a"),Kpe=a("span"),f(l6.$$.fragment),Gfr=l(),Zpe=a("span"),Ofr=o("TFAutoModelForTableQuestionAnswering"),Exe=l(),yr=a("div"),f(i6.$$.fragment),Xfr=l(),Ac=a("p"),Vfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),e_e=a("code"),zfr=o("from_pretrained()"),Wfr=o("class method or the "),o_e=a("code"),Qfr=o("from_config()"),Hfr=o(`class
method.`),Ufr=l(),d6=a("p"),Jfr=o("This class cannot be instantiated directly using "),r_e=a("code"),Yfr=o("__init__()"),Kfr=o(" (throws an error)."),Zfr=l(),bt=a("div"),f(c6.$$.fragment),emr=l(),t_e=a("p"),omr=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),rmr=l(),Lc=a("p"),tmr=o(`Note:
Loading a model from its configuration file does `),a_e=a("strong"),amr=o("not"),nmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),n_e=a("code"),smr=o("from_pretrained()"),lmr=o("to load the model weights."),imr=l(),s_e=a("p"),dmr=o("Examples:"),cmr=l(),f(f6.$$.fragment),fmr=l(),Fo=a("div"),f(m6.$$.fragment),mmr=l(),l_e=a("p"),gmr=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),hmr=l(),bn=a("p"),pmr=o("The model class to instantiate is selected based on the "),i_e=a("code"),_mr=o("model_type"),umr=o(` property of the config object (either
passed as an argument or loaded from `),d_e=a("code"),bmr=o("pretrained_model_name_or_path"),vmr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),c_e=a("code"),Tmr=o("pretrained_model_name_or_path"),Fmr=o(":"),Cmr=l(),f_e=a("ul"),$F=a("li"),m_e=a("strong"),Mmr=o("tapas"),Emr=o(" \u2014 "),hO=a("a"),ymr=o("TFTapasForQuestionAnswering"),wmr=o(" (TAPAS model)"),Amr=l(),g_e=a("p"),Lmr=o("Examples:"),Bmr=l(),f(g6.$$.fragment),yxe=l(),Bc=a("h2"),IF=a("a"),h_e=a("span"),f(h6.$$.fragment),xmr=l(),p_e=a("span"),kmr=o("TFAutoModelForTokenClassification"),wxe=l(),wr=a("div"),f(p6.$$.fragment),Rmr=l(),xc=a("p"),Smr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),__e=a("code"),Pmr=o("from_pretrained()"),$mr=o("class method or the "),u_e=a("code"),Imr=o("from_config()"),Dmr=o(`class
method.`),jmr=l(),_6=a("p"),Nmr=o("This class cannot be instantiated directly using "),b_e=a("code"),qmr=o("__init__()"),Gmr=o(" (throws an error)."),Omr=l(),vt=a("div"),f(u6.$$.fragment),Xmr=l(),v_e=a("p"),Vmr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),zmr=l(),kc=a("p"),Wmr=o(`Note:
Loading a model from its configuration file does `),T_e=a("strong"),Qmr=o("not"),Hmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F_e=a("code"),Umr=o("from_pretrained()"),Jmr=o("to load the model weights."),Ymr=l(),C_e=a("p"),Kmr=o("Examples:"),Zmr=l(),f(b6.$$.fragment),egr=l(),Co=a("div"),f(v6.$$.fragment),ogr=l(),M_e=a("p"),rgr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),tgr=l(),vn=a("p"),agr=o("The model class to instantiate is selected based on the "),E_e=a("code"),ngr=o("model_type"),sgr=o(` property of the config object (either
passed as an argument or loaded from `),y_e=a("code"),lgr=o("pretrained_model_name_or_path"),igr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w_e=a("code"),dgr=o("pretrained_model_name_or_path"),cgr=o(":"),fgr=l(),K=a("ul"),DF=a("li"),A_e=a("strong"),mgr=o("albert"),ggr=o(" \u2014 "),pO=a("a"),hgr=o("TFAlbertForTokenClassification"),pgr=o(" (ALBERT model)"),_gr=l(),jF=a("li"),L_e=a("strong"),ugr=o("bert"),bgr=o(" \u2014 "),_O=a("a"),vgr=o("TFBertForTokenClassification"),Tgr=o(" (BERT model)"),Fgr=l(),NF=a("li"),B_e=a("strong"),Cgr=o("camembert"),Mgr=o(" \u2014 "),uO=a("a"),Egr=o("TFCamembertForTokenClassification"),ygr=o(" (CamemBERT model)"),wgr=l(),qF=a("li"),x_e=a("strong"),Agr=o("convbert"),Lgr=o(" \u2014 "),bO=a("a"),Bgr=o("TFConvBertForTokenClassification"),xgr=o(" (ConvBERT model)"),kgr=l(),GF=a("li"),k_e=a("strong"),Rgr=o("deberta"),Sgr=o(" \u2014 "),vO=a("a"),Pgr=o("TFDebertaForTokenClassification"),$gr=o(" (DeBERTa model)"),Igr=l(),OF=a("li"),R_e=a("strong"),Dgr=o("deberta-v2"),jgr=o(" \u2014 "),TO=a("a"),Ngr=o("TFDebertaV2ForTokenClassification"),qgr=o(" (DeBERTa-v2 model)"),Ggr=l(),XF=a("li"),S_e=a("strong"),Ogr=o("distilbert"),Xgr=o(" \u2014 "),FO=a("a"),Vgr=o("TFDistilBertForTokenClassification"),zgr=o(" (DistilBERT model)"),Wgr=l(),VF=a("li"),P_e=a("strong"),Qgr=o("electra"),Hgr=o(" \u2014 "),CO=a("a"),Ugr=o("TFElectraForTokenClassification"),Jgr=o(" (ELECTRA model)"),Ygr=l(),zF=a("li"),$_e=a("strong"),Kgr=o("flaubert"),Zgr=o(" \u2014 "),MO=a("a"),ehr=o("TFFlaubertForTokenClassification"),ohr=o(" (FlauBERT model)"),rhr=l(),WF=a("li"),I_e=a("strong"),thr=o("funnel"),ahr=o(" \u2014 "),EO=a("a"),nhr=o("TFFunnelForTokenClassification"),shr=o(" (Funnel Transformer model)"),lhr=l(),QF=a("li"),D_e=a("strong"),ihr=o("layoutlm"),dhr=o(" \u2014 "),yO=a("a"),chr=o("TFLayoutLMForTokenClassification"),fhr=o(" (LayoutLM model)"),mhr=l(),HF=a("li"),j_e=a("strong"),ghr=o("longformer"),hhr=o(" \u2014 "),wO=a("a"),phr=o("TFLongformerForTokenClassification"),_hr=o(" (Longformer model)"),uhr=l(),UF=a("li"),N_e=a("strong"),bhr=o("mobilebert"),vhr=o(" \u2014 "),AO=a("a"),Thr=o("TFMobileBertForTokenClassification"),Fhr=o(" (MobileBERT model)"),Chr=l(),JF=a("li"),q_e=a("strong"),Mhr=o("mpnet"),Ehr=o(" \u2014 "),LO=a("a"),yhr=o("TFMPNetForTokenClassification"),whr=o(" (MPNet model)"),Ahr=l(),YF=a("li"),G_e=a("strong"),Lhr=o("rembert"),Bhr=o(" \u2014 "),BO=a("a"),xhr=o("TFRemBertForTokenClassification"),khr=o(" (RemBERT model)"),Rhr=l(),KF=a("li"),O_e=a("strong"),Shr=o("roberta"),Phr=o(" \u2014 "),xO=a("a"),$hr=o("TFRobertaForTokenClassification"),Ihr=o(" (RoBERTa model)"),Dhr=l(),ZF=a("li"),X_e=a("strong"),jhr=o("roformer"),Nhr=o(" \u2014 "),kO=a("a"),qhr=o("TFRoFormerForTokenClassification"),Ghr=o(" (RoFormer model)"),Ohr=l(),e9=a("li"),V_e=a("strong"),Xhr=o("xlm"),Vhr=o(" \u2014 "),RO=a("a"),zhr=o("TFXLMForTokenClassification"),Whr=o(" (XLM model)"),Qhr=l(),o9=a("li"),z_e=a("strong"),Hhr=o("xlm-roberta"),Uhr=o(" \u2014 "),SO=a("a"),Jhr=o("TFXLMRobertaForTokenClassification"),Yhr=o(" (XLM-RoBERTa model)"),Khr=l(),r9=a("li"),W_e=a("strong"),Zhr=o("xlnet"),epr=o(" \u2014 "),PO=a("a"),opr=o("TFXLNetForTokenClassification"),rpr=o(" (XLNet model)"),tpr=l(),Q_e=a("p"),apr=o("Examples:"),npr=l(),f(T6.$$.fragment),Axe=l(),Rc=a("h2"),t9=a("a"),H_e=a("span"),f(F6.$$.fragment),spr=l(),U_e=a("span"),lpr=o("TFAutoModelForQuestionAnswering"),Lxe=l(),Ar=a("div"),f(C6.$$.fragment),ipr=l(),Sc=a("p"),dpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),J_e=a("code"),cpr=o("from_pretrained()"),fpr=o("class method or the "),Y_e=a("code"),mpr=o("from_config()"),gpr=o(`class
method.`),hpr=l(),M6=a("p"),ppr=o("This class cannot be instantiated directly using "),K_e=a("code"),_pr=o("__init__()"),upr=o(" (throws an error)."),bpr=l(),Tt=a("div"),f(E6.$$.fragment),vpr=l(),Z_e=a("p"),Tpr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Fpr=l(),Pc=a("p"),Cpr=o(`Note:
Loading a model from its configuration file does `),eue=a("strong"),Mpr=o("not"),Epr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),oue=a("code"),ypr=o("from_pretrained()"),wpr=o("to load the model weights."),Apr=l(),rue=a("p"),Lpr=o("Examples:"),Bpr=l(),f(y6.$$.fragment),xpr=l(),Mo=a("div"),f(w6.$$.fragment),kpr=l(),tue=a("p"),Rpr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Spr=l(),Tn=a("p"),Ppr=o("The model class to instantiate is selected based on the "),aue=a("code"),$pr=o("model_type"),Ipr=o(` property of the config object (either
passed as an argument or loaded from `),nue=a("code"),Dpr=o("pretrained_model_name_or_path"),jpr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sue=a("code"),Npr=o("pretrained_model_name_or_path"),qpr=o(":"),Gpr=l(),Z=a("ul"),a9=a("li"),lue=a("strong"),Opr=o("albert"),Xpr=o(" \u2014 "),$O=a("a"),Vpr=o("TFAlbertForQuestionAnswering"),zpr=o(" (ALBERT model)"),Wpr=l(),n9=a("li"),iue=a("strong"),Qpr=o("bert"),Hpr=o(" \u2014 "),IO=a("a"),Upr=o("TFBertForQuestionAnswering"),Jpr=o(" (BERT model)"),Ypr=l(),s9=a("li"),due=a("strong"),Kpr=o("camembert"),Zpr=o(" \u2014 "),DO=a("a"),e_r=o("TFCamembertForQuestionAnswering"),o_r=o(" (CamemBERT model)"),r_r=l(),l9=a("li"),cue=a("strong"),t_r=o("convbert"),a_r=o(" \u2014 "),jO=a("a"),n_r=o("TFConvBertForQuestionAnswering"),s_r=o(" (ConvBERT model)"),l_r=l(),i9=a("li"),fue=a("strong"),i_r=o("deberta"),d_r=o(" \u2014 "),NO=a("a"),c_r=o("TFDebertaForQuestionAnswering"),f_r=o(" (DeBERTa model)"),m_r=l(),d9=a("li"),mue=a("strong"),g_r=o("deberta-v2"),h_r=o(" \u2014 "),qO=a("a"),p_r=o("TFDebertaV2ForQuestionAnswering"),__r=o(" (DeBERTa-v2 model)"),u_r=l(),c9=a("li"),gue=a("strong"),b_r=o("distilbert"),v_r=o(" \u2014 "),GO=a("a"),T_r=o("TFDistilBertForQuestionAnswering"),F_r=o(" (DistilBERT model)"),C_r=l(),f9=a("li"),hue=a("strong"),M_r=o("electra"),E_r=o(" \u2014 "),OO=a("a"),y_r=o("TFElectraForQuestionAnswering"),w_r=o(" (ELECTRA model)"),A_r=l(),m9=a("li"),pue=a("strong"),L_r=o("flaubert"),B_r=o(" \u2014 "),XO=a("a"),x_r=o("TFFlaubertForQuestionAnsweringSimple"),k_r=o(" (FlauBERT model)"),R_r=l(),g9=a("li"),_ue=a("strong"),S_r=o("funnel"),P_r=o(" \u2014 "),VO=a("a"),$_r=o("TFFunnelForQuestionAnswering"),I_r=o(" (Funnel Transformer model)"),D_r=l(),h9=a("li"),uue=a("strong"),j_r=o("longformer"),N_r=o(" \u2014 "),zO=a("a"),q_r=o("TFLongformerForQuestionAnswering"),G_r=o(" (Longformer model)"),O_r=l(),p9=a("li"),bue=a("strong"),X_r=o("mobilebert"),V_r=o(" \u2014 "),WO=a("a"),z_r=o("TFMobileBertForQuestionAnswering"),W_r=o(" (MobileBERT model)"),Q_r=l(),_9=a("li"),vue=a("strong"),H_r=o("mpnet"),U_r=o(" \u2014 "),QO=a("a"),J_r=o("TFMPNetForQuestionAnswering"),Y_r=o(" (MPNet model)"),K_r=l(),u9=a("li"),Tue=a("strong"),Z_r=o("rembert"),eur=o(" \u2014 "),HO=a("a"),our=o("TFRemBertForQuestionAnswering"),rur=o(" (RemBERT model)"),tur=l(),b9=a("li"),Fue=a("strong"),aur=o("roberta"),nur=o(" \u2014 "),UO=a("a"),sur=o("TFRobertaForQuestionAnswering"),lur=o(" (RoBERTa model)"),iur=l(),v9=a("li"),Cue=a("strong"),dur=o("roformer"),cur=o(" \u2014 "),JO=a("a"),fur=o("TFRoFormerForQuestionAnswering"),mur=o(" (RoFormer model)"),gur=l(),T9=a("li"),Mue=a("strong"),hur=o("xlm"),pur=o(" \u2014 "),YO=a("a"),_ur=o("TFXLMForQuestionAnsweringSimple"),uur=o(" (XLM model)"),bur=l(),F9=a("li"),Eue=a("strong"),vur=o("xlm-roberta"),Tur=o(" \u2014 "),KO=a("a"),Fur=o("TFXLMRobertaForQuestionAnswering"),Cur=o(" (XLM-RoBERTa model)"),Mur=l(),C9=a("li"),yue=a("strong"),Eur=o("xlnet"),yur=o(" \u2014 "),ZO=a("a"),wur=o("TFXLNetForQuestionAnsweringSimple"),Aur=o(" (XLNet model)"),Lur=l(),wue=a("p"),Bur=o("Examples:"),xur=l(),f(A6.$$.fragment),Bxe=l(),$c=a("h2"),M9=a("a"),Aue=a("span"),f(L6.$$.fragment),kur=l(),Lue=a("span"),Rur=o("TFAutoModelForVision2Seq"),xxe=l(),Lr=a("div"),f(B6.$$.fragment),Sur=l(),Ic=a("p"),Pur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Bue=a("code"),$ur=o("from_pretrained()"),Iur=o("class method or the "),xue=a("code"),Dur=o("from_config()"),jur=o(`class
method.`),Nur=l(),x6=a("p"),qur=o("This class cannot be instantiated directly using "),kue=a("code"),Gur=o("__init__()"),Our=o(" (throws an error)."),Xur=l(),Ft=a("div"),f(k6.$$.fragment),Vur=l(),Rue=a("p"),zur=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Wur=l(),Dc=a("p"),Qur=o(`Note:
Loading a model from its configuration file does `),Sue=a("strong"),Hur=o("not"),Uur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pue=a("code"),Jur=o("from_pretrained()"),Yur=o("to load the model weights."),Kur=l(),$ue=a("p"),Zur=o("Examples:"),e0r=l(),f(R6.$$.fragment),o0r=l(),Eo=a("div"),f(S6.$$.fragment),r0r=l(),Iue=a("p"),t0r=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),a0r=l(),Fn=a("p"),n0r=o("The model class to instantiate is selected based on the "),Due=a("code"),s0r=o("model_type"),l0r=o(` property of the config object (either
passed as an argument or loaded from `),jue=a("code"),i0r=o("pretrained_model_name_or_path"),d0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=a("code"),c0r=o("pretrained_model_name_or_path"),f0r=o(":"),m0r=l(),que=a("ul"),E9=a("li"),Gue=a("strong"),g0r=o("vision-encoder-decoder"),h0r=o(" \u2014 "),eX=a("a"),p0r=o("TFVisionEncoderDecoderModel"),_0r=o(" (Vision Encoder decoder model)"),u0r=l(),Oue=a("p"),b0r=o("Examples:"),v0r=l(),f(P6.$$.fragment),kxe=l(),jc=a("h2"),y9=a("a"),Xue=a("span"),f($6.$$.fragment),T0r=l(),Vue=a("span"),F0r=o("TFAutoModelForSpeechSeq2Seq"),Rxe=l(),Br=a("div"),f(I6.$$.fragment),C0r=l(),Nc=a("p"),M0r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),zue=a("code"),E0r=o("from_pretrained()"),y0r=o("class method or the "),Wue=a("code"),w0r=o("from_config()"),A0r=o(`class
method.`),L0r=l(),D6=a("p"),B0r=o("This class cannot be instantiated directly using "),Que=a("code"),x0r=o("__init__()"),k0r=o(" (throws an error)."),R0r=l(),Ct=a("div"),f(j6.$$.fragment),S0r=l(),Hue=a("p"),P0r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),$0r=l(),qc=a("p"),I0r=o(`Note:
Loading a model from its configuration file does `),Uue=a("strong"),D0r=o("not"),j0r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jue=a("code"),N0r=o("from_pretrained()"),q0r=o("to load the model weights."),G0r=l(),Yue=a("p"),O0r=o("Examples:"),X0r=l(),f(N6.$$.fragment),V0r=l(),yo=a("div"),f(q6.$$.fragment),z0r=l(),Kue=a("p"),W0r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Q0r=l(),Cn=a("p"),H0r=o("The model class to instantiate is selected based on the "),Zue=a("code"),U0r=o("model_type"),J0r=o(` property of the config object (either
passed as an argument or loaded from `),e0e=a("code"),Y0r=o("pretrained_model_name_or_path"),K0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o0e=a("code"),Z0r=o("pretrained_model_name_or_path"),e1r=o(":"),o1r=l(),r0e=a("ul"),w9=a("li"),t0e=a("strong"),r1r=o("speech_to_text"),t1r=o(" \u2014 "),oX=a("a"),a1r=o("TFSpeech2TextForConditionalGeneration"),n1r=o(" (Speech2Text model)"),s1r=l(),a0e=a("p"),l1r=o("Examples:"),i1r=l(),f(G6.$$.fragment),Sxe=l(),Gc=a("h2"),A9=a("a"),n0e=a("span"),f(O6.$$.fragment),d1r=l(),s0e=a("span"),c1r=o("FlaxAutoModel"),Pxe=l(),xr=a("div"),f(X6.$$.fragment),f1r=l(),Oc=a("p"),m1r=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),l0e=a("code"),g1r=o("from_pretrained()"),h1r=o("class method or the "),i0e=a("code"),p1r=o("from_config()"),_1r=o(`class
method.`),u1r=l(),V6=a("p"),b1r=o("This class cannot be instantiated directly using "),d0e=a("code"),v1r=o("__init__()"),T1r=o(" (throws an error)."),F1r=l(),Mt=a("div"),f(z6.$$.fragment),C1r=l(),c0e=a("p"),M1r=o("Instantiates one of the base model classes of the library from a configuration."),E1r=l(),Xc=a("p"),y1r=o(`Note:
Loading a model from its configuration file does `),f0e=a("strong"),w1r=o("not"),A1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),m0e=a("code"),L1r=o("from_pretrained()"),B1r=o("to load the model weights."),x1r=l(),g0e=a("p"),k1r=o("Examples:"),R1r=l(),f(W6.$$.fragment),S1r=l(),wo=a("div"),f(Q6.$$.fragment),P1r=l(),h0e=a("p"),$1r=o("Instantiate one of the base model classes of the library from a pretrained model."),I1r=l(),Mn=a("p"),D1r=o("The model class to instantiate is selected based on the "),p0e=a("code"),j1r=o("model_type"),N1r=o(` property of the config object (either
passed as an argument or loaded from `),_0e=a("code"),q1r=o("pretrained_model_name_or_path"),G1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u0e=a("code"),O1r=o("pretrained_model_name_or_path"),X1r=o(":"),V1r=l(),V=a("ul"),L9=a("li"),b0e=a("strong"),z1r=o("albert"),W1r=o(" \u2014 "),rX=a("a"),Q1r=o("FlaxAlbertModel"),H1r=o(" (ALBERT model)"),U1r=l(),B9=a("li"),v0e=a("strong"),J1r=o("bart"),Y1r=o(" \u2014 "),tX=a("a"),K1r=o("FlaxBartModel"),Z1r=o(" (BART model)"),ebr=l(),x9=a("li"),T0e=a("strong"),obr=o("beit"),rbr=o(" \u2014 "),aX=a("a"),tbr=o("FlaxBeitModel"),abr=o(" (BEiT model)"),nbr=l(),k9=a("li"),F0e=a("strong"),sbr=o("bert"),lbr=o(" \u2014 "),nX=a("a"),ibr=o("FlaxBertModel"),dbr=o(" (BERT model)"),cbr=l(),R9=a("li"),C0e=a("strong"),fbr=o("big_bird"),mbr=o(" \u2014 "),sX=a("a"),gbr=o("FlaxBigBirdModel"),hbr=o(" (BigBird model)"),pbr=l(),S9=a("li"),M0e=a("strong"),_br=o("blenderbot"),ubr=o(" \u2014 "),lX=a("a"),bbr=o("FlaxBlenderbotModel"),vbr=o(" (Blenderbot model)"),Tbr=l(),P9=a("li"),E0e=a("strong"),Fbr=o("blenderbot-small"),Cbr=o(" \u2014 "),iX=a("a"),Mbr=o("FlaxBlenderbotSmallModel"),Ebr=o(" (BlenderbotSmall model)"),ybr=l(),$9=a("li"),y0e=a("strong"),wbr=o("clip"),Abr=o(" \u2014 "),dX=a("a"),Lbr=o("FlaxCLIPModel"),Bbr=o(" (CLIP model)"),xbr=l(),I9=a("li"),w0e=a("strong"),kbr=o("distilbert"),Rbr=o(" \u2014 "),cX=a("a"),Sbr=o("FlaxDistilBertModel"),Pbr=o(" (DistilBERT model)"),$br=l(),D9=a("li"),A0e=a("strong"),Ibr=o("electra"),Dbr=o(" \u2014 "),fX=a("a"),jbr=o("FlaxElectraModel"),Nbr=o(" (ELECTRA model)"),qbr=l(),j9=a("li"),L0e=a("strong"),Gbr=o("gpt2"),Obr=o(" \u2014 "),mX=a("a"),Xbr=o("FlaxGPT2Model"),Vbr=o(" (OpenAI GPT-2 model)"),zbr=l(),N9=a("li"),B0e=a("strong"),Wbr=o("gpt_neo"),Qbr=o(" \u2014 "),gX=a("a"),Hbr=o("FlaxGPTNeoModel"),Ubr=o(" (GPT Neo model)"),Jbr=l(),q9=a("li"),x0e=a("strong"),Ybr=o("gptj"),Kbr=o(" \u2014 "),hX=a("a"),Zbr=o("FlaxGPTJModel"),e5r=o(" (GPT-J model)"),o5r=l(),G9=a("li"),k0e=a("strong"),r5r=o("marian"),t5r=o(" \u2014 "),pX=a("a"),a5r=o("FlaxMarianModel"),n5r=o(" (Marian model)"),s5r=l(),O9=a("li"),R0e=a("strong"),l5r=o("mbart"),i5r=o(" \u2014 "),_X=a("a"),d5r=o("FlaxMBartModel"),c5r=o(" (mBART model)"),f5r=l(),X9=a("li"),S0e=a("strong"),m5r=o("mt5"),g5r=o(" \u2014 "),uX=a("a"),h5r=o("FlaxMT5Model"),p5r=o(" (mT5 model)"),_5r=l(),V9=a("li"),P0e=a("strong"),u5r=o("pegasus"),b5r=o(" \u2014 "),bX=a("a"),v5r=o("FlaxPegasusModel"),T5r=o(" (Pegasus model)"),F5r=l(),z9=a("li"),$0e=a("strong"),C5r=o("roberta"),M5r=o(" \u2014 "),vX=a("a"),E5r=o("FlaxRobertaModel"),y5r=o(" (RoBERTa model)"),w5r=l(),W9=a("li"),I0e=a("strong"),A5r=o("roformer"),L5r=o(" \u2014 "),TX=a("a"),B5r=o("FlaxRoFormerModel"),x5r=o(" (RoFormer model)"),k5r=l(),Q9=a("li"),D0e=a("strong"),R5r=o("t5"),S5r=o(" \u2014 "),FX=a("a"),P5r=o("FlaxT5Model"),$5r=o(" (T5 model)"),I5r=l(),H9=a("li"),j0e=a("strong"),D5r=o("vision-text-dual-encoder"),j5r=o(" \u2014 "),CX=a("a"),N5r=o("FlaxVisionTextDualEncoderModel"),q5r=o(" (VisionTextDualEncoder model)"),G5r=l(),U9=a("li"),N0e=a("strong"),O5r=o("vit"),X5r=o(" \u2014 "),MX=a("a"),V5r=o("FlaxViTModel"),z5r=o(" (ViT model)"),W5r=l(),J9=a("li"),q0e=a("strong"),Q5r=o("wav2vec2"),H5r=o(" \u2014 "),EX=a("a"),U5r=o("FlaxWav2Vec2Model"),J5r=o(" (Wav2Vec2 model)"),Y5r=l(),Y9=a("li"),G0e=a("strong"),K5r=o("xglm"),Z5r=o(" \u2014 "),yX=a("a"),e2r=o("FlaxXGLMModel"),o2r=o(" (XGLM model)"),r2r=l(),K9=a("li"),O0e=a("strong"),t2r=o("xlm-roberta"),a2r=o(" \u2014 "),wX=a("a"),n2r=o("FlaxXLMRobertaModel"),s2r=o(" (XLM-RoBERTa model)"),l2r=l(),X0e=a("p"),i2r=o("Examples:"),d2r=l(),f(H6.$$.fragment),$xe=l(),Vc=a("h2"),Z9=a("a"),V0e=a("span"),f(U6.$$.fragment),c2r=l(),z0e=a("span"),f2r=o("FlaxAutoModelForCausalLM"),Ixe=l(),kr=a("div"),f(J6.$$.fragment),m2r=l(),zc=a("p"),g2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),W0e=a("code"),h2r=o("from_pretrained()"),p2r=o("class method or the "),Q0e=a("code"),_2r=o("from_config()"),u2r=o(`class
method.`),b2r=l(),Y6=a("p"),v2r=o("This class cannot be instantiated directly using "),H0e=a("code"),T2r=o("__init__()"),F2r=o(" (throws an error)."),C2r=l(),Et=a("div"),f(K6.$$.fragment),M2r=l(),U0e=a("p"),E2r=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),y2r=l(),Wc=a("p"),w2r=o(`Note:
Loading a model from its configuration file does `),J0e=a("strong"),A2r=o("not"),L2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Y0e=a("code"),B2r=o("from_pretrained()"),x2r=o("to load the model weights."),k2r=l(),K0e=a("p"),R2r=o("Examples:"),S2r=l(),f(Z6.$$.fragment),P2r=l(),Ao=a("div"),f(eA.$$.fragment),$2r=l(),Z0e=a("p"),I2r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),D2r=l(),En=a("p"),j2r=o("The model class to instantiate is selected based on the "),e1e=a("code"),N2r=o("model_type"),q2r=o(` property of the config object (either
passed as an argument or loaded from `),o1e=a("code"),G2r=o("pretrained_model_name_or_path"),O2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r1e=a("code"),X2r=o("pretrained_model_name_or_path"),V2r=o(":"),z2r=l(),yn=a("ul"),eC=a("li"),t1e=a("strong"),W2r=o("gpt2"),Q2r=o(" \u2014 "),AX=a("a"),H2r=o("FlaxGPT2LMHeadModel"),U2r=o(" (OpenAI GPT-2 model)"),J2r=l(),oC=a("li"),a1e=a("strong"),Y2r=o("gpt_neo"),K2r=o(" \u2014 "),LX=a("a"),Z2r=o("FlaxGPTNeoForCausalLM"),evr=o(" (GPT Neo model)"),ovr=l(),rC=a("li"),n1e=a("strong"),rvr=o("gptj"),tvr=o(" \u2014 "),BX=a("a"),avr=o("FlaxGPTJForCausalLM"),nvr=o(" (GPT-J model)"),svr=l(),tC=a("li"),s1e=a("strong"),lvr=o("xglm"),ivr=o(" \u2014 "),xX=a("a"),dvr=o("FlaxXGLMForCausalLM"),cvr=o(" (XGLM model)"),fvr=l(),l1e=a("p"),mvr=o("Examples:"),gvr=l(),f(oA.$$.fragment),Dxe=l(),Qc=a("h2"),aC=a("a"),i1e=a("span"),f(rA.$$.fragment),hvr=l(),d1e=a("span"),pvr=o("FlaxAutoModelForPreTraining"),jxe=l(),Rr=a("div"),f(tA.$$.fragment),_vr=l(),Hc=a("p"),uvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),c1e=a("code"),bvr=o("from_pretrained()"),vvr=o("class method or the "),f1e=a("code"),Tvr=o("from_config()"),Fvr=o(`class
method.`),Cvr=l(),aA=a("p"),Mvr=o("This class cannot be instantiated directly using "),m1e=a("code"),Evr=o("__init__()"),yvr=o(" (throws an error)."),wvr=l(),yt=a("div"),f(nA.$$.fragment),Avr=l(),g1e=a("p"),Lvr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Bvr=l(),Uc=a("p"),xvr=o(`Note:
Loading a model from its configuration file does `),h1e=a("strong"),kvr=o("not"),Rvr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),p1e=a("code"),Svr=o("from_pretrained()"),Pvr=o("to load the model weights."),$vr=l(),_1e=a("p"),Ivr=o("Examples:"),Dvr=l(),f(sA.$$.fragment),jvr=l(),Lo=a("div"),f(lA.$$.fragment),Nvr=l(),u1e=a("p"),qvr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Gvr=l(),wn=a("p"),Ovr=o("The model class to instantiate is selected based on the "),b1e=a("code"),Xvr=o("model_type"),Vvr=o(` property of the config object (either
passed as an argument or loaded from `),v1e=a("code"),zvr=o("pretrained_model_name_or_path"),Wvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),T1e=a("code"),Qvr=o("pretrained_model_name_or_path"),Hvr=o(":"),Uvr=l(),ce=a("ul"),nC=a("li"),F1e=a("strong"),Jvr=o("albert"),Yvr=o(" \u2014 "),kX=a("a"),Kvr=o("FlaxAlbertForPreTraining"),Zvr=o(" (ALBERT model)"),eTr=l(),sC=a("li"),C1e=a("strong"),oTr=o("bart"),rTr=o(" \u2014 "),RX=a("a"),tTr=o("FlaxBartForConditionalGeneration"),aTr=o(" (BART model)"),nTr=l(),lC=a("li"),M1e=a("strong"),sTr=o("bert"),lTr=o(" \u2014 "),SX=a("a"),iTr=o("FlaxBertForPreTraining"),dTr=o(" (BERT model)"),cTr=l(),iC=a("li"),E1e=a("strong"),fTr=o("big_bird"),mTr=o(" \u2014 "),PX=a("a"),gTr=o("FlaxBigBirdForPreTraining"),hTr=o(" (BigBird model)"),pTr=l(),dC=a("li"),y1e=a("strong"),_Tr=o("electra"),uTr=o(" \u2014 "),$X=a("a"),bTr=o("FlaxElectraForPreTraining"),vTr=o(" (ELECTRA model)"),TTr=l(),cC=a("li"),w1e=a("strong"),FTr=o("mbart"),CTr=o(" \u2014 "),IX=a("a"),MTr=o("FlaxMBartForConditionalGeneration"),ETr=o(" (mBART model)"),yTr=l(),fC=a("li"),A1e=a("strong"),wTr=o("mt5"),ATr=o(" \u2014 "),DX=a("a"),LTr=o("FlaxMT5ForConditionalGeneration"),BTr=o(" (mT5 model)"),xTr=l(),mC=a("li"),L1e=a("strong"),kTr=o("roberta"),RTr=o(" \u2014 "),jX=a("a"),STr=o("FlaxRobertaForMaskedLM"),PTr=o(" (RoBERTa model)"),$Tr=l(),gC=a("li"),B1e=a("strong"),ITr=o("roformer"),DTr=o(" \u2014 "),NX=a("a"),jTr=o("FlaxRoFormerForMaskedLM"),NTr=o(" (RoFormer model)"),qTr=l(),hC=a("li"),x1e=a("strong"),GTr=o("t5"),OTr=o(" \u2014 "),qX=a("a"),XTr=o("FlaxT5ForConditionalGeneration"),VTr=o(" (T5 model)"),zTr=l(),pC=a("li"),k1e=a("strong"),WTr=o("wav2vec2"),QTr=o(" \u2014 "),GX=a("a"),HTr=o("FlaxWav2Vec2ForPreTraining"),UTr=o(" (Wav2Vec2 model)"),JTr=l(),_C=a("li"),R1e=a("strong"),YTr=o("xlm-roberta"),KTr=o(" \u2014 "),OX=a("a"),ZTr=o("FlaxXLMRobertaForMaskedLM"),eFr=o(" (XLM-RoBERTa model)"),oFr=l(),S1e=a("p"),rFr=o("Examples:"),tFr=l(),f(iA.$$.fragment),Nxe=l(),Jc=a("h2"),uC=a("a"),P1e=a("span"),f(dA.$$.fragment),aFr=l(),$1e=a("span"),nFr=o("FlaxAutoModelForMaskedLM"),qxe=l(),Sr=a("div"),f(cA.$$.fragment),sFr=l(),Yc=a("p"),lFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),I1e=a("code"),iFr=o("from_pretrained()"),dFr=o("class method or the "),D1e=a("code"),cFr=o("from_config()"),fFr=o(`class
method.`),mFr=l(),fA=a("p"),gFr=o("This class cannot be instantiated directly using "),j1e=a("code"),hFr=o("__init__()"),pFr=o(" (throws an error)."),_Fr=l(),wt=a("div"),f(mA.$$.fragment),uFr=l(),N1e=a("p"),bFr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),vFr=l(),Kc=a("p"),TFr=o(`Note:
Loading a model from its configuration file does `),q1e=a("strong"),FFr=o("not"),CFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),G1e=a("code"),MFr=o("from_pretrained()"),EFr=o("to load the model weights."),yFr=l(),O1e=a("p"),wFr=o("Examples:"),AFr=l(),f(gA.$$.fragment),LFr=l(),Bo=a("div"),f(hA.$$.fragment),BFr=l(),X1e=a("p"),xFr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),kFr=l(),An=a("p"),RFr=o("The model class to instantiate is selected based on the "),V1e=a("code"),SFr=o("model_type"),PFr=o(` property of the config object (either
passed as an argument or loaded from `),z1e=a("code"),$Fr=o("pretrained_model_name_or_path"),IFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),W1e=a("code"),DFr=o("pretrained_model_name_or_path"),jFr=o(":"),NFr=l(),ue=a("ul"),bC=a("li"),Q1e=a("strong"),qFr=o("albert"),GFr=o(" \u2014 "),XX=a("a"),OFr=o("FlaxAlbertForMaskedLM"),XFr=o(" (ALBERT model)"),VFr=l(),vC=a("li"),H1e=a("strong"),zFr=o("bart"),WFr=o(" \u2014 "),VX=a("a"),QFr=o("FlaxBartForConditionalGeneration"),HFr=o(" (BART model)"),UFr=l(),TC=a("li"),U1e=a("strong"),JFr=o("bert"),YFr=o(" \u2014 "),zX=a("a"),KFr=o("FlaxBertForMaskedLM"),ZFr=o(" (BERT model)"),e9r=l(),FC=a("li"),J1e=a("strong"),o9r=o("big_bird"),r9r=o(" \u2014 "),WX=a("a"),t9r=o("FlaxBigBirdForMaskedLM"),a9r=o(" (BigBird model)"),n9r=l(),CC=a("li"),Y1e=a("strong"),s9r=o("distilbert"),l9r=o(" \u2014 "),QX=a("a"),i9r=o("FlaxDistilBertForMaskedLM"),d9r=o(" (DistilBERT model)"),c9r=l(),MC=a("li"),K1e=a("strong"),f9r=o("electra"),m9r=o(" \u2014 "),HX=a("a"),g9r=o("FlaxElectraForMaskedLM"),h9r=o(" (ELECTRA model)"),p9r=l(),EC=a("li"),Z1e=a("strong"),_9r=o("mbart"),u9r=o(" \u2014 "),UX=a("a"),b9r=o("FlaxMBartForConditionalGeneration"),v9r=o(" (mBART model)"),T9r=l(),yC=a("li"),ebe=a("strong"),F9r=o("roberta"),C9r=o(" \u2014 "),JX=a("a"),M9r=o("FlaxRobertaForMaskedLM"),E9r=o(" (RoBERTa model)"),y9r=l(),wC=a("li"),obe=a("strong"),w9r=o("roformer"),A9r=o(" \u2014 "),YX=a("a"),L9r=o("FlaxRoFormerForMaskedLM"),B9r=o(" (RoFormer model)"),x9r=l(),AC=a("li"),rbe=a("strong"),k9r=o("xlm-roberta"),R9r=o(" \u2014 "),KX=a("a"),S9r=o("FlaxXLMRobertaForMaskedLM"),P9r=o(" (XLM-RoBERTa model)"),$9r=l(),tbe=a("p"),I9r=o("Examples:"),D9r=l(),f(pA.$$.fragment),Gxe=l(),Zc=a("h2"),LC=a("a"),abe=a("span"),f(_A.$$.fragment),j9r=l(),nbe=a("span"),N9r=o("FlaxAutoModelForSeq2SeqLM"),Oxe=l(),Pr=a("div"),f(uA.$$.fragment),q9r=l(),ef=a("p"),G9r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),sbe=a("code"),O9r=o("from_pretrained()"),X9r=o("class method or the "),lbe=a("code"),V9r=o("from_config()"),z9r=o(`class
method.`),W9r=l(),bA=a("p"),Q9r=o("This class cannot be instantiated directly using "),ibe=a("code"),H9r=o("__init__()"),U9r=o(" (throws an error)."),J9r=l(),At=a("div"),f(vA.$$.fragment),Y9r=l(),dbe=a("p"),K9r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Z9r=l(),of=a("p"),eCr=o(`Note:
Loading a model from its configuration file does `),cbe=a("strong"),oCr=o("not"),rCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fbe=a("code"),tCr=o("from_pretrained()"),aCr=o("to load the model weights."),nCr=l(),mbe=a("p"),sCr=o("Examples:"),lCr=l(),f(TA.$$.fragment),iCr=l(),xo=a("div"),f(FA.$$.fragment),dCr=l(),gbe=a("p"),cCr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),fCr=l(),Ln=a("p"),mCr=o("The model class to instantiate is selected based on the "),hbe=a("code"),gCr=o("model_type"),hCr=o(` property of the config object (either
passed as an argument or loaded from `),pbe=a("code"),pCr=o("pretrained_model_name_or_path"),_Cr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_be=a("code"),uCr=o("pretrained_model_name_or_path"),bCr=o(":"),vCr=l(),Ce=a("ul"),BC=a("li"),ube=a("strong"),TCr=o("bart"),FCr=o(" \u2014 "),ZX=a("a"),CCr=o("FlaxBartForConditionalGeneration"),MCr=o(" (BART model)"),ECr=l(),xC=a("li"),bbe=a("strong"),yCr=o("blenderbot"),wCr=o(" \u2014 "),eV=a("a"),ACr=o("FlaxBlenderbotForConditionalGeneration"),LCr=o(" (Blenderbot model)"),BCr=l(),kC=a("li"),vbe=a("strong"),xCr=o("blenderbot-small"),kCr=o(" \u2014 "),oV=a("a"),RCr=o("FlaxBlenderbotSmallForConditionalGeneration"),SCr=o(" (BlenderbotSmall model)"),PCr=l(),RC=a("li"),Tbe=a("strong"),$Cr=o("encoder-decoder"),ICr=o(" \u2014 "),rV=a("a"),DCr=o("FlaxEncoderDecoderModel"),jCr=o(" (Encoder decoder model)"),NCr=l(),SC=a("li"),Fbe=a("strong"),qCr=o("marian"),GCr=o(" \u2014 "),tV=a("a"),OCr=o("FlaxMarianMTModel"),XCr=o(" (Marian model)"),VCr=l(),PC=a("li"),Cbe=a("strong"),zCr=o("mbart"),WCr=o(" \u2014 "),aV=a("a"),QCr=o("FlaxMBartForConditionalGeneration"),HCr=o(" (mBART model)"),UCr=l(),$C=a("li"),Mbe=a("strong"),JCr=o("mt5"),YCr=o(" \u2014 "),nV=a("a"),KCr=o("FlaxMT5ForConditionalGeneration"),ZCr=o(" (mT5 model)"),eMr=l(),IC=a("li"),Ebe=a("strong"),oMr=o("pegasus"),rMr=o(" \u2014 "),sV=a("a"),tMr=o("FlaxPegasusForConditionalGeneration"),aMr=o(" (Pegasus model)"),nMr=l(),DC=a("li"),ybe=a("strong"),sMr=o("t5"),lMr=o(" \u2014 "),lV=a("a"),iMr=o("FlaxT5ForConditionalGeneration"),dMr=o(" (T5 model)"),cMr=l(),wbe=a("p"),fMr=o("Examples:"),mMr=l(),f(CA.$$.fragment),Xxe=l(),rf=a("h2"),jC=a("a"),Abe=a("span"),f(MA.$$.fragment),gMr=l(),Lbe=a("span"),hMr=o("FlaxAutoModelForSequenceClassification"),Vxe=l(),$r=a("div"),f(EA.$$.fragment),pMr=l(),tf=a("p"),_Mr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Bbe=a("code"),uMr=o("from_pretrained()"),bMr=o("class method or the "),xbe=a("code"),vMr=o("from_config()"),TMr=o(`class
method.`),FMr=l(),yA=a("p"),CMr=o("This class cannot be instantiated directly using "),kbe=a("code"),MMr=o("__init__()"),EMr=o(" (throws an error)."),yMr=l(),Lt=a("div"),f(wA.$$.fragment),wMr=l(),Rbe=a("p"),AMr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),LMr=l(),af=a("p"),BMr=o(`Note:
Loading a model from its configuration file does `),Sbe=a("strong"),xMr=o("not"),kMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=a("code"),RMr=o("from_pretrained()"),SMr=o("to load the model weights."),PMr=l(),$be=a("p"),$Mr=o("Examples:"),IMr=l(),f(AA.$$.fragment),DMr=l(),ko=a("div"),f(LA.$$.fragment),jMr=l(),Ibe=a("p"),NMr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),qMr=l(),Bn=a("p"),GMr=o("The model class to instantiate is selected based on the "),Dbe=a("code"),OMr=o("model_type"),XMr=o(` property of the config object (either
passed as an argument or loaded from `),jbe=a("code"),VMr=o("pretrained_model_name_or_path"),zMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nbe=a("code"),WMr=o("pretrained_model_name_or_path"),QMr=o(":"),HMr=l(),be=a("ul"),NC=a("li"),qbe=a("strong"),UMr=o("albert"),JMr=o(" \u2014 "),iV=a("a"),YMr=o("FlaxAlbertForSequenceClassification"),KMr=o(" (ALBERT model)"),ZMr=l(),qC=a("li"),Gbe=a("strong"),e4r=o("bart"),o4r=o(" \u2014 "),dV=a("a"),r4r=o("FlaxBartForSequenceClassification"),t4r=o(" (BART model)"),a4r=l(),GC=a("li"),Obe=a("strong"),n4r=o("bert"),s4r=o(" \u2014 "),cV=a("a"),l4r=o("FlaxBertForSequenceClassification"),i4r=o(" (BERT model)"),d4r=l(),OC=a("li"),Xbe=a("strong"),c4r=o("big_bird"),f4r=o(" \u2014 "),fV=a("a"),m4r=o("FlaxBigBirdForSequenceClassification"),g4r=o(" (BigBird model)"),h4r=l(),XC=a("li"),Vbe=a("strong"),p4r=o("distilbert"),_4r=o(" \u2014 "),mV=a("a"),u4r=o("FlaxDistilBertForSequenceClassification"),b4r=o(" (DistilBERT model)"),v4r=l(),VC=a("li"),zbe=a("strong"),T4r=o("electra"),F4r=o(" \u2014 "),gV=a("a"),C4r=o("FlaxElectraForSequenceClassification"),M4r=o(" (ELECTRA model)"),E4r=l(),zC=a("li"),Wbe=a("strong"),y4r=o("mbart"),w4r=o(" \u2014 "),hV=a("a"),A4r=o("FlaxMBartForSequenceClassification"),L4r=o(" (mBART model)"),B4r=l(),WC=a("li"),Qbe=a("strong"),x4r=o("roberta"),k4r=o(" \u2014 "),pV=a("a"),R4r=o("FlaxRobertaForSequenceClassification"),S4r=o(" (RoBERTa model)"),P4r=l(),QC=a("li"),Hbe=a("strong"),$4r=o("roformer"),I4r=o(" \u2014 "),_V=a("a"),D4r=o("FlaxRoFormerForSequenceClassification"),j4r=o(" (RoFormer model)"),N4r=l(),HC=a("li"),Ube=a("strong"),q4r=o("xlm-roberta"),G4r=o(" \u2014 "),uV=a("a"),O4r=o("FlaxXLMRobertaForSequenceClassification"),X4r=o(" (XLM-RoBERTa model)"),V4r=l(),Jbe=a("p"),z4r=o("Examples:"),W4r=l(),f(BA.$$.fragment),zxe=l(),nf=a("h2"),UC=a("a"),Ybe=a("span"),f(xA.$$.fragment),Q4r=l(),Kbe=a("span"),H4r=o("FlaxAutoModelForQuestionAnswering"),Wxe=l(),Ir=a("div"),f(kA.$$.fragment),U4r=l(),sf=a("p"),J4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Zbe=a("code"),Y4r=o("from_pretrained()"),K4r=o("class method or the "),e5e=a("code"),Z4r=o("from_config()"),eEr=o(`class
method.`),oEr=l(),RA=a("p"),rEr=o("This class cannot be instantiated directly using "),o5e=a("code"),tEr=o("__init__()"),aEr=o(" (throws an error)."),nEr=l(),Bt=a("div"),f(SA.$$.fragment),sEr=l(),r5e=a("p"),lEr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),iEr=l(),lf=a("p"),dEr=o(`Note:
Loading a model from its configuration file does `),t5e=a("strong"),cEr=o("not"),fEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),a5e=a("code"),mEr=o("from_pretrained()"),gEr=o("to load the model weights."),hEr=l(),n5e=a("p"),pEr=o("Examples:"),_Er=l(),f(PA.$$.fragment),uEr=l(),Ro=a("div"),f($A.$$.fragment),bEr=l(),s5e=a("p"),vEr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),TEr=l(),xn=a("p"),FEr=o("The model class to instantiate is selected based on the "),l5e=a("code"),CEr=o("model_type"),MEr=o(` property of the config object (either
passed as an argument or loaded from `),i5e=a("code"),EEr=o("pretrained_model_name_or_path"),yEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d5e=a("code"),wEr=o("pretrained_model_name_or_path"),AEr=o(":"),LEr=l(),ve=a("ul"),JC=a("li"),c5e=a("strong"),BEr=o("albert"),xEr=o(" \u2014 "),bV=a("a"),kEr=o("FlaxAlbertForQuestionAnswering"),REr=o(" (ALBERT model)"),SEr=l(),YC=a("li"),f5e=a("strong"),PEr=o("bart"),$Er=o(" \u2014 "),vV=a("a"),IEr=o("FlaxBartForQuestionAnswering"),DEr=o(" (BART model)"),jEr=l(),KC=a("li"),m5e=a("strong"),NEr=o("bert"),qEr=o(" \u2014 "),TV=a("a"),GEr=o("FlaxBertForQuestionAnswering"),OEr=o(" (BERT model)"),XEr=l(),ZC=a("li"),g5e=a("strong"),VEr=o("big_bird"),zEr=o(" \u2014 "),FV=a("a"),WEr=o("FlaxBigBirdForQuestionAnswering"),QEr=o(" (BigBird model)"),HEr=l(),eM=a("li"),h5e=a("strong"),UEr=o("distilbert"),JEr=o(" \u2014 "),CV=a("a"),YEr=o("FlaxDistilBertForQuestionAnswering"),KEr=o(" (DistilBERT model)"),ZEr=l(),oM=a("li"),p5e=a("strong"),e3r=o("electra"),o3r=o(" \u2014 "),MV=a("a"),r3r=o("FlaxElectraForQuestionAnswering"),t3r=o(" (ELECTRA model)"),a3r=l(),rM=a("li"),_5e=a("strong"),n3r=o("mbart"),s3r=o(" \u2014 "),EV=a("a"),l3r=o("FlaxMBartForQuestionAnswering"),i3r=o(" (mBART model)"),d3r=l(),tM=a("li"),u5e=a("strong"),c3r=o("roberta"),f3r=o(" \u2014 "),yV=a("a"),m3r=o("FlaxRobertaForQuestionAnswering"),g3r=o(" (RoBERTa model)"),h3r=l(),aM=a("li"),b5e=a("strong"),p3r=o("roformer"),_3r=o(" \u2014 "),wV=a("a"),u3r=o("FlaxRoFormerForQuestionAnswering"),b3r=o(" (RoFormer model)"),v3r=l(),nM=a("li"),v5e=a("strong"),T3r=o("xlm-roberta"),F3r=o(" \u2014 "),AV=a("a"),C3r=o("FlaxXLMRobertaForQuestionAnswering"),M3r=o(" (XLM-RoBERTa model)"),E3r=l(),T5e=a("p"),y3r=o("Examples:"),w3r=l(),f(IA.$$.fragment),Qxe=l(),df=a("h2"),sM=a("a"),F5e=a("span"),f(DA.$$.fragment),A3r=l(),C5e=a("span"),L3r=o("FlaxAutoModelForTokenClassification"),Hxe=l(),Dr=a("div"),f(jA.$$.fragment),B3r=l(),cf=a("p"),x3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),M5e=a("code"),k3r=o("from_pretrained()"),R3r=o("class method or the "),E5e=a("code"),S3r=o("from_config()"),P3r=o(`class
method.`),$3r=l(),NA=a("p"),I3r=o("This class cannot be instantiated directly using "),y5e=a("code"),D3r=o("__init__()"),j3r=o(" (throws an error)."),N3r=l(),xt=a("div"),f(qA.$$.fragment),q3r=l(),w5e=a("p"),G3r=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),O3r=l(),ff=a("p"),X3r=o(`Note:
Loading a model from its configuration file does `),A5e=a("strong"),V3r=o("not"),z3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),L5e=a("code"),W3r=o("from_pretrained()"),Q3r=o("to load the model weights."),H3r=l(),B5e=a("p"),U3r=o("Examples:"),J3r=l(),f(GA.$$.fragment),Y3r=l(),So=a("div"),f(OA.$$.fragment),K3r=l(),x5e=a("p"),Z3r=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),eyr=l(),kn=a("p"),oyr=o("The model class to instantiate is selected based on the "),k5e=a("code"),ryr=o("model_type"),tyr=o(` property of the config object (either
passed as an argument or loaded from `),R5e=a("code"),ayr=o("pretrained_model_name_or_path"),nyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S5e=a("code"),syr=o("pretrained_model_name_or_path"),lyr=o(":"),iyr=l(),Re=a("ul"),lM=a("li"),P5e=a("strong"),dyr=o("albert"),cyr=o(" \u2014 "),LV=a("a"),fyr=o("FlaxAlbertForTokenClassification"),myr=o(" (ALBERT model)"),gyr=l(),iM=a("li"),$5e=a("strong"),hyr=o("bert"),pyr=o(" \u2014 "),BV=a("a"),_yr=o("FlaxBertForTokenClassification"),uyr=o(" (BERT model)"),byr=l(),dM=a("li"),I5e=a("strong"),vyr=o("big_bird"),Tyr=o(" \u2014 "),xV=a("a"),Fyr=o("FlaxBigBirdForTokenClassification"),Cyr=o(" (BigBird model)"),Myr=l(),cM=a("li"),D5e=a("strong"),Eyr=o("distilbert"),yyr=o(" \u2014 "),kV=a("a"),wyr=o("FlaxDistilBertForTokenClassification"),Ayr=o(" (DistilBERT model)"),Lyr=l(),fM=a("li"),j5e=a("strong"),Byr=o("electra"),xyr=o(" \u2014 "),RV=a("a"),kyr=o("FlaxElectraForTokenClassification"),Ryr=o(" (ELECTRA model)"),Syr=l(),mM=a("li"),N5e=a("strong"),Pyr=o("roberta"),$yr=o(" \u2014 "),SV=a("a"),Iyr=o("FlaxRobertaForTokenClassification"),Dyr=o(" (RoBERTa model)"),jyr=l(),gM=a("li"),q5e=a("strong"),Nyr=o("roformer"),qyr=o(" \u2014 "),PV=a("a"),Gyr=o("FlaxRoFormerForTokenClassification"),Oyr=o(" (RoFormer model)"),Xyr=l(),hM=a("li"),G5e=a("strong"),Vyr=o("xlm-roberta"),zyr=o(" \u2014 "),$V=a("a"),Wyr=o("FlaxXLMRobertaForTokenClassification"),Qyr=o(" (XLM-RoBERTa model)"),Hyr=l(),O5e=a("p"),Uyr=o("Examples:"),Jyr=l(),f(XA.$$.fragment),Uxe=l(),mf=a("h2"),pM=a("a"),X5e=a("span"),f(VA.$$.fragment),Yyr=l(),V5e=a("span"),Kyr=o("FlaxAutoModelForMultipleChoice"),Jxe=l(),jr=a("div"),f(zA.$$.fragment),Zyr=l(),gf=a("p"),ewr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),z5e=a("code"),owr=o("from_pretrained()"),rwr=o("class method or the "),W5e=a("code"),twr=o("from_config()"),awr=o(`class
method.`),nwr=l(),WA=a("p"),swr=o("This class cannot be instantiated directly using "),Q5e=a("code"),lwr=o("__init__()"),iwr=o(" (throws an error)."),dwr=l(),kt=a("div"),f(QA.$$.fragment),cwr=l(),H5e=a("p"),fwr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),mwr=l(),hf=a("p"),gwr=o(`Note:
Loading a model from its configuration file does `),U5e=a("strong"),hwr=o("not"),pwr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),J5e=a("code"),_wr=o("from_pretrained()"),uwr=o("to load the model weights."),bwr=l(),Y5e=a("p"),vwr=o("Examples:"),Twr=l(),f(HA.$$.fragment),Fwr=l(),Po=a("div"),f(UA.$$.fragment),Cwr=l(),K5e=a("p"),Mwr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Ewr=l(),Rn=a("p"),ywr=o("The model class to instantiate is selected based on the "),Z5e=a("code"),wwr=o("model_type"),Awr=o(` property of the config object (either
passed as an argument or loaded from `),e2e=a("code"),Lwr=o("pretrained_model_name_or_path"),Bwr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o2e=a("code"),xwr=o("pretrained_model_name_or_path"),kwr=o(":"),Rwr=l(),Se=a("ul"),_M=a("li"),r2e=a("strong"),Swr=o("albert"),Pwr=o(" \u2014 "),IV=a("a"),$wr=o("FlaxAlbertForMultipleChoice"),Iwr=o(" (ALBERT model)"),Dwr=l(),uM=a("li"),t2e=a("strong"),jwr=o("bert"),Nwr=o(" \u2014 "),DV=a("a"),qwr=o("FlaxBertForMultipleChoice"),Gwr=o(" (BERT model)"),Owr=l(),bM=a("li"),a2e=a("strong"),Xwr=o("big_bird"),Vwr=o(" \u2014 "),jV=a("a"),zwr=o("FlaxBigBirdForMultipleChoice"),Wwr=o(" (BigBird model)"),Qwr=l(),vM=a("li"),n2e=a("strong"),Hwr=o("distilbert"),Uwr=o(" \u2014 "),NV=a("a"),Jwr=o("FlaxDistilBertForMultipleChoice"),Ywr=o(" (DistilBERT model)"),Kwr=l(),TM=a("li"),s2e=a("strong"),Zwr=o("electra"),e6r=o(" \u2014 "),qV=a("a"),o6r=o("FlaxElectraForMultipleChoice"),r6r=o(" (ELECTRA model)"),t6r=l(),FM=a("li"),l2e=a("strong"),a6r=o("roberta"),n6r=o(" \u2014 "),GV=a("a"),s6r=o("FlaxRobertaForMultipleChoice"),l6r=o(" (RoBERTa model)"),i6r=l(),CM=a("li"),i2e=a("strong"),d6r=o("roformer"),c6r=o(" \u2014 "),OV=a("a"),f6r=o("FlaxRoFormerForMultipleChoice"),m6r=o(" (RoFormer model)"),g6r=l(),MM=a("li"),d2e=a("strong"),h6r=o("xlm-roberta"),p6r=o(" \u2014 "),XV=a("a"),_6r=o("FlaxXLMRobertaForMultipleChoice"),u6r=o(" (XLM-RoBERTa model)"),b6r=l(),c2e=a("p"),v6r=o("Examples:"),T6r=l(),f(JA.$$.fragment),Yxe=l(),pf=a("h2"),EM=a("a"),f2e=a("span"),f(YA.$$.fragment),F6r=l(),m2e=a("span"),C6r=o("FlaxAutoModelForNextSentencePrediction"),Kxe=l(),Nr=a("div"),f(KA.$$.fragment),M6r=l(),_f=a("p"),E6r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),g2e=a("code"),y6r=o("from_pretrained()"),w6r=o("class method or the "),h2e=a("code"),A6r=o("from_config()"),L6r=o(`class
method.`),B6r=l(),ZA=a("p"),x6r=o("This class cannot be instantiated directly using "),p2e=a("code"),k6r=o("__init__()"),R6r=o(" (throws an error)."),S6r=l(),Rt=a("div"),f(eL.$$.fragment),P6r=l(),_2e=a("p"),$6r=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),I6r=l(),uf=a("p"),D6r=o(`Note:
Loading a model from its configuration file does `),u2e=a("strong"),j6r=o("not"),N6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),b2e=a("code"),q6r=o("from_pretrained()"),G6r=o("to load the model weights."),O6r=l(),v2e=a("p"),X6r=o("Examples:"),V6r=l(),f(oL.$$.fragment),z6r=l(),$o=a("div"),f(rL.$$.fragment),W6r=l(),T2e=a("p"),Q6r=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),H6r=l(),Sn=a("p"),U6r=o("The model class to instantiate is selected based on the "),F2e=a("code"),J6r=o("model_type"),Y6r=o(` property of the config object (either
passed as an argument or loaded from `),C2e=a("code"),K6r=o("pretrained_model_name_or_path"),Z6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),M2e=a("code"),eAr=o("pretrained_model_name_or_path"),oAr=o(":"),rAr=l(),E2e=a("ul"),yM=a("li"),y2e=a("strong"),tAr=o("bert"),aAr=o(" \u2014 "),VV=a("a"),nAr=o("FlaxBertForNextSentencePrediction"),sAr=o(" (BERT model)"),lAr=l(),w2e=a("p"),iAr=o("Examples:"),dAr=l(),f(tL.$$.fragment),Zxe=l(),bf=a("h2"),wM=a("a"),A2e=a("span"),f(aL.$$.fragment),cAr=l(),L2e=a("span"),fAr=o("FlaxAutoModelForImageClassification"),eke=l(),qr=a("div"),f(nL.$$.fragment),mAr=l(),vf=a("p"),gAr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),B2e=a("code"),hAr=o("from_pretrained()"),pAr=o("class method or the "),x2e=a("code"),_Ar=o("from_config()"),uAr=o(`class
method.`),bAr=l(),sL=a("p"),vAr=o("This class cannot be instantiated directly using "),k2e=a("code"),TAr=o("__init__()"),FAr=o(" (throws an error)."),CAr=l(),St=a("div"),f(lL.$$.fragment),MAr=l(),R2e=a("p"),EAr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),yAr=l(),Tf=a("p"),wAr=o(`Note:
Loading a model from its configuration file does `),S2e=a("strong"),AAr=o("not"),LAr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),P2e=a("code"),BAr=o("from_pretrained()"),xAr=o("to load the model weights."),kAr=l(),$2e=a("p"),RAr=o("Examples:"),SAr=l(),f(iL.$$.fragment),PAr=l(),Io=a("div"),f(dL.$$.fragment),$Ar=l(),I2e=a("p"),IAr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),DAr=l(),Pn=a("p"),jAr=o("The model class to instantiate is selected based on the "),D2e=a("code"),NAr=o("model_type"),qAr=o(` property of the config object (either
passed as an argument or loaded from `),j2e=a("code"),GAr=o("pretrained_model_name_or_path"),OAr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N2e=a("code"),XAr=o("pretrained_model_name_or_path"),VAr=o(":"),zAr=l(),cL=a("ul"),AM=a("li"),q2e=a("strong"),WAr=o("beit"),QAr=o(" \u2014 "),zV=a("a"),HAr=o("FlaxBeitForImageClassification"),UAr=o(" (BEiT model)"),JAr=l(),LM=a("li"),G2e=a("strong"),YAr=o("vit"),KAr=o(" \u2014 "),WV=a("a"),ZAr=o("FlaxViTForImageClassification"),eLr=o(" (ViT model)"),oLr=l(),O2e=a("p"),rLr=o("Examples:"),tLr=l(),f(fL.$$.fragment),oke=l(),Ff=a("h2"),BM=a("a"),X2e=a("span"),f(mL.$$.fragment),aLr=l(),V2e=a("span"),nLr=o("FlaxAutoModelForVision2Seq"),rke=l(),Gr=a("div"),f(gL.$$.fragment),sLr=l(),Cf=a("p"),lLr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),z2e=a("code"),iLr=o("from_pretrained()"),dLr=o("class method or the "),W2e=a("code"),cLr=o("from_config()"),fLr=o(`class
method.`),mLr=l(),hL=a("p"),gLr=o("This class cannot be instantiated directly using "),Q2e=a("code"),hLr=o("__init__()"),pLr=o(" (throws an error)."),_Lr=l(),Pt=a("div"),f(pL.$$.fragment),uLr=l(),H2e=a("p"),bLr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),vLr=l(),Mf=a("p"),TLr=o(`Note:
Loading a model from its configuration file does `),U2e=a("strong"),FLr=o("not"),CLr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),J2e=a("code"),MLr=o("from_pretrained()"),ELr=o("to load the model weights."),yLr=l(),Y2e=a("p"),wLr=o("Examples:"),ALr=l(),f(_L.$$.fragment),LLr=l(),Do=a("div"),f(uL.$$.fragment),BLr=l(),K2e=a("p"),xLr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),kLr=l(),$n=a("p"),RLr=o("The model class to instantiate is selected based on the "),Z2e=a("code"),SLr=o("model_type"),PLr=o(` property of the config object (either
passed as an argument or loaded from `),eve=a("code"),$Lr=o("pretrained_model_name_or_path"),ILr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ove=a("code"),DLr=o("pretrained_model_name_or_path"),jLr=o(":"),NLr=l(),rve=a("ul"),xM=a("li"),tve=a("strong"),qLr=o("vision-encoder-decoder"),GLr=o(" \u2014 "),QV=a("a"),OLr=o("FlaxVisionEncoderDecoderModel"),XLr=o(" (Vision Encoder decoder model)"),VLr=l(),ave=a("p"),zLr=o("Examples:"),WLr=l(),f(bL.$$.fragment),this.h()},l(c){const u=Y5t('[data-svelte="svelte-1phssyn"]',document.head);J=n(u,"META",{name:!0,content:!0}),u.forEach(t),Pe=i(c),ie=n(c,"H1",{class:!0});var vL=s(ie);ge=n(vL,"A",{id:!0,class:!0,href:!0});var nve=s(ge);lo=n(nve,"SPAN",{});var sve=s(lo);m(fe.$$.fragment,sve),sve.forEach(t),nve.forEach(t),Te=i(vL),Xo=n(vL,"SPAN",{});var HLr=s(Xo);Bi=r(HLr,"Auto Classes"),HLr.forEach(t),vL.forEach(t),yf=i(c),sa=n(c,"P",{});var ake=s(sa);xi=r(ake,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),ki=n(ake,"CODE",{});var ULr=s(ki);x4=r(ULr,"from_pretrained()"),ULr.forEach(t),wf=r(ake,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ake.forEach(t),Le=i(c),io=n(c,"P",{});var kM=s(io);Ri=r(kM,"Instantiating one of "),In=n(kM,"A",{href:!0});var JLr=s(In);k4=r(JLr,"AutoConfig"),JLr.forEach(t),Dn=r(kM,", "),jn=n(kM,"A",{href:!0});var YLr=s(jn);R4=r(YLr,"AutoModel"),YLr.forEach(t),Si=r(kM,`, and
`),Nn=n(kM,"A",{href:!0});var KLr=s(Nn);S4=r(KLr,"AutoTokenizer"),KLr.forEach(t),Pi=r(kM," will directly create a class of the relevant architecture. For instance"),kM.forEach(t),Af=i(c),m($a.$$.fragment,c),co=i(c),he=n(c,"P",{});var nke=s(he);_8=r(nke,"will create a model that is an instance of "),$i=n(nke,"A",{href:!0});var ZLr=s($i);u8=r(ZLr,"BertModel"),ZLr.forEach(t),b8=r(nke,"."),nke.forEach(t),Vo=i(c),Ia=n(c,"P",{});var ske=s(Ia);v8=r(ske,"There is one class of "),Lf=n(ske,"CODE",{});var e8r=s(Lf);T8=r(e8r,"AutoModel"),e8r.forEach(t),hSe=r(ske," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),ske.forEach(t),aBe=i(c),Ii=n(c,"H2",{class:!0});var lke=s(Ii);Bf=n(lke,"A",{id:!0,class:!0,href:!0});var o8r=s(Bf);jW=n(o8r,"SPAN",{});var r8r=s(jW);m(P4.$$.fragment,r8r),r8r.forEach(t),o8r.forEach(t),pSe=i(lke),NW=n(lke,"SPAN",{});var t8r=s(NW);_Se=r(t8r,"Extending the Auto Classes"),t8r.forEach(t),lke.forEach(t),nBe=i(c),qn=n(c,"P",{});var HV=s(qn);uSe=r(HV,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),qW=n(HV,"CODE",{});var a8r=s(qW);bSe=r(a8r,"NewModel"),a8r.forEach(t),vSe=r(HV,", make sure you have a "),GW=n(HV,"CODE",{});var n8r=s(GW);TSe=r(n8r,"NewModelConfig"),n8r.forEach(t),FSe=r(HV,` then you can add those to the auto
classes like this:`),HV.forEach(t),sBe=i(c),m($4.$$.fragment,c),lBe=i(c),F8=n(c,"P",{});var s8r=s(F8);CSe=r(s8r,"You will then be able to use the auto classes like you would usually do!"),s8r.forEach(t),iBe=i(c),m(xf.$$.fragment,c),dBe=i(c),Di=n(c,"H2",{class:!0});var ike=s(Di);kf=n(ike,"A",{id:!0,class:!0,href:!0});var l8r=s(kf);OW=n(l8r,"SPAN",{});var i8r=s(OW);m(I4.$$.fragment,i8r),i8r.forEach(t),l8r.forEach(t),MSe=i(ike),XW=n(ike,"SPAN",{});var d8r=s(XW);ESe=r(d8r,"AutoConfig"),d8r.forEach(t),ike.forEach(t),cBe=i(c),zo=n(c,"DIV",{class:!0});var Ds=s(zo);m(D4.$$.fragment,Ds),ySe=i(Ds),j4=n(Ds,"P",{});var dke=s(j4);wSe=r(dke,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),C8=n(dke,"A",{href:!0});var c8r=s(C8);ASe=r(c8r,"from_pretrained()"),c8r.forEach(t),LSe=r(dke," class method."),dke.forEach(t),BSe=i(Ds),N4=n(Ds,"P",{});var cke=s(N4);xSe=r(cke,"This class cannot be instantiated directly using "),VW=n(cke,"CODE",{});var f8r=s(VW);kSe=r(f8r,"__init__()"),f8r.forEach(t),RSe=r(cke," (throws an error)."),cke.forEach(t),SSe=i(Ds),fo=n(Ds,"DIV",{class:!0});var ia=s(fo);m(q4.$$.fragment,ia),PSe=i(ia),zW=n(ia,"P",{});var m8r=s(zW);$Se=r(m8r,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),m8r.forEach(t),ISe=i(ia),ji=n(ia,"P",{});var UV=s(ji);DSe=r(UV,"The configuration class to instantiate is selected based on the "),WW=n(UV,"CODE",{});var g8r=s(WW);jSe=r(g8r,"model_type"),g8r.forEach(t),NSe=r(UV,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),QW=n(UV,"CODE",{});var h8r=s(QW);qSe=r(h8r,"pretrained_model_name_or_path"),h8r.forEach(t),GSe=r(UV,":"),UV.forEach(t),OSe=i(ia),v=n(ia,"UL",{});var T=s(v);Rf=n(T,"LI",{});var lve=s(Rf);HW=n(lve,"STRONG",{});var p8r=s(HW);XSe=r(p8r,"albert"),p8r.forEach(t),VSe=r(lve," \u2014 "),M8=n(lve,"A",{href:!0});var _8r=s(M8);zSe=r(_8r,"AlbertConfig"),_8r.forEach(t),WSe=r(lve," (ALBERT model)"),lve.forEach(t),QSe=i(T),Sf=n(T,"LI",{});var ive=s(Sf);UW=n(ive,"STRONG",{});var u8r=s(UW);HSe=r(u8r,"bart"),u8r.forEach(t),USe=r(ive," \u2014 "),E8=n(ive,"A",{href:!0});var b8r=s(E8);JSe=r(b8r,"BartConfig"),b8r.forEach(t),YSe=r(ive," (BART model)"),ive.forEach(t),KSe=i(T),Pf=n(T,"LI",{});var dve=s(Pf);JW=n(dve,"STRONG",{});var v8r=s(JW);ZSe=r(v8r,"beit"),v8r.forEach(t),ePe=r(dve," \u2014 "),y8=n(dve,"A",{href:!0});var T8r=s(y8);oPe=r(T8r,"BeitConfig"),T8r.forEach(t),rPe=r(dve," (BEiT model)"),dve.forEach(t),tPe=i(T),$f=n(T,"LI",{});var cve=s($f);YW=n(cve,"STRONG",{});var F8r=s(YW);aPe=r(F8r,"bert"),F8r.forEach(t),nPe=r(cve," \u2014 "),w8=n(cve,"A",{href:!0});var C8r=s(w8);sPe=r(C8r,"BertConfig"),C8r.forEach(t),lPe=r(cve," (BERT model)"),cve.forEach(t),iPe=i(T),If=n(T,"LI",{});var fve=s(If);KW=n(fve,"STRONG",{});var M8r=s(KW);dPe=r(M8r,"bert-generation"),M8r.forEach(t),cPe=r(fve," \u2014 "),A8=n(fve,"A",{href:!0});var E8r=s(A8);fPe=r(E8r,"BertGenerationConfig"),E8r.forEach(t),mPe=r(fve," (Bert Generation model)"),fve.forEach(t),gPe=i(T),Df=n(T,"LI",{});var mve=s(Df);ZW=n(mve,"STRONG",{});var y8r=s(ZW);hPe=r(y8r,"big_bird"),y8r.forEach(t),pPe=r(mve," \u2014 "),L8=n(mve,"A",{href:!0});var w8r=s(L8);_Pe=r(w8r,"BigBirdConfig"),w8r.forEach(t),uPe=r(mve," (BigBird model)"),mve.forEach(t),bPe=i(T),jf=n(T,"LI",{});var gve=s(jf);eQ=n(gve,"STRONG",{});var A8r=s(eQ);vPe=r(A8r,"bigbird_pegasus"),A8r.forEach(t),TPe=r(gve," \u2014 "),B8=n(gve,"A",{href:!0});var L8r=s(B8);FPe=r(L8r,"BigBirdPegasusConfig"),L8r.forEach(t),CPe=r(gve," (BigBirdPegasus model)"),gve.forEach(t),MPe=i(T),Nf=n(T,"LI",{});var hve=s(Nf);oQ=n(hve,"STRONG",{});var B8r=s(oQ);EPe=r(B8r,"blenderbot"),B8r.forEach(t),yPe=r(hve," \u2014 "),x8=n(hve,"A",{href:!0});var x8r=s(x8);wPe=r(x8r,"BlenderbotConfig"),x8r.forEach(t),APe=r(hve," (Blenderbot model)"),hve.forEach(t),LPe=i(T),qf=n(T,"LI",{});var pve=s(qf);rQ=n(pve,"STRONG",{});var k8r=s(rQ);BPe=r(k8r,"blenderbot-small"),k8r.forEach(t),xPe=r(pve," \u2014 "),k8=n(pve,"A",{href:!0});var R8r=s(k8);kPe=r(R8r,"BlenderbotSmallConfig"),R8r.forEach(t),RPe=r(pve," (BlenderbotSmall model)"),pve.forEach(t),SPe=i(T),Gf=n(T,"LI",{});var _ve=s(Gf);tQ=n(_ve,"STRONG",{});var S8r=s(tQ);PPe=r(S8r,"camembert"),S8r.forEach(t),$Pe=r(_ve," \u2014 "),R8=n(_ve,"A",{href:!0});var P8r=s(R8);IPe=r(P8r,"CamembertConfig"),P8r.forEach(t),DPe=r(_ve," (CamemBERT model)"),_ve.forEach(t),jPe=i(T),Of=n(T,"LI",{});var uve=s(Of);aQ=n(uve,"STRONG",{});var $8r=s(aQ);NPe=r($8r,"canine"),$8r.forEach(t),qPe=r(uve," \u2014 "),S8=n(uve,"A",{href:!0});var I8r=s(S8);GPe=r(I8r,"CanineConfig"),I8r.forEach(t),OPe=r(uve," (Canine model)"),uve.forEach(t),XPe=i(T),Xf=n(T,"LI",{});var bve=s(Xf);nQ=n(bve,"STRONG",{});var D8r=s(nQ);VPe=r(D8r,"clip"),D8r.forEach(t),zPe=r(bve," \u2014 "),P8=n(bve,"A",{href:!0});var j8r=s(P8);WPe=r(j8r,"CLIPConfig"),j8r.forEach(t),QPe=r(bve," (CLIP model)"),bve.forEach(t),HPe=i(T),Vf=n(T,"LI",{});var vve=s(Vf);sQ=n(vve,"STRONG",{});var N8r=s(sQ);UPe=r(N8r,"convbert"),N8r.forEach(t),JPe=r(vve," \u2014 "),$8=n(vve,"A",{href:!0});var q8r=s($8);YPe=r(q8r,"ConvBertConfig"),q8r.forEach(t),KPe=r(vve," (ConvBERT model)"),vve.forEach(t),ZPe=i(T),zf=n(T,"LI",{});var Tve=s(zf);lQ=n(Tve,"STRONG",{});var G8r=s(lQ);e$e=r(G8r,"convnext"),G8r.forEach(t),o$e=r(Tve," \u2014 "),I8=n(Tve,"A",{href:!0});var O8r=s(I8);r$e=r(O8r,"ConvNextConfig"),O8r.forEach(t),t$e=r(Tve," (ConvNext model)"),Tve.forEach(t),a$e=i(T),Wf=n(T,"LI",{});var Fve=s(Wf);iQ=n(Fve,"STRONG",{});var X8r=s(iQ);n$e=r(X8r,"ctrl"),X8r.forEach(t),s$e=r(Fve," \u2014 "),D8=n(Fve,"A",{href:!0});var V8r=s(D8);l$e=r(V8r,"CTRLConfig"),V8r.forEach(t),i$e=r(Fve," (CTRL model)"),Fve.forEach(t),d$e=i(T),Qf=n(T,"LI",{});var Cve=s(Qf);dQ=n(Cve,"STRONG",{});var z8r=s(dQ);c$e=r(z8r,"data2vec-audio"),z8r.forEach(t),f$e=r(Cve," \u2014 "),j8=n(Cve,"A",{href:!0});var W8r=s(j8);m$e=r(W8r,"Data2VecAudioConfig"),W8r.forEach(t),g$e=r(Cve," (Data2VecAudio model)"),Cve.forEach(t),h$e=i(T),Hf=n(T,"LI",{});var Mve=s(Hf);cQ=n(Mve,"STRONG",{});var Q8r=s(cQ);p$e=r(Q8r,"data2vec-text"),Q8r.forEach(t),_$e=r(Mve," \u2014 "),N8=n(Mve,"A",{href:!0});var H8r=s(N8);u$e=r(H8r,"Data2VecTextConfig"),H8r.forEach(t),b$e=r(Mve," (Data2VecText model)"),Mve.forEach(t),v$e=i(T),Uf=n(T,"LI",{});var Eve=s(Uf);fQ=n(Eve,"STRONG",{});var U8r=s(fQ);T$e=r(U8r,"deberta"),U8r.forEach(t),F$e=r(Eve," \u2014 "),q8=n(Eve,"A",{href:!0});var J8r=s(q8);C$e=r(J8r,"DebertaConfig"),J8r.forEach(t),M$e=r(Eve," (DeBERTa model)"),Eve.forEach(t),E$e=i(T),Jf=n(T,"LI",{});var yve=s(Jf);mQ=n(yve,"STRONG",{});var Y8r=s(mQ);y$e=r(Y8r,"deberta-v2"),Y8r.forEach(t),w$e=r(yve," \u2014 "),G8=n(yve,"A",{href:!0});var K8r=s(G8);A$e=r(K8r,"DebertaV2Config"),K8r.forEach(t),L$e=r(yve," (DeBERTa-v2 model)"),yve.forEach(t),B$e=i(T),Yf=n(T,"LI",{});var wve=s(Yf);gQ=n(wve,"STRONG",{});var Z8r=s(gQ);x$e=r(Z8r,"deit"),Z8r.forEach(t),k$e=r(wve," \u2014 "),O8=n(wve,"A",{href:!0});var e7r=s(O8);R$e=r(e7r,"DeiTConfig"),e7r.forEach(t),S$e=r(wve," (DeiT model)"),wve.forEach(t),P$e=i(T),Kf=n(T,"LI",{});var Ave=s(Kf);hQ=n(Ave,"STRONG",{});var o7r=s(hQ);$$e=r(o7r,"detr"),o7r.forEach(t),I$e=r(Ave," \u2014 "),X8=n(Ave,"A",{href:!0});var r7r=s(X8);D$e=r(r7r,"DetrConfig"),r7r.forEach(t),j$e=r(Ave," (DETR model)"),Ave.forEach(t),N$e=i(T),Zf=n(T,"LI",{});var Lve=s(Zf);pQ=n(Lve,"STRONG",{});var t7r=s(pQ);q$e=r(t7r,"distilbert"),t7r.forEach(t),G$e=r(Lve," \u2014 "),V8=n(Lve,"A",{href:!0});var a7r=s(V8);O$e=r(a7r,"DistilBertConfig"),a7r.forEach(t),X$e=r(Lve," (DistilBERT model)"),Lve.forEach(t),V$e=i(T),em=n(T,"LI",{});var Bve=s(em);_Q=n(Bve,"STRONG",{});var n7r=s(_Q);z$e=r(n7r,"dpr"),n7r.forEach(t),W$e=r(Bve," \u2014 "),z8=n(Bve,"A",{href:!0});var s7r=s(z8);Q$e=r(s7r,"DPRConfig"),s7r.forEach(t),H$e=r(Bve," (DPR model)"),Bve.forEach(t),U$e=i(T),om=n(T,"LI",{});var xve=s(om);uQ=n(xve,"STRONG",{});var l7r=s(uQ);J$e=r(l7r,"electra"),l7r.forEach(t),Y$e=r(xve," \u2014 "),W8=n(xve,"A",{href:!0});var i7r=s(W8);K$e=r(i7r,"ElectraConfig"),i7r.forEach(t),Z$e=r(xve," (ELECTRA model)"),xve.forEach(t),eIe=i(T),rm=n(T,"LI",{});var kve=s(rm);bQ=n(kve,"STRONG",{});var d7r=s(bQ);oIe=r(d7r,"encoder-decoder"),d7r.forEach(t),rIe=r(kve," \u2014 "),Q8=n(kve,"A",{href:!0});var c7r=s(Q8);tIe=r(c7r,"EncoderDecoderConfig"),c7r.forEach(t),aIe=r(kve," (Encoder decoder model)"),kve.forEach(t),nIe=i(T),tm=n(T,"LI",{});var Rve=s(tm);vQ=n(Rve,"STRONG",{});var f7r=s(vQ);sIe=r(f7r,"flaubert"),f7r.forEach(t),lIe=r(Rve," \u2014 "),H8=n(Rve,"A",{href:!0});var m7r=s(H8);iIe=r(m7r,"FlaubertConfig"),m7r.forEach(t),dIe=r(Rve," (FlauBERT model)"),Rve.forEach(t),cIe=i(T),am=n(T,"LI",{});var Sve=s(am);TQ=n(Sve,"STRONG",{});var g7r=s(TQ);fIe=r(g7r,"fnet"),g7r.forEach(t),mIe=r(Sve," \u2014 "),U8=n(Sve,"A",{href:!0});var h7r=s(U8);gIe=r(h7r,"FNetConfig"),h7r.forEach(t),hIe=r(Sve," (FNet model)"),Sve.forEach(t),pIe=i(T),nm=n(T,"LI",{});var Pve=s(nm);FQ=n(Pve,"STRONG",{});var p7r=s(FQ);_Ie=r(p7r,"fsmt"),p7r.forEach(t),uIe=r(Pve," \u2014 "),J8=n(Pve,"A",{href:!0});var _7r=s(J8);bIe=r(_7r,"FSMTConfig"),_7r.forEach(t),vIe=r(Pve," (FairSeq Machine-Translation model)"),Pve.forEach(t),TIe=i(T),sm=n(T,"LI",{});var $ve=s(sm);CQ=n($ve,"STRONG",{});var u7r=s(CQ);FIe=r(u7r,"funnel"),u7r.forEach(t),CIe=r($ve," \u2014 "),Y8=n($ve,"A",{href:!0});var b7r=s(Y8);MIe=r(b7r,"FunnelConfig"),b7r.forEach(t),EIe=r($ve," (Funnel Transformer model)"),$ve.forEach(t),yIe=i(T),lm=n(T,"LI",{});var Ive=s(lm);MQ=n(Ive,"STRONG",{});var v7r=s(MQ);wIe=r(v7r,"gpt2"),v7r.forEach(t),AIe=r(Ive," \u2014 "),K8=n(Ive,"A",{href:!0});var T7r=s(K8);LIe=r(T7r,"GPT2Config"),T7r.forEach(t),BIe=r(Ive," (OpenAI GPT-2 model)"),Ive.forEach(t),xIe=i(T),im=n(T,"LI",{});var Dve=s(im);EQ=n(Dve,"STRONG",{});var F7r=s(EQ);kIe=r(F7r,"gpt_neo"),F7r.forEach(t),RIe=r(Dve," \u2014 "),Z8=n(Dve,"A",{href:!0});var C7r=s(Z8);SIe=r(C7r,"GPTNeoConfig"),C7r.forEach(t),PIe=r(Dve," (GPT Neo model)"),Dve.forEach(t),$Ie=i(T),dm=n(T,"LI",{});var jve=s(dm);yQ=n(jve,"STRONG",{});var M7r=s(yQ);IIe=r(M7r,"gptj"),M7r.forEach(t),DIe=r(jve," \u2014 "),e7=n(jve,"A",{href:!0});var E7r=s(e7);jIe=r(E7r,"GPTJConfig"),E7r.forEach(t),NIe=r(jve," (GPT-J model)"),jve.forEach(t),qIe=i(T),cm=n(T,"LI",{});var Nve=s(cm);wQ=n(Nve,"STRONG",{});var y7r=s(wQ);GIe=r(y7r,"hubert"),y7r.forEach(t),OIe=r(Nve," \u2014 "),o7=n(Nve,"A",{href:!0});var w7r=s(o7);XIe=r(w7r,"HubertConfig"),w7r.forEach(t),VIe=r(Nve," (Hubert model)"),Nve.forEach(t),zIe=i(T),fm=n(T,"LI",{});var qve=s(fm);AQ=n(qve,"STRONG",{});var A7r=s(AQ);WIe=r(A7r,"ibert"),A7r.forEach(t),QIe=r(qve," \u2014 "),r7=n(qve,"A",{href:!0});var L7r=s(r7);HIe=r(L7r,"IBertConfig"),L7r.forEach(t),UIe=r(qve," (I-BERT model)"),qve.forEach(t),JIe=i(T),mm=n(T,"LI",{});var Gve=s(mm);LQ=n(Gve,"STRONG",{});var B7r=s(LQ);YIe=r(B7r,"imagegpt"),B7r.forEach(t),KIe=r(Gve," \u2014 "),t7=n(Gve,"A",{href:!0});var x7r=s(t7);ZIe=r(x7r,"ImageGPTConfig"),x7r.forEach(t),eDe=r(Gve," (ImageGPT model)"),Gve.forEach(t),oDe=i(T),gm=n(T,"LI",{});var Ove=s(gm);BQ=n(Ove,"STRONG",{});var k7r=s(BQ);rDe=r(k7r,"layoutlm"),k7r.forEach(t),tDe=r(Ove," \u2014 "),a7=n(Ove,"A",{href:!0});var R7r=s(a7);aDe=r(R7r,"LayoutLMConfig"),R7r.forEach(t),nDe=r(Ove," (LayoutLM model)"),Ove.forEach(t),sDe=i(T),hm=n(T,"LI",{});var Xve=s(hm);xQ=n(Xve,"STRONG",{});var S7r=s(xQ);lDe=r(S7r,"layoutlmv2"),S7r.forEach(t),iDe=r(Xve," \u2014 "),n7=n(Xve,"A",{href:!0});var P7r=s(n7);dDe=r(P7r,"LayoutLMv2Config"),P7r.forEach(t),cDe=r(Xve," (LayoutLMv2 model)"),Xve.forEach(t),fDe=i(T),pm=n(T,"LI",{});var Vve=s(pm);kQ=n(Vve,"STRONG",{});var $7r=s(kQ);mDe=r($7r,"led"),$7r.forEach(t),gDe=r(Vve," \u2014 "),s7=n(Vve,"A",{href:!0});var I7r=s(s7);hDe=r(I7r,"LEDConfig"),I7r.forEach(t),pDe=r(Vve," (LED model)"),Vve.forEach(t),_De=i(T),_m=n(T,"LI",{});var zve=s(_m);RQ=n(zve,"STRONG",{});var D7r=s(RQ);uDe=r(D7r,"longformer"),D7r.forEach(t),bDe=r(zve," \u2014 "),l7=n(zve,"A",{href:!0});var j7r=s(l7);vDe=r(j7r,"LongformerConfig"),j7r.forEach(t),TDe=r(zve," (Longformer model)"),zve.forEach(t),FDe=i(T),um=n(T,"LI",{});var Wve=s(um);SQ=n(Wve,"STRONG",{});var N7r=s(SQ);CDe=r(N7r,"luke"),N7r.forEach(t),MDe=r(Wve," \u2014 "),i7=n(Wve,"A",{href:!0});var q7r=s(i7);EDe=r(q7r,"LukeConfig"),q7r.forEach(t),yDe=r(Wve," (LUKE model)"),Wve.forEach(t),wDe=i(T),bm=n(T,"LI",{});var Qve=s(bm);PQ=n(Qve,"STRONG",{});var G7r=s(PQ);ADe=r(G7r,"lxmert"),G7r.forEach(t),LDe=r(Qve," \u2014 "),d7=n(Qve,"A",{href:!0});var O7r=s(d7);BDe=r(O7r,"LxmertConfig"),O7r.forEach(t),xDe=r(Qve," (LXMERT model)"),Qve.forEach(t),kDe=i(T),vm=n(T,"LI",{});var Hve=s(vm);$Q=n(Hve,"STRONG",{});var X7r=s($Q);RDe=r(X7r,"m2m_100"),X7r.forEach(t),SDe=r(Hve," \u2014 "),c7=n(Hve,"A",{href:!0});var V7r=s(c7);PDe=r(V7r,"M2M100Config"),V7r.forEach(t),$De=r(Hve," (M2M100 model)"),Hve.forEach(t),IDe=i(T),Tm=n(T,"LI",{});var Uve=s(Tm);IQ=n(Uve,"STRONG",{});var z7r=s(IQ);DDe=r(z7r,"marian"),z7r.forEach(t),jDe=r(Uve," \u2014 "),f7=n(Uve,"A",{href:!0});var W7r=s(f7);NDe=r(W7r,"MarianConfig"),W7r.forEach(t),qDe=r(Uve," (Marian model)"),Uve.forEach(t),GDe=i(T),Fm=n(T,"LI",{});var Jve=s(Fm);DQ=n(Jve,"STRONG",{});var Q7r=s(DQ);ODe=r(Q7r,"maskformer"),Q7r.forEach(t),XDe=r(Jve," \u2014 "),m7=n(Jve,"A",{href:!0});var H7r=s(m7);VDe=r(H7r,"MaskFormerConfig"),H7r.forEach(t),zDe=r(Jve," (MaskFormer model)"),Jve.forEach(t),WDe=i(T),Cm=n(T,"LI",{});var Yve=s(Cm);jQ=n(Yve,"STRONG",{});var U7r=s(jQ);QDe=r(U7r,"mbart"),U7r.forEach(t),HDe=r(Yve," \u2014 "),g7=n(Yve,"A",{href:!0});var J7r=s(g7);UDe=r(J7r,"MBartConfig"),J7r.forEach(t),JDe=r(Yve," (mBART model)"),Yve.forEach(t),YDe=i(T),Mm=n(T,"LI",{});var Kve=s(Mm);NQ=n(Kve,"STRONG",{});var Y7r=s(NQ);KDe=r(Y7r,"megatron-bert"),Y7r.forEach(t),ZDe=r(Kve," \u2014 "),h7=n(Kve,"A",{href:!0});var K7r=s(h7);eje=r(K7r,"MegatronBertConfig"),K7r.forEach(t),oje=r(Kve," (MegatronBert model)"),Kve.forEach(t),rje=i(T),Em=n(T,"LI",{});var Zve=s(Em);qQ=n(Zve,"STRONG",{});var Z7r=s(qQ);tje=r(Z7r,"mobilebert"),Z7r.forEach(t),aje=r(Zve," \u2014 "),p7=n(Zve,"A",{href:!0});var eBr=s(p7);nje=r(eBr,"MobileBertConfig"),eBr.forEach(t),sje=r(Zve," (MobileBERT model)"),Zve.forEach(t),lje=i(T),ym=n(T,"LI",{});var eTe=s(ym);GQ=n(eTe,"STRONG",{});var oBr=s(GQ);ije=r(oBr,"mpnet"),oBr.forEach(t),dje=r(eTe," \u2014 "),_7=n(eTe,"A",{href:!0});var rBr=s(_7);cje=r(rBr,"MPNetConfig"),rBr.forEach(t),fje=r(eTe," (MPNet model)"),eTe.forEach(t),mje=i(T),wm=n(T,"LI",{});var oTe=s(wm);OQ=n(oTe,"STRONG",{});var tBr=s(OQ);gje=r(tBr,"mt5"),tBr.forEach(t),hje=r(oTe," \u2014 "),u7=n(oTe,"A",{href:!0});var aBr=s(u7);pje=r(aBr,"MT5Config"),aBr.forEach(t),_je=r(oTe," (mT5 model)"),oTe.forEach(t),uje=i(T),Am=n(T,"LI",{});var rTe=s(Am);XQ=n(rTe,"STRONG",{});var nBr=s(XQ);bje=r(nBr,"nystromformer"),nBr.forEach(t),vje=r(rTe," \u2014 "),b7=n(rTe,"A",{href:!0});var sBr=s(b7);Tje=r(sBr,"NystromformerConfig"),sBr.forEach(t),Fje=r(rTe," (Nystromformer model)"),rTe.forEach(t),Cje=i(T),Lm=n(T,"LI",{});var tTe=s(Lm);VQ=n(tTe,"STRONG",{});var lBr=s(VQ);Mje=r(lBr,"openai-gpt"),lBr.forEach(t),Eje=r(tTe," \u2014 "),v7=n(tTe,"A",{href:!0});var iBr=s(v7);yje=r(iBr,"OpenAIGPTConfig"),iBr.forEach(t),wje=r(tTe," (OpenAI GPT model)"),tTe.forEach(t),Aje=i(T),Bm=n(T,"LI",{});var aTe=s(Bm);zQ=n(aTe,"STRONG",{});var dBr=s(zQ);Lje=r(dBr,"pegasus"),dBr.forEach(t),Bje=r(aTe," \u2014 "),T7=n(aTe,"A",{href:!0});var cBr=s(T7);xje=r(cBr,"PegasusConfig"),cBr.forEach(t),kje=r(aTe," (Pegasus model)"),aTe.forEach(t),Rje=i(T),xm=n(T,"LI",{});var nTe=s(xm);WQ=n(nTe,"STRONG",{});var fBr=s(WQ);Sje=r(fBr,"perceiver"),fBr.forEach(t),Pje=r(nTe," \u2014 "),F7=n(nTe,"A",{href:!0});var mBr=s(F7);$je=r(mBr,"PerceiverConfig"),mBr.forEach(t),Ije=r(nTe," (Perceiver model)"),nTe.forEach(t),Dje=i(T),km=n(T,"LI",{});var sTe=s(km);QQ=n(sTe,"STRONG",{});var gBr=s(QQ);jje=r(gBr,"plbart"),gBr.forEach(t),Nje=r(sTe," \u2014 "),C7=n(sTe,"A",{href:!0});var hBr=s(C7);qje=r(hBr,"PLBartConfig"),hBr.forEach(t),Gje=r(sTe," (PLBart model)"),sTe.forEach(t),Oje=i(T),Rm=n(T,"LI",{});var lTe=s(Rm);HQ=n(lTe,"STRONG",{});var pBr=s(HQ);Xje=r(pBr,"poolformer"),pBr.forEach(t),Vje=r(lTe," \u2014 "),M7=n(lTe,"A",{href:!0});var _Br=s(M7);zje=r(_Br,"PoolFormerConfig"),_Br.forEach(t),Wje=r(lTe," (PoolFormer model)"),lTe.forEach(t),Qje=i(T),Sm=n(T,"LI",{});var iTe=s(Sm);UQ=n(iTe,"STRONG",{});var uBr=s(UQ);Hje=r(uBr,"prophetnet"),uBr.forEach(t),Uje=r(iTe," \u2014 "),E7=n(iTe,"A",{href:!0});var bBr=s(E7);Jje=r(bBr,"ProphetNetConfig"),bBr.forEach(t),Yje=r(iTe," (ProphetNet model)"),iTe.forEach(t),Kje=i(T),Pm=n(T,"LI",{});var dTe=s(Pm);JQ=n(dTe,"STRONG",{});var vBr=s(JQ);Zje=r(vBr,"qdqbert"),vBr.forEach(t),eNe=r(dTe," \u2014 "),y7=n(dTe,"A",{href:!0});var TBr=s(y7);oNe=r(TBr,"QDQBertConfig"),TBr.forEach(t),rNe=r(dTe," (QDQBert model)"),dTe.forEach(t),tNe=i(T),$m=n(T,"LI",{});var cTe=s($m);YQ=n(cTe,"STRONG",{});var FBr=s(YQ);aNe=r(FBr,"rag"),FBr.forEach(t),nNe=r(cTe," \u2014 "),w7=n(cTe,"A",{href:!0});var CBr=s(w7);sNe=r(CBr,"RagConfig"),CBr.forEach(t),lNe=r(cTe," (RAG model)"),cTe.forEach(t),iNe=i(T),Im=n(T,"LI",{});var fTe=s(Im);KQ=n(fTe,"STRONG",{});var MBr=s(KQ);dNe=r(MBr,"realm"),MBr.forEach(t),cNe=r(fTe," \u2014 "),A7=n(fTe,"A",{href:!0});var EBr=s(A7);fNe=r(EBr,"RealmConfig"),EBr.forEach(t),mNe=r(fTe," (Realm model)"),fTe.forEach(t),gNe=i(T),Dm=n(T,"LI",{});var mTe=s(Dm);ZQ=n(mTe,"STRONG",{});var yBr=s(ZQ);hNe=r(yBr,"reformer"),yBr.forEach(t),pNe=r(mTe," \u2014 "),L7=n(mTe,"A",{href:!0});var wBr=s(L7);_Ne=r(wBr,"ReformerConfig"),wBr.forEach(t),uNe=r(mTe," (Reformer model)"),mTe.forEach(t),bNe=i(T),jm=n(T,"LI",{});var gTe=s(jm);eH=n(gTe,"STRONG",{});var ABr=s(eH);vNe=r(ABr,"rembert"),ABr.forEach(t),TNe=r(gTe," \u2014 "),B7=n(gTe,"A",{href:!0});var LBr=s(B7);FNe=r(LBr,"RemBertConfig"),LBr.forEach(t),CNe=r(gTe," (RemBERT model)"),gTe.forEach(t),MNe=i(T),Nm=n(T,"LI",{});var hTe=s(Nm);oH=n(hTe,"STRONG",{});var BBr=s(oH);ENe=r(BBr,"retribert"),BBr.forEach(t),yNe=r(hTe," \u2014 "),x7=n(hTe,"A",{href:!0});var xBr=s(x7);wNe=r(xBr,"RetriBertConfig"),xBr.forEach(t),ANe=r(hTe," (RetriBERT model)"),hTe.forEach(t),LNe=i(T),qm=n(T,"LI",{});var pTe=s(qm);rH=n(pTe,"STRONG",{});var kBr=s(rH);BNe=r(kBr,"roberta"),kBr.forEach(t),xNe=r(pTe," \u2014 "),k7=n(pTe,"A",{href:!0});var RBr=s(k7);kNe=r(RBr,"RobertaConfig"),RBr.forEach(t),RNe=r(pTe," (RoBERTa model)"),pTe.forEach(t),SNe=i(T),Gm=n(T,"LI",{});var _Te=s(Gm);tH=n(_Te,"STRONG",{});var SBr=s(tH);PNe=r(SBr,"roformer"),SBr.forEach(t),$Ne=r(_Te," \u2014 "),R7=n(_Te,"A",{href:!0});var PBr=s(R7);INe=r(PBr,"RoFormerConfig"),PBr.forEach(t),DNe=r(_Te," (RoFormer model)"),_Te.forEach(t),jNe=i(T),Om=n(T,"LI",{});var uTe=s(Om);aH=n(uTe,"STRONG",{});var $Br=s(aH);NNe=r($Br,"segformer"),$Br.forEach(t),qNe=r(uTe," \u2014 "),S7=n(uTe,"A",{href:!0});var IBr=s(S7);GNe=r(IBr,"SegformerConfig"),IBr.forEach(t),ONe=r(uTe," (SegFormer model)"),uTe.forEach(t),XNe=i(T),Xm=n(T,"LI",{});var bTe=s(Xm);nH=n(bTe,"STRONG",{});var DBr=s(nH);VNe=r(DBr,"sew"),DBr.forEach(t),zNe=r(bTe," \u2014 "),P7=n(bTe,"A",{href:!0});var jBr=s(P7);WNe=r(jBr,"SEWConfig"),jBr.forEach(t),QNe=r(bTe," (SEW model)"),bTe.forEach(t),HNe=i(T),Vm=n(T,"LI",{});var vTe=s(Vm);sH=n(vTe,"STRONG",{});var NBr=s(sH);UNe=r(NBr,"sew-d"),NBr.forEach(t),JNe=r(vTe," \u2014 "),$7=n(vTe,"A",{href:!0});var qBr=s($7);YNe=r(qBr,"SEWDConfig"),qBr.forEach(t),KNe=r(vTe," (SEW-D model)"),vTe.forEach(t),ZNe=i(T),zm=n(T,"LI",{});var TTe=s(zm);lH=n(TTe,"STRONG",{});var GBr=s(lH);eqe=r(GBr,"speech-encoder-decoder"),GBr.forEach(t),oqe=r(TTe," \u2014 "),I7=n(TTe,"A",{href:!0});var OBr=s(I7);rqe=r(OBr,"SpeechEncoderDecoderConfig"),OBr.forEach(t),tqe=r(TTe," (Speech Encoder decoder model)"),TTe.forEach(t),aqe=i(T),Wm=n(T,"LI",{});var FTe=s(Wm);iH=n(FTe,"STRONG",{});var XBr=s(iH);nqe=r(XBr,"speech_to_text"),XBr.forEach(t),sqe=r(FTe," \u2014 "),D7=n(FTe,"A",{href:!0});var VBr=s(D7);lqe=r(VBr,"Speech2TextConfig"),VBr.forEach(t),iqe=r(FTe," (Speech2Text model)"),FTe.forEach(t),dqe=i(T),Qm=n(T,"LI",{});var CTe=s(Qm);dH=n(CTe,"STRONG",{});var zBr=s(dH);cqe=r(zBr,"speech_to_text_2"),zBr.forEach(t),fqe=r(CTe," \u2014 "),j7=n(CTe,"A",{href:!0});var WBr=s(j7);mqe=r(WBr,"Speech2Text2Config"),WBr.forEach(t),gqe=r(CTe," (Speech2Text2 model)"),CTe.forEach(t),hqe=i(T),Hm=n(T,"LI",{});var MTe=s(Hm);cH=n(MTe,"STRONG",{});var QBr=s(cH);pqe=r(QBr,"splinter"),QBr.forEach(t),_qe=r(MTe," \u2014 "),N7=n(MTe,"A",{href:!0});var HBr=s(N7);uqe=r(HBr,"SplinterConfig"),HBr.forEach(t),bqe=r(MTe," (Splinter model)"),MTe.forEach(t),vqe=i(T),Um=n(T,"LI",{});var ETe=s(Um);fH=n(ETe,"STRONG",{});var UBr=s(fH);Tqe=r(UBr,"squeezebert"),UBr.forEach(t),Fqe=r(ETe," \u2014 "),q7=n(ETe,"A",{href:!0});var JBr=s(q7);Cqe=r(JBr,"SqueezeBertConfig"),JBr.forEach(t),Mqe=r(ETe," (SqueezeBERT model)"),ETe.forEach(t),Eqe=i(T),Jm=n(T,"LI",{});var yTe=s(Jm);mH=n(yTe,"STRONG",{});var YBr=s(mH);yqe=r(YBr,"swin"),YBr.forEach(t),wqe=r(yTe," \u2014 "),G7=n(yTe,"A",{href:!0});var KBr=s(G7);Aqe=r(KBr,"SwinConfig"),KBr.forEach(t),Lqe=r(yTe," (Swin model)"),yTe.forEach(t),Bqe=i(T),Ym=n(T,"LI",{});var wTe=s(Ym);gH=n(wTe,"STRONG",{});var ZBr=s(gH);xqe=r(ZBr,"t5"),ZBr.forEach(t),kqe=r(wTe," \u2014 "),O7=n(wTe,"A",{href:!0});var exr=s(O7);Rqe=r(exr,"T5Config"),exr.forEach(t),Sqe=r(wTe," (T5 model)"),wTe.forEach(t),Pqe=i(T),Km=n(T,"LI",{});var ATe=s(Km);hH=n(ATe,"STRONG",{});var oxr=s(hH);$qe=r(oxr,"tapas"),oxr.forEach(t),Iqe=r(ATe," \u2014 "),X7=n(ATe,"A",{href:!0});var rxr=s(X7);Dqe=r(rxr,"TapasConfig"),rxr.forEach(t),jqe=r(ATe," (TAPAS model)"),ATe.forEach(t),Nqe=i(T),Zm=n(T,"LI",{});var LTe=s(Zm);pH=n(LTe,"STRONG",{});var txr=s(pH);qqe=r(txr,"transfo-xl"),txr.forEach(t),Gqe=r(LTe," \u2014 "),V7=n(LTe,"A",{href:!0});var axr=s(V7);Oqe=r(axr,"TransfoXLConfig"),axr.forEach(t),Xqe=r(LTe," (Transformer-XL model)"),LTe.forEach(t),Vqe=i(T),eg=n(T,"LI",{});var BTe=s(eg);_H=n(BTe,"STRONG",{});var nxr=s(_H);zqe=r(nxr,"trocr"),nxr.forEach(t),Wqe=r(BTe," \u2014 "),z7=n(BTe,"A",{href:!0});var sxr=s(z7);Qqe=r(sxr,"TrOCRConfig"),sxr.forEach(t),Hqe=r(BTe," (TrOCR model)"),BTe.forEach(t),Uqe=i(T),og=n(T,"LI",{});var xTe=s(og);uH=n(xTe,"STRONG",{});var lxr=s(uH);Jqe=r(lxr,"unispeech"),lxr.forEach(t),Yqe=r(xTe," \u2014 "),W7=n(xTe,"A",{href:!0});var ixr=s(W7);Kqe=r(ixr,"UniSpeechConfig"),ixr.forEach(t),Zqe=r(xTe," (UniSpeech model)"),xTe.forEach(t),eGe=i(T),rg=n(T,"LI",{});var kTe=s(rg);bH=n(kTe,"STRONG",{});var dxr=s(bH);oGe=r(dxr,"unispeech-sat"),dxr.forEach(t),rGe=r(kTe," \u2014 "),Q7=n(kTe,"A",{href:!0});var cxr=s(Q7);tGe=r(cxr,"UniSpeechSatConfig"),cxr.forEach(t),aGe=r(kTe," (UniSpeechSat model)"),kTe.forEach(t),nGe=i(T),tg=n(T,"LI",{});var RTe=s(tg);vH=n(RTe,"STRONG",{});var fxr=s(vH);sGe=r(fxr,"vilt"),fxr.forEach(t),lGe=r(RTe," \u2014 "),H7=n(RTe,"A",{href:!0});var mxr=s(H7);iGe=r(mxr,"ViltConfig"),mxr.forEach(t),dGe=r(RTe," (ViLT model)"),RTe.forEach(t),cGe=i(T),ag=n(T,"LI",{});var STe=s(ag);TH=n(STe,"STRONG",{});var gxr=s(TH);fGe=r(gxr,"vision-encoder-decoder"),gxr.forEach(t),mGe=r(STe," \u2014 "),U7=n(STe,"A",{href:!0});var hxr=s(U7);gGe=r(hxr,"VisionEncoderDecoderConfig"),hxr.forEach(t),hGe=r(STe," (Vision Encoder decoder model)"),STe.forEach(t),pGe=i(T),ng=n(T,"LI",{});var PTe=s(ng);FH=n(PTe,"STRONG",{});var pxr=s(FH);_Ge=r(pxr,"vision-text-dual-encoder"),pxr.forEach(t),uGe=r(PTe," \u2014 "),J7=n(PTe,"A",{href:!0});var _xr=s(J7);bGe=r(_xr,"VisionTextDualEncoderConfig"),_xr.forEach(t),vGe=r(PTe," (VisionTextDualEncoder model)"),PTe.forEach(t),TGe=i(T),sg=n(T,"LI",{});var $Te=s(sg);CH=n($Te,"STRONG",{});var uxr=s(CH);FGe=r(uxr,"visual_bert"),uxr.forEach(t),CGe=r($Te," \u2014 "),Y7=n($Te,"A",{href:!0});var bxr=s(Y7);MGe=r(bxr,"VisualBertConfig"),bxr.forEach(t),EGe=r($Te," (VisualBert model)"),$Te.forEach(t),yGe=i(T),lg=n(T,"LI",{});var ITe=s(lg);MH=n(ITe,"STRONG",{});var vxr=s(MH);wGe=r(vxr,"vit"),vxr.forEach(t),AGe=r(ITe," \u2014 "),K7=n(ITe,"A",{href:!0});var Txr=s(K7);LGe=r(Txr,"ViTConfig"),Txr.forEach(t),BGe=r(ITe," (ViT model)"),ITe.forEach(t),xGe=i(T),ig=n(T,"LI",{});var DTe=s(ig);EH=n(DTe,"STRONG",{});var Fxr=s(EH);kGe=r(Fxr,"vit_mae"),Fxr.forEach(t),RGe=r(DTe," \u2014 "),Z7=n(DTe,"A",{href:!0});var Cxr=s(Z7);SGe=r(Cxr,"ViTMAEConfig"),Cxr.forEach(t),PGe=r(DTe," (ViTMAE model)"),DTe.forEach(t),$Ge=i(T),dg=n(T,"LI",{});var jTe=s(dg);yH=n(jTe,"STRONG",{});var Mxr=s(yH);IGe=r(Mxr,"wav2vec2"),Mxr.forEach(t),DGe=r(jTe," \u2014 "),eB=n(jTe,"A",{href:!0});var Exr=s(eB);jGe=r(Exr,"Wav2Vec2Config"),Exr.forEach(t),NGe=r(jTe," (Wav2Vec2 model)"),jTe.forEach(t),qGe=i(T),cg=n(T,"LI",{});var NTe=s(cg);wH=n(NTe,"STRONG",{});var yxr=s(wH);GGe=r(yxr,"wavlm"),yxr.forEach(t),OGe=r(NTe," \u2014 "),oB=n(NTe,"A",{href:!0});var wxr=s(oB);XGe=r(wxr,"WavLMConfig"),wxr.forEach(t),VGe=r(NTe," (WavLM model)"),NTe.forEach(t),zGe=i(T),fg=n(T,"LI",{});var qTe=s(fg);AH=n(qTe,"STRONG",{});var Axr=s(AH);WGe=r(Axr,"xglm"),Axr.forEach(t),QGe=r(qTe," \u2014 "),rB=n(qTe,"A",{href:!0});var Lxr=s(rB);HGe=r(Lxr,"XGLMConfig"),Lxr.forEach(t),UGe=r(qTe," (XGLM model)"),qTe.forEach(t),JGe=i(T),mg=n(T,"LI",{});var GTe=s(mg);LH=n(GTe,"STRONG",{});var Bxr=s(LH);YGe=r(Bxr,"xlm"),Bxr.forEach(t),KGe=r(GTe," \u2014 "),tB=n(GTe,"A",{href:!0});var xxr=s(tB);ZGe=r(xxr,"XLMConfig"),xxr.forEach(t),eOe=r(GTe," (XLM model)"),GTe.forEach(t),oOe=i(T),gg=n(T,"LI",{});var OTe=s(gg);BH=n(OTe,"STRONG",{});var kxr=s(BH);rOe=r(kxr,"xlm-prophetnet"),kxr.forEach(t),tOe=r(OTe," \u2014 "),aB=n(OTe,"A",{href:!0});var Rxr=s(aB);aOe=r(Rxr,"XLMProphetNetConfig"),Rxr.forEach(t),nOe=r(OTe," (XLMProphetNet model)"),OTe.forEach(t),sOe=i(T),hg=n(T,"LI",{});var XTe=s(hg);xH=n(XTe,"STRONG",{});var Sxr=s(xH);lOe=r(Sxr,"xlm-roberta"),Sxr.forEach(t),iOe=r(XTe," \u2014 "),nB=n(XTe,"A",{href:!0});var Pxr=s(nB);dOe=r(Pxr,"XLMRobertaConfig"),Pxr.forEach(t),cOe=r(XTe," (XLM-RoBERTa model)"),XTe.forEach(t),fOe=i(T),pg=n(T,"LI",{});var VTe=s(pg);kH=n(VTe,"STRONG",{});var $xr=s(kH);mOe=r($xr,"xlm-roberta-xl"),$xr.forEach(t),gOe=r(VTe," \u2014 "),sB=n(VTe,"A",{href:!0});var Ixr=s(sB);hOe=r(Ixr,"XLMRobertaXLConfig"),Ixr.forEach(t),pOe=r(VTe," (XLM-RoBERTa-XL model)"),VTe.forEach(t),_Oe=i(T),_g=n(T,"LI",{});var zTe=s(_g);RH=n(zTe,"STRONG",{});var Dxr=s(RH);uOe=r(Dxr,"xlnet"),Dxr.forEach(t),bOe=r(zTe," \u2014 "),lB=n(zTe,"A",{href:!0});var jxr=s(lB);vOe=r(jxr,"XLNetConfig"),jxr.forEach(t),TOe=r(zTe," (XLNet model)"),zTe.forEach(t),FOe=i(T),ug=n(T,"LI",{});var WTe=s(ug);SH=n(WTe,"STRONG",{});var Nxr=s(SH);COe=r(Nxr,"yoso"),Nxr.forEach(t),MOe=r(WTe," \u2014 "),iB=n(WTe,"A",{href:!0});var qxr=s(iB);EOe=r(qxr,"YosoConfig"),qxr.forEach(t),yOe=r(WTe," (YOSO model)"),WTe.forEach(t),T.forEach(t),wOe=i(ia),PH=n(ia,"P",{});var Gxr=s(PH);AOe=r(Gxr,"Examples:"),Gxr.forEach(t),LOe=i(ia),m(G4.$$.fragment,ia),ia.forEach(t),BOe=i(Ds),bg=n(Ds,"DIV",{class:!0});var fke=s(bg);m(O4.$$.fragment,fke),xOe=i(fke),$H=n(fke,"P",{});var Oxr=s($H);kOe=r(Oxr,"Register a new configuration for this class."),Oxr.forEach(t),fke.forEach(t),Ds.forEach(t),fBe=i(c),Ni=n(c,"H2",{class:!0});var mke=s(Ni);vg=n(mke,"A",{id:!0,class:!0,href:!0});var Xxr=s(vg);IH=n(Xxr,"SPAN",{});var Vxr=s(IH);m(X4.$$.fragment,Vxr),Vxr.forEach(t),Xxr.forEach(t),ROe=i(mke),DH=n(mke,"SPAN",{});var zxr=s(DH);SOe=r(zxr,"AutoTokenizer"),zxr.forEach(t),mke.forEach(t),mBe=i(c),Wo=n(c,"DIV",{class:!0});var js=s(Wo);m(V4.$$.fragment,js),POe=i(js),z4=n(js,"P",{});var gke=s(z4);$Oe=r(gke,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),dB=n(gke,"A",{href:!0});var Wxr=s(dB);IOe=r(Wxr,"AutoTokenizer.from_pretrained()"),Wxr.forEach(t),DOe=r(gke," class method."),gke.forEach(t),jOe=i(js),W4=n(js,"P",{});var hke=s(W4);NOe=r(hke,"This class cannot be instantiated directly using "),jH=n(hke,"CODE",{});var Qxr=s(jH);qOe=r(Qxr,"__init__()"),Qxr.forEach(t),GOe=r(hke," (throws an error)."),hke.forEach(t),OOe=i(js),mo=n(js,"DIV",{class:!0});var da=s(mo);m(Q4.$$.fragment,da),XOe=i(da),NH=n(da,"P",{});var Hxr=s(NH);VOe=r(Hxr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Hxr.forEach(t),zOe=i(da),Da=n(da,"P",{});var RM=s(Da);WOe=r(RM,"The tokenizer class to instantiate is selected based on the "),qH=n(RM,"CODE",{});var Uxr=s(qH);QOe=r(Uxr,"model_type"),Uxr.forEach(t),HOe=r(RM,` property of the config object (either
passed as an argument or loaded from `),GH=n(RM,"CODE",{});var Jxr=s(GH);UOe=r(Jxr,"pretrained_model_name_or_path"),Jxr.forEach(t),JOe=r(RM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),OH=n(RM,"CODE",{});var Yxr=s(OH);YOe=r(Yxr,"pretrained_model_name_or_path"),Yxr.forEach(t),KOe=r(RM,":"),RM.forEach(t),ZOe=i(da),M=n(da,"UL",{});var y=s(M);Gn=n(y,"LI",{});var TL=s(Gn);XH=n(TL,"STRONG",{});var Kxr=s(XH);eXe=r(Kxr,"albert"),Kxr.forEach(t),oXe=r(TL," \u2014 "),cB=n(TL,"A",{href:!0});var Zxr=s(cB);rXe=r(Zxr,"AlbertTokenizer"),Zxr.forEach(t),tXe=r(TL," or "),fB=n(TL,"A",{href:!0});var ekr=s(fB);aXe=r(ekr,"AlbertTokenizerFast"),ekr.forEach(t),nXe=r(TL," (ALBERT model)"),TL.forEach(t),sXe=i(y),On=n(y,"LI",{});var FL=s(On);VH=n(FL,"STRONG",{});var okr=s(VH);lXe=r(okr,"bart"),okr.forEach(t),iXe=r(FL," \u2014 "),mB=n(FL,"A",{href:!0});var rkr=s(mB);dXe=r(rkr,"BartTokenizer"),rkr.forEach(t),cXe=r(FL," or "),gB=n(FL,"A",{href:!0});var tkr=s(gB);fXe=r(tkr,"BartTokenizerFast"),tkr.forEach(t),mXe=r(FL," (BART model)"),FL.forEach(t),gXe=i(y),Xn=n(y,"LI",{});var CL=s(Xn);zH=n(CL,"STRONG",{});var akr=s(zH);hXe=r(akr,"barthez"),akr.forEach(t),pXe=r(CL," \u2014 "),hB=n(CL,"A",{href:!0});var nkr=s(hB);_Xe=r(nkr,"BarthezTokenizer"),nkr.forEach(t),uXe=r(CL," or "),pB=n(CL,"A",{href:!0});var skr=s(pB);bXe=r(skr,"BarthezTokenizerFast"),skr.forEach(t),vXe=r(CL," (BARThez model)"),CL.forEach(t),TXe=i(y),Tg=n(y,"LI",{});var QTe=s(Tg);WH=n(QTe,"STRONG",{});var lkr=s(WH);FXe=r(lkr,"bartpho"),lkr.forEach(t),CXe=r(QTe," \u2014 "),_B=n(QTe,"A",{href:!0});var ikr=s(_B);MXe=r(ikr,"BartphoTokenizer"),ikr.forEach(t),EXe=r(QTe," (BARTpho model)"),QTe.forEach(t),yXe=i(y),Vn=n(y,"LI",{});var ML=s(Vn);QH=n(ML,"STRONG",{});var dkr=s(QH);wXe=r(dkr,"bert"),dkr.forEach(t),AXe=r(ML," \u2014 "),uB=n(ML,"A",{href:!0});var ckr=s(uB);LXe=r(ckr,"BertTokenizer"),ckr.forEach(t),BXe=r(ML," or "),bB=n(ML,"A",{href:!0});var fkr=s(bB);xXe=r(fkr,"BertTokenizerFast"),fkr.forEach(t),kXe=r(ML," (BERT model)"),ML.forEach(t),RXe=i(y),Fg=n(y,"LI",{});var HTe=s(Fg);HH=n(HTe,"STRONG",{});var mkr=s(HH);SXe=r(mkr,"bert-generation"),mkr.forEach(t),PXe=r(HTe," \u2014 "),vB=n(HTe,"A",{href:!0});var gkr=s(vB);$Xe=r(gkr,"BertGenerationTokenizer"),gkr.forEach(t),IXe=r(HTe," (Bert Generation model)"),HTe.forEach(t),DXe=i(y),Cg=n(y,"LI",{});var UTe=s(Cg);UH=n(UTe,"STRONG",{});var hkr=s(UH);jXe=r(hkr,"bert-japanese"),hkr.forEach(t),NXe=r(UTe," \u2014 "),TB=n(UTe,"A",{href:!0});var pkr=s(TB);qXe=r(pkr,"BertJapaneseTokenizer"),pkr.forEach(t),GXe=r(UTe," (BertJapanese model)"),UTe.forEach(t),OXe=i(y),Mg=n(y,"LI",{});var JTe=s(Mg);JH=n(JTe,"STRONG",{});var _kr=s(JH);XXe=r(_kr,"bertweet"),_kr.forEach(t),VXe=r(JTe," \u2014 "),FB=n(JTe,"A",{href:!0});var ukr=s(FB);zXe=r(ukr,"BertweetTokenizer"),ukr.forEach(t),WXe=r(JTe," (Bertweet model)"),JTe.forEach(t),QXe=i(y),zn=n(y,"LI",{});var EL=s(zn);YH=n(EL,"STRONG",{});var bkr=s(YH);HXe=r(bkr,"big_bird"),bkr.forEach(t),UXe=r(EL," \u2014 "),CB=n(EL,"A",{href:!0});var vkr=s(CB);JXe=r(vkr,"BigBirdTokenizer"),vkr.forEach(t),YXe=r(EL," or "),MB=n(EL,"A",{href:!0});var Tkr=s(MB);KXe=r(Tkr,"BigBirdTokenizerFast"),Tkr.forEach(t),ZXe=r(EL," (BigBird model)"),EL.forEach(t),eVe=i(y),Wn=n(y,"LI",{});var yL=s(Wn);KH=n(yL,"STRONG",{});var Fkr=s(KH);oVe=r(Fkr,"bigbird_pegasus"),Fkr.forEach(t),rVe=r(yL," \u2014 "),EB=n(yL,"A",{href:!0});var Ckr=s(EB);tVe=r(Ckr,"PegasusTokenizer"),Ckr.forEach(t),aVe=r(yL," or "),yB=n(yL,"A",{href:!0});var Mkr=s(yB);nVe=r(Mkr,"PegasusTokenizerFast"),Mkr.forEach(t),sVe=r(yL," (BigBirdPegasus model)"),yL.forEach(t),lVe=i(y),Qn=n(y,"LI",{});var wL=s(Qn);ZH=n(wL,"STRONG",{});var Ekr=s(ZH);iVe=r(Ekr,"blenderbot"),Ekr.forEach(t),dVe=r(wL," \u2014 "),wB=n(wL,"A",{href:!0});var ykr=s(wB);cVe=r(ykr,"BlenderbotTokenizer"),ykr.forEach(t),fVe=r(wL," or "),AB=n(wL,"A",{href:!0});var wkr=s(AB);mVe=r(wkr,"BlenderbotTokenizerFast"),wkr.forEach(t),gVe=r(wL," (Blenderbot model)"),wL.forEach(t),hVe=i(y),Eg=n(y,"LI",{});var YTe=s(Eg);eU=n(YTe,"STRONG",{});var Akr=s(eU);pVe=r(Akr,"blenderbot-small"),Akr.forEach(t),_Ve=r(YTe," \u2014 "),LB=n(YTe,"A",{href:!0});var Lkr=s(LB);uVe=r(Lkr,"BlenderbotSmallTokenizer"),Lkr.forEach(t),bVe=r(YTe," (BlenderbotSmall model)"),YTe.forEach(t),vVe=i(y),yg=n(y,"LI",{});var KTe=s(yg);oU=n(KTe,"STRONG",{});var Bkr=s(oU);TVe=r(Bkr,"byt5"),Bkr.forEach(t),FVe=r(KTe," \u2014 "),BB=n(KTe,"A",{href:!0});var xkr=s(BB);CVe=r(xkr,"ByT5Tokenizer"),xkr.forEach(t),MVe=r(KTe," (ByT5 model)"),KTe.forEach(t),EVe=i(y),Hn=n(y,"LI",{});var AL=s(Hn);rU=n(AL,"STRONG",{});var kkr=s(rU);yVe=r(kkr,"camembert"),kkr.forEach(t),wVe=r(AL," \u2014 "),xB=n(AL,"A",{href:!0});var Rkr=s(xB);AVe=r(Rkr,"CamembertTokenizer"),Rkr.forEach(t),LVe=r(AL," or "),kB=n(AL,"A",{href:!0});var Skr=s(kB);BVe=r(Skr,"CamembertTokenizerFast"),Skr.forEach(t),xVe=r(AL," (CamemBERT model)"),AL.forEach(t),kVe=i(y),wg=n(y,"LI",{});var ZTe=s(wg);tU=n(ZTe,"STRONG",{});var Pkr=s(tU);RVe=r(Pkr,"canine"),Pkr.forEach(t),SVe=r(ZTe," \u2014 "),RB=n(ZTe,"A",{href:!0});var $kr=s(RB);PVe=r($kr,"CanineTokenizer"),$kr.forEach(t),$Ve=r(ZTe," (Canine model)"),ZTe.forEach(t),IVe=i(y),Un=n(y,"LI",{});var LL=s(Un);aU=n(LL,"STRONG",{});var Ikr=s(aU);DVe=r(Ikr,"clip"),Ikr.forEach(t),jVe=r(LL," \u2014 "),SB=n(LL,"A",{href:!0});var Dkr=s(SB);NVe=r(Dkr,"CLIPTokenizer"),Dkr.forEach(t),qVe=r(LL," or "),PB=n(LL,"A",{href:!0});var jkr=s(PB);GVe=r(jkr,"CLIPTokenizerFast"),jkr.forEach(t),OVe=r(LL," (CLIP model)"),LL.forEach(t),XVe=i(y),Jn=n(y,"LI",{});var BL=s(Jn);nU=n(BL,"STRONG",{});var Nkr=s(nU);VVe=r(Nkr,"convbert"),Nkr.forEach(t),zVe=r(BL," \u2014 "),$B=n(BL,"A",{href:!0});var qkr=s($B);WVe=r(qkr,"ConvBertTokenizer"),qkr.forEach(t),QVe=r(BL," or "),IB=n(BL,"A",{href:!0});var Gkr=s(IB);HVe=r(Gkr,"ConvBertTokenizerFast"),Gkr.forEach(t),UVe=r(BL," (ConvBERT model)"),BL.forEach(t),JVe=i(y),Yn=n(y,"LI",{});var xL=s(Yn);sU=n(xL,"STRONG",{});var Okr=s(sU);YVe=r(Okr,"cpm"),Okr.forEach(t),KVe=r(xL," \u2014 "),DB=n(xL,"A",{href:!0});var Xkr=s(DB);ZVe=r(Xkr,"CpmTokenizer"),Xkr.forEach(t),eze=r(xL," or "),lU=n(xL,"CODE",{});var Vkr=s(lU);oze=r(Vkr,"CpmTokenizerFast"),Vkr.forEach(t),rze=r(xL," (CPM model)"),xL.forEach(t),tze=i(y),Ag=n(y,"LI",{});var eFe=s(Ag);iU=n(eFe,"STRONG",{});var zkr=s(iU);aze=r(zkr,"ctrl"),zkr.forEach(t),nze=r(eFe," \u2014 "),jB=n(eFe,"A",{href:!0});var Wkr=s(jB);sze=r(Wkr,"CTRLTokenizer"),Wkr.forEach(t),lze=r(eFe," (CTRL model)"),eFe.forEach(t),ize=i(y),Kn=n(y,"LI",{});var kL=s(Kn);dU=n(kL,"STRONG",{});var Qkr=s(dU);dze=r(Qkr,"deberta"),Qkr.forEach(t),cze=r(kL," \u2014 "),NB=n(kL,"A",{href:!0});var Hkr=s(NB);fze=r(Hkr,"DebertaTokenizer"),Hkr.forEach(t),mze=r(kL," or "),qB=n(kL,"A",{href:!0});var Ukr=s(qB);gze=r(Ukr,"DebertaTokenizerFast"),Ukr.forEach(t),hze=r(kL," (DeBERTa model)"),kL.forEach(t),pze=i(y),Lg=n(y,"LI",{});var oFe=s(Lg);cU=n(oFe,"STRONG",{});var Jkr=s(cU);_ze=r(Jkr,"deberta-v2"),Jkr.forEach(t),uze=r(oFe," \u2014 "),GB=n(oFe,"A",{href:!0});var Ykr=s(GB);bze=r(Ykr,"DebertaV2Tokenizer"),Ykr.forEach(t),vze=r(oFe," (DeBERTa-v2 model)"),oFe.forEach(t),Tze=i(y),Zn=n(y,"LI",{});var RL=s(Zn);fU=n(RL,"STRONG",{});var Kkr=s(fU);Fze=r(Kkr,"distilbert"),Kkr.forEach(t),Cze=r(RL," \u2014 "),OB=n(RL,"A",{href:!0});var Zkr=s(OB);Mze=r(Zkr,"DistilBertTokenizer"),Zkr.forEach(t),Eze=r(RL," or "),XB=n(RL,"A",{href:!0});var eRr=s(XB);yze=r(eRr,"DistilBertTokenizerFast"),eRr.forEach(t),wze=r(RL," (DistilBERT model)"),RL.forEach(t),Aze=i(y),es=n(y,"LI",{});var SL=s(es);mU=n(SL,"STRONG",{});var oRr=s(mU);Lze=r(oRr,"dpr"),oRr.forEach(t),Bze=r(SL," \u2014 "),VB=n(SL,"A",{href:!0});var rRr=s(VB);xze=r(rRr,"DPRQuestionEncoderTokenizer"),rRr.forEach(t),kze=r(SL," or "),zB=n(SL,"A",{href:!0});var tRr=s(zB);Rze=r(tRr,"DPRQuestionEncoderTokenizerFast"),tRr.forEach(t),Sze=r(SL," (DPR model)"),SL.forEach(t),Pze=i(y),os=n(y,"LI",{});var PL=s(os);gU=n(PL,"STRONG",{});var aRr=s(gU);$ze=r(aRr,"electra"),aRr.forEach(t),Ize=r(PL," \u2014 "),WB=n(PL,"A",{href:!0});var nRr=s(WB);Dze=r(nRr,"ElectraTokenizer"),nRr.forEach(t),jze=r(PL," or "),QB=n(PL,"A",{href:!0});var sRr=s(QB);Nze=r(sRr,"ElectraTokenizerFast"),sRr.forEach(t),qze=r(PL," (ELECTRA model)"),PL.forEach(t),Gze=i(y),Bg=n(y,"LI",{});var rFe=s(Bg);hU=n(rFe,"STRONG",{});var lRr=s(hU);Oze=r(lRr,"flaubert"),lRr.forEach(t),Xze=r(rFe," \u2014 "),HB=n(rFe,"A",{href:!0});var iRr=s(HB);Vze=r(iRr,"FlaubertTokenizer"),iRr.forEach(t),zze=r(rFe," (FlauBERT model)"),rFe.forEach(t),Wze=i(y),rs=n(y,"LI",{});var $L=s(rs);pU=n($L,"STRONG",{});var dRr=s(pU);Qze=r(dRr,"fnet"),dRr.forEach(t),Hze=r($L," \u2014 "),UB=n($L,"A",{href:!0});var cRr=s(UB);Uze=r(cRr,"FNetTokenizer"),cRr.forEach(t),Jze=r($L," or "),JB=n($L,"A",{href:!0});var fRr=s(JB);Yze=r(fRr,"FNetTokenizerFast"),fRr.forEach(t),Kze=r($L," (FNet model)"),$L.forEach(t),Zze=i(y),xg=n(y,"LI",{});var tFe=s(xg);_U=n(tFe,"STRONG",{});var mRr=s(_U);eWe=r(mRr,"fsmt"),mRr.forEach(t),oWe=r(tFe," \u2014 "),YB=n(tFe,"A",{href:!0});var gRr=s(YB);rWe=r(gRr,"FSMTTokenizer"),gRr.forEach(t),tWe=r(tFe," (FairSeq Machine-Translation model)"),tFe.forEach(t),aWe=i(y),ts=n(y,"LI",{});var IL=s(ts);uU=n(IL,"STRONG",{});var hRr=s(uU);nWe=r(hRr,"funnel"),hRr.forEach(t),sWe=r(IL," \u2014 "),KB=n(IL,"A",{href:!0});var pRr=s(KB);lWe=r(pRr,"FunnelTokenizer"),pRr.forEach(t),iWe=r(IL," or "),ZB=n(IL,"A",{href:!0});var _Rr=s(ZB);dWe=r(_Rr,"FunnelTokenizerFast"),_Rr.forEach(t),cWe=r(IL," (Funnel Transformer model)"),IL.forEach(t),fWe=i(y),as=n(y,"LI",{});var DL=s(as);bU=n(DL,"STRONG",{});var uRr=s(bU);mWe=r(uRr,"gpt2"),uRr.forEach(t),gWe=r(DL," \u2014 "),ex=n(DL,"A",{href:!0});var bRr=s(ex);hWe=r(bRr,"GPT2Tokenizer"),bRr.forEach(t),pWe=r(DL," or "),ox=n(DL,"A",{href:!0});var vRr=s(ox);_We=r(vRr,"GPT2TokenizerFast"),vRr.forEach(t),uWe=r(DL," (OpenAI GPT-2 model)"),DL.forEach(t),bWe=i(y),ns=n(y,"LI",{});var jL=s(ns);vU=n(jL,"STRONG",{});var TRr=s(vU);vWe=r(TRr,"gpt_neo"),TRr.forEach(t),TWe=r(jL," \u2014 "),rx=n(jL,"A",{href:!0});var FRr=s(rx);FWe=r(FRr,"GPT2Tokenizer"),FRr.forEach(t),CWe=r(jL," or "),tx=n(jL,"A",{href:!0});var CRr=s(tx);MWe=r(CRr,"GPT2TokenizerFast"),CRr.forEach(t),EWe=r(jL," (GPT Neo model)"),jL.forEach(t),yWe=i(y),ss=n(y,"LI",{});var NL=s(ss);TU=n(NL,"STRONG",{});var MRr=s(TU);wWe=r(MRr,"herbert"),MRr.forEach(t),AWe=r(NL," \u2014 "),ax=n(NL,"A",{href:!0});var ERr=s(ax);LWe=r(ERr,"HerbertTokenizer"),ERr.forEach(t),BWe=r(NL," or "),nx=n(NL,"A",{href:!0});var yRr=s(nx);xWe=r(yRr,"HerbertTokenizerFast"),yRr.forEach(t),kWe=r(NL," (HerBERT model)"),NL.forEach(t),RWe=i(y),kg=n(y,"LI",{});var aFe=s(kg);FU=n(aFe,"STRONG",{});var wRr=s(FU);SWe=r(wRr,"hubert"),wRr.forEach(t),PWe=r(aFe," \u2014 "),sx=n(aFe,"A",{href:!0});var ARr=s(sx);$We=r(ARr,"Wav2Vec2CTCTokenizer"),ARr.forEach(t),IWe=r(aFe," (Hubert model)"),aFe.forEach(t),DWe=i(y),ls=n(y,"LI",{});var qL=s(ls);CU=n(qL,"STRONG",{});var LRr=s(CU);jWe=r(LRr,"ibert"),LRr.forEach(t),NWe=r(qL," \u2014 "),lx=n(qL,"A",{href:!0});var BRr=s(lx);qWe=r(BRr,"RobertaTokenizer"),BRr.forEach(t),GWe=r(qL," or "),ix=n(qL,"A",{href:!0});var xRr=s(ix);OWe=r(xRr,"RobertaTokenizerFast"),xRr.forEach(t),XWe=r(qL," (I-BERT model)"),qL.forEach(t),VWe=i(y),is=n(y,"LI",{});var GL=s(is);MU=n(GL,"STRONG",{});var kRr=s(MU);zWe=r(kRr,"layoutlm"),kRr.forEach(t),WWe=r(GL," \u2014 "),dx=n(GL,"A",{href:!0});var RRr=s(dx);QWe=r(RRr,"LayoutLMTokenizer"),RRr.forEach(t),HWe=r(GL," or "),cx=n(GL,"A",{href:!0});var SRr=s(cx);UWe=r(SRr,"LayoutLMTokenizerFast"),SRr.forEach(t),JWe=r(GL," (LayoutLM model)"),GL.forEach(t),YWe=i(y),ds=n(y,"LI",{});var OL=s(ds);EU=n(OL,"STRONG",{});var PRr=s(EU);KWe=r(PRr,"layoutlmv2"),PRr.forEach(t),ZWe=r(OL," \u2014 "),fx=n(OL,"A",{href:!0});var $Rr=s(fx);eQe=r($Rr,"LayoutLMv2Tokenizer"),$Rr.forEach(t),oQe=r(OL," or "),mx=n(OL,"A",{href:!0});var IRr=s(mx);rQe=r(IRr,"LayoutLMv2TokenizerFast"),IRr.forEach(t),tQe=r(OL," (LayoutLMv2 model)"),OL.forEach(t),aQe=i(y),cs=n(y,"LI",{});var XL=s(cs);yU=n(XL,"STRONG",{});var DRr=s(yU);nQe=r(DRr,"layoutxlm"),DRr.forEach(t),sQe=r(XL," \u2014 "),gx=n(XL,"A",{href:!0});var jRr=s(gx);lQe=r(jRr,"LayoutXLMTokenizer"),jRr.forEach(t),iQe=r(XL," or "),hx=n(XL,"A",{href:!0});var NRr=s(hx);dQe=r(NRr,"LayoutXLMTokenizerFast"),NRr.forEach(t),cQe=r(XL," (LayoutXLM model)"),XL.forEach(t),fQe=i(y),fs=n(y,"LI",{});var VL=s(fs);wU=n(VL,"STRONG",{});var qRr=s(wU);mQe=r(qRr,"led"),qRr.forEach(t),gQe=r(VL," \u2014 "),px=n(VL,"A",{href:!0});var GRr=s(px);hQe=r(GRr,"LEDTokenizer"),GRr.forEach(t),pQe=r(VL," or "),_x=n(VL,"A",{href:!0});var ORr=s(_x);_Qe=r(ORr,"LEDTokenizerFast"),ORr.forEach(t),uQe=r(VL," (LED model)"),VL.forEach(t),bQe=i(y),ms=n(y,"LI",{});var zL=s(ms);AU=n(zL,"STRONG",{});var XRr=s(AU);vQe=r(XRr,"longformer"),XRr.forEach(t),TQe=r(zL," \u2014 "),ux=n(zL,"A",{href:!0});var VRr=s(ux);FQe=r(VRr,"LongformerTokenizer"),VRr.forEach(t),CQe=r(zL," or "),bx=n(zL,"A",{href:!0});var zRr=s(bx);MQe=r(zRr,"LongformerTokenizerFast"),zRr.forEach(t),EQe=r(zL," (Longformer model)"),zL.forEach(t),yQe=i(y),Rg=n(y,"LI",{});var nFe=s(Rg);LU=n(nFe,"STRONG",{});var WRr=s(LU);wQe=r(WRr,"luke"),WRr.forEach(t),AQe=r(nFe," \u2014 "),vx=n(nFe,"A",{href:!0});var QRr=s(vx);LQe=r(QRr,"LukeTokenizer"),QRr.forEach(t),BQe=r(nFe," (LUKE model)"),nFe.forEach(t),xQe=i(y),gs=n(y,"LI",{});var WL=s(gs);BU=n(WL,"STRONG",{});var HRr=s(BU);kQe=r(HRr,"lxmert"),HRr.forEach(t),RQe=r(WL," \u2014 "),Tx=n(WL,"A",{href:!0});var URr=s(Tx);SQe=r(URr,"LxmertTokenizer"),URr.forEach(t),PQe=r(WL," or "),Fx=n(WL,"A",{href:!0});var JRr=s(Fx);$Qe=r(JRr,"LxmertTokenizerFast"),JRr.forEach(t),IQe=r(WL," (LXMERT model)"),WL.forEach(t),DQe=i(y),Sg=n(y,"LI",{});var sFe=s(Sg);xU=n(sFe,"STRONG",{});var YRr=s(xU);jQe=r(YRr,"m2m_100"),YRr.forEach(t),NQe=r(sFe," \u2014 "),Cx=n(sFe,"A",{href:!0});var KRr=s(Cx);qQe=r(KRr,"M2M100Tokenizer"),KRr.forEach(t),GQe=r(sFe," (M2M100 model)"),sFe.forEach(t),OQe=i(y),Pg=n(y,"LI",{});var lFe=s(Pg);kU=n(lFe,"STRONG",{});var ZRr=s(kU);XQe=r(ZRr,"marian"),ZRr.forEach(t),VQe=r(lFe," \u2014 "),Mx=n(lFe,"A",{href:!0});var eSr=s(Mx);zQe=r(eSr,"MarianTokenizer"),eSr.forEach(t),WQe=r(lFe," (Marian model)"),lFe.forEach(t),QQe=i(y),hs=n(y,"LI",{});var QL=s(hs);RU=n(QL,"STRONG",{});var oSr=s(RU);HQe=r(oSr,"mbart"),oSr.forEach(t),UQe=r(QL," \u2014 "),Ex=n(QL,"A",{href:!0});var rSr=s(Ex);JQe=r(rSr,"MBartTokenizer"),rSr.forEach(t),YQe=r(QL," or "),yx=n(QL,"A",{href:!0});var tSr=s(yx);KQe=r(tSr,"MBartTokenizerFast"),tSr.forEach(t),ZQe=r(QL," (mBART model)"),QL.forEach(t),eHe=i(y),ps=n(y,"LI",{});var HL=s(ps);SU=n(HL,"STRONG",{});var aSr=s(SU);oHe=r(aSr,"mbart50"),aSr.forEach(t),rHe=r(HL," \u2014 "),wx=n(HL,"A",{href:!0});var nSr=s(wx);tHe=r(nSr,"MBart50Tokenizer"),nSr.forEach(t),aHe=r(HL," or "),Ax=n(HL,"A",{href:!0});var sSr=s(Ax);nHe=r(sSr,"MBart50TokenizerFast"),sSr.forEach(t),sHe=r(HL," (mBART-50 model)"),HL.forEach(t),lHe=i(y),$g=n(y,"LI",{});var iFe=s($g);PU=n(iFe,"STRONG",{});var lSr=s(PU);iHe=r(lSr,"mluke"),lSr.forEach(t),dHe=r(iFe," \u2014 "),Lx=n(iFe,"A",{href:!0});var iSr=s(Lx);cHe=r(iSr,"MLukeTokenizer"),iSr.forEach(t),fHe=r(iFe," (mLUKE model)"),iFe.forEach(t),mHe=i(y),_s=n(y,"LI",{});var UL=s(_s);$U=n(UL,"STRONG",{});var dSr=s($U);gHe=r(dSr,"mobilebert"),dSr.forEach(t),hHe=r(UL," \u2014 "),Bx=n(UL,"A",{href:!0});var cSr=s(Bx);pHe=r(cSr,"MobileBertTokenizer"),cSr.forEach(t),_He=r(UL," or "),xx=n(UL,"A",{href:!0});var fSr=s(xx);uHe=r(fSr,"MobileBertTokenizerFast"),fSr.forEach(t),bHe=r(UL," (MobileBERT model)"),UL.forEach(t),vHe=i(y),us=n(y,"LI",{});var JL=s(us);IU=n(JL,"STRONG",{});var mSr=s(IU);THe=r(mSr,"mpnet"),mSr.forEach(t),FHe=r(JL," \u2014 "),kx=n(JL,"A",{href:!0});var gSr=s(kx);CHe=r(gSr,"MPNetTokenizer"),gSr.forEach(t),MHe=r(JL," or "),Rx=n(JL,"A",{href:!0});var hSr=s(Rx);EHe=r(hSr,"MPNetTokenizerFast"),hSr.forEach(t),yHe=r(JL," (MPNet model)"),JL.forEach(t),wHe=i(y),bs=n(y,"LI",{});var YL=s(bs);DU=n(YL,"STRONG",{});var pSr=s(DU);AHe=r(pSr,"mt5"),pSr.forEach(t),LHe=r(YL," \u2014 "),Sx=n(YL,"A",{href:!0});var _Sr=s(Sx);BHe=r(_Sr,"MT5Tokenizer"),_Sr.forEach(t),xHe=r(YL," or "),Px=n(YL,"A",{href:!0});var uSr=s(Px);kHe=r(uSr,"MT5TokenizerFast"),uSr.forEach(t),RHe=r(YL," (mT5 model)"),YL.forEach(t),SHe=i(y),vs=n(y,"LI",{});var KL=s(vs);jU=n(KL,"STRONG",{});var bSr=s(jU);PHe=r(bSr,"openai-gpt"),bSr.forEach(t),$He=r(KL," \u2014 "),$x=n(KL,"A",{href:!0});var vSr=s($x);IHe=r(vSr,"OpenAIGPTTokenizer"),vSr.forEach(t),DHe=r(KL," or "),Ix=n(KL,"A",{href:!0});var TSr=s(Ix);jHe=r(TSr,"OpenAIGPTTokenizerFast"),TSr.forEach(t),NHe=r(KL," (OpenAI GPT model)"),KL.forEach(t),qHe=i(y),Ts=n(y,"LI",{});var ZL=s(Ts);NU=n(ZL,"STRONG",{});var FSr=s(NU);GHe=r(FSr,"pegasus"),FSr.forEach(t),OHe=r(ZL," \u2014 "),Dx=n(ZL,"A",{href:!0});var CSr=s(Dx);XHe=r(CSr,"PegasusTokenizer"),CSr.forEach(t),VHe=r(ZL," or "),jx=n(ZL,"A",{href:!0});var MSr=s(jx);zHe=r(MSr,"PegasusTokenizerFast"),MSr.forEach(t),WHe=r(ZL," (Pegasus model)"),ZL.forEach(t),QHe=i(y),Ig=n(y,"LI",{});var dFe=s(Ig);qU=n(dFe,"STRONG",{});var ESr=s(qU);HHe=r(ESr,"perceiver"),ESr.forEach(t),UHe=r(dFe," \u2014 "),Nx=n(dFe,"A",{href:!0});var ySr=s(Nx);JHe=r(ySr,"PerceiverTokenizer"),ySr.forEach(t),YHe=r(dFe," (Perceiver model)"),dFe.forEach(t),KHe=i(y),Dg=n(y,"LI",{});var cFe=s(Dg);GU=n(cFe,"STRONG",{});var wSr=s(GU);ZHe=r(wSr,"phobert"),wSr.forEach(t),eUe=r(cFe," \u2014 "),qx=n(cFe,"A",{href:!0});var ASr=s(qx);oUe=r(ASr,"PhobertTokenizer"),ASr.forEach(t),rUe=r(cFe," (PhoBERT model)"),cFe.forEach(t),tUe=i(y),jg=n(y,"LI",{});var fFe=s(jg);OU=n(fFe,"STRONG",{});var LSr=s(OU);aUe=r(LSr,"plbart"),LSr.forEach(t),nUe=r(fFe," \u2014 "),Gx=n(fFe,"A",{href:!0});var BSr=s(Gx);sUe=r(BSr,"PLBartTokenizer"),BSr.forEach(t),lUe=r(fFe," (PLBart model)"),fFe.forEach(t),iUe=i(y),Ng=n(y,"LI",{});var mFe=s(Ng);XU=n(mFe,"STRONG",{});var xSr=s(XU);dUe=r(xSr,"prophetnet"),xSr.forEach(t),cUe=r(mFe," \u2014 "),Ox=n(mFe,"A",{href:!0});var kSr=s(Ox);fUe=r(kSr,"ProphetNetTokenizer"),kSr.forEach(t),mUe=r(mFe," (ProphetNet model)"),mFe.forEach(t),gUe=i(y),Fs=n(y,"LI",{});var e8=s(Fs);VU=n(e8,"STRONG",{});var RSr=s(VU);hUe=r(RSr,"qdqbert"),RSr.forEach(t),pUe=r(e8," \u2014 "),Xx=n(e8,"A",{href:!0});var SSr=s(Xx);_Ue=r(SSr,"BertTokenizer"),SSr.forEach(t),uUe=r(e8," or "),Vx=n(e8,"A",{href:!0});var PSr=s(Vx);bUe=r(PSr,"BertTokenizerFast"),PSr.forEach(t),vUe=r(e8," (QDQBert model)"),e8.forEach(t),TUe=i(y),qg=n(y,"LI",{});var gFe=s(qg);zU=n(gFe,"STRONG",{});var $Sr=s(zU);FUe=r($Sr,"rag"),$Sr.forEach(t),CUe=r(gFe," \u2014 "),zx=n(gFe,"A",{href:!0});var ISr=s(zx);MUe=r(ISr,"RagTokenizer"),ISr.forEach(t),EUe=r(gFe," (RAG model)"),gFe.forEach(t),yUe=i(y),Cs=n(y,"LI",{});var o8=s(Cs);WU=n(o8,"STRONG",{});var DSr=s(WU);wUe=r(DSr,"realm"),DSr.forEach(t),AUe=r(o8," \u2014 "),Wx=n(o8,"A",{href:!0});var jSr=s(Wx);LUe=r(jSr,"RealmTokenizer"),jSr.forEach(t),BUe=r(o8," or "),Qx=n(o8,"A",{href:!0});var NSr=s(Qx);xUe=r(NSr,"RealmTokenizerFast"),NSr.forEach(t),kUe=r(o8," (Realm model)"),o8.forEach(t),RUe=i(y),Ms=n(y,"LI",{});var r8=s(Ms);QU=n(r8,"STRONG",{});var qSr=s(QU);SUe=r(qSr,"reformer"),qSr.forEach(t),PUe=r(r8," \u2014 "),Hx=n(r8,"A",{href:!0});var GSr=s(Hx);$Ue=r(GSr,"ReformerTokenizer"),GSr.forEach(t),IUe=r(r8," or "),Ux=n(r8,"A",{href:!0});var OSr=s(Ux);DUe=r(OSr,"ReformerTokenizerFast"),OSr.forEach(t),jUe=r(r8," (Reformer model)"),r8.forEach(t),NUe=i(y),Es=n(y,"LI",{});var t8=s(Es);HU=n(t8,"STRONG",{});var XSr=s(HU);qUe=r(XSr,"rembert"),XSr.forEach(t),GUe=r(t8," \u2014 "),Jx=n(t8,"A",{href:!0});var VSr=s(Jx);OUe=r(VSr,"RemBertTokenizer"),VSr.forEach(t),XUe=r(t8," or "),Yx=n(t8,"A",{href:!0});var zSr=s(Yx);VUe=r(zSr,"RemBertTokenizerFast"),zSr.forEach(t),zUe=r(t8," (RemBERT model)"),t8.forEach(t),WUe=i(y),ys=n(y,"LI",{});var a8=s(ys);UU=n(a8,"STRONG",{});var WSr=s(UU);QUe=r(WSr,"retribert"),WSr.forEach(t),HUe=r(a8," \u2014 "),Kx=n(a8,"A",{href:!0});var QSr=s(Kx);UUe=r(QSr,"RetriBertTokenizer"),QSr.forEach(t),JUe=r(a8," or "),Zx=n(a8,"A",{href:!0});var HSr=s(Zx);YUe=r(HSr,"RetriBertTokenizerFast"),HSr.forEach(t),KUe=r(a8," (RetriBERT model)"),a8.forEach(t),ZUe=i(y),ws=n(y,"LI",{});var n8=s(ws);JU=n(n8,"STRONG",{});var USr=s(JU);eJe=r(USr,"roberta"),USr.forEach(t),oJe=r(n8," \u2014 "),ek=n(n8,"A",{href:!0});var JSr=s(ek);rJe=r(JSr,"RobertaTokenizer"),JSr.forEach(t),tJe=r(n8," or "),ok=n(n8,"A",{href:!0});var YSr=s(ok);aJe=r(YSr,"RobertaTokenizerFast"),YSr.forEach(t),nJe=r(n8," (RoBERTa model)"),n8.forEach(t),sJe=i(y),As=n(y,"LI",{});var s8=s(As);YU=n(s8,"STRONG",{});var KSr=s(YU);lJe=r(KSr,"roformer"),KSr.forEach(t),iJe=r(s8," \u2014 "),rk=n(s8,"A",{href:!0});var ZSr=s(rk);dJe=r(ZSr,"RoFormerTokenizer"),ZSr.forEach(t),cJe=r(s8," or "),tk=n(s8,"A",{href:!0});var ePr=s(tk);fJe=r(ePr,"RoFormerTokenizerFast"),ePr.forEach(t),mJe=r(s8," (RoFormer model)"),s8.forEach(t),gJe=i(y),Gg=n(y,"LI",{});var hFe=s(Gg);KU=n(hFe,"STRONG",{});var oPr=s(KU);hJe=r(oPr,"speech_to_text"),oPr.forEach(t),pJe=r(hFe," \u2014 "),ak=n(hFe,"A",{href:!0});var rPr=s(ak);_Je=r(rPr,"Speech2TextTokenizer"),rPr.forEach(t),uJe=r(hFe," (Speech2Text model)"),hFe.forEach(t),bJe=i(y),Og=n(y,"LI",{});var pFe=s(Og);ZU=n(pFe,"STRONG",{});var tPr=s(ZU);vJe=r(tPr,"speech_to_text_2"),tPr.forEach(t),TJe=r(pFe," \u2014 "),nk=n(pFe,"A",{href:!0});var aPr=s(nk);FJe=r(aPr,"Speech2Text2Tokenizer"),aPr.forEach(t),CJe=r(pFe," (Speech2Text2 model)"),pFe.forEach(t),MJe=i(y),Ls=n(y,"LI",{});var l8=s(Ls);eJ=n(l8,"STRONG",{});var nPr=s(eJ);EJe=r(nPr,"splinter"),nPr.forEach(t),yJe=r(l8," \u2014 "),sk=n(l8,"A",{href:!0});var sPr=s(sk);wJe=r(sPr,"SplinterTokenizer"),sPr.forEach(t),AJe=r(l8," or "),lk=n(l8,"A",{href:!0});var lPr=s(lk);LJe=r(lPr,"SplinterTokenizerFast"),lPr.forEach(t),BJe=r(l8," (Splinter model)"),l8.forEach(t),xJe=i(y),Bs=n(y,"LI",{});var i8=s(Bs);oJ=n(i8,"STRONG",{});var iPr=s(oJ);kJe=r(iPr,"squeezebert"),iPr.forEach(t),RJe=r(i8," \u2014 "),ik=n(i8,"A",{href:!0});var dPr=s(ik);SJe=r(dPr,"SqueezeBertTokenizer"),dPr.forEach(t),PJe=r(i8," or "),dk=n(i8,"A",{href:!0});var cPr=s(dk);$Je=r(cPr,"SqueezeBertTokenizerFast"),cPr.forEach(t),IJe=r(i8," (SqueezeBERT model)"),i8.forEach(t),DJe=i(y),xs=n(y,"LI",{});var d8=s(xs);rJ=n(d8,"STRONG",{});var fPr=s(rJ);jJe=r(fPr,"t5"),fPr.forEach(t),NJe=r(d8," \u2014 "),ck=n(d8,"A",{href:!0});var mPr=s(ck);qJe=r(mPr,"T5Tokenizer"),mPr.forEach(t),GJe=r(d8," or "),fk=n(d8,"A",{href:!0});var gPr=s(fk);OJe=r(gPr,"T5TokenizerFast"),gPr.forEach(t),XJe=r(d8," (T5 model)"),d8.forEach(t),VJe=i(y),Xg=n(y,"LI",{});var _Fe=s(Xg);tJ=n(_Fe,"STRONG",{});var hPr=s(tJ);zJe=r(hPr,"tapas"),hPr.forEach(t),WJe=r(_Fe," \u2014 "),mk=n(_Fe,"A",{href:!0});var pPr=s(mk);QJe=r(pPr,"TapasTokenizer"),pPr.forEach(t),HJe=r(_Fe," (TAPAS model)"),_Fe.forEach(t),UJe=i(y),Vg=n(y,"LI",{});var uFe=s(Vg);aJ=n(uFe,"STRONG",{});var _Pr=s(aJ);JJe=r(_Pr,"transfo-xl"),_Pr.forEach(t),YJe=r(uFe," \u2014 "),gk=n(uFe,"A",{href:!0});var uPr=s(gk);KJe=r(uPr,"TransfoXLTokenizer"),uPr.forEach(t),ZJe=r(uFe," (Transformer-XL model)"),uFe.forEach(t),eYe=i(y),zg=n(y,"LI",{});var bFe=s(zg);nJ=n(bFe,"STRONG",{});var bPr=s(nJ);oYe=r(bPr,"wav2vec2"),bPr.forEach(t),rYe=r(bFe," \u2014 "),hk=n(bFe,"A",{href:!0});var vPr=s(hk);tYe=r(vPr,"Wav2Vec2CTCTokenizer"),vPr.forEach(t),aYe=r(bFe," (Wav2Vec2 model)"),bFe.forEach(t),nYe=i(y),Wg=n(y,"LI",{});var vFe=s(Wg);sJ=n(vFe,"STRONG",{});var TPr=s(sJ);sYe=r(TPr,"wav2vec2_phoneme"),TPr.forEach(t),lYe=r(vFe," \u2014 "),pk=n(vFe,"A",{href:!0});var FPr=s(pk);iYe=r(FPr,"Wav2Vec2PhonemeCTCTokenizer"),FPr.forEach(t),dYe=r(vFe," (Wav2Vec2Phoneme model)"),vFe.forEach(t),cYe=i(y),ks=n(y,"LI",{});var c8=s(ks);lJ=n(c8,"STRONG",{});var CPr=s(lJ);fYe=r(CPr,"xglm"),CPr.forEach(t),mYe=r(c8," \u2014 "),_k=n(c8,"A",{href:!0});var MPr=s(_k);gYe=r(MPr,"XGLMTokenizer"),MPr.forEach(t),hYe=r(c8," or "),uk=n(c8,"A",{href:!0});var EPr=s(uk);pYe=r(EPr,"XGLMTokenizerFast"),EPr.forEach(t),_Ye=r(c8," (XGLM model)"),c8.forEach(t),uYe=i(y),Qg=n(y,"LI",{});var TFe=s(Qg);iJ=n(TFe,"STRONG",{});var yPr=s(iJ);bYe=r(yPr,"xlm"),yPr.forEach(t),vYe=r(TFe," \u2014 "),bk=n(TFe,"A",{href:!0});var wPr=s(bk);TYe=r(wPr,"XLMTokenizer"),wPr.forEach(t),FYe=r(TFe," (XLM model)"),TFe.forEach(t),CYe=i(y),Hg=n(y,"LI",{});var FFe=s(Hg);dJ=n(FFe,"STRONG",{});var APr=s(dJ);MYe=r(APr,"xlm-prophetnet"),APr.forEach(t),EYe=r(FFe," \u2014 "),vk=n(FFe,"A",{href:!0});var LPr=s(vk);yYe=r(LPr,"XLMProphetNetTokenizer"),LPr.forEach(t),wYe=r(FFe," (XLMProphetNet model)"),FFe.forEach(t),AYe=i(y),Rs=n(y,"LI",{});var f8=s(Rs);cJ=n(f8,"STRONG",{});var BPr=s(cJ);LYe=r(BPr,"xlm-roberta"),BPr.forEach(t),BYe=r(f8," \u2014 "),Tk=n(f8,"A",{href:!0});var xPr=s(Tk);xYe=r(xPr,"XLMRobertaTokenizer"),xPr.forEach(t),kYe=r(f8," or "),Fk=n(f8,"A",{href:!0});var kPr=s(Fk);RYe=r(kPr,"XLMRobertaTokenizerFast"),kPr.forEach(t),SYe=r(f8," (XLM-RoBERTa model)"),f8.forEach(t),PYe=i(y),Ss=n(y,"LI",{});var m8=s(Ss);fJ=n(m8,"STRONG",{});var RPr=s(fJ);$Ye=r(RPr,"xlnet"),RPr.forEach(t),IYe=r(m8," \u2014 "),Ck=n(m8,"A",{href:!0});var SPr=s(Ck);DYe=r(SPr,"XLNetTokenizer"),SPr.forEach(t),jYe=r(m8," or "),Mk=n(m8,"A",{href:!0});var PPr=s(Mk);NYe=r(PPr,"XLNetTokenizerFast"),PPr.forEach(t),qYe=r(m8," (XLNet model)"),m8.forEach(t),y.forEach(t),GYe=i(da),mJ=n(da,"P",{});var $Pr=s(mJ);OYe=r($Pr,"Examples:"),$Pr.forEach(t),XYe=i(da),m(H4.$$.fragment,da),da.forEach(t),VYe=i(js),Ug=n(js,"DIV",{class:!0});var pke=s(Ug);m(U4.$$.fragment,pke),zYe=i(pke),gJ=n(pke,"P",{});var IPr=s(gJ);WYe=r(IPr,"Register a new tokenizer in this mapping."),IPr.forEach(t),pke.forEach(t),js.forEach(t),gBe=i(c),qi=n(c,"H2",{class:!0});var _ke=s(qi);Jg=n(_ke,"A",{id:!0,class:!0,href:!0});var DPr=s(Jg);hJ=n(DPr,"SPAN",{});var jPr=s(hJ);m(J4.$$.fragment,jPr),jPr.forEach(t),DPr.forEach(t),QYe=i(_ke),pJ=n(_ke,"SPAN",{});var NPr=s(pJ);HYe=r(NPr,"AutoFeatureExtractor"),NPr.forEach(t),_ke.forEach(t),hBe=i(c),Qo=n(c,"DIV",{class:!0});var Ns=s(Qo);m(Y4.$$.fragment,Ns),UYe=i(Ns),K4=n(Ns,"P",{});var uke=s(K4);JYe=r(uke,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ek=n(uke,"A",{href:!0});var qPr=s(Ek);YYe=r(qPr,"AutoFeatureExtractor.from_pretrained()"),qPr.forEach(t),KYe=r(uke," class method."),uke.forEach(t),ZYe=i(Ns),Z4=n(Ns,"P",{});var bke=s(Z4);eKe=r(bke,"This class cannot be instantiated directly using "),_J=n(bke,"CODE",{});var GPr=s(_J);oKe=r(GPr,"__init__()"),GPr.forEach(t),rKe=r(bke," (throws an error)."),bke.forEach(t),tKe=i(Ns),$e=n(Ns,"DIV",{class:!0});var $t=s($e);m(eE.$$.fragment,$t),aKe=i($t),uJ=n($t,"P",{});var OPr=s(uJ);nKe=r(OPr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),OPr.forEach(t),sKe=i($t),ja=n($t,"P",{});var SM=s(ja);lKe=r(SM,"The feature extractor class to instantiate is selected based on the "),bJ=n(SM,"CODE",{});var XPr=s(bJ);iKe=r(XPr,"model_type"),XPr.forEach(t),dKe=r(SM,` property of the config object
(either passed as an argument or loaded from `),vJ=n(SM,"CODE",{});var VPr=s(vJ);cKe=r(VPr,"pretrained_model_name_or_path"),VPr.forEach(t),fKe=r(SM,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),TJ=n(SM,"CODE",{});var zPr=s(TJ);mKe=r(zPr,"pretrained_model_name_or_path"),zPr.forEach(t),gKe=r(SM,":"),SM.forEach(t),hKe=i($t),se=n($t,"UL",{});var de=s(se);Yg=n(de,"LI",{});var CFe=s(Yg);FJ=n(CFe,"STRONG",{});var WPr=s(FJ);pKe=r(WPr,"beit"),WPr.forEach(t),_Ke=r(CFe," \u2014 "),yk=n(CFe,"A",{href:!0});var QPr=s(yk);uKe=r(QPr,"BeitFeatureExtractor"),QPr.forEach(t),bKe=r(CFe," (BEiT model)"),CFe.forEach(t),vKe=i(de),Kg=n(de,"LI",{});var MFe=s(Kg);CJ=n(MFe,"STRONG",{});var HPr=s(CJ);TKe=r(HPr,"clip"),HPr.forEach(t),FKe=r(MFe," \u2014 "),wk=n(MFe,"A",{href:!0});var UPr=s(wk);CKe=r(UPr,"CLIPFeatureExtractor"),UPr.forEach(t),MKe=r(MFe," (CLIP model)"),MFe.forEach(t),EKe=i(de),Zg=n(de,"LI",{});var EFe=s(Zg);MJ=n(EFe,"STRONG",{});var JPr=s(MJ);yKe=r(JPr,"convnext"),JPr.forEach(t),wKe=r(EFe," \u2014 "),Ak=n(EFe,"A",{href:!0});var YPr=s(Ak);AKe=r(YPr,"ConvNextFeatureExtractor"),YPr.forEach(t),LKe=r(EFe," (ConvNext model)"),EFe.forEach(t),BKe=i(de),eh=n(de,"LI",{});var yFe=s(eh);EJ=n(yFe,"STRONG",{});var KPr=s(EJ);xKe=r(KPr,"deit"),KPr.forEach(t),kKe=r(yFe," \u2014 "),Lk=n(yFe,"A",{href:!0});var ZPr=s(Lk);RKe=r(ZPr,"DeiTFeatureExtractor"),ZPr.forEach(t),SKe=r(yFe," (DeiT model)"),yFe.forEach(t),PKe=i(de),oh=n(de,"LI",{});var wFe=s(oh);yJ=n(wFe,"STRONG",{});var e$r=s(yJ);$Ke=r(e$r,"detr"),e$r.forEach(t),IKe=r(wFe," \u2014 "),Bk=n(wFe,"A",{href:!0});var o$r=s(Bk);DKe=r(o$r,"DetrFeatureExtractor"),o$r.forEach(t),jKe=r(wFe," (DETR model)"),wFe.forEach(t),NKe=i(de),rh=n(de,"LI",{});var AFe=s(rh);wJ=n(AFe,"STRONG",{});var r$r=s(wJ);qKe=r(r$r,"hubert"),r$r.forEach(t),GKe=r(AFe," \u2014 "),xk=n(AFe,"A",{href:!0});var t$r=s(xk);OKe=r(t$r,"Wav2Vec2FeatureExtractor"),t$r.forEach(t),XKe=r(AFe," (Hubert model)"),AFe.forEach(t),VKe=i(de),th=n(de,"LI",{});var LFe=s(th);AJ=n(LFe,"STRONG",{});var a$r=s(AJ);zKe=r(a$r,"layoutlmv2"),a$r.forEach(t),WKe=r(LFe," \u2014 "),kk=n(LFe,"A",{href:!0});var n$r=s(kk);QKe=r(n$r,"LayoutLMv2FeatureExtractor"),n$r.forEach(t),HKe=r(LFe," (LayoutLMv2 model)"),LFe.forEach(t),UKe=i(de),ah=n(de,"LI",{});var BFe=s(ah);LJ=n(BFe,"STRONG",{});var s$r=s(LJ);JKe=r(s$r,"perceiver"),s$r.forEach(t),YKe=r(BFe," \u2014 "),Rk=n(BFe,"A",{href:!0});var l$r=s(Rk);KKe=r(l$r,"PerceiverFeatureExtractor"),l$r.forEach(t),ZKe=r(BFe," (Perceiver model)"),BFe.forEach(t),eZe=i(de),nh=n(de,"LI",{});var xFe=s(nh);BJ=n(xFe,"STRONG",{});var i$r=s(BJ);oZe=r(i$r,"poolformer"),i$r.forEach(t),rZe=r(xFe," \u2014 "),Sk=n(xFe,"A",{href:!0});var d$r=s(Sk);tZe=r(d$r,"PoolFormerFeatureExtractor"),d$r.forEach(t),aZe=r(xFe," (PoolFormer model)"),xFe.forEach(t),nZe=i(de),sh=n(de,"LI",{});var kFe=s(sh);xJ=n(kFe,"STRONG",{});var c$r=s(xJ);sZe=r(c$r,"segformer"),c$r.forEach(t),lZe=r(kFe," \u2014 "),Pk=n(kFe,"A",{href:!0});var f$r=s(Pk);iZe=r(f$r,"SegformerFeatureExtractor"),f$r.forEach(t),dZe=r(kFe," (SegFormer model)"),kFe.forEach(t),cZe=i(de),lh=n(de,"LI",{});var RFe=s(lh);kJ=n(RFe,"STRONG",{});var m$r=s(kJ);fZe=r(m$r,"speech_to_text"),m$r.forEach(t),mZe=r(RFe," \u2014 "),$k=n(RFe,"A",{href:!0});var g$r=s($k);gZe=r(g$r,"Speech2TextFeatureExtractor"),g$r.forEach(t),hZe=r(RFe," (Speech2Text model)"),RFe.forEach(t),pZe=i(de),ih=n(de,"LI",{});var SFe=s(ih);RJ=n(SFe,"STRONG",{});var h$r=s(RJ);_Ze=r(h$r,"swin"),h$r.forEach(t),uZe=r(SFe," \u2014 "),Ik=n(SFe,"A",{href:!0});var p$r=s(Ik);bZe=r(p$r,"ViTFeatureExtractor"),p$r.forEach(t),vZe=r(SFe," (Swin model)"),SFe.forEach(t),TZe=i(de),dh=n(de,"LI",{});var PFe=s(dh);SJ=n(PFe,"STRONG",{});var _$r=s(SJ);FZe=r(_$r,"vit"),_$r.forEach(t),CZe=r(PFe," \u2014 "),Dk=n(PFe,"A",{href:!0});var u$r=s(Dk);MZe=r(u$r,"ViTFeatureExtractor"),u$r.forEach(t),EZe=r(PFe," (ViT model)"),PFe.forEach(t),yZe=i(de),ch=n(de,"LI",{});var $Fe=s(ch);PJ=n($Fe,"STRONG",{});var b$r=s(PJ);wZe=r(b$r,"vit_mae"),b$r.forEach(t),AZe=r($Fe," \u2014 "),jk=n($Fe,"A",{href:!0});var v$r=s(jk);LZe=r(v$r,"ViTFeatureExtractor"),v$r.forEach(t),BZe=r($Fe," (ViTMAE model)"),$Fe.forEach(t),xZe=i(de),fh=n(de,"LI",{});var IFe=s(fh);$J=n(IFe,"STRONG",{});var T$r=s($J);kZe=r(T$r,"wav2vec2"),T$r.forEach(t),RZe=r(IFe," \u2014 "),Nk=n(IFe,"A",{href:!0});var F$r=s(Nk);SZe=r(F$r,"Wav2Vec2FeatureExtractor"),F$r.forEach(t),PZe=r(IFe," (Wav2Vec2 model)"),IFe.forEach(t),de.forEach(t),$Ze=i($t),m(mh.$$.fragment,$t),IZe=i($t),IJ=n($t,"P",{});var C$r=s(IJ);DZe=r(C$r,"Examples:"),C$r.forEach(t),jZe=i($t),m(oE.$$.fragment,$t),$t.forEach(t),NZe=i(Ns),gh=n(Ns,"DIV",{class:!0});var vke=s(gh);m(rE.$$.fragment,vke),qZe=i(vke),DJ=n(vke,"P",{});var M$r=s(DJ);GZe=r(M$r,"Register a new feature extractor for this class."),M$r.forEach(t),vke.forEach(t),Ns.forEach(t),pBe=i(c),Gi=n(c,"H2",{class:!0});var Tke=s(Gi);hh=n(Tke,"A",{id:!0,class:!0,href:!0});var E$r=s(hh);jJ=n(E$r,"SPAN",{});var y$r=s(jJ);m(tE.$$.fragment,y$r),y$r.forEach(t),E$r.forEach(t),OZe=i(Tke),NJ=n(Tke,"SPAN",{});var w$r=s(NJ);XZe=r(w$r,"AutoProcessor"),w$r.forEach(t),Tke.forEach(t),_Be=i(c),Ho=n(c,"DIV",{class:!0});var qs=s(Ho);m(aE.$$.fragment,qs),VZe=i(qs),nE=n(qs,"P",{});var Fke=s(nE);zZe=r(Fke,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),qk=n(Fke,"A",{href:!0});var A$r=s(qk);WZe=r(A$r,"AutoProcessor.from_pretrained()"),A$r.forEach(t),QZe=r(Fke," class method."),Fke.forEach(t),HZe=i(qs),sE=n(qs,"P",{});var Cke=s(sE);UZe=r(Cke,"This class cannot be instantiated directly using "),qJ=n(Cke,"CODE",{});var L$r=s(qJ);JZe=r(L$r,"__init__()"),L$r.forEach(t),YZe=r(Cke," (throws an error)."),Cke.forEach(t),KZe=i(qs),Ie=n(qs,"DIV",{class:!0});var It=s(Ie);m(lE.$$.fragment,It),ZZe=i(It),GJ=n(It,"P",{});var B$r=s(GJ);eeo=r(B$r,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),B$r.forEach(t),oeo=i(It),Oi=n(It,"P",{});var JV=s(Oi);reo=r(JV,"The processor class to instantiate is selected based on the "),OJ=n(JV,"CODE",{});var x$r=s(OJ);teo=r(x$r,"model_type"),x$r.forEach(t),aeo=r(JV,` property of the config object (either
passed as an argument or loaded from `),XJ=n(JV,"CODE",{});var k$r=s(XJ);neo=r(k$r,"pretrained_model_name_or_path"),k$r.forEach(t),seo=r(JV," if possible):"),JV.forEach(t),leo=i(It),Be=n(It,"UL",{});var jo=s(Be);ph=n(jo,"LI",{});var DFe=s(ph);VJ=n(DFe,"STRONG",{});var R$r=s(VJ);ieo=r(R$r,"clip"),R$r.forEach(t),deo=r(DFe," \u2014 "),Gk=n(DFe,"A",{href:!0});var S$r=s(Gk);ceo=r(S$r,"CLIPProcessor"),S$r.forEach(t),feo=r(DFe," (CLIP model)"),DFe.forEach(t),meo=i(jo),_h=n(jo,"LI",{});var jFe=s(_h);zJ=n(jFe,"STRONG",{});var P$r=s(zJ);geo=r(P$r,"layoutlmv2"),P$r.forEach(t),heo=r(jFe," \u2014 "),Ok=n(jFe,"A",{href:!0});var $$r=s(Ok);peo=r($$r,"LayoutLMv2Processor"),$$r.forEach(t),_eo=r(jFe," (LayoutLMv2 model)"),jFe.forEach(t),ueo=i(jo),uh=n(jo,"LI",{});var NFe=s(uh);WJ=n(NFe,"STRONG",{});var I$r=s(WJ);beo=r(I$r,"layoutxlm"),I$r.forEach(t),veo=r(NFe," \u2014 "),Xk=n(NFe,"A",{href:!0});var D$r=s(Xk);Teo=r(D$r,"LayoutXLMProcessor"),D$r.forEach(t),Feo=r(NFe," (LayoutXLM model)"),NFe.forEach(t),Ceo=i(jo),bh=n(jo,"LI",{});var qFe=s(bh);QJ=n(qFe,"STRONG",{});var j$r=s(QJ);Meo=r(j$r,"speech_to_text"),j$r.forEach(t),Eeo=r(qFe," \u2014 "),Vk=n(qFe,"A",{href:!0});var N$r=s(Vk);yeo=r(N$r,"Speech2TextProcessor"),N$r.forEach(t),weo=r(qFe," (Speech2Text model)"),qFe.forEach(t),Aeo=i(jo),vh=n(jo,"LI",{});var GFe=s(vh);HJ=n(GFe,"STRONG",{});var q$r=s(HJ);Leo=r(q$r,"speech_to_text_2"),q$r.forEach(t),Beo=r(GFe," \u2014 "),zk=n(GFe,"A",{href:!0});var G$r=s(zk);xeo=r(G$r,"Speech2Text2Processor"),G$r.forEach(t),keo=r(GFe," (Speech2Text2 model)"),GFe.forEach(t),Reo=i(jo),Th=n(jo,"LI",{});var OFe=s(Th);UJ=n(OFe,"STRONG",{});var O$r=s(UJ);Seo=r(O$r,"trocr"),O$r.forEach(t),Peo=r(OFe," \u2014 "),Wk=n(OFe,"A",{href:!0});var X$r=s(Wk);$eo=r(X$r,"TrOCRProcessor"),X$r.forEach(t),Ieo=r(OFe," (TrOCR model)"),OFe.forEach(t),Deo=i(jo),Fh=n(jo,"LI",{});var XFe=s(Fh);JJ=n(XFe,"STRONG",{});var V$r=s(JJ);jeo=r(V$r,"vision-text-dual-encoder"),V$r.forEach(t),Neo=r(XFe," \u2014 "),Qk=n(XFe,"A",{href:!0});var z$r=s(Qk);qeo=r(z$r,"VisionTextDualEncoderProcessor"),z$r.forEach(t),Geo=r(XFe," (VisionTextDualEncoder model)"),XFe.forEach(t),Oeo=i(jo),Ch=n(jo,"LI",{});var VFe=s(Ch);YJ=n(VFe,"STRONG",{});var W$r=s(YJ);Xeo=r(W$r,"wav2vec2"),W$r.forEach(t),Veo=r(VFe," \u2014 "),Hk=n(VFe,"A",{href:!0});var Q$r=s(Hk);zeo=r(Q$r,"Wav2Vec2Processor"),Q$r.forEach(t),Weo=r(VFe," (Wav2Vec2 model)"),VFe.forEach(t),jo.forEach(t),Qeo=i(It),m(Mh.$$.fragment,It),Heo=i(It),KJ=n(It,"P",{});var H$r=s(KJ);Ueo=r(H$r,"Examples:"),H$r.forEach(t),Jeo=i(It),m(iE.$$.fragment,It),It.forEach(t),Yeo=i(qs),Eh=n(qs,"DIV",{class:!0});var Mke=s(Eh);m(dE.$$.fragment,Mke),Keo=i(Mke),ZJ=n(Mke,"P",{});var U$r=s(ZJ);Zeo=r(U$r,"Register a new processor for this class."),U$r.forEach(t),Mke.forEach(t),qs.forEach(t),uBe=i(c),Xi=n(c,"H2",{class:!0});var Eke=s(Xi);yh=n(Eke,"A",{id:!0,class:!0,href:!0});var J$r=s(yh);eY=n(J$r,"SPAN",{});var Y$r=s(eY);m(cE.$$.fragment,Y$r),Y$r.forEach(t),J$r.forEach(t),eoo=i(Eke),oY=n(Eke,"SPAN",{});var K$r=s(oY);ooo=r(K$r,"AutoModel"),K$r.forEach(t),Eke.forEach(t),bBe=i(c),Uo=n(c,"DIV",{class:!0});var Gs=s(Uo);m(fE.$$.fragment,Gs),roo=i(Gs),Vi=n(Gs,"P",{});var YV=s(Vi);too=r(YV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),rY=n(YV,"CODE",{});var Z$r=s(rY);aoo=r(Z$r,"from_pretrained()"),Z$r.forEach(t),noo=r(YV,"class method or the "),tY=n(YV,"CODE",{});var eIr=s(tY);soo=r(eIr,"from_config()"),eIr.forEach(t),loo=r(YV,`class
method.`),YV.forEach(t),ioo=i(Gs),mE=n(Gs,"P",{});var yke=s(mE);doo=r(yke,"This class cannot be instantiated directly using "),aY=n(yke,"CODE",{});var oIr=s(aY);coo=r(oIr,"__init__()"),oIr.forEach(t),foo=r(yke," (throws an error)."),yke.forEach(t),moo=i(Gs),Or=n(Gs,"DIV",{class:!0});var Os=s(Or);m(gE.$$.fragment,Os),goo=i(Os),nY=n(Os,"P",{});var rIr=s(nY);hoo=r(rIr,"Instantiates one of the base model classes of the library from a configuration."),rIr.forEach(t),poo=i(Os),zi=n(Os,"P",{});var KV=s(zi);_oo=r(KV,`Note:
Loading a model from its configuration file does `),sY=n(KV,"STRONG",{});var tIr=s(sY);uoo=r(tIr,"not"),tIr.forEach(t),boo=r(KV,` load the model weights. It only affects the
model\u2019s configuration. Use `),lY=n(KV,"CODE",{});var aIr=s(lY);voo=r(aIr,"from_pretrained()"),aIr.forEach(t),Too=r(KV,"to load the model weights."),KV.forEach(t),Foo=i(Os),iY=n(Os,"P",{});var nIr=s(iY);Coo=r(nIr,"Examples:"),nIr.forEach(t),Moo=i(Os),m(hE.$$.fragment,Os),Os.forEach(t),Eoo=i(Gs),De=n(Gs,"DIV",{class:!0});var Dt=s(De);m(pE.$$.fragment,Dt),yoo=i(Dt),dY=n(Dt,"P",{});var sIr=s(dY);woo=r(sIr,"Instantiate one of the base model classes of the library from a pretrained model."),sIr.forEach(t),Aoo=i(Dt),Na=n(Dt,"P",{});var PM=s(Na);Loo=r(PM,"The model class to instantiate is selected based on the "),cY=n(PM,"CODE",{});var lIr=s(cY);Boo=r(lIr,"model_type"),lIr.forEach(t),xoo=r(PM,` property of the config object (either
passed as an argument or loaded from `),fY=n(PM,"CODE",{});var iIr=s(fY);koo=r(iIr,"pretrained_model_name_or_path"),iIr.forEach(t),Roo=r(PM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mY=n(PM,"CODE",{});var dIr=s(mY);Soo=r(dIr,"pretrained_model_name_or_path"),dIr.forEach(t),Poo=r(PM,":"),PM.forEach(t),$oo=i(Dt),F=n(Dt,"UL",{});var C=s(F);wh=n(C,"LI",{});var zFe=s(wh);gY=n(zFe,"STRONG",{});var cIr=s(gY);Ioo=r(cIr,"albert"),cIr.forEach(t),Doo=r(zFe," \u2014 "),Uk=n(zFe,"A",{href:!0});var fIr=s(Uk);joo=r(fIr,"AlbertModel"),fIr.forEach(t),Noo=r(zFe," (ALBERT model)"),zFe.forEach(t),qoo=i(C),Ah=n(C,"LI",{});var WFe=s(Ah);hY=n(WFe,"STRONG",{});var mIr=s(hY);Goo=r(mIr,"bart"),mIr.forEach(t),Ooo=r(WFe," \u2014 "),Jk=n(WFe,"A",{href:!0});var gIr=s(Jk);Xoo=r(gIr,"BartModel"),gIr.forEach(t),Voo=r(WFe," (BART model)"),WFe.forEach(t),zoo=i(C),Lh=n(C,"LI",{});var QFe=s(Lh);pY=n(QFe,"STRONG",{});var hIr=s(pY);Woo=r(hIr,"beit"),hIr.forEach(t),Qoo=r(QFe," \u2014 "),Yk=n(QFe,"A",{href:!0});var pIr=s(Yk);Hoo=r(pIr,"BeitModel"),pIr.forEach(t),Uoo=r(QFe," (BEiT model)"),QFe.forEach(t),Joo=i(C),Bh=n(C,"LI",{});var HFe=s(Bh);_Y=n(HFe,"STRONG",{});var _Ir=s(_Y);Yoo=r(_Ir,"bert"),_Ir.forEach(t),Koo=r(HFe," \u2014 "),Kk=n(HFe,"A",{href:!0});var uIr=s(Kk);Zoo=r(uIr,"BertModel"),uIr.forEach(t),ero=r(HFe," (BERT model)"),HFe.forEach(t),oro=i(C),xh=n(C,"LI",{});var UFe=s(xh);uY=n(UFe,"STRONG",{});var bIr=s(uY);rro=r(bIr,"bert-generation"),bIr.forEach(t),tro=r(UFe," \u2014 "),Zk=n(UFe,"A",{href:!0});var vIr=s(Zk);aro=r(vIr,"BertGenerationEncoder"),vIr.forEach(t),nro=r(UFe," (Bert Generation model)"),UFe.forEach(t),sro=i(C),kh=n(C,"LI",{});var JFe=s(kh);bY=n(JFe,"STRONG",{});var TIr=s(bY);lro=r(TIr,"big_bird"),TIr.forEach(t),iro=r(JFe," \u2014 "),eR=n(JFe,"A",{href:!0});var FIr=s(eR);dro=r(FIr,"BigBirdModel"),FIr.forEach(t),cro=r(JFe," (BigBird model)"),JFe.forEach(t),fro=i(C),Rh=n(C,"LI",{});var YFe=s(Rh);vY=n(YFe,"STRONG",{});var CIr=s(vY);mro=r(CIr,"bigbird_pegasus"),CIr.forEach(t),gro=r(YFe," \u2014 "),oR=n(YFe,"A",{href:!0});var MIr=s(oR);hro=r(MIr,"BigBirdPegasusModel"),MIr.forEach(t),pro=r(YFe," (BigBirdPegasus model)"),YFe.forEach(t),_ro=i(C),Sh=n(C,"LI",{});var KFe=s(Sh);TY=n(KFe,"STRONG",{});var EIr=s(TY);uro=r(EIr,"blenderbot"),EIr.forEach(t),bro=r(KFe," \u2014 "),rR=n(KFe,"A",{href:!0});var yIr=s(rR);vro=r(yIr,"BlenderbotModel"),yIr.forEach(t),Tro=r(KFe," (Blenderbot model)"),KFe.forEach(t),Fro=i(C),Ph=n(C,"LI",{});var ZFe=s(Ph);FY=n(ZFe,"STRONG",{});var wIr=s(FY);Cro=r(wIr,"blenderbot-small"),wIr.forEach(t),Mro=r(ZFe," \u2014 "),tR=n(ZFe,"A",{href:!0});var AIr=s(tR);Ero=r(AIr,"BlenderbotSmallModel"),AIr.forEach(t),yro=r(ZFe," (BlenderbotSmall model)"),ZFe.forEach(t),wro=i(C),$h=n(C,"LI",{});var e9e=s($h);CY=n(e9e,"STRONG",{});var LIr=s(CY);Aro=r(LIr,"camembert"),LIr.forEach(t),Lro=r(e9e," \u2014 "),aR=n(e9e,"A",{href:!0});var BIr=s(aR);Bro=r(BIr,"CamembertModel"),BIr.forEach(t),xro=r(e9e," (CamemBERT model)"),e9e.forEach(t),kro=i(C),Ih=n(C,"LI",{});var o9e=s(Ih);MY=n(o9e,"STRONG",{});var xIr=s(MY);Rro=r(xIr,"canine"),xIr.forEach(t),Sro=r(o9e," \u2014 "),nR=n(o9e,"A",{href:!0});var kIr=s(nR);Pro=r(kIr,"CanineModel"),kIr.forEach(t),$ro=r(o9e," (Canine model)"),o9e.forEach(t),Iro=i(C),Dh=n(C,"LI",{});var r9e=s(Dh);EY=n(r9e,"STRONG",{});var RIr=s(EY);Dro=r(RIr,"clip"),RIr.forEach(t),jro=r(r9e," \u2014 "),sR=n(r9e,"A",{href:!0});var SIr=s(sR);Nro=r(SIr,"CLIPModel"),SIr.forEach(t),qro=r(r9e," (CLIP model)"),r9e.forEach(t),Gro=i(C),jh=n(C,"LI",{});var t9e=s(jh);yY=n(t9e,"STRONG",{});var PIr=s(yY);Oro=r(PIr,"convbert"),PIr.forEach(t),Xro=r(t9e," \u2014 "),lR=n(t9e,"A",{href:!0});var $Ir=s(lR);Vro=r($Ir,"ConvBertModel"),$Ir.forEach(t),zro=r(t9e," (ConvBERT model)"),t9e.forEach(t),Wro=i(C),Nh=n(C,"LI",{});var a9e=s(Nh);wY=n(a9e,"STRONG",{});var IIr=s(wY);Qro=r(IIr,"convnext"),IIr.forEach(t),Hro=r(a9e," \u2014 "),iR=n(a9e,"A",{href:!0});var DIr=s(iR);Uro=r(DIr,"ConvNextModel"),DIr.forEach(t),Jro=r(a9e," (ConvNext model)"),a9e.forEach(t),Yro=i(C),qh=n(C,"LI",{});var n9e=s(qh);AY=n(n9e,"STRONG",{});var jIr=s(AY);Kro=r(jIr,"ctrl"),jIr.forEach(t),Zro=r(n9e," \u2014 "),dR=n(n9e,"A",{href:!0});var NIr=s(dR);eto=r(NIr,"CTRLModel"),NIr.forEach(t),oto=r(n9e," (CTRL model)"),n9e.forEach(t),rto=i(C),Gh=n(C,"LI",{});var s9e=s(Gh);LY=n(s9e,"STRONG",{});var qIr=s(LY);tto=r(qIr,"data2vec-audio"),qIr.forEach(t),ato=r(s9e," \u2014 "),cR=n(s9e,"A",{href:!0});var GIr=s(cR);nto=r(GIr,"Data2VecAudioModel"),GIr.forEach(t),sto=r(s9e," (Data2VecAudio model)"),s9e.forEach(t),lto=i(C),Oh=n(C,"LI",{});var l9e=s(Oh);BY=n(l9e,"STRONG",{});var OIr=s(BY);ito=r(OIr,"data2vec-text"),OIr.forEach(t),dto=r(l9e," \u2014 "),fR=n(l9e,"A",{href:!0});var XIr=s(fR);cto=r(XIr,"Data2VecTextModel"),XIr.forEach(t),fto=r(l9e," (Data2VecText model)"),l9e.forEach(t),mto=i(C),Xh=n(C,"LI",{});var i9e=s(Xh);xY=n(i9e,"STRONG",{});var VIr=s(xY);gto=r(VIr,"deberta"),VIr.forEach(t),hto=r(i9e," \u2014 "),mR=n(i9e,"A",{href:!0});var zIr=s(mR);pto=r(zIr,"DebertaModel"),zIr.forEach(t),_to=r(i9e," (DeBERTa model)"),i9e.forEach(t),uto=i(C),Vh=n(C,"LI",{});var d9e=s(Vh);kY=n(d9e,"STRONG",{});var WIr=s(kY);bto=r(WIr,"deberta-v2"),WIr.forEach(t),vto=r(d9e," \u2014 "),gR=n(d9e,"A",{href:!0});var QIr=s(gR);Tto=r(QIr,"DebertaV2Model"),QIr.forEach(t),Fto=r(d9e," (DeBERTa-v2 model)"),d9e.forEach(t),Cto=i(C),zh=n(C,"LI",{});var c9e=s(zh);RY=n(c9e,"STRONG",{});var HIr=s(RY);Mto=r(HIr,"deit"),HIr.forEach(t),Eto=r(c9e," \u2014 "),hR=n(c9e,"A",{href:!0});var UIr=s(hR);yto=r(UIr,"DeiTModel"),UIr.forEach(t),wto=r(c9e," (DeiT model)"),c9e.forEach(t),Ato=i(C),Wh=n(C,"LI",{});var f9e=s(Wh);SY=n(f9e,"STRONG",{});var JIr=s(SY);Lto=r(JIr,"detr"),JIr.forEach(t),Bto=r(f9e," \u2014 "),pR=n(f9e,"A",{href:!0});var YIr=s(pR);xto=r(YIr,"DetrModel"),YIr.forEach(t),kto=r(f9e," (DETR model)"),f9e.forEach(t),Rto=i(C),Qh=n(C,"LI",{});var m9e=s(Qh);PY=n(m9e,"STRONG",{});var KIr=s(PY);Sto=r(KIr,"distilbert"),KIr.forEach(t),Pto=r(m9e," \u2014 "),_R=n(m9e,"A",{href:!0});var ZIr=s(_R);$to=r(ZIr,"DistilBertModel"),ZIr.forEach(t),Ito=r(m9e," (DistilBERT model)"),m9e.forEach(t),Dto=i(C),Hh=n(C,"LI",{});var g9e=s(Hh);$Y=n(g9e,"STRONG",{});var eDr=s($Y);jto=r(eDr,"dpr"),eDr.forEach(t),Nto=r(g9e," \u2014 "),uR=n(g9e,"A",{href:!0});var oDr=s(uR);qto=r(oDr,"DPRQuestionEncoder"),oDr.forEach(t),Gto=r(g9e," (DPR model)"),g9e.forEach(t),Oto=i(C),Uh=n(C,"LI",{});var h9e=s(Uh);IY=n(h9e,"STRONG",{});var rDr=s(IY);Xto=r(rDr,"electra"),rDr.forEach(t),Vto=r(h9e," \u2014 "),bR=n(h9e,"A",{href:!0});var tDr=s(bR);zto=r(tDr,"ElectraModel"),tDr.forEach(t),Wto=r(h9e," (ELECTRA model)"),h9e.forEach(t),Qto=i(C),Jh=n(C,"LI",{});var p9e=s(Jh);DY=n(p9e,"STRONG",{});var aDr=s(DY);Hto=r(aDr,"flaubert"),aDr.forEach(t),Uto=r(p9e," \u2014 "),vR=n(p9e,"A",{href:!0});var nDr=s(vR);Jto=r(nDr,"FlaubertModel"),nDr.forEach(t),Yto=r(p9e," (FlauBERT model)"),p9e.forEach(t),Kto=i(C),Yh=n(C,"LI",{});var _9e=s(Yh);jY=n(_9e,"STRONG",{});var sDr=s(jY);Zto=r(sDr,"fnet"),sDr.forEach(t),eao=r(_9e," \u2014 "),TR=n(_9e,"A",{href:!0});var lDr=s(TR);oao=r(lDr,"FNetModel"),lDr.forEach(t),rao=r(_9e," (FNet model)"),_9e.forEach(t),tao=i(C),Kh=n(C,"LI",{});var u9e=s(Kh);NY=n(u9e,"STRONG",{});var iDr=s(NY);aao=r(iDr,"fsmt"),iDr.forEach(t),nao=r(u9e," \u2014 "),FR=n(u9e,"A",{href:!0});var dDr=s(FR);sao=r(dDr,"FSMTModel"),dDr.forEach(t),lao=r(u9e," (FairSeq Machine-Translation model)"),u9e.forEach(t),iao=i(C),Ps=n(C,"LI",{});var g8=s(Ps);qY=n(g8,"STRONG",{});var cDr=s(qY);dao=r(cDr,"funnel"),cDr.forEach(t),cao=r(g8," \u2014 "),CR=n(g8,"A",{href:!0});var fDr=s(CR);fao=r(fDr,"FunnelModel"),fDr.forEach(t),mao=r(g8," or "),MR=n(g8,"A",{href:!0});var mDr=s(MR);gao=r(mDr,"FunnelBaseModel"),mDr.forEach(t),hao=r(g8," (Funnel Transformer model)"),g8.forEach(t),pao=i(C),Zh=n(C,"LI",{});var b9e=s(Zh);GY=n(b9e,"STRONG",{});var gDr=s(GY);_ao=r(gDr,"gpt2"),gDr.forEach(t),uao=r(b9e," \u2014 "),ER=n(b9e,"A",{href:!0});var hDr=s(ER);bao=r(hDr,"GPT2Model"),hDr.forEach(t),vao=r(b9e," (OpenAI GPT-2 model)"),b9e.forEach(t),Tao=i(C),ep=n(C,"LI",{});var v9e=s(ep);OY=n(v9e,"STRONG",{});var pDr=s(OY);Fao=r(pDr,"gpt_neo"),pDr.forEach(t),Cao=r(v9e," \u2014 "),yR=n(v9e,"A",{href:!0});var _Dr=s(yR);Mao=r(_Dr,"GPTNeoModel"),_Dr.forEach(t),Eao=r(v9e," (GPT Neo model)"),v9e.forEach(t),yao=i(C),op=n(C,"LI",{});var T9e=s(op);XY=n(T9e,"STRONG",{});var uDr=s(XY);wao=r(uDr,"gptj"),uDr.forEach(t),Aao=r(T9e," \u2014 "),wR=n(T9e,"A",{href:!0});var bDr=s(wR);Lao=r(bDr,"GPTJModel"),bDr.forEach(t),Bao=r(T9e," (GPT-J model)"),T9e.forEach(t),xao=i(C),rp=n(C,"LI",{});var F9e=s(rp);VY=n(F9e,"STRONG",{});var vDr=s(VY);kao=r(vDr,"hubert"),vDr.forEach(t),Rao=r(F9e," \u2014 "),AR=n(F9e,"A",{href:!0});var TDr=s(AR);Sao=r(TDr,"HubertModel"),TDr.forEach(t),Pao=r(F9e," (Hubert model)"),F9e.forEach(t),$ao=i(C),tp=n(C,"LI",{});var C9e=s(tp);zY=n(C9e,"STRONG",{});var FDr=s(zY);Iao=r(FDr,"ibert"),FDr.forEach(t),Dao=r(C9e," \u2014 "),LR=n(C9e,"A",{href:!0});var CDr=s(LR);jao=r(CDr,"IBertModel"),CDr.forEach(t),Nao=r(C9e," (I-BERT model)"),C9e.forEach(t),qao=i(C),ap=n(C,"LI",{});var M9e=s(ap);WY=n(M9e,"STRONG",{});var MDr=s(WY);Gao=r(MDr,"imagegpt"),MDr.forEach(t),Oao=r(M9e," \u2014 "),BR=n(M9e,"A",{href:!0});var EDr=s(BR);Xao=r(EDr,"ImageGPTModel"),EDr.forEach(t),Vao=r(M9e," (ImageGPT model)"),M9e.forEach(t),zao=i(C),np=n(C,"LI",{});var E9e=s(np);QY=n(E9e,"STRONG",{});var yDr=s(QY);Wao=r(yDr,"layoutlm"),yDr.forEach(t),Qao=r(E9e," \u2014 "),xR=n(E9e,"A",{href:!0});var wDr=s(xR);Hao=r(wDr,"LayoutLMModel"),wDr.forEach(t),Uao=r(E9e," (LayoutLM model)"),E9e.forEach(t),Jao=i(C),sp=n(C,"LI",{});var y9e=s(sp);HY=n(y9e,"STRONG",{});var ADr=s(HY);Yao=r(ADr,"layoutlmv2"),ADr.forEach(t),Kao=r(y9e," \u2014 "),kR=n(y9e,"A",{href:!0});var LDr=s(kR);Zao=r(LDr,"LayoutLMv2Model"),LDr.forEach(t),eno=r(y9e," (LayoutLMv2 model)"),y9e.forEach(t),ono=i(C),lp=n(C,"LI",{});var w9e=s(lp);UY=n(w9e,"STRONG",{});var BDr=s(UY);rno=r(BDr,"led"),BDr.forEach(t),tno=r(w9e," \u2014 "),RR=n(w9e,"A",{href:!0});var xDr=s(RR);ano=r(xDr,"LEDModel"),xDr.forEach(t),nno=r(w9e," (LED model)"),w9e.forEach(t),sno=i(C),ip=n(C,"LI",{});var A9e=s(ip);JY=n(A9e,"STRONG",{});var kDr=s(JY);lno=r(kDr,"longformer"),kDr.forEach(t),ino=r(A9e," \u2014 "),SR=n(A9e,"A",{href:!0});var RDr=s(SR);dno=r(RDr,"LongformerModel"),RDr.forEach(t),cno=r(A9e," (Longformer model)"),A9e.forEach(t),fno=i(C),dp=n(C,"LI",{});var L9e=s(dp);YY=n(L9e,"STRONG",{});var SDr=s(YY);mno=r(SDr,"luke"),SDr.forEach(t),gno=r(L9e," \u2014 "),PR=n(L9e,"A",{href:!0});var PDr=s(PR);hno=r(PDr,"LukeModel"),PDr.forEach(t),pno=r(L9e," (LUKE model)"),L9e.forEach(t),_no=i(C),cp=n(C,"LI",{});var B9e=s(cp);KY=n(B9e,"STRONG",{});var $Dr=s(KY);uno=r($Dr,"lxmert"),$Dr.forEach(t),bno=r(B9e," \u2014 "),$R=n(B9e,"A",{href:!0});var IDr=s($R);vno=r(IDr,"LxmertModel"),IDr.forEach(t),Tno=r(B9e," (LXMERT model)"),B9e.forEach(t),Fno=i(C),fp=n(C,"LI",{});var x9e=s(fp);ZY=n(x9e,"STRONG",{});var DDr=s(ZY);Cno=r(DDr,"m2m_100"),DDr.forEach(t),Mno=r(x9e," \u2014 "),IR=n(x9e,"A",{href:!0});var jDr=s(IR);Eno=r(jDr,"M2M100Model"),jDr.forEach(t),yno=r(x9e," (M2M100 model)"),x9e.forEach(t),wno=i(C),mp=n(C,"LI",{});var k9e=s(mp);eK=n(k9e,"STRONG",{});var NDr=s(eK);Ano=r(NDr,"marian"),NDr.forEach(t),Lno=r(k9e," \u2014 "),DR=n(k9e,"A",{href:!0});var qDr=s(DR);Bno=r(qDr,"MarianModel"),qDr.forEach(t),xno=r(k9e," (Marian model)"),k9e.forEach(t),kno=i(C),gp=n(C,"LI",{});var R9e=s(gp);oK=n(R9e,"STRONG",{});var GDr=s(oK);Rno=r(GDr,"maskformer"),GDr.forEach(t),Sno=r(R9e," \u2014 "),jR=n(R9e,"A",{href:!0});var ODr=s(jR);Pno=r(ODr,"MaskFormerModel"),ODr.forEach(t),$no=r(R9e," (MaskFormer model)"),R9e.forEach(t),Ino=i(C),hp=n(C,"LI",{});var S9e=s(hp);rK=n(S9e,"STRONG",{});var XDr=s(rK);Dno=r(XDr,"mbart"),XDr.forEach(t),jno=r(S9e," \u2014 "),NR=n(S9e,"A",{href:!0});var VDr=s(NR);Nno=r(VDr,"MBartModel"),VDr.forEach(t),qno=r(S9e," (mBART model)"),S9e.forEach(t),Gno=i(C),pp=n(C,"LI",{});var P9e=s(pp);tK=n(P9e,"STRONG",{});var zDr=s(tK);Ono=r(zDr,"megatron-bert"),zDr.forEach(t),Xno=r(P9e," \u2014 "),qR=n(P9e,"A",{href:!0});var WDr=s(qR);Vno=r(WDr,"MegatronBertModel"),WDr.forEach(t),zno=r(P9e," (MegatronBert model)"),P9e.forEach(t),Wno=i(C),_p=n(C,"LI",{});var $9e=s(_p);aK=n($9e,"STRONG",{});var QDr=s(aK);Qno=r(QDr,"mobilebert"),QDr.forEach(t),Hno=r($9e," \u2014 "),GR=n($9e,"A",{href:!0});var HDr=s(GR);Uno=r(HDr,"MobileBertModel"),HDr.forEach(t),Jno=r($9e," (MobileBERT model)"),$9e.forEach(t),Yno=i(C),up=n(C,"LI",{});var I9e=s(up);nK=n(I9e,"STRONG",{});var UDr=s(nK);Kno=r(UDr,"mpnet"),UDr.forEach(t),Zno=r(I9e," \u2014 "),OR=n(I9e,"A",{href:!0});var JDr=s(OR);eso=r(JDr,"MPNetModel"),JDr.forEach(t),oso=r(I9e," (MPNet model)"),I9e.forEach(t),rso=i(C),bp=n(C,"LI",{});var D9e=s(bp);sK=n(D9e,"STRONG",{});var YDr=s(sK);tso=r(YDr,"mt5"),YDr.forEach(t),aso=r(D9e," \u2014 "),XR=n(D9e,"A",{href:!0});var KDr=s(XR);nso=r(KDr,"MT5Model"),KDr.forEach(t),sso=r(D9e," (mT5 model)"),D9e.forEach(t),lso=i(C),vp=n(C,"LI",{});var j9e=s(vp);lK=n(j9e,"STRONG",{});var ZDr=s(lK);iso=r(ZDr,"nystromformer"),ZDr.forEach(t),dso=r(j9e," \u2014 "),VR=n(j9e,"A",{href:!0});var ejr=s(VR);cso=r(ejr,"NystromformerModel"),ejr.forEach(t),fso=r(j9e," (Nystromformer model)"),j9e.forEach(t),mso=i(C),Tp=n(C,"LI",{});var N9e=s(Tp);iK=n(N9e,"STRONG",{});var ojr=s(iK);gso=r(ojr,"openai-gpt"),ojr.forEach(t),hso=r(N9e," \u2014 "),zR=n(N9e,"A",{href:!0});var rjr=s(zR);pso=r(rjr,"OpenAIGPTModel"),rjr.forEach(t),_so=r(N9e," (OpenAI GPT model)"),N9e.forEach(t),uso=i(C),Fp=n(C,"LI",{});var q9e=s(Fp);dK=n(q9e,"STRONG",{});var tjr=s(dK);bso=r(tjr,"pegasus"),tjr.forEach(t),vso=r(q9e," \u2014 "),WR=n(q9e,"A",{href:!0});var ajr=s(WR);Tso=r(ajr,"PegasusModel"),ajr.forEach(t),Fso=r(q9e," (Pegasus model)"),q9e.forEach(t),Cso=i(C),Cp=n(C,"LI",{});var G9e=s(Cp);cK=n(G9e,"STRONG",{});var njr=s(cK);Mso=r(njr,"perceiver"),njr.forEach(t),Eso=r(G9e," \u2014 "),QR=n(G9e,"A",{href:!0});var sjr=s(QR);yso=r(sjr,"PerceiverModel"),sjr.forEach(t),wso=r(G9e," (Perceiver model)"),G9e.forEach(t),Aso=i(C),Mp=n(C,"LI",{});var O9e=s(Mp);fK=n(O9e,"STRONG",{});var ljr=s(fK);Lso=r(ljr,"plbart"),ljr.forEach(t),Bso=r(O9e," \u2014 "),HR=n(O9e,"A",{href:!0});var ijr=s(HR);xso=r(ijr,"PLBartModel"),ijr.forEach(t),kso=r(O9e," (PLBart model)"),O9e.forEach(t),Rso=i(C),Ep=n(C,"LI",{});var X9e=s(Ep);mK=n(X9e,"STRONG",{});var djr=s(mK);Sso=r(djr,"poolformer"),djr.forEach(t),Pso=r(X9e," \u2014 "),UR=n(X9e,"A",{href:!0});var cjr=s(UR);$so=r(cjr,"PoolFormerModel"),cjr.forEach(t),Iso=r(X9e," (PoolFormer model)"),X9e.forEach(t),Dso=i(C),yp=n(C,"LI",{});var V9e=s(yp);gK=n(V9e,"STRONG",{});var fjr=s(gK);jso=r(fjr,"prophetnet"),fjr.forEach(t),Nso=r(V9e," \u2014 "),JR=n(V9e,"A",{href:!0});var mjr=s(JR);qso=r(mjr,"ProphetNetModel"),mjr.forEach(t),Gso=r(V9e," (ProphetNet model)"),V9e.forEach(t),Oso=i(C),wp=n(C,"LI",{});var z9e=s(wp);hK=n(z9e,"STRONG",{});var gjr=s(hK);Xso=r(gjr,"qdqbert"),gjr.forEach(t),Vso=r(z9e," \u2014 "),YR=n(z9e,"A",{href:!0});var hjr=s(YR);zso=r(hjr,"QDQBertModel"),hjr.forEach(t),Wso=r(z9e," (QDQBert model)"),z9e.forEach(t),Qso=i(C),Ap=n(C,"LI",{});var W9e=s(Ap);pK=n(W9e,"STRONG",{});var pjr=s(pK);Hso=r(pjr,"reformer"),pjr.forEach(t),Uso=r(W9e," \u2014 "),KR=n(W9e,"A",{href:!0});var _jr=s(KR);Jso=r(_jr,"ReformerModel"),_jr.forEach(t),Yso=r(W9e," (Reformer model)"),W9e.forEach(t),Kso=i(C),Lp=n(C,"LI",{});var Q9e=s(Lp);_K=n(Q9e,"STRONG",{});var ujr=s(_K);Zso=r(ujr,"rembert"),ujr.forEach(t),elo=r(Q9e," \u2014 "),ZR=n(Q9e,"A",{href:!0});var bjr=s(ZR);olo=r(bjr,"RemBertModel"),bjr.forEach(t),rlo=r(Q9e," (RemBERT model)"),Q9e.forEach(t),tlo=i(C),Bp=n(C,"LI",{});var H9e=s(Bp);uK=n(H9e,"STRONG",{});var vjr=s(uK);alo=r(vjr,"retribert"),vjr.forEach(t),nlo=r(H9e," \u2014 "),eS=n(H9e,"A",{href:!0});var Tjr=s(eS);slo=r(Tjr,"RetriBertModel"),Tjr.forEach(t),llo=r(H9e," (RetriBERT model)"),H9e.forEach(t),ilo=i(C),xp=n(C,"LI",{});var U9e=s(xp);bK=n(U9e,"STRONG",{});var Fjr=s(bK);dlo=r(Fjr,"roberta"),Fjr.forEach(t),clo=r(U9e," \u2014 "),oS=n(U9e,"A",{href:!0});var Cjr=s(oS);flo=r(Cjr,"RobertaModel"),Cjr.forEach(t),mlo=r(U9e," (RoBERTa model)"),U9e.forEach(t),glo=i(C),kp=n(C,"LI",{});var J9e=s(kp);vK=n(J9e,"STRONG",{});var Mjr=s(vK);hlo=r(Mjr,"roformer"),Mjr.forEach(t),plo=r(J9e," \u2014 "),rS=n(J9e,"A",{href:!0});var Ejr=s(rS);_lo=r(Ejr,"RoFormerModel"),Ejr.forEach(t),ulo=r(J9e," (RoFormer model)"),J9e.forEach(t),blo=i(C),Rp=n(C,"LI",{});var Y9e=s(Rp);TK=n(Y9e,"STRONG",{});var yjr=s(TK);vlo=r(yjr,"segformer"),yjr.forEach(t),Tlo=r(Y9e," \u2014 "),tS=n(Y9e,"A",{href:!0});var wjr=s(tS);Flo=r(wjr,"SegformerModel"),wjr.forEach(t),Clo=r(Y9e," (SegFormer model)"),Y9e.forEach(t),Mlo=i(C),Sp=n(C,"LI",{});var K9e=s(Sp);FK=n(K9e,"STRONG",{});var Ajr=s(FK);Elo=r(Ajr,"sew"),Ajr.forEach(t),ylo=r(K9e," \u2014 "),aS=n(K9e,"A",{href:!0});var Ljr=s(aS);wlo=r(Ljr,"SEWModel"),Ljr.forEach(t),Alo=r(K9e," (SEW model)"),K9e.forEach(t),Llo=i(C),Pp=n(C,"LI",{});var Z9e=s(Pp);CK=n(Z9e,"STRONG",{});var Bjr=s(CK);Blo=r(Bjr,"sew-d"),Bjr.forEach(t),xlo=r(Z9e," \u2014 "),nS=n(Z9e,"A",{href:!0});var xjr=s(nS);klo=r(xjr,"SEWDModel"),xjr.forEach(t),Rlo=r(Z9e," (SEW-D model)"),Z9e.forEach(t),Slo=i(C),$p=n(C,"LI",{});var eCe=s($p);MK=n(eCe,"STRONG",{});var kjr=s(MK);Plo=r(kjr,"speech_to_text"),kjr.forEach(t),$lo=r(eCe," \u2014 "),sS=n(eCe,"A",{href:!0});var Rjr=s(sS);Ilo=r(Rjr,"Speech2TextModel"),Rjr.forEach(t),Dlo=r(eCe," (Speech2Text model)"),eCe.forEach(t),jlo=i(C),Ip=n(C,"LI",{});var oCe=s(Ip);EK=n(oCe,"STRONG",{});var Sjr=s(EK);Nlo=r(Sjr,"splinter"),Sjr.forEach(t),qlo=r(oCe," \u2014 "),lS=n(oCe,"A",{href:!0});var Pjr=s(lS);Glo=r(Pjr,"SplinterModel"),Pjr.forEach(t),Olo=r(oCe," (Splinter model)"),oCe.forEach(t),Xlo=i(C),Dp=n(C,"LI",{});var rCe=s(Dp);yK=n(rCe,"STRONG",{});var $jr=s(yK);Vlo=r($jr,"squeezebert"),$jr.forEach(t),zlo=r(rCe," \u2014 "),iS=n(rCe,"A",{href:!0});var Ijr=s(iS);Wlo=r(Ijr,"SqueezeBertModel"),Ijr.forEach(t),Qlo=r(rCe," (SqueezeBERT model)"),rCe.forEach(t),Hlo=i(C),jp=n(C,"LI",{});var tCe=s(jp);wK=n(tCe,"STRONG",{});var Djr=s(wK);Ulo=r(Djr,"swin"),Djr.forEach(t),Jlo=r(tCe," \u2014 "),dS=n(tCe,"A",{href:!0});var jjr=s(dS);Ylo=r(jjr,"SwinModel"),jjr.forEach(t),Klo=r(tCe," (Swin model)"),tCe.forEach(t),Zlo=i(C),Np=n(C,"LI",{});var aCe=s(Np);AK=n(aCe,"STRONG",{});var Njr=s(AK);eio=r(Njr,"t5"),Njr.forEach(t),oio=r(aCe," \u2014 "),cS=n(aCe,"A",{href:!0});var qjr=s(cS);rio=r(qjr,"T5Model"),qjr.forEach(t),tio=r(aCe," (T5 model)"),aCe.forEach(t),aio=i(C),qp=n(C,"LI",{});var nCe=s(qp);LK=n(nCe,"STRONG",{});var Gjr=s(LK);nio=r(Gjr,"tapas"),Gjr.forEach(t),sio=r(nCe," \u2014 "),fS=n(nCe,"A",{href:!0});var Ojr=s(fS);lio=r(Ojr,"TapasModel"),Ojr.forEach(t),iio=r(nCe," (TAPAS model)"),nCe.forEach(t),dio=i(C),Gp=n(C,"LI",{});var sCe=s(Gp);BK=n(sCe,"STRONG",{});var Xjr=s(BK);cio=r(Xjr,"transfo-xl"),Xjr.forEach(t),fio=r(sCe," \u2014 "),mS=n(sCe,"A",{href:!0});var Vjr=s(mS);mio=r(Vjr,"TransfoXLModel"),Vjr.forEach(t),gio=r(sCe," (Transformer-XL model)"),sCe.forEach(t),hio=i(C),Op=n(C,"LI",{});var lCe=s(Op);xK=n(lCe,"STRONG",{});var zjr=s(xK);pio=r(zjr,"unispeech"),zjr.forEach(t),_io=r(lCe," \u2014 "),gS=n(lCe,"A",{href:!0});var Wjr=s(gS);uio=r(Wjr,"UniSpeechModel"),Wjr.forEach(t),bio=r(lCe," (UniSpeech model)"),lCe.forEach(t),vio=i(C),Xp=n(C,"LI",{});var iCe=s(Xp);kK=n(iCe,"STRONG",{});var Qjr=s(kK);Tio=r(Qjr,"unispeech-sat"),Qjr.forEach(t),Fio=r(iCe," \u2014 "),hS=n(iCe,"A",{href:!0});var Hjr=s(hS);Cio=r(Hjr,"UniSpeechSatModel"),Hjr.forEach(t),Mio=r(iCe," (UniSpeechSat model)"),iCe.forEach(t),Eio=i(C),Vp=n(C,"LI",{});var dCe=s(Vp);RK=n(dCe,"STRONG",{});var Ujr=s(RK);yio=r(Ujr,"vilt"),Ujr.forEach(t),wio=r(dCe," \u2014 "),pS=n(dCe,"A",{href:!0});var Jjr=s(pS);Aio=r(Jjr,"ViltModel"),Jjr.forEach(t),Lio=r(dCe," (ViLT model)"),dCe.forEach(t),Bio=i(C),zp=n(C,"LI",{});var cCe=s(zp);SK=n(cCe,"STRONG",{});var Yjr=s(SK);xio=r(Yjr,"vision-text-dual-encoder"),Yjr.forEach(t),kio=r(cCe," \u2014 "),_S=n(cCe,"A",{href:!0});var Kjr=s(_S);Rio=r(Kjr,"VisionTextDualEncoderModel"),Kjr.forEach(t),Sio=r(cCe," (VisionTextDualEncoder model)"),cCe.forEach(t),Pio=i(C),Wp=n(C,"LI",{});var fCe=s(Wp);PK=n(fCe,"STRONG",{});var Zjr=s(PK);$io=r(Zjr,"visual_bert"),Zjr.forEach(t),Iio=r(fCe," \u2014 "),uS=n(fCe,"A",{href:!0});var eNr=s(uS);Dio=r(eNr,"VisualBertModel"),eNr.forEach(t),jio=r(fCe," (VisualBert model)"),fCe.forEach(t),Nio=i(C),Qp=n(C,"LI",{});var mCe=s(Qp);$K=n(mCe,"STRONG",{});var oNr=s($K);qio=r(oNr,"vit"),oNr.forEach(t),Gio=r(mCe," \u2014 "),bS=n(mCe,"A",{href:!0});var rNr=s(bS);Oio=r(rNr,"ViTModel"),rNr.forEach(t),Xio=r(mCe," (ViT model)"),mCe.forEach(t),Vio=i(C),Hp=n(C,"LI",{});var gCe=s(Hp);IK=n(gCe,"STRONG",{});var tNr=s(IK);zio=r(tNr,"vit_mae"),tNr.forEach(t),Wio=r(gCe," \u2014 "),vS=n(gCe,"A",{href:!0});var aNr=s(vS);Qio=r(aNr,"ViTMAEModel"),aNr.forEach(t),Hio=r(gCe," (ViTMAE model)"),gCe.forEach(t),Uio=i(C),Up=n(C,"LI",{});var hCe=s(Up);DK=n(hCe,"STRONG",{});var nNr=s(DK);Jio=r(nNr,"wav2vec2"),nNr.forEach(t),Yio=r(hCe," \u2014 "),TS=n(hCe,"A",{href:!0});var sNr=s(TS);Kio=r(sNr,"Wav2Vec2Model"),sNr.forEach(t),Zio=r(hCe," (Wav2Vec2 model)"),hCe.forEach(t),edo=i(C),Jp=n(C,"LI",{});var pCe=s(Jp);jK=n(pCe,"STRONG",{});var lNr=s(jK);odo=r(lNr,"wavlm"),lNr.forEach(t),rdo=r(pCe," \u2014 "),FS=n(pCe,"A",{href:!0});var iNr=s(FS);tdo=r(iNr,"WavLMModel"),iNr.forEach(t),ado=r(pCe," (WavLM model)"),pCe.forEach(t),ndo=i(C),Yp=n(C,"LI",{});var _Ce=s(Yp);NK=n(_Ce,"STRONG",{});var dNr=s(NK);sdo=r(dNr,"xglm"),dNr.forEach(t),ldo=r(_Ce," \u2014 "),CS=n(_Ce,"A",{href:!0});var cNr=s(CS);ido=r(cNr,"XGLMModel"),cNr.forEach(t),ddo=r(_Ce," (XGLM model)"),_Ce.forEach(t),cdo=i(C),Kp=n(C,"LI",{});var uCe=s(Kp);qK=n(uCe,"STRONG",{});var fNr=s(qK);fdo=r(fNr,"xlm"),fNr.forEach(t),mdo=r(uCe," \u2014 "),MS=n(uCe,"A",{href:!0});var mNr=s(MS);gdo=r(mNr,"XLMModel"),mNr.forEach(t),hdo=r(uCe," (XLM model)"),uCe.forEach(t),pdo=i(C),Zp=n(C,"LI",{});var bCe=s(Zp);GK=n(bCe,"STRONG",{});var gNr=s(GK);_do=r(gNr,"xlm-prophetnet"),gNr.forEach(t),udo=r(bCe," \u2014 "),ES=n(bCe,"A",{href:!0});var hNr=s(ES);bdo=r(hNr,"XLMProphetNetModel"),hNr.forEach(t),vdo=r(bCe," (XLMProphetNet model)"),bCe.forEach(t),Tdo=i(C),e_=n(C,"LI",{});var vCe=s(e_);OK=n(vCe,"STRONG",{});var pNr=s(OK);Fdo=r(pNr,"xlm-roberta"),pNr.forEach(t),Cdo=r(vCe," \u2014 "),yS=n(vCe,"A",{href:!0});var _Nr=s(yS);Mdo=r(_Nr,"XLMRobertaModel"),_Nr.forEach(t),Edo=r(vCe," (XLM-RoBERTa model)"),vCe.forEach(t),ydo=i(C),o_=n(C,"LI",{});var TCe=s(o_);XK=n(TCe,"STRONG",{});var uNr=s(XK);wdo=r(uNr,"xlm-roberta-xl"),uNr.forEach(t),Ado=r(TCe," \u2014 "),wS=n(TCe,"A",{href:!0});var bNr=s(wS);Ldo=r(bNr,"XLMRobertaXLModel"),bNr.forEach(t),Bdo=r(TCe," (XLM-RoBERTa-XL model)"),TCe.forEach(t),xdo=i(C),r_=n(C,"LI",{});var FCe=s(r_);VK=n(FCe,"STRONG",{});var vNr=s(VK);kdo=r(vNr,"xlnet"),vNr.forEach(t),Rdo=r(FCe," \u2014 "),AS=n(FCe,"A",{href:!0});var TNr=s(AS);Sdo=r(TNr,"XLNetModel"),TNr.forEach(t),Pdo=r(FCe," (XLNet model)"),FCe.forEach(t),$do=i(C),t_=n(C,"LI",{});var CCe=s(t_);zK=n(CCe,"STRONG",{});var FNr=s(zK);Ido=r(FNr,"yoso"),FNr.forEach(t),Ddo=r(CCe," \u2014 "),LS=n(CCe,"A",{href:!0});var CNr=s(LS);jdo=r(CNr,"YosoModel"),CNr.forEach(t),Ndo=r(CCe," (YOSO model)"),CCe.forEach(t),C.forEach(t),qdo=i(Dt),a_=n(Dt,"P",{});var MCe=s(a_);Gdo=r(MCe,"The model is set in evaluation mode by default using "),WK=n(MCe,"CODE",{});var MNr=s(WK);Odo=r(MNr,"model.eval()"),MNr.forEach(t),Xdo=r(MCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),QK=n(MCe,"CODE",{});var ENr=s(QK);Vdo=r(ENr,"model.train()"),ENr.forEach(t),MCe.forEach(t),zdo=i(Dt),HK=n(Dt,"P",{});var yNr=s(HK);Wdo=r(yNr,"Examples:"),yNr.forEach(t),Qdo=i(Dt),m(_E.$$.fragment,Dt),Dt.forEach(t),Gs.forEach(t),vBe=i(c),Wi=n(c,"H2",{class:!0});var wke=s(Wi);n_=n(wke,"A",{id:!0,class:!0,href:!0});var wNr=s(n_);UK=n(wNr,"SPAN",{});var ANr=s(UK);m(uE.$$.fragment,ANr),ANr.forEach(t),wNr.forEach(t),Hdo=i(wke),JK=n(wke,"SPAN",{});var LNr=s(JK);Udo=r(LNr,"AutoModelForPreTraining"),LNr.forEach(t),wke.forEach(t),TBe=i(c),Jo=n(c,"DIV",{class:!0});var Xs=s(Jo);m(bE.$$.fragment,Xs),Jdo=i(Xs),Qi=n(Xs,"P",{});var ZV=s(Qi);Ydo=r(ZV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),YK=n(ZV,"CODE",{});var BNr=s(YK);Kdo=r(BNr,"from_pretrained()"),BNr.forEach(t),Zdo=r(ZV,"class method or the "),KK=n(ZV,"CODE",{});var xNr=s(KK);eco=r(xNr,"from_config()"),xNr.forEach(t),oco=r(ZV,`class
method.`),ZV.forEach(t),rco=i(Xs),vE=n(Xs,"P",{});var Ake=s(vE);tco=r(Ake,"This class cannot be instantiated directly using "),ZK=n(Ake,"CODE",{});var kNr=s(ZK);aco=r(kNr,"__init__()"),kNr.forEach(t),nco=r(Ake," (throws an error)."),Ake.forEach(t),sco=i(Xs),Xr=n(Xs,"DIV",{class:!0});var Vs=s(Xr);m(TE.$$.fragment,Vs),lco=i(Vs),eZ=n(Vs,"P",{});var RNr=s(eZ);ico=r(RNr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),RNr.forEach(t),dco=i(Vs),Hi=n(Vs,"P",{});var ez=s(Hi);cco=r(ez,`Note:
Loading a model from its configuration file does `),oZ=n(ez,"STRONG",{});var SNr=s(oZ);fco=r(SNr,"not"),SNr.forEach(t),mco=r(ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),rZ=n(ez,"CODE",{});var PNr=s(rZ);gco=r(PNr,"from_pretrained()"),PNr.forEach(t),hco=r(ez,"to load the model weights."),ez.forEach(t),pco=i(Vs),tZ=n(Vs,"P",{});var $Nr=s(tZ);_co=r($Nr,"Examples:"),$Nr.forEach(t),uco=i(Vs),m(FE.$$.fragment,Vs),Vs.forEach(t),bco=i(Xs),je=n(Xs,"DIV",{class:!0});var jt=s(je);m(CE.$$.fragment,jt),vco=i(jt),aZ=n(jt,"P",{});var INr=s(aZ);Tco=r(INr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),INr.forEach(t),Fco=i(jt),qa=n(jt,"P",{});var $M=s(qa);Cco=r($M,"The model class to instantiate is selected based on the "),nZ=n($M,"CODE",{});var DNr=s(nZ);Mco=r(DNr,"model_type"),DNr.forEach(t),Eco=r($M,` property of the config object (either
passed as an argument or loaded from `),sZ=n($M,"CODE",{});var jNr=s(sZ);yco=r(jNr,"pretrained_model_name_or_path"),jNr.forEach(t),wco=r($M,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lZ=n($M,"CODE",{});var NNr=s(lZ);Aco=r(NNr,"pretrained_model_name_or_path"),NNr.forEach(t),Lco=r($M,":"),$M.forEach(t),Bco=i(jt),k=n(jt,"UL",{});var S=s(k);s_=n(S,"LI",{});var ECe=s(s_);iZ=n(ECe,"STRONG",{});var qNr=s(iZ);xco=r(qNr,"albert"),qNr.forEach(t),kco=r(ECe," \u2014 "),BS=n(ECe,"A",{href:!0});var GNr=s(BS);Rco=r(GNr,"AlbertForPreTraining"),GNr.forEach(t),Sco=r(ECe," (ALBERT model)"),ECe.forEach(t),Pco=i(S),l_=n(S,"LI",{});var yCe=s(l_);dZ=n(yCe,"STRONG",{});var ONr=s(dZ);$co=r(ONr,"bart"),ONr.forEach(t),Ico=r(yCe," \u2014 "),xS=n(yCe,"A",{href:!0});var XNr=s(xS);Dco=r(XNr,"BartForConditionalGeneration"),XNr.forEach(t),jco=r(yCe," (BART model)"),yCe.forEach(t),Nco=i(S),i_=n(S,"LI",{});var wCe=s(i_);cZ=n(wCe,"STRONG",{});var VNr=s(cZ);qco=r(VNr,"bert"),VNr.forEach(t),Gco=r(wCe," \u2014 "),kS=n(wCe,"A",{href:!0});var zNr=s(kS);Oco=r(zNr,"BertForPreTraining"),zNr.forEach(t),Xco=r(wCe," (BERT model)"),wCe.forEach(t),Vco=i(S),d_=n(S,"LI",{});var ACe=s(d_);fZ=n(ACe,"STRONG",{});var WNr=s(fZ);zco=r(WNr,"big_bird"),WNr.forEach(t),Wco=r(ACe," \u2014 "),RS=n(ACe,"A",{href:!0});var QNr=s(RS);Qco=r(QNr,"BigBirdForPreTraining"),QNr.forEach(t),Hco=r(ACe," (BigBird model)"),ACe.forEach(t),Uco=i(S),c_=n(S,"LI",{});var LCe=s(c_);mZ=n(LCe,"STRONG",{});var HNr=s(mZ);Jco=r(HNr,"camembert"),HNr.forEach(t),Yco=r(LCe," \u2014 "),SS=n(LCe,"A",{href:!0});var UNr=s(SS);Kco=r(UNr,"CamembertForMaskedLM"),UNr.forEach(t),Zco=r(LCe," (CamemBERT model)"),LCe.forEach(t),efo=i(S),f_=n(S,"LI",{});var BCe=s(f_);gZ=n(BCe,"STRONG",{});var JNr=s(gZ);ofo=r(JNr,"ctrl"),JNr.forEach(t),rfo=r(BCe," \u2014 "),PS=n(BCe,"A",{href:!0});var YNr=s(PS);tfo=r(YNr,"CTRLLMHeadModel"),YNr.forEach(t),afo=r(BCe," (CTRL model)"),BCe.forEach(t),nfo=i(S),m_=n(S,"LI",{});var xCe=s(m_);hZ=n(xCe,"STRONG",{});var KNr=s(hZ);sfo=r(KNr,"data2vec-text"),KNr.forEach(t),lfo=r(xCe," \u2014 "),$S=n(xCe,"A",{href:!0});var ZNr=s($S);ifo=r(ZNr,"Data2VecTextForMaskedLM"),ZNr.forEach(t),dfo=r(xCe," (Data2VecText model)"),xCe.forEach(t),cfo=i(S),g_=n(S,"LI",{});var kCe=s(g_);pZ=n(kCe,"STRONG",{});var eqr=s(pZ);ffo=r(eqr,"deberta"),eqr.forEach(t),mfo=r(kCe," \u2014 "),IS=n(kCe,"A",{href:!0});var oqr=s(IS);gfo=r(oqr,"DebertaForMaskedLM"),oqr.forEach(t),hfo=r(kCe," (DeBERTa model)"),kCe.forEach(t),pfo=i(S),h_=n(S,"LI",{});var RCe=s(h_);_Z=n(RCe,"STRONG",{});var rqr=s(_Z);_fo=r(rqr,"deberta-v2"),rqr.forEach(t),ufo=r(RCe," \u2014 "),DS=n(RCe,"A",{href:!0});var tqr=s(DS);bfo=r(tqr,"DebertaV2ForMaskedLM"),tqr.forEach(t),vfo=r(RCe," (DeBERTa-v2 model)"),RCe.forEach(t),Tfo=i(S),p_=n(S,"LI",{});var SCe=s(p_);uZ=n(SCe,"STRONG",{});var aqr=s(uZ);Ffo=r(aqr,"distilbert"),aqr.forEach(t),Cfo=r(SCe," \u2014 "),jS=n(SCe,"A",{href:!0});var nqr=s(jS);Mfo=r(nqr,"DistilBertForMaskedLM"),nqr.forEach(t),Efo=r(SCe," (DistilBERT model)"),SCe.forEach(t),yfo=i(S),__=n(S,"LI",{});var PCe=s(__);bZ=n(PCe,"STRONG",{});var sqr=s(bZ);wfo=r(sqr,"electra"),sqr.forEach(t),Afo=r(PCe," \u2014 "),NS=n(PCe,"A",{href:!0});var lqr=s(NS);Lfo=r(lqr,"ElectraForPreTraining"),lqr.forEach(t),Bfo=r(PCe," (ELECTRA model)"),PCe.forEach(t),xfo=i(S),u_=n(S,"LI",{});var $Ce=s(u_);vZ=n($Ce,"STRONG",{});var iqr=s(vZ);kfo=r(iqr,"flaubert"),iqr.forEach(t),Rfo=r($Ce," \u2014 "),qS=n($Ce,"A",{href:!0});var dqr=s(qS);Sfo=r(dqr,"FlaubertWithLMHeadModel"),dqr.forEach(t),Pfo=r($Ce," (FlauBERT model)"),$Ce.forEach(t),$fo=i(S),b_=n(S,"LI",{});var ICe=s(b_);TZ=n(ICe,"STRONG",{});var cqr=s(TZ);Ifo=r(cqr,"fnet"),cqr.forEach(t),Dfo=r(ICe," \u2014 "),GS=n(ICe,"A",{href:!0});var fqr=s(GS);jfo=r(fqr,"FNetForPreTraining"),fqr.forEach(t),Nfo=r(ICe," (FNet model)"),ICe.forEach(t),qfo=i(S),v_=n(S,"LI",{});var DCe=s(v_);FZ=n(DCe,"STRONG",{});var mqr=s(FZ);Gfo=r(mqr,"fsmt"),mqr.forEach(t),Ofo=r(DCe," \u2014 "),OS=n(DCe,"A",{href:!0});var gqr=s(OS);Xfo=r(gqr,"FSMTForConditionalGeneration"),gqr.forEach(t),Vfo=r(DCe," (FairSeq Machine-Translation model)"),DCe.forEach(t),zfo=i(S),T_=n(S,"LI",{});var jCe=s(T_);CZ=n(jCe,"STRONG",{});var hqr=s(CZ);Wfo=r(hqr,"funnel"),hqr.forEach(t),Qfo=r(jCe," \u2014 "),XS=n(jCe,"A",{href:!0});var pqr=s(XS);Hfo=r(pqr,"FunnelForPreTraining"),pqr.forEach(t),Ufo=r(jCe," (Funnel Transformer model)"),jCe.forEach(t),Jfo=i(S),F_=n(S,"LI",{});var NCe=s(F_);MZ=n(NCe,"STRONG",{});var _qr=s(MZ);Yfo=r(_qr,"gpt2"),_qr.forEach(t),Kfo=r(NCe," \u2014 "),VS=n(NCe,"A",{href:!0});var uqr=s(VS);Zfo=r(uqr,"GPT2LMHeadModel"),uqr.forEach(t),emo=r(NCe," (OpenAI GPT-2 model)"),NCe.forEach(t),omo=i(S),C_=n(S,"LI",{});var qCe=s(C_);EZ=n(qCe,"STRONG",{});var bqr=s(EZ);rmo=r(bqr,"ibert"),bqr.forEach(t),tmo=r(qCe," \u2014 "),zS=n(qCe,"A",{href:!0});var vqr=s(zS);amo=r(vqr,"IBertForMaskedLM"),vqr.forEach(t),nmo=r(qCe," (I-BERT model)"),qCe.forEach(t),smo=i(S),M_=n(S,"LI",{});var GCe=s(M_);yZ=n(GCe,"STRONG",{});var Tqr=s(yZ);lmo=r(Tqr,"layoutlm"),Tqr.forEach(t),imo=r(GCe," \u2014 "),WS=n(GCe,"A",{href:!0});var Fqr=s(WS);dmo=r(Fqr,"LayoutLMForMaskedLM"),Fqr.forEach(t),cmo=r(GCe," (LayoutLM model)"),GCe.forEach(t),fmo=i(S),E_=n(S,"LI",{});var OCe=s(E_);wZ=n(OCe,"STRONG",{});var Cqr=s(wZ);mmo=r(Cqr,"longformer"),Cqr.forEach(t),gmo=r(OCe," \u2014 "),QS=n(OCe,"A",{href:!0});var Mqr=s(QS);hmo=r(Mqr,"LongformerForMaskedLM"),Mqr.forEach(t),pmo=r(OCe," (Longformer model)"),OCe.forEach(t),_mo=i(S),y_=n(S,"LI",{});var XCe=s(y_);AZ=n(XCe,"STRONG",{});var Eqr=s(AZ);umo=r(Eqr,"lxmert"),Eqr.forEach(t),bmo=r(XCe," \u2014 "),HS=n(XCe,"A",{href:!0});var yqr=s(HS);vmo=r(yqr,"LxmertForPreTraining"),yqr.forEach(t),Tmo=r(XCe," (LXMERT model)"),XCe.forEach(t),Fmo=i(S),w_=n(S,"LI",{});var VCe=s(w_);LZ=n(VCe,"STRONG",{});var wqr=s(LZ);Cmo=r(wqr,"megatron-bert"),wqr.forEach(t),Mmo=r(VCe," \u2014 "),US=n(VCe,"A",{href:!0});var Aqr=s(US);Emo=r(Aqr,"MegatronBertForPreTraining"),Aqr.forEach(t),ymo=r(VCe," (MegatronBert model)"),VCe.forEach(t),wmo=i(S),A_=n(S,"LI",{});var zCe=s(A_);BZ=n(zCe,"STRONG",{});var Lqr=s(BZ);Amo=r(Lqr,"mobilebert"),Lqr.forEach(t),Lmo=r(zCe," \u2014 "),JS=n(zCe,"A",{href:!0});var Bqr=s(JS);Bmo=r(Bqr,"MobileBertForPreTraining"),Bqr.forEach(t),xmo=r(zCe," (MobileBERT model)"),zCe.forEach(t),kmo=i(S),L_=n(S,"LI",{});var WCe=s(L_);xZ=n(WCe,"STRONG",{});var xqr=s(xZ);Rmo=r(xqr,"mpnet"),xqr.forEach(t),Smo=r(WCe," \u2014 "),YS=n(WCe,"A",{href:!0});var kqr=s(YS);Pmo=r(kqr,"MPNetForMaskedLM"),kqr.forEach(t),$mo=r(WCe," (MPNet model)"),WCe.forEach(t),Imo=i(S),B_=n(S,"LI",{});var QCe=s(B_);kZ=n(QCe,"STRONG",{});var Rqr=s(kZ);Dmo=r(Rqr,"openai-gpt"),Rqr.forEach(t),jmo=r(QCe," \u2014 "),KS=n(QCe,"A",{href:!0});var Sqr=s(KS);Nmo=r(Sqr,"OpenAIGPTLMHeadModel"),Sqr.forEach(t),qmo=r(QCe," (OpenAI GPT model)"),QCe.forEach(t),Gmo=i(S),x_=n(S,"LI",{});var HCe=s(x_);RZ=n(HCe,"STRONG",{});var Pqr=s(RZ);Omo=r(Pqr,"retribert"),Pqr.forEach(t),Xmo=r(HCe," \u2014 "),ZS=n(HCe,"A",{href:!0});var $qr=s(ZS);Vmo=r($qr,"RetriBertModel"),$qr.forEach(t),zmo=r(HCe," (RetriBERT model)"),HCe.forEach(t),Wmo=i(S),k_=n(S,"LI",{});var UCe=s(k_);SZ=n(UCe,"STRONG",{});var Iqr=s(SZ);Qmo=r(Iqr,"roberta"),Iqr.forEach(t),Hmo=r(UCe," \u2014 "),eP=n(UCe,"A",{href:!0});var Dqr=s(eP);Umo=r(Dqr,"RobertaForMaskedLM"),Dqr.forEach(t),Jmo=r(UCe," (RoBERTa model)"),UCe.forEach(t),Ymo=i(S),R_=n(S,"LI",{});var JCe=s(R_);PZ=n(JCe,"STRONG",{});var jqr=s(PZ);Kmo=r(jqr,"squeezebert"),jqr.forEach(t),Zmo=r(JCe," \u2014 "),oP=n(JCe,"A",{href:!0});var Nqr=s(oP);ego=r(Nqr,"SqueezeBertForMaskedLM"),Nqr.forEach(t),ogo=r(JCe," (SqueezeBERT model)"),JCe.forEach(t),rgo=i(S),S_=n(S,"LI",{});var YCe=s(S_);$Z=n(YCe,"STRONG",{});var qqr=s($Z);tgo=r(qqr,"t5"),qqr.forEach(t),ago=r(YCe," \u2014 "),rP=n(YCe,"A",{href:!0});var Gqr=s(rP);ngo=r(Gqr,"T5ForConditionalGeneration"),Gqr.forEach(t),sgo=r(YCe," (T5 model)"),YCe.forEach(t),lgo=i(S),P_=n(S,"LI",{});var KCe=s(P_);IZ=n(KCe,"STRONG",{});var Oqr=s(IZ);igo=r(Oqr,"tapas"),Oqr.forEach(t),dgo=r(KCe," \u2014 "),tP=n(KCe,"A",{href:!0});var Xqr=s(tP);cgo=r(Xqr,"TapasForMaskedLM"),Xqr.forEach(t),fgo=r(KCe," (TAPAS model)"),KCe.forEach(t),mgo=i(S),$_=n(S,"LI",{});var ZCe=s($_);DZ=n(ZCe,"STRONG",{});var Vqr=s(DZ);ggo=r(Vqr,"transfo-xl"),Vqr.forEach(t),hgo=r(ZCe," \u2014 "),aP=n(ZCe,"A",{href:!0});var zqr=s(aP);pgo=r(zqr,"TransfoXLLMHeadModel"),zqr.forEach(t),_go=r(ZCe," (Transformer-XL model)"),ZCe.forEach(t),ugo=i(S),I_=n(S,"LI",{});var eMe=s(I_);jZ=n(eMe,"STRONG",{});var Wqr=s(jZ);bgo=r(Wqr,"unispeech"),Wqr.forEach(t),vgo=r(eMe," \u2014 "),nP=n(eMe,"A",{href:!0});var Qqr=s(nP);Tgo=r(Qqr,"UniSpeechForPreTraining"),Qqr.forEach(t),Fgo=r(eMe," (UniSpeech model)"),eMe.forEach(t),Cgo=i(S),D_=n(S,"LI",{});var oMe=s(D_);NZ=n(oMe,"STRONG",{});var Hqr=s(NZ);Mgo=r(Hqr,"unispeech-sat"),Hqr.forEach(t),Ego=r(oMe," \u2014 "),sP=n(oMe,"A",{href:!0});var Uqr=s(sP);ygo=r(Uqr,"UniSpeechSatForPreTraining"),Uqr.forEach(t),wgo=r(oMe," (UniSpeechSat model)"),oMe.forEach(t),Ago=i(S),j_=n(S,"LI",{});var rMe=s(j_);qZ=n(rMe,"STRONG",{});var Jqr=s(qZ);Lgo=r(Jqr,"visual_bert"),Jqr.forEach(t),Bgo=r(rMe," \u2014 "),lP=n(rMe,"A",{href:!0});var Yqr=s(lP);xgo=r(Yqr,"VisualBertForPreTraining"),Yqr.forEach(t),kgo=r(rMe," (VisualBert model)"),rMe.forEach(t),Rgo=i(S),N_=n(S,"LI",{});var tMe=s(N_);GZ=n(tMe,"STRONG",{});var Kqr=s(GZ);Sgo=r(Kqr,"vit_mae"),Kqr.forEach(t),Pgo=r(tMe," \u2014 "),iP=n(tMe,"A",{href:!0});var Zqr=s(iP);$go=r(Zqr,"ViTMAEForPreTraining"),Zqr.forEach(t),Igo=r(tMe," (ViTMAE model)"),tMe.forEach(t),Dgo=i(S),q_=n(S,"LI",{});var aMe=s(q_);OZ=n(aMe,"STRONG",{});var eGr=s(OZ);jgo=r(eGr,"wav2vec2"),eGr.forEach(t),Ngo=r(aMe," \u2014 "),dP=n(aMe,"A",{href:!0});var oGr=s(dP);qgo=r(oGr,"Wav2Vec2ForPreTraining"),oGr.forEach(t),Ggo=r(aMe," (Wav2Vec2 model)"),aMe.forEach(t),Ogo=i(S),G_=n(S,"LI",{});var nMe=s(G_);XZ=n(nMe,"STRONG",{});var rGr=s(XZ);Xgo=r(rGr,"xlm"),rGr.forEach(t),Vgo=r(nMe," \u2014 "),cP=n(nMe,"A",{href:!0});var tGr=s(cP);zgo=r(tGr,"XLMWithLMHeadModel"),tGr.forEach(t),Wgo=r(nMe," (XLM model)"),nMe.forEach(t),Qgo=i(S),O_=n(S,"LI",{});var sMe=s(O_);VZ=n(sMe,"STRONG",{});var aGr=s(VZ);Hgo=r(aGr,"xlm-roberta"),aGr.forEach(t),Ugo=r(sMe," \u2014 "),fP=n(sMe,"A",{href:!0});var nGr=s(fP);Jgo=r(nGr,"XLMRobertaForMaskedLM"),nGr.forEach(t),Ygo=r(sMe," (XLM-RoBERTa model)"),sMe.forEach(t),Kgo=i(S),X_=n(S,"LI",{});var lMe=s(X_);zZ=n(lMe,"STRONG",{});var sGr=s(zZ);Zgo=r(sGr,"xlm-roberta-xl"),sGr.forEach(t),eho=r(lMe," \u2014 "),mP=n(lMe,"A",{href:!0});var lGr=s(mP);oho=r(lGr,"XLMRobertaXLForMaskedLM"),lGr.forEach(t),rho=r(lMe," (XLM-RoBERTa-XL model)"),lMe.forEach(t),tho=i(S),V_=n(S,"LI",{});var iMe=s(V_);WZ=n(iMe,"STRONG",{});var iGr=s(WZ);aho=r(iGr,"xlnet"),iGr.forEach(t),nho=r(iMe," \u2014 "),gP=n(iMe,"A",{href:!0});var dGr=s(gP);sho=r(dGr,"XLNetLMHeadModel"),dGr.forEach(t),lho=r(iMe," (XLNet model)"),iMe.forEach(t),S.forEach(t),iho=i(jt),z_=n(jt,"P",{});var dMe=s(z_);dho=r(dMe,"The model is set in evaluation mode by default using "),QZ=n(dMe,"CODE",{});var cGr=s(QZ);cho=r(cGr,"model.eval()"),cGr.forEach(t),fho=r(dMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HZ=n(dMe,"CODE",{});var fGr=s(HZ);mho=r(fGr,"model.train()"),fGr.forEach(t),dMe.forEach(t),gho=i(jt),UZ=n(jt,"P",{});var mGr=s(UZ);hho=r(mGr,"Examples:"),mGr.forEach(t),pho=i(jt),m(ME.$$.fragment,jt),jt.forEach(t),Xs.forEach(t),FBe=i(c),Ui=n(c,"H2",{class:!0});var Lke=s(Ui);W_=n(Lke,"A",{id:!0,class:!0,href:!0});var gGr=s(W_);JZ=n(gGr,"SPAN",{});var hGr=s(JZ);m(EE.$$.fragment,hGr),hGr.forEach(t),gGr.forEach(t),_ho=i(Lke),YZ=n(Lke,"SPAN",{});var pGr=s(YZ);uho=r(pGr,"AutoModelForCausalLM"),pGr.forEach(t),Lke.forEach(t),CBe=i(c),Yo=n(c,"DIV",{class:!0});var zs=s(Yo);m(yE.$$.fragment,zs),bho=i(zs),Ji=n(zs,"P",{});var oz=s(Ji);vho=r(oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),KZ=n(oz,"CODE",{});var _Gr=s(KZ);Tho=r(_Gr,"from_pretrained()"),_Gr.forEach(t),Fho=r(oz,"class method or the "),ZZ=n(oz,"CODE",{});var uGr=s(ZZ);Cho=r(uGr,"from_config()"),uGr.forEach(t),Mho=r(oz,`class
method.`),oz.forEach(t),Eho=i(zs),wE=n(zs,"P",{});var Bke=s(wE);yho=r(Bke,"This class cannot be instantiated directly using "),eee=n(Bke,"CODE",{});var bGr=s(eee);who=r(bGr,"__init__()"),bGr.forEach(t),Aho=r(Bke," (throws an error)."),Bke.forEach(t),Lho=i(zs),Vr=n(zs,"DIV",{class:!0});var Ws=s(Vr);m(AE.$$.fragment,Ws),Bho=i(Ws),oee=n(Ws,"P",{});var vGr=s(oee);xho=r(vGr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),vGr.forEach(t),kho=i(Ws),Yi=n(Ws,"P",{});var rz=s(Yi);Rho=r(rz,`Note:
Loading a model from its configuration file does `),ree=n(rz,"STRONG",{});var TGr=s(ree);Sho=r(TGr,"not"),TGr.forEach(t),Pho=r(rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tee=n(rz,"CODE",{});var FGr=s(tee);$ho=r(FGr,"from_pretrained()"),FGr.forEach(t),Iho=r(rz,"to load the model weights."),rz.forEach(t),Dho=i(Ws),aee=n(Ws,"P",{});var CGr=s(aee);jho=r(CGr,"Examples:"),CGr.forEach(t),Nho=i(Ws),m(LE.$$.fragment,Ws),Ws.forEach(t),qho=i(zs),Ne=n(zs,"DIV",{class:!0});var Nt=s(Ne);m(BE.$$.fragment,Nt),Gho=i(Nt),nee=n(Nt,"P",{});var MGr=s(nee);Oho=r(MGr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),MGr.forEach(t),Xho=i(Nt),Ga=n(Nt,"P",{});var IM=s(Ga);Vho=r(IM,"The model class to instantiate is selected based on the "),see=n(IM,"CODE",{});var EGr=s(see);zho=r(EGr,"model_type"),EGr.forEach(t),Who=r(IM,` property of the config object (either
passed as an argument or loaded from `),lee=n(IM,"CODE",{});var yGr=s(lee);Qho=r(yGr,"pretrained_model_name_or_path"),yGr.forEach(t),Hho=r(IM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iee=n(IM,"CODE",{});var wGr=s(iee);Uho=r(wGr,"pretrained_model_name_or_path"),wGr.forEach(t),Jho=r(IM,":"),IM.forEach(t),Yho=i(Nt),$=n(Nt,"UL",{});var D=s($);Q_=n(D,"LI",{});var cMe=s(Q_);dee=n(cMe,"STRONG",{});var AGr=s(dee);Kho=r(AGr,"bart"),AGr.forEach(t),Zho=r(cMe," \u2014 "),hP=n(cMe,"A",{href:!0});var LGr=s(hP);epo=r(LGr,"BartForCausalLM"),LGr.forEach(t),opo=r(cMe," (BART model)"),cMe.forEach(t),rpo=i(D),H_=n(D,"LI",{});var fMe=s(H_);cee=n(fMe,"STRONG",{});var BGr=s(cee);tpo=r(BGr,"bert"),BGr.forEach(t),apo=r(fMe," \u2014 "),pP=n(fMe,"A",{href:!0});var xGr=s(pP);npo=r(xGr,"BertLMHeadModel"),xGr.forEach(t),spo=r(fMe," (BERT model)"),fMe.forEach(t),lpo=i(D),U_=n(D,"LI",{});var mMe=s(U_);fee=n(mMe,"STRONG",{});var kGr=s(fee);ipo=r(kGr,"bert-generation"),kGr.forEach(t),dpo=r(mMe," \u2014 "),_P=n(mMe,"A",{href:!0});var RGr=s(_P);cpo=r(RGr,"BertGenerationDecoder"),RGr.forEach(t),fpo=r(mMe," (Bert Generation model)"),mMe.forEach(t),mpo=i(D),J_=n(D,"LI",{});var gMe=s(J_);mee=n(gMe,"STRONG",{});var SGr=s(mee);gpo=r(SGr,"big_bird"),SGr.forEach(t),hpo=r(gMe," \u2014 "),uP=n(gMe,"A",{href:!0});var PGr=s(uP);ppo=r(PGr,"BigBirdForCausalLM"),PGr.forEach(t),_po=r(gMe," (BigBird model)"),gMe.forEach(t),upo=i(D),Y_=n(D,"LI",{});var hMe=s(Y_);gee=n(hMe,"STRONG",{});var $Gr=s(gee);bpo=r($Gr,"bigbird_pegasus"),$Gr.forEach(t),vpo=r(hMe," \u2014 "),bP=n(hMe,"A",{href:!0});var IGr=s(bP);Tpo=r(IGr,"BigBirdPegasusForCausalLM"),IGr.forEach(t),Fpo=r(hMe," (BigBirdPegasus model)"),hMe.forEach(t),Cpo=i(D),K_=n(D,"LI",{});var pMe=s(K_);hee=n(pMe,"STRONG",{});var DGr=s(hee);Mpo=r(DGr,"blenderbot"),DGr.forEach(t),Epo=r(pMe," \u2014 "),vP=n(pMe,"A",{href:!0});var jGr=s(vP);ypo=r(jGr,"BlenderbotForCausalLM"),jGr.forEach(t),wpo=r(pMe," (Blenderbot model)"),pMe.forEach(t),Apo=i(D),Z_=n(D,"LI",{});var _Me=s(Z_);pee=n(_Me,"STRONG",{});var NGr=s(pee);Lpo=r(NGr,"blenderbot-small"),NGr.forEach(t),Bpo=r(_Me," \u2014 "),TP=n(_Me,"A",{href:!0});var qGr=s(TP);xpo=r(qGr,"BlenderbotSmallForCausalLM"),qGr.forEach(t),kpo=r(_Me," (BlenderbotSmall model)"),_Me.forEach(t),Rpo=i(D),eu=n(D,"LI",{});var uMe=s(eu);_ee=n(uMe,"STRONG",{});var GGr=s(_ee);Spo=r(GGr,"camembert"),GGr.forEach(t),Ppo=r(uMe," \u2014 "),FP=n(uMe,"A",{href:!0});var OGr=s(FP);$po=r(OGr,"CamembertForCausalLM"),OGr.forEach(t),Ipo=r(uMe," (CamemBERT model)"),uMe.forEach(t),Dpo=i(D),ou=n(D,"LI",{});var bMe=s(ou);uee=n(bMe,"STRONG",{});var XGr=s(uee);jpo=r(XGr,"ctrl"),XGr.forEach(t),Npo=r(bMe," \u2014 "),CP=n(bMe,"A",{href:!0});var VGr=s(CP);qpo=r(VGr,"CTRLLMHeadModel"),VGr.forEach(t),Gpo=r(bMe," (CTRL model)"),bMe.forEach(t),Opo=i(D),ru=n(D,"LI",{});var vMe=s(ru);bee=n(vMe,"STRONG",{});var zGr=s(bee);Xpo=r(zGr,"data2vec-text"),zGr.forEach(t),Vpo=r(vMe," \u2014 "),MP=n(vMe,"A",{href:!0});var WGr=s(MP);zpo=r(WGr,"Data2VecTextForCausalLM"),WGr.forEach(t),Wpo=r(vMe," (Data2VecText model)"),vMe.forEach(t),Qpo=i(D),tu=n(D,"LI",{});var TMe=s(tu);vee=n(TMe,"STRONG",{});var QGr=s(vee);Hpo=r(QGr,"electra"),QGr.forEach(t),Upo=r(TMe," \u2014 "),EP=n(TMe,"A",{href:!0});var HGr=s(EP);Jpo=r(HGr,"ElectraForCausalLM"),HGr.forEach(t),Ypo=r(TMe," (ELECTRA model)"),TMe.forEach(t),Kpo=i(D),au=n(D,"LI",{});var FMe=s(au);Tee=n(FMe,"STRONG",{});var UGr=s(Tee);Zpo=r(UGr,"gpt2"),UGr.forEach(t),e_o=r(FMe," \u2014 "),yP=n(FMe,"A",{href:!0});var JGr=s(yP);o_o=r(JGr,"GPT2LMHeadModel"),JGr.forEach(t),r_o=r(FMe," (OpenAI GPT-2 model)"),FMe.forEach(t),t_o=i(D),nu=n(D,"LI",{});var CMe=s(nu);Fee=n(CMe,"STRONG",{});var YGr=s(Fee);a_o=r(YGr,"gpt_neo"),YGr.forEach(t),n_o=r(CMe," \u2014 "),wP=n(CMe,"A",{href:!0});var KGr=s(wP);s_o=r(KGr,"GPTNeoForCausalLM"),KGr.forEach(t),l_o=r(CMe," (GPT Neo model)"),CMe.forEach(t),i_o=i(D),su=n(D,"LI",{});var MMe=s(su);Cee=n(MMe,"STRONG",{});var ZGr=s(Cee);d_o=r(ZGr,"gptj"),ZGr.forEach(t),c_o=r(MMe," \u2014 "),AP=n(MMe,"A",{href:!0});var eOr=s(AP);f_o=r(eOr,"GPTJForCausalLM"),eOr.forEach(t),m_o=r(MMe," (GPT-J model)"),MMe.forEach(t),g_o=i(D),lu=n(D,"LI",{});var EMe=s(lu);Mee=n(EMe,"STRONG",{});var oOr=s(Mee);h_o=r(oOr,"marian"),oOr.forEach(t),p_o=r(EMe," \u2014 "),LP=n(EMe,"A",{href:!0});var rOr=s(LP);__o=r(rOr,"MarianForCausalLM"),rOr.forEach(t),u_o=r(EMe," (Marian model)"),EMe.forEach(t),b_o=i(D),iu=n(D,"LI",{});var yMe=s(iu);Eee=n(yMe,"STRONG",{});var tOr=s(Eee);v_o=r(tOr,"mbart"),tOr.forEach(t),T_o=r(yMe," \u2014 "),BP=n(yMe,"A",{href:!0});var aOr=s(BP);F_o=r(aOr,"MBartForCausalLM"),aOr.forEach(t),C_o=r(yMe," (mBART model)"),yMe.forEach(t),M_o=i(D),du=n(D,"LI",{});var wMe=s(du);yee=n(wMe,"STRONG",{});var nOr=s(yee);E_o=r(nOr,"megatron-bert"),nOr.forEach(t),y_o=r(wMe," \u2014 "),xP=n(wMe,"A",{href:!0});var sOr=s(xP);w_o=r(sOr,"MegatronBertForCausalLM"),sOr.forEach(t),A_o=r(wMe," (MegatronBert model)"),wMe.forEach(t),L_o=i(D),cu=n(D,"LI",{});var AMe=s(cu);wee=n(AMe,"STRONG",{});var lOr=s(wee);B_o=r(lOr,"openai-gpt"),lOr.forEach(t),x_o=r(AMe," \u2014 "),kP=n(AMe,"A",{href:!0});var iOr=s(kP);k_o=r(iOr,"OpenAIGPTLMHeadModel"),iOr.forEach(t),R_o=r(AMe," (OpenAI GPT model)"),AMe.forEach(t),S_o=i(D),fu=n(D,"LI",{});var LMe=s(fu);Aee=n(LMe,"STRONG",{});var dOr=s(Aee);P_o=r(dOr,"pegasus"),dOr.forEach(t),$_o=r(LMe," \u2014 "),RP=n(LMe,"A",{href:!0});var cOr=s(RP);I_o=r(cOr,"PegasusForCausalLM"),cOr.forEach(t),D_o=r(LMe," (Pegasus model)"),LMe.forEach(t),j_o=i(D),mu=n(D,"LI",{});var BMe=s(mu);Lee=n(BMe,"STRONG",{});var fOr=s(Lee);N_o=r(fOr,"plbart"),fOr.forEach(t),q_o=r(BMe," \u2014 "),SP=n(BMe,"A",{href:!0});var mOr=s(SP);G_o=r(mOr,"PLBartForCausalLM"),mOr.forEach(t),O_o=r(BMe," (PLBart model)"),BMe.forEach(t),X_o=i(D),gu=n(D,"LI",{});var xMe=s(gu);Bee=n(xMe,"STRONG",{});var gOr=s(Bee);V_o=r(gOr,"prophetnet"),gOr.forEach(t),z_o=r(xMe," \u2014 "),PP=n(xMe,"A",{href:!0});var hOr=s(PP);W_o=r(hOr,"ProphetNetForCausalLM"),hOr.forEach(t),Q_o=r(xMe," (ProphetNet model)"),xMe.forEach(t),H_o=i(D),hu=n(D,"LI",{});var kMe=s(hu);xee=n(kMe,"STRONG",{});var pOr=s(xee);U_o=r(pOr,"qdqbert"),pOr.forEach(t),J_o=r(kMe," \u2014 "),$P=n(kMe,"A",{href:!0});var _Or=s($P);Y_o=r(_Or,"QDQBertLMHeadModel"),_Or.forEach(t),K_o=r(kMe," (QDQBert model)"),kMe.forEach(t),Z_o=i(D),pu=n(D,"LI",{});var RMe=s(pu);kee=n(RMe,"STRONG",{});var uOr=s(kee);euo=r(uOr,"reformer"),uOr.forEach(t),ouo=r(RMe," \u2014 "),IP=n(RMe,"A",{href:!0});var bOr=s(IP);ruo=r(bOr,"ReformerModelWithLMHead"),bOr.forEach(t),tuo=r(RMe," (Reformer model)"),RMe.forEach(t),auo=i(D),_u=n(D,"LI",{});var SMe=s(_u);Ree=n(SMe,"STRONG",{});var vOr=s(Ree);nuo=r(vOr,"rembert"),vOr.forEach(t),suo=r(SMe," \u2014 "),DP=n(SMe,"A",{href:!0});var TOr=s(DP);luo=r(TOr,"RemBertForCausalLM"),TOr.forEach(t),iuo=r(SMe," (RemBERT model)"),SMe.forEach(t),duo=i(D),uu=n(D,"LI",{});var PMe=s(uu);See=n(PMe,"STRONG",{});var FOr=s(See);cuo=r(FOr,"roberta"),FOr.forEach(t),fuo=r(PMe," \u2014 "),jP=n(PMe,"A",{href:!0});var COr=s(jP);muo=r(COr,"RobertaForCausalLM"),COr.forEach(t),guo=r(PMe," (RoBERTa model)"),PMe.forEach(t),huo=i(D),bu=n(D,"LI",{});var $Me=s(bu);Pee=n($Me,"STRONG",{});var MOr=s(Pee);puo=r(MOr,"roformer"),MOr.forEach(t),_uo=r($Me," \u2014 "),NP=n($Me,"A",{href:!0});var EOr=s(NP);uuo=r(EOr,"RoFormerForCausalLM"),EOr.forEach(t),buo=r($Me," (RoFormer model)"),$Me.forEach(t),vuo=i(D),vu=n(D,"LI",{});var IMe=s(vu);$ee=n(IMe,"STRONG",{});var yOr=s($ee);Tuo=r(yOr,"speech_to_text_2"),yOr.forEach(t),Fuo=r(IMe," \u2014 "),qP=n(IMe,"A",{href:!0});var wOr=s(qP);Cuo=r(wOr,"Speech2Text2ForCausalLM"),wOr.forEach(t),Muo=r(IMe," (Speech2Text2 model)"),IMe.forEach(t),Euo=i(D),Tu=n(D,"LI",{});var DMe=s(Tu);Iee=n(DMe,"STRONG",{});var AOr=s(Iee);yuo=r(AOr,"transfo-xl"),AOr.forEach(t),wuo=r(DMe," \u2014 "),GP=n(DMe,"A",{href:!0});var LOr=s(GP);Auo=r(LOr,"TransfoXLLMHeadModel"),LOr.forEach(t),Luo=r(DMe," (Transformer-XL model)"),DMe.forEach(t),Buo=i(D),Fu=n(D,"LI",{});var jMe=s(Fu);Dee=n(jMe,"STRONG",{});var BOr=s(Dee);xuo=r(BOr,"trocr"),BOr.forEach(t),kuo=r(jMe," \u2014 "),OP=n(jMe,"A",{href:!0});var xOr=s(OP);Ruo=r(xOr,"TrOCRForCausalLM"),xOr.forEach(t),Suo=r(jMe," (TrOCR model)"),jMe.forEach(t),Puo=i(D),Cu=n(D,"LI",{});var NMe=s(Cu);jee=n(NMe,"STRONG",{});var kOr=s(jee);$uo=r(kOr,"xglm"),kOr.forEach(t),Iuo=r(NMe," \u2014 "),XP=n(NMe,"A",{href:!0});var ROr=s(XP);Duo=r(ROr,"XGLMForCausalLM"),ROr.forEach(t),juo=r(NMe," (XGLM model)"),NMe.forEach(t),Nuo=i(D),Mu=n(D,"LI",{});var qMe=s(Mu);Nee=n(qMe,"STRONG",{});var SOr=s(Nee);quo=r(SOr,"xlm"),SOr.forEach(t),Guo=r(qMe," \u2014 "),VP=n(qMe,"A",{href:!0});var POr=s(VP);Ouo=r(POr,"XLMWithLMHeadModel"),POr.forEach(t),Xuo=r(qMe," (XLM model)"),qMe.forEach(t),Vuo=i(D),Eu=n(D,"LI",{});var GMe=s(Eu);qee=n(GMe,"STRONG",{});var $Or=s(qee);zuo=r($Or,"xlm-prophetnet"),$Or.forEach(t),Wuo=r(GMe," \u2014 "),zP=n(GMe,"A",{href:!0});var IOr=s(zP);Quo=r(IOr,"XLMProphetNetForCausalLM"),IOr.forEach(t),Huo=r(GMe," (XLMProphetNet model)"),GMe.forEach(t),Uuo=i(D),yu=n(D,"LI",{});var OMe=s(yu);Gee=n(OMe,"STRONG",{});var DOr=s(Gee);Juo=r(DOr,"xlm-roberta"),DOr.forEach(t),Yuo=r(OMe," \u2014 "),WP=n(OMe,"A",{href:!0});var jOr=s(WP);Kuo=r(jOr,"XLMRobertaForCausalLM"),jOr.forEach(t),Zuo=r(OMe," (XLM-RoBERTa model)"),OMe.forEach(t),e0o=i(D),wu=n(D,"LI",{});var XMe=s(wu);Oee=n(XMe,"STRONG",{});var NOr=s(Oee);o0o=r(NOr,"xlm-roberta-xl"),NOr.forEach(t),r0o=r(XMe," \u2014 "),QP=n(XMe,"A",{href:!0});var qOr=s(QP);t0o=r(qOr,"XLMRobertaXLForCausalLM"),qOr.forEach(t),a0o=r(XMe," (XLM-RoBERTa-XL model)"),XMe.forEach(t),n0o=i(D),Au=n(D,"LI",{});var VMe=s(Au);Xee=n(VMe,"STRONG",{});var GOr=s(Xee);s0o=r(GOr,"xlnet"),GOr.forEach(t),l0o=r(VMe," \u2014 "),HP=n(VMe,"A",{href:!0});var OOr=s(HP);i0o=r(OOr,"XLNetLMHeadModel"),OOr.forEach(t),d0o=r(VMe," (XLNet model)"),VMe.forEach(t),D.forEach(t),c0o=i(Nt),Lu=n(Nt,"P",{});var zMe=s(Lu);f0o=r(zMe,"The model is set in evaluation mode by default using "),Vee=n(zMe,"CODE",{});var XOr=s(Vee);m0o=r(XOr,"model.eval()"),XOr.forEach(t),g0o=r(zMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zee=n(zMe,"CODE",{});var VOr=s(zee);h0o=r(VOr,"model.train()"),VOr.forEach(t),zMe.forEach(t),p0o=i(Nt),Wee=n(Nt,"P",{});var zOr=s(Wee);_0o=r(zOr,"Examples:"),zOr.forEach(t),u0o=i(Nt),m(xE.$$.fragment,Nt),Nt.forEach(t),zs.forEach(t),MBe=i(c),Ki=n(c,"H2",{class:!0});var xke=s(Ki);Bu=n(xke,"A",{id:!0,class:!0,href:!0});var WOr=s(Bu);Qee=n(WOr,"SPAN",{});var QOr=s(Qee);m(kE.$$.fragment,QOr),QOr.forEach(t),WOr.forEach(t),b0o=i(xke),Hee=n(xke,"SPAN",{});var HOr=s(Hee);v0o=r(HOr,"AutoModelForMaskedLM"),HOr.forEach(t),xke.forEach(t),EBe=i(c),Ko=n(c,"DIV",{class:!0});var Qs=s(Ko);m(RE.$$.fragment,Qs),T0o=i(Qs),Zi=n(Qs,"P",{});var tz=s(Zi);F0o=r(tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Uee=n(tz,"CODE",{});var UOr=s(Uee);C0o=r(UOr,"from_pretrained()"),UOr.forEach(t),M0o=r(tz,"class method or the "),Jee=n(tz,"CODE",{});var JOr=s(Jee);E0o=r(JOr,"from_config()"),JOr.forEach(t),y0o=r(tz,`class
method.`),tz.forEach(t),w0o=i(Qs),SE=n(Qs,"P",{});var kke=s(SE);A0o=r(kke,"This class cannot be instantiated directly using "),Yee=n(kke,"CODE",{});var YOr=s(Yee);L0o=r(YOr,"__init__()"),YOr.forEach(t),B0o=r(kke," (throws an error)."),kke.forEach(t),x0o=i(Qs),zr=n(Qs,"DIV",{class:!0});var Hs=s(zr);m(PE.$$.fragment,Hs),k0o=i(Hs),Kee=n(Hs,"P",{});var KOr=s(Kee);R0o=r(KOr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),KOr.forEach(t),S0o=i(Hs),ed=n(Hs,"P",{});var az=s(ed);P0o=r(az,`Note:
Loading a model from its configuration file does `),Zee=n(az,"STRONG",{});var ZOr=s(Zee);$0o=r(ZOr,"not"),ZOr.forEach(t),I0o=r(az,` load the model weights. It only affects the
model\u2019s configuration. Use `),eoe=n(az,"CODE",{});var eXr=s(eoe);D0o=r(eXr,"from_pretrained()"),eXr.forEach(t),j0o=r(az,"to load the model weights."),az.forEach(t),N0o=i(Hs),ooe=n(Hs,"P",{});var oXr=s(ooe);q0o=r(oXr,"Examples:"),oXr.forEach(t),G0o=i(Hs),m($E.$$.fragment,Hs),Hs.forEach(t),O0o=i(Qs),qe=n(Qs,"DIV",{class:!0});var qt=s(qe);m(IE.$$.fragment,qt),X0o=i(qt),roe=n(qt,"P",{});var rXr=s(roe);V0o=r(rXr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),rXr.forEach(t),z0o=i(qt),Oa=n(qt,"P",{});var DM=s(Oa);W0o=r(DM,"The model class to instantiate is selected based on the "),toe=n(DM,"CODE",{});var tXr=s(toe);Q0o=r(tXr,"model_type"),tXr.forEach(t),H0o=r(DM,` property of the config object (either
passed as an argument or loaded from `),aoe=n(DM,"CODE",{});var aXr=s(aoe);U0o=r(aXr,"pretrained_model_name_or_path"),aXr.forEach(t),J0o=r(DM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),noe=n(DM,"CODE",{});var nXr=s(noe);Y0o=r(nXr,"pretrained_model_name_or_path"),nXr.forEach(t),K0o=r(DM,":"),DM.forEach(t),Z0o=i(qt),I=n(qt,"UL",{});var j=s(I);xu=n(j,"LI",{});var WMe=s(xu);soe=n(WMe,"STRONG",{});var sXr=s(soe);e1o=r(sXr,"albert"),sXr.forEach(t),o1o=r(WMe," \u2014 "),UP=n(WMe,"A",{href:!0});var lXr=s(UP);r1o=r(lXr,"AlbertForMaskedLM"),lXr.forEach(t),t1o=r(WMe," (ALBERT model)"),WMe.forEach(t),a1o=i(j),ku=n(j,"LI",{});var QMe=s(ku);loe=n(QMe,"STRONG",{});var iXr=s(loe);n1o=r(iXr,"bart"),iXr.forEach(t),s1o=r(QMe," \u2014 "),JP=n(QMe,"A",{href:!0});var dXr=s(JP);l1o=r(dXr,"BartForConditionalGeneration"),dXr.forEach(t),i1o=r(QMe," (BART model)"),QMe.forEach(t),d1o=i(j),Ru=n(j,"LI",{});var HMe=s(Ru);ioe=n(HMe,"STRONG",{});var cXr=s(ioe);c1o=r(cXr,"bert"),cXr.forEach(t),f1o=r(HMe," \u2014 "),YP=n(HMe,"A",{href:!0});var fXr=s(YP);m1o=r(fXr,"BertForMaskedLM"),fXr.forEach(t),g1o=r(HMe," (BERT model)"),HMe.forEach(t),h1o=i(j),Su=n(j,"LI",{});var UMe=s(Su);doe=n(UMe,"STRONG",{});var mXr=s(doe);p1o=r(mXr,"big_bird"),mXr.forEach(t),_1o=r(UMe," \u2014 "),KP=n(UMe,"A",{href:!0});var gXr=s(KP);u1o=r(gXr,"BigBirdForMaskedLM"),gXr.forEach(t),b1o=r(UMe," (BigBird model)"),UMe.forEach(t),v1o=i(j),Pu=n(j,"LI",{});var JMe=s(Pu);coe=n(JMe,"STRONG",{});var hXr=s(coe);T1o=r(hXr,"camembert"),hXr.forEach(t),F1o=r(JMe," \u2014 "),ZP=n(JMe,"A",{href:!0});var pXr=s(ZP);C1o=r(pXr,"CamembertForMaskedLM"),pXr.forEach(t),M1o=r(JMe," (CamemBERT model)"),JMe.forEach(t),E1o=i(j),$u=n(j,"LI",{});var YMe=s($u);foe=n(YMe,"STRONG",{});var _Xr=s(foe);y1o=r(_Xr,"convbert"),_Xr.forEach(t),w1o=r(YMe," \u2014 "),e$=n(YMe,"A",{href:!0});var uXr=s(e$);A1o=r(uXr,"ConvBertForMaskedLM"),uXr.forEach(t),L1o=r(YMe," (ConvBERT model)"),YMe.forEach(t),B1o=i(j),Iu=n(j,"LI",{});var KMe=s(Iu);moe=n(KMe,"STRONG",{});var bXr=s(moe);x1o=r(bXr,"data2vec-text"),bXr.forEach(t),k1o=r(KMe," \u2014 "),o$=n(KMe,"A",{href:!0});var vXr=s(o$);R1o=r(vXr,"Data2VecTextForMaskedLM"),vXr.forEach(t),S1o=r(KMe," (Data2VecText model)"),KMe.forEach(t),P1o=i(j),Du=n(j,"LI",{});var ZMe=s(Du);goe=n(ZMe,"STRONG",{});var TXr=s(goe);$1o=r(TXr,"deberta"),TXr.forEach(t),I1o=r(ZMe," \u2014 "),r$=n(ZMe,"A",{href:!0});var FXr=s(r$);D1o=r(FXr,"DebertaForMaskedLM"),FXr.forEach(t),j1o=r(ZMe," (DeBERTa model)"),ZMe.forEach(t),N1o=i(j),ju=n(j,"LI",{});var e4e=s(ju);hoe=n(e4e,"STRONG",{});var CXr=s(hoe);q1o=r(CXr,"deberta-v2"),CXr.forEach(t),G1o=r(e4e," \u2014 "),t$=n(e4e,"A",{href:!0});var MXr=s(t$);O1o=r(MXr,"DebertaV2ForMaskedLM"),MXr.forEach(t),X1o=r(e4e," (DeBERTa-v2 model)"),e4e.forEach(t),V1o=i(j),Nu=n(j,"LI",{});var o4e=s(Nu);poe=n(o4e,"STRONG",{});var EXr=s(poe);z1o=r(EXr,"distilbert"),EXr.forEach(t),W1o=r(o4e," \u2014 "),a$=n(o4e,"A",{href:!0});var yXr=s(a$);Q1o=r(yXr,"DistilBertForMaskedLM"),yXr.forEach(t),H1o=r(o4e," (DistilBERT model)"),o4e.forEach(t),U1o=i(j),qu=n(j,"LI",{});var r4e=s(qu);_oe=n(r4e,"STRONG",{});var wXr=s(_oe);J1o=r(wXr,"electra"),wXr.forEach(t),Y1o=r(r4e," \u2014 "),n$=n(r4e,"A",{href:!0});var AXr=s(n$);K1o=r(AXr,"ElectraForMaskedLM"),AXr.forEach(t),Z1o=r(r4e," (ELECTRA model)"),r4e.forEach(t),ebo=i(j),Gu=n(j,"LI",{});var t4e=s(Gu);uoe=n(t4e,"STRONG",{});var LXr=s(uoe);obo=r(LXr,"flaubert"),LXr.forEach(t),rbo=r(t4e," \u2014 "),s$=n(t4e,"A",{href:!0});var BXr=s(s$);tbo=r(BXr,"FlaubertWithLMHeadModel"),BXr.forEach(t),abo=r(t4e," (FlauBERT model)"),t4e.forEach(t),nbo=i(j),Ou=n(j,"LI",{});var a4e=s(Ou);boe=n(a4e,"STRONG",{});var xXr=s(boe);sbo=r(xXr,"fnet"),xXr.forEach(t),lbo=r(a4e," \u2014 "),l$=n(a4e,"A",{href:!0});var kXr=s(l$);ibo=r(kXr,"FNetForMaskedLM"),kXr.forEach(t),dbo=r(a4e," (FNet model)"),a4e.forEach(t),cbo=i(j),Xu=n(j,"LI",{});var n4e=s(Xu);voe=n(n4e,"STRONG",{});var RXr=s(voe);fbo=r(RXr,"funnel"),RXr.forEach(t),mbo=r(n4e," \u2014 "),i$=n(n4e,"A",{href:!0});var SXr=s(i$);gbo=r(SXr,"FunnelForMaskedLM"),SXr.forEach(t),hbo=r(n4e," (Funnel Transformer model)"),n4e.forEach(t),pbo=i(j),Vu=n(j,"LI",{});var s4e=s(Vu);Toe=n(s4e,"STRONG",{});var PXr=s(Toe);_bo=r(PXr,"ibert"),PXr.forEach(t),ubo=r(s4e," \u2014 "),d$=n(s4e,"A",{href:!0});var $Xr=s(d$);bbo=r($Xr,"IBertForMaskedLM"),$Xr.forEach(t),vbo=r(s4e," (I-BERT model)"),s4e.forEach(t),Tbo=i(j),zu=n(j,"LI",{});var l4e=s(zu);Foe=n(l4e,"STRONG",{});var IXr=s(Foe);Fbo=r(IXr,"layoutlm"),IXr.forEach(t),Cbo=r(l4e," \u2014 "),c$=n(l4e,"A",{href:!0});var DXr=s(c$);Mbo=r(DXr,"LayoutLMForMaskedLM"),DXr.forEach(t),Ebo=r(l4e," (LayoutLM model)"),l4e.forEach(t),ybo=i(j),Wu=n(j,"LI",{});var i4e=s(Wu);Coe=n(i4e,"STRONG",{});var jXr=s(Coe);wbo=r(jXr,"longformer"),jXr.forEach(t),Abo=r(i4e," \u2014 "),f$=n(i4e,"A",{href:!0});var NXr=s(f$);Lbo=r(NXr,"LongformerForMaskedLM"),NXr.forEach(t),Bbo=r(i4e," (Longformer model)"),i4e.forEach(t),xbo=i(j),Qu=n(j,"LI",{});var d4e=s(Qu);Moe=n(d4e,"STRONG",{});var qXr=s(Moe);kbo=r(qXr,"mbart"),qXr.forEach(t),Rbo=r(d4e," \u2014 "),m$=n(d4e,"A",{href:!0});var GXr=s(m$);Sbo=r(GXr,"MBartForConditionalGeneration"),GXr.forEach(t),Pbo=r(d4e," (mBART model)"),d4e.forEach(t),$bo=i(j),Hu=n(j,"LI",{});var c4e=s(Hu);Eoe=n(c4e,"STRONG",{});var OXr=s(Eoe);Ibo=r(OXr,"megatron-bert"),OXr.forEach(t),Dbo=r(c4e," \u2014 "),g$=n(c4e,"A",{href:!0});var XXr=s(g$);jbo=r(XXr,"MegatronBertForMaskedLM"),XXr.forEach(t),Nbo=r(c4e," (MegatronBert model)"),c4e.forEach(t),qbo=i(j),Uu=n(j,"LI",{});var f4e=s(Uu);yoe=n(f4e,"STRONG",{});var VXr=s(yoe);Gbo=r(VXr,"mobilebert"),VXr.forEach(t),Obo=r(f4e," \u2014 "),h$=n(f4e,"A",{href:!0});var zXr=s(h$);Xbo=r(zXr,"MobileBertForMaskedLM"),zXr.forEach(t),Vbo=r(f4e," (MobileBERT model)"),f4e.forEach(t),zbo=i(j),Ju=n(j,"LI",{});var m4e=s(Ju);woe=n(m4e,"STRONG",{});var WXr=s(woe);Wbo=r(WXr,"mpnet"),WXr.forEach(t),Qbo=r(m4e," \u2014 "),p$=n(m4e,"A",{href:!0});var QXr=s(p$);Hbo=r(QXr,"MPNetForMaskedLM"),QXr.forEach(t),Ubo=r(m4e," (MPNet model)"),m4e.forEach(t),Jbo=i(j),Yu=n(j,"LI",{});var g4e=s(Yu);Aoe=n(g4e,"STRONG",{});var HXr=s(Aoe);Ybo=r(HXr,"nystromformer"),HXr.forEach(t),Kbo=r(g4e," \u2014 "),_$=n(g4e,"A",{href:!0});var UXr=s(_$);Zbo=r(UXr,"NystromformerForMaskedLM"),UXr.forEach(t),e5o=r(g4e," (Nystromformer model)"),g4e.forEach(t),o5o=i(j),Ku=n(j,"LI",{});var h4e=s(Ku);Loe=n(h4e,"STRONG",{});var JXr=s(Loe);r5o=r(JXr,"perceiver"),JXr.forEach(t),t5o=r(h4e," \u2014 "),u$=n(h4e,"A",{href:!0});var YXr=s(u$);a5o=r(YXr,"PerceiverForMaskedLM"),YXr.forEach(t),n5o=r(h4e," (Perceiver model)"),h4e.forEach(t),s5o=i(j),Zu=n(j,"LI",{});var p4e=s(Zu);Boe=n(p4e,"STRONG",{});var KXr=s(Boe);l5o=r(KXr,"qdqbert"),KXr.forEach(t),i5o=r(p4e," \u2014 "),b$=n(p4e,"A",{href:!0});var ZXr=s(b$);d5o=r(ZXr,"QDQBertForMaskedLM"),ZXr.forEach(t),c5o=r(p4e," (QDQBert model)"),p4e.forEach(t),f5o=i(j),e0=n(j,"LI",{});var _4e=s(e0);xoe=n(_4e,"STRONG",{});var eVr=s(xoe);m5o=r(eVr,"reformer"),eVr.forEach(t),g5o=r(_4e," \u2014 "),v$=n(_4e,"A",{href:!0});var oVr=s(v$);h5o=r(oVr,"ReformerForMaskedLM"),oVr.forEach(t),p5o=r(_4e," (Reformer model)"),_4e.forEach(t),_5o=i(j),o0=n(j,"LI",{});var u4e=s(o0);koe=n(u4e,"STRONG",{});var rVr=s(koe);u5o=r(rVr,"rembert"),rVr.forEach(t),b5o=r(u4e," \u2014 "),T$=n(u4e,"A",{href:!0});var tVr=s(T$);v5o=r(tVr,"RemBertForMaskedLM"),tVr.forEach(t),T5o=r(u4e," (RemBERT model)"),u4e.forEach(t),F5o=i(j),r0=n(j,"LI",{});var b4e=s(r0);Roe=n(b4e,"STRONG",{});var aVr=s(Roe);C5o=r(aVr,"roberta"),aVr.forEach(t),M5o=r(b4e," \u2014 "),F$=n(b4e,"A",{href:!0});var nVr=s(F$);E5o=r(nVr,"RobertaForMaskedLM"),nVr.forEach(t),y5o=r(b4e," (RoBERTa model)"),b4e.forEach(t),w5o=i(j),t0=n(j,"LI",{});var v4e=s(t0);Soe=n(v4e,"STRONG",{});var sVr=s(Soe);A5o=r(sVr,"roformer"),sVr.forEach(t),L5o=r(v4e," \u2014 "),C$=n(v4e,"A",{href:!0});var lVr=s(C$);B5o=r(lVr,"RoFormerForMaskedLM"),lVr.forEach(t),x5o=r(v4e," (RoFormer model)"),v4e.forEach(t),k5o=i(j),a0=n(j,"LI",{});var T4e=s(a0);Poe=n(T4e,"STRONG",{});var iVr=s(Poe);R5o=r(iVr,"squeezebert"),iVr.forEach(t),S5o=r(T4e," \u2014 "),M$=n(T4e,"A",{href:!0});var dVr=s(M$);P5o=r(dVr,"SqueezeBertForMaskedLM"),dVr.forEach(t),$5o=r(T4e," (SqueezeBERT model)"),T4e.forEach(t),I5o=i(j),n0=n(j,"LI",{});var F4e=s(n0);$oe=n(F4e,"STRONG",{});var cVr=s($oe);D5o=r(cVr,"tapas"),cVr.forEach(t),j5o=r(F4e," \u2014 "),E$=n(F4e,"A",{href:!0});var fVr=s(E$);N5o=r(fVr,"TapasForMaskedLM"),fVr.forEach(t),q5o=r(F4e," (TAPAS model)"),F4e.forEach(t),G5o=i(j),s0=n(j,"LI",{});var C4e=s(s0);Ioe=n(C4e,"STRONG",{});var mVr=s(Ioe);O5o=r(mVr,"wav2vec2"),mVr.forEach(t),X5o=r(C4e," \u2014 "),Doe=n(C4e,"CODE",{});var gVr=s(Doe);V5o=r(gVr,"Wav2Vec2ForMaskedLM"),gVr.forEach(t),z5o=r(C4e,"(Wav2Vec2 model)"),C4e.forEach(t),W5o=i(j),l0=n(j,"LI",{});var M4e=s(l0);joe=n(M4e,"STRONG",{});var hVr=s(joe);Q5o=r(hVr,"xlm"),hVr.forEach(t),H5o=r(M4e," \u2014 "),y$=n(M4e,"A",{href:!0});var pVr=s(y$);U5o=r(pVr,"XLMWithLMHeadModel"),pVr.forEach(t),J5o=r(M4e," (XLM model)"),M4e.forEach(t),Y5o=i(j),i0=n(j,"LI",{});var E4e=s(i0);Noe=n(E4e,"STRONG",{});var _Vr=s(Noe);K5o=r(_Vr,"xlm-roberta"),_Vr.forEach(t),Z5o=r(E4e," \u2014 "),w$=n(E4e,"A",{href:!0});var uVr=s(w$);e2o=r(uVr,"XLMRobertaForMaskedLM"),uVr.forEach(t),o2o=r(E4e," (XLM-RoBERTa model)"),E4e.forEach(t),r2o=i(j),d0=n(j,"LI",{});var y4e=s(d0);qoe=n(y4e,"STRONG",{});var bVr=s(qoe);t2o=r(bVr,"xlm-roberta-xl"),bVr.forEach(t),a2o=r(y4e," \u2014 "),A$=n(y4e,"A",{href:!0});var vVr=s(A$);n2o=r(vVr,"XLMRobertaXLForMaskedLM"),vVr.forEach(t),s2o=r(y4e," (XLM-RoBERTa-XL model)"),y4e.forEach(t),l2o=i(j),c0=n(j,"LI",{});var w4e=s(c0);Goe=n(w4e,"STRONG",{});var TVr=s(Goe);i2o=r(TVr,"yoso"),TVr.forEach(t),d2o=r(w4e," \u2014 "),L$=n(w4e,"A",{href:!0});var FVr=s(L$);c2o=r(FVr,"YosoForMaskedLM"),FVr.forEach(t),f2o=r(w4e," (YOSO model)"),w4e.forEach(t),j.forEach(t),m2o=i(qt),f0=n(qt,"P",{});var A4e=s(f0);g2o=r(A4e,"The model is set in evaluation mode by default using "),Ooe=n(A4e,"CODE",{});var CVr=s(Ooe);h2o=r(CVr,"model.eval()"),CVr.forEach(t),p2o=r(A4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xoe=n(A4e,"CODE",{});var MVr=s(Xoe);_2o=r(MVr,"model.train()"),MVr.forEach(t),A4e.forEach(t),u2o=i(qt),Voe=n(qt,"P",{});var EVr=s(Voe);b2o=r(EVr,"Examples:"),EVr.forEach(t),v2o=i(qt),m(DE.$$.fragment,qt),qt.forEach(t),Qs.forEach(t),yBe=i(c),od=n(c,"H2",{class:!0});var Rke=s(od);m0=n(Rke,"A",{id:!0,class:!0,href:!0});var yVr=s(m0);zoe=n(yVr,"SPAN",{});var wVr=s(zoe);m(jE.$$.fragment,wVr),wVr.forEach(t),yVr.forEach(t),T2o=i(Rke),Woe=n(Rke,"SPAN",{});var AVr=s(Woe);F2o=r(AVr,"AutoModelForSeq2SeqLM"),AVr.forEach(t),Rke.forEach(t),wBe=i(c),Zo=n(c,"DIV",{class:!0});var Us=s(Zo);m(NE.$$.fragment,Us),C2o=i(Us),rd=n(Us,"P",{});var nz=s(rd);M2o=r(nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Qoe=n(nz,"CODE",{});var LVr=s(Qoe);E2o=r(LVr,"from_pretrained()"),LVr.forEach(t),y2o=r(nz,"class method or the "),Hoe=n(nz,"CODE",{});var BVr=s(Hoe);w2o=r(BVr,"from_config()"),BVr.forEach(t),A2o=r(nz,`class
method.`),nz.forEach(t),L2o=i(Us),qE=n(Us,"P",{});var Ske=s(qE);B2o=r(Ske,"This class cannot be instantiated directly using "),Uoe=n(Ske,"CODE",{});var xVr=s(Uoe);x2o=r(xVr,"__init__()"),xVr.forEach(t),k2o=r(Ske," (throws an error)."),Ske.forEach(t),R2o=i(Us),Wr=n(Us,"DIV",{class:!0});var Js=s(Wr);m(GE.$$.fragment,Js),S2o=i(Js),Joe=n(Js,"P",{});var kVr=s(Joe);P2o=r(kVr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),kVr.forEach(t),$2o=i(Js),td=n(Js,"P",{});var sz=s(td);I2o=r(sz,`Note:
Loading a model from its configuration file does `),Yoe=n(sz,"STRONG",{});var RVr=s(Yoe);D2o=r(RVr,"not"),RVr.forEach(t),j2o=r(sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Koe=n(sz,"CODE",{});var SVr=s(Koe);N2o=r(SVr,"from_pretrained()"),SVr.forEach(t),q2o=r(sz,"to load the model weights."),sz.forEach(t),G2o=i(Js),Zoe=n(Js,"P",{});var PVr=s(Zoe);O2o=r(PVr,"Examples:"),PVr.forEach(t),X2o=i(Js),m(OE.$$.fragment,Js),Js.forEach(t),V2o=i(Us),Ge=n(Us,"DIV",{class:!0});var Gt=s(Ge);m(XE.$$.fragment,Gt),z2o=i(Gt),ere=n(Gt,"P",{});var $Vr=s(ere);W2o=r($Vr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),$Vr.forEach(t),Q2o=i(Gt),Xa=n(Gt,"P",{});var jM=s(Xa);H2o=r(jM,"The model class to instantiate is selected based on the "),ore=n(jM,"CODE",{});var IVr=s(ore);U2o=r(IVr,"model_type"),IVr.forEach(t),J2o=r(jM,` property of the config object (either
passed as an argument or loaded from `),rre=n(jM,"CODE",{});var DVr=s(rre);Y2o=r(DVr,"pretrained_model_name_or_path"),DVr.forEach(t),K2o=r(jM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tre=n(jM,"CODE",{});var jVr=s(tre);Z2o=r(jVr,"pretrained_model_name_or_path"),jVr.forEach(t),evo=r(jM,":"),jM.forEach(t),ovo=i(Gt),ae=n(Gt,"UL",{});var le=s(ae);g0=n(le,"LI",{});var L4e=s(g0);are=n(L4e,"STRONG",{});var NVr=s(are);rvo=r(NVr,"bart"),NVr.forEach(t),tvo=r(L4e," \u2014 "),B$=n(L4e,"A",{href:!0});var qVr=s(B$);avo=r(qVr,"BartForConditionalGeneration"),qVr.forEach(t),nvo=r(L4e," (BART model)"),L4e.forEach(t),svo=i(le),h0=n(le,"LI",{});var B4e=s(h0);nre=n(B4e,"STRONG",{});var GVr=s(nre);lvo=r(GVr,"bigbird_pegasus"),GVr.forEach(t),ivo=r(B4e," \u2014 "),x$=n(B4e,"A",{href:!0});var OVr=s(x$);dvo=r(OVr,"BigBirdPegasusForConditionalGeneration"),OVr.forEach(t),cvo=r(B4e," (BigBirdPegasus model)"),B4e.forEach(t),fvo=i(le),p0=n(le,"LI",{});var x4e=s(p0);sre=n(x4e,"STRONG",{});var XVr=s(sre);mvo=r(XVr,"blenderbot"),XVr.forEach(t),gvo=r(x4e," \u2014 "),k$=n(x4e,"A",{href:!0});var VVr=s(k$);hvo=r(VVr,"BlenderbotForConditionalGeneration"),VVr.forEach(t),pvo=r(x4e," (Blenderbot model)"),x4e.forEach(t),_vo=i(le),_0=n(le,"LI",{});var k4e=s(_0);lre=n(k4e,"STRONG",{});var zVr=s(lre);uvo=r(zVr,"blenderbot-small"),zVr.forEach(t),bvo=r(k4e," \u2014 "),R$=n(k4e,"A",{href:!0});var WVr=s(R$);vvo=r(WVr,"BlenderbotSmallForConditionalGeneration"),WVr.forEach(t),Tvo=r(k4e," (BlenderbotSmall model)"),k4e.forEach(t),Fvo=i(le),u0=n(le,"LI",{});var R4e=s(u0);ire=n(R4e,"STRONG",{});var QVr=s(ire);Cvo=r(QVr,"encoder-decoder"),QVr.forEach(t),Mvo=r(R4e," \u2014 "),S$=n(R4e,"A",{href:!0});var HVr=s(S$);Evo=r(HVr,"EncoderDecoderModel"),HVr.forEach(t),yvo=r(R4e," (Encoder decoder model)"),R4e.forEach(t),wvo=i(le),b0=n(le,"LI",{});var S4e=s(b0);dre=n(S4e,"STRONG",{});var UVr=s(dre);Avo=r(UVr,"fsmt"),UVr.forEach(t),Lvo=r(S4e," \u2014 "),P$=n(S4e,"A",{href:!0});var JVr=s(P$);Bvo=r(JVr,"FSMTForConditionalGeneration"),JVr.forEach(t),xvo=r(S4e," (FairSeq Machine-Translation model)"),S4e.forEach(t),kvo=i(le),v0=n(le,"LI",{});var P4e=s(v0);cre=n(P4e,"STRONG",{});var YVr=s(cre);Rvo=r(YVr,"led"),YVr.forEach(t),Svo=r(P4e," \u2014 "),$$=n(P4e,"A",{href:!0});var KVr=s($$);Pvo=r(KVr,"LEDForConditionalGeneration"),KVr.forEach(t),$vo=r(P4e," (LED model)"),P4e.forEach(t),Ivo=i(le),T0=n(le,"LI",{});var $4e=s(T0);fre=n($4e,"STRONG",{});var ZVr=s(fre);Dvo=r(ZVr,"m2m_100"),ZVr.forEach(t),jvo=r($4e," \u2014 "),I$=n($4e,"A",{href:!0});var ezr=s(I$);Nvo=r(ezr,"M2M100ForConditionalGeneration"),ezr.forEach(t),qvo=r($4e," (M2M100 model)"),$4e.forEach(t),Gvo=i(le),F0=n(le,"LI",{});var I4e=s(F0);mre=n(I4e,"STRONG",{});var ozr=s(mre);Ovo=r(ozr,"marian"),ozr.forEach(t),Xvo=r(I4e," \u2014 "),D$=n(I4e,"A",{href:!0});var rzr=s(D$);Vvo=r(rzr,"MarianMTModel"),rzr.forEach(t),zvo=r(I4e," (Marian model)"),I4e.forEach(t),Wvo=i(le),C0=n(le,"LI",{});var D4e=s(C0);gre=n(D4e,"STRONG",{});var tzr=s(gre);Qvo=r(tzr,"mbart"),tzr.forEach(t),Hvo=r(D4e," \u2014 "),j$=n(D4e,"A",{href:!0});var azr=s(j$);Uvo=r(azr,"MBartForConditionalGeneration"),azr.forEach(t),Jvo=r(D4e," (mBART model)"),D4e.forEach(t),Yvo=i(le),M0=n(le,"LI",{});var j4e=s(M0);hre=n(j4e,"STRONG",{});var nzr=s(hre);Kvo=r(nzr,"mt5"),nzr.forEach(t),Zvo=r(j4e," \u2014 "),N$=n(j4e,"A",{href:!0});var szr=s(N$);eTo=r(szr,"MT5ForConditionalGeneration"),szr.forEach(t),oTo=r(j4e," (mT5 model)"),j4e.forEach(t),rTo=i(le),E0=n(le,"LI",{});var N4e=s(E0);pre=n(N4e,"STRONG",{});var lzr=s(pre);tTo=r(lzr,"pegasus"),lzr.forEach(t),aTo=r(N4e," \u2014 "),q$=n(N4e,"A",{href:!0});var izr=s(q$);nTo=r(izr,"PegasusForConditionalGeneration"),izr.forEach(t),sTo=r(N4e," (Pegasus model)"),N4e.forEach(t),lTo=i(le),y0=n(le,"LI",{});var q4e=s(y0);_re=n(q4e,"STRONG",{});var dzr=s(_re);iTo=r(dzr,"plbart"),dzr.forEach(t),dTo=r(q4e," \u2014 "),G$=n(q4e,"A",{href:!0});var czr=s(G$);cTo=r(czr,"PLBartForConditionalGeneration"),czr.forEach(t),fTo=r(q4e," (PLBart model)"),q4e.forEach(t),mTo=i(le),w0=n(le,"LI",{});var G4e=s(w0);ure=n(G4e,"STRONG",{});var fzr=s(ure);gTo=r(fzr,"prophetnet"),fzr.forEach(t),hTo=r(G4e," \u2014 "),O$=n(G4e,"A",{href:!0});var mzr=s(O$);pTo=r(mzr,"ProphetNetForConditionalGeneration"),mzr.forEach(t),_To=r(G4e," (ProphetNet model)"),G4e.forEach(t),uTo=i(le),A0=n(le,"LI",{});var O4e=s(A0);bre=n(O4e,"STRONG",{});var gzr=s(bre);bTo=r(gzr,"t5"),gzr.forEach(t),vTo=r(O4e," \u2014 "),X$=n(O4e,"A",{href:!0});var hzr=s(X$);TTo=r(hzr,"T5ForConditionalGeneration"),hzr.forEach(t),FTo=r(O4e," (T5 model)"),O4e.forEach(t),CTo=i(le),L0=n(le,"LI",{});var X4e=s(L0);vre=n(X4e,"STRONG",{});var pzr=s(vre);MTo=r(pzr,"xlm-prophetnet"),pzr.forEach(t),ETo=r(X4e," \u2014 "),V$=n(X4e,"A",{href:!0});var _zr=s(V$);yTo=r(_zr,"XLMProphetNetForConditionalGeneration"),_zr.forEach(t),wTo=r(X4e," (XLMProphetNet model)"),X4e.forEach(t),le.forEach(t),ATo=i(Gt),B0=n(Gt,"P",{});var V4e=s(B0);LTo=r(V4e,"The model is set in evaluation mode by default using "),Tre=n(V4e,"CODE",{});var uzr=s(Tre);BTo=r(uzr,"model.eval()"),uzr.forEach(t),xTo=r(V4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fre=n(V4e,"CODE",{});var bzr=s(Fre);kTo=r(bzr,"model.train()"),bzr.forEach(t),V4e.forEach(t),RTo=i(Gt),Cre=n(Gt,"P",{});var vzr=s(Cre);STo=r(vzr,"Examples:"),vzr.forEach(t),PTo=i(Gt),m(VE.$$.fragment,Gt),Gt.forEach(t),Us.forEach(t),ABe=i(c),ad=n(c,"H2",{class:!0});var Pke=s(ad);x0=n(Pke,"A",{id:!0,class:!0,href:!0});var Tzr=s(x0);Mre=n(Tzr,"SPAN",{});var Fzr=s(Mre);m(zE.$$.fragment,Fzr),Fzr.forEach(t),Tzr.forEach(t),$To=i(Pke),Ere=n(Pke,"SPAN",{});var Czr=s(Ere);ITo=r(Czr,"AutoModelForSequenceClassification"),Czr.forEach(t),Pke.forEach(t),LBe=i(c),er=n(c,"DIV",{class:!0});var Ys=s(er);m(WE.$$.fragment,Ys),DTo=i(Ys),nd=n(Ys,"P",{});var lz=s(nd);jTo=r(lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),yre=n(lz,"CODE",{});var Mzr=s(yre);NTo=r(Mzr,"from_pretrained()"),Mzr.forEach(t),qTo=r(lz,"class method or the "),wre=n(lz,"CODE",{});var Ezr=s(wre);GTo=r(Ezr,"from_config()"),Ezr.forEach(t),OTo=r(lz,`class
method.`),lz.forEach(t),XTo=i(Ys),QE=n(Ys,"P",{});var $ke=s(QE);VTo=r($ke,"This class cannot be instantiated directly using "),Are=n($ke,"CODE",{});var yzr=s(Are);zTo=r(yzr,"__init__()"),yzr.forEach(t),WTo=r($ke," (throws an error)."),$ke.forEach(t),QTo=i(Ys),Qr=n(Ys,"DIV",{class:!0});var Ks=s(Qr);m(HE.$$.fragment,Ks),HTo=i(Ks),Lre=n(Ks,"P",{});var wzr=s(Lre);UTo=r(wzr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),wzr.forEach(t),JTo=i(Ks),sd=n(Ks,"P",{});var iz=s(sd);YTo=r(iz,`Note:
Loading a model from its configuration file does `),Bre=n(iz,"STRONG",{});var Azr=s(Bre);KTo=r(Azr,"not"),Azr.forEach(t),ZTo=r(iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xre=n(iz,"CODE",{});var Lzr=s(xre);eFo=r(Lzr,"from_pretrained()"),Lzr.forEach(t),oFo=r(iz,"to load the model weights."),iz.forEach(t),rFo=i(Ks),kre=n(Ks,"P",{});var Bzr=s(kre);tFo=r(Bzr,"Examples:"),Bzr.forEach(t),aFo=i(Ks),m(UE.$$.fragment,Ks),Ks.forEach(t),nFo=i(Ys),Oe=n(Ys,"DIV",{class:!0});var Ot=s(Oe);m(JE.$$.fragment,Ot),sFo=i(Ot),Rre=n(Ot,"P",{});var xzr=s(Rre);lFo=r(xzr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),xzr.forEach(t),iFo=i(Ot),Va=n(Ot,"P",{});var NM=s(Va);dFo=r(NM,"The model class to instantiate is selected based on the "),Sre=n(NM,"CODE",{});var kzr=s(Sre);cFo=r(kzr,"model_type"),kzr.forEach(t),fFo=r(NM,` property of the config object (either
passed as an argument or loaded from `),Pre=n(NM,"CODE",{});var Rzr=s(Pre);mFo=r(Rzr,"pretrained_model_name_or_path"),Rzr.forEach(t),gFo=r(NM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$re=n(NM,"CODE",{});var Szr=s($re);hFo=r(Szr,"pretrained_model_name_or_path"),Szr.forEach(t),pFo=r(NM,":"),NM.forEach(t),_Fo=i(Ot),A=n(Ot,"UL",{});var L=s(A);k0=n(L,"LI",{});var z4e=s(k0);Ire=n(z4e,"STRONG",{});var Pzr=s(Ire);uFo=r(Pzr,"albert"),Pzr.forEach(t),bFo=r(z4e," \u2014 "),z$=n(z4e,"A",{href:!0});var $zr=s(z$);vFo=r($zr,"AlbertForSequenceClassification"),$zr.forEach(t),TFo=r(z4e," (ALBERT model)"),z4e.forEach(t),FFo=i(L),R0=n(L,"LI",{});var W4e=s(R0);Dre=n(W4e,"STRONG",{});var Izr=s(Dre);CFo=r(Izr,"bart"),Izr.forEach(t),MFo=r(W4e," \u2014 "),W$=n(W4e,"A",{href:!0});var Dzr=s(W$);EFo=r(Dzr,"BartForSequenceClassification"),Dzr.forEach(t),yFo=r(W4e," (BART model)"),W4e.forEach(t),wFo=i(L),S0=n(L,"LI",{});var Q4e=s(S0);jre=n(Q4e,"STRONG",{});var jzr=s(jre);AFo=r(jzr,"bert"),jzr.forEach(t),LFo=r(Q4e," \u2014 "),Q$=n(Q4e,"A",{href:!0});var Nzr=s(Q$);BFo=r(Nzr,"BertForSequenceClassification"),Nzr.forEach(t),xFo=r(Q4e," (BERT model)"),Q4e.forEach(t),kFo=i(L),P0=n(L,"LI",{});var H4e=s(P0);Nre=n(H4e,"STRONG",{});var qzr=s(Nre);RFo=r(qzr,"big_bird"),qzr.forEach(t),SFo=r(H4e," \u2014 "),H$=n(H4e,"A",{href:!0});var Gzr=s(H$);PFo=r(Gzr,"BigBirdForSequenceClassification"),Gzr.forEach(t),$Fo=r(H4e," (BigBird model)"),H4e.forEach(t),IFo=i(L),$0=n(L,"LI",{});var U4e=s($0);qre=n(U4e,"STRONG",{});var Ozr=s(qre);DFo=r(Ozr,"bigbird_pegasus"),Ozr.forEach(t),jFo=r(U4e," \u2014 "),U$=n(U4e,"A",{href:!0});var Xzr=s(U$);NFo=r(Xzr,"BigBirdPegasusForSequenceClassification"),Xzr.forEach(t),qFo=r(U4e," (BigBirdPegasus model)"),U4e.forEach(t),GFo=i(L),I0=n(L,"LI",{});var J4e=s(I0);Gre=n(J4e,"STRONG",{});var Vzr=s(Gre);OFo=r(Vzr,"camembert"),Vzr.forEach(t),XFo=r(J4e," \u2014 "),J$=n(J4e,"A",{href:!0});var zzr=s(J$);VFo=r(zzr,"CamembertForSequenceClassification"),zzr.forEach(t),zFo=r(J4e," (CamemBERT model)"),J4e.forEach(t),WFo=i(L),D0=n(L,"LI",{});var Y4e=s(D0);Ore=n(Y4e,"STRONG",{});var Wzr=s(Ore);QFo=r(Wzr,"canine"),Wzr.forEach(t),HFo=r(Y4e," \u2014 "),Y$=n(Y4e,"A",{href:!0});var Qzr=s(Y$);UFo=r(Qzr,"CanineForSequenceClassification"),Qzr.forEach(t),JFo=r(Y4e," (Canine model)"),Y4e.forEach(t),YFo=i(L),j0=n(L,"LI",{});var K4e=s(j0);Xre=n(K4e,"STRONG",{});var Hzr=s(Xre);KFo=r(Hzr,"convbert"),Hzr.forEach(t),ZFo=r(K4e," \u2014 "),K$=n(K4e,"A",{href:!0});var Uzr=s(K$);e9o=r(Uzr,"ConvBertForSequenceClassification"),Uzr.forEach(t),o9o=r(K4e," (ConvBERT model)"),K4e.forEach(t),r9o=i(L),N0=n(L,"LI",{});var Z4e=s(N0);Vre=n(Z4e,"STRONG",{});var Jzr=s(Vre);t9o=r(Jzr,"ctrl"),Jzr.forEach(t),a9o=r(Z4e," \u2014 "),Z$=n(Z4e,"A",{href:!0});var Yzr=s(Z$);n9o=r(Yzr,"CTRLForSequenceClassification"),Yzr.forEach(t),s9o=r(Z4e," (CTRL model)"),Z4e.forEach(t),l9o=i(L),q0=n(L,"LI",{});var eEe=s(q0);zre=n(eEe,"STRONG",{});var Kzr=s(zre);i9o=r(Kzr,"data2vec-text"),Kzr.forEach(t),d9o=r(eEe," \u2014 "),eI=n(eEe,"A",{href:!0});var Zzr=s(eI);c9o=r(Zzr,"Data2VecTextForSequenceClassification"),Zzr.forEach(t),f9o=r(eEe," (Data2VecText model)"),eEe.forEach(t),m9o=i(L),G0=n(L,"LI",{});var oEe=s(G0);Wre=n(oEe,"STRONG",{});var eWr=s(Wre);g9o=r(eWr,"deberta"),eWr.forEach(t),h9o=r(oEe," \u2014 "),oI=n(oEe,"A",{href:!0});var oWr=s(oI);p9o=r(oWr,"DebertaForSequenceClassification"),oWr.forEach(t),_9o=r(oEe," (DeBERTa model)"),oEe.forEach(t),u9o=i(L),O0=n(L,"LI",{});var rEe=s(O0);Qre=n(rEe,"STRONG",{});var rWr=s(Qre);b9o=r(rWr,"deberta-v2"),rWr.forEach(t),v9o=r(rEe," \u2014 "),rI=n(rEe,"A",{href:!0});var tWr=s(rI);T9o=r(tWr,"DebertaV2ForSequenceClassification"),tWr.forEach(t),F9o=r(rEe," (DeBERTa-v2 model)"),rEe.forEach(t),C9o=i(L),X0=n(L,"LI",{});var tEe=s(X0);Hre=n(tEe,"STRONG",{});var aWr=s(Hre);M9o=r(aWr,"distilbert"),aWr.forEach(t),E9o=r(tEe," \u2014 "),tI=n(tEe,"A",{href:!0});var nWr=s(tI);y9o=r(nWr,"DistilBertForSequenceClassification"),nWr.forEach(t),w9o=r(tEe," (DistilBERT model)"),tEe.forEach(t),A9o=i(L),V0=n(L,"LI",{});var aEe=s(V0);Ure=n(aEe,"STRONG",{});var sWr=s(Ure);L9o=r(sWr,"electra"),sWr.forEach(t),B9o=r(aEe," \u2014 "),aI=n(aEe,"A",{href:!0});var lWr=s(aI);x9o=r(lWr,"ElectraForSequenceClassification"),lWr.forEach(t),k9o=r(aEe," (ELECTRA model)"),aEe.forEach(t),R9o=i(L),z0=n(L,"LI",{});var nEe=s(z0);Jre=n(nEe,"STRONG",{});var iWr=s(Jre);S9o=r(iWr,"flaubert"),iWr.forEach(t),P9o=r(nEe," \u2014 "),nI=n(nEe,"A",{href:!0});var dWr=s(nI);$9o=r(dWr,"FlaubertForSequenceClassification"),dWr.forEach(t),I9o=r(nEe," (FlauBERT model)"),nEe.forEach(t),D9o=i(L),W0=n(L,"LI",{});var sEe=s(W0);Yre=n(sEe,"STRONG",{});var cWr=s(Yre);j9o=r(cWr,"fnet"),cWr.forEach(t),N9o=r(sEe," \u2014 "),sI=n(sEe,"A",{href:!0});var fWr=s(sI);q9o=r(fWr,"FNetForSequenceClassification"),fWr.forEach(t),G9o=r(sEe," (FNet model)"),sEe.forEach(t),O9o=i(L),Q0=n(L,"LI",{});var lEe=s(Q0);Kre=n(lEe,"STRONG",{});var mWr=s(Kre);X9o=r(mWr,"funnel"),mWr.forEach(t),V9o=r(lEe," \u2014 "),lI=n(lEe,"A",{href:!0});var gWr=s(lI);z9o=r(gWr,"FunnelForSequenceClassification"),gWr.forEach(t),W9o=r(lEe," (Funnel Transformer model)"),lEe.forEach(t),Q9o=i(L),H0=n(L,"LI",{});var iEe=s(H0);Zre=n(iEe,"STRONG",{});var hWr=s(Zre);H9o=r(hWr,"gpt2"),hWr.forEach(t),U9o=r(iEe," \u2014 "),iI=n(iEe,"A",{href:!0});var pWr=s(iI);J9o=r(pWr,"GPT2ForSequenceClassification"),pWr.forEach(t),Y9o=r(iEe," (OpenAI GPT-2 model)"),iEe.forEach(t),K9o=i(L),U0=n(L,"LI",{});var dEe=s(U0);ete=n(dEe,"STRONG",{});var _Wr=s(ete);Z9o=r(_Wr,"gpt_neo"),_Wr.forEach(t),eCo=r(dEe," \u2014 "),dI=n(dEe,"A",{href:!0});var uWr=s(dI);oCo=r(uWr,"GPTNeoForSequenceClassification"),uWr.forEach(t),rCo=r(dEe," (GPT Neo model)"),dEe.forEach(t),tCo=i(L),J0=n(L,"LI",{});var cEe=s(J0);ote=n(cEe,"STRONG",{});var bWr=s(ote);aCo=r(bWr,"gptj"),bWr.forEach(t),nCo=r(cEe," \u2014 "),cI=n(cEe,"A",{href:!0});var vWr=s(cI);sCo=r(vWr,"GPTJForSequenceClassification"),vWr.forEach(t),lCo=r(cEe," (GPT-J model)"),cEe.forEach(t),iCo=i(L),Y0=n(L,"LI",{});var fEe=s(Y0);rte=n(fEe,"STRONG",{});var TWr=s(rte);dCo=r(TWr,"ibert"),TWr.forEach(t),cCo=r(fEe," \u2014 "),fI=n(fEe,"A",{href:!0});var FWr=s(fI);fCo=r(FWr,"IBertForSequenceClassification"),FWr.forEach(t),mCo=r(fEe," (I-BERT model)"),fEe.forEach(t),gCo=i(L),K0=n(L,"LI",{});var mEe=s(K0);tte=n(mEe,"STRONG",{});var CWr=s(tte);hCo=r(CWr,"layoutlm"),CWr.forEach(t),pCo=r(mEe," \u2014 "),mI=n(mEe,"A",{href:!0});var MWr=s(mI);_Co=r(MWr,"LayoutLMForSequenceClassification"),MWr.forEach(t),uCo=r(mEe," (LayoutLM model)"),mEe.forEach(t),bCo=i(L),Z0=n(L,"LI",{});var gEe=s(Z0);ate=n(gEe,"STRONG",{});var EWr=s(ate);vCo=r(EWr,"layoutlmv2"),EWr.forEach(t),TCo=r(gEe," \u2014 "),gI=n(gEe,"A",{href:!0});var yWr=s(gI);FCo=r(yWr,"LayoutLMv2ForSequenceClassification"),yWr.forEach(t),CCo=r(gEe," (LayoutLMv2 model)"),gEe.forEach(t),MCo=i(L),e1=n(L,"LI",{});var hEe=s(e1);nte=n(hEe,"STRONG",{});var wWr=s(nte);ECo=r(wWr,"led"),wWr.forEach(t),yCo=r(hEe," \u2014 "),hI=n(hEe,"A",{href:!0});var AWr=s(hI);wCo=r(AWr,"LEDForSequenceClassification"),AWr.forEach(t),ACo=r(hEe," (LED model)"),hEe.forEach(t),LCo=i(L),o1=n(L,"LI",{});var pEe=s(o1);ste=n(pEe,"STRONG",{});var LWr=s(ste);BCo=r(LWr,"longformer"),LWr.forEach(t),xCo=r(pEe," \u2014 "),pI=n(pEe,"A",{href:!0});var BWr=s(pI);kCo=r(BWr,"LongformerForSequenceClassification"),BWr.forEach(t),RCo=r(pEe," (Longformer model)"),pEe.forEach(t),SCo=i(L),r1=n(L,"LI",{});var _Ee=s(r1);lte=n(_Ee,"STRONG",{});var xWr=s(lte);PCo=r(xWr,"mbart"),xWr.forEach(t),$Co=r(_Ee," \u2014 "),_I=n(_Ee,"A",{href:!0});var kWr=s(_I);ICo=r(kWr,"MBartForSequenceClassification"),kWr.forEach(t),DCo=r(_Ee," (mBART model)"),_Ee.forEach(t),jCo=i(L),t1=n(L,"LI",{});var uEe=s(t1);ite=n(uEe,"STRONG",{});var RWr=s(ite);NCo=r(RWr,"megatron-bert"),RWr.forEach(t),qCo=r(uEe," \u2014 "),uI=n(uEe,"A",{href:!0});var SWr=s(uI);GCo=r(SWr,"MegatronBertForSequenceClassification"),SWr.forEach(t),OCo=r(uEe," (MegatronBert model)"),uEe.forEach(t),XCo=i(L),a1=n(L,"LI",{});var bEe=s(a1);dte=n(bEe,"STRONG",{});var PWr=s(dte);VCo=r(PWr,"mobilebert"),PWr.forEach(t),zCo=r(bEe," \u2014 "),bI=n(bEe,"A",{href:!0});var $Wr=s(bI);WCo=r($Wr,"MobileBertForSequenceClassification"),$Wr.forEach(t),QCo=r(bEe," (MobileBERT model)"),bEe.forEach(t),HCo=i(L),n1=n(L,"LI",{});var vEe=s(n1);cte=n(vEe,"STRONG",{});var IWr=s(cte);UCo=r(IWr,"mpnet"),IWr.forEach(t),JCo=r(vEe," \u2014 "),vI=n(vEe,"A",{href:!0});var DWr=s(vI);YCo=r(DWr,"MPNetForSequenceClassification"),DWr.forEach(t),KCo=r(vEe," (MPNet model)"),vEe.forEach(t),ZCo=i(L),s1=n(L,"LI",{});var TEe=s(s1);fte=n(TEe,"STRONG",{});var jWr=s(fte);eMo=r(jWr,"nystromformer"),jWr.forEach(t),oMo=r(TEe," \u2014 "),TI=n(TEe,"A",{href:!0});var NWr=s(TI);rMo=r(NWr,"NystromformerForSequenceClassification"),NWr.forEach(t),tMo=r(TEe," (Nystromformer model)"),TEe.forEach(t),aMo=i(L),l1=n(L,"LI",{});var FEe=s(l1);mte=n(FEe,"STRONG",{});var qWr=s(mte);nMo=r(qWr,"openai-gpt"),qWr.forEach(t),sMo=r(FEe," \u2014 "),FI=n(FEe,"A",{href:!0});var GWr=s(FI);lMo=r(GWr,"OpenAIGPTForSequenceClassification"),GWr.forEach(t),iMo=r(FEe," (OpenAI GPT model)"),FEe.forEach(t),dMo=i(L),i1=n(L,"LI",{});var CEe=s(i1);gte=n(CEe,"STRONG",{});var OWr=s(gte);cMo=r(OWr,"perceiver"),OWr.forEach(t),fMo=r(CEe," \u2014 "),CI=n(CEe,"A",{href:!0});var XWr=s(CI);mMo=r(XWr,"PerceiverForSequenceClassification"),XWr.forEach(t),gMo=r(CEe," (Perceiver model)"),CEe.forEach(t),hMo=i(L),d1=n(L,"LI",{});var MEe=s(d1);hte=n(MEe,"STRONG",{});var VWr=s(hte);pMo=r(VWr,"plbart"),VWr.forEach(t),_Mo=r(MEe," \u2014 "),MI=n(MEe,"A",{href:!0});var zWr=s(MI);uMo=r(zWr,"PLBartForSequenceClassification"),zWr.forEach(t),bMo=r(MEe," (PLBart model)"),MEe.forEach(t),vMo=i(L),c1=n(L,"LI",{});var EEe=s(c1);pte=n(EEe,"STRONG",{});var WWr=s(pte);TMo=r(WWr,"qdqbert"),WWr.forEach(t),FMo=r(EEe," \u2014 "),EI=n(EEe,"A",{href:!0});var QWr=s(EI);CMo=r(QWr,"QDQBertForSequenceClassification"),QWr.forEach(t),MMo=r(EEe," (QDQBert model)"),EEe.forEach(t),EMo=i(L),f1=n(L,"LI",{});var yEe=s(f1);_te=n(yEe,"STRONG",{});var HWr=s(_te);yMo=r(HWr,"reformer"),HWr.forEach(t),wMo=r(yEe," \u2014 "),yI=n(yEe,"A",{href:!0});var UWr=s(yI);AMo=r(UWr,"ReformerForSequenceClassification"),UWr.forEach(t),LMo=r(yEe," (Reformer model)"),yEe.forEach(t),BMo=i(L),m1=n(L,"LI",{});var wEe=s(m1);ute=n(wEe,"STRONG",{});var JWr=s(ute);xMo=r(JWr,"rembert"),JWr.forEach(t),kMo=r(wEe," \u2014 "),wI=n(wEe,"A",{href:!0});var YWr=s(wI);RMo=r(YWr,"RemBertForSequenceClassification"),YWr.forEach(t),SMo=r(wEe," (RemBERT model)"),wEe.forEach(t),PMo=i(L),g1=n(L,"LI",{});var AEe=s(g1);bte=n(AEe,"STRONG",{});var KWr=s(bte);$Mo=r(KWr,"roberta"),KWr.forEach(t),IMo=r(AEe," \u2014 "),AI=n(AEe,"A",{href:!0});var ZWr=s(AI);DMo=r(ZWr,"RobertaForSequenceClassification"),ZWr.forEach(t),jMo=r(AEe," (RoBERTa model)"),AEe.forEach(t),NMo=i(L),h1=n(L,"LI",{});var LEe=s(h1);vte=n(LEe,"STRONG",{});var eQr=s(vte);qMo=r(eQr,"roformer"),eQr.forEach(t),GMo=r(LEe," \u2014 "),LI=n(LEe,"A",{href:!0});var oQr=s(LI);OMo=r(oQr,"RoFormerForSequenceClassification"),oQr.forEach(t),XMo=r(LEe," (RoFormer model)"),LEe.forEach(t),VMo=i(L),p1=n(L,"LI",{});var BEe=s(p1);Tte=n(BEe,"STRONG",{});var rQr=s(Tte);zMo=r(rQr,"squeezebert"),rQr.forEach(t),WMo=r(BEe," \u2014 "),BI=n(BEe,"A",{href:!0});var tQr=s(BI);QMo=r(tQr,"SqueezeBertForSequenceClassification"),tQr.forEach(t),HMo=r(BEe," (SqueezeBERT model)"),BEe.forEach(t),UMo=i(L),_1=n(L,"LI",{});var xEe=s(_1);Fte=n(xEe,"STRONG",{});var aQr=s(Fte);JMo=r(aQr,"tapas"),aQr.forEach(t),YMo=r(xEe," \u2014 "),xI=n(xEe,"A",{href:!0});var nQr=s(xI);KMo=r(nQr,"TapasForSequenceClassification"),nQr.forEach(t),ZMo=r(xEe," (TAPAS model)"),xEe.forEach(t),e4o=i(L),u1=n(L,"LI",{});var kEe=s(u1);Cte=n(kEe,"STRONG",{});var sQr=s(Cte);o4o=r(sQr,"transfo-xl"),sQr.forEach(t),r4o=r(kEe," \u2014 "),kI=n(kEe,"A",{href:!0});var lQr=s(kI);t4o=r(lQr,"TransfoXLForSequenceClassification"),lQr.forEach(t),a4o=r(kEe," (Transformer-XL model)"),kEe.forEach(t),n4o=i(L),b1=n(L,"LI",{});var REe=s(b1);Mte=n(REe,"STRONG",{});var iQr=s(Mte);s4o=r(iQr,"xlm"),iQr.forEach(t),l4o=r(REe," \u2014 "),RI=n(REe,"A",{href:!0});var dQr=s(RI);i4o=r(dQr,"XLMForSequenceClassification"),dQr.forEach(t),d4o=r(REe," (XLM model)"),REe.forEach(t),c4o=i(L),v1=n(L,"LI",{});var SEe=s(v1);Ete=n(SEe,"STRONG",{});var cQr=s(Ete);f4o=r(cQr,"xlm-roberta"),cQr.forEach(t),m4o=r(SEe," \u2014 "),SI=n(SEe,"A",{href:!0});var fQr=s(SI);g4o=r(fQr,"XLMRobertaForSequenceClassification"),fQr.forEach(t),h4o=r(SEe," (XLM-RoBERTa model)"),SEe.forEach(t),p4o=i(L),T1=n(L,"LI",{});var PEe=s(T1);yte=n(PEe,"STRONG",{});var mQr=s(yte);_4o=r(mQr,"xlm-roberta-xl"),mQr.forEach(t),u4o=r(PEe," \u2014 "),PI=n(PEe,"A",{href:!0});var gQr=s(PI);b4o=r(gQr,"XLMRobertaXLForSequenceClassification"),gQr.forEach(t),v4o=r(PEe," (XLM-RoBERTa-XL model)"),PEe.forEach(t),T4o=i(L),F1=n(L,"LI",{});var $Ee=s(F1);wte=n($Ee,"STRONG",{});var hQr=s(wte);F4o=r(hQr,"xlnet"),hQr.forEach(t),C4o=r($Ee," \u2014 "),$I=n($Ee,"A",{href:!0});var pQr=s($I);M4o=r(pQr,"XLNetForSequenceClassification"),pQr.forEach(t),E4o=r($Ee," (XLNet model)"),$Ee.forEach(t),y4o=i(L),C1=n(L,"LI",{});var IEe=s(C1);Ate=n(IEe,"STRONG",{});var _Qr=s(Ate);w4o=r(_Qr,"yoso"),_Qr.forEach(t),A4o=r(IEe," \u2014 "),II=n(IEe,"A",{href:!0});var uQr=s(II);L4o=r(uQr,"YosoForSequenceClassification"),uQr.forEach(t),B4o=r(IEe," (YOSO model)"),IEe.forEach(t),L.forEach(t),x4o=i(Ot),M1=n(Ot,"P",{});var DEe=s(M1);k4o=r(DEe,"The model is set in evaluation mode by default using "),Lte=n(DEe,"CODE",{});var bQr=s(Lte);R4o=r(bQr,"model.eval()"),bQr.forEach(t),S4o=r(DEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bte=n(DEe,"CODE",{});var vQr=s(Bte);P4o=r(vQr,"model.train()"),vQr.forEach(t),DEe.forEach(t),$4o=i(Ot),xte=n(Ot,"P",{});var TQr=s(xte);I4o=r(TQr,"Examples:"),TQr.forEach(t),D4o=i(Ot),m(YE.$$.fragment,Ot),Ot.forEach(t),Ys.forEach(t),BBe=i(c),ld=n(c,"H2",{class:!0});var Ike=s(ld);E1=n(Ike,"A",{id:!0,class:!0,href:!0});var FQr=s(E1);kte=n(FQr,"SPAN",{});var CQr=s(kte);m(KE.$$.fragment,CQr),CQr.forEach(t),FQr.forEach(t),j4o=i(Ike),Rte=n(Ike,"SPAN",{});var MQr=s(Rte);N4o=r(MQr,"AutoModelForMultipleChoice"),MQr.forEach(t),Ike.forEach(t),xBe=i(c),or=n(c,"DIV",{class:!0});var Zs=s(or);m(ZE.$$.fragment,Zs),q4o=i(Zs),id=n(Zs,"P",{});var dz=s(id);G4o=r(dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Ste=n(dz,"CODE",{});var EQr=s(Ste);O4o=r(EQr,"from_pretrained()"),EQr.forEach(t),X4o=r(dz,"class method or the "),Pte=n(dz,"CODE",{});var yQr=s(Pte);V4o=r(yQr,"from_config()"),yQr.forEach(t),z4o=r(dz,`class
method.`),dz.forEach(t),W4o=i(Zs),e3=n(Zs,"P",{});var Dke=s(e3);Q4o=r(Dke,"This class cannot be instantiated directly using "),$te=n(Dke,"CODE",{});var wQr=s($te);H4o=r(wQr,"__init__()"),wQr.forEach(t),U4o=r(Dke," (throws an error)."),Dke.forEach(t),J4o=i(Zs),Hr=n(Zs,"DIV",{class:!0});var el=s(Hr);m(o3.$$.fragment,el),Y4o=i(el),Ite=n(el,"P",{});var AQr=s(Ite);K4o=r(AQr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),AQr.forEach(t),Z4o=i(el),dd=n(el,"P",{});var cz=s(dd);eEo=r(cz,`Note:
Loading a model from its configuration file does `),Dte=n(cz,"STRONG",{});var LQr=s(Dte);oEo=r(LQr,"not"),LQr.forEach(t),rEo=r(cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),jte=n(cz,"CODE",{});var BQr=s(jte);tEo=r(BQr,"from_pretrained()"),BQr.forEach(t),aEo=r(cz,"to load the model weights."),cz.forEach(t),nEo=i(el),Nte=n(el,"P",{});var xQr=s(Nte);sEo=r(xQr,"Examples:"),xQr.forEach(t),lEo=i(el),m(r3.$$.fragment,el),el.forEach(t),iEo=i(Zs),Xe=n(Zs,"DIV",{class:!0});var Xt=s(Xe);m(t3.$$.fragment,Xt),dEo=i(Xt),qte=n(Xt,"P",{});var kQr=s(qte);cEo=r(kQr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),kQr.forEach(t),fEo=i(Xt),za=n(Xt,"P",{});var qM=s(za);mEo=r(qM,"The model class to instantiate is selected based on the "),Gte=n(qM,"CODE",{});var RQr=s(Gte);gEo=r(RQr,"model_type"),RQr.forEach(t),hEo=r(qM,` property of the config object (either
passed as an argument or loaded from `),Ote=n(qM,"CODE",{});var SQr=s(Ote);pEo=r(SQr,"pretrained_model_name_or_path"),SQr.forEach(t),_Eo=r(qM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xte=n(qM,"CODE",{});var PQr=s(Xte);uEo=r(PQr,"pretrained_model_name_or_path"),PQr.forEach(t),bEo=r(qM,":"),qM.forEach(t),vEo=i(Xt),G=n(Xt,"UL",{});var O=s(G);y1=n(O,"LI",{});var jEe=s(y1);Vte=n(jEe,"STRONG",{});var $Qr=s(Vte);TEo=r($Qr,"albert"),$Qr.forEach(t),FEo=r(jEe," \u2014 "),DI=n(jEe,"A",{href:!0});var IQr=s(DI);CEo=r(IQr,"AlbertForMultipleChoice"),IQr.forEach(t),MEo=r(jEe," (ALBERT model)"),jEe.forEach(t),EEo=i(O),w1=n(O,"LI",{});var NEe=s(w1);zte=n(NEe,"STRONG",{});var DQr=s(zte);yEo=r(DQr,"bert"),DQr.forEach(t),wEo=r(NEe," \u2014 "),jI=n(NEe,"A",{href:!0});var jQr=s(jI);AEo=r(jQr,"BertForMultipleChoice"),jQr.forEach(t),LEo=r(NEe," (BERT model)"),NEe.forEach(t),BEo=i(O),A1=n(O,"LI",{});var qEe=s(A1);Wte=n(qEe,"STRONG",{});var NQr=s(Wte);xEo=r(NQr,"big_bird"),NQr.forEach(t),kEo=r(qEe," \u2014 "),NI=n(qEe,"A",{href:!0});var qQr=s(NI);REo=r(qQr,"BigBirdForMultipleChoice"),qQr.forEach(t),SEo=r(qEe," (BigBird model)"),qEe.forEach(t),PEo=i(O),L1=n(O,"LI",{});var GEe=s(L1);Qte=n(GEe,"STRONG",{});var GQr=s(Qte);$Eo=r(GQr,"camembert"),GQr.forEach(t),IEo=r(GEe," \u2014 "),qI=n(GEe,"A",{href:!0});var OQr=s(qI);DEo=r(OQr,"CamembertForMultipleChoice"),OQr.forEach(t),jEo=r(GEe," (CamemBERT model)"),GEe.forEach(t),NEo=i(O),B1=n(O,"LI",{});var OEe=s(B1);Hte=n(OEe,"STRONG",{});var XQr=s(Hte);qEo=r(XQr,"canine"),XQr.forEach(t),GEo=r(OEe," \u2014 "),GI=n(OEe,"A",{href:!0});var VQr=s(GI);OEo=r(VQr,"CanineForMultipleChoice"),VQr.forEach(t),XEo=r(OEe," (Canine model)"),OEe.forEach(t),VEo=i(O),x1=n(O,"LI",{});var XEe=s(x1);Ute=n(XEe,"STRONG",{});var zQr=s(Ute);zEo=r(zQr,"convbert"),zQr.forEach(t),WEo=r(XEe," \u2014 "),OI=n(XEe,"A",{href:!0});var WQr=s(OI);QEo=r(WQr,"ConvBertForMultipleChoice"),WQr.forEach(t),HEo=r(XEe," (ConvBERT model)"),XEe.forEach(t),UEo=i(O),k1=n(O,"LI",{});var VEe=s(k1);Jte=n(VEe,"STRONG",{});var QQr=s(Jte);JEo=r(QQr,"data2vec-text"),QQr.forEach(t),YEo=r(VEe," \u2014 "),XI=n(VEe,"A",{href:!0});var HQr=s(XI);KEo=r(HQr,"Data2VecTextForMultipleChoice"),HQr.forEach(t),ZEo=r(VEe," (Data2VecText model)"),VEe.forEach(t),e3o=i(O),R1=n(O,"LI",{});var zEe=s(R1);Yte=n(zEe,"STRONG",{});var UQr=s(Yte);o3o=r(UQr,"distilbert"),UQr.forEach(t),r3o=r(zEe," \u2014 "),VI=n(zEe,"A",{href:!0});var JQr=s(VI);t3o=r(JQr,"DistilBertForMultipleChoice"),JQr.forEach(t),a3o=r(zEe," (DistilBERT model)"),zEe.forEach(t),n3o=i(O),S1=n(O,"LI",{});var WEe=s(S1);Kte=n(WEe,"STRONG",{});var YQr=s(Kte);s3o=r(YQr,"electra"),YQr.forEach(t),l3o=r(WEe," \u2014 "),zI=n(WEe,"A",{href:!0});var KQr=s(zI);i3o=r(KQr,"ElectraForMultipleChoice"),KQr.forEach(t),d3o=r(WEe," (ELECTRA model)"),WEe.forEach(t),c3o=i(O),P1=n(O,"LI",{});var QEe=s(P1);Zte=n(QEe,"STRONG",{});var ZQr=s(Zte);f3o=r(ZQr,"flaubert"),ZQr.forEach(t),m3o=r(QEe," \u2014 "),WI=n(QEe,"A",{href:!0});var eHr=s(WI);g3o=r(eHr,"FlaubertForMultipleChoice"),eHr.forEach(t),h3o=r(QEe," (FlauBERT model)"),QEe.forEach(t),p3o=i(O),$1=n(O,"LI",{});var HEe=s($1);eae=n(HEe,"STRONG",{});var oHr=s(eae);_3o=r(oHr,"fnet"),oHr.forEach(t),u3o=r(HEe," \u2014 "),QI=n(HEe,"A",{href:!0});var rHr=s(QI);b3o=r(rHr,"FNetForMultipleChoice"),rHr.forEach(t),v3o=r(HEe," (FNet model)"),HEe.forEach(t),T3o=i(O),I1=n(O,"LI",{});var UEe=s(I1);oae=n(UEe,"STRONG",{});var tHr=s(oae);F3o=r(tHr,"funnel"),tHr.forEach(t),C3o=r(UEe," \u2014 "),HI=n(UEe,"A",{href:!0});var aHr=s(HI);M3o=r(aHr,"FunnelForMultipleChoice"),aHr.forEach(t),E3o=r(UEe," (Funnel Transformer model)"),UEe.forEach(t),y3o=i(O),D1=n(O,"LI",{});var JEe=s(D1);rae=n(JEe,"STRONG",{});var nHr=s(rae);w3o=r(nHr,"ibert"),nHr.forEach(t),A3o=r(JEe," \u2014 "),UI=n(JEe,"A",{href:!0});var sHr=s(UI);L3o=r(sHr,"IBertForMultipleChoice"),sHr.forEach(t),B3o=r(JEe," (I-BERT model)"),JEe.forEach(t),x3o=i(O),j1=n(O,"LI",{});var YEe=s(j1);tae=n(YEe,"STRONG",{});var lHr=s(tae);k3o=r(lHr,"longformer"),lHr.forEach(t),R3o=r(YEe," \u2014 "),JI=n(YEe,"A",{href:!0});var iHr=s(JI);S3o=r(iHr,"LongformerForMultipleChoice"),iHr.forEach(t),P3o=r(YEe," (Longformer model)"),YEe.forEach(t),$3o=i(O),N1=n(O,"LI",{});var KEe=s(N1);aae=n(KEe,"STRONG",{});var dHr=s(aae);I3o=r(dHr,"megatron-bert"),dHr.forEach(t),D3o=r(KEe," \u2014 "),YI=n(KEe,"A",{href:!0});var cHr=s(YI);j3o=r(cHr,"MegatronBertForMultipleChoice"),cHr.forEach(t),N3o=r(KEe," (MegatronBert model)"),KEe.forEach(t),q3o=i(O),q1=n(O,"LI",{});var ZEe=s(q1);nae=n(ZEe,"STRONG",{});var fHr=s(nae);G3o=r(fHr,"mobilebert"),fHr.forEach(t),O3o=r(ZEe," \u2014 "),KI=n(ZEe,"A",{href:!0});var mHr=s(KI);X3o=r(mHr,"MobileBertForMultipleChoice"),mHr.forEach(t),V3o=r(ZEe," (MobileBERT model)"),ZEe.forEach(t),z3o=i(O),G1=n(O,"LI",{});var e3e=s(G1);sae=n(e3e,"STRONG",{});var gHr=s(sae);W3o=r(gHr,"mpnet"),gHr.forEach(t),Q3o=r(e3e," \u2014 "),ZI=n(e3e,"A",{href:!0});var hHr=s(ZI);H3o=r(hHr,"MPNetForMultipleChoice"),hHr.forEach(t),U3o=r(e3e," (MPNet model)"),e3e.forEach(t),J3o=i(O),O1=n(O,"LI",{});var o3e=s(O1);lae=n(o3e,"STRONG",{});var pHr=s(lae);Y3o=r(pHr,"nystromformer"),pHr.forEach(t),K3o=r(o3e," \u2014 "),eD=n(o3e,"A",{href:!0});var _Hr=s(eD);Z3o=r(_Hr,"NystromformerForMultipleChoice"),_Hr.forEach(t),eyo=r(o3e," (Nystromformer model)"),o3e.forEach(t),oyo=i(O),X1=n(O,"LI",{});var r3e=s(X1);iae=n(r3e,"STRONG",{});var uHr=s(iae);ryo=r(uHr,"qdqbert"),uHr.forEach(t),tyo=r(r3e," \u2014 "),oD=n(r3e,"A",{href:!0});var bHr=s(oD);ayo=r(bHr,"QDQBertForMultipleChoice"),bHr.forEach(t),nyo=r(r3e," (QDQBert model)"),r3e.forEach(t),syo=i(O),V1=n(O,"LI",{});var t3e=s(V1);dae=n(t3e,"STRONG",{});var vHr=s(dae);lyo=r(vHr,"rembert"),vHr.forEach(t),iyo=r(t3e," \u2014 "),rD=n(t3e,"A",{href:!0});var THr=s(rD);dyo=r(THr,"RemBertForMultipleChoice"),THr.forEach(t),cyo=r(t3e," (RemBERT model)"),t3e.forEach(t),fyo=i(O),z1=n(O,"LI",{});var a3e=s(z1);cae=n(a3e,"STRONG",{});var FHr=s(cae);myo=r(FHr,"roberta"),FHr.forEach(t),gyo=r(a3e," \u2014 "),tD=n(a3e,"A",{href:!0});var CHr=s(tD);hyo=r(CHr,"RobertaForMultipleChoice"),CHr.forEach(t),pyo=r(a3e," (RoBERTa model)"),a3e.forEach(t),_yo=i(O),W1=n(O,"LI",{});var n3e=s(W1);fae=n(n3e,"STRONG",{});var MHr=s(fae);uyo=r(MHr,"roformer"),MHr.forEach(t),byo=r(n3e," \u2014 "),aD=n(n3e,"A",{href:!0});var EHr=s(aD);vyo=r(EHr,"RoFormerForMultipleChoice"),EHr.forEach(t),Tyo=r(n3e," (RoFormer model)"),n3e.forEach(t),Fyo=i(O),Q1=n(O,"LI",{});var s3e=s(Q1);mae=n(s3e,"STRONG",{});var yHr=s(mae);Cyo=r(yHr,"squeezebert"),yHr.forEach(t),Myo=r(s3e," \u2014 "),nD=n(s3e,"A",{href:!0});var wHr=s(nD);Eyo=r(wHr,"SqueezeBertForMultipleChoice"),wHr.forEach(t),yyo=r(s3e," (SqueezeBERT model)"),s3e.forEach(t),wyo=i(O),H1=n(O,"LI",{});var l3e=s(H1);gae=n(l3e,"STRONG",{});var AHr=s(gae);Ayo=r(AHr,"xlm"),AHr.forEach(t),Lyo=r(l3e," \u2014 "),sD=n(l3e,"A",{href:!0});var LHr=s(sD);Byo=r(LHr,"XLMForMultipleChoice"),LHr.forEach(t),xyo=r(l3e," (XLM model)"),l3e.forEach(t),kyo=i(O),U1=n(O,"LI",{});var i3e=s(U1);hae=n(i3e,"STRONG",{});var BHr=s(hae);Ryo=r(BHr,"xlm-roberta"),BHr.forEach(t),Syo=r(i3e," \u2014 "),lD=n(i3e,"A",{href:!0});var xHr=s(lD);Pyo=r(xHr,"XLMRobertaForMultipleChoice"),xHr.forEach(t),$yo=r(i3e," (XLM-RoBERTa model)"),i3e.forEach(t),Iyo=i(O),J1=n(O,"LI",{});var d3e=s(J1);pae=n(d3e,"STRONG",{});var kHr=s(pae);Dyo=r(kHr,"xlm-roberta-xl"),kHr.forEach(t),jyo=r(d3e," \u2014 "),iD=n(d3e,"A",{href:!0});var RHr=s(iD);Nyo=r(RHr,"XLMRobertaXLForMultipleChoice"),RHr.forEach(t),qyo=r(d3e," (XLM-RoBERTa-XL model)"),d3e.forEach(t),Gyo=i(O),Y1=n(O,"LI",{});var c3e=s(Y1);_ae=n(c3e,"STRONG",{});var SHr=s(_ae);Oyo=r(SHr,"xlnet"),SHr.forEach(t),Xyo=r(c3e," \u2014 "),dD=n(c3e,"A",{href:!0});var PHr=s(dD);Vyo=r(PHr,"XLNetForMultipleChoice"),PHr.forEach(t),zyo=r(c3e," (XLNet model)"),c3e.forEach(t),Wyo=i(O),K1=n(O,"LI",{});var f3e=s(K1);uae=n(f3e,"STRONG",{});var $Hr=s(uae);Qyo=r($Hr,"yoso"),$Hr.forEach(t),Hyo=r(f3e," \u2014 "),cD=n(f3e,"A",{href:!0});var IHr=s(cD);Uyo=r(IHr,"YosoForMultipleChoice"),IHr.forEach(t),Jyo=r(f3e," (YOSO model)"),f3e.forEach(t),O.forEach(t),Yyo=i(Xt),Z1=n(Xt,"P",{});var m3e=s(Z1);Kyo=r(m3e,"The model is set in evaluation mode by default using "),bae=n(m3e,"CODE",{});var DHr=s(bae);Zyo=r(DHr,"model.eval()"),DHr.forEach(t),ewo=r(m3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=n(m3e,"CODE",{});var jHr=s(vae);owo=r(jHr,"model.train()"),jHr.forEach(t),m3e.forEach(t),rwo=i(Xt),Tae=n(Xt,"P",{});var NHr=s(Tae);two=r(NHr,"Examples:"),NHr.forEach(t),awo=i(Xt),m(a3.$$.fragment,Xt),Xt.forEach(t),Zs.forEach(t),kBe=i(c),cd=n(c,"H2",{class:!0});var jke=s(cd);eb=n(jke,"A",{id:!0,class:!0,href:!0});var qHr=s(eb);Fae=n(qHr,"SPAN",{});var GHr=s(Fae);m(n3.$$.fragment,GHr),GHr.forEach(t),qHr.forEach(t),nwo=i(jke),Cae=n(jke,"SPAN",{});var OHr=s(Cae);swo=r(OHr,"AutoModelForNextSentencePrediction"),OHr.forEach(t),jke.forEach(t),RBe=i(c),rr=n(c,"DIV",{class:!0});var ol=s(rr);m(s3.$$.fragment,ol),lwo=i(ol),fd=n(ol,"P",{});var fz=s(fd);iwo=r(fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Mae=n(fz,"CODE",{});var XHr=s(Mae);dwo=r(XHr,"from_pretrained()"),XHr.forEach(t),cwo=r(fz,"class method or the "),Eae=n(fz,"CODE",{});var VHr=s(Eae);fwo=r(VHr,"from_config()"),VHr.forEach(t),mwo=r(fz,`class
method.`),fz.forEach(t),gwo=i(ol),l3=n(ol,"P",{});var Nke=s(l3);hwo=r(Nke,"This class cannot be instantiated directly using "),yae=n(Nke,"CODE",{});var zHr=s(yae);pwo=r(zHr,"__init__()"),zHr.forEach(t),_wo=r(Nke," (throws an error)."),Nke.forEach(t),uwo=i(ol),Ur=n(ol,"DIV",{class:!0});var rl=s(Ur);m(i3.$$.fragment,rl),bwo=i(rl),wae=n(rl,"P",{});var WHr=s(wae);vwo=r(WHr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),WHr.forEach(t),Two=i(rl),md=n(rl,"P",{});var mz=s(md);Fwo=r(mz,`Note:
Loading a model from its configuration file does `),Aae=n(mz,"STRONG",{});var QHr=s(Aae);Cwo=r(QHr,"not"),QHr.forEach(t),Mwo=r(mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=n(mz,"CODE",{});var HHr=s(Lae);Ewo=r(HHr,"from_pretrained()"),HHr.forEach(t),ywo=r(mz,"to load the model weights."),mz.forEach(t),wwo=i(rl),Bae=n(rl,"P",{});var UHr=s(Bae);Awo=r(UHr,"Examples:"),UHr.forEach(t),Lwo=i(rl),m(d3.$$.fragment,rl),rl.forEach(t),Bwo=i(ol),Ve=n(ol,"DIV",{class:!0});var Vt=s(Ve);m(c3.$$.fragment,Vt),xwo=i(Vt),xae=n(Vt,"P",{});var JHr=s(xae);kwo=r(JHr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),JHr.forEach(t),Rwo=i(Vt),Wa=n(Vt,"P",{});var GM=s(Wa);Swo=r(GM,"The model class to instantiate is selected based on the "),kae=n(GM,"CODE",{});var YHr=s(kae);Pwo=r(YHr,"model_type"),YHr.forEach(t),$wo=r(GM,` property of the config object (either
passed as an argument or loaded from `),Rae=n(GM,"CODE",{});var KHr=s(Rae);Iwo=r(KHr,"pretrained_model_name_or_path"),KHr.forEach(t),Dwo=r(GM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=n(GM,"CODE",{});var ZHr=s(Sae);jwo=r(ZHr,"pretrained_model_name_or_path"),ZHr.forEach(t),Nwo=r(GM,":"),GM.forEach(t),qwo=i(Vt),na=n(Vt,"UL",{});var tl=s(na);ob=n(tl,"LI",{});var g3e=s(ob);Pae=n(g3e,"STRONG",{});var eUr=s(Pae);Gwo=r(eUr,"bert"),eUr.forEach(t),Owo=r(g3e," \u2014 "),fD=n(g3e,"A",{href:!0});var oUr=s(fD);Xwo=r(oUr,"BertForNextSentencePrediction"),oUr.forEach(t),Vwo=r(g3e," (BERT model)"),g3e.forEach(t),zwo=i(tl),rb=n(tl,"LI",{});var h3e=s(rb);$ae=n(h3e,"STRONG",{});var rUr=s($ae);Wwo=r(rUr,"fnet"),rUr.forEach(t),Qwo=r(h3e," \u2014 "),mD=n(h3e,"A",{href:!0});var tUr=s(mD);Hwo=r(tUr,"FNetForNextSentencePrediction"),tUr.forEach(t),Uwo=r(h3e," (FNet model)"),h3e.forEach(t),Jwo=i(tl),tb=n(tl,"LI",{});var p3e=s(tb);Iae=n(p3e,"STRONG",{});var aUr=s(Iae);Ywo=r(aUr,"megatron-bert"),aUr.forEach(t),Kwo=r(p3e," \u2014 "),gD=n(p3e,"A",{href:!0});var nUr=s(gD);Zwo=r(nUr,"MegatronBertForNextSentencePrediction"),nUr.forEach(t),e6o=r(p3e," (MegatronBert model)"),p3e.forEach(t),o6o=i(tl),ab=n(tl,"LI",{});var _3e=s(ab);Dae=n(_3e,"STRONG",{});var sUr=s(Dae);r6o=r(sUr,"mobilebert"),sUr.forEach(t),t6o=r(_3e," \u2014 "),hD=n(_3e,"A",{href:!0});var lUr=s(hD);a6o=r(lUr,"MobileBertForNextSentencePrediction"),lUr.forEach(t),n6o=r(_3e," (MobileBERT model)"),_3e.forEach(t),s6o=i(tl),nb=n(tl,"LI",{});var u3e=s(nb);jae=n(u3e,"STRONG",{});var iUr=s(jae);l6o=r(iUr,"qdqbert"),iUr.forEach(t),i6o=r(u3e," \u2014 "),pD=n(u3e,"A",{href:!0});var dUr=s(pD);d6o=r(dUr,"QDQBertForNextSentencePrediction"),dUr.forEach(t),c6o=r(u3e," (QDQBert model)"),u3e.forEach(t),tl.forEach(t),f6o=i(Vt),sb=n(Vt,"P",{});var b3e=s(sb);m6o=r(b3e,"The model is set in evaluation mode by default using "),Nae=n(b3e,"CODE",{});var cUr=s(Nae);g6o=r(cUr,"model.eval()"),cUr.forEach(t),h6o=r(b3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),qae=n(b3e,"CODE",{});var fUr=s(qae);p6o=r(fUr,"model.train()"),fUr.forEach(t),b3e.forEach(t),_6o=i(Vt),Gae=n(Vt,"P",{});var mUr=s(Gae);u6o=r(mUr,"Examples:"),mUr.forEach(t),b6o=i(Vt),m(f3.$$.fragment,Vt),Vt.forEach(t),ol.forEach(t),SBe=i(c),gd=n(c,"H2",{class:!0});var qke=s(gd);lb=n(qke,"A",{id:!0,class:!0,href:!0});var gUr=s(lb);Oae=n(gUr,"SPAN",{});var hUr=s(Oae);m(m3.$$.fragment,hUr),hUr.forEach(t),gUr.forEach(t),v6o=i(qke),Xae=n(qke,"SPAN",{});var pUr=s(Xae);T6o=r(pUr,"AutoModelForTokenClassification"),pUr.forEach(t),qke.forEach(t),PBe=i(c),tr=n(c,"DIV",{class:!0});var al=s(tr);m(g3.$$.fragment,al),F6o=i(al),hd=n(al,"P",{});var gz=s(hd);C6o=r(gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Vae=n(gz,"CODE",{});var _Ur=s(Vae);M6o=r(_Ur,"from_pretrained()"),_Ur.forEach(t),E6o=r(gz,"class method or the "),zae=n(gz,"CODE",{});var uUr=s(zae);y6o=r(uUr,"from_config()"),uUr.forEach(t),w6o=r(gz,`class
method.`),gz.forEach(t),A6o=i(al),h3=n(al,"P",{});var Gke=s(h3);L6o=r(Gke,"This class cannot be instantiated directly using "),Wae=n(Gke,"CODE",{});var bUr=s(Wae);B6o=r(bUr,"__init__()"),bUr.forEach(t),x6o=r(Gke," (throws an error)."),Gke.forEach(t),k6o=i(al),Jr=n(al,"DIV",{class:!0});var nl=s(Jr);m(p3.$$.fragment,nl),R6o=i(nl),Qae=n(nl,"P",{});var vUr=s(Qae);S6o=r(vUr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),vUr.forEach(t),P6o=i(nl),pd=n(nl,"P",{});var hz=s(pd);$6o=r(hz,`Note:
Loading a model from its configuration file does `),Hae=n(hz,"STRONG",{});var TUr=s(Hae);I6o=r(TUr,"not"),TUr.forEach(t),D6o=r(hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uae=n(hz,"CODE",{});var FUr=s(Uae);j6o=r(FUr,"from_pretrained()"),FUr.forEach(t),N6o=r(hz,"to load the model weights."),hz.forEach(t),q6o=i(nl),Jae=n(nl,"P",{});var CUr=s(Jae);G6o=r(CUr,"Examples:"),CUr.forEach(t),O6o=i(nl),m(_3.$$.fragment,nl),nl.forEach(t),X6o=i(al),ze=n(al,"DIV",{class:!0});var zt=s(ze);m(u3.$$.fragment,zt),V6o=i(zt),Yae=n(zt,"P",{});var MUr=s(Yae);z6o=r(MUr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),MUr.forEach(t),W6o=i(zt),Qa=n(zt,"P",{});var OM=s(Qa);Q6o=r(OM,"The model class to instantiate is selected based on the "),Kae=n(OM,"CODE",{});var EUr=s(Kae);H6o=r(EUr,"model_type"),EUr.forEach(t),U6o=r(OM,` property of the config object (either
passed as an argument or loaded from `),Zae=n(OM,"CODE",{});var yUr=s(Zae);J6o=r(yUr,"pretrained_model_name_or_path"),yUr.forEach(t),Y6o=r(OM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ene=n(OM,"CODE",{});var wUr=s(ene);K6o=r(wUr,"pretrained_model_name_or_path"),wUr.forEach(t),Z6o=r(OM,":"),OM.forEach(t),eAo=i(zt),N=n(zt,"UL",{});var q=s(N);ib=n(q,"LI",{});var v3e=s(ib);one=n(v3e,"STRONG",{});var AUr=s(one);oAo=r(AUr,"albert"),AUr.forEach(t),rAo=r(v3e," \u2014 "),_D=n(v3e,"A",{href:!0});var LUr=s(_D);tAo=r(LUr,"AlbertForTokenClassification"),LUr.forEach(t),aAo=r(v3e," (ALBERT model)"),v3e.forEach(t),nAo=i(q),db=n(q,"LI",{});var T3e=s(db);rne=n(T3e,"STRONG",{});var BUr=s(rne);sAo=r(BUr,"bert"),BUr.forEach(t),lAo=r(T3e," \u2014 "),uD=n(T3e,"A",{href:!0});var xUr=s(uD);iAo=r(xUr,"BertForTokenClassification"),xUr.forEach(t),dAo=r(T3e," (BERT model)"),T3e.forEach(t),cAo=i(q),cb=n(q,"LI",{});var F3e=s(cb);tne=n(F3e,"STRONG",{});var kUr=s(tne);fAo=r(kUr,"big_bird"),kUr.forEach(t),mAo=r(F3e," \u2014 "),bD=n(F3e,"A",{href:!0});var RUr=s(bD);gAo=r(RUr,"BigBirdForTokenClassification"),RUr.forEach(t),hAo=r(F3e," (BigBird model)"),F3e.forEach(t),pAo=i(q),fb=n(q,"LI",{});var C3e=s(fb);ane=n(C3e,"STRONG",{});var SUr=s(ane);_Ao=r(SUr,"camembert"),SUr.forEach(t),uAo=r(C3e," \u2014 "),vD=n(C3e,"A",{href:!0});var PUr=s(vD);bAo=r(PUr,"CamembertForTokenClassification"),PUr.forEach(t),vAo=r(C3e," (CamemBERT model)"),C3e.forEach(t),TAo=i(q),mb=n(q,"LI",{});var M3e=s(mb);nne=n(M3e,"STRONG",{});var $Ur=s(nne);FAo=r($Ur,"canine"),$Ur.forEach(t),CAo=r(M3e," \u2014 "),TD=n(M3e,"A",{href:!0});var IUr=s(TD);MAo=r(IUr,"CanineForTokenClassification"),IUr.forEach(t),EAo=r(M3e," (Canine model)"),M3e.forEach(t),yAo=i(q),gb=n(q,"LI",{});var E3e=s(gb);sne=n(E3e,"STRONG",{});var DUr=s(sne);wAo=r(DUr,"convbert"),DUr.forEach(t),AAo=r(E3e," \u2014 "),FD=n(E3e,"A",{href:!0});var jUr=s(FD);LAo=r(jUr,"ConvBertForTokenClassification"),jUr.forEach(t),BAo=r(E3e," (ConvBERT model)"),E3e.forEach(t),xAo=i(q),hb=n(q,"LI",{});var y3e=s(hb);lne=n(y3e,"STRONG",{});var NUr=s(lne);kAo=r(NUr,"data2vec-text"),NUr.forEach(t),RAo=r(y3e," \u2014 "),CD=n(y3e,"A",{href:!0});var qUr=s(CD);SAo=r(qUr,"Data2VecTextForTokenClassification"),qUr.forEach(t),PAo=r(y3e," (Data2VecText model)"),y3e.forEach(t),$Ao=i(q),pb=n(q,"LI",{});var w3e=s(pb);ine=n(w3e,"STRONG",{});var GUr=s(ine);IAo=r(GUr,"deberta"),GUr.forEach(t),DAo=r(w3e," \u2014 "),MD=n(w3e,"A",{href:!0});var OUr=s(MD);jAo=r(OUr,"DebertaForTokenClassification"),OUr.forEach(t),NAo=r(w3e," (DeBERTa model)"),w3e.forEach(t),qAo=i(q),_b=n(q,"LI",{});var A3e=s(_b);dne=n(A3e,"STRONG",{});var XUr=s(dne);GAo=r(XUr,"deberta-v2"),XUr.forEach(t),OAo=r(A3e," \u2014 "),ED=n(A3e,"A",{href:!0});var VUr=s(ED);XAo=r(VUr,"DebertaV2ForTokenClassification"),VUr.forEach(t),VAo=r(A3e," (DeBERTa-v2 model)"),A3e.forEach(t),zAo=i(q),ub=n(q,"LI",{});var L3e=s(ub);cne=n(L3e,"STRONG",{});var zUr=s(cne);WAo=r(zUr,"distilbert"),zUr.forEach(t),QAo=r(L3e," \u2014 "),yD=n(L3e,"A",{href:!0});var WUr=s(yD);HAo=r(WUr,"DistilBertForTokenClassification"),WUr.forEach(t),UAo=r(L3e," (DistilBERT model)"),L3e.forEach(t),JAo=i(q),bb=n(q,"LI",{});var B3e=s(bb);fne=n(B3e,"STRONG",{});var QUr=s(fne);YAo=r(QUr,"electra"),QUr.forEach(t),KAo=r(B3e," \u2014 "),wD=n(B3e,"A",{href:!0});var HUr=s(wD);ZAo=r(HUr,"ElectraForTokenClassification"),HUr.forEach(t),eLo=r(B3e," (ELECTRA model)"),B3e.forEach(t),oLo=i(q),vb=n(q,"LI",{});var x3e=s(vb);mne=n(x3e,"STRONG",{});var UUr=s(mne);rLo=r(UUr,"flaubert"),UUr.forEach(t),tLo=r(x3e," \u2014 "),AD=n(x3e,"A",{href:!0});var JUr=s(AD);aLo=r(JUr,"FlaubertForTokenClassification"),JUr.forEach(t),nLo=r(x3e," (FlauBERT model)"),x3e.forEach(t),sLo=i(q),Tb=n(q,"LI",{});var k3e=s(Tb);gne=n(k3e,"STRONG",{});var YUr=s(gne);lLo=r(YUr,"fnet"),YUr.forEach(t),iLo=r(k3e," \u2014 "),LD=n(k3e,"A",{href:!0});var KUr=s(LD);dLo=r(KUr,"FNetForTokenClassification"),KUr.forEach(t),cLo=r(k3e," (FNet model)"),k3e.forEach(t),fLo=i(q),Fb=n(q,"LI",{});var R3e=s(Fb);hne=n(R3e,"STRONG",{});var ZUr=s(hne);mLo=r(ZUr,"funnel"),ZUr.forEach(t),gLo=r(R3e," \u2014 "),BD=n(R3e,"A",{href:!0});var eJr=s(BD);hLo=r(eJr,"FunnelForTokenClassification"),eJr.forEach(t),pLo=r(R3e," (Funnel Transformer model)"),R3e.forEach(t),_Lo=i(q),Cb=n(q,"LI",{});var S3e=s(Cb);pne=n(S3e,"STRONG",{});var oJr=s(pne);uLo=r(oJr,"gpt2"),oJr.forEach(t),bLo=r(S3e," \u2014 "),xD=n(S3e,"A",{href:!0});var rJr=s(xD);vLo=r(rJr,"GPT2ForTokenClassification"),rJr.forEach(t),TLo=r(S3e," (OpenAI GPT-2 model)"),S3e.forEach(t),FLo=i(q),Mb=n(q,"LI",{});var P3e=s(Mb);_ne=n(P3e,"STRONG",{});var tJr=s(_ne);CLo=r(tJr,"ibert"),tJr.forEach(t),MLo=r(P3e," \u2014 "),kD=n(P3e,"A",{href:!0});var aJr=s(kD);ELo=r(aJr,"IBertForTokenClassification"),aJr.forEach(t),yLo=r(P3e," (I-BERT model)"),P3e.forEach(t),wLo=i(q),Eb=n(q,"LI",{});var $3e=s(Eb);une=n($3e,"STRONG",{});var nJr=s(une);ALo=r(nJr,"layoutlm"),nJr.forEach(t),LLo=r($3e," \u2014 "),RD=n($3e,"A",{href:!0});var sJr=s(RD);BLo=r(sJr,"LayoutLMForTokenClassification"),sJr.forEach(t),xLo=r($3e," (LayoutLM model)"),$3e.forEach(t),kLo=i(q),yb=n(q,"LI",{});var I3e=s(yb);bne=n(I3e,"STRONG",{});var lJr=s(bne);RLo=r(lJr,"layoutlmv2"),lJr.forEach(t),SLo=r(I3e," \u2014 "),SD=n(I3e,"A",{href:!0});var iJr=s(SD);PLo=r(iJr,"LayoutLMv2ForTokenClassification"),iJr.forEach(t),$Lo=r(I3e," (LayoutLMv2 model)"),I3e.forEach(t),ILo=i(q),wb=n(q,"LI",{});var D3e=s(wb);vne=n(D3e,"STRONG",{});var dJr=s(vne);DLo=r(dJr,"longformer"),dJr.forEach(t),jLo=r(D3e," \u2014 "),PD=n(D3e,"A",{href:!0});var cJr=s(PD);NLo=r(cJr,"LongformerForTokenClassification"),cJr.forEach(t),qLo=r(D3e," (Longformer model)"),D3e.forEach(t),GLo=i(q),Ab=n(q,"LI",{});var j3e=s(Ab);Tne=n(j3e,"STRONG",{});var fJr=s(Tne);OLo=r(fJr,"megatron-bert"),fJr.forEach(t),XLo=r(j3e," \u2014 "),$D=n(j3e,"A",{href:!0});var mJr=s($D);VLo=r(mJr,"MegatronBertForTokenClassification"),mJr.forEach(t),zLo=r(j3e," (MegatronBert model)"),j3e.forEach(t),WLo=i(q),Lb=n(q,"LI",{});var N3e=s(Lb);Fne=n(N3e,"STRONG",{});var gJr=s(Fne);QLo=r(gJr,"mobilebert"),gJr.forEach(t),HLo=r(N3e," \u2014 "),ID=n(N3e,"A",{href:!0});var hJr=s(ID);ULo=r(hJr,"MobileBertForTokenClassification"),hJr.forEach(t),JLo=r(N3e," (MobileBERT model)"),N3e.forEach(t),YLo=i(q),Bb=n(q,"LI",{});var q3e=s(Bb);Cne=n(q3e,"STRONG",{});var pJr=s(Cne);KLo=r(pJr,"mpnet"),pJr.forEach(t),ZLo=r(q3e," \u2014 "),DD=n(q3e,"A",{href:!0});var _Jr=s(DD);e8o=r(_Jr,"MPNetForTokenClassification"),_Jr.forEach(t),o8o=r(q3e," (MPNet model)"),q3e.forEach(t),r8o=i(q),xb=n(q,"LI",{});var G3e=s(xb);Mne=n(G3e,"STRONG",{});var uJr=s(Mne);t8o=r(uJr,"nystromformer"),uJr.forEach(t),a8o=r(G3e," \u2014 "),jD=n(G3e,"A",{href:!0});var bJr=s(jD);n8o=r(bJr,"NystromformerForTokenClassification"),bJr.forEach(t),s8o=r(G3e," (Nystromformer model)"),G3e.forEach(t),l8o=i(q),kb=n(q,"LI",{});var O3e=s(kb);Ene=n(O3e,"STRONG",{});var vJr=s(Ene);i8o=r(vJr,"qdqbert"),vJr.forEach(t),d8o=r(O3e," \u2014 "),ND=n(O3e,"A",{href:!0});var TJr=s(ND);c8o=r(TJr,"QDQBertForTokenClassification"),TJr.forEach(t),f8o=r(O3e," (QDQBert model)"),O3e.forEach(t),m8o=i(q),Rb=n(q,"LI",{});var X3e=s(Rb);yne=n(X3e,"STRONG",{});var FJr=s(yne);g8o=r(FJr,"rembert"),FJr.forEach(t),h8o=r(X3e," \u2014 "),qD=n(X3e,"A",{href:!0});var CJr=s(qD);p8o=r(CJr,"RemBertForTokenClassification"),CJr.forEach(t),_8o=r(X3e," (RemBERT model)"),X3e.forEach(t),u8o=i(q),Sb=n(q,"LI",{});var V3e=s(Sb);wne=n(V3e,"STRONG",{});var MJr=s(wne);b8o=r(MJr,"roberta"),MJr.forEach(t),v8o=r(V3e," \u2014 "),GD=n(V3e,"A",{href:!0});var EJr=s(GD);T8o=r(EJr,"RobertaForTokenClassification"),EJr.forEach(t),F8o=r(V3e," (RoBERTa model)"),V3e.forEach(t),C8o=i(q),Pb=n(q,"LI",{});var z3e=s(Pb);Ane=n(z3e,"STRONG",{});var yJr=s(Ane);M8o=r(yJr,"roformer"),yJr.forEach(t),E8o=r(z3e," \u2014 "),OD=n(z3e,"A",{href:!0});var wJr=s(OD);y8o=r(wJr,"RoFormerForTokenClassification"),wJr.forEach(t),w8o=r(z3e," (RoFormer model)"),z3e.forEach(t),A8o=i(q),$b=n(q,"LI",{});var W3e=s($b);Lne=n(W3e,"STRONG",{});var AJr=s(Lne);L8o=r(AJr,"squeezebert"),AJr.forEach(t),B8o=r(W3e," \u2014 "),XD=n(W3e,"A",{href:!0});var LJr=s(XD);x8o=r(LJr,"SqueezeBertForTokenClassification"),LJr.forEach(t),k8o=r(W3e," (SqueezeBERT model)"),W3e.forEach(t),R8o=i(q),Ib=n(q,"LI",{});var Q3e=s(Ib);Bne=n(Q3e,"STRONG",{});var BJr=s(Bne);S8o=r(BJr,"xlm"),BJr.forEach(t),P8o=r(Q3e," \u2014 "),VD=n(Q3e,"A",{href:!0});var xJr=s(VD);$8o=r(xJr,"XLMForTokenClassification"),xJr.forEach(t),I8o=r(Q3e," (XLM model)"),Q3e.forEach(t),D8o=i(q),Db=n(q,"LI",{});var H3e=s(Db);xne=n(H3e,"STRONG",{});var kJr=s(xne);j8o=r(kJr,"xlm-roberta"),kJr.forEach(t),N8o=r(H3e," \u2014 "),zD=n(H3e,"A",{href:!0});var RJr=s(zD);q8o=r(RJr,"XLMRobertaForTokenClassification"),RJr.forEach(t),G8o=r(H3e," (XLM-RoBERTa model)"),H3e.forEach(t),O8o=i(q),jb=n(q,"LI",{});var U3e=s(jb);kne=n(U3e,"STRONG",{});var SJr=s(kne);X8o=r(SJr,"xlm-roberta-xl"),SJr.forEach(t),V8o=r(U3e," \u2014 "),WD=n(U3e,"A",{href:!0});var PJr=s(WD);z8o=r(PJr,"XLMRobertaXLForTokenClassification"),PJr.forEach(t),W8o=r(U3e," (XLM-RoBERTa-XL model)"),U3e.forEach(t),Q8o=i(q),Nb=n(q,"LI",{});var J3e=s(Nb);Rne=n(J3e,"STRONG",{});var $Jr=s(Rne);H8o=r($Jr,"xlnet"),$Jr.forEach(t),U8o=r(J3e," \u2014 "),QD=n(J3e,"A",{href:!0});var IJr=s(QD);J8o=r(IJr,"XLNetForTokenClassification"),IJr.forEach(t),Y8o=r(J3e," (XLNet model)"),J3e.forEach(t),K8o=i(q),qb=n(q,"LI",{});var Y3e=s(qb);Sne=n(Y3e,"STRONG",{});var DJr=s(Sne);Z8o=r(DJr,"yoso"),DJr.forEach(t),e7o=r(Y3e," \u2014 "),HD=n(Y3e,"A",{href:!0});var jJr=s(HD);o7o=r(jJr,"YosoForTokenClassification"),jJr.forEach(t),r7o=r(Y3e," (YOSO model)"),Y3e.forEach(t),q.forEach(t),t7o=i(zt),Gb=n(zt,"P",{});var K3e=s(Gb);a7o=r(K3e,"The model is set in evaluation mode by default using "),Pne=n(K3e,"CODE",{});var NJr=s(Pne);n7o=r(NJr,"model.eval()"),NJr.forEach(t),s7o=r(K3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$ne=n(K3e,"CODE",{});var qJr=s($ne);l7o=r(qJr,"model.train()"),qJr.forEach(t),K3e.forEach(t),i7o=i(zt),Ine=n(zt,"P",{});var GJr=s(Ine);d7o=r(GJr,"Examples:"),GJr.forEach(t),c7o=i(zt),m(b3.$$.fragment,zt),zt.forEach(t),al.forEach(t),$Be=i(c),_d=n(c,"H2",{class:!0});var Oke=s(_d);Ob=n(Oke,"A",{id:!0,class:!0,href:!0});var OJr=s(Ob);Dne=n(OJr,"SPAN",{});var XJr=s(Dne);m(v3.$$.fragment,XJr),XJr.forEach(t),OJr.forEach(t),f7o=i(Oke),jne=n(Oke,"SPAN",{});var VJr=s(jne);m7o=r(VJr,"AutoModelForQuestionAnswering"),VJr.forEach(t),Oke.forEach(t),IBe=i(c),ar=n(c,"DIV",{class:!0});var sl=s(ar);m(T3.$$.fragment,sl),g7o=i(sl),ud=n(sl,"P",{});var pz=s(ud);h7o=r(pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Nne=n(pz,"CODE",{});var zJr=s(Nne);p7o=r(zJr,"from_pretrained()"),zJr.forEach(t),_7o=r(pz,"class method or the "),qne=n(pz,"CODE",{});var WJr=s(qne);u7o=r(WJr,"from_config()"),WJr.forEach(t),b7o=r(pz,`class
method.`),pz.forEach(t),v7o=i(sl),F3=n(sl,"P",{});var Xke=s(F3);T7o=r(Xke,"This class cannot be instantiated directly using "),Gne=n(Xke,"CODE",{});var QJr=s(Gne);F7o=r(QJr,"__init__()"),QJr.forEach(t),C7o=r(Xke," (throws an error)."),Xke.forEach(t),M7o=i(sl),Yr=n(sl,"DIV",{class:!0});var ll=s(Yr);m(C3.$$.fragment,ll),E7o=i(ll),One=n(ll,"P",{});var HJr=s(One);y7o=r(HJr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),HJr.forEach(t),w7o=i(ll),bd=n(ll,"P",{});var _z=s(bd);A7o=r(_z,`Note:
Loading a model from its configuration file does `),Xne=n(_z,"STRONG",{});var UJr=s(Xne);L7o=r(UJr,"not"),UJr.forEach(t),B7o=r(_z,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vne=n(_z,"CODE",{});var JJr=s(Vne);x7o=r(JJr,"from_pretrained()"),JJr.forEach(t),k7o=r(_z,"to load the model weights."),_z.forEach(t),R7o=i(ll),zne=n(ll,"P",{});var YJr=s(zne);S7o=r(YJr,"Examples:"),YJr.forEach(t),P7o=i(ll),m(M3.$$.fragment,ll),ll.forEach(t),$7o=i(sl),We=n(sl,"DIV",{class:!0});var Wt=s(We);m(E3.$$.fragment,Wt),I7o=i(Wt),Wne=n(Wt,"P",{});var KJr=s(Wne);D7o=r(KJr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),KJr.forEach(t),j7o=i(Wt),Ha=n(Wt,"P",{});var XM=s(Ha);N7o=r(XM,"The model class to instantiate is selected based on the "),Qne=n(XM,"CODE",{});var ZJr=s(Qne);q7o=r(ZJr,"model_type"),ZJr.forEach(t),G7o=r(XM,` property of the config object (either
passed as an argument or loaded from `),Hne=n(XM,"CODE",{});var eYr=s(Hne);O7o=r(eYr,"pretrained_model_name_or_path"),eYr.forEach(t),X7o=r(XM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Une=n(XM,"CODE",{});var oYr=s(Une);V7o=r(oYr,"pretrained_model_name_or_path"),oYr.forEach(t),z7o=r(XM,":"),XM.forEach(t),W7o=i(Wt),R=n(Wt,"UL",{});var P=s(R);Xb=n(P,"LI",{});var Z3e=s(Xb);Jne=n(Z3e,"STRONG",{});var rYr=s(Jne);Q7o=r(rYr,"albert"),rYr.forEach(t),H7o=r(Z3e," \u2014 "),UD=n(Z3e,"A",{href:!0});var tYr=s(UD);U7o=r(tYr,"AlbertForQuestionAnswering"),tYr.forEach(t),J7o=r(Z3e," (ALBERT model)"),Z3e.forEach(t),Y7o=i(P),Vb=n(P,"LI",{});var eye=s(Vb);Yne=n(eye,"STRONG",{});var aYr=s(Yne);K7o=r(aYr,"bart"),aYr.forEach(t),Z7o=r(eye," \u2014 "),JD=n(eye,"A",{href:!0});var nYr=s(JD);eBo=r(nYr,"BartForQuestionAnswering"),nYr.forEach(t),oBo=r(eye," (BART model)"),eye.forEach(t),rBo=i(P),zb=n(P,"LI",{});var oye=s(zb);Kne=n(oye,"STRONG",{});var sYr=s(Kne);tBo=r(sYr,"bert"),sYr.forEach(t),aBo=r(oye," \u2014 "),YD=n(oye,"A",{href:!0});var lYr=s(YD);nBo=r(lYr,"BertForQuestionAnswering"),lYr.forEach(t),sBo=r(oye," (BERT model)"),oye.forEach(t),lBo=i(P),Wb=n(P,"LI",{});var rye=s(Wb);Zne=n(rye,"STRONG",{});var iYr=s(Zne);iBo=r(iYr,"big_bird"),iYr.forEach(t),dBo=r(rye," \u2014 "),KD=n(rye,"A",{href:!0});var dYr=s(KD);cBo=r(dYr,"BigBirdForQuestionAnswering"),dYr.forEach(t),fBo=r(rye," (BigBird model)"),rye.forEach(t),mBo=i(P),Qb=n(P,"LI",{});var tye=s(Qb);ese=n(tye,"STRONG",{});var cYr=s(ese);gBo=r(cYr,"bigbird_pegasus"),cYr.forEach(t),hBo=r(tye," \u2014 "),ZD=n(tye,"A",{href:!0});var fYr=s(ZD);pBo=r(fYr,"BigBirdPegasusForQuestionAnswering"),fYr.forEach(t),_Bo=r(tye," (BigBirdPegasus model)"),tye.forEach(t),uBo=i(P),Hb=n(P,"LI",{});var aye=s(Hb);ose=n(aye,"STRONG",{});var mYr=s(ose);bBo=r(mYr,"camembert"),mYr.forEach(t),vBo=r(aye," \u2014 "),ej=n(aye,"A",{href:!0});var gYr=s(ej);TBo=r(gYr,"CamembertForQuestionAnswering"),gYr.forEach(t),FBo=r(aye," (CamemBERT model)"),aye.forEach(t),CBo=i(P),Ub=n(P,"LI",{});var nye=s(Ub);rse=n(nye,"STRONG",{});var hYr=s(rse);MBo=r(hYr,"canine"),hYr.forEach(t),EBo=r(nye," \u2014 "),oj=n(nye,"A",{href:!0});var pYr=s(oj);yBo=r(pYr,"CanineForQuestionAnswering"),pYr.forEach(t),wBo=r(nye," (Canine model)"),nye.forEach(t),ABo=i(P),Jb=n(P,"LI",{});var sye=s(Jb);tse=n(sye,"STRONG",{});var _Yr=s(tse);LBo=r(_Yr,"convbert"),_Yr.forEach(t),BBo=r(sye," \u2014 "),rj=n(sye,"A",{href:!0});var uYr=s(rj);xBo=r(uYr,"ConvBertForQuestionAnswering"),uYr.forEach(t),kBo=r(sye," (ConvBERT model)"),sye.forEach(t),RBo=i(P),Yb=n(P,"LI",{});var lye=s(Yb);ase=n(lye,"STRONG",{});var bYr=s(ase);SBo=r(bYr,"data2vec-text"),bYr.forEach(t),PBo=r(lye," \u2014 "),tj=n(lye,"A",{href:!0});var vYr=s(tj);$Bo=r(vYr,"Data2VecTextForQuestionAnswering"),vYr.forEach(t),IBo=r(lye," (Data2VecText model)"),lye.forEach(t),DBo=i(P),Kb=n(P,"LI",{});var iye=s(Kb);nse=n(iye,"STRONG",{});var TYr=s(nse);jBo=r(TYr,"deberta"),TYr.forEach(t),NBo=r(iye," \u2014 "),aj=n(iye,"A",{href:!0});var FYr=s(aj);qBo=r(FYr,"DebertaForQuestionAnswering"),FYr.forEach(t),GBo=r(iye," (DeBERTa model)"),iye.forEach(t),OBo=i(P),Zb=n(P,"LI",{});var dye=s(Zb);sse=n(dye,"STRONG",{});var CYr=s(sse);XBo=r(CYr,"deberta-v2"),CYr.forEach(t),VBo=r(dye," \u2014 "),nj=n(dye,"A",{href:!0});var MYr=s(nj);zBo=r(MYr,"DebertaV2ForQuestionAnswering"),MYr.forEach(t),WBo=r(dye," (DeBERTa-v2 model)"),dye.forEach(t),QBo=i(P),e5=n(P,"LI",{});var cye=s(e5);lse=n(cye,"STRONG",{});var EYr=s(lse);HBo=r(EYr,"distilbert"),EYr.forEach(t),UBo=r(cye," \u2014 "),sj=n(cye,"A",{href:!0});var yYr=s(sj);JBo=r(yYr,"DistilBertForQuestionAnswering"),yYr.forEach(t),YBo=r(cye," (DistilBERT model)"),cye.forEach(t),KBo=i(P),o5=n(P,"LI",{});var fye=s(o5);ise=n(fye,"STRONG",{});var wYr=s(ise);ZBo=r(wYr,"electra"),wYr.forEach(t),exo=r(fye," \u2014 "),lj=n(fye,"A",{href:!0});var AYr=s(lj);oxo=r(AYr,"ElectraForQuestionAnswering"),AYr.forEach(t),rxo=r(fye," (ELECTRA model)"),fye.forEach(t),txo=i(P),r5=n(P,"LI",{});var mye=s(r5);dse=n(mye,"STRONG",{});var LYr=s(dse);axo=r(LYr,"flaubert"),LYr.forEach(t),nxo=r(mye," \u2014 "),ij=n(mye,"A",{href:!0});var BYr=s(ij);sxo=r(BYr,"FlaubertForQuestionAnsweringSimple"),BYr.forEach(t),lxo=r(mye," (FlauBERT model)"),mye.forEach(t),ixo=i(P),t5=n(P,"LI",{});var gye=s(t5);cse=n(gye,"STRONG",{});var xYr=s(cse);dxo=r(xYr,"fnet"),xYr.forEach(t),cxo=r(gye," \u2014 "),dj=n(gye,"A",{href:!0});var kYr=s(dj);fxo=r(kYr,"FNetForQuestionAnswering"),kYr.forEach(t),mxo=r(gye," (FNet model)"),gye.forEach(t),gxo=i(P),a5=n(P,"LI",{});var hye=s(a5);fse=n(hye,"STRONG",{});var RYr=s(fse);hxo=r(RYr,"funnel"),RYr.forEach(t),pxo=r(hye," \u2014 "),cj=n(hye,"A",{href:!0});var SYr=s(cj);_xo=r(SYr,"FunnelForQuestionAnswering"),SYr.forEach(t),uxo=r(hye," (Funnel Transformer model)"),hye.forEach(t),bxo=i(P),n5=n(P,"LI",{});var pye=s(n5);mse=n(pye,"STRONG",{});var PYr=s(mse);vxo=r(PYr,"gptj"),PYr.forEach(t),Txo=r(pye," \u2014 "),fj=n(pye,"A",{href:!0});var $Yr=s(fj);Fxo=r($Yr,"GPTJForQuestionAnswering"),$Yr.forEach(t),Cxo=r(pye," (GPT-J model)"),pye.forEach(t),Mxo=i(P),s5=n(P,"LI",{});var _ye=s(s5);gse=n(_ye,"STRONG",{});var IYr=s(gse);Exo=r(IYr,"ibert"),IYr.forEach(t),yxo=r(_ye," \u2014 "),mj=n(_ye,"A",{href:!0});var DYr=s(mj);wxo=r(DYr,"IBertForQuestionAnswering"),DYr.forEach(t),Axo=r(_ye," (I-BERT model)"),_ye.forEach(t),Lxo=i(P),l5=n(P,"LI",{});var uye=s(l5);hse=n(uye,"STRONG",{});var jYr=s(hse);Bxo=r(jYr,"layoutlmv2"),jYr.forEach(t),xxo=r(uye," \u2014 "),gj=n(uye,"A",{href:!0});var NYr=s(gj);kxo=r(NYr,"LayoutLMv2ForQuestionAnswering"),NYr.forEach(t),Rxo=r(uye," (LayoutLMv2 model)"),uye.forEach(t),Sxo=i(P),i5=n(P,"LI",{});var bye=s(i5);pse=n(bye,"STRONG",{});var qYr=s(pse);Pxo=r(qYr,"led"),qYr.forEach(t),$xo=r(bye," \u2014 "),hj=n(bye,"A",{href:!0});var GYr=s(hj);Ixo=r(GYr,"LEDForQuestionAnswering"),GYr.forEach(t),Dxo=r(bye," (LED model)"),bye.forEach(t),jxo=i(P),d5=n(P,"LI",{});var vye=s(d5);_se=n(vye,"STRONG",{});var OYr=s(_se);Nxo=r(OYr,"longformer"),OYr.forEach(t),qxo=r(vye," \u2014 "),pj=n(vye,"A",{href:!0});var XYr=s(pj);Gxo=r(XYr,"LongformerForQuestionAnswering"),XYr.forEach(t),Oxo=r(vye," (Longformer model)"),vye.forEach(t),Xxo=i(P),c5=n(P,"LI",{});var Tye=s(c5);use=n(Tye,"STRONG",{});var VYr=s(use);Vxo=r(VYr,"lxmert"),VYr.forEach(t),zxo=r(Tye," \u2014 "),_j=n(Tye,"A",{href:!0});var zYr=s(_j);Wxo=r(zYr,"LxmertForQuestionAnswering"),zYr.forEach(t),Qxo=r(Tye," (LXMERT model)"),Tye.forEach(t),Hxo=i(P),f5=n(P,"LI",{});var Fye=s(f5);bse=n(Fye,"STRONG",{});var WYr=s(bse);Uxo=r(WYr,"mbart"),WYr.forEach(t),Jxo=r(Fye," \u2014 "),uj=n(Fye,"A",{href:!0});var QYr=s(uj);Yxo=r(QYr,"MBartForQuestionAnswering"),QYr.forEach(t),Kxo=r(Fye," (mBART model)"),Fye.forEach(t),Zxo=i(P),m5=n(P,"LI",{});var Cye=s(m5);vse=n(Cye,"STRONG",{});var HYr=s(vse);eko=r(HYr,"megatron-bert"),HYr.forEach(t),oko=r(Cye," \u2014 "),bj=n(Cye,"A",{href:!0});var UYr=s(bj);rko=r(UYr,"MegatronBertForQuestionAnswering"),UYr.forEach(t),tko=r(Cye," (MegatronBert model)"),Cye.forEach(t),ako=i(P),g5=n(P,"LI",{});var Mye=s(g5);Tse=n(Mye,"STRONG",{});var JYr=s(Tse);nko=r(JYr,"mobilebert"),JYr.forEach(t),sko=r(Mye," \u2014 "),vj=n(Mye,"A",{href:!0});var YYr=s(vj);lko=r(YYr,"MobileBertForQuestionAnswering"),YYr.forEach(t),iko=r(Mye," (MobileBERT model)"),Mye.forEach(t),dko=i(P),h5=n(P,"LI",{});var Eye=s(h5);Fse=n(Eye,"STRONG",{});var KYr=s(Fse);cko=r(KYr,"mpnet"),KYr.forEach(t),fko=r(Eye," \u2014 "),Tj=n(Eye,"A",{href:!0});var ZYr=s(Tj);mko=r(ZYr,"MPNetForQuestionAnswering"),ZYr.forEach(t),gko=r(Eye," (MPNet model)"),Eye.forEach(t),hko=i(P),p5=n(P,"LI",{});var yye=s(p5);Cse=n(yye,"STRONG",{});var eKr=s(Cse);pko=r(eKr,"nystromformer"),eKr.forEach(t),_ko=r(yye," \u2014 "),Fj=n(yye,"A",{href:!0});var oKr=s(Fj);uko=r(oKr,"NystromformerForQuestionAnswering"),oKr.forEach(t),bko=r(yye," (Nystromformer model)"),yye.forEach(t),vko=i(P),_5=n(P,"LI",{});var wye=s(_5);Mse=n(wye,"STRONG",{});var rKr=s(Mse);Tko=r(rKr,"qdqbert"),rKr.forEach(t),Fko=r(wye," \u2014 "),Cj=n(wye,"A",{href:!0});var tKr=s(Cj);Cko=r(tKr,"QDQBertForQuestionAnswering"),tKr.forEach(t),Mko=r(wye," (QDQBert model)"),wye.forEach(t),Eko=i(P),u5=n(P,"LI",{});var Aye=s(u5);Ese=n(Aye,"STRONG",{});var aKr=s(Ese);yko=r(aKr,"reformer"),aKr.forEach(t),wko=r(Aye," \u2014 "),Mj=n(Aye,"A",{href:!0});var nKr=s(Mj);Ako=r(nKr,"ReformerForQuestionAnswering"),nKr.forEach(t),Lko=r(Aye," (Reformer model)"),Aye.forEach(t),Bko=i(P),b5=n(P,"LI",{});var Lye=s(b5);yse=n(Lye,"STRONG",{});var sKr=s(yse);xko=r(sKr,"rembert"),sKr.forEach(t),kko=r(Lye," \u2014 "),Ej=n(Lye,"A",{href:!0});var lKr=s(Ej);Rko=r(lKr,"RemBertForQuestionAnswering"),lKr.forEach(t),Sko=r(Lye," (RemBERT model)"),Lye.forEach(t),Pko=i(P),v5=n(P,"LI",{});var Bye=s(v5);wse=n(Bye,"STRONG",{});var iKr=s(wse);$ko=r(iKr,"roberta"),iKr.forEach(t),Iko=r(Bye," \u2014 "),yj=n(Bye,"A",{href:!0});var dKr=s(yj);Dko=r(dKr,"RobertaForQuestionAnswering"),dKr.forEach(t),jko=r(Bye," (RoBERTa model)"),Bye.forEach(t),Nko=i(P),T5=n(P,"LI",{});var xye=s(T5);Ase=n(xye,"STRONG",{});var cKr=s(Ase);qko=r(cKr,"roformer"),cKr.forEach(t),Gko=r(xye," \u2014 "),wj=n(xye,"A",{href:!0});var fKr=s(wj);Oko=r(fKr,"RoFormerForQuestionAnswering"),fKr.forEach(t),Xko=r(xye," (RoFormer model)"),xye.forEach(t),Vko=i(P),F5=n(P,"LI",{});var kye=s(F5);Lse=n(kye,"STRONG",{});var mKr=s(Lse);zko=r(mKr,"splinter"),mKr.forEach(t),Wko=r(kye," \u2014 "),Aj=n(kye,"A",{href:!0});var gKr=s(Aj);Qko=r(gKr,"SplinterForQuestionAnswering"),gKr.forEach(t),Hko=r(kye," (Splinter model)"),kye.forEach(t),Uko=i(P),C5=n(P,"LI",{});var Rye=s(C5);Bse=n(Rye,"STRONG",{});var hKr=s(Bse);Jko=r(hKr,"squeezebert"),hKr.forEach(t),Yko=r(Rye," \u2014 "),Lj=n(Rye,"A",{href:!0});var pKr=s(Lj);Kko=r(pKr,"SqueezeBertForQuestionAnswering"),pKr.forEach(t),Zko=r(Rye," (SqueezeBERT model)"),Rye.forEach(t),eRo=i(P),M5=n(P,"LI",{});var Sye=s(M5);xse=n(Sye,"STRONG",{});var _Kr=s(xse);oRo=r(_Kr,"xlm"),_Kr.forEach(t),rRo=r(Sye," \u2014 "),Bj=n(Sye,"A",{href:!0});var uKr=s(Bj);tRo=r(uKr,"XLMForQuestionAnsweringSimple"),uKr.forEach(t),aRo=r(Sye," (XLM model)"),Sye.forEach(t),nRo=i(P),E5=n(P,"LI",{});var Pye=s(E5);kse=n(Pye,"STRONG",{});var bKr=s(kse);sRo=r(bKr,"xlm-roberta"),bKr.forEach(t),lRo=r(Pye," \u2014 "),xj=n(Pye,"A",{href:!0});var vKr=s(xj);iRo=r(vKr,"XLMRobertaForQuestionAnswering"),vKr.forEach(t),dRo=r(Pye," (XLM-RoBERTa model)"),Pye.forEach(t),cRo=i(P),y5=n(P,"LI",{});var $ye=s(y5);Rse=n($ye,"STRONG",{});var TKr=s(Rse);fRo=r(TKr,"xlm-roberta-xl"),TKr.forEach(t),mRo=r($ye," \u2014 "),kj=n($ye,"A",{href:!0});var FKr=s(kj);gRo=r(FKr,"XLMRobertaXLForQuestionAnswering"),FKr.forEach(t),hRo=r($ye," (XLM-RoBERTa-XL model)"),$ye.forEach(t),pRo=i(P),w5=n(P,"LI",{});var Iye=s(w5);Sse=n(Iye,"STRONG",{});var CKr=s(Sse);_Ro=r(CKr,"xlnet"),CKr.forEach(t),uRo=r(Iye," \u2014 "),Rj=n(Iye,"A",{href:!0});var MKr=s(Rj);bRo=r(MKr,"XLNetForQuestionAnsweringSimple"),MKr.forEach(t),vRo=r(Iye," (XLNet model)"),Iye.forEach(t),TRo=i(P),A5=n(P,"LI",{});var Dye=s(A5);Pse=n(Dye,"STRONG",{});var EKr=s(Pse);FRo=r(EKr,"yoso"),EKr.forEach(t),CRo=r(Dye," \u2014 "),Sj=n(Dye,"A",{href:!0});var yKr=s(Sj);MRo=r(yKr,"YosoForQuestionAnswering"),yKr.forEach(t),ERo=r(Dye," (YOSO model)"),Dye.forEach(t),P.forEach(t),yRo=i(Wt),L5=n(Wt,"P",{});var jye=s(L5);wRo=r(jye,"The model is set in evaluation mode by default using "),$se=n(jye,"CODE",{});var wKr=s($se);ARo=r(wKr,"model.eval()"),wKr.forEach(t),LRo=r(jye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ise=n(jye,"CODE",{});var AKr=s(Ise);BRo=r(AKr,"model.train()"),AKr.forEach(t),jye.forEach(t),xRo=i(Wt),Dse=n(Wt,"P",{});var LKr=s(Dse);kRo=r(LKr,"Examples:"),LKr.forEach(t),RRo=i(Wt),m(y3.$$.fragment,Wt),Wt.forEach(t),sl.forEach(t),DBe=i(c),vd=n(c,"H2",{class:!0});var Vke=s(vd);B5=n(Vke,"A",{id:!0,class:!0,href:!0});var BKr=s(B5);jse=n(BKr,"SPAN",{});var xKr=s(jse);m(w3.$$.fragment,xKr),xKr.forEach(t),BKr.forEach(t),SRo=i(Vke),Nse=n(Vke,"SPAN",{});var kKr=s(Nse);PRo=r(kKr,"AutoModelForTableQuestionAnswering"),kKr.forEach(t),Vke.forEach(t),jBe=i(c),nr=n(c,"DIV",{class:!0});var il=s(nr);m(A3.$$.fragment,il),$Ro=i(il),Td=n(il,"P",{});var uz=s(Td);IRo=r(uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),qse=n(uz,"CODE",{});var RKr=s(qse);DRo=r(RKr,"from_pretrained()"),RKr.forEach(t),jRo=r(uz,"class method or the "),Gse=n(uz,"CODE",{});var SKr=s(Gse);NRo=r(SKr,"from_config()"),SKr.forEach(t),qRo=r(uz,`class
method.`),uz.forEach(t),GRo=i(il),L3=n(il,"P",{});var zke=s(L3);ORo=r(zke,"This class cannot be instantiated directly using "),Ose=n(zke,"CODE",{});var PKr=s(Ose);XRo=r(PKr,"__init__()"),PKr.forEach(t),VRo=r(zke," (throws an error)."),zke.forEach(t),zRo=i(il),Kr=n(il,"DIV",{class:!0});var dl=s(Kr);m(B3.$$.fragment,dl),WRo=i(dl),Xse=n(dl,"P",{});var $Kr=s(Xse);QRo=r($Kr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),$Kr.forEach(t),HRo=i(dl),Fd=n(dl,"P",{});var bz=s(Fd);URo=r(bz,`Note:
Loading a model from its configuration file does `),Vse=n(bz,"STRONG",{});var IKr=s(Vse);JRo=r(IKr,"not"),IKr.forEach(t),YRo=r(bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),zse=n(bz,"CODE",{});var DKr=s(zse);KRo=r(DKr,"from_pretrained()"),DKr.forEach(t),ZRo=r(bz,"to load the model weights."),bz.forEach(t),eSo=i(dl),Wse=n(dl,"P",{});var jKr=s(Wse);oSo=r(jKr,"Examples:"),jKr.forEach(t),rSo=i(dl),m(x3.$$.fragment,dl),dl.forEach(t),tSo=i(il),Qe=n(il,"DIV",{class:!0});var Qt=s(Qe);m(k3.$$.fragment,Qt),aSo=i(Qt),Qse=n(Qt,"P",{});var NKr=s(Qse);nSo=r(NKr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),NKr.forEach(t),sSo=i(Qt),Ua=n(Qt,"P",{});var VM=s(Ua);lSo=r(VM,"The model class to instantiate is selected based on the "),Hse=n(VM,"CODE",{});var qKr=s(Hse);iSo=r(qKr,"model_type"),qKr.forEach(t),dSo=r(VM,` property of the config object (either
passed as an argument or loaded from `),Use=n(VM,"CODE",{});var GKr=s(Use);cSo=r(GKr,"pretrained_model_name_or_path"),GKr.forEach(t),fSo=r(VM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jse=n(VM,"CODE",{});var OKr=s(Jse);mSo=r(OKr,"pretrained_model_name_or_path"),OKr.forEach(t),gSo=r(VM,":"),VM.forEach(t),hSo=i(Qt),Yse=n(Qt,"UL",{});var XKr=s(Yse);x5=n(XKr,"LI",{});var Nye=s(x5);Kse=n(Nye,"STRONG",{});var VKr=s(Kse);pSo=r(VKr,"tapas"),VKr.forEach(t),_So=r(Nye," \u2014 "),Pj=n(Nye,"A",{href:!0});var zKr=s(Pj);uSo=r(zKr,"TapasForQuestionAnswering"),zKr.forEach(t),bSo=r(Nye," (TAPAS model)"),Nye.forEach(t),XKr.forEach(t),vSo=i(Qt),k5=n(Qt,"P",{});var qye=s(k5);TSo=r(qye,"The model is set in evaluation mode by default using "),Zse=n(qye,"CODE",{});var WKr=s(Zse);FSo=r(WKr,"model.eval()"),WKr.forEach(t),CSo=r(qye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ele=n(qye,"CODE",{});var QKr=s(ele);MSo=r(QKr,"model.train()"),QKr.forEach(t),qye.forEach(t),ESo=i(Qt),ole=n(Qt,"P",{});var HKr=s(ole);ySo=r(HKr,"Examples:"),HKr.forEach(t),wSo=i(Qt),m(R3.$$.fragment,Qt),Qt.forEach(t),il.forEach(t),NBe=i(c),Cd=n(c,"H2",{class:!0});var Wke=s(Cd);R5=n(Wke,"A",{id:!0,class:!0,href:!0});var UKr=s(R5);rle=n(UKr,"SPAN",{});var JKr=s(rle);m(S3.$$.fragment,JKr),JKr.forEach(t),UKr.forEach(t),ASo=i(Wke),tle=n(Wke,"SPAN",{});var YKr=s(tle);LSo=r(YKr,"AutoModelForImageClassification"),YKr.forEach(t),Wke.forEach(t),qBe=i(c),sr=n(c,"DIV",{class:!0});var cl=s(sr);m(P3.$$.fragment,cl),BSo=i(cl),Md=n(cl,"P",{});var vz=s(Md);xSo=r(vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),ale=n(vz,"CODE",{});var KKr=s(ale);kSo=r(KKr,"from_pretrained()"),KKr.forEach(t),RSo=r(vz,"class method or the "),nle=n(vz,"CODE",{});var ZKr=s(nle);SSo=r(ZKr,"from_config()"),ZKr.forEach(t),PSo=r(vz,`class
method.`),vz.forEach(t),$So=i(cl),$3=n(cl,"P",{});var Qke=s($3);ISo=r(Qke,"This class cannot be instantiated directly using "),sle=n(Qke,"CODE",{});var eZr=s(sle);DSo=r(eZr,"__init__()"),eZr.forEach(t),jSo=r(Qke," (throws an error)."),Qke.forEach(t),NSo=i(cl),Zr=n(cl,"DIV",{class:!0});var fl=s(Zr);m(I3.$$.fragment,fl),qSo=i(fl),lle=n(fl,"P",{});var oZr=s(lle);GSo=r(oZr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),oZr.forEach(t),OSo=i(fl),Ed=n(fl,"P",{});var Tz=s(Ed);XSo=r(Tz,`Note:
Loading a model from its configuration file does `),ile=n(Tz,"STRONG",{});var rZr=s(ile);VSo=r(rZr,"not"),rZr.forEach(t),zSo=r(Tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),dle=n(Tz,"CODE",{});var tZr=s(dle);WSo=r(tZr,"from_pretrained()"),tZr.forEach(t),QSo=r(Tz,"to load the model weights."),Tz.forEach(t),HSo=i(fl),cle=n(fl,"P",{});var aZr=s(cle);USo=r(aZr,"Examples:"),aZr.forEach(t),JSo=i(fl),m(D3.$$.fragment,fl),fl.forEach(t),YSo=i(cl),He=n(cl,"DIV",{class:!0});var Ht=s(He);m(j3.$$.fragment,Ht),KSo=i(Ht),fle=n(Ht,"P",{});var nZr=s(fle);ZSo=r(nZr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),nZr.forEach(t),ePo=i(Ht),Ja=n(Ht,"P",{});var zM=s(Ja);oPo=r(zM,"The model class to instantiate is selected based on the "),mle=n(zM,"CODE",{});var sZr=s(mle);rPo=r(sZr,"model_type"),sZr.forEach(t),tPo=r(zM,` property of the config object (either
passed as an argument or loaded from `),gle=n(zM,"CODE",{});var lZr=s(gle);aPo=r(lZr,"pretrained_model_name_or_path"),lZr.forEach(t),nPo=r(zM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hle=n(zM,"CODE",{});var iZr=s(hle);sPo=r(iZr,"pretrained_model_name_or_path"),iZr.forEach(t),lPo=r(zM,":"),zM.forEach(t),iPo=i(Ht),Fe=n(Ht,"UL",{});var no=s(Fe);S5=n(no,"LI",{});var Gye=s(S5);ple=n(Gye,"STRONG",{});var dZr=s(ple);dPo=r(dZr,"beit"),dZr.forEach(t),cPo=r(Gye," \u2014 "),$j=n(Gye,"A",{href:!0});var cZr=s($j);fPo=r(cZr,"BeitForImageClassification"),cZr.forEach(t),mPo=r(Gye," (BEiT model)"),Gye.forEach(t),gPo=i(no),P5=n(no,"LI",{});var Oye=s(P5);_le=n(Oye,"STRONG",{});var fZr=s(_le);hPo=r(fZr,"convnext"),fZr.forEach(t),pPo=r(Oye," \u2014 "),Ij=n(Oye,"A",{href:!0});var mZr=s(Ij);_Po=r(mZr,"ConvNextForImageClassification"),mZr.forEach(t),uPo=r(Oye," (ConvNext model)"),Oye.forEach(t),bPo=i(no),$s=n(no,"LI",{});var h8=s($s);ule=n(h8,"STRONG",{});var gZr=s(ule);vPo=r(gZr,"deit"),gZr.forEach(t),TPo=r(h8," \u2014 "),Dj=n(h8,"A",{href:!0});var hZr=s(Dj);FPo=r(hZr,"DeiTForImageClassification"),hZr.forEach(t),CPo=r(h8," or "),jj=n(h8,"A",{href:!0});var pZr=s(jj);MPo=r(pZr,"DeiTForImageClassificationWithTeacher"),pZr.forEach(t),EPo=r(h8," (DeiT model)"),h8.forEach(t),yPo=i(no),$5=n(no,"LI",{});var Xye=s($5);ble=n(Xye,"STRONG",{});var _Zr=s(ble);wPo=r(_Zr,"imagegpt"),_Zr.forEach(t),APo=r(Xye," \u2014 "),Nj=n(Xye,"A",{href:!0});var uZr=s(Nj);LPo=r(uZr,"ImageGPTForImageClassification"),uZr.forEach(t),BPo=r(Xye," (ImageGPT model)"),Xye.forEach(t),xPo=i(no),la=n(no,"LI",{});var Ef=s(la);vle=n(Ef,"STRONG",{});var bZr=s(vle);kPo=r(bZr,"perceiver"),bZr.forEach(t),RPo=r(Ef," \u2014 "),qj=n(Ef,"A",{href:!0});var vZr=s(qj);SPo=r(vZr,"PerceiverForImageClassificationLearned"),vZr.forEach(t),PPo=r(Ef," or "),Gj=n(Ef,"A",{href:!0});var TZr=s(Gj);$Po=r(TZr,"PerceiverForImageClassificationFourier"),TZr.forEach(t),IPo=r(Ef," or "),Oj=n(Ef,"A",{href:!0});var FZr=s(Oj);DPo=r(FZr,"PerceiverForImageClassificationConvProcessing"),FZr.forEach(t),jPo=r(Ef," (Perceiver model)"),Ef.forEach(t),NPo=i(no),I5=n(no,"LI",{});var Vye=s(I5);Tle=n(Vye,"STRONG",{});var CZr=s(Tle);qPo=r(CZr,"poolformer"),CZr.forEach(t),GPo=r(Vye," \u2014 "),Xj=n(Vye,"A",{href:!0});var MZr=s(Xj);OPo=r(MZr,"PoolFormerForImageClassification"),MZr.forEach(t),XPo=r(Vye," (PoolFormer model)"),Vye.forEach(t),VPo=i(no),D5=n(no,"LI",{});var zye=s(D5);Fle=n(zye,"STRONG",{});var EZr=s(Fle);zPo=r(EZr,"segformer"),EZr.forEach(t),WPo=r(zye," \u2014 "),Vj=n(zye,"A",{href:!0});var yZr=s(Vj);QPo=r(yZr,"SegformerForImageClassification"),yZr.forEach(t),HPo=r(zye," (SegFormer model)"),zye.forEach(t),UPo=i(no),j5=n(no,"LI",{});var Wye=s(j5);Cle=n(Wye,"STRONG",{});var wZr=s(Cle);JPo=r(wZr,"swin"),wZr.forEach(t),YPo=r(Wye," \u2014 "),zj=n(Wye,"A",{href:!0});var AZr=s(zj);KPo=r(AZr,"SwinForImageClassification"),AZr.forEach(t),ZPo=r(Wye," (Swin model)"),Wye.forEach(t),e$o=i(no),N5=n(no,"LI",{});var Qye=s(N5);Mle=n(Qye,"STRONG",{});var LZr=s(Mle);o$o=r(LZr,"vit"),LZr.forEach(t),r$o=r(Qye," \u2014 "),Wj=n(Qye,"A",{href:!0});var BZr=s(Wj);t$o=r(BZr,"ViTForImageClassification"),BZr.forEach(t),a$o=r(Qye," (ViT model)"),Qye.forEach(t),no.forEach(t),n$o=i(Ht),q5=n(Ht,"P",{});var Hye=s(q5);s$o=r(Hye,"The model is set in evaluation mode by default using "),Ele=n(Hye,"CODE",{});var xZr=s(Ele);l$o=r(xZr,"model.eval()"),xZr.forEach(t),i$o=r(Hye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yle=n(Hye,"CODE",{});var kZr=s(yle);d$o=r(kZr,"model.train()"),kZr.forEach(t),Hye.forEach(t),c$o=i(Ht),wle=n(Ht,"P",{});var RZr=s(wle);f$o=r(RZr,"Examples:"),RZr.forEach(t),m$o=i(Ht),m(N3.$$.fragment,Ht),Ht.forEach(t),cl.forEach(t),GBe=i(c),yd=n(c,"H2",{class:!0});var Hke=s(yd);G5=n(Hke,"A",{id:!0,class:!0,href:!0});var SZr=s(G5);Ale=n(SZr,"SPAN",{});var PZr=s(Ale);m(q3.$$.fragment,PZr),PZr.forEach(t),SZr.forEach(t),g$o=i(Hke),Lle=n(Hke,"SPAN",{});var $Zr=s(Lle);h$o=r($Zr,"AutoModelForVision2Seq"),$Zr.forEach(t),Hke.forEach(t),OBe=i(c),lr=n(c,"DIV",{class:!0});var ml=s(lr);m(G3.$$.fragment,ml),p$o=i(ml),wd=n(ml,"P",{});var Fz=s(wd);_$o=r(Fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ble=n(Fz,"CODE",{});var IZr=s(Ble);u$o=r(IZr,"from_pretrained()"),IZr.forEach(t),b$o=r(Fz,"class method or the "),xle=n(Fz,"CODE",{});var DZr=s(xle);v$o=r(DZr,"from_config()"),DZr.forEach(t),T$o=r(Fz,`class
method.`),Fz.forEach(t),F$o=i(ml),O3=n(ml,"P",{});var Uke=s(O3);C$o=r(Uke,"This class cannot be instantiated directly using "),kle=n(Uke,"CODE",{});var jZr=s(kle);M$o=r(jZr,"__init__()"),jZr.forEach(t),E$o=r(Uke," (throws an error)."),Uke.forEach(t),y$o=i(ml),et=n(ml,"DIV",{class:!0});var gl=s(et);m(X3.$$.fragment,gl),w$o=i(gl),Rle=n(gl,"P",{});var NZr=s(Rle);A$o=r(NZr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),NZr.forEach(t),L$o=i(gl),Ad=n(gl,"P",{});var Cz=s(Ad);B$o=r(Cz,`Note:
Loading a model from its configuration file does `),Sle=n(Cz,"STRONG",{});var qZr=s(Sle);x$o=r(qZr,"not"),qZr.forEach(t),k$o=r(Cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ple=n(Cz,"CODE",{});var GZr=s(Ple);R$o=r(GZr,"from_pretrained()"),GZr.forEach(t),S$o=r(Cz,"to load the model weights."),Cz.forEach(t),P$o=i(gl),$le=n(gl,"P",{});var OZr=s($le);$$o=r(OZr,"Examples:"),OZr.forEach(t),I$o=i(gl),m(V3.$$.fragment,gl),gl.forEach(t),D$o=i(ml),Ue=n(ml,"DIV",{class:!0});var Ut=s(Ue);m(z3.$$.fragment,Ut),j$o=i(Ut),Ile=n(Ut,"P",{});var XZr=s(Ile);N$o=r(XZr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),XZr.forEach(t),q$o=i(Ut),Ya=n(Ut,"P",{});var WM=s(Ya);G$o=r(WM,"The model class to instantiate is selected based on the "),Dle=n(WM,"CODE",{});var VZr=s(Dle);O$o=r(VZr,"model_type"),VZr.forEach(t),X$o=r(WM,` property of the config object (either
passed as an argument or loaded from `),jle=n(WM,"CODE",{});var zZr=s(jle);V$o=r(zZr,"pretrained_model_name_or_path"),zZr.forEach(t),z$o=r(WM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nle=n(WM,"CODE",{});var WZr=s(Nle);W$o=r(WZr,"pretrained_model_name_or_path"),WZr.forEach(t),Q$o=r(WM,":"),WM.forEach(t),H$o=i(Ut),qle=n(Ut,"UL",{});var QZr=s(qle);O5=n(QZr,"LI",{});var Uye=s(O5);Gle=n(Uye,"STRONG",{});var HZr=s(Gle);U$o=r(HZr,"vision-encoder-decoder"),HZr.forEach(t),J$o=r(Uye," \u2014 "),Qj=n(Uye,"A",{href:!0});var UZr=s(Qj);Y$o=r(UZr,"VisionEncoderDecoderModel"),UZr.forEach(t),K$o=r(Uye," (Vision Encoder decoder model)"),Uye.forEach(t),QZr.forEach(t),Z$o=i(Ut),X5=n(Ut,"P",{});var Jye=s(X5);eIo=r(Jye,"The model is set in evaluation mode by default using "),Ole=n(Jye,"CODE",{});var JZr=s(Ole);oIo=r(JZr,"model.eval()"),JZr.forEach(t),rIo=r(Jye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xle=n(Jye,"CODE",{});var YZr=s(Xle);tIo=r(YZr,"model.train()"),YZr.forEach(t),Jye.forEach(t),aIo=i(Ut),Vle=n(Ut,"P",{});var KZr=s(Vle);nIo=r(KZr,"Examples:"),KZr.forEach(t),sIo=i(Ut),m(W3.$$.fragment,Ut),Ut.forEach(t),ml.forEach(t),XBe=i(c),Ld=n(c,"H2",{class:!0});var Jke=s(Ld);V5=n(Jke,"A",{id:!0,class:!0,href:!0});var ZZr=s(V5);zle=n(ZZr,"SPAN",{});var eet=s(zle);m(Q3.$$.fragment,eet),eet.forEach(t),ZZr.forEach(t),lIo=i(Jke),Wle=n(Jke,"SPAN",{});var oet=s(Wle);iIo=r(oet,"AutoModelForAudioClassification"),oet.forEach(t),Jke.forEach(t),VBe=i(c),ir=n(c,"DIV",{class:!0});var hl=s(ir);m(H3.$$.fragment,hl),dIo=i(hl),Bd=n(hl,"P",{});var Mz=s(Bd);cIo=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Qle=n(Mz,"CODE",{});var ret=s(Qle);fIo=r(ret,"from_pretrained()"),ret.forEach(t),mIo=r(Mz,"class method or the "),Hle=n(Mz,"CODE",{});var tet=s(Hle);gIo=r(tet,"from_config()"),tet.forEach(t),hIo=r(Mz,`class
method.`),Mz.forEach(t),pIo=i(hl),U3=n(hl,"P",{});var Yke=s(U3);_Io=r(Yke,"This class cannot be instantiated directly using "),Ule=n(Yke,"CODE",{});var aet=s(Ule);uIo=r(aet,"__init__()"),aet.forEach(t),bIo=r(Yke," (throws an error)."),Yke.forEach(t),vIo=i(hl),ot=n(hl,"DIV",{class:!0});var pl=s(ot);m(J3.$$.fragment,pl),TIo=i(pl),Jle=n(pl,"P",{});var net=s(Jle);FIo=r(net,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),net.forEach(t),CIo=i(pl),xd=n(pl,"P",{});var Ez=s(xd);MIo=r(Ez,`Note:
Loading a model from its configuration file does `),Yle=n(Ez,"STRONG",{});var set=s(Yle);EIo=r(set,"not"),set.forEach(t),yIo=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kle=n(Ez,"CODE",{});var iet=s(Kle);wIo=r(iet,"from_pretrained()"),iet.forEach(t),AIo=r(Ez,"to load the model weights."),Ez.forEach(t),LIo=i(pl),Zle=n(pl,"P",{});var det=s(Zle);BIo=r(det,"Examples:"),det.forEach(t),xIo=i(pl),m(Y3.$$.fragment,pl),pl.forEach(t),kIo=i(hl),Je=n(hl,"DIV",{class:!0});var Jt=s(Je);m(K3.$$.fragment,Jt),RIo=i(Jt),eie=n(Jt,"P",{});var cet=s(eie);SIo=r(cet,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),cet.forEach(t),PIo=i(Jt),Ka=n(Jt,"P",{});var QM=s(Ka);$Io=r(QM,"The model class to instantiate is selected based on the "),oie=n(QM,"CODE",{});var fet=s(oie);IIo=r(fet,"model_type"),fet.forEach(t),DIo=r(QM,` property of the config object (either
passed as an argument or loaded from `),rie=n(QM,"CODE",{});var met=s(rie);jIo=r(met,"pretrained_model_name_or_path"),met.forEach(t),NIo=r(QM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tie=n(QM,"CODE",{});var get=s(tie);qIo=r(get,"pretrained_model_name_or_path"),get.forEach(t),GIo=r(QM,":"),QM.forEach(t),OIo=i(Jt),xe=n(Jt,"UL",{});var No=s(xe);z5=n(No,"LI",{});var Yye=s(z5);aie=n(Yye,"STRONG",{});var het=s(aie);XIo=r(het,"data2vec-audio"),het.forEach(t),VIo=r(Yye," \u2014 "),Hj=n(Yye,"A",{href:!0});var pet=s(Hj);zIo=r(pet,"Data2VecAudioForSequenceClassification"),pet.forEach(t),WIo=r(Yye," (Data2VecAudio model)"),Yye.forEach(t),QIo=i(No),W5=n(No,"LI",{});var Kye=s(W5);nie=n(Kye,"STRONG",{});var _et=s(nie);HIo=r(_et,"hubert"),_et.forEach(t),UIo=r(Kye," \u2014 "),Uj=n(Kye,"A",{href:!0});var uet=s(Uj);JIo=r(uet,"HubertForSequenceClassification"),uet.forEach(t),YIo=r(Kye," (Hubert model)"),Kye.forEach(t),KIo=i(No),Q5=n(No,"LI",{});var Zye=s(Q5);sie=n(Zye,"STRONG",{});var bet=s(sie);ZIo=r(bet,"sew"),bet.forEach(t),eDo=r(Zye," \u2014 "),Jj=n(Zye,"A",{href:!0});var vet=s(Jj);oDo=r(vet,"SEWForSequenceClassification"),vet.forEach(t),rDo=r(Zye," (SEW model)"),Zye.forEach(t),tDo=i(No),H5=n(No,"LI",{});var ewe=s(H5);lie=n(ewe,"STRONG",{});var Tet=s(lie);aDo=r(Tet,"sew-d"),Tet.forEach(t),nDo=r(ewe," \u2014 "),Yj=n(ewe,"A",{href:!0});var Fet=s(Yj);sDo=r(Fet,"SEWDForSequenceClassification"),Fet.forEach(t),lDo=r(ewe," (SEW-D model)"),ewe.forEach(t),iDo=i(No),U5=n(No,"LI",{});var owe=s(U5);iie=n(owe,"STRONG",{});var Cet=s(iie);dDo=r(Cet,"unispeech"),Cet.forEach(t),cDo=r(owe," \u2014 "),Kj=n(owe,"A",{href:!0});var Met=s(Kj);fDo=r(Met,"UniSpeechForSequenceClassification"),Met.forEach(t),mDo=r(owe," (UniSpeech model)"),owe.forEach(t),gDo=i(No),J5=n(No,"LI",{});var rwe=s(J5);die=n(rwe,"STRONG",{});var Eet=s(die);hDo=r(Eet,"unispeech-sat"),Eet.forEach(t),pDo=r(rwe," \u2014 "),Zj=n(rwe,"A",{href:!0});var yet=s(Zj);_Do=r(yet,"UniSpeechSatForSequenceClassification"),yet.forEach(t),uDo=r(rwe," (UniSpeechSat model)"),rwe.forEach(t),bDo=i(No),Y5=n(No,"LI",{});var twe=s(Y5);cie=n(twe,"STRONG",{});var wet=s(cie);vDo=r(wet,"wav2vec2"),wet.forEach(t),TDo=r(twe," \u2014 "),eN=n(twe,"A",{href:!0});var Aet=s(eN);FDo=r(Aet,"Wav2Vec2ForSequenceClassification"),Aet.forEach(t),CDo=r(twe," (Wav2Vec2 model)"),twe.forEach(t),MDo=i(No),K5=n(No,"LI",{});var awe=s(K5);fie=n(awe,"STRONG",{});var Let=s(fie);EDo=r(Let,"wavlm"),Let.forEach(t),yDo=r(awe," \u2014 "),oN=n(awe,"A",{href:!0});var Bet=s(oN);wDo=r(Bet,"WavLMForSequenceClassification"),Bet.forEach(t),ADo=r(awe," (WavLM model)"),awe.forEach(t),No.forEach(t),LDo=i(Jt),Z5=n(Jt,"P",{});var nwe=s(Z5);BDo=r(nwe,"The model is set in evaluation mode by default using "),mie=n(nwe,"CODE",{});var xet=s(mie);xDo=r(xet,"model.eval()"),xet.forEach(t),kDo=r(nwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gie=n(nwe,"CODE",{});var ket=s(gie);RDo=r(ket,"model.train()"),ket.forEach(t),nwe.forEach(t),SDo=i(Jt),hie=n(Jt,"P",{});var Ret=s(hie);PDo=r(Ret,"Examples:"),Ret.forEach(t),$Do=i(Jt),m(Z3.$$.fragment,Jt),Jt.forEach(t),hl.forEach(t),zBe=i(c),kd=n(c,"H2",{class:!0});var Kke=s(kd);e2=n(Kke,"A",{id:!0,class:!0,href:!0});var Set=s(e2);pie=n(Set,"SPAN",{});var Pet=s(pie);m(ey.$$.fragment,Pet),Pet.forEach(t),Set.forEach(t),IDo=i(Kke),_ie=n(Kke,"SPAN",{});var $et=s(_ie);DDo=r($et,"AutoModelForAudioFrameClassification"),$et.forEach(t),Kke.forEach(t),WBe=i(c),dr=n(c,"DIV",{class:!0});var _l=s(dr);m(oy.$$.fragment,_l),jDo=i(_l),Rd=n(_l,"P",{});var yz=s(Rd);NDo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),uie=n(yz,"CODE",{});var Iet=s(uie);qDo=r(Iet,"from_pretrained()"),Iet.forEach(t),GDo=r(yz,"class method or the "),bie=n(yz,"CODE",{});var Det=s(bie);ODo=r(Det,"from_config()"),Det.forEach(t),XDo=r(yz,`class
method.`),yz.forEach(t),VDo=i(_l),ry=n(_l,"P",{});var Zke=s(ry);zDo=r(Zke,"This class cannot be instantiated directly using "),vie=n(Zke,"CODE",{});var jet=s(vie);WDo=r(jet,"__init__()"),jet.forEach(t),QDo=r(Zke," (throws an error)."),Zke.forEach(t),HDo=i(_l),rt=n(_l,"DIV",{class:!0});var ul=s(rt);m(ty.$$.fragment,ul),UDo=i(ul),Tie=n(ul,"P",{});var Net=s(Tie);JDo=r(Net,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),Net.forEach(t),YDo=i(ul),Sd=n(ul,"P",{});var wz=s(Sd);KDo=r(wz,`Note:
Loading a model from its configuration file does `),Fie=n(wz,"STRONG",{});var qet=s(Fie);ZDo=r(qet,"not"),qet.forEach(t),ejo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cie=n(wz,"CODE",{});var Get=s(Cie);ojo=r(Get,"from_pretrained()"),Get.forEach(t),rjo=r(wz,"to load the model weights."),wz.forEach(t),tjo=i(ul),Mie=n(ul,"P",{});var Oet=s(Mie);ajo=r(Oet,"Examples:"),Oet.forEach(t),njo=i(ul),m(ay.$$.fragment,ul),ul.forEach(t),sjo=i(_l),Ye=n(_l,"DIV",{class:!0});var Yt=s(Ye);m(ny.$$.fragment,Yt),ljo=i(Yt),Eie=n(Yt,"P",{});var Xet=s(Eie);ijo=r(Xet,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),Xet.forEach(t),djo=i(Yt),Za=n(Yt,"P",{});var HM=s(Za);cjo=r(HM,"The model class to instantiate is selected based on the "),yie=n(HM,"CODE",{});var Vet=s(yie);fjo=r(Vet,"model_type"),Vet.forEach(t),mjo=r(HM,` property of the config object (either
passed as an argument or loaded from `),wie=n(HM,"CODE",{});var zet=s(wie);gjo=r(zet,"pretrained_model_name_or_path"),zet.forEach(t),hjo=r(HM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Aie=n(HM,"CODE",{});var Wet=s(Aie);pjo=r(Wet,"pretrained_model_name_or_path"),Wet.forEach(t),_jo=r(HM,":"),HM.forEach(t),ujo=i(Yt),en=n(Yt,"UL",{});var UM=s(en);o2=n(UM,"LI",{});var swe=s(o2);Lie=n(swe,"STRONG",{});var Qet=s(Lie);bjo=r(Qet,"data2vec-audio"),Qet.forEach(t),vjo=r(swe," \u2014 "),rN=n(swe,"A",{href:!0});var Het=s(rN);Tjo=r(Het,"Data2VecAudioForAudioFrameClassification"),Het.forEach(t),Fjo=r(swe," (Data2VecAudio model)"),swe.forEach(t),Cjo=i(UM),r2=n(UM,"LI",{});var lwe=s(r2);Bie=n(lwe,"STRONG",{});var Uet=s(Bie);Mjo=r(Uet,"unispeech-sat"),Uet.forEach(t),Ejo=r(lwe," \u2014 "),tN=n(lwe,"A",{href:!0});var Jet=s(tN);yjo=r(Jet,"UniSpeechSatForAudioFrameClassification"),Jet.forEach(t),wjo=r(lwe," (UniSpeechSat model)"),lwe.forEach(t),Ajo=i(UM),t2=n(UM,"LI",{});var iwe=s(t2);xie=n(iwe,"STRONG",{});var Yet=s(xie);Ljo=r(Yet,"wav2vec2"),Yet.forEach(t),Bjo=r(iwe," \u2014 "),aN=n(iwe,"A",{href:!0});var Ket=s(aN);xjo=r(Ket,"Wav2Vec2ForAudioFrameClassification"),Ket.forEach(t),kjo=r(iwe," (Wav2Vec2 model)"),iwe.forEach(t),Rjo=i(UM),a2=n(UM,"LI",{});var dwe=s(a2);kie=n(dwe,"STRONG",{});var Zet=s(kie);Sjo=r(Zet,"wavlm"),Zet.forEach(t),Pjo=r(dwe," \u2014 "),nN=n(dwe,"A",{href:!0});var eot=s(nN);$jo=r(eot,"WavLMForAudioFrameClassification"),eot.forEach(t),Ijo=r(dwe," (WavLM model)"),dwe.forEach(t),UM.forEach(t),Djo=i(Yt),n2=n(Yt,"P",{});var cwe=s(n2);jjo=r(cwe,"The model is set in evaluation mode by default using "),Rie=n(cwe,"CODE",{});var oot=s(Rie);Njo=r(oot,"model.eval()"),oot.forEach(t),qjo=r(cwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Sie=n(cwe,"CODE",{});var rot=s(Sie);Gjo=r(rot,"model.train()"),rot.forEach(t),cwe.forEach(t),Ojo=i(Yt),Pie=n(Yt,"P",{});var tot=s(Pie);Xjo=r(tot,"Examples:"),tot.forEach(t),Vjo=i(Yt),m(sy.$$.fragment,Yt),Yt.forEach(t),_l.forEach(t),QBe=i(c),Pd=n(c,"H2",{class:!0});var eRe=s(Pd);s2=n(eRe,"A",{id:!0,class:!0,href:!0});var aot=s(s2);$ie=n(aot,"SPAN",{});var not=s($ie);m(ly.$$.fragment,not),not.forEach(t),aot.forEach(t),zjo=i(eRe),Iie=n(eRe,"SPAN",{});var sot=s(Iie);Wjo=r(sot,"AutoModelForCTC"),sot.forEach(t),eRe.forEach(t),HBe=i(c),cr=n(c,"DIV",{class:!0});var bl=s(cr);m(iy.$$.fragment,bl),Qjo=i(bl),$d=n(bl,"P",{});var Az=s($d);Hjo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Die=n(Az,"CODE",{});var lot=s(Die);Ujo=r(lot,"from_pretrained()"),lot.forEach(t),Jjo=r(Az,"class method or the "),jie=n(Az,"CODE",{});var iot=s(jie);Yjo=r(iot,"from_config()"),iot.forEach(t),Kjo=r(Az,`class
method.`),Az.forEach(t),Zjo=i(bl),dy=n(bl,"P",{});var oRe=s(dy);eNo=r(oRe,"This class cannot be instantiated directly using "),Nie=n(oRe,"CODE",{});var dot=s(Nie);oNo=r(dot,"__init__()"),dot.forEach(t),rNo=r(oRe," (throws an error)."),oRe.forEach(t),tNo=i(bl),tt=n(bl,"DIV",{class:!0});var vl=s(tt);m(cy.$$.fragment,vl),aNo=i(vl),qie=n(vl,"P",{});var cot=s(qie);nNo=r(cot,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),cot.forEach(t),sNo=i(vl),Id=n(vl,"P",{});var Lz=s(Id);lNo=r(Lz,`Note:
Loading a model from its configuration file does `),Gie=n(Lz,"STRONG",{});var fot=s(Gie);iNo=r(fot,"not"),fot.forEach(t),dNo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Oie=n(Lz,"CODE",{});var mot=s(Oie);cNo=r(mot,"from_pretrained()"),mot.forEach(t),fNo=r(Lz,"to load the model weights."),Lz.forEach(t),mNo=i(vl),Xie=n(vl,"P",{});var got=s(Xie);gNo=r(got,"Examples:"),got.forEach(t),hNo=i(vl),m(fy.$$.fragment,vl),vl.forEach(t),pNo=i(bl),Ke=n(bl,"DIV",{class:!0});var Kt=s(Ke);m(my.$$.fragment,Kt),_No=i(Kt),Vie=n(Kt,"P",{});var hot=s(Vie);uNo=r(hot,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),hot.forEach(t),bNo=i(Kt),on=n(Kt,"P",{});var JM=s(on);vNo=r(JM,"The model class to instantiate is selected based on the "),zie=n(JM,"CODE",{});var pot=s(zie);TNo=r(pot,"model_type"),pot.forEach(t),FNo=r(JM,` property of the config object (either
passed as an argument or loaded from `),Wie=n(JM,"CODE",{});var _ot=s(Wie);CNo=r(_ot,"pretrained_model_name_or_path"),_ot.forEach(t),MNo=r(JM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Qie=n(JM,"CODE",{});var uot=s(Qie);ENo=r(uot,"pretrained_model_name_or_path"),uot.forEach(t),yNo=r(JM,":"),JM.forEach(t),wNo=i(Kt),ke=n(Kt,"UL",{});var qo=s(ke);l2=n(qo,"LI",{});var fwe=s(l2);Hie=n(fwe,"STRONG",{});var bot=s(Hie);ANo=r(bot,"data2vec-audio"),bot.forEach(t),LNo=r(fwe," \u2014 "),sN=n(fwe,"A",{href:!0});var vot=s(sN);BNo=r(vot,"Data2VecAudioForCTC"),vot.forEach(t),xNo=r(fwe," (Data2VecAudio model)"),fwe.forEach(t),kNo=i(qo),i2=n(qo,"LI",{});var mwe=s(i2);Uie=n(mwe,"STRONG",{});var Tot=s(Uie);RNo=r(Tot,"hubert"),Tot.forEach(t),SNo=r(mwe," \u2014 "),lN=n(mwe,"A",{href:!0});var Fot=s(lN);PNo=r(Fot,"HubertForCTC"),Fot.forEach(t),$No=r(mwe," (Hubert model)"),mwe.forEach(t),INo=i(qo),d2=n(qo,"LI",{});var gwe=s(d2);Jie=n(gwe,"STRONG",{});var Cot=s(Jie);DNo=r(Cot,"sew"),Cot.forEach(t),jNo=r(gwe," \u2014 "),iN=n(gwe,"A",{href:!0});var Mot=s(iN);NNo=r(Mot,"SEWForCTC"),Mot.forEach(t),qNo=r(gwe," (SEW model)"),gwe.forEach(t),GNo=i(qo),c2=n(qo,"LI",{});var hwe=s(c2);Yie=n(hwe,"STRONG",{});var Eot=s(Yie);ONo=r(Eot,"sew-d"),Eot.forEach(t),XNo=r(hwe," \u2014 "),dN=n(hwe,"A",{href:!0});var yot=s(dN);VNo=r(yot,"SEWDForCTC"),yot.forEach(t),zNo=r(hwe," (SEW-D model)"),hwe.forEach(t),WNo=i(qo),f2=n(qo,"LI",{});var pwe=s(f2);Kie=n(pwe,"STRONG",{});var wot=s(Kie);QNo=r(wot,"unispeech"),wot.forEach(t),HNo=r(pwe," \u2014 "),cN=n(pwe,"A",{href:!0});var Aot=s(cN);UNo=r(Aot,"UniSpeechForCTC"),Aot.forEach(t),JNo=r(pwe," (UniSpeech model)"),pwe.forEach(t),YNo=i(qo),m2=n(qo,"LI",{});var _we=s(m2);Zie=n(_we,"STRONG",{});var Lot=s(Zie);KNo=r(Lot,"unispeech-sat"),Lot.forEach(t),ZNo=r(_we," \u2014 "),fN=n(_we,"A",{href:!0});var Bot=s(fN);eqo=r(Bot,"UniSpeechSatForCTC"),Bot.forEach(t),oqo=r(_we," (UniSpeechSat model)"),_we.forEach(t),rqo=i(qo),g2=n(qo,"LI",{});var uwe=s(g2);ede=n(uwe,"STRONG",{});var xot=s(ede);tqo=r(xot,"wav2vec2"),xot.forEach(t),aqo=r(uwe," \u2014 "),mN=n(uwe,"A",{href:!0});var kot=s(mN);nqo=r(kot,"Wav2Vec2ForCTC"),kot.forEach(t),sqo=r(uwe," (Wav2Vec2 model)"),uwe.forEach(t),lqo=i(qo),h2=n(qo,"LI",{});var bwe=s(h2);ode=n(bwe,"STRONG",{});var Rot=s(ode);iqo=r(Rot,"wavlm"),Rot.forEach(t),dqo=r(bwe," \u2014 "),gN=n(bwe,"A",{href:!0});var Sot=s(gN);cqo=r(Sot,"WavLMForCTC"),Sot.forEach(t),fqo=r(bwe," (WavLM model)"),bwe.forEach(t),qo.forEach(t),mqo=i(Kt),p2=n(Kt,"P",{});var vwe=s(p2);gqo=r(vwe,"The model is set in evaluation mode by default using "),rde=n(vwe,"CODE",{});var Pot=s(rde);hqo=r(Pot,"model.eval()"),Pot.forEach(t),pqo=r(vwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=n(vwe,"CODE",{});var $ot=s(tde);_qo=r($ot,"model.train()"),$ot.forEach(t),vwe.forEach(t),uqo=i(Kt),ade=n(Kt,"P",{});var Iot=s(ade);bqo=r(Iot,"Examples:"),Iot.forEach(t),vqo=i(Kt),m(gy.$$.fragment,Kt),Kt.forEach(t),bl.forEach(t),UBe=i(c),Dd=n(c,"H2",{class:!0});var rRe=s(Dd);_2=n(rRe,"A",{id:!0,class:!0,href:!0});var Dot=s(_2);nde=n(Dot,"SPAN",{});var jot=s(nde);m(hy.$$.fragment,jot),jot.forEach(t),Dot.forEach(t),Tqo=i(rRe),sde=n(rRe,"SPAN",{});var Not=s(sde);Fqo=r(Not,"AutoModelForSpeechSeq2Seq"),Not.forEach(t),rRe.forEach(t),JBe=i(c),fr=n(c,"DIV",{class:!0});var Tl=s(fr);m(py.$$.fragment,Tl),Cqo=i(Tl),jd=n(Tl,"P",{});var Bz=s(jd);Mqo=r(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),lde=n(Bz,"CODE",{});var qot=s(lde);Eqo=r(qot,"from_pretrained()"),qot.forEach(t),yqo=r(Bz,"class method or the "),ide=n(Bz,"CODE",{});var Got=s(ide);wqo=r(Got,"from_config()"),Got.forEach(t),Aqo=r(Bz,`class
method.`),Bz.forEach(t),Lqo=i(Tl),_y=n(Tl,"P",{});var tRe=s(_y);Bqo=r(tRe,"This class cannot be instantiated directly using "),dde=n(tRe,"CODE",{});var Oot=s(dde);xqo=r(Oot,"__init__()"),Oot.forEach(t),kqo=r(tRe," (throws an error)."),tRe.forEach(t),Rqo=i(Tl),at=n(Tl,"DIV",{class:!0});var Fl=s(at);m(uy.$$.fragment,Fl),Sqo=i(Fl),cde=n(Fl,"P",{});var Xot=s(cde);Pqo=r(Xot,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Xot.forEach(t),$qo=i(Fl),Nd=n(Fl,"P",{});var xz=s(Nd);Iqo=r(xz,`Note:
Loading a model from its configuration file does `),fde=n(xz,"STRONG",{});var Vot=s(fde);Dqo=r(Vot,"not"),Vot.forEach(t),jqo=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mde=n(xz,"CODE",{});var zot=s(mde);Nqo=r(zot,"from_pretrained()"),zot.forEach(t),qqo=r(xz,"to load the model weights."),xz.forEach(t),Gqo=i(Fl),gde=n(Fl,"P",{});var Wot=s(gde);Oqo=r(Wot,"Examples:"),Wot.forEach(t),Xqo=i(Fl),m(by.$$.fragment,Fl),Fl.forEach(t),Vqo=i(Tl),Ze=n(Tl,"DIV",{class:!0});var Zt=s(Ze);m(vy.$$.fragment,Zt),zqo=i(Zt),hde=n(Zt,"P",{});var Qot=s(hde);Wqo=r(Qot,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Qot.forEach(t),Qqo=i(Zt),rn=n(Zt,"P",{});var YM=s(rn);Hqo=r(YM,"The model class to instantiate is selected based on the "),pde=n(YM,"CODE",{});var Hot=s(pde);Uqo=r(Hot,"model_type"),Hot.forEach(t),Jqo=r(YM,` property of the config object (either
passed as an argument or loaded from `),_de=n(YM,"CODE",{});var Uot=s(_de);Yqo=r(Uot,"pretrained_model_name_or_path"),Uot.forEach(t),Kqo=r(YM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ude=n(YM,"CODE",{});var Jot=s(ude);Zqo=r(Jot,"pretrained_model_name_or_path"),Jot.forEach(t),eGo=r(YM,":"),YM.forEach(t),oGo=i(Zt),Ty=n(Zt,"UL",{});var aRe=s(Ty);u2=n(aRe,"LI",{});var Twe=s(u2);bde=n(Twe,"STRONG",{});var Yot=s(bde);rGo=r(Yot,"speech-encoder-decoder"),Yot.forEach(t),tGo=r(Twe," \u2014 "),hN=n(Twe,"A",{href:!0});var Kot=s(hN);aGo=r(Kot,"SpeechEncoderDecoderModel"),Kot.forEach(t),nGo=r(Twe," (Speech Encoder decoder model)"),Twe.forEach(t),sGo=i(aRe),b2=n(aRe,"LI",{});var Fwe=s(b2);vde=n(Fwe,"STRONG",{});var Zot=s(vde);lGo=r(Zot,"speech_to_text"),Zot.forEach(t),iGo=r(Fwe," \u2014 "),pN=n(Fwe,"A",{href:!0});var ert=s(pN);dGo=r(ert,"Speech2TextForConditionalGeneration"),ert.forEach(t),cGo=r(Fwe," (Speech2Text model)"),Fwe.forEach(t),aRe.forEach(t),fGo=i(Zt),v2=n(Zt,"P",{});var Cwe=s(v2);mGo=r(Cwe,"The model is set in evaluation mode by default using "),Tde=n(Cwe,"CODE",{});var ort=s(Tde);gGo=r(ort,"model.eval()"),ort.forEach(t),hGo=r(Cwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=n(Cwe,"CODE",{});var rrt=s(Fde);pGo=r(rrt,"model.train()"),rrt.forEach(t),Cwe.forEach(t),_Go=i(Zt),Cde=n(Zt,"P",{});var trt=s(Cde);uGo=r(trt,"Examples:"),trt.forEach(t),bGo=i(Zt),m(Fy.$$.fragment,Zt),Zt.forEach(t),Tl.forEach(t),YBe=i(c),qd=n(c,"H2",{class:!0});var nRe=s(qd);T2=n(nRe,"A",{id:!0,class:!0,href:!0});var art=s(T2);Mde=n(art,"SPAN",{});var nrt=s(Mde);m(Cy.$$.fragment,nrt),nrt.forEach(t),art.forEach(t),vGo=i(nRe),Ede=n(nRe,"SPAN",{});var srt=s(Ede);TGo=r(srt,"AutoModelForAudioXVector"),srt.forEach(t),nRe.forEach(t),KBe=i(c),mr=n(c,"DIV",{class:!0});var Cl=s(mr);m(My.$$.fragment,Cl),FGo=i(Cl),Gd=n(Cl,"P",{});var kz=s(Gd);CGo=r(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),yde=n(kz,"CODE",{});var lrt=s(yde);MGo=r(lrt,"from_pretrained()"),lrt.forEach(t),EGo=r(kz,"class method or the "),wde=n(kz,"CODE",{});var irt=s(wde);yGo=r(irt,"from_config()"),irt.forEach(t),wGo=r(kz,`class
method.`),kz.forEach(t),AGo=i(Cl),Ey=n(Cl,"P",{});var sRe=s(Ey);LGo=r(sRe,"This class cannot be instantiated directly using "),Ade=n(sRe,"CODE",{});var drt=s(Ade);BGo=r(drt,"__init__()"),drt.forEach(t),xGo=r(sRe," (throws an error)."),sRe.forEach(t),kGo=i(Cl),nt=n(Cl,"DIV",{class:!0});var Ml=s(nt);m(yy.$$.fragment,Ml),RGo=i(Ml),Lde=n(Ml,"P",{});var crt=s(Lde);SGo=r(crt,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),crt.forEach(t),PGo=i(Ml),Od=n(Ml,"P",{});var Rz=s(Od);$Go=r(Rz,`Note:
Loading a model from its configuration file does `),Bde=n(Rz,"STRONG",{});var frt=s(Bde);IGo=r(frt,"not"),frt.forEach(t),DGo=r(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xde=n(Rz,"CODE",{});var mrt=s(xde);jGo=r(mrt,"from_pretrained()"),mrt.forEach(t),NGo=r(Rz,"to load the model weights."),Rz.forEach(t),qGo=i(Ml),kde=n(Ml,"P",{});var grt=s(kde);GGo=r(grt,"Examples:"),grt.forEach(t),OGo=i(Ml),m(wy.$$.fragment,Ml),Ml.forEach(t),XGo=i(Cl),eo=n(Cl,"DIV",{class:!0});var ea=s(eo);m(Ay.$$.fragment,ea),VGo=i(ea),Rde=n(ea,"P",{});var hrt=s(Rde);zGo=r(hrt,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),hrt.forEach(t),WGo=i(ea),tn=n(ea,"P",{});var KM=s(tn);QGo=r(KM,"The model class to instantiate is selected based on the "),Sde=n(KM,"CODE",{});var prt=s(Sde);HGo=r(prt,"model_type"),prt.forEach(t),UGo=r(KM,` property of the config object (either
passed as an argument or loaded from `),Pde=n(KM,"CODE",{});var _rt=s(Pde);JGo=r(_rt,"pretrained_model_name_or_path"),_rt.forEach(t),YGo=r(KM,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=n(KM,"CODE",{});var urt=s($de);KGo=r(urt,"pretrained_model_name_or_path"),urt.forEach(t),ZGo=r(KM,":"),KM.forEach(t),eOo=i(ea),an=n(ea,"UL",{});var ZM=s(an);F2=n(ZM,"LI",{});var Mwe=s(F2);Ide=n(Mwe,"STRONG",{});var brt=s(Ide);oOo=r(brt,"data2vec-audio"),brt.forEach(t),rOo=r(Mwe," \u2014 "),_N=n(Mwe,"A",{href:!0});var vrt=s(_N);tOo=r(vrt,"Data2VecAudioForXVector"),vrt.forEach(t),aOo=r(Mwe," (Data2VecAudio model)"),Mwe.forEach(t),nOo=i(ZM),C2=n(ZM,"LI",{});var Ewe=s(C2);Dde=n(Ewe,"STRONG",{});var Trt=s(Dde);sOo=r(Trt,"unispeech-sat"),Trt.forEach(t),lOo=r(Ewe," \u2014 "),uN=n(Ewe,"A",{href:!0});var Frt=s(uN);iOo=r(Frt,"UniSpeechSatForXVector"),Frt.forEach(t),dOo=r(Ewe," (UniSpeechSat model)"),Ewe.forEach(t),cOo=i(ZM),M2=n(ZM,"LI",{});var ywe=s(M2);jde=n(ywe,"STRONG",{});var Crt=s(jde);fOo=r(Crt,"wav2vec2"),Crt.forEach(t),mOo=r(ywe," \u2014 "),bN=n(ywe,"A",{href:!0});var Mrt=s(bN);gOo=r(Mrt,"Wav2Vec2ForXVector"),Mrt.forEach(t),hOo=r(ywe," (Wav2Vec2 model)"),ywe.forEach(t),pOo=i(ZM),E2=n(ZM,"LI",{});var wwe=s(E2);Nde=n(wwe,"STRONG",{});var Ert=s(Nde);_Oo=r(Ert,"wavlm"),Ert.forEach(t),uOo=r(wwe," \u2014 "),vN=n(wwe,"A",{href:!0});var yrt=s(vN);bOo=r(yrt,"WavLMForXVector"),yrt.forEach(t),vOo=r(wwe," (WavLM model)"),wwe.forEach(t),ZM.forEach(t),TOo=i(ea),y2=n(ea,"P",{});var Awe=s(y2);FOo=r(Awe,"The model is set in evaluation mode by default using "),qde=n(Awe,"CODE",{});var wrt=s(qde);COo=r(wrt,"model.eval()"),wrt.forEach(t),MOo=r(Awe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gde=n(Awe,"CODE",{});var Art=s(Gde);EOo=r(Art,"model.train()"),Art.forEach(t),Awe.forEach(t),yOo=i(ea),Ode=n(ea,"P",{});var Lrt=s(Ode);wOo=r(Lrt,"Examples:"),Lrt.forEach(t),AOo=i(ea),m(Ly.$$.fragment,ea),ea.forEach(t),Cl.forEach(t),ZBe=i(c),Xd=n(c,"H2",{class:!0});var lRe=s(Xd);w2=n(lRe,"A",{id:!0,class:!0,href:!0});var Brt=s(w2);Xde=n(Brt,"SPAN",{});var xrt=s(Xde);m(By.$$.fragment,xrt),xrt.forEach(t),Brt.forEach(t),LOo=i(lRe),Vde=n(lRe,"SPAN",{});var krt=s(Vde);BOo=r(krt,"AutoModelForMaskedImageModeling"),krt.forEach(t),lRe.forEach(t),exe=i(c),gr=n(c,"DIV",{class:!0});var El=s(gr);m(xy.$$.fragment,El),xOo=i(El),Vd=n(El,"P",{});var Sz=s(Vd);kOo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),zde=n(Sz,"CODE",{});var Rrt=s(zde);ROo=r(Rrt,"from_pretrained()"),Rrt.forEach(t),SOo=r(Sz,"class method or the "),Wde=n(Sz,"CODE",{});var Srt=s(Wde);POo=r(Srt,"from_config()"),Srt.forEach(t),$Oo=r(Sz,`class
method.`),Sz.forEach(t),IOo=i(El),ky=n(El,"P",{});var iRe=s(ky);DOo=r(iRe,"This class cannot be instantiated directly using "),Qde=n(iRe,"CODE",{});var Prt=s(Qde);jOo=r(Prt,"__init__()"),Prt.forEach(t),NOo=r(iRe," (throws an error)."),iRe.forEach(t),qOo=i(El),st=n(El,"DIV",{class:!0});var yl=s(st);m(Ry.$$.fragment,yl),GOo=i(yl),Hde=n(yl,"P",{});var $rt=s(Hde);OOo=r($rt,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),$rt.forEach(t),XOo=i(yl),zd=n(yl,"P",{});var Pz=s(zd);VOo=r(Pz,`Note:
Loading a model from its configuration file does `),Ude=n(Pz,"STRONG",{});var Irt=s(Ude);zOo=r(Irt,"not"),Irt.forEach(t),WOo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jde=n(Pz,"CODE",{});var Drt=s(Jde);QOo=r(Drt,"from_pretrained()"),Drt.forEach(t),HOo=r(Pz,"to load the model weights."),Pz.forEach(t),UOo=i(yl),Yde=n(yl,"P",{});var jrt=s(Yde);JOo=r(jrt,"Examples:"),jrt.forEach(t),YOo=i(yl),m(Sy.$$.fragment,yl),yl.forEach(t),KOo=i(El),oo=n(El,"DIV",{class:!0});var oa=s(oo);m(Py.$$.fragment,oa),ZOo=i(oa),Kde=n(oa,"P",{});var Nrt=s(Kde);eXo=r(Nrt,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),Nrt.forEach(t),oXo=i(oa),nn=n(oa,"P",{});var e4=s(nn);rXo=r(e4,"The model class to instantiate is selected based on the "),Zde=n(e4,"CODE",{});var qrt=s(Zde);tXo=r(qrt,"model_type"),qrt.forEach(t),aXo=r(e4,` property of the config object (either
passed as an argument or loaded from `),ece=n(e4,"CODE",{});var Grt=s(ece);nXo=r(Grt,"pretrained_model_name_or_path"),Grt.forEach(t),sXo=r(e4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),oce=n(e4,"CODE",{});var Ort=s(oce);lXo=r(Ort,"pretrained_model_name_or_path"),Ort.forEach(t),iXo=r(e4,":"),e4.forEach(t),dXo=i(oa),Wd=n(oa,"UL",{});var $z=s(Wd);A2=n($z,"LI",{});var Lwe=s(A2);rce=n(Lwe,"STRONG",{});var Xrt=s(rce);cXo=r(Xrt,"deit"),Xrt.forEach(t),fXo=r(Lwe," \u2014 "),TN=n(Lwe,"A",{href:!0});var Vrt=s(TN);mXo=r(Vrt,"DeiTForMaskedImageModeling"),Vrt.forEach(t),gXo=r(Lwe," (DeiT model)"),Lwe.forEach(t),hXo=i($z),L2=n($z,"LI",{});var Bwe=s(L2);tce=n(Bwe,"STRONG",{});var zrt=s(tce);pXo=r(zrt,"swin"),zrt.forEach(t),_Xo=r(Bwe," \u2014 "),FN=n(Bwe,"A",{href:!0});var Wrt=s(FN);uXo=r(Wrt,"SwinForMaskedImageModeling"),Wrt.forEach(t),bXo=r(Bwe," (Swin model)"),Bwe.forEach(t),vXo=i($z),B2=n($z,"LI",{});var xwe=s(B2);ace=n(xwe,"STRONG",{});var Qrt=s(ace);TXo=r(Qrt,"vit"),Qrt.forEach(t),FXo=r(xwe," \u2014 "),CN=n(xwe,"A",{href:!0});var Hrt=s(CN);CXo=r(Hrt,"ViTForMaskedImageModeling"),Hrt.forEach(t),MXo=r(xwe," (ViT model)"),xwe.forEach(t),$z.forEach(t),EXo=i(oa),x2=n(oa,"P",{});var kwe=s(x2);yXo=r(kwe,"The model is set in evaluation mode by default using "),nce=n(kwe,"CODE",{});var Urt=s(nce);wXo=r(Urt,"model.eval()"),Urt.forEach(t),AXo=r(kwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sce=n(kwe,"CODE",{});var Jrt=s(sce);LXo=r(Jrt,"model.train()"),Jrt.forEach(t),kwe.forEach(t),BXo=i(oa),lce=n(oa,"P",{});var Yrt=s(lce);xXo=r(Yrt,"Examples:"),Yrt.forEach(t),kXo=i(oa),m($y.$$.fragment,oa),oa.forEach(t),El.forEach(t),oxe=i(c),Qd=n(c,"H2",{class:!0});var dRe=s(Qd);k2=n(dRe,"A",{id:!0,class:!0,href:!0});var Krt=s(k2);ice=n(Krt,"SPAN",{});var Zrt=s(ice);m(Iy.$$.fragment,Zrt),Zrt.forEach(t),Krt.forEach(t),RXo=i(dRe),dce=n(dRe,"SPAN",{});var ett=s(dce);SXo=r(ett,"AutoModelForObjectDetection"),ett.forEach(t),dRe.forEach(t),rxe=i(c),hr=n(c,"DIV",{class:!0});var wl=s(hr);m(Dy.$$.fragment,wl),PXo=i(wl),Hd=n(wl,"P",{});var Iz=s(Hd);$Xo=r(Iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),cce=n(Iz,"CODE",{});var ott=s(cce);IXo=r(ott,"from_pretrained()"),ott.forEach(t),DXo=r(Iz,"class method or the "),fce=n(Iz,"CODE",{});var rtt=s(fce);jXo=r(rtt,"from_config()"),rtt.forEach(t),NXo=r(Iz,`class
method.`),Iz.forEach(t),qXo=i(wl),jy=n(wl,"P",{});var cRe=s(jy);GXo=r(cRe,"This class cannot be instantiated directly using "),mce=n(cRe,"CODE",{});var ttt=s(mce);OXo=r(ttt,"__init__()"),ttt.forEach(t),XXo=r(cRe," (throws an error)."),cRe.forEach(t),VXo=i(wl),lt=n(wl,"DIV",{class:!0});var Al=s(lt);m(Ny.$$.fragment,Al),zXo=i(Al),gce=n(Al,"P",{});var att=s(gce);WXo=r(att,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),att.forEach(t),QXo=i(Al),Ud=n(Al,"P",{});var Dz=s(Ud);HXo=r(Dz,`Note:
Loading a model from its configuration file does `),hce=n(Dz,"STRONG",{});var ntt=s(hce);UXo=r(ntt,"not"),ntt.forEach(t),JXo=r(Dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),pce=n(Dz,"CODE",{});var stt=s(pce);YXo=r(stt,"from_pretrained()"),stt.forEach(t),KXo=r(Dz,"to load the model weights."),Dz.forEach(t),ZXo=i(Al),_ce=n(Al,"P",{});var ltt=s(_ce);eVo=r(ltt,"Examples:"),ltt.forEach(t),oVo=i(Al),m(qy.$$.fragment,Al),Al.forEach(t),rVo=i(wl),ro=n(wl,"DIV",{class:!0});var ra=s(ro);m(Gy.$$.fragment,ra),tVo=i(ra),uce=n(ra,"P",{});var itt=s(uce);aVo=r(itt,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),itt.forEach(t),nVo=i(ra),sn=n(ra,"P",{});var o4=s(sn);sVo=r(o4,"The model class to instantiate is selected based on the "),bce=n(o4,"CODE",{});var dtt=s(bce);lVo=r(dtt,"model_type"),dtt.forEach(t),iVo=r(o4,` property of the config object (either
passed as an argument or loaded from `),vce=n(o4,"CODE",{});var ctt=s(vce);dVo=r(ctt,"pretrained_model_name_or_path"),ctt.forEach(t),cVo=r(o4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tce=n(o4,"CODE",{});var ftt=s(Tce);fVo=r(ftt,"pretrained_model_name_or_path"),ftt.forEach(t),mVo=r(o4,":"),o4.forEach(t),gVo=i(ra),Fce=n(ra,"UL",{});var mtt=s(Fce);R2=n(mtt,"LI",{});var Rwe=s(R2);Cce=n(Rwe,"STRONG",{});var gtt=s(Cce);hVo=r(gtt,"detr"),gtt.forEach(t),pVo=r(Rwe," \u2014 "),MN=n(Rwe,"A",{href:!0});var htt=s(MN);_Vo=r(htt,"DetrForObjectDetection"),htt.forEach(t),uVo=r(Rwe," (DETR model)"),Rwe.forEach(t),mtt.forEach(t),bVo=i(ra),S2=n(ra,"P",{});var Swe=s(S2);vVo=r(Swe,"The model is set in evaluation mode by default using "),Mce=n(Swe,"CODE",{});var ptt=s(Mce);TVo=r(ptt,"model.eval()"),ptt.forEach(t),FVo=r(Swe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ece=n(Swe,"CODE",{});var _tt=s(Ece);CVo=r(_tt,"model.train()"),_tt.forEach(t),Swe.forEach(t),MVo=i(ra),yce=n(ra,"P",{});var utt=s(yce);EVo=r(utt,"Examples:"),utt.forEach(t),yVo=i(ra),m(Oy.$$.fragment,ra),ra.forEach(t),wl.forEach(t),txe=i(c),Jd=n(c,"H2",{class:!0});var fRe=s(Jd);P2=n(fRe,"A",{id:!0,class:!0,href:!0});var btt=s(P2);wce=n(btt,"SPAN",{});var vtt=s(wce);m(Xy.$$.fragment,vtt),vtt.forEach(t),btt.forEach(t),wVo=i(fRe),Ace=n(fRe,"SPAN",{});var Ttt=s(Ace);AVo=r(Ttt,"AutoModelForImageSegmentation"),Ttt.forEach(t),fRe.forEach(t),axe=i(c),pr=n(c,"DIV",{class:!0});var Ll=s(pr);m(Vy.$$.fragment,Ll),LVo=i(Ll),Yd=n(Ll,"P",{});var jz=s(Yd);BVo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Lce=n(jz,"CODE",{});var Ftt=s(Lce);xVo=r(Ftt,"from_pretrained()"),Ftt.forEach(t),kVo=r(jz,"class method or the "),Bce=n(jz,"CODE",{});var Ctt=s(Bce);RVo=r(Ctt,"from_config()"),Ctt.forEach(t),SVo=r(jz,`class
method.`),jz.forEach(t),PVo=i(Ll),zy=n(Ll,"P",{});var mRe=s(zy);$Vo=r(mRe,"This class cannot be instantiated directly using "),xce=n(mRe,"CODE",{});var Mtt=s(xce);IVo=r(Mtt,"__init__()"),Mtt.forEach(t),DVo=r(mRe," (throws an error)."),mRe.forEach(t),jVo=i(Ll),it=n(Ll,"DIV",{class:!0});var Bl=s(it);m(Wy.$$.fragment,Bl),NVo=i(Bl),kce=n(Bl,"P",{});var Ett=s(kce);qVo=r(Ett,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),Ett.forEach(t),GVo=i(Bl),Kd=n(Bl,"P",{});var Nz=s(Kd);OVo=r(Nz,`Note:
Loading a model from its configuration file does `),Rce=n(Nz,"STRONG",{});var ytt=s(Rce);XVo=r(ytt,"not"),ytt.forEach(t),VVo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sce=n(Nz,"CODE",{});var wtt=s(Sce);zVo=r(wtt,"from_pretrained()"),wtt.forEach(t),WVo=r(Nz,"to load the model weights."),Nz.forEach(t),QVo=i(Bl),Pce=n(Bl,"P",{});var Att=s(Pce);HVo=r(Att,"Examples:"),Att.forEach(t),UVo=i(Bl),m(Qy.$$.fragment,Bl),Bl.forEach(t),JVo=i(Ll),to=n(Ll,"DIV",{class:!0});var ta=s(to);m(Hy.$$.fragment,ta),YVo=i(ta),$ce=n(ta,"P",{});var Ltt=s($ce);KVo=r(Ltt,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),Ltt.forEach(t),ZVo=i(ta),ln=n(ta,"P",{});var r4=s(ln);ezo=r(r4,"The model class to instantiate is selected based on the "),Ice=n(r4,"CODE",{});var Btt=s(Ice);ozo=r(Btt,"model_type"),Btt.forEach(t),rzo=r(r4,` property of the config object (either
passed as an argument or loaded from `),Dce=n(r4,"CODE",{});var xtt=s(Dce);tzo=r(xtt,"pretrained_model_name_or_path"),xtt.forEach(t),azo=r(r4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jce=n(r4,"CODE",{});var ktt=s(jce);nzo=r(ktt,"pretrained_model_name_or_path"),ktt.forEach(t),szo=r(r4,":"),r4.forEach(t),lzo=i(ta),Nce=n(ta,"UL",{});var Rtt=s(Nce);$2=n(Rtt,"LI",{});var Pwe=s($2);qce=n(Pwe,"STRONG",{});var Stt=s(qce);izo=r(Stt,"detr"),Stt.forEach(t),dzo=r(Pwe," \u2014 "),EN=n(Pwe,"A",{href:!0});var Ptt=s(EN);czo=r(Ptt,"DetrForSegmentation"),Ptt.forEach(t),fzo=r(Pwe," (DETR model)"),Pwe.forEach(t),Rtt.forEach(t),mzo=i(ta),I2=n(ta,"P",{});var $we=s(I2);gzo=r($we,"The model is set in evaluation mode by default using "),Gce=n($we,"CODE",{});var $tt=s(Gce);hzo=r($tt,"model.eval()"),$tt.forEach(t),pzo=r($we,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Oce=n($we,"CODE",{});var Itt=s(Oce);_zo=r(Itt,"model.train()"),Itt.forEach(t),$we.forEach(t),uzo=i(ta),Xce=n(ta,"P",{});var Dtt=s(Xce);bzo=r(Dtt,"Examples:"),Dtt.forEach(t),vzo=i(ta),m(Uy.$$.fragment,ta),ta.forEach(t),Ll.forEach(t),nxe=i(c),Zd=n(c,"H2",{class:!0});var gRe=s(Zd);D2=n(gRe,"A",{id:!0,class:!0,href:!0});var jtt=s(D2);Vce=n(jtt,"SPAN",{});var Ntt=s(Vce);m(Jy.$$.fragment,Ntt),Ntt.forEach(t),jtt.forEach(t),Tzo=i(gRe),zce=n(gRe,"SPAN",{});var qtt=s(zce);Fzo=r(qtt,"AutoModelForSemanticSegmentation"),qtt.forEach(t),gRe.forEach(t),sxe=i(c),_r=n(c,"DIV",{class:!0});var xl=s(_r);m(Yy.$$.fragment,xl),Czo=i(xl),ec=n(xl,"P",{});var qz=s(ec);Mzo=r(qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Wce=n(qz,"CODE",{});var Gtt=s(Wce);Ezo=r(Gtt,"from_pretrained()"),Gtt.forEach(t),yzo=r(qz,"class method or the "),Qce=n(qz,"CODE",{});var Ott=s(Qce);wzo=r(Ott,"from_config()"),Ott.forEach(t),Azo=r(qz,`class
method.`),qz.forEach(t),Lzo=i(xl),Ky=n(xl,"P",{});var hRe=s(Ky);Bzo=r(hRe,"This class cannot be instantiated directly using "),Hce=n(hRe,"CODE",{});var Xtt=s(Hce);xzo=r(Xtt,"__init__()"),Xtt.forEach(t),kzo=r(hRe," (throws an error)."),hRe.forEach(t),Rzo=i(xl),dt=n(xl,"DIV",{class:!0});var kl=s(dt);m(Zy.$$.fragment,kl),Szo=i(kl),Uce=n(kl,"P",{});var Vtt=s(Uce);Pzo=r(Vtt,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Vtt.forEach(t),$zo=i(kl),oc=n(kl,"P",{});var Gz=s(oc);Izo=r(Gz,`Note:
Loading a model from its configuration file does `),Jce=n(Gz,"STRONG",{});var ztt=s(Jce);Dzo=r(ztt,"not"),ztt.forEach(t),jzo=r(Gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yce=n(Gz,"CODE",{});var Wtt=s(Yce);Nzo=r(Wtt,"from_pretrained()"),Wtt.forEach(t),qzo=r(Gz,"to load the model weights."),Gz.forEach(t),Gzo=i(kl),Kce=n(kl,"P",{});var Qtt=s(Kce);Ozo=r(Qtt,"Examples:"),Qtt.forEach(t),Xzo=i(kl),m(ew.$$.fragment,kl),kl.forEach(t),Vzo=i(xl),ao=n(xl,"DIV",{class:!0});var aa=s(ao);m(ow.$$.fragment,aa),zzo=i(aa),Zce=n(aa,"P",{});var Htt=s(Zce);Wzo=r(Htt,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),Htt.forEach(t),Qzo=i(aa),dn=n(aa,"P",{});var t4=s(dn);Hzo=r(t4,"The model class to instantiate is selected based on the "),efe=n(t4,"CODE",{});var Utt=s(efe);Uzo=r(Utt,"model_type"),Utt.forEach(t),Jzo=r(t4,` property of the config object (either
passed as an argument or loaded from `),ofe=n(t4,"CODE",{});var Jtt=s(ofe);Yzo=r(Jtt,"pretrained_model_name_or_path"),Jtt.forEach(t),Kzo=r(t4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rfe=n(t4,"CODE",{});var Ytt=s(rfe);Zzo=r(Ytt,"pretrained_model_name_or_path"),Ytt.forEach(t),eWo=r(t4,":"),t4.forEach(t),oWo=i(aa),rw=n(aa,"UL",{});var pRe=s(rw);j2=n(pRe,"LI",{});var Iwe=s(j2);tfe=n(Iwe,"STRONG",{});var Ktt=s(tfe);rWo=r(Ktt,"beit"),Ktt.forEach(t),tWo=r(Iwe," \u2014 "),yN=n(Iwe,"A",{href:!0});var Ztt=s(yN);aWo=r(Ztt,"BeitForSemanticSegmentation"),Ztt.forEach(t),nWo=r(Iwe," (BEiT model)"),Iwe.forEach(t),sWo=i(pRe),N2=n(pRe,"LI",{});var Dwe=s(N2);afe=n(Dwe,"STRONG",{});var eat=s(afe);lWo=r(eat,"segformer"),eat.forEach(t),iWo=r(Dwe," \u2014 "),wN=n(Dwe,"A",{href:!0});var oat=s(wN);dWo=r(oat,"SegformerForSemanticSegmentation"),oat.forEach(t),cWo=r(Dwe," (SegFormer model)"),Dwe.forEach(t),pRe.forEach(t),fWo=i(aa),q2=n(aa,"P",{});var jwe=s(q2);mWo=r(jwe,"The model is set in evaluation mode by default using "),nfe=n(jwe,"CODE",{});var rat=s(nfe);gWo=r(rat,"model.eval()"),rat.forEach(t),hWo=r(jwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sfe=n(jwe,"CODE",{});var tat=s(sfe);pWo=r(tat,"model.train()"),tat.forEach(t),jwe.forEach(t),_Wo=i(aa),lfe=n(aa,"P",{});var aat=s(lfe);uWo=r(aat,"Examples:"),aat.forEach(t),bWo=i(aa),m(tw.$$.fragment,aa),aa.forEach(t),xl.forEach(t),lxe=i(c),rc=n(c,"H2",{class:!0});var _Re=s(rc);G2=n(_Re,"A",{id:!0,class:!0,href:!0});var nat=s(G2);ife=n(nat,"SPAN",{});var sat=s(ife);m(aw.$$.fragment,sat),sat.forEach(t),nat.forEach(t),vWo=i(_Re),dfe=n(_Re,"SPAN",{});var lat=s(dfe);TWo=r(lat,"TFAutoModel"),lat.forEach(t),_Re.forEach(t),ixe=i(c),ur=n(c,"DIV",{class:!0});var Rl=s(ur);m(nw.$$.fragment,Rl),FWo=i(Rl),tc=n(Rl,"P",{});var Oz=s(tc);CWo=r(Oz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),cfe=n(Oz,"CODE",{});var iat=s(cfe);MWo=r(iat,"from_pretrained()"),iat.forEach(t),EWo=r(Oz,"class method or the "),ffe=n(Oz,"CODE",{});var dat=s(ffe);yWo=r(dat,"from_config()"),dat.forEach(t),wWo=r(Oz,`class
method.`),Oz.forEach(t),AWo=i(Rl),sw=n(Rl,"P",{});var uRe=s(sw);LWo=r(uRe,"This class cannot be instantiated directly using "),mfe=n(uRe,"CODE",{});var cat=s(mfe);BWo=r(cat,"__init__()"),cat.forEach(t),xWo=r(uRe," (throws an error)."),uRe.forEach(t),kWo=i(Rl),ct=n(Rl,"DIV",{class:!0});var Sl=s(ct);m(lw.$$.fragment,Sl),RWo=i(Sl),gfe=n(Sl,"P",{});var fat=s(gfe);SWo=r(fat,"Instantiates one of the base model classes of the library from a configuration."),fat.forEach(t),PWo=i(Sl),ac=n(Sl,"P",{});var Xz=s(ac);$Wo=r(Xz,`Note:
Loading a model from its configuration file does `),hfe=n(Xz,"STRONG",{});var mat=s(hfe);IWo=r(mat,"not"),mat.forEach(t),DWo=r(Xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),pfe=n(Xz,"CODE",{});var gat=s(pfe);jWo=r(gat,"from_pretrained()"),gat.forEach(t),NWo=r(Xz,"to load the model weights."),Xz.forEach(t),qWo=i(Sl),_fe=n(Sl,"P",{});var hat=s(_fe);GWo=r(hat,"Examples:"),hat.forEach(t),OWo=i(Sl),m(iw.$$.fragment,Sl),Sl.forEach(t),XWo=i(Rl),go=n(Rl,"DIV",{class:!0});var ca=s(go);m(dw.$$.fragment,ca),VWo=i(ca),ufe=n(ca,"P",{});var pat=s(ufe);zWo=r(pat,"Instantiate one of the base model classes of the library from a pretrained model."),pat.forEach(t),WWo=i(ca),cn=n(ca,"P",{});var a4=s(cn);QWo=r(a4,"The model class to instantiate is selected based on the "),bfe=n(a4,"CODE",{});var _at=s(bfe);HWo=r(_at,"model_type"),_at.forEach(t),UWo=r(a4,` property of the config object (either
passed as an argument or loaded from `),vfe=n(a4,"CODE",{});var uat=s(vfe);JWo=r(uat,"pretrained_model_name_or_path"),uat.forEach(t),YWo=r(a4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tfe=n(a4,"CODE",{});var bat=s(Tfe);KWo=r(bat,"pretrained_model_name_or_path"),bat.forEach(t),ZWo=r(a4,":"),a4.forEach(t),eQo=i(ca),B=n(ca,"UL",{});var x=s(B);O2=n(x,"LI",{});var Nwe=s(O2);Ffe=n(Nwe,"STRONG",{});var vat=s(Ffe);oQo=r(vat,"albert"),vat.forEach(t),rQo=r(Nwe," \u2014 "),AN=n(Nwe,"A",{href:!0});var Tat=s(AN);tQo=r(Tat,"TFAlbertModel"),Tat.forEach(t),aQo=r(Nwe," (ALBERT model)"),Nwe.forEach(t),nQo=i(x),X2=n(x,"LI",{});var qwe=s(X2);Cfe=n(qwe,"STRONG",{});var Fat=s(Cfe);sQo=r(Fat,"bart"),Fat.forEach(t),lQo=r(qwe," \u2014 "),LN=n(qwe,"A",{href:!0});var Cat=s(LN);iQo=r(Cat,"TFBartModel"),Cat.forEach(t),dQo=r(qwe," (BART model)"),qwe.forEach(t),cQo=i(x),V2=n(x,"LI",{});var Gwe=s(V2);Mfe=n(Gwe,"STRONG",{});var Mat=s(Mfe);fQo=r(Mat,"bert"),Mat.forEach(t),mQo=r(Gwe," \u2014 "),BN=n(Gwe,"A",{href:!0});var Eat=s(BN);gQo=r(Eat,"TFBertModel"),Eat.forEach(t),hQo=r(Gwe," (BERT model)"),Gwe.forEach(t),pQo=i(x),z2=n(x,"LI",{});var Owe=s(z2);Efe=n(Owe,"STRONG",{});var yat=s(Efe);_Qo=r(yat,"blenderbot"),yat.forEach(t),uQo=r(Owe," \u2014 "),xN=n(Owe,"A",{href:!0});var wat=s(xN);bQo=r(wat,"TFBlenderbotModel"),wat.forEach(t),vQo=r(Owe," (Blenderbot model)"),Owe.forEach(t),TQo=i(x),W2=n(x,"LI",{});var Xwe=s(W2);yfe=n(Xwe,"STRONG",{});var Aat=s(yfe);FQo=r(Aat,"blenderbot-small"),Aat.forEach(t),CQo=r(Xwe," \u2014 "),kN=n(Xwe,"A",{href:!0});var Lat=s(kN);MQo=r(Lat,"TFBlenderbotSmallModel"),Lat.forEach(t),EQo=r(Xwe," (BlenderbotSmall model)"),Xwe.forEach(t),yQo=i(x),Q2=n(x,"LI",{});var Vwe=s(Q2);wfe=n(Vwe,"STRONG",{});var Bat=s(wfe);wQo=r(Bat,"camembert"),Bat.forEach(t),AQo=r(Vwe," \u2014 "),RN=n(Vwe,"A",{href:!0});var xat=s(RN);LQo=r(xat,"TFCamembertModel"),xat.forEach(t),BQo=r(Vwe," (CamemBERT model)"),Vwe.forEach(t),xQo=i(x),H2=n(x,"LI",{});var zwe=s(H2);Afe=n(zwe,"STRONG",{});var kat=s(Afe);kQo=r(kat,"clip"),kat.forEach(t),RQo=r(zwe," \u2014 "),SN=n(zwe,"A",{href:!0});var Rat=s(SN);SQo=r(Rat,"TFCLIPModel"),Rat.forEach(t),PQo=r(zwe," (CLIP model)"),zwe.forEach(t),$Qo=i(x),U2=n(x,"LI",{});var Wwe=s(U2);Lfe=n(Wwe,"STRONG",{});var Sat=s(Lfe);IQo=r(Sat,"convbert"),Sat.forEach(t),DQo=r(Wwe," \u2014 "),PN=n(Wwe,"A",{href:!0});var Pat=s(PN);jQo=r(Pat,"TFConvBertModel"),Pat.forEach(t),NQo=r(Wwe," (ConvBERT model)"),Wwe.forEach(t),qQo=i(x),J2=n(x,"LI",{});var Qwe=s(J2);Bfe=n(Qwe,"STRONG",{});var $at=s(Bfe);GQo=r($at,"convnext"),$at.forEach(t),OQo=r(Qwe," \u2014 "),$N=n(Qwe,"A",{href:!0});var Iat=s($N);XQo=r(Iat,"TFConvNextModel"),Iat.forEach(t),VQo=r(Qwe," (ConvNext model)"),Qwe.forEach(t),zQo=i(x),Y2=n(x,"LI",{});var Hwe=s(Y2);xfe=n(Hwe,"STRONG",{});var Dat=s(xfe);WQo=r(Dat,"ctrl"),Dat.forEach(t),QQo=r(Hwe," \u2014 "),IN=n(Hwe,"A",{href:!0});var jat=s(IN);HQo=r(jat,"TFCTRLModel"),jat.forEach(t),UQo=r(Hwe," (CTRL model)"),Hwe.forEach(t),JQo=i(x),K2=n(x,"LI",{});var Uwe=s(K2);kfe=n(Uwe,"STRONG",{});var Nat=s(kfe);YQo=r(Nat,"deberta"),Nat.forEach(t),KQo=r(Uwe," \u2014 "),DN=n(Uwe,"A",{href:!0});var qat=s(DN);ZQo=r(qat,"TFDebertaModel"),qat.forEach(t),eHo=r(Uwe," (DeBERTa model)"),Uwe.forEach(t),oHo=i(x),Z2=n(x,"LI",{});var Jwe=s(Z2);Rfe=n(Jwe,"STRONG",{});var Gat=s(Rfe);rHo=r(Gat,"deberta-v2"),Gat.forEach(t),tHo=r(Jwe," \u2014 "),jN=n(Jwe,"A",{href:!0});var Oat=s(jN);aHo=r(Oat,"TFDebertaV2Model"),Oat.forEach(t),nHo=r(Jwe," (DeBERTa-v2 model)"),Jwe.forEach(t),sHo=i(x),ev=n(x,"LI",{});var Ywe=s(ev);Sfe=n(Ywe,"STRONG",{});var Xat=s(Sfe);lHo=r(Xat,"distilbert"),Xat.forEach(t),iHo=r(Ywe," \u2014 "),NN=n(Ywe,"A",{href:!0});var Vat=s(NN);dHo=r(Vat,"TFDistilBertModel"),Vat.forEach(t),cHo=r(Ywe," (DistilBERT model)"),Ywe.forEach(t),fHo=i(x),ov=n(x,"LI",{});var Kwe=s(ov);Pfe=n(Kwe,"STRONG",{});var zat=s(Pfe);mHo=r(zat,"dpr"),zat.forEach(t),gHo=r(Kwe," \u2014 "),qN=n(Kwe,"A",{href:!0});var Wat=s(qN);hHo=r(Wat,"TFDPRQuestionEncoder"),Wat.forEach(t),pHo=r(Kwe," (DPR model)"),Kwe.forEach(t),_Ho=i(x),rv=n(x,"LI",{});var Zwe=s(rv);$fe=n(Zwe,"STRONG",{});var Qat=s($fe);uHo=r(Qat,"electra"),Qat.forEach(t),bHo=r(Zwe," \u2014 "),GN=n(Zwe,"A",{href:!0});var Hat=s(GN);vHo=r(Hat,"TFElectraModel"),Hat.forEach(t),THo=r(Zwe," (ELECTRA model)"),Zwe.forEach(t),FHo=i(x),tv=n(x,"LI",{});var e6e=s(tv);Ife=n(e6e,"STRONG",{});var Uat=s(Ife);CHo=r(Uat,"flaubert"),Uat.forEach(t),MHo=r(e6e," \u2014 "),ON=n(e6e,"A",{href:!0});var Jat=s(ON);EHo=r(Jat,"TFFlaubertModel"),Jat.forEach(t),yHo=r(e6e," (FlauBERT model)"),e6e.forEach(t),wHo=i(x),Is=n(x,"LI",{});var p8=s(Is);Dfe=n(p8,"STRONG",{});var Yat=s(Dfe);AHo=r(Yat,"funnel"),Yat.forEach(t),LHo=r(p8," \u2014 "),XN=n(p8,"A",{href:!0});var Kat=s(XN);BHo=r(Kat,"TFFunnelModel"),Kat.forEach(t),xHo=r(p8," or "),VN=n(p8,"A",{href:!0});var Zat=s(VN);kHo=r(Zat,"TFFunnelBaseModel"),Zat.forEach(t),RHo=r(p8," (Funnel Transformer model)"),p8.forEach(t),SHo=i(x),av=n(x,"LI",{});var o6e=s(av);jfe=n(o6e,"STRONG",{});var ent=s(jfe);PHo=r(ent,"gpt2"),ent.forEach(t),$Ho=r(o6e," \u2014 "),zN=n(o6e,"A",{href:!0});var ont=s(zN);IHo=r(ont,"TFGPT2Model"),ont.forEach(t),DHo=r(o6e," (OpenAI GPT-2 model)"),o6e.forEach(t),jHo=i(x),nv=n(x,"LI",{});var r6e=s(nv);Nfe=n(r6e,"STRONG",{});var rnt=s(Nfe);NHo=r(rnt,"hubert"),rnt.forEach(t),qHo=r(r6e," \u2014 "),WN=n(r6e,"A",{href:!0});var tnt=s(WN);GHo=r(tnt,"TFHubertModel"),tnt.forEach(t),OHo=r(r6e," (Hubert model)"),r6e.forEach(t),XHo=i(x),sv=n(x,"LI",{});var t6e=s(sv);qfe=n(t6e,"STRONG",{});var ant=s(qfe);VHo=r(ant,"layoutlm"),ant.forEach(t),zHo=r(t6e," \u2014 "),QN=n(t6e,"A",{href:!0});var nnt=s(QN);WHo=r(nnt,"TFLayoutLMModel"),nnt.forEach(t),QHo=r(t6e," (LayoutLM model)"),t6e.forEach(t),HHo=i(x),lv=n(x,"LI",{});var a6e=s(lv);Gfe=n(a6e,"STRONG",{});var snt=s(Gfe);UHo=r(snt,"led"),snt.forEach(t),JHo=r(a6e," \u2014 "),HN=n(a6e,"A",{href:!0});var lnt=s(HN);YHo=r(lnt,"TFLEDModel"),lnt.forEach(t),KHo=r(a6e," (LED model)"),a6e.forEach(t),ZHo=i(x),iv=n(x,"LI",{});var n6e=s(iv);Ofe=n(n6e,"STRONG",{});var int=s(Ofe);eUo=r(int,"longformer"),int.forEach(t),oUo=r(n6e," \u2014 "),UN=n(n6e,"A",{href:!0});var dnt=s(UN);rUo=r(dnt,"TFLongformerModel"),dnt.forEach(t),tUo=r(n6e," (Longformer model)"),n6e.forEach(t),aUo=i(x),dv=n(x,"LI",{});var s6e=s(dv);Xfe=n(s6e,"STRONG",{});var cnt=s(Xfe);nUo=r(cnt,"lxmert"),cnt.forEach(t),sUo=r(s6e," \u2014 "),JN=n(s6e,"A",{href:!0});var fnt=s(JN);lUo=r(fnt,"TFLxmertModel"),fnt.forEach(t),iUo=r(s6e," (LXMERT model)"),s6e.forEach(t),dUo=i(x),cv=n(x,"LI",{});var l6e=s(cv);Vfe=n(l6e,"STRONG",{});var mnt=s(Vfe);cUo=r(mnt,"marian"),mnt.forEach(t),fUo=r(l6e," \u2014 "),YN=n(l6e,"A",{href:!0});var gnt=s(YN);mUo=r(gnt,"TFMarianModel"),gnt.forEach(t),gUo=r(l6e," (Marian model)"),l6e.forEach(t),hUo=i(x),fv=n(x,"LI",{});var i6e=s(fv);zfe=n(i6e,"STRONG",{});var hnt=s(zfe);pUo=r(hnt,"mbart"),hnt.forEach(t),_Uo=r(i6e," \u2014 "),KN=n(i6e,"A",{href:!0});var pnt=s(KN);uUo=r(pnt,"TFMBartModel"),pnt.forEach(t),bUo=r(i6e," (mBART model)"),i6e.forEach(t),vUo=i(x),mv=n(x,"LI",{});var d6e=s(mv);Wfe=n(d6e,"STRONG",{});var _nt=s(Wfe);TUo=r(_nt,"mobilebert"),_nt.forEach(t),FUo=r(d6e," \u2014 "),ZN=n(d6e,"A",{href:!0});var unt=s(ZN);CUo=r(unt,"TFMobileBertModel"),unt.forEach(t),MUo=r(d6e," (MobileBERT model)"),d6e.forEach(t),EUo=i(x),gv=n(x,"LI",{});var c6e=s(gv);Qfe=n(c6e,"STRONG",{});var bnt=s(Qfe);yUo=r(bnt,"mpnet"),bnt.forEach(t),wUo=r(c6e," \u2014 "),eq=n(c6e,"A",{href:!0});var vnt=s(eq);AUo=r(vnt,"TFMPNetModel"),vnt.forEach(t),LUo=r(c6e," (MPNet model)"),c6e.forEach(t),BUo=i(x),hv=n(x,"LI",{});var f6e=s(hv);Hfe=n(f6e,"STRONG",{});var Tnt=s(Hfe);xUo=r(Tnt,"mt5"),Tnt.forEach(t),kUo=r(f6e," \u2014 "),oq=n(f6e,"A",{href:!0});var Fnt=s(oq);RUo=r(Fnt,"TFMT5Model"),Fnt.forEach(t),SUo=r(f6e," (mT5 model)"),f6e.forEach(t),PUo=i(x),pv=n(x,"LI",{});var m6e=s(pv);Ufe=n(m6e,"STRONG",{});var Cnt=s(Ufe);$Uo=r(Cnt,"openai-gpt"),Cnt.forEach(t),IUo=r(m6e," \u2014 "),rq=n(m6e,"A",{href:!0});var Mnt=s(rq);DUo=r(Mnt,"TFOpenAIGPTModel"),Mnt.forEach(t),jUo=r(m6e," (OpenAI GPT model)"),m6e.forEach(t),NUo=i(x),_v=n(x,"LI",{});var g6e=s(_v);Jfe=n(g6e,"STRONG",{});var Ent=s(Jfe);qUo=r(Ent,"pegasus"),Ent.forEach(t),GUo=r(g6e," \u2014 "),tq=n(g6e,"A",{href:!0});var ynt=s(tq);OUo=r(ynt,"TFPegasusModel"),ynt.forEach(t),XUo=r(g6e," (Pegasus model)"),g6e.forEach(t),VUo=i(x),uv=n(x,"LI",{});var h6e=s(uv);Yfe=n(h6e,"STRONG",{});var wnt=s(Yfe);zUo=r(wnt,"rembert"),wnt.forEach(t),WUo=r(h6e," \u2014 "),aq=n(h6e,"A",{href:!0});var Ant=s(aq);QUo=r(Ant,"TFRemBertModel"),Ant.forEach(t),HUo=r(h6e," (RemBERT model)"),h6e.forEach(t),UUo=i(x),bv=n(x,"LI",{});var p6e=s(bv);Kfe=n(p6e,"STRONG",{});var Lnt=s(Kfe);JUo=r(Lnt,"roberta"),Lnt.forEach(t),YUo=r(p6e," \u2014 "),nq=n(p6e,"A",{href:!0});var Bnt=s(nq);KUo=r(Bnt,"TFRobertaModel"),Bnt.forEach(t),ZUo=r(p6e," (RoBERTa model)"),p6e.forEach(t),eJo=i(x),vv=n(x,"LI",{});var _6e=s(vv);Zfe=n(_6e,"STRONG",{});var xnt=s(Zfe);oJo=r(xnt,"roformer"),xnt.forEach(t),rJo=r(_6e," \u2014 "),sq=n(_6e,"A",{href:!0});var knt=s(sq);tJo=r(knt,"TFRoFormerModel"),knt.forEach(t),aJo=r(_6e," (RoFormer model)"),_6e.forEach(t),nJo=i(x),Tv=n(x,"LI",{});var u6e=s(Tv);eme=n(u6e,"STRONG",{});var Rnt=s(eme);sJo=r(Rnt,"speech_to_text"),Rnt.forEach(t),lJo=r(u6e," \u2014 "),lq=n(u6e,"A",{href:!0});var Snt=s(lq);iJo=r(Snt,"TFSpeech2TextModel"),Snt.forEach(t),dJo=r(u6e," (Speech2Text model)"),u6e.forEach(t),cJo=i(x),Fv=n(x,"LI",{});var b6e=s(Fv);ome=n(b6e,"STRONG",{});var Pnt=s(ome);fJo=r(Pnt,"t5"),Pnt.forEach(t),mJo=r(b6e," \u2014 "),iq=n(b6e,"A",{href:!0});var $nt=s(iq);gJo=r($nt,"TFT5Model"),$nt.forEach(t),hJo=r(b6e," (T5 model)"),b6e.forEach(t),pJo=i(x),Cv=n(x,"LI",{});var v6e=s(Cv);rme=n(v6e,"STRONG",{});var Int=s(rme);_Jo=r(Int,"tapas"),Int.forEach(t),uJo=r(v6e," \u2014 "),dq=n(v6e,"A",{href:!0});var Dnt=s(dq);bJo=r(Dnt,"TFTapasModel"),Dnt.forEach(t),vJo=r(v6e," (TAPAS model)"),v6e.forEach(t),TJo=i(x),Mv=n(x,"LI",{});var T6e=s(Mv);tme=n(T6e,"STRONG",{});var jnt=s(tme);FJo=r(jnt,"transfo-xl"),jnt.forEach(t),CJo=r(T6e," \u2014 "),cq=n(T6e,"A",{href:!0});var Nnt=s(cq);MJo=r(Nnt,"TFTransfoXLModel"),Nnt.forEach(t),EJo=r(T6e," (Transformer-XL model)"),T6e.forEach(t),yJo=i(x),Ev=n(x,"LI",{});var F6e=s(Ev);ame=n(F6e,"STRONG",{});var qnt=s(ame);wJo=r(qnt,"vit"),qnt.forEach(t),AJo=r(F6e," \u2014 "),fq=n(F6e,"A",{href:!0});var Gnt=s(fq);LJo=r(Gnt,"TFViTModel"),Gnt.forEach(t),BJo=r(F6e," (ViT model)"),F6e.forEach(t),xJo=i(x),yv=n(x,"LI",{});var C6e=s(yv);nme=n(C6e,"STRONG",{});var Ont=s(nme);kJo=r(Ont,"wav2vec2"),Ont.forEach(t),RJo=r(C6e," \u2014 "),mq=n(C6e,"A",{href:!0});var Xnt=s(mq);SJo=r(Xnt,"TFWav2Vec2Model"),Xnt.forEach(t),PJo=r(C6e," (Wav2Vec2 model)"),C6e.forEach(t),$Jo=i(x),wv=n(x,"LI",{});var M6e=s(wv);sme=n(M6e,"STRONG",{});var Vnt=s(sme);IJo=r(Vnt,"xlm"),Vnt.forEach(t),DJo=r(M6e," \u2014 "),gq=n(M6e,"A",{href:!0});var znt=s(gq);jJo=r(znt,"TFXLMModel"),znt.forEach(t),NJo=r(M6e," (XLM model)"),M6e.forEach(t),qJo=i(x),Av=n(x,"LI",{});var E6e=s(Av);lme=n(E6e,"STRONG",{});var Wnt=s(lme);GJo=r(Wnt,"xlm-roberta"),Wnt.forEach(t),OJo=r(E6e," \u2014 "),hq=n(E6e,"A",{href:!0});var Qnt=s(hq);XJo=r(Qnt,"TFXLMRobertaModel"),Qnt.forEach(t),VJo=r(E6e," (XLM-RoBERTa model)"),E6e.forEach(t),zJo=i(x),Lv=n(x,"LI",{});var y6e=s(Lv);ime=n(y6e,"STRONG",{});var Hnt=s(ime);WJo=r(Hnt,"xlnet"),Hnt.forEach(t),QJo=r(y6e," \u2014 "),pq=n(y6e,"A",{href:!0});var Unt=s(pq);HJo=r(Unt,"TFXLNetModel"),Unt.forEach(t),UJo=r(y6e," (XLNet model)"),y6e.forEach(t),x.forEach(t),JJo=i(ca),dme=n(ca,"P",{});var Jnt=s(dme);YJo=r(Jnt,"Examples:"),Jnt.forEach(t),KJo=i(ca),m(cw.$$.fragment,ca),ca.forEach(t),Rl.forEach(t),dxe=i(c),nc=n(c,"H2",{class:!0});var bRe=s(nc);Bv=n(bRe,"A",{id:!0,class:!0,href:!0});var Ynt=s(Bv);cme=n(Ynt,"SPAN",{});var Knt=s(cme);m(fw.$$.fragment,Knt),Knt.forEach(t),Ynt.forEach(t),ZJo=i(bRe),fme=n(bRe,"SPAN",{});var Znt=s(fme);eYo=r(Znt,"TFAutoModelForPreTraining"),Znt.forEach(t),bRe.forEach(t),cxe=i(c),br=n(c,"DIV",{class:!0});var Pl=s(br);m(mw.$$.fragment,Pl),oYo=i(Pl),sc=n(Pl,"P",{});var Vz=s(sc);rYo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),mme=n(Vz,"CODE",{});var est=s(mme);tYo=r(est,"from_pretrained()"),est.forEach(t),aYo=r(Vz,"class method or the "),gme=n(Vz,"CODE",{});var ost=s(gme);nYo=r(ost,"from_config()"),ost.forEach(t),sYo=r(Vz,`class
method.`),Vz.forEach(t),lYo=i(Pl),gw=n(Pl,"P",{});var vRe=s(gw);iYo=r(vRe,"This class cannot be instantiated directly using "),hme=n(vRe,"CODE",{});var rst=s(hme);dYo=r(rst,"__init__()"),rst.forEach(t),cYo=r(vRe," (throws an error)."),vRe.forEach(t),fYo=i(Pl),ft=n(Pl,"DIV",{class:!0});var $l=s(ft);m(hw.$$.fragment,$l),mYo=i($l),pme=n($l,"P",{});var tst=s(pme);gYo=r(tst,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),tst.forEach(t),hYo=i($l),lc=n($l,"P",{});var zz=s(lc);pYo=r(zz,`Note:
Loading a model from its configuration file does `),_me=n(zz,"STRONG",{});var ast=s(_me);_Yo=r(ast,"not"),ast.forEach(t),uYo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ume=n(zz,"CODE",{});var nst=s(ume);bYo=r(nst,"from_pretrained()"),nst.forEach(t),vYo=r(zz,"to load the model weights."),zz.forEach(t),TYo=i($l),bme=n($l,"P",{});var sst=s(bme);FYo=r(sst,"Examples:"),sst.forEach(t),CYo=i($l),m(pw.$$.fragment,$l),$l.forEach(t),MYo=i(Pl),ho=n(Pl,"DIV",{class:!0});var fa=s(ho);m(_w.$$.fragment,fa),EYo=i(fa),vme=n(fa,"P",{});var lst=s(vme);yYo=r(lst,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),lst.forEach(t),wYo=i(fa),fn=n(fa,"P",{});var n4=s(fn);AYo=r(n4,"The model class to instantiate is selected based on the "),Tme=n(n4,"CODE",{});var ist=s(Tme);LYo=r(ist,"model_type"),ist.forEach(t),BYo=r(n4,` property of the config object (either
passed as an argument or loaded from `),Fme=n(n4,"CODE",{});var dst=s(Fme);xYo=r(dst,"pretrained_model_name_or_path"),dst.forEach(t),kYo=r(n4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cme=n(n4,"CODE",{});var cst=s(Cme);RYo=r(cst,"pretrained_model_name_or_path"),cst.forEach(t),SYo=r(n4,":"),n4.forEach(t),PYo=i(fa),H=n(fa,"UL",{});var U=s(H);xv=n(U,"LI",{});var w6e=s(xv);Mme=n(w6e,"STRONG",{});var fst=s(Mme);$Yo=r(fst,"albert"),fst.forEach(t),IYo=r(w6e," \u2014 "),_q=n(w6e,"A",{href:!0});var mst=s(_q);DYo=r(mst,"TFAlbertForPreTraining"),mst.forEach(t),jYo=r(w6e," (ALBERT model)"),w6e.forEach(t),NYo=i(U),kv=n(U,"LI",{});var A6e=s(kv);Eme=n(A6e,"STRONG",{});var gst=s(Eme);qYo=r(gst,"bart"),gst.forEach(t),GYo=r(A6e," \u2014 "),uq=n(A6e,"A",{href:!0});var hst=s(uq);OYo=r(hst,"TFBartForConditionalGeneration"),hst.forEach(t),XYo=r(A6e," (BART model)"),A6e.forEach(t),VYo=i(U),Rv=n(U,"LI",{});var L6e=s(Rv);yme=n(L6e,"STRONG",{});var pst=s(yme);zYo=r(pst,"bert"),pst.forEach(t),WYo=r(L6e," \u2014 "),bq=n(L6e,"A",{href:!0});var _st=s(bq);QYo=r(_st,"TFBertForPreTraining"),_st.forEach(t),HYo=r(L6e," (BERT model)"),L6e.forEach(t),UYo=i(U),Sv=n(U,"LI",{});var B6e=s(Sv);wme=n(B6e,"STRONG",{});var ust=s(wme);JYo=r(ust,"camembert"),ust.forEach(t),YYo=r(B6e," \u2014 "),vq=n(B6e,"A",{href:!0});var bst=s(vq);KYo=r(bst,"TFCamembertForMaskedLM"),bst.forEach(t),ZYo=r(B6e," (CamemBERT model)"),B6e.forEach(t),eKo=i(U),Pv=n(U,"LI",{});var x6e=s(Pv);Ame=n(x6e,"STRONG",{});var vst=s(Ame);oKo=r(vst,"ctrl"),vst.forEach(t),rKo=r(x6e," \u2014 "),Tq=n(x6e,"A",{href:!0});var Tst=s(Tq);tKo=r(Tst,"TFCTRLLMHeadModel"),Tst.forEach(t),aKo=r(x6e," (CTRL model)"),x6e.forEach(t),nKo=i(U),$v=n(U,"LI",{});var k6e=s($v);Lme=n(k6e,"STRONG",{});var Fst=s(Lme);sKo=r(Fst,"distilbert"),Fst.forEach(t),lKo=r(k6e," \u2014 "),Fq=n(k6e,"A",{href:!0});var Cst=s(Fq);iKo=r(Cst,"TFDistilBertForMaskedLM"),Cst.forEach(t),dKo=r(k6e," (DistilBERT model)"),k6e.forEach(t),cKo=i(U),Iv=n(U,"LI",{});var R6e=s(Iv);Bme=n(R6e,"STRONG",{});var Mst=s(Bme);fKo=r(Mst,"electra"),Mst.forEach(t),mKo=r(R6e," \u2014 "),Cq=n(R6e,"A",{href:!0});var Est=s(Cq);gKo=r(Est,"TFElectraForPreTraining"),Est.forEach(t),hKo=r(R6e," (ELECTRA model)"),R6e.forEach(t),pKo=i(U),Dv=n(U,"LI",{});var S6e=s(Dv);xme=n(S6e,"STRONG",{});var yst=s(xme);_Ko=r(yst,"flaubert"),yst.forEach(t),uKo=r(S6e," \u2014 "),Mq=n(S6e,"A",{href:!0});var wst=s(Mq);bKo=r(wst,"TFFlaubertWithLMHeadModel"),wst.forEach(t),vKo=r(S6e," (FlauBERT model)"),S6e.forEach(t),TKo=i(U),jv=n(U,"LI",{});var P6e=s(jv);kme=n(P6e,"STRONG",{});var Ast=s(kme);FKo=r(Ast,"funnel"),Ast.forEach(t),CKo=r(P6e," \u2014 "),Eq=n(P6e,"A",{href:!0});var Lst=s(Eq);MKo=r(Lst,"TFFunnelForPreTraining"),Lst.forEach(t),EKo=r(P6e," (Funnel Transformer model)"),P6e.forEach(t),yKo=i(U),Nv=n(U,"LI",{});var $6e=s(Nv);Rme=n($6e,"STRONG",{});var Bst=s(Rme);wKo=r(Bst,"gpt2"),Bst.forEach(t),AKo=r($6e," \u2014 "),yq=n($6e,"A",{href:!0});var xst=s(yq);LKo=r(xst,"TFGPT2LMHeadModel"),xst.forEach(t),BKo=r($6e," (OpenAI GPT-2 model)"),$6e.forEach(t),xKo=i(U),qv=n(U,"LI",{});var I6e=s(qv);Sme=n(I6e,"STRONG",{});var kst=s(Sme);kKo=r(kst,"layoutlm"),kst.forEach(t),RKo=r(I6e," \u2014 "),wq=n(I6e,"A",{href:!0});var Rst=s(wq);SKo=r(Rst,"TFLayoutLMForMaskedLM"),Rst.forEach(t),PKo=r(I6e," (LayoutLM model)"),I6e.forEach(t),$Ko=i(U),Gv=n(U,"LI",{});var D6e=s(Gv);Pme=n(D6e,"STRONG",{});var Sst=s(Pme);IKo=r(Sst,"lxmert"),Sst.forEach(t),DKo=r(D6e," \u2014 "),Aq=n(D6e,"A",{href:!0});var Pst=s(Aq);jKo=r(Pst,"TFLxmertForPreTraining"),Pst.forEach(t),NKo=r(D6e," (LXMERT model)"),D6e.forEach(t),qKo=i(U),Ov=n(U,"LI",{});var j6e=s(Ov);$me=n(j6e,"STRONG",{});var $st=s($me);GKo=r($st,"mobilebert"),$st.forEach(t),OKo=r(j6e," \u2014 "),Lq=n(j6e,"A",{href:!0});var Ist=s(Lq);XKo=r(Ist,"TFMobileBertForPreTraining"),Ist.forEach(t),VKo=r(j6e," (MobileBERT model)"),j6e.forEach(t),zKo=i(U),Xv=n(U,"LI",{});var N6e=s(Xv);Ime=n(N6e,"STRONG",{});var Dst=s(Ime);WKo=r(Dst,"mpnet"),Dst.forEach(t),QKo=r(N6e," \u2014 "),Bq=n(N6e,"A",{href:!0});var jst=s(Bq);HKo=r(jst,"TFMPNetForMaskedLM"),jst.forEach(t),UKo=r(N6e," (MPNet model)"),N6e.forEach(t),JKo=i(U),Vv=n(U,"LI",{});var q6e=s(Vv);Dme=n(q6e,"STRONG",{});var Nst=s(Dme);YKo=r(Nst,"openai-gpt"),Nst.forEach(t),KKo=r(q6e," \u2014 "),xq=n(q6e,"A",{href:!0});var qst=s(xq);ZKo=r(qst,"TFOpenAIGPTLMHeadModel"),qst.forEach(t),eZo=r(q6e," (OpenAI GPT model)"),q6e.forEach(t),oZo=i(U),zv=n(U,"LI",{});var G6e=s(zv);jme=n(G6e,"STRONG",{});var Gst=s(jme);rZo=r(Gst,"roberta"),Gst.forEach(t),tZo=r(G6e," \u2014 "),kq=n(G6e,"A",{href:!0});var Ost=s(kq);aZo=r(Ost,"TFRobertaForMaskedLM"),Ost.forEach(t),nZo=r(G6e," (RoBERTa model)"),G6e.forEach(t),sZo=i(U),Wv=n(U,"LI",{});var O6e=s(Wv);Nme=n(O6e,"STRONG",{});var Xst=s(Nme);lZo=r(Xst,"t5"),Xst.forEach(t),iZo=r(O6e," \u2014 "),Rq=n(O6e,"A",{href:!0});var Vst=s(Rq);dZo=r(Vst,"TFT5ForConditionalGeneration"),Vst.forEach(t),cZo=r(O6e," (T5 model)"),O6e.forEach(t),fZo=i(U),Qv=n(U,"LI",{});var X6e=s(Qv);qme=n(X6e,"STRONG",{});var zst=s(qme);mZo=r(zst,"tapas"),zst.forEach(t),gZo=r(X6e," \u2014 "),Sq=n(X6e,"A",{href:!0});var Wst=s(Sq);hZo=r(Wst,"TFTapasForMaskedLM"),Wst.forEach(t),pZo=r(X6e," (TAPAS model)"),X6e.forEach(t),_Zo=i(U),Hv=n(U,"LI",{});var V6e=s(Hv);Gme=n(V6e,"STRONG",{});var Qst=s(Gme);uZo=r(Qst,"transfo-xl"),Qst.forEach(t),bZo=r(V6e," \u2014 "),Pq=n(V6e,"A",{href:!0});var Hst=s(Pq);vZo=r(Hst,"TFTransfoXLLMHeadModel"),Hst.forEach(t),TZo=r(V6e," (Transformer-XL model)"),V6e.forEach(t),FZo=i(U),Uv=n(U,"LI",{});var z6e=s(Uv);Ome=n(z6e,"STRONG",{});var Ust=s(Ome);CZo=r(Ust,"xlm"),Ust.forEach(t),MZo=r(z6e," \u2014 "),$q=n(z6e,"A",{href:!0});var Jst=s($q);EZo=r(Jst,"TFXLMWithLMHeadModel"),Jst.forEach(t),yZo=r(z6e," (XLM model)"),z6e.forEach(t),wZo=i(U),Jv=n(U,"LI",{});var W6e=s(Jv);Xme=n(W6e,"STRONG",{});var Yst=s(Xme);AZo=r(Yst,"xlm-roberta"),Yst.forEach(t),LZo=r(W6e," \u2014 "),Iq=n(W6e,"A",{href:!0});var Kst=s(Iq);BZo=r(Kst,"TFXLMRobertaForMaskedLM"),Kst.forEach(t),xZo=r(W6e," (XLM-RoBERTa model)"),W6e.forEach(t),kZo=i(U),Yv=n(U,"LI",{});var Q6e=s(Yv);Vme=n(Q6e,"STRONG",{});var Zst=s(Vme);RZo=r(Zst,"xlnet"),Zst.forEach(t),SZo=r(Q6e," \u2014 "),Dq=n(Q6e,"A",{href:!0});var elt=s(Dq);PZo=r(elt,"TFXLNetLMHeadModel"),elt.forEach(t),$Zo=r(Q6e," (XLNet model)"),Q6e.forEach(t),U.forEach(t),IZo=i(fa),zme=n(fa,"P",{});var olt=s(zme);DZo=r(olt,"Examples:"),olt.forEach(t),jZo=i(fa),m(uw.$$.fragment,fa),fa.forEach(t),Pl.forEach(t),fxe=i(c),ic=n(c,"H2",{class:!0});var TRe=s(ic);Kv=n(TRe,"A",{id:!0,class:!0,href:!0});var rlt=s(Kv);Wme=n(rlt,"SPAN",{});var tlt=s(Wme);m(bw.$$.fragment,tlt),tlt.forEach(t),rlt.forEach(t),NZo=i(TRe),Qme=n(TRe,"SPAN",{});var alt=s(Qme);qZo=r(alt,"TFAutoModelForCausalLM"),alt.forEach(t),TRe.forEach(t),mxe=i(c),vr=n(c,"DIV",{class:!0});var Il=s(vr);m(vw.$$.fragment,Il),GZo=i(Il),dc=n(Il,"P",{});var Wz=s(dc);OZo=r(Wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Hme=n(Wz,"CODE",{});var nlt=s(Hme);XZo=r(nlt,"from_pretrained()"),nlt.forEach(t),VZo=r(Wz,"class method or the "),Ume=n(Wz,"CODE",{});var slt=s(Ume);zZo=r(slt,"from_config()"),slt.forEach(t),WZo=r(Wz,`class
method.`),Wz.forEach(t),QZo=i(Il),Tw=n(Il,"P",{});var FRe=s(Tw);HZo=r(FRe,"This class cannot be instantiated directly using "),Jme=n(FRe,"CODE",{});var llt=s(Jme);UZo=r(llt,"__init__()"),llt.forEach(t),JZo=r(FRe," (throws an error)."),FRe.forEach(t),YZo=i(Il),mt=n(Il,"DIV",{class:!0});var Dl=s(mt);m(Fw.$$.fragment,Dl),KZo=i(Dl),Yme=n(Dl,"P",{});var ilt=s(Yme);ZZo=r(ilt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ilt.forEach(t),eer=i(Dl),cc=n(Dl,"P",{});var Qz=s(cc);oer=r(Qz,`Note:
Loading a model from its configuration file does `),Kme=n(Qz,"STRONG",{});var dlt=s(Kme);rer=r(dlt,"not"),dlt.forEach(t),ter=r(Qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zme=n(Qz,"CODE",{});var clt=s(Zme);aer=r(clt,"from_pretrained()"),clt.forEach(t),ner=r(Qz,"to load the model weights."),Qz.forEach(t),ser=i(Dl),ege=n(Dl,"P",{});var flt=s(ege);ler=r(flt,"Examples:"),flt.forEach(t),ier=i(Dl),m(Cw.$$.fragment,Dl),Dl.forEach(t),der=i(Il),po=n(Il,"DIV",{class:!0});var ma=s(po);m(Mw.$$.fragment,ma),cer=i(ma),oge=n(ma,"P",{});var mlt=s(oge);fer=r(mlt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),mlt.forEach(t),mer=i(ma),mn=n(ma,"P",{});var s4=s(mn);ger=r(s4,"The model class to instantiate is selected based on the "),rge=n(s4,"CODE",{});var glt=s(rge);her=r(glt,"model_type"),glt.forEach(t),per=r(s4,` property of the config object (either
passed as an argument or loaded from `),tge=n(s4,"CODE",{});var hlt=s(tge);_er=r(hlt,"pretrained_model_name_or_path"),hlt.forEach(t),uer=r(s4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),age=n(s4,"CODE",{});var plt=s(age);ber=r(plt,"pretrained_model_name_or_path"),plt.forEach(t),ver=r(s4,":"),s4.forEach(t),Ter=i(ma),pe=n(ma,"UL",{});var Me=s(pe);Zv=n(Me,"LI",{});var H6e=s(Zv);nge=n(H6e,"STRONG",{});var _lt=s(nge);Fer=r(_lt,"bert"),_lt.forEach(t),Cer=r(H6e," \u2014 "),jq=n(H6e,"A",{href:!0});var ult=s(jq);Mer=r(ult,"TFBertLMHeadModel"),ult.forEach(t),Eer=r(H6e," (BERT model)"),H6e.forEach(t),yer=i(Me),eT=n(Me,"LI",{});var U6e=s(eT);sge=n(U6e,"STRONG",{});var blt=s(sge);wer=r(blt,"ctrl"),blt.forEach(t),Aer=r(U6e," \u2014 "),Nq=n(U6e,"A",{href:!0});var vlt=s(Nq);Ler=r(vlt,"TFCTRLLMHeadModel"),vlt.forEach(t),Ber=r(U6e," (CTRL model)"),U6e.forEach(t),xer=i(Me),oT=n(Me,"LI",{});var J6e=s(oT);lge=n(J6e,"STRONG",{});var Tlt=s(lge);ker=r(Tlt,"gpt2"),Tlt.forEach(t),Rer=r(J6e," \u2014 "),qq=n(J6e,"A",{href:!0});var Flt=s(qq);Ser=r(Flt,"TFGPT2LMHeadModel"),Flt.forEach(t),Per=r(J6e," (OpenAI GPT-2 model)"),J6e.forEach(t),$er=i(Me),rT=n(Me,"LI",{});var Y6e=s(rT);ige=n(Y6e,"STRONG",{});var Clt=s(ige);Ier=r(Clt,"openai-gpt"),Clt.forEach(t),Der=r(Y6e," \u2014 "),Gq=n(Y6e,"A",{href:!0});var Mlt=s(Gq);jer=r(Mlt,"TFOpenAIGPTLMHeadModel"),Mlt.forEach(t),Ner=r(Y6e," (OpenAI GPT model)"),Y6e.forEach(t),qer=i(Me),tT=n(Me,"LI",{});var K6e=s(tT);dge=n(K6e,"STRONG",{});var Elt=s(dge);Ger=r(Elt,"rembert"),Elt.forEach(t),Oer=r(K6e," \u2014 "),Oq=n(K6e,"A",{href:!0});var ylt=s(Oq);Xer=r(ylt,"TFRemBertForCausalLM"),ylt.forEach(t),Ver=r(K6e," (RemBERT model)"),K6e.forEach(t),zer=i(Me),aT=n(Me,"LI",{});var Z6e=s(aT);cge=n(Z6e,"STRONG",{});var wlt=s(cge);Wer=r(wlt,"roberta"),wlt.forEach(t),Qer=r(Z6e," \u2014 "),Xq=n(Z6e,"A",{href:!0});var Alt=s(Xq);Her=r(Alt,"TFRobertaForCausalLM"),Alt.forEach(t),Uer=r(Z6e," (RoBERTa model)"),Z6e.forEach(t),Jer=i(Me),nT=n(Me,"LI",{});var eAe=s(nT);fge=n(eAe,"STRONG",{});var Llt=s(fge);Yer=r(Llt,"roformer"),Llt.forEach(t),Ker=r(eAe," \u2014 "),Vq=n(eAe,"A",{href:!0});var Blt=s(Vq);Zer=r(Blt,"TFRoFormerForCausalLM"),Blt.forEach(t),eor=r(eAe," (RoFormer model)"),eAe.forEach(t),oor=i(Me),sT=n(Me,"LI",{});var oAe=s(sT);mge=n(oAe,"STRONG",{});var xlt=s(mge);ror=r(xlt,"transfo-xl"),xlt.forEach(t),tor=r(oAe," \u2014 "),zq=n(oAe,"A",{href:!0});var klt=s(zq);aor=r(klt,"TFTransfoXLLMHeadModel"),klt.forEach(t),nor=r(oAe," (Transformer-XL model)"),oAe.forEach(t),sor=i(Me),lT=n(Me,"LI",{});var rAe=s(lT);gge=n(rAe,"STRONG",{});var Rlt=s(gge);lor=r(Rlt,"xlm"),Rlt.forEach(t),ior=r(rAe," \u2014 "),Wq=n(rAe,"A",{href:!0});var Slt=s(Wq);dor=r(Slt,"TFXLMWithLMHeadModel"),Slt.forEach(t),cor=r(rAe," (XLM model)"),rAe.forEach(t),mor=i(Me),iT=n(Me,"LI",{});var tAe=s(iT);hge=n(tAe,"STRONG",{});var Plt=s(hge);gor=r(Plt,"xlnet"),Plt.forEach(t),hor=r(tAe," \u2014 "),Qq=n(tAe,"A",{href:!0});var $lt=s(Qq);por=r($lt,"TFXLNetLMHeadModel"),$lt.forEach(t),_or=r(tAe," (XLNet model)"),tAe.forEach(t),Me.forEach(t),uor=i(ma),pge=n(ma,"P",{});var Ilt=s(pge);bor=r(Ilt,"Examples:"),Ilt.forEach(t),vor=i(ma),m(Ew.$$.fragment,ma),ma.forEach(t),Il.forEach(t),gxe=i(c),fc=n(c,"H2",{class:!0});var CRe=s(fc);dT=n(CRe,"A",{id:!0,class:!0,href:!0});var Dlt=s(dT);_ge=n(Dlt,"SPAN",{});var jlt=s(_ge);m(yw.$$.fragment,jlt),jlt.forEach(t),Dlt.forEach(t),Tor=i(CRe),uge=n(CRe,"SPAN",{});var Nlt=s(uge);For=r(Nlt,"TFAutoModelForImageClassification"),Nlt.forEach(t),CRe.forEach(t),hxe=i(c),Tr=n(c,"DIV",{class:!0});var jl=s(Tr);m(ww.$$.fragment,jl),Cor=i(jl),mc=n(jl,"P",{});var Hz=s(mc);Mor=r(Hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),bge=n(Hz,"CODE",{});var qlt=s(bge);Eor=r(qlt,"from_pretrained()"),qlt.forEach(t),yor=r(Hz,"class method or the "),vge=n(Hz,"CODE",{});var Glt=s(vge);wor=r(Glt,"from_config()"),Glt.forEach(t),Aor=r(Hz,`class
method.`),Hz.forEach(t),Lor=i(jl),Aw=n(jl,"P",{});var MRe=s(Aw);Bor=r(MRe,"This class cannot be instantiated directly using "),Tge=n(MRe,"CODE",{});var Olt=s(Tge);xor=r(Olt,"__init__()"),Olt.forEach(t),kor=r(MRe," (throws an error)."),MRe.forEach(t),Ror=i(jl),gt=n(jl,"DIV",{class:!0});var Nl=s(gt);m(Lw.$$.fragment,Nl),Sor=i(Nl),Fge=n(Nl,"P",{});var Xlt=s(Fge);Por=r(Xlt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Xlt.forEach(t),$or=i(Nl),gc=n(Nl,"P",{});var Uz=s(gc);Ior=r(Uz,`Note:
Loading a model from its configuration file does `),Cge=n(Uz,"STRONG",{});var Vlt=s(Cge);Dor=r(Vlt,"not"),Vlt.forEach(t),jor=r(Uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mge=n(Uz,"CODE",{});var zlt=s(Mge);Nor=r(zlt,"from_pretrained()"),zlt.forEach(t),qor=r(Uz,"to load the model weights."),Uz.forEach(t),Gor=i(Nl),Ege=n(Nl,"P",{});var Wlt=s(Ege);Oor=r(Wlt,"Examples:"),Wlt.forEach(t),Xor=i(Nl),m(Bw.$$.fragment,Nl),Nl.forEach(t),Vor=i(jl),_o=n(jl,"DIV",{class:!0});var ga=s(_o);m(xw.$$.fragment,ga),zor=i(ga),yge=n(ga,"P",{});var Qlt=s(yge);Wor=r(Qlt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Qlt.forEach(t),Qor=i(ga),gn=n(ga,"P",{});var l4=s(gn);Hor=r(l4,"The model class to instantiate is selected based on the "),wge=n(l4,"CODE",{});var Hlt=s(wge);Uor=r(Hlt,"model_type"),Hlt.forEach(t),Jor=r(l4,` property of the config object (either
passed as an argument or loaded from `),Age=n(l4,"CODE",{});var Ult=s(Age);Yor=r(Ult,"pretrained_model_name_or_path"),Ult.forEach(t),Kor=r(l4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lge=n(l4,"CODE",{});var Jlt=s(Lge);Zor=r(Jlt,"pretrained_model_name_or_path"),Jlt.forEach(t),err=r(l4,":"),l4.forEach(t),orr=i(ga),kw=n(ga,"UL",{});var ERe=s(kw);cT=n(ERe,"LI",{});var aAe=s(cT);Bge=n(aAe,"STRONG",{});var Ylt=s(Bge);rrr=r(Ylt,"convnext"),Ylt.forEach(t),trr=r(aAe," \u2014 "),Hq=n(aAe,"A",{href:!0});var Klt=s(Hq);arr=r(Klt,"TFConvNextForImageClassification"),Klt.forEach(t),nrr=r(aAe," (ConvNext model)"),aAe.forEach(t),srr=i(ERe),fT=n(ERe,"LI",{});var nAe=s(fT);xge=n(nAe,"STRONG",{});var Zlt=s(xge);lrr=r(Zlt,"vit"),Zlt.forEach(t),irr=r(nAe," \u2014 "),Uq=n(nAe,"A",{href:!0});var eit=s(Uq);drr=r(eit,"TFViTForImageClassification"),eit.forEach(t),crr=r(nAe," (ViT model)"),nAe.forEach(t),ERe.forEach(t),frr=i(ga),kge=n(ga,"P",{});var oit=s(kge);mrr=r(oit,"Examples:"),oit.forEach(t),grr=i(ga),m(Rw.$$.fragment,ga),ga.forEach(t),jl.forEach(t),pxe=i(c),hc=n(c,"H2",{class:!0});var yRe=s(hc);mT=n(yRe,"A",{id:!0,class:!0,href:!0});var rit=s(mT);Rge=n(rit,"SPAN",{});var tit=s(Rge);m(Sw.$$.fragment,tit),tit.forEach(t),rit.forEach(t),hrr=i(yRe),Sge=n(yRe,"SPAN",{});var ait=s(Sge);prr=r(ait,"TFAutoModelForMaskedLM"),ait.forEach(t),yRe.forEach(t),_xe=i(c),Fr=n(c,"DIV",{class:!0});var ql=s(Fr);m(Pw.$$.fragment,ql),_rr=i(ql),pc=n(ql,"P",{});var Jz=s(pc);urr=r(Jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Pge=n(Jz,"CODE",{});var nit=s(Pge);brr=r(nit,"from_pretrained()"),nit.forEach(t),vrr=r(Jz,"class method or the "),$ge=n(Jz,"CODE",{});var sit=s($ge);Trr=r(sit,"from_config()"),sit.forEach(t),Frr=r(Jz,`class
method.`),Jz.forEach(t),Crr=i(ql),$w=n(ql,"P",{});var wRe=s($w);Mrr=r(wRe,"This class cannot be instantiated directly using "),Ige=n(wRe,"CODE",{});var lit=s(Ige);Err=r(lit,"__init__()"),lit.forEach(t),yrr=r(wRe," (throws an error)."),wRe.forEach(t),wrr=i(ql),ht=n(ql,"DIV",{class:!0});var Gl=s(ht);m(Iw.$$.fragment,Gl),Arr=i(Gl),Dge=n(Gl,"P",{});var iit=s(Dge);Lrr=r(iit,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),iit.forEach(t),Brr=i(Gl),_c=n(Gl,"P",{});var Yz=s(_c);xrr=r(Yz,`Note:
Loading a model from its configuration file does `),jge=n(Yz,"STRONG",{});var dit=s(jge);krr=r(dit,"not"),dit.forEach(t),Rrr=r(Yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Nge=n(Yz,"CODE",{});var cit=s(Nge);Srr=r(cit,"from_pretrained()"),cit.forEach(t),Prr=r(Yz,"to load the model weights."),Yz.forEach(t),$rr=i(Gl),qge=n(Gl,"P",{});var fit=s(qge);Irr=r(fit,"Examples:"),fit.forEach(t),Drr=i(Gl),m(Dw.$$.fragment,Gl),Gl.forEach(t),jrr=i(ql),uo=n(ql,"DIV",{class:!0});var ha=s(uo);m(jw.$$.fragment,ha),Nrr=i(ha),Gge=n(ha,"P",{});var mit=s(Gge);qrr=r(mit,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),mit.forEach(t),Grr=i(ha),hn=n(ha,"P",{});var i4=s(hn);Orr=r(i4,"The model class to instantiate is selected based on the "),Oge=n(i4,"CODE",{});var git=s(Oge);Xrr=r(git,"model_type"),git.forEach(t),Vrr=r(i4,` property of the config object (either
passed as an argument or loaded from `),Xge=n(i4,"CODE",{});var hit=s(Xge);zrr=r(hit,"pretrained_model_name_or_path"),hit.forEach(t),Wrr=r(i4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vge=n(i4,"CODE",{});var pit=s(Vge);Qrr=r(pit,"pretrained_model_name_or_path"),pit.forEach(t),Hrr=r(i4,":"),i4.forEach(t),Urr=i(ha),Y=n(ha,"UL",{});var ee=s(Y);gT=n(ee,"LI",{});var sAe=s(gT);zge=n(sAe,"STRONG",{});var _it=s(zge);Jrr=r(_it,"albert"),_it.forEach(t),Yrr=r(sAe," \u2014 "),Jq=n(sAe,"A",{href:!0});var uit=s(Jq);Krr=r(uit,"TFAlbertForMaskedLM"),uit.forEach(t),Zrr=r(sAe," (ALBERT model)"),sAe.forEach(t),etr=i(ee),hT=n(ee,"LI",{});var lAe=s(hT);Wge=n(lAe,"STRONG",{});var bit=s(Wge);otr=r(bit,"bert"),bit.forEach(t),rtr=r(lAe," \u2014 "),Yq=n(lAe,"A",{href:!0});var vit=s(Yq);ttr=r(vit,"TFBertForMaskedLM"),vit.forEach(t),atr=r(lAe," (BERT model)"),lAe.forEach(t),ntr=i(ee),pT=n(ee,"LI",{});var iAe=s(pT);Qge=n(iAe,"STRONG",{});var Tit=s(Qge);str=r(Tit,"camembert"),Tit.forEach(t),ltr=r(iAe," \u2014 "),Kq=n(iAe,"A",{href:!0});var Fit=s(Kq);itr=r(Fit,"TFCamembertForMaskedLM"),Fit.forEach(t),dtr=r(iAe," (CamemBERT model)"),iAe.forEach(t),ctr=i(ee),_T=n(ee,"LI",{});var dAe=s(_T);Hge=n(dAe,"STRONG",{});var Cit=s(Hge);ftr=r(Cit,"convbert"),Cit.forEach(t),mtr=r(dAe," \u2014 "),Zq=n(dAe,"A",{href:!0});var Mit=s(Zq);gtr=r(Mit,"TFConvBertForMaskedLM"),Mit.forEach(t),htr=r(dAe," (ConvBERT model)"),dAe.forEach(t),ptr=i(ee),uT=n(ee,"LI",{});var cAe=s(uT);Uge=n(cAe,"STRONG",{});var Eit=s(Uge);_tr=r(Eit,"deberta"),Eit.forEach(t),utr=r(cAe," \u2014 "),eG=n(cAe,"A",{href:!0});var yit=s(eG);btr=r(yit,"TFDebertaForMaskedLM"),yit.forEach(t),vtr=r(cAe," (DeBERTa model)"),cAe.forEach(t),Ttr=i(ee),bT=n(ee,"LI",{});var fAe=s(bT);Jge=n(fAe,"STRONG",{});var wit=s(Jge);Ftr=r(wit,"deberta-v2"),wit.forEach(t),Ctr=r(fAe," \u2014 "),oG=n(fAe,"A",{href:!0});var Ait=s(oG);Mtr=r(Ait,"TFDebertaV2ForMaskedLM"),Ait.forEach(t),Etr=r(fAe," (DeBERTa-v2 model)"),fAe.forEach(t),ytr=i(ee),vT=n(ee,"LI",{});var mAe=s(vT);Yge=n(mAe,"STRONG",{});var Lit=s(Yge);wtr=r(Lit,"distilbert"),Lit.forEach(t),Atr=r(mAe," \u2014 "),rG=n(mAe,"A",{href:!0});var Bit=s(rG);Ltr=r(Bit,"TFDistilBertForMaskedLM"),Bit.forEach(t),Btr=r(mAe," (DistilBERT model)"),mAe.forEach(t),xtr=i(ee),TT=n(ee,"LI",{});var gAe=s(TT);Kge=n(gAe,"STRONG",{});var xit=s(Kge);ktr=r(xit,"electra"),xit.forEach(t),Rtr=r(gAe," \u2014 "),tG=n(gAe,"A",{href:!0});var kit=s(tG);Str=r(kit,"TFElectraForMaskedLM"),kit.forEach(t),Ptr=r(gAe," (ELECTRA model)"),gAe.forEach(t),$tr=i(ee),FT=n(ee,"LI",{});var hAe=s(FT);Zge=n(hAe,"STRONG",{});var Rit=s(Zge);Itr=r(Rit,"flaubert"),Rit.forEach(t),Dtr=r(hAe," \u2014 "),aG=n(hAe,"A",{href:!0});var Sit=s(aG);jtr=r(Sit,"TFFlaubertWithLMHeadModel"),Sit.forEach(t),Ntr=r(hAe," (FlauBERT model)"),hAe.forEach(t),qtr=i(ee),CT=n(ee,"LI",{});var pAe=s(CT);ehe=n(pAe,"STRONG",{});var Pit=s(ehe);Gtr=r(Pit,"funnel"),Pit.forEach(t),Otr=r(pAe," \u2014 "),nG=n(pAe,"A",{href:!0});var $it=s(nG);Xtr=r($it,"TFFunnelForMaskedLM"),$it.forEach(t),Vtr=r(pAe," (Funnel Transformer model)"),pAe.forEach(t),ztr=i(ee),MT=n(ee,"LI",{});var _Ae=s(MT);ohe=n(_Ae,"STRONG",{});var Iit=s(ohe);Wtr=r(Iit,"layoutlm"),Iit.forEach(t),Qtr=r(_Ae," \u2014 "),sG=n(_Ae,"A",{href:!0});var Dit=s(sG);Htr=r(Dit,"TFLayoutLMForMaskedLM"),Dit.forEach(t),Utr=r(_Ae," (LayoutLM model)"),_Ae.forEach(t),Jtr=i(ee),ET=n(ee,"LI",{});var uAe=s(ET);rhe=n(uAe,"STRONG",{});var jit=s(rhe);Ytr=r(jit,"longformer"),jit.forEach(t),Ktr=r(uAe," \u2014 "),lG=n(uAe,"A",{href:!0});var Nit=s(lG);Ztr=r(Nit,"TFLongformerForMaskedLM"),Nit.forEach(t),ear=r(uAe," (Longformer model)"),uAe.forEach(t),oar=i(ee),yT=n(ee,"LI",{});var bAe=s(yT);the=n(bAe,"STRONG",{});var qit=s(the);rar=r(qit,"mobilebert"),qit.forEach(t),tar=r(bAe," \u2014 "),iG=n(bAe,"A",{href:!0});var Git=s(iG);aar=r(Git,"TFMobileBertForMaskedLM"),Git.forEach(t),nar=r(bAe," (MobileBERT model)"),bAe.forEach(t),sar=i(ee),wT=n(ee,"LI",{});var vAe=s(wT);ahe=n(vAe,"STRONG",{});var Oit=s(ahe);lar=r(Oit,"mpnet"),Oit.forEach(t),iar=r(vAe," \u2014 "),dG=n(vAe,"A",{href:!0});var Xit=s(dG);dar=r(Xit,"TFMPNetForMaskedLM"),Xit.forEach(t),car=r(vAe," (MPNet model)"),vAe.forEach(t),far=i(ee),AT=n(ee,"LI",{});var TAe=s(AT);nhe=n(TAe,"STRONG",{});var Vit=s(nhe);mar=r(Vit,"rembert"),Vit.forEach(t),gar=r(TAe," \u2014 "),cG=n(TAe,"A",{href:!0});var zit=s(cG);har=r(zit,"TFRemBertForMaskedLM"),zit.forEach(t),par=r(TAe," (RemBERT model)"),TAe.forEach(t),_ar=i(ee),LT=n(ee,"LI",{});var FAe=s(LT);she=n(FAe,"STRONG",{});var Wit=s(she);uar=r(Wit,"roberta"),Wit.forEach(t),bar=r(FAe," \u2014 "),fG=n(FAe,"A",{href:!0});var Qit=s(fG);Tar=r(Qit,"TFRobertaForMaskedLM"),Qit.forEach(t),Far=r(FAe," (RoBERTa model)"),FAe.forEach(t),Car=i(ee),BT=n(ee,"LI",{});var CAe=s(BT);lhe=n(CAe,"STRONG",{});var Hit=s(lhe);Mar=r(Hit,"roformer"),Hit.forEach(t),Ear=r(CAe," \u2014 "),mG=n(CAe,"A",{href:!0});var Uit=s(mG);yar=r(Uit,"TFRoFormerForMaskedLM"),Uit.forEach(t),war=r(CAe," (RoFormer model)"),CAe.forEach(t),Aar=i(ee),xT=n(ee,"LI",{});var MAe=s(xT);ihe=n(MAe,"STRONG",{});var Jit=s(ihe);Lar=r(Jit,"tapas"),Jit.forEach(t),Bar=r(MAe," \u2014 "),gG=n(MAe,"A",{href:!0});var Yit=s(gG);xar=r(Yit,"TFTapasForMaskedLM"),Yit.forEach(t),kar=r(MAe," (TAPAS model)"),MAe.forEach(t),Rar=i(ee),kT=n(ee,"LI",{});var EAe=s(kT);dhe=n(EAe,"STRONG",{});var Kit=s(dhe);Sar=r(Kit,"xlm"),Kit.forEach(t),Par=r(EAe," \u2014 "),hG=n(EAe,"A",{href:!0});var Zit=s(hG);$ar=r(Zit,"TFXLMWithLMHeadModel"),Zit.forEach(t),Iar=r(EAe," (XLM model)"),EAe.forEach(t),Dar=i(ee),RT=n(ee,"LI",{});var yAe=s(RT);che=n(yAe,"STRONG",{});var edt=s(che);jar=r(edt,"xlm-roberta"),edt.forEach(t),Nar=r(yAe," \u2014 "),pG=n(yAe,"A",{href:!0});var odt=s(pG);qar=r(odt,"TFXLMRobertaForMaskedLM"),odt.forEach(t),Gar=r(yAe," (XLM-RoBERTa model)"),yAe.forEach(t),ee.forEach(t),Oar=i(ha),fhe=n(ha,"P",{});var rdt=s(fhe);Xar=r(rdt,"Examples:"),rdt.forEach(t),Var=i(ha),m(Nw.$$.fragment,ha),ha.forEach(t),ql.forEach(t),uxe=i(c),uc=n(c,"H2",{class:!0});var ARe=s(uc);ST=n(ARe,"A",{id:!0,class:!0,href:!0});var tdt=s(ST);mhe=n(tdt,"SPAN",{});var adt=s(mhe);m(qw.$$.fragment,adt),adt.forEach(t),tdt.forEach(t),zar=i(ARe),ghe=n(ARe,"SPAN",{});var ndt=s(ghe);War=r(ndt,"TFAutoModelForSeq2SeqLM"),ndt.forEach(t),ARe.forEach(t),bxe=i(c),Cr=n(c,"DIV",{class:!0});var Ol=s(Cr);m(Gw.$$.fragment,Ol),Qar=i(Ol),bc=n(Ol,"P",{});var Kz=s(bc);Har=r(Kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),hhe=n(Kz,"CODE",{});var sdt=s(hhe);Uar=r(sdt,"from_pretrained()"),sdt.forEach(t),Jar=r(Kz,"class method or the "),phe=n(Kz,"CODE",{});var ldt=s(phe);Yar=r(ldt,"from_config()"),ldt.forEach(t),Kar=r(Kz,`class
method.`),Kz.forEach(t),Zar=i(Ol),Ow=n(Ol,"P",{});var LRe=s(Ow);enr=r(LRe,"This class cannot be instantiated directly using "),_he=n(LRe,"CODE",{});var idt=s(_he);onr=r(idt,"__init__()"),idt.forEach(t),rnr=r(LRe," (throws an error)."),LRe.forEach(t),tnr=i(Ol),pt=n(Ol,"DIV",{class:!0});var Xl=s(pt);m(Xw.$$.fragment,Xl),anr=i(Xl),uhe=n(Xl,"P",{});var ddt=s(uhe);nnr=r(ddt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ddt.forEach(t),snr=i(Xl),vc=n(Xl,"P",{});var Zz=s(vc);lnr=r(Zz,`Note:
Loading a model from its configuration file does `),bhe=n(Zz,"STRONG",{});var cdt=s(bhe);inr=r(cdt,"not"),cdt.forEach(t),dnr=r(Zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),vhe=n(Zz,"CODE",{});var fdt=s(vhe);cnr=r(fdt,"from_pretrained()"),fdt.forEach(t),fnr=r(Zz,"to load the model weights."),Zz.forEach(t),mnr=i(Xl),The=n(Xl,"P",{});var mdt=s(The);gnr=r(mdt,"Examples:"),mdt.forEach(t),hnr=i(Xl),m(Vw.$$.fragment,Xl),Xl.forEach(t),pnr=i(Ol),bo=n(Ol,"DIV",{class:!0});var pa=s(bo);m(zw.$$.fragment,pa),_nr=i(pa),Fhe=n(pa,"P",{});var gdt=s(Fhe);unr=r(gdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),gdt.forEach(t),bnr=i(pa),pn=n(pa,"P",{});var d4=s(pn);vnr=r(d4,"The model class to instantiate is selected based on the "),Che=n(d4,"CODE",{});var hdt=s(Che);Tnr=r(hdt,"model_type"),hdt.forEach(t),Fnr=r(d4,` property of the config object (either
passed as an argument or loaded from `),Mhe=n(d4,"CODE",{});var pdt=s(Mhe);Cnr=r(pdt,"pretrained_model_name_or_path"),pdt.forEach(t),Mnr=r(d4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ehe=n(d4,"CODE",{});var _dt=s(Ehe);Enr=r(_dt,"pretrained_model_name_or_path"),_dt.forEach(t),ynr=r(d4,":"),d4.forEach(t),wnr=i(pa),_e=n(pa,"UL",{});var Ee=s(_e);PT=n(Ee,"LI",{});var wAe=s(PT);yhe=n(wAe,"STRONG",{});var udt=s(yhe);Anr=r(udt,"bart"),udt.forEach(t),Lnr=r(wAe," \u2014 "),_G=n(wAe,"A",{href:!0});var bdt=s(_G);Bnr=r(bdt,"TFBartForConditionalGeneration"),bdt.forEach(t),xnr=r(wAe," (BART model)"),wAe.forEach(t),knr=i(Ee),$T=n(Ee,"LI",{});var AAe=s($T);whe=n(AAe,"STRONG",{});var vdt=s(whe);Rnr=r(vdt,"blenderbot"),vdt.forEach(t),Snr=r(AAe," \u2014 "),uG=n(AAe,"A",{href:!0});var Tdt=s(uG);Pnr=r(Tdt,"TFBlenderbotForConditionalGeneration"),Tdt.forEach(t),$nr=r(AAe," (Blenderbot model)"),AAe.forEach(t),Inr=i(Ee),IT=n(Ee,"LI",{});var LAe=s(IT);Ahe=n(LAe,"STRONG",{});var Fdt=s(Ahe);Dnr=r(Fdt,"blenderbot-small"),Fdt.forEach(t),jnr=r(LAe," \u2014 "),bG=n(LAe,"A",{href:!0});var Cdt=s(bG);Nnr=r(Cdt,"TFBlenderbotSmallForConditionalGeneration"),Cdt.forEach(t),qnr=r(LAe," (BlenderbotSmall model)"),LAe.forEach(t),Gnr=i(Ee),DT=n(Ee,"LI",{});var BAe=s(DT);Lhe=n(BAe,"STRONG",{});var Mdt=s(Lhe);Onr=r(Mdt,"encoder-decoder"),Mdt.forEach(t),Xnr=r(BAe," \u2014 "),vG=n(BAe,"A",{href:!0});var Edt=s(vG);Vnr=r(Edt,"TFEncoderDecoderModel"),Edt.forEach(t),znr=r(BAe," (Encoder decoder model)"),BAe.forEach(t),Wnr=i(Ee),jT=n(Ee,"LI",{});var xAe=s(jT);Bhe=n(xAe,"STRONG",{});var ydt=s(Bhe);Qnr=r(ydt,"led"),ydt.forEach(t),Hnr=r(xAe," \u2014 "),TG=n(xAe,"A",{href:!0});var wdt=s(TG);Unr=r(wdt,"TFLEDForConditionalGeneration"),wdt.forEach(t),Jnr=r(xAe," (LED model)"),xAe.forEach(t),Ynr=i(Ee),NT=n(Ee,"LI",{});var kAe=s(NT);xhe=n(kAe,"STRONG",{});var Adt=s(xhe);Knr=r(Adt,"marian"),Adt.forEach(t),Znr=r(kAe," \u2014 "),FG=n(kAe,"A",{href:!0});var Ldt=s(FG);esr=r(Ldt,"TFMarianMTModel"),Ldt.forEach(t),osr=r(kAe," (Marian model)"),kAe.forEach(t),rsr=i(Ee),qT=n(Ee,"LI",{});var RAe=s(qT);khe=n(RAe,"STRONG",{});var Bdt=s(khe);tsr=r(Bdt,"mbart"),Bdt.forEach(t),asr=r(RAe," \u2014 "),CG=n(RAe,"A",{href:!0});var xdt=s(CG);nsr=r(xdt,"TFMBartForConditionalGeneration"),xdt.forEach(t),ssr=r(RAe," (mBART model)"),RAe.forEach(t),lsr=i(Ee),GT=n(Ee,"LI",{});var SAe=s(GT);Rhe=n(SAe,"STRONG",{});var kdt=s(Rhe);isr=r(kdt,"mt5"),kdt.forEach(t),dsr=r(SAe," \u2014 "),MG=n(SAe,"A",{href:!0});var Rdt=s(MG);csr=r(Rdt,"TFMT5ForConditionalGeneration"),Rdt.forEach(t),fsr=r(SAe," (mT5 model)"),SAe.forEach(t),msr=i(Ee),OT=n(Ee,"LI",{});var PAe=s(OT);She=n(PAe,"STRONG",{});var Sdt=s(She);gsr=r(Sdt,"pegasus"),Sdt.forEach(t),hsr=r(PAe," \u2014 "),EG=n(PAe,"A",{href:!0});var Pdt=s(EG);psr=r(Pdt,"TFPegasusForConditionalGeneration"),Pdt.forEach(t),_sr=r(PAe," (Pegasus model)"),PAe.forEach(t),usr=i(Ee),XT=n(Ee,"LI",{});var $Ae=s(XT);Phe=n($Ae,"STRONG",{});var $dt=s(Phe);bsr=r($dt,"t5"),$dt.forEach(t),vsr=r($Ae," \u2014 "),yG=n($Ae,"A",{href:!0});var Idt=s(yG);Tsr=r(Idt,"TFT5ForConditionalGeneration"),Idt.forEach(t),Fsr=r($Ae," (T5 model)"),$Ae.forEach(t),Ee.forEach(t),Csr=i(pa),$he=n(pa,"P",{});var Ddt=s($he);Msr=r(Ddt,"Examples:"),Ddt.forEach(t),Esr=i(pa),m(Ww.$$.fragment,pa),pa.forEach(t),Ol.forEach(t),vxe=i(c),Tc=n(c,"H2",{class:!0});var BRe=s(Tc);VT=n(BRe,"A",{id:!0,class:!0,href:!0});var jdt=s(VT);Ihe=n(jdt,"SPAN",{});var Ndt=s(Ihe);m(Qw.$$.fragment,Ndt),Ndt.forEach(t),jdt.forEach(t),ysr=i(BRe),Dhe=n(BRe,"SPAN",{});var qdt=s(Dhe);wsr=r(qdt,"TFAutoModelForSequenceClassification"),qdt.forEach(t),BRe.forEach(t),Txe=i(c),Mr=n(c,"DIV",{class:!0});var Vl=s(Mr);m(Hw.$$.fragment,Vl),Asr=i(Vl),Fc=n(Vl,"P",{});var eW=s(Fc);Lsr=r(eW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),jhe=n(eW,"CODE",{});var Gdt=s(jhe);Bsr=r(Gdt,"from_pretrained()"),Gdt.forEach(t),xsr=r(eW,"class method or the "),Nhe=n(eW,"CODE",{});var Odt=s(Nhe);ksr=r(Odt,"from_config()"),Odt.forEach(t),Rsr=r(eW,`class
method.`),eW.forEach(t),Ssr=i(Vl),Uw=n(Vl,"P",{});var xRe=s(Uw);Psr=r(xRe,"This class cannot be instantiated directly using "),qhe=n(xRe,"CODE",{});var Xdt=s(qhe);$sr=r(Xdt,"__init__()"),Xdt.forEach(t),Isr=r(xRe," (throws an error)."),xRe.forEach(t),Dsr=i(Vl),_t=n(Vl,"DIV",{class:!0});var zl=s(_t);m(Jw.$$.fragment,zl),jsr=i(zl),Ghe=n(zl,"P",{});var Vdt=s(Ghe);Nsr=r(Vdt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Vdt.forEach(t),qsr=i(zl),Cc=n(zl,"P",{});var oW=s(Cc);Gsr=r(oW,`Note:
Loading a model from its configuration file does `),Ohe=n(oW,"STRONG",{});var zdt=s(Ohe);Osr=r(zdt,"not"),zdt.forEach(t),Xsr=r(oW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xhe=n(oW,"CODE",{});var Wdt=s(Xhe);Vsr=r(Wdt,"from_pretrained()"),Wdt.forEach(t),zsr=r(oW,"to load the model weights."),oW.forEach(t),Wsr=i(zl),Vhe=n(zl,"P",{});var Qdt=s(Vhe);Qsr=r(Qdt,"Examples:"),Qdt.forEach(t),Hsr=i(zl),m(Yw.$$.fragment,zl),zl.forEach(t),Usr=i(Vl),vo=n(Vl,"DIV",{class:!0});var _a=s(vo);m(Kw.$$.fragment,_a),Jsr=i(_a),zhe=n(_a,"P",{});var Hdt=s(zhe);Ysr=r(Hdt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Hdt.forEach(t),Ksr=i(_a),_n=n(_a,"P",{});var c4=s(_n);Zsr=r(c4,"The model class to instantiate is selected based on the "),Whe=n(c4,"CODE",{});var Udt=s(Whe);elr=r(Udt,"model_type"),Udt.forEach(t),olr=r(c4,` property of the config object (either
passed as an argument or loaded from `),Qhe=n(c4,"CODE",{});var Jdt=s(Qhe);rlr=r(Jdt,"pretrained_model_name_or_path"),Jdt.forEach(t),tlr=r(c4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hhe=n(c4,"CODE",{});var Ydt=s(Hhe);alr=r(Ydt,"pretrained_model_name_or_path"),Ydt.forEach(t),nlr=r(c4,":"),c4.forEach(t),slr=i(_a),X=n(_a,"UL",{});var W=s(X);zT=n(W,"LI",{});var IAe=s(zT);Uhe=n(IAe,"STRONG",{});var Kdt=s(Uhe);llr=r(Kdt,"albert"),Kdt.forEach(t),ilr=r(IAe," \u2014 "),wG=n(IAe,"A",{href:!0});var Zdt=s(wG);dlr=r(Zdt,"TFAlbertForSequenceClassification"),Zdt.forEach(t),clr=r(IAe," (ALBERT model)"),IAe.forEach(t),flr=i(W),WT=n(W,"LI",{});var DAe=s(WT);Jhe=n(DAe,"STRONG",{});var ect=s(Jhe);mlr=r(ect,"bert"),ect.forEach(t),glr=r(DAe," \u2014 "),AG=n(DAe,"A",{href:!0});var oct=s(AG);hlr=r(oct,"TFBertForSequenceClassification"),oct.forEach(t),plr=r(DAe," (BERT model)"),DAe.forEach(t),_lr=i(W),QT=n(W,"LI",{});var jAe=s(QT);Yhe=n(jAe,"STRONG",{});var rct=s(Yhe);ulr=r(rct,"camembert"),rct.forEach(t),blr=r(jAe," \u2014 "),LG=n(jAe,"A",{href:!0});var tct=s(LG);vlr=r(tct,"TFCamembertForSequenceClassification"),tct.forEach(t),Tlr=r(jAe," (CamemBERT model)"),jAe.forEach(t),Flr=i(W),HT=n(W,"LI",{});var NAe=s(HT);Khe=n(NAe,"STRONG",{});var act=s(Khe);Clr=r(act,"convbert"),act.forEach(t),Mlr=r(NAe," \u2014 "),BG=n(NAe,"A",{href:!0});var nct=s(BG);Elr=r(nct,"TFConvBertForSequenceClassification"),nct.forEach(t),ylr=r(NAe," (ConvBERT model)"),NAe.forEach(t),wlr=i(W),UT=n(W,"LI",{});var qAe=s(UT);Zhe=n(qAe,"STRONG",{});var sct=s(Zhe);Alr=r(sct,"ctrl"),sct.forEach(t),Llr=r(qAe," \u2014 "),xG=n(qAe,"A",{href:!0});var lct=s(xG);Blr=r(lct,"TFCTRLForSequenceClassification"),lct.forEach(t),xlr=r(qAe," (CTRL model)"),qAe.forEach(t),klr=i(W),JT=n(W,"LI",{});var GAe=s(JT);epe=n(GAe,"STRONG",{});var ict=s(epe);Rlr=r(ict,"deberta"),ict.forEach(t),Slr=r(GAe," \u2014 "),kG=n(GAe,"A",{href:!0});var dct=s(kG);Plr=r(dct,"TFDebertaForSequenceClassification"),dct.forEach(t),$lr=r(GAe," (DeBERTa model)"),GAe.forEach(t),Ilr=i(W),YT=n(W,"LI",{});var OAe=s(YT);ope=n(OAe,"STRONG",{});var cct=s(ope);Dlr=r(cct,"deberta-v2"),cct.forEach(t),jlr=r(OAe," \u2014 "),RG=n(OAe,"A",{href:!0});var fct=s(RG);Nlr=r(fct,"TFDebertaV2ForSequenceClassification"),fct.forEach(t),qlr=r(OAe," (DeBERTa-v2 model)"),OAe.forEach(t),Glr=i(W),KT=n(W,"LI",{});var XAe=s(KT);rpe=n(XAe,"STRONG",{});var mct=s(rpe);Olr=r(mct,"distilbert"),mct.forEach(t),Xlr=r(XAe," \u2014 "),SG=n(XAe,"A",{href:!0});var gct=s(SG);Vlr=r(gct,"TFDistilBertForSequenceClassification"),gct.forEach(t),zlr=r(XAe," (DistilBERT model)"),XAe.forEach(t),Wlr=i(W),ZT=n(W,"LI",{});var VAe=s(ZT);tpe=n(VAe,"STRONG",{});var hct=s(tpe);Qlr=r(hct,"electra"),hct.forEach(t),Hlr=r(VAe," \u2014 "),PG=n(VAe,"A",{href:!0});var pct=s(PG);Ulr=r(pct,"TFElectraForSequenceClassification"),pct.forEach(t),Jlr=r(VAe," (ELECTRA model)"),VAe.forEach(t),Ylr=i(W),eF=n(W,"LI",{});var zAe=s(eF);ape=n(zAe,"STRONG",{});var _ct=s(ape);Klr=r(_ct,"flaubert"),_ct.forEach(t),Zlr=r(zAe," \u2014 "),$G=n(zAe,"A",{href:!0});var uct=s($G);eir=r(uct,"TFFlaubertForSequenceClassification"),uct.forEach(t),oir=r(zAe," (FlauBERT model)"),zAe.forEach(t),rir=i(W),oF=n(W,"LI",{});var WAe=s(oF);npe=n(WAe,"STRONG",{});var bct=s(npe);tir=r(bct,"funnel"),bct.forEach(t),air=r(WAe," \u2014 "),IG=n(WAe,"A",{href:!0});var vct=s(IG);nir=r(vct,"TFFunnelForSequenceClassification"),vct.forEach(t),sir=r(WAe," (Funnel Transformer model)"),WAe.forEach(t),lir=i(W),rF=n(W,"LI",{});var QAe=s(rF);spe=n(QAe,"STRONG",{});var Tct=s(spe);iir=r(Tct,"gpt2"),Tct.forEach(t),dir=r(QAe," \u2014 "),DG=n(QAe,"A",{href:!0});var Fct=s(DG);cir=r(Fct,"TFGPT2ForSequenceClassification"),Fct.forEach(t),fir=r(QAe," (OpenAI GPT-2 model)"),QAe.forEach(t),mir=i(W),tF=n(W,"LI",{});var HAe=s(tF);lpe=n(HAe,"STRONG",{});var Cct=s(lpe);gir=r(Cct,"layoutlm"),Cct.forEach(t),hir=r(HAe," \u2014 "),jG=n(HAe,"A",{href:!0});var Mct=s(jG);pir=r(Mct,"TFLayoutLMForSequenceClassification"),Mct.forEach(t),_ir=r(HAe," (LayoutLM model)"),HAe.forEach(t),uir=i(W),aF=n(W,"LI",{});var UAe=s(aF);ipe=n(UAe,"STRONG",{});var Ect=s(ipe);bir=r(Ect,"longformer"),Ect.forEach(t),vir=r(UAe," \u2014 "),NG=n(UAe,"A",{href:!0});var yct=s(NG);Tir=r(yct,"TFLongformerForSequenceClassification"),yct.forEach(t),Fir=r(UAe," (Longformer model)"),UAe.forEach(t),Cir=i(W),nF=n(W,"LI",{});var JAe=s(nF);dpe=n(JAe,"STRONG",{});var wct=s(dpe);Mir=r(wct,"mobilebert"),wct.forEach(t),Eir=r(JAe," \u2014 "),qG=n(JAe,"A",{href:!0});var Act=s(qG);yir=r(Act,"TFMobileBertForSequenceClassification"),Act.forEach(t),wir=r(JAe," (MobileBERT model)"),JAe.forEach(t),Air=i(W),sF=n(W,"LI",{});var YAe=s(sF);cpe=n(YAe,"STRONG",{});var Lct=s(cpe);Lir=r(Lct,"mpnet"),Lct.forEach(t),Bir=r(YAe," \u2014 "),GG=n(YAe,"A",{href:!0});var Bct=s(GG);xir=r(Bct,"TFMPNetForSequenceClassification"),Bct.forEach(t),kir=r(YAe," (MPNet model)"),YAe.forEach(t),Rir=i(W),lF=n(W,"LI",{});var KAe=s(lF);fpe=n(KAe,"STRONG",{});var xct=s(fpe);Sir=r(xct,"openai-gpt"),xct.forEach(t),Pir=r(KAe," \u2014 "),OG=n(KAe,"A",{href:!0});var kct=s(OG);$ir=r(kct,"TFOpenAIGPTForSequenceClassification"),kct.forEach(t),Iir=r(KAe," (OpenAI GPT model)"),KAe.forEach(t),Dir=i(W),iF=n(W,"LI",{});var ZAe=s(iF);mpe=n(ZAe,"STRONG",{});var Rct=s(mpe);jir=r(Rct,"rembert"),Rct.forEach(t),Nir=r(ZAe," \u2014 "),XG=n(ZAe,"A",{href:!0});var Sct=s(XG);qir=r(Sct,"TFRemBertForSequenceClassification"),Sct.forEach(t),Gir=r(ZAe," (RemBERT model)"),ZAe.forEach(t),Oir=i(W),dF=n(W,"LI",{});var eLe=s(dF);gpe=n(eLe,"STRONG",{});var Pct=s(gpe);Xir=r(Pct,"roberta"),Pct.forEach(t),Vir=r(eLe," \u2014 "),VG=n(eLe,"A",{href:!0});var $ct=s(VG);zir=r($ct,"TFRobertaForSequenceClassification"),$ct.forEach(t),Wir=r(eLe," (RoBERTa model)"),eLe.forEach(t),Qir=i(W),cF=n(W,"LI",{});var oLe=s(cF);hpe=n(oLe,"STRONG",{});var Ict=s(hpe);Hir=r(Ict,"roformer"),Ict.forEach(t),Uir=r(oLe," \u2014 "),zG=n(oLe,"A",{href:!0});var Dct=s(zG);Jir=r(Dct,"TFRoFormerForSequenceClassification"),Dct.forEach(t),Yir=r(oLe," (RoFormer model)"),oLe.forEach(t),Kir=i(W),fF=n(W,"LI",{});var rLe=s(fF);ppe=n(rLe,"STRONG",{});var jct=s(ppe);Zir=r(jct,"tapas"),jct.forEach(t),edr=r(rLe," \u2014 "),WG=n(rLe,"A",{href:!0});var Nct=s(WG);odr=r(Nct,"TFTapasForSequenceClassification"),Nct.forEach(t),rdr=r(rLe," (TAPAS model)"),rLe.forEach(t),tdr=i(W),mF=n(W,"LI",{});var tLe=s(mF);_pe=n(tLe,"STRONG",{});var qct=s(_pe);adr=r(qct,"transfo-xl"),qct.forEach(t),ndr=r(tLe," \u2014 "),QG=n(tLe,"A",{href:!0});var Gct=s(QG);sdr=r(Gct,"TFTransfoXLForSequenceClassification"),Gct.forEach(t),ldr=r(tLe," (Transformer-XL model)"),tLe.forEach(t),idr=i(W),gF=n(W,"LI",{});var aLe=s(gF);upe=n(aLe,"STRONG",{});var Oct=s(upe);ddr=r(Oct,"xlm"),Oct.forEach(t),cdr=r(aLe," \u2014 "),HG=n(aLe,"A",{href:!0});var Xct=s(HG);fdr=r(Xct,"TFXLMForSequenceClassification"),Xct.forEach(t),mdr=r(aLe," (XLM model)"),aLe.forEach(t),gdr=i(W),hF=n(W,"LI",{});var nLe=s(hF);bpe=n(nLe,"STRONG",{});var Vct=s(bpe);hdr=r(Vct,"xlm-roberta"),Vct.forEach(t),pdr=r(nLe," \u2014 "),UG=n(nLe,"A",{href:!0});var zct=s(UG);_dr=r(zct,"TFXLMRobertaForSequenceClassification"),zct.forEach(t),udr=r(nLe," (XLM-RoBERTa model)"),nLe.forEach(t),bdr=i(W),pF=n(W,"LI",{});var sLe=s(pF);vpe=n(sLe,"STRONG",{});var Wct=s(vpe);vdr=r(Wct,"xlnet"),Wct.forEach(t),Tdr=r(sLe," \u2014 "),JG=n(sLe,"A",{href:!0});var Qct=s(JG);Fdr=r(Qct,"TFXLNetForSequenceClassification"),Qct.forEach(t),Cdr=r(sLe," (XLNet model)"),sLe.forEach(t),W.forEach(t),Mdr=i(_a),Tpe=n(_a,"P",{});var Hct=s(Tpe);Edr=r(Hct,"Examples:"),Hct.forEach(t),ydr=i(_a),m(Zw.$$.fragment,_a),_a.forEach(t),Vl.forEach(t),Fxe=i(c),Mc=n(c,"H2",{class:!0});var kRe=s(Mc);_F=n(kRe,"A",{id:!0,class:!0,href:!0});var Uct=s(_F);Fpe=n(Uct,"SPAN",{});var Jct=s(Fpe);m(e6.$$.fragment,Jct),Jct.forEach(t),Uct.forEach(t),wdr=i(kRe),Cpe=n(kRe,"SPAN",{});var Yct=s(Cpe);Adr=r(Yct,"TFAutoModelForMultipleChoice"),Yct.forEach(t),kRe.forEach(t),Cxe=i(c),Er=n(c,"DIV",{class:!0});var Wl=s(Er);m(o6.$$.fragment,Wl),Ldr=i(Wl),Ec=n(Wl,"P",{});var rW=s(Ec);Bdr=r(rW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Mpe=n(rW,"CODE",{});var Kct=s(Mpe);xdr=r(Kct,"from_pretrained()"),Kct.forEach(t),kdr=r(rW,"class method or the "),Epe=n(rW,"CODE",{});var Zct=s(Epe);Rdr=r(Zct,"from_config()"),Zct.forEach(t),Sdr=r(rW,`class
method.`),rW.forEach(t),Pdr=i(Wl),r6=n(Wl,"P",{});var RRe=s(r6);$dr=r(RRe,"This class cannot be instantiated directly using "),ype=n(RRe,"CODE",{});var eft=s(ype);Idr=r(eft,"__init__()"),eft.forEach(t),Ddr=r(RRe," (throws an error)."),RRe.forEach(t),jdr=i(Wl),ut=n(Wl,"DIV",{class:!0});var Ql=s(ut);m(t6.$$.fragment,Ql),Ndr=i(Ql),wpe=n(Ql,"P",{});var oft=s(wpe);qdr=r(oft,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),oft.forEach(t),Gdr=i(Ql),yc=n(Ql,"P",{});var tW=s(yc);Odr=r(tW,`Note:
Loading a model from its configuration file does `),Ape=n(tW,"STRONG",{});var rft=s(Ape);Xdr=r(rft,"not"),rft.forEach(t),Vdr=r(tW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lpe=n(tW,"CODE",{});var tft=s(Lpe);zdr=r(tft,"from_pretrained()"),tft.forEach(t),Wdr=r(tW,"to load the model weights."),tW.forEach(t),Qdr=i(Ql),Bpe=n(Ql,"P",{});var aft=s(Bpe);Hdr=r(aft,"Examples:"),aft.forEach(t),Udr=i(Ql),m(a6.$$.fragment,Ql),Ql.forEach(t),Jdr=i(Wl),To=n(Wl,"DIV",{class:!0});var ua=s(To);m(n6.$$.fragment,ua),Ydr=i(ua),xpe=n(ua,"P",{});var nft=s(xpe);Kdr=r(nft,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),nft.forEach(t),Zdr=i(ua),un=n(ua,"P",{});var f4=s(un);ecr=r(f4,"The model class to instantiate is selected based on the "),kpe=n(f4,"CODE",{});var sft=s(kpe);ocr=r(sft,"model_type"),sft.forEach(t),rcr=r(f4,` property of the config object (either
passed as an argument or loaded from `),Rpe=n(f4,"CODE",{});var lft=s(Rpe);tcr=r(lft,"pretrained_model_name_or_path"),lft.forEach(t),acr=r(f4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Spe=n(f4,"CODE",{});var ift=s(Spe);ncr=r(ift,"pretrained_model_name_or_path"),ift.forEach(t),scr=r(f4,":"),f4.forEach(t),lcr=i(ua),te=n(ua,"UL",{});var ne=s(te);uF=n(ne,"LI",{});var lLe=s(uF);Ppe=n(lLe,"STRONG",{});var dft=s(Ppe);icr=r(dft,"albert"),dft.forEach(t),dcr=r(lLe," \u2014 "),YG=n(lLe,"A",{href:!0});var cft=s(YG);ccr=r(cft,"TFAlbertForMultipleChoice"),cft.forEach(t),fcr=r(lLe," (ALBERT model)"),lLe.forEach(t),mcr=i(ne),bF=n(ne,"LI",{});var iLe=s(bF);$pe=n(iLe,"STRONG",{});var fft=s($pe);gcr=r(fft,"bert"),fft.forEach(t),hcr=r(iLe," \u2014 "),KG=n(iLe,"A",{href:!0});var mft=s(KG);pcr=r(mft,"TFBertForMultipleChoice"),mft.forEach(t),_cr=r(iLe," (BERT model)"),iLe.forEach(t),ucr=i(ne),vF=n(ne,"LI",{});var dLe=s(vF);Ipe=n(dLe,"STRONG",{});var gft=s(Ipe);bcr=r(gft,"camembert"),gft.forEach(t),vcr=r(dLe," \u2014 "),ZG=n(dLe,"A",{href:!0});var hft=s(ZG);Tcr=r(hft,"TFCamembertForMultipleChoice"),hft.forEach(t),Fcr=r(dLe," (CamemBERT model)"),dLe.forEach(t),Ccr=i(ne),TF=n(ne,"LI",{});var cLe=s(TF);Dpe=n(cLe,"STRONG",{});var pft=s(Dpe);Mcr=r(pft,"convbert"),pft.forEach(t),Ecr=r(cLe," \u2014 "),eO=n(cLe,"A",{href:!0});var _ft=s(eO);ycr=r(_ft,"TFConvBertForMultipleChoice"),_ft.forEach(t),wcr=r(cLe," (ConvBERT model)"),cLe.forEach(t),Acr=i(ne),FF=n(ne,"LI",{});var fLe=s(FF);jpe=n(fLe,"STRONG",{});var uft=s(jpe);Lcr=r(uft,"distilbert"),uft.forEach(t),Bcr=r(fLe," \u2014 "),oO=n(fLe,"A",{href:!0});var bft=s(oO);xcr=r(bft,"TFDistilBertForMultipleChoice"),bft.forEach(t),kcr=r(fLe," (DistilBERT model)"),fLe.forEach(t),Rcr=i(ne),CF=n(ne,"LI",{});var mLe=s(CF);Npe=n(mLe,"STRONG",{});var vft=s(Npe);Scr=r(vft,"electra"),vft.forEach(t),Pcr=r(mLe," \u2014 "),rO=n(mLe,"A",{href:!0});var Tft=s(rO);$cr=r(Tft,"TFElectraForMultipleChoice"),Tft.forEach(t),Icr=r(mLe," (ELECTRA model)"),mLe.forEach(t),Dcr=i(ne),MF=n(ne,"LI",{});var gLe=s(MF);qpe=n(gLe,"STRONG",{});var Fft=s(qpe);jcr=r(Fft,"flaubert"),Fft.forEach(t),Ncr=r(gLe," \u2014 "),tO=n(gLe,"A",{href:!0});var Cft=s(tO);qcr=r(Cft,"TFFlaubertForMultipleChoice"),Cft.forEach(t),Gcr=r(gLe," (FlauBERT model)"),gLe.forEach(t),Ocr=i(ne),EF=n(ne,"LI",{});var hLe=s(EF);Gpe=n(hLe,"STRONG",{});var Mft=s(Gpe);Xcr=r(Mft,"funnel"),Mft.forEach(t),Vcr=r(hLe," \u2014 "),aO=n(hLe,"A",{href:!0});var Eft=s(aO);zcr=r(Eft,"TFFunnelForMultipleChoice"),Eft.forEach(t),Wcr=r(hLe," (Funnel Transformer model)"),hLe.forEach(t),Qcr=i(ne),yF=n(ne,"LI",{});var pLe=s(yF);Ope=n(pLe,"STRONG",{});var yft=s(Ope);Hcr=r(yft,"longformer"),yft.forEach(t),Ucr=r(pLe," \u2014 "),nO=n(pLe,"A",{href:!0});var wft=s(nO);Jcr=r(wft,"TFLongformerForMultipleChoice"),wft.forEach(t),Ycr=r(pLe," (Longformer model)"),pLe.forEach(t),Kcr=i(ne),wF=n(ne,"LI",{});var _Le=s(wF);Xpe=n(_Le,"STRONG",{});var Aft=s(Xpe);Zcr=r(Aft,"mobilebert"),Aft.forEach(t),efr=r(_Le," \u2014 "),sO=n(_Le,"A",{href:!0});var Lft=s(sO);ofr=r(Lft,"TFMobileBertForMultipleChoice"),Lft.forEach(t),rfr=r(_Le," (MobileBERT model)"),_Le.forEach(t),tfr=i(ne),AF=n(ne,"LI",{});var uLe=s(AF);Vpe=n(uLe,"STRONG",{});var Bft=s(Vpe);afr=r(Bft,"mpnet"),Bft.forEach(t),nfr=r(uLe," \u2014 "),lO=n(uLe,"A",{href:!0});var xft=s(lO);sfr=r(xft,"TFMPNetForMultipleChoice"),xft.forEach(t),lfr=r(uLe," (MPNet model)"),uLe.forEach(t),ifr=i(ne),LF=n(ne,"LI",{});var bLe=s(LF);zpe=n(bLe,"STRONG",{});var kft=s(zpe);dfr=r(kft,"rembert"),kft.forEach(t),cfr=r(bLe," \u2014 "),iO=n(bLe,"A",{href:!0});var Rft=s(iO);ffr=r(Rft,"TFRemBertForMultipleChoice"),Rft.forEach(t),mfr=r(bLe," (RemBERT model)"),bLe.forEach(t),gfr=i(ne),BF=n(ne,"LI",{});var vLe=s(BF);Wpe=n(vLe,"STRONG",{});var Sft=s(Wpe);hfr=r(Sft,"roberta"),Sft.forEach(t),pfr=r(vLe," \u2014 "),dO=n(vLe,"A",{href:!0});var Pft=s(dO);_fr=r(Pft,"TFRobertaForMultipleChoice"),Pft.forEach(t),ufr=r(vLe," (RoBERTa model)"),vLe.forEach(t),bfr=i(ne),xF=n(ne,"LI",{});var TLe=s(xF);Qpe=n(TLe,"STRONG",{});var $ft=s(Qpe);vfr=r($ft,"roformer"),$ft.forEach(t),Tfr=r(TLe," \u2014 "),cO=n(TLe,"A",{href:!0});var Ift=s(cO);Ffr=r(Ift,"TFRoFormerForMultipleChoice"),Ift.forEach(t),Cfr=r(TLe," (RoFormer model)"),TLe.forEach(t),Mfr=i(ne),kF=n(ne,"LI",{});var FLe=s(kF);Hpe=n(FLe,"STRONG",{});var Dft=s(Hpe);Efr=r(Dft,"xlm"),Dft.forEach(t),yfr=r(FLe," \u2014 "),fO=n(FLe,"A",{href:!0});var jft=s(fO);wfr=r(jft,"TFXLMForMultipleChoice"),jft.forEach(t),Afr=r(FLe," (XLM model)"),FLe.forEach(t),Lfr=i(ne),RF=n(ne,"LI",{});var CLe=s(RF);Upe=n(CLe,"STRONG",{});var Nft=s(Upe);Bfr=r(Nft,"xlm-roberta"),Nft.forEach(t),xfr=r(CLe," \u2014 "),mO=n(CLe,"A",{href:!0});var qft=s(mO);kfr=r(qft,"TFXLMRobertaForMultipleChoice"),qft.forEach(t),Rfr=r(CLe," (XLM-RoBERTa model)"),CLe.forEach(t),Sfr=i(ne),SF=n(ne,"LI",{});var MLe=s(SF);Jpe=n(MLe,"STRONG",{});var Gft=s(Jpe);Pfr=r(Gft,"xlnet"),Gft.forEach(t),$fr=r(MLe," \u2014 "),gO=n(MLe,"A",{href:!0});var Oft=s(gO);Ifr=r(Oft,"TFXLNetForMultipleChoice"),Oft.forEach(t),Dfr=r(MLe," (XLNet model)"),MLe.forEach(t),ne.forEach(t),jfr=i(ua),Ype=n(ua,"P",{});var Xft=s(Ype);Nfr=r(Xft,"Examples:"),Xft.forEach(t),qfr=i(ua),m(s6.$$.fragment,ua),ua.forEach(t),Wl.forEach(t),Mxe=i(c),wc=n(c,"H2",{class:!0});var SRe=s(wc);PF=n(SRe,"A",{id:!0,class:!0,href:!0});var Vft=s(PF);Kpe=n(Vft,"SPAN",{});var zft=s(Kpe);m(l6.$$.fragment,zft),zft.forEach(t),Vft.forEach(t),Gfr=i(SRe),Zpe=n(SRe,"SPAN",{});var Wft=s(Zpe);Ofr=r(Wft,"TFAutoModelForTableQuestionAnswering"),Wft.forEach(t),SRe.forEach(t),Exe=i(c),yr=n(c,"DIV",{class:!0});var Hl=s(yr);m(i6.$$.fragment,Hl),Xfr=i(Hl),Ac=n(Hl,"P",{});var aW=s(Ac);Vfr=r(aW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),e_e=n(aW,"CODE",{});var Qft=s(e_e);zfr=r(Qft,"from_pretrained()"),Qft.forEach(t),Wfr=r(aW,"class method or the "),o_e=n(aW,"CODE",{});var Hft=s(o_e);Qfr=r(Hft,"from_config()"),Hft.forEach(t),Hfr=r(aW,`class
method.`),aW.forEach(t),Ufr=i(Hl),d6=n(Hl,"P",{});var PRe=s(d6);Jfr=r(PRe,"This class cannot be instantiated directly using "),r_e=n(PRe,"CODE",{});var Uft=s(r_e);Yfr=r(Uft,"__init__()"),Uft.forEach(t),Kfr=r(PRe," (throws an error)."),PRe.forEach(t),Zfr=i(Hl),bt=n(Hl,"DIV",{class:!0});var Ul=s(bt);m(c6.$$.fragment,Ul),emr=i(Ul),t_e=n(Ul,"P",{});var Jft=s(t_e);omr=r(Jft,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Jft.forEach(t),rmr=i(Ul),Lc=n(Ul,"P",{});var nW=s(Lc);tmr=r(nW,`Note:
Loading a model from its configuration file does `),a_e=n(nW,"STRONG",{});var Yft=s(a_e);amr=r(Yft,"not"),Yft.forEach(t),nmr=r(nW,` load the model weights. It only affects the
model\u2019s configuration. Use `),n_e=n(nW,"CODE",{});var Kft=s(n_e);smr=r(Kft,"from_pretrained()"),Kft.forEach(t),lmr=r(nW,"to load the model weights."),nW.forEach(t),imr=i(Ul),s_e=n(Ul,"P",{});var Zft=s(s_e);dmr=r(Zft,"Examples:"),Zft.forEach(t),cmr=i(Ul),m(f6.$$.fragment,Ul),Ul.forEach(t),fmr=i(Hl),Fo=n(Hl,"DIV",{class:!0});var ba=s(Fo);m(m6.$$.fragment,ba),mmr=i(ba),l_e=n(ba,"P",{});var emt=s(l_e);gmr=r(emt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),emt.forEach(t),hmr=i(ba),bn=n(ba,"P",{});var m4=s(bn);pmr=r(m4,"The model class to instantiate is selected based on the "),i_e=n(m4,"CODE",{});var omt=s(i_e);_mr=r(omt,"model_type"),omt.forEach(t),umr=r(m4,` property of the config object (either
passed as an argument or loaded from `),d_e=n(m4,"CODE",{});var rmt=s(d_e);bmr=r(rmt,"pretrained_model_name_or_path"),rmt.forEach(t),vmr=r(m4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),c_e=n(m4,"CODE",{});var tmt=s(c_e);Tmr=r(tmt,"pretrained_model_name_or_path"),tmt.forEach(t),Fmr=r(m4,":"),m4.forEach(t),Cmr=i(ba),f_e=n(ba,"UL",{});var amt=s(f_e);$F=n(amt,"LI",{});var ELe=s($F);m_e=n(ELe,"STRONG",{});var nmt=s(m_e);Mmr=r(nmt,"tapas"),nmt.forEach(t),Emr=r(ELe," \u2014 "),hO=n(ELe,"A",{href:!0});var smt=s(hO);ymr=r(smt,"TFTapasForQuestionAnswering"),smt.forEach(t),wmr=r(ELe," (TAPAS model)"),ELe.forEach(t),amt.forEach(t),Amr=i(ba),g_e=n(ba,"P",{});var lmt=s(g_e);Lmr=r(lmt,"Examples:"),lmt.forEach(t),Bmr=i(ba),m(g6.$$.fragment,ba),ba.forEach(t),Hl.forEach(t),yxe=i(c),Bc=n(c,"H2",{class:!0});var $Re=s(Bc);IF=n($Re,"A",{id:!0,class:!0,href:!0});var imt=s(IF);h_e=n(imt,"SPAN",{});var dmt=s(h_e);m(h6.$$.fragment,dmt),dmt.forEach(t),imt.forEach(t),xmr=i($Re),p_e=n($Re,"SPAN",{});var cmt=s(p_e);kmr=r(cmt,"TFAutoModelForTokenClassification"),cmt.forEach(t),$Re.forEach(t),wxe=i(c),wr=n(c,"DIV",{class:!0});var Jl=s(wr);m(p6.$$.fragment,Jl),Rmr=i(Jl),xc=n(Jl,"P",{});var sW=s(xc);Smr=r(sW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),__e=n(sW,"CODE",{});var fmt=s(__e);Pmr=r(fmt,"from_pretrained()"),fmt.forEach(t),$mr=r(sW,"class method or the "),u_e=n(sW,"CODE",{});var mmt=s(u_e);Imr=r(mmt,"from_config()"),mmt.forEach(t),Dmr=r(sW,`class
method.`),sW.forEach(t),jmr=i(Jl),_6=n(Jl,"P",{});var IRe=s(_6);Nmr=r(IRe,"This class cannot be instantiated directly using "),b_e=n(IRe,"CODE",{});var gmt=s(b_e);qmr=r(gmt,"__init__()"),gmt.forEach(t),Gmr=r(IRe," (throws an error)."),IRe.forEach(t),Omr=i(Jl),vt=n(Jl,"DIV",{class:!0});var Yl=s(vt);m(u6.$$.fragment,Yl),Xmr=i(Yl),v_e=n(Yl,"P",{});var hmt=s(v_e);Vmr=r(hmt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),hmt.forEach(t),zmr=i(Yl),kc=n(Yl,"P",{});var lW=s(kc);Wmr=r(lW,`Note:
Loading a model from its configuration file does `),T_e=n(lW,"STRONG",{});var pmt=s(T_e);Qmr=r(pmt,"not"),pmt.forEach(t),Hmr=r(lW,` load the model weights. It only affects the
model\u2019s configuration. Use `),F_e=n(lW,"CODE",{});var _mt=s(F_e);Umr=r(_mt,"from_pretrained()"),_mt.forEach(t),Jmr=r(lW,"to load the model weights."),lW.forEach(t),Ymr=i(Yl),C_e=n(Yl,"P",{});var umt=s(C_e);Kmr=r(umt,"Examples:"),umt.forEach(t),Zmr=i(Yl),m(b6.$$.fragment,Yl),Yl.forEach(t),egr=i(Jl),Co=n(Jl,"DIV",{class:!0});var va=s(Co);m(v6.$$.fragment,va),ogr=i(va),M_e=n(va,"P",{});var bmt=s(M_e);rgr=r(bmt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),bmt.forEach(t),tgr=i(va),vn=n(va,"P",{});var g4=s(vn);agr=r(g4,"The model class to instantiate is selected based on the "),E_e=n(g4,"CODE",{});var vmt=s(E_e);ngr=r(vmt,"model_type"),vmt.forEach(t),sgr=r(g4,` property of the config object (either
passed as an argument or loaded from `),y_e=n(g4,"CODE",{});var Tmt=s(y_e);lgr=r(Tmt,"pretrained_model_name_or_path"),Tmt.forEach(t),igr=r(g4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w_e=n(g4,"CODE",{});var Fmt=s(w_e);dgr=r(Fmt,"pretrained_model_name_or_path"),Fmt.forEach(t),cgr=r(g4,":"),g4.forEach(t),fgr=i(va),K=n(va,"UL",{});var oe=s(K);DF=n(oe,"LI",{});var yLe=s(DF);A_e=n(yLe,"STRONG",{});var Cmt=s(A_e);mgr=r(Cmt,"albert"),Cmt.forEach(t),ggr=r(yLe," \u2014 "),pO=n(yLe,"A",{href:!0});var Mmt=s(pO);hgr=r(Mmt,"TFAlbertForTokenClassification"),Mmt.forEach(t),pgr=r(yLe," (ALBERT model)"),yLe.forEach(t),_gr=i(oe),jF=n(oe,"LI",{});var wLe=s(jF);L_e=n(wLe,"STRONG",{});var Emt=s(L_e);ugr=r(Emt,"bert"),Emt.forEach(t),bgr=r(wLe," \u2014 "),_O=n(wLe,"A",{href:!0});var ymt=s(_O);vgr=r(ymt,"TFBertForTokenClassification"),ymt.forEach(t),Tgr=r(wLe," (BERT model)"),wLe.forEach(t),Fgr=i(oe),NF=n(oe,"LI",{});var ALe=s(NF);B_e=n(ALe,"STRONG",{});var wmt=s(B_e);Cgr=r(wmt,"camembert"),wmt.forEach(t),Mgr=r(ALe," \u2014 "),uO=n(ALe,"A",{href:!0});var Amt=s(uO);Egr=r(Amt,"TFCamembertForTokenClassification"),Amt.forEach(t),ygr=r(ALe," (CamemBERT model)"),ALe.forEach(t),wgr=i(oe),qF=n(oe,"LI",{});var LLe=s(qF);x_e=n(LLe,"STRONG",{});var Lmt=s(x_e);Agr=r(Lmt,"convbert"),Lmt.forEach(t),Lgr=r(LLe," \u2014 "),bO=n(LLe,"A",{href:!0});var Bmt=s(bO);Bgr=r(Bmt,"TFConvBertForTokenClassification"),Bmt.forEach(t),xgr=r(LLe," (ConvBERT model)"),LLe.forEach(t),kgr=i(oe),GF=n(oe,"LI",{});var BLe=s(GF);k_e=n(BLe,"STRONG",{});var xmt=s(k_e);Rgr=r(xmt,"deberta"),xmt.forEach(t),Sgr=r(BLe," \u2014 "),vO=n(BLe,"A",{href:!0});var kmt=s(vO);Pgr=r(kmt,"TFDebertaForTokenClassification"),kmt.forEach(t),$gr=r(BLe," (DeBERTa model)"),BLe.forEach(t),Igr=i(oe),OF=n(oe,"LI",{});var xLe=s(OF);R_e=n(xLe,"STRONG",{});var Rmt=s(R_e);Dgr=r(Rmt,"deberta-v2"),Rmt.forEach(t),jgr=r(xLe," \u2014 "),TO=n(xLe,"A",{href:!0});var Smt=s(TO);Ngr=r(Smt,"TFDebertaV2ForTokenClassification"),Smt.forEach(t),qgr=r(xLe," (DeBERTa-v2 model)"),xLe.forEach(t),Ggr=i(oe),XF=n(oe,"LI",{});var kLe=s(XF);S_e=n(kLe,"STRONG",{});var Pmt=s(S_e);Ogr=r(Pmt,"distilbert"),Pmt.forEach(t),Xgr=r(kLe," \u2014 "),FO=n(kLe,"A",{href:!0});var $mt=s(FO);Vgr=r($mt,"TFDistilBertForTokenClassification"),$mt.forEach(t),zgr=r(kLe," (DistilBERT model)"),kLe.forEach(t),Wgr=i(oe),VF=n(oe,"LI",{});var RLe=s(VF);P_e=n(RLe,"STRONG",{});var Imt=s(P_e);Qgr=r(Imt,"electra"),Imt.forEach(t),Hgr=r(RLe," \u2014 "),CO=n(RLe,"A",{href:!0});var Dmt=s(CO);Ugr=r(Dmt,"TFElectraForTokenClassification"),Dmt.forEach(t),Jgr=r(RLe," (ELECTRA model)"),RLe.forEach(t),Ygr=i(oe),zF=n(oe,"LI",{});var SLe=s(zF);$_e=n(SLe,"STRONG",{});var jmt=s($_e);Kgr=r(jmt,"flaubert"),jmt.forEach(t),Zgr=r(SLe," \u2014 "),MO=n(SLe,"A",{href:!0});var Nmt=s(MO);ehr=r(Nmt,"TFFlaubertForTokenClassification"),Nmt.forEach(t),ohr=r(SLe," (FlauBERT model)"),SLe.forEach(t),rhr=i(oe),WF=n(oe,"LI",{});var PLe=s(WF);I_e=n(PLe,"STRONG",{});var qmt=s(I_e);thr=r(qmt,"funnel"),qmt.forEach(t),ahr=r(PLe," \u2014 "),EO=n(PLe,"A",{href:!0});var Gmt=s(EO);nhr=r(Gmt,"TFFunnelForTokenClassification"),Gmt.forEach(t),shr=r(PLe," (Funnel Transformer model)"),PLe.forEach(t),lhr=i(oe),QF=n(oe,"LI",{});var $Le=s(QF);D_e=n($Le,"STRONG",{});var Omt=s(D_e);ihr=r(Omt,"layoutlm"),Omt.forEach(t),dhr=r($Le," \u2014 "),yO=n($Le,"A",{href:!0});var Xmt=s(yO);chr=r(Xmt,"TFLayoutLMForTokenClassification"),Xmt.forEach(t),fhr=r($Le," (LayoutLM model)"),$Le.forEach(t),mhr=i(oe),HF=n(oe,"LI",{});var ILe=s(HF);j_e=n(ILe,"STRONG",{});var Vmt=s(j_e);ghr=r(Vmt,"longformer"),Vmt.forEach(t),hhr=r(ILe," \u2014 "),wO=n(ILe,"A",{href:!0});var zmt=s(wO);phr=r(zmt,"TFLongformerForTokenClassification"),zmt.forEach(t),_hr=r(ILe," (Longformer model)"),ILe.forEach(t),uhr=i(oe),UF=n(oe,"LI",{});var DLe=s(UF);N_e=n(DLe,"STRONG",{});var Wmt=s(N_e);bhr=r(Wmt,"mobilebert"),Wmt.forEach(t),vhr=r(DLe," \u2014 "),AO=n(DLe,"A",{href:!0});var Qmt=s(AO);Thr=r(Qmt,"TFMobileBertForTokenClassification"),Qmt.forEach(t),Fhr=r(DLe," (MobileBERT model)"),DLe.forEach(t),Chr=i(oe),JF=n(oe,"LI",{});var jLe=s(JF);q_e=n(jLe,"STRONG",{});var Hmt=s(q_e);Mhr=r(Hmt,"mpnet"),Hmt.forEach(t),Ehr=r(jLe," \u2014 "),LO=n(jLe,"A",{href:!0});var Umt=s(LO);yhr=r(Umt,"TFMPNetForTokenClassification"),Umt.forEach(t),whr=r(jLe," (MPNet model)"),jLe.forEach(t),Ahr=i(oe),YF=n(oe,"LI",{});var NLe=s(YF);G_e=n(NLe,"STRONG",{});var Jmt=s(G_e);Lhr=r(Jmt,"rembert"),Jmt.forEach(t),Bhr=r(NLe," \u2014 "),BO=n(NLe,"A",{href:!0});var Ymt=s(BO);xhr=r(Ymt,"TFRemBertForTokenClassification"),Ymt.forEach(t),khr=r(NLe," (RemBERT model)"),NLe.forEach(t),Rhr=i(oe),KF=n(oe,"LI",{});var qLe=s(KF);O_e=n(qLe,"STRONG",{});var Kmt=s(O_e);Shr=r(Kmt,"roberta"),Kmt.forEach(t),Phr=r(qLe," \u2014 "),xO=n(qLe,"A",{href:!0});var Zmt=s(xO);$hr=r(Zmt,"TFRobertaForTokenClassification"),Zmt.forEach(t),Ihr=r(qLe," (RoBERTa model)"),qLe.forEach(t),Dhr=i(oe),ZF=n(oe,"LI",{});var GLe=s(ZF);X_e=n(GLe,"STRONG",{});var egt=s(X_e);jhr=r(egt,"roformer"),egt.forEach(t),Nhr=r(GLe," \u2014 "),kO=n(GLe,"A",{href:!0});var ogt=s(kO);qhr=r(ogt,"TFRoFormerForTokenClassification"),ogt.forEach(t),Ghr=r(GLe," (RoFormer model)"),GLe.forEach(t),Ohr=i(oe),e9=n(oe,"LI",{});var OLe=s(e9);V_e=n(OLe,"STRONG",{});var rgt=s(V_e);Xhr=r(rgt,"xlm"),rgt.forEach(t),Vhr=r(OLe," \u2014 "),RO=n(OLe,"A",{href:!0});var tgt=s(RO);zhr=r(tgt,"TFXLMForTokenClassification"),tgt.forEach(t),Whr=r(OLe," (XLM model)"),OLe.forEach(t),Qhr=i(oe),o9=n(oe,"LI",{});var XLe=s(o9);z_e=n(XLe,"STRONG",{});var agt=s(z_e);Hhr=r(agt,"xlm-roberta"),agt.forEach(t),Uhr=r(XLe," \u2014 "),SO=n(XLe,"A",{href:!0});var ngt=s(SO);Jhr=r(ngt,"TFXLMRobertaForTokenClassification"),ngt.forEach(t),Yhr=r(XLe," (XLM-RoBERTa model)"),XLe.forEach(t),Khr=i(oe),r9=n(oe,"LI",{});var VLe=s(r9);W_e=n(VLe,"STRONG",{});var sgt=s(W_e);Zhr=r(sgt,"xlnet"),sgt.forEach(t),epr=r(VLe," \u2014 "),PO=n(VLe,"A",{href:!0});var lgt=s(PO);opr=r(lgt,"TFXLNetForTokenClassification"),lgt.forEach(t),rpr=r(VLe," (XLNet model)"),VLe.forEach(t),oe.forEach(t),tpr=i(va),Q_e=n(va,"P",{});var igt=s(Q_e);apr=r(igt,"Examples:"),igt.forEach(t),npr=i(va),m(T6.$$.fragment,va),va.forEach(t),Jl.forEach(t),Axe=i(c),Rc=n(c,"H2",{class:!0});var DRe=s(Rc);t9=n(DRe,"A",{id:!0,class:!0,href:!0});var dgt=s(t9);H_e=n(dgt,"SPAN",{});var cgt=s(H_e);m(F6.$$.fragment,cgt),cgt.forEach(t),dgt.forEach(t),spr=i(DRe),U_e=n(DRe,"SPAN",{});var fgt=s(U_e);lpr=r(fgt,"TFAutoModelForQuestionAnswering"),fgt.forEach(t),DRe.forEach(t),Lxe=i(c),Ar=n(c,"DIV",{class:!0});var Kl=s(Ar);m(C6.$$.fragment,Kl),ipr=i(Kl),Sc=n(Kl,"P",{});var iW=s(Sc);dpr=r(iW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),J_e=n(iW,"CODE",{});var mgt=s(J_e);cpr=r(mgt,"from_pretrained()"),mgt.forEach(t),fpr=r(iW,"class method or the "),Y_e=n(iW,"CODE",{});var ggt=s(Y_e);mpr=r(ggt,"from_config()"),ggt.forEach(t),gpr=r(iW,`class
method.`),iW.forEach(t),hpr=i(Kl),M6=n(Kl,"P",{});var jRe=s(M6);ppr=r(jRe,"This class cannot be instantiated directly using "),K_e=n(jRe,"CODE",{});var hgt=s(K_e);_pr=r(hgt,"__init__()"),hgt.forEach(t),upr=r(jRe," (throws an error)."),jRe.forEach(t),bpr=i(Kl),Tt=n(Kl,"DIV",{class:!0});var Zl=s(Tt);m(E6.$$.fragment,Zl),vpr=i(Zl),Z_e=n(Zl,"P",{});var pgt=s(Z_e);Tpr=r(pgt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),pgt.forEach(t),Fpr=i(Zl),Pc=n(Zl,"P",{});var dW=s(Pc);Cpr=r(dW,`Note:
Loading a model from its configuration file does `),eue=n(dW,"STRONG",{});var _gt=s(eue);Mpr=r(_gt,"not"),_gt.forEach(t),Epr=r(dW,` load the model weights. It only affects the
model\u2019s configuration. Use `),oue=n(dW,"CODE",{});var ugt=s(oue);ypr=r(ugt,"from_pretrained()"),ugt.forEach(t),wpr=r(dW,"to load the model weights."),dW.forEach(t),Apr=i(Zl),rue=n(Zl,"P",{});var bgt=s(rue);Lpr=r(bgt,"Examples:"),bgt.forEach(t),Bpr=i(Zl),m(y6.$$.fragment,Zl),Zl.forEach(t),xpr=i(Kl),Mo=n(Kl,"DIV",{class:!0});var Ta=s(Mo);m(w6.$$.fragment,Ta),kpr=i(Ta),tue=n(Ta,"P",{});var vgt=s(tue);Rpr=r(vgt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),vgt.forEach(t),Spr=i(Ta),Tn=n(Ta,"P",{});var h4=s(Tn);Ppr=r(h4,"The model class to instantiate is selected based on the "),aue=n(h4,"CODE",{});var Tgt=s(aue);$pr=r(Tgt,"model_type"),Tgt.forEach(t),Ipr=r(h4,` property of the config object (either
passed as an argument or loaded from `),nue=n(h4,"CODE",{});var Fgt=s(nue);Dpr=r(Fgt,"pretrained_model_name_or_path"),Fgt.forEach(t),jpr=r(h4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sue=n(h4,"CODE",{});var Cgt=s(sue);Npr=r(Cgt,"pretrained_model_name_or_path"),Cgt.forEach(t),qpr=r(h4,":"),h4.forEach(t),Gpr=i(Ta),Z=n(Ta,"UL",{});var re=s(Z);a9=n(re,"LI",{});var zLe=s(a9);lue=n(zLe,"STRONG",{});var Mgt=s(lue);Opr=r(Mgt,"albert"),Mgt.forEach(t),Xpr=r(zLe," \u2014 "),$O=n(zLe,"A",{href:!0});var Egt=s($O);Vpr=r(Egt,"TFAlbertForQuestionAnswering"),Egt.forEach(t),zpr=r(zLe," (ALBERT model)"),zLe.forEach(t),Wpr=i(re),n9=n(re,"LI",{});var WLe=s(n9);iue=n(WLe,"STRONG",{});var ygt=s(iue);Qpr=r(ygt,"bert"),ygt.forEach(t),Hpr=r(WLe," \u2014 "),IO=n(WLe,"A",{href:!0});var wgt=s(IO);Upr=r(wgt,"TFBertForQuestionAnswering"),wgt.forEach(t),Jpr=r(WLe," (BERT model)"),WLe.forEach(t),Ypr=i(re),s9=n(re,"LI",{});var QLe=s(s9);due=n(QLe,"STRONG",{});var Agt=s(due);Kpr=r(Agt,"camembert"),Agt.forEach(t),Zpr=r(QLe," \u2014 "),DO=n(QLe,"A",{href:!0});var Lgt=s(DO);e_r=r(Lgt,"TFCamembertForQuestionAnswering"),Lgt.forEach(t),o_r=r(QLe," (CamemBERT model)"),QLe.forEach(t),r_r=i(re),l9=n(re,"LI",{});var HLe=s(l9);cue=n(HLe,"STRONG",{});var Bgt=s(cue);t_r=r(Bgt,"convbert"),Bgt.forEach(t),a_r=r(HLe," \u2014 "),jO=n(HLe,"A",{href:!0});var xgt=s(jO);n_r=r(xgt,"TFConvBertForQuestionAnswering"),xgt.forEach(t),s_r=r(HLe," (ConvBERT model)"),HLe.forEach(t),l_r=i(re),i9=n(re,"LI",{});var ULe=s(i9);fue=n(ULe,"STRONG",{});var kgt=s(fue);i_r=r(kgt,"deberta"),kgt.forEach(t),d_r=r(ULe," \u2014 "),NO=n(ULe,"A",{href:!0});var Rgt=s(NO);c_r=r(Rgt,"TFDebertaForQuestionAnswering"),Rgt.forEach(t),f_r=r(ULe," (DeBERTa model)"),ULe.forEach(t),m_r=i(re),d9=n(re,"LI",{});var JLe=s(d9);mue=n(JLe,"STRONG",{});var Sgt=s(mue);g_r=r(Sgt,"deberta-v2"),Sgt.forEach(t),h_r=r(JLe," \u2014 "),qO=n(JLe,"A",{href:!0});var Pgt=s(qO);p_r=r(Pgt,"TFDebertaV2ForQuestionAnswering"),Pgt.forEach(t),__r=r(JLe," (DeBERTa-v2 model)"),JLe.forEach(t),u_r=i(re),c9=n(re,"LI",{});var YLe=s(c9);gue=n(YLe,"STRONG",{});var $gt=s(gue);b_r=r($gt,"distilbert"),$gt.forEach(t),v_r=r(YLe," \u2014 "),GO=n(YLe,"A",{href:!0});var Igt=s(GO);T_r=r(Igt,"TFDistilBertForQuestionAnswering"),Igt.forEach(t),F_r=r(YLe," (DistilBERT model)"),YLe.forEach(t),C_r=i(re),f9=n(re,"LI",{});var KLe=s(f9);hue=n(KLe,"STRONG",{});var Dgt=s(hue);M_r=r(Dgt,"electra"),Dgt.forEach(t),E_r=r(KLe," \u2014 "),OO=n(KLe,"A",{href:!0});var jgt=s(OO);y_r=r(jgt,"TFElectraForQuestionAnswering"),jgt.forEach(t),w_r=r(KLe," (ELECTRA model)"),KLe.forEach(t),A_r=i(re),m9=n(re,"LI",{});var ZLe=s(m9);pue=n(ZLe,"STRONG",{});var Ngt=s(pue);L_r=r(Ngt,"flaubert"),Ngt.forEach(t),B_r=r(ZLe," \u2014 "),XO=n(ZLe,"A",{href:!0});var qgt=s(XO);x_r=r(qgt,"TFFlaubertForQuestionAnsweringSimple"),qgt.forEach(t),k_r=r(ZLe," (FlauBERT model)"),ZLe.forEach(t),R_r=i(re),g9=n(re,"LI",{});var e8e=s(g9);_ue=n(e8e,"STRONG",{});var Ggt=s(_ue);S_r=r(Ggt,"funnel"),Ggt.forEach(t),P_r=r(e8e," \u2014 "),VO=n(e8e,"A",{href:!0});var Ogt=s(VO);$_r=r(Ogt,"TFFunnelForQuestionAnswering"),Ogt.forEach(t),I_r=r(e8e," (Funnel Transformer model)"),e8e.forEach(t),D_r=i(re),h9=n(re,"LI",{});var o8e=s(h9);uue=n(o8e,"STRONG",{});var Xgt=s(uue);j_r=r(Xgt,"longformer"),Xgt.forEach(t),N_r=r(o8e," \u2014 "),zO=n(o8e,"A",{href:!0});var Vgt=s(zO);q_r=r(Vgt,"TFLongformerForQuestionAnswering"),Vgt.forEach(t),G_r=r(o8e," (Longformer model)"),o8e.forEach(t),O_r=i(re),p9=n(re,"LI",{});var r8e=s(p9);bue=n(r8e,"STRONG",{});var zgt=s(bue);X_r=r(zgt,"mobilebert"),zgt.forEach(t),V_r=r(r8e," \u2014 "),WO=n(r8e,"A",{href:!0});var Wgt=s(WO);z_r=r(Wgt,"TFMobileBertForQuestionAnswering"),Wgt.forEach(t),W_r=r(r8e," (MobileBERT model)"),r8e.forEach(t),Q_r=i(re),_9=n(re,"LI",{});var t8e=s(_9);vue=n(t8e,"STRONG",{});var Qgt=s(vue);H_r=r(Qgt,"mpnet"),Qgt.forEach(t),U_r=r(t8e," \u2014 "),QO=n(t8e,"A",{href:!0});var Hgt=s(QO);J_r=r(Hgt,"TFMPNetForQuestionAnswering"),Hgt.forEach(t),Y_r=r(t8e," (MPNet model)"),t8e.forEach(t),K_r=i(re),u9=n(re,"LI",{});var a8e=s(u9);Tue=n(a8e,"STRONG",{});var Ugt=s(Tue);Z_r=r(Ugt,"rembert"),Ugt.forEach(t),eur=r(a8e," \u2014 "),HO=n(a8e,"A",{href:!0});var Jgt=s(HO);our=r(Jgt,"TFRemBertForQuestionAnswering"),Jgt.forEach(t),rur=r(a8e," (RemBERT model)"),a8e.forEach(t),tur=i(re),b9=n(re,"LI",{});var n8e=s(b9);Fue=n(n8e,"STRONG",{});var Ygt=s(Fue);aur=r(Ygt,"roberta"),Ygt.forEach(t),nur=r(n8e," \u2014 "),UO=n(n8e,"A",{href:!0});var Kgt=s(UO);sur=r(Kgt,"TFRobertaForQuestionAnswering"),Kgt.forEach(t),lur=r(n8e," (RoBERTa model)"),n8e.forEach(t),iur=i(re),v9=n(re,"LI",{});var s8e=s(v9);Cue=n(s8e,"STRONG",{});var Zgt=s(Cue);dur=r(Zgt,"roformer"),Zgt.forEach(t),cur=r(s8e," \u2014 "),JO=n(s8e,"A",{href:!0});var eht=s(JO);fur=r(eht,"TFRoFormerForQuestionAnswering"),eht.forEach(t),mur=r(s8e," (RoFormer model)"),s8e.forEach(t),gur=i(re),T9=n(re,"LI",{});var l8e=s(T9);Mue=n(l8e,"STRONG",{});var oht=s(Mue);hur=r(oht,"xlm"),oht.forEach(t),pur=r(l8e," \u2014 "),YO=n(l8e,"A",{href:!0});var rht=s(YO);_ur=r(rht,"TFXLMForQuestionAnsweringSimple"),rht.forEach(t),uur=r(l8e," (XLM model)"),l8e.forEach(t),bur=i(re),F9=n(re,"LI",{});var i8e=s(F9);Eue=n(i8e,"STRONG",{});var tht=s(Eue);vur=r(tht,"xlm-roberta"),tht.forEach(t),Tur=r(i8e," \u2014 "),KO=n(i8e,"A",{href:!0});var aht=s(KO);Fur=r(aht,"TFXLMRobertaForQuestionAnswering"),aht.forEach(t),Cur=r(i8e," (XLM-RoBERTa model)"),i8e.forEach(t),Mur=i(re),C9=n(re,"LI",{});var d8e=s(C9);yue=n(d8e,"STRONG",{});var nht=s(yue);Eur=r(nht,"xlnet"),nht.forEach(t),yur=r(d8e," \u2014 "),ZO=n(d8e,"A",{href:!0});var sht=s(ZO);wur=r(sht,"TFXLNetForQuestionAnsweringSimple"),sht.forEach(t),Aur=r(d8e," (XLNet model)"),d8e.forEach(t),re.forEach(t),Lur=i(Ta),wue=n(Ta,"P",{});var lht=s(wue);Bur=r(lht,"Examples:"),lht.forEach(t),xur=i(Ta),m(A6.$$.fragment,Ta),Ta.forEach(t),Kl.forEach(t),Bxe=i(c),$c=n(c,"H2",{class:!0});var NRe=s($c);M9=n(NRe,"A",{id:!0,class:!0,href:!0});var iht=s(M9);Aue=n(iht,"SPAN",{});var dht=s(Aue);m(L6.$$.fragment,dht),dht.forEach(t),iht.forEach(t),kur=i(NRe),Lue=n(NRe,"SPAN",{});var cht=s(Lue);Rur=r(cht,"TFAutoModelForVision2Seq"),cht.forEach(t),NRe.forEach(t),xxe=i(c),Lr=n(c,"DIV",{class:!0});var ei=s(Lr);m(B6.$$.fragment,ei),Sur=i(ei),Ic=n(ei,"P",{});var cW=s(Ic);Pur=r(cW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Bue=n(cW,"CODE",{});var fht=s(Bue);$ur=r(fht,"from_pretrained()"),fht.forEach(t),Iur=r(cW,"class method or the "),xue=n(cW,"CODE",{});var mht=s(xue);Dur=r(mht,"from_config()"),mht.forEach(t),jur=r(cW,`class
method.`),cW.forEach(t),Nur=i(ei),x6=n(ei,"P",{});var qRe=s(x6);qur=r(qRe,"This class cannot be instantiated directly using "),kue=n(qRe,"CODE",{});var ght=s(kue);Gur=r(ght,"__init__()"),ght.forEach(t),Our=r(qRe," (throws an error)."),qRe.forEach(t),Xur=i(ei),Ft=n(ei,"DIV",{class:!0});var oi=s(Ft);m(k6.$$.fragment,oi),Vur=i(oi),Rue=n(oi,"P",{});var hht=s(Rue);zur=r(hht,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),hht.forEach(t),Wur=i(oi),Dc=n(oi,"P",{});var fW=s(Dc);Qur=r(fW,`Note:
Loading a model from its configuration file does `),Sue=n(fW,"STRONG",{});var pht=s(Sue);Hur=r(pht,"not"),pht.forEach(t),Uur=r(fW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pue=n(fW,"CODE",{});var _ht=s(Pue);Jur=r(_ht,"from_pretrained()"),_ht.forEach(t),Yur=r(fW,"to load the model weights."),fW.forEach(t),Kur=i(oi),$ue=n(oi,"P",{});var uht=s($ue);Zur=r(uht,"Examples:"),uht.forEach(t),e0r=i(oi),m(R6.$$.fragment,oi),oi.forEach(t),o0r=i(ei),Eo=n(ei,"DIV",{class:!0});var Fa=s(Eo);m(S6.$$.fragment,Fa),r0r=i(Fa),Iue=n(Fa,"P",{});var bht=s(Iue);t0r=r(bht,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),bht.forEach(t),a0r=i(Fa),Fn=n(Fa,"P",{});var p4=s(Fn);n0r=r(p4,"The model class to instantiate is selected based on the "),Due=n(p4,"CODE",{});var vht=s(Due);s0r=r(vht,"model_type"),vht.forEach(t),l0r=r(p4,` property of the config object (either
passed as an argument or loaded from `),jue=n(p4,"CODE",{});var Tht=s(jue);i0r=r(Tht,"pretrained_model_name_or_path"),Tht.forEach(t),d0r=r(p4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=n(p4,"CODE",{});var Fht=s(Nue);c0r=r(Fht,"pretrained_model_name_or_path"),Fht.forEach(t),f0r=r(p4,":"),p4.forEach(t),m0r=i(Fa),que=n(Fa,"UL",{});var Cht=s(que);E9=n(Cht,"LI",{});var c8e=s(E9);Gue=n(c8e,"STRONG",{});var Mht=s(Gue);g0r=r(Mht,"vision-encoder-decoder"),Mht.forEach(t),h0r=r(c8e," \u2014 "),eX=n(c8e,"A",{href:!0});var Eht=s(eX);p0r=r(Eht,"TFVisionEncoderDecoderModel"),Eht.forEach(t),_0r=r(c8e," (Vision Encoder decoder model)"),c8e.forEach(t),Cht.forEach(t),u0r=i(Fa),Oue=n(Fa,"P",{});var yht=s(Oue);b0r=r(yht,"Examples:"),yht.forEach(t),v0r=i(Fa),m(P6.$$.fragment,Fa),Fa.forEach(t),ei.forEach(t),kxe=i(c),jc=n(c,"H2",{class:!0});var GRe=s(jc);y9=n(GRe,"A",{id:!0,class:!0,href:!0});var wht=s(y9);Xue=n(wht,"SPAN",{});var Aht=s(Xue);m($6.$$.fragment,Aht),Aht.forEach(t),wht.forEach(t),T0r=i(GRe),Vue=n(GRe,"SPAN",{});var Lht=s(Vue);F0r=r(Lht,"TFAutoModelForSpeechSeq2Seq"),Lht.forEach(t),GRe.forEach(t),Rxe=i(c),Br=n(c,"DIV",{class:!0});var ri=s(Br);m(I6.$$.fragment,ri),C0r=i(ri),Nc=n(ri,"P",{});var mW=s(Nc);M0r=r(mW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),zue=n(mW,"CODE",{});var Bht=s(zue);E0r=r(Bht,"from_pretrained()"),Bht.forEach(t),y0r=r(mW,"class method or the "),Wue=n(mW,"CODE",{});var xht=s(Wue);w0r=r(xht,"from_config()"),xht.forEach(t),A0r=r(mW,`class
method.`),mW.forEach(t),L0r=i(ri),D6=n(ri,"P",{});var ORe=s(D6);B0r=r(ORe,"This class cannot be instantiated directly using "),Que=n(ORe,"CODE",{});var kht=s(Que);x0r=r(kht,"__init__()"),kht.forEach(t),k0r=r(ORe," (throws an error)."),ORe.forEach(t),R0r=i(ri),Ct=n(ri,"DIV",{class:!0});var ti=s(Ct);m(j6.$$.fragment,ti),S0r=i(ti),Hue=n(ti,"P",{});var Rht=s(Hue);P0r=r(Rht,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Rht.forEach(t),$0r=i(ti),qc=n(ti,"P",{});var gW=s(qc);I0r=r(gW,`Note:
Loading a model from its configuration file does `),Uue=n(gW,"STRONG",{});var Sht=s(Uue);D0r=r(Sht,"not"),Sht.forEach(t),j0r=r(gW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jue=n(gW,"CODE",{});var Pht=s(Jue);N0r=r(Pht,"from_pretrained()"),Pht.forEach(t),q0r=r(gW,"to load the model weights."),gW.forEach(t),G0r=i(ti),Yue=n(ti,"P",{});var $ht=s(Yue);O0r=r($ht,"Examples:"),$ht.forEach(t),X0r=i(ti),m(N6.$$.fragment,ti),ti.forEach(t),V0r=i(ri),yo=n(ri,"DIV",{class:!0});var Ca=s(yo);m(q6.$$.fragment,Ca),z0r=i(Ca),Kue=n(Ca,"P",{});var Iht=s(Kue);W0r=r(Iht,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Iht.forEach(t),Q0r=i(Ca),Cn=n(Ca,"P",{});var _4=s(Cn);H0r=r(_4,"The model class to instantiate is selected based on the "),Zue=n(_4,"CODE",{});var Dht=s(Zue);U0r=r(Dht,"model_type"),Dht.forEach(t),J0r=r(_4,` property of the config object (either
passed as an argument or loaded from `),e0e=n(_4,"CODE",{});var jht=s(e0e);Y0r=r(jht,"pretrained_model_name_or_path"),jht.forEach(t),K0r=r(_4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o0e=n(_4,"CODE",{});var Nht=s(o0e);Z0r=r(Nht,"pretrained_model_name_or_path"),Nht.forEach(t),e1r=r(_4,":"),_4.forEach(t),o1r=i(Ca),r0e=n(Ca,"UL",{});var qht=s(r0e);w9=n(qht,"LI",{});var f8e=s(w9);t0e=n(f8e,"STRONG",{});var Ght=s(t0e);r1r=r(Ght,"speech_to_text"),Ght.forEach(t),t1r=r(f8e," \u2014 "),oX=n(f8e,"A",{href:!0});var Oht=s(oX);a1r=r(Oht,"TFSpeech2TextForConditionalGeneration"),Oht.forEach(t),n1r=r(f8e," (Speech2Text model)"),f8e.forEach(t),qht.forEach(t),s1r=i(Ca),a0e=n(Ca,"P",{});var Xht=s(a0e);l1r=r(Xht,"Examples:"),Xht.forEach(t),i1r=i(Ca),m(G6.$$.fragment,Ca),Ca.forEach(t),ri.forEach(t),Sxe=i(c),Gc=n(c,"H2",{class:!0});var XRe=s(Gc);A9=n(XRe,"A",{id:!0,class:!0,href:!0});var Vht=s(A9);n0e=n(Vht,"SPAN",{});var zht=s(n0e);m(O6.$$.fragment,zht),zht.forEach(t),Vht.forEach(t),d1r=i(XRe),s0e=n(XRe,"SPAN",{});var Wht=s(s0e);c1r=r(Wht,"FlaxAutoModel"),Wht.forEach(t),XRe.forEach(t),Pxe=i(c),xr=n(c,"DIV",{class:!0});var ai=s(xr);m(X6.$$.fragment,ai),f1r=i(ai),Oc=n(ai,"P",{});var hW=s(Oc);m1r=r(hW,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),l0e=n(hW,"CODE",{});var Qht=s(l0e);g1r=r(Qht,"from_pretrained()"),Qht.forEach(t),h1r=r(hW,"class method or the "),i0e=n(hW,"CODE",{});var Hht=s(i0e);p1r=r(Hht,"from_config()"),Hht.forEach(t),_1r=r(hW,`class
method.`),hW.forEach(t),u1r=i(ai),V6=n(ai,"P",{});var VRe=s(V6);b1r=r(VRe,"This class cannot be instantiated directly using "),d0e=n(VRe,"CODE",{});var Uht=s(d0e);v1r=r(Uht,"__init__()"),Uht.forEach(t),T1r=r(VRe," (throws an error)."),VRe.forEach(t),F1r=i(ai),Mt=n(ai,"DIV",{class:!0});var ni=s(Mt);m(z6.$$.fragment,ni),C1r=i(ni),c0e=n(ni,"P",{});var Jht=s(c0e);M1r=r(Jht,"Instantiates one of the base model classes of the library from a configuration."),Jht.forEach(t),E1r=i(ni),Xc=n(ni,"P",{});var pW=s(Xc);y1r=r(pW,`Note:
Loading a model from its configuration file does `),f0e=n(pW,"STRONG",{});var Yht=s(f0e);w1r=r(Yht,"not"),Yht.forEach(t),A1r=r(pW,` load the model weights. It only affects the
model\u2019s configuration. Use `),m0e=n(pW,"CODE",{});var Kht=s(m0e);L1r=r(Kht,"from_pretrained()"),Kht.forEach(t),B1r=r(pW,"to load the model weights."),pW.forEach(t),x1r=i(ni),g0e=n(ni,"P",{});var Zht=s(g0e);k1r=r(Zht,"Examples:"),Zht.forEach(t),R1r=i(ni),m(W6.$$.fragment,ni),ni.forEach(t),S1r=i(ai),wo=n(ai,"DIV",{class:!0});var Ma=s(wo);m(Q6.$$.fragment,Ma),P1r=i(Ma),h0e=n(Ma,"P",{});var ept=s(h0e);$1r=r(ept,"Instantiate one of the base model classes of the library from a pretrained model."),ept.forEach(t),I1r=i(Ma),Mn=n(Ma,"P",{});var u4=s(Mn);D1r=r(u4,"The model class to instantiate is selected based on the "),p0e=n(u4,"CODE",{});var opt=s(p0e);j1r=r(opt,"model_type"),opt.forEach(t),N1r=r(u4,` property of the config object (either
passed as an argument or loaded from `),_0e=n(u4,"CODE",{});var rpt=s(_0e);q1r=r(rpt,"pretrained_model_name_or_path"),rpt.forEach(t),G1r=r(u4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),u0e=n(u4,"CODE",{});var tpt=s(u0e);O1r=r(tpt,"pretrained_model_name_or_path"),tpt.forEach(t),X1r=r(u4,":"),u4.forEach(t),V1r=i(Ma),V=n(Ma,"UL",{});var Q=s(V);L9=n(Q,"LI",{});var m8e=s(L9);b0e=n(m8e,"STRONG",{});var apt=s(b0e);z1r=r(apt,"albert"),apt.forEach(t),W1r=r(m8e," \u2014 "),rX=n(m8e,"A",{href:!0});var npt=s(rX);Q1r=r(npt,"FlaxAlbertModel"),npt.forEach(t),H1r=r(m8e," (ALBERT model)"),m8e.forEach(t),U1r=i(Q),B9=n(Q,"LI",{});var g8e=s(B9);v0e=n(g8e,"STRONG",{});var spt=s(v0e);J1r=r(spt,"bart"),spt.forEach(t),Y1r=r(g8e," \u2014 "),tX=n(g8e,"A",{href:!0});var lpt=s(tX);K1r=r(lpt,"FlaxBartModel"),lpt.forEach(t),Z1r=r(g8e," (BART model)"),g8e.forEach(t),ebr=i(Q),x9=n(Q,"LI",{});var h8e=s(x9);T0e=n(h8e,"STRONG",{});var ipt=s(T0e);obr=r(ipt,"beit"),ipt.forEach(t),rbr=r(h8e," \u2014 "),aX=n(h8e,"A",{href:!0});var dpt=s(aX);tbr=r(dpt,"FlaxBeitModel"),dpt.forEach(t),abr=r(h8e," (BEiT model)"),h8e.forEach(t),nbr=i(Q),k9=n(Q,"LI",{});var p8e=s(k9);F0e=n(p8e,"STRONG",{});var cpt=s(F0e);sbr=r(cpt,"bert"),cpt.forEach(t),lbr=r(p8e," \u2014 "),nX=n(p8e,"A",{href:!0});var fpt=s(nX);ibr=r(fpt,"FlaxBertModel"),fpt.forEach(t),dbr=r(p8e," (BERT model)"),p8e.forEach(t),cbr=i(Q),R9=n(Q,"LI",{});var _8e=s(R9);C0e=n(_8e,"STRONG",{});var mpt=s(C0e);fbr=r(mpt,"big_bird"),mpt.forEach(t),mbr=r(_8e," \u2014 "),sX=n(_8e,"A",{href:!0});var gpt=s(sX);gbr=r(gpt,"FlaxBigBirdModel"),gpt.forEach(t),hbr=r(_8e," (BigBird model)"),_8e.forEach(t),pbr=i(Q),S9=n(Q,"LI",{});var u8e=s(S9);M0e=n(u8e,"STRONG",{});var hpt=s(M0e);_br=r(hpt,"blenderbot"),hpt.forEach(t),ubr=r(u8e," \u2014 "),lX=n(u8e,"A",{href:!0});var ppt=s(lX);bbr=r(ppt,"FlaxBlenderbotModel"),ppt.forEach(t),vbr=r(u8e," (Blenderbot model)"),u8e.forEach(t),Tbr=i(Q),P9=n(Q,"LI",{});var b8e=s(P9);E0e=n(b8e,"STRONG",{});var _pt=s(E0e);Fbr=r(_pt,"blenderbot-small"),_pt.forEach(t),Cbr=r(b8e," \u2014 "),iX=n(b8e,"A",{href:!0});var upt=s(iX);Mbr=r(upt,"FlaxBlenderbotSmallModel"),upt.forEach(t),Ebr=r(b8e," (BlenderbotSmall model)"),b8e.forEach(t),ybr=i(Q),$9=n(Q,"LI",{});var v8e=s($9);y0e=n(v8e,"STRONG",{});var bpt=s(y0e);wbr=r(bpt,"clip"),bpt.forEach(t),Abr=r(v8e," \u2014 "),dX=n(v8e,"A",{href:!0});var vpt=s(dX);Lbr=r(vpt,"FlaxCLIPModel"),vpt.forEach(t),Bbr=r(v8e," (CLIP model)"),v8e.forEach(t),xbr=i(Q),I9=n(Q,"LI",{});var T8e=s(I9);w0e=n(T8e,"STRONG",{});var Tpt=s(w0e);kbr=r(Tpt,"distilbert"),Tpt.forEach(t),Rbr=r(T8e," \u2014 "),cX=n(T8e,"A",{href:!0});var Fpt=s(cX);Sbr=r(Fpt,"FlaxDistilBertModel"),Fpt.forEach(t),Pbr=r(T8e," (DistilBERT model)"),T8e.forEach(t),$br=i(Q),D9=n(Q,"LI",{});var F8e=s(D9);A0e=n(F8e,"STRONG",{});var Cpt=s(A0e);Ibr=r(Cpt,"electra"),Cpt.forEach(t),Dbr=r(F8e," \u2014 "),fX=n(F8e,"A",{href:!0});var Mpt=s(fX);jbr=r(Mpt,"FlaxElectraModel"),Mpt.forEach(t),Nbr=r(F8e," (ELECTRA model)"),F8e.forEach(t),qbr=i(Q),j9=n(Q,"LI",{});var C8e=s(j9);L0e=n(C8e,"STRONG",{});var Ept=s(L0e);Gbr=r(Ept,"gpt2"),Ept.forEach(t),Obr=r(C8e," \u2014 "),mX=n(C8e,"A",{href:!0});var ypt=s(mX);Xbr=r(ypt,"FlaxGPT2Model"),ypt.forEach(t),Vbr=r(C8e," (OpenAI GPT-2 model)"),C8e.forEach(t),zbr=i(Q),N9=n(Q,"LI",{});var M8e=s(N9);B0e=n(M8e,"STRONG",{});var wpt=s(B0e);Wbr=r(wpt,"gpt_neo"),wpt.forEach(t),Qbr=r(M8e," \u2014 "),gX=n(M8e,"A",{href:!0});var Apt=s(gX);Hbr=r(Apt,"FlaxGPTNeoModel"),Apt.forEach(t),Ubr=r(M8e," (GPT Neo model)"),M8e.forEach(t),Jbr=i(Q),q9=n(Q,"LI",{});var E8e=s(q9);x0e=n(E8e,"STRONG",{});var Lpt=s(x0e);Ybr=r(Lpt,"gptj"),Lpt.forEach(t),Kbr=r(E8e," \u2014 "),hX=n(E8e,"A",{href:!0});var Bpt=s(hX);Zbr=r(Bpt,"FlaxGPTJModel"),Bpt.forEach(t),e5r=r(E8e," (GPT-J model)"),E8e.forEach(t),o5r=i(Q),G9=n(Q,"LI",{});var y8e=s(G9);k0e=n(y8e,"STRONG",{});var xpt=s(k0e);r5r=r(xpt,"marian"),xpt.forEach(t),t5r=r(y8e," \u2014 "),pX=n(y8e,"A",{href:!0});var kpt=s(pX);a5r=r(kpt,"FlaxMarianModel"),kpt.forEach(t),n5r=r(y8e," (Marian model)"),y8e.forEach(t),s5r=i(Q),O9=n(Q,"LI",{});var w8e=s(O9);R0e=n(w8e,"STRONG",{});var Rpt=s(R0e);l5r=r(Rpt,"mbart"),Rpt.forEach(t),i5r=r(w8e," \u2014 "),_X=n(w8e,"A",{href:!0});var Spt=s(_X);d5r=r(Spt,"FlaxMBartModel"),Spt.forEach(t),c5r=r(w8e," (mBART model)"),w8e.forEach(t),f5r=i(Q),X9=n(Q,"LI",{});var A8e=s(X9);S0e=n(A8e,"STRONG",{});var Ppt=s(S0e);m5r=r(Ppt,"mt5"),Ppt.forEach(t),g5r=r(A8e," \u2014 "),uX=n(A8e,"A",{href:!0});var $pt=s(uX);h5r=r($pt,"FlaxMT5Model"),$pt.forEach(t),p5r=r(A8e," (mT5 model)"),A8e.forEach(t),_5r=i(Q),V9=n(Q,"LI",{});var L8e=s(V9);P0e=n(L8e,"STRONG",{});var Ipt=s(P0e);u5r=r(Ipt,"pegasus"),Ipt.forEach(t),b5r=r(L8e," \u2014 "),bX=n(L8e,"A",{href:!0});var Dpt=s(bX);v5r=r(Dpt,"FlaxPegasusModel"),Dpt.forEach(t),T5r=r(L8e," (Pegasus model)"),L8e.forEach(t),F5r=i(Q),z9=n(Q,"LI",{});var B8e=s(z9);$0e=n(B8e,"STRONG",{});var jpt=s($0e);C5r=r(jpt,"roberta"),jpt.forEach(t),M5r=r(B8e," \u2014 "),vX=n(B8e,"A",{href:!0});var Npt=s(vX);E5r=r(Npt,"FlaxRobertaModel"),Npt.forEach(t),y5r=r(B8e," (RoBERTa model)"),B8e.forEach(t),w5r=i(Q),W9=n(Q,"LI",{});var x8e=s(W9);I0e=n(x8e,"STRONG",{});var qpt=s(I0e);A5r=r(qpt,"roformer"),qpt.forEach(t),L5r=r(x8e," \u2014 "),TX=n(x8e,"A",{href:!0});var Gpt=s(TX);B5r=r(Gpt,"FlaxRoFormerModel"),Gpt.forEach(t),x5r=r(x8e," (RoFormer model)"),x8e.forEach(t),k5r=i(Q),Q9=n(Q,"LI",{});var k8e=s(Q9);D0e=n(k8e,"STRONG",{});var Opt=s(D0e);R5r=r(Opt,"t5"),Opt.forEach(t),S5r=r(k8e," \u2014 "),FX=n(k8e,"A",{href:!0});var Xpt=s(FX);P5r=r(Xpt,"FlaxT5Model"),Xpt.forEach(t),$5r=r(k8e," (T5 model)"),k8e.forEach(t),I5r=i(Q),H9=n(Q,"LI",{});var R8e=s(H9);j0e=n(R8e,"STRONG",{});var Vpt=s(j0e);D5r=r(Vpt,"vision-text-dual-encoder"),Vpt.forEach(t),j5r=r(R8e," \u2014 "),CX=n(R8e,"A",{href:!0});var zpt=s(CX);N5r=r(zpt,"FlaxVisionTextDualEncoderModel"),zpt.forEach(t),q5r=r(R8e," (VisionTextDualEncoder model)"),R8e.forEach(t),G5r=i(Q),U9=n(Q,"LI",{});var S8e=s(U9);N0e=n(S8e,"STRONG",{});var Wpt=s(N0e);O5r=r(Wpt,"vit"),Wpt.forEach(t),X5r=r(S8e," \u2014 "),MX=n(S8e,"A",{href:!0});var Qpt=s(MX);V5r=r(Qpt,"FlaxViTModel"),Qpt.forEach(t),z5r=r(S8e," (ViT model)"),S8e.forEach(t),W5r=i(Q),J9=n(Q,"LI",{});var P8e=s(J9);q0e=n(P8e,"STRONG",{});var Hpt=s(q0e);Q5r=r(Hpt,"wav2vec2"),Hpt.forEach(t),H5r=r(P8e," \u2014 "),EX=n(P8e,"A",{href:!0});var Upt=s(EX);U5r=r(Upt,"FlaxWav2Vec2Model"),Upt.forEach(t),J5r=r(P8e," (Wav2Vec2 model)"),P8e.forEach(t),Y5r=i(Q),Y9=n(Q,"LI",{});var $8e=s(Y9);G0e=n($8e,"STRONG",{});var Jpt=s(G0e);K5r=r(Jpt,"xglm"),Jpt.forEach(t),Z5r=r($8e," \u2014 "),yX=n($8e,"A",{href:!0});var Ypt=s(yX);e2r=r(Ypt,"FlaxXGLMModel"),Ypt.forEach(t),o2r=r($8e," (XGLM model)"),$8e.forEach(t),r2r=i(Q),K9=n(Q,"LI",{});var I8e=s(K9);O0e=n(I8e,"STRONG",{});var Kpt=s(O0e);t2r=r(Kpt,"xlm-roberta"),Kpt.forEach(t),a2r=r(I8e," \u2014 "),wX=n(I8e,"A",{href:!0});var Zpt=s(wX);n2r=r(Zpt,"FlaxXLMRobertaModel"),Zpt.forEach(t),s2r=r(I8e," (XLM-RoBERTa model)"),I8e.forEach(t),Q.forEach(t),l2r=i(Ma),X0e=n(Ma,"P",{});var e_t=s(X0e);i2r=r(e_t,"Examples:"),e_t.forEach(t),d2r=i(Ma),m(H6.$$.fragment,Ma),Ma.forEach(t),ai.forEach(t),$xe=i(c),Vc=n(c,"H2",{class:!0});var zRe=s(Vc);Z9=n(zRe,"A",{id:!0,class:!0,href:!0});var o_t=s(Z9);V0e=n(o_t,"SPAN",{});var r_t=s(V0e);m(U6.$$.fragment,r_t),r_t.forEach(t),o_t.forEach(t),c2r=i(zRe),z0e=n(zRe,"SPAN",{});var t_t=s(z0e);f2r=r(t_t,"FlaxAutoModelForCausalLM"),t_t.forEach(t),zRe.forEach(t),Ixe=i(c),kr=n(c,"DIV",{class:!0});var si=s(kr);m(J6.$$.fragment,si),m2r=i(si),zc=n(si,"P",{});var _W=s(zc);g2r=r(_W,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),W0e=n(_W,"CODE",{});var a_t=s(W0e);h2r=r(a_t,"from_pretrained()"),a_t.forEach(t),p2r=r(_W,"class method or the "),Q0e=n(_W,"CODE",{});var n_t=s(Q0e);_2r=r(n_t,"from_config()"),n_t.forEach(t),u2r=r(_W,`class
method.`),_W.forEach(t),b2r=i(si),Y6=n(si,"P",{});var WRe=s(Y6);v2r=r(WRe,"This class cannot be instantiated directly using "),H0e=n(WRe,"CODE",{});var s_t=s(H0e);T2r=r(s_t,"__init__()"),s_t.forEach(t),F2r=r(WRe," (throws an error)."),WRe.forEach(t),C2r=i(si),Et=n(si,"DIV",{class:!0});var li=s(Et);m(K6.$$.fragment,li),M2r=i(li),U0e=n(li,"P",{});var l_t=s(U0e);E2r=r(l_t,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),l_t.forEach(t),y2r=i(li),Wc=n(li,"P",{});var uW=s(Wc);w2r=r(uW,`Note:
Loading a model from its configuration file does `),J0e=n(uW,"STRONG",{});var i_t=s(J0e);A2r=r(i_t,"not"),i_t.forEach(t),L2r=r(uW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Y0e=n(uW,"CODE",{});var d_t=s(Y0e);B2r=r(d_t,"from_pretrained()"),d_t.forEach(t),x2r=r(uW,"to load the model weights."),uW.forEach(t),k2r=i(li),K0e=n(li,"P",{});var c_t=s(K0e);R2r=r(c_t,"Examples:"),c_t.forEach(t),S2r=i(li),m(Z6.$$.fragment,li),li.forEach(t),P2r=i(si),Ao=n(si,"DIV",{class:!0});var Ea=s(Ao);m(eA.$$.fragment,Ea),$2r=i(Ea),Z0e=n(Ea,"P",{});var f_t=s(Z0e);I2r=r(f_t,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),f_t.forEach(t),D2r=i(Ea),En=n(Ea,"P",{});var b4=s(En);j2r=r(b4,"The model class to instantiate is selected based on the "),e1e=n(b4,"CODE",{});var m_t=s(e1e);N2r=r(m_t,"model_type"),m_t.forEach(t),q2r=r(b4,` property of the config object (either
passed as an argument or loaded from `),o1e=n(b4,"CODE",{});var g_t=s(o1e);G2r=r(g_t,"pretrained_model_name_or_path"),g_t.forEach(t),O2r=r(b4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r1e=n(b4,"CODE",{});var h_t=s(r1e);X2r=r(h_t,"pretrained_model_name_or_path"),h_t.forEach(t),V2r=r(b4,":"),b4.forEach(t),z2r=i(Ea),yn=n(Ea,"UL",{});var v4=s(yn);eC=n(v4,"LI",{});var D8e=s(eC);t1e=n(D8e,"STRONG",{});var p_t=s(t1e);W2r=r(p_t,"gpt2"),p_t.forEach(t),Q2r=r(D8e," \u2014 "),AX=n(D8e,"A",{href:!0});var __t=s(AX);H2r=r(__t,"FlaxGPT2LMHeadModel"),__t.forEach(t),U2r=r(D8e," (OpenAI GPT-2 model)"),D8e.forEach(t),J2r=i(v4),oC=n(v4,"LI",{});var j8e=s(oC);a1e=n(j8e,"STRONG",{});var u_t=s(a1e);Y2r=r(u_t,"gpt_neo"),u_t.forEach(t),K2r=r(j8e," \u2014 "),LX=n(j8e,"A",{href:!0});var b_t=s(LX);Z2r=r(b_t,"FlaxGPTNeoForCausalLM"),b_t.forEach(t),evr=r(j8e," (GPT Neo model)"),j8e.forEach(t),ovr=i(v4),rC=n(v4,"LI",{});var N8e=s(rC);n1e=n(N8e,"STRONG",{});var v_t=s(n1e);rvr=r(v_t,"gptj"),v_t.forEach(t),tvr=r(N8e," \u2014 "),BX=n(N8e,"A",{href:!0});var T_t=s(BX);avr=r(T_t,"FlaxGPTJForCausalLM"),T_t.forEach(t),nvr=r(N8e," (GPT-J model)"),N8e.forEach(t),svr=i(v4),tC=n(v4,"LI",{});var q8e=s(tC);s1e=n(q8e,"STRONG",{});var F_t=s(s1e);lvr=r(F_t,"xglm"),F_t.forEach(t),ivr=r(q8e," \u2014 "),xX=n(q8e,"A",{href:!0});var C_t=s(xX);dvr=r(C_t,"FlaxXGLMForCausalLM"),C_t.forEach(t),cvr=r(q8e," (XGLM model)"),q8e.forEach(t),v4.forEach(t),fvr=i(Ea),l1e=n(Ea,"P",{});var M_t=s(l1e);mvr=r(M_t,"Examples:"),M_t.forEach(t),gvr=i(Ea),m(oA.$$.fragment,Ea),Ea.forEach(t),si.forEach(t),Dxe=i(c),Qc=n(c,"H2",{class:!0});var QRe=s(Qc);aC=n(QRe,"A",{id:!0,class:!0,href:!0});var E_t=s(aC);i1e=n(E_t,"SPAN",{});var y_t=s(i1e);m(rA.$$.fragment,y_t),y_t.forEach(t),E_t.forEach(t),hvr=i(QRe),d1e=n(QRe,"SPAN",{});var w_t=s(d1e);pvr=r(w_t,"FlaxAutoModelForPreTraining"),w_t.forEach(t),QRe.forEach(t),jxe=i(c),Rr=n(c,"DIV",{class:!0});var ii=s(Rr);m(tA.$$.fragment,ii),_vr=i(ii),Hc=n(ii,"P",{});var bW=s(Hc);uvr=r(bW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),c1e=n(bW,"CODE",{});var A_t=s(c1e);bvr=r(A_t,"from_pretrained()"),A_t.forEach(t),vvr=r(bW,"class method or the "),f1e=n(bW,"CODE",{});var L_t=s(f1e);Tvr=r(L_t,"from_config()"),L_t.forEach(t),Fvr=r(bW,`class
method.`),bW.forEach(t),Cvr=i(ii),aA=n(ii,"P",{});var HRe=s(aA);Mvr=r(HRe,"This class cannot be instantiated directly using "),m1e=n(HRe,"CODE",{});var B_t=s(m1e);Evr=r(B_t,"__init__()"),B_t.forEach(t),yvr=r(HRe," (throws an error)."),HRe.forEach(t),wvr=i(ii),yt=n(ii,"DIV",{class:!0});var di=s(yt);m(nA.$$.fragment,di),Avr=i(di),g1e=n(di,"P",{});var x_t=s(g1e);Lvr=r(x_t,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),x_t.forEach(t),Bvr=i(di),Uc=n(di,"P",{});var vW=s(Uc);xvr=r(vW,`Note:
Loading a model from its configuration file does `),h1e=n(vW,"STRONG",{});var k_t=s(h1e);kvr=r(k_t,"not"),k_t.forEach(t),Rvr=r(vW,` load the model weights. It only affects the
model\u2019s configuration. Use `),p1e=n(vW,"CODE",{});var R_t=s(p1e);Svr=r(R_t,"from_pretrained()"),R_t.forEach(t),Pvr=r(vW,"to load the model weights."),vW.forEach(t),$vr=i(di),_1e=n(di,"P",{});var S_t=s(_1e);Ivr=r(S_t,"Examples:"),S_t.forEach(t),Dvr=i(di),m(sA.$$.fragment,di),di.forEach(t),jvr=i(ii),Lo=n(ii,"DIV",{class:!0});var ya=s(Lo);m(lA.$$.fragment,ya),Nvr=i(ya),u1e=n(ya,"P",{});var P_t=s(u1e);qvr=r(P_t,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),P_t.forEach(t),Gvr=i(ya),wn=n(ya,"P",{});var T4=s(wn);Ovr=r(T4,"The model class to instantiate is selected based on the "),b1e=n(T4,"CODE",{});var $_t=s(b1e);Xvr=r($_t,"model_type"),$_t.forEach(t),Vvr=r(T4,` property of the config object (either
passed as an argument or loaded from `),v1e=n(T4,"CODE",{});var I_t=s(v1e);zvr=r(I_t,"pretrained_model_name_or_path"),I_t.forEach(t),Wvr=r(T4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),T1e=n(T4,"CODE",{});var D_t=s(T1e);Qvr=r(D_t,"pretrained_model_name_or_path"),D_t.forEach(t),Hvr=r(T4,":"),T4.forEach(t),Uvr=i(ya),ce=n(ya,"UL",{});var me=s(ce);nC=n(me,"LI",{});var G8e=s(nC);F1e=n(G8e,"STRONG",{});var j_t=s(F1e);Jvr=r(j_t,"albert"),j_t.forEach(t),Yvr=r(G8e," \u2014 "),kX=n(G8e,"A",{href:!0});var N_t=s(kX);Kvr=r(N_t,"FlaxAlbertForPreTraining"),N_t.forEach(t),Zvr=r(G8e," (ALBERT model)"),G8e.forEach(t),eTr=i(me),sC=n(me,"LI",{});var O8e=s(sC);C1e=n(O8e,"STRONG",{});var q_t=s(C1e);oTr=r(q_t,"bart"),q_t.forEach(t),rTr=r(O8e," \u2014 "),RX=n(O8e,"A",{href:!0});var G_t=s(RX);tTr=r(G_t,"FlaxBartForConditionalGeneration"),G_t.forEach(t),aTr=r(O8e," (BART model)"),O8e.forEach(t),nTr=i(me),lC=n(me,"LI",{});var X8e=s(lC);M1e=n(X8e,"STRONG",{});var O_t=s(M1e);sTr=r(O_t,"bert"),O_t.forEach(t),lTr=r(X8e," \u2014 "),SX=n(X8e,"A",{href:!0});var X_t=s(SX);iTr=r(X_t,"FlaxBertForPreTraining"),X_t.forEach(t),dTr=r(X8e," (BERT model)"),X8e.forEach(t),cTr=i(me),iC=n(me,"LI",{});var V8e=s(iC);E1e=n(V8e,"STRONG",{});var V_t=s(E1e);fTr=r(V_t,"big_bird"),V_t.forEach(t),mTr=r(V8e," \u2014 "),PX=n(V8e,"A",{href:!0});var z_t=s(PX);gTr=r(z_t,"FlaxBigBirdForPreTraining"),z_t.forEach(t),hTr=r(V8e," (BigBird model)"),V8e.forEach(t),pTr=i(me),dC=n(me,"LI",{});var z8e=s(dC);y1e=n(z8e,"STRONG",{});var W_t=s(y1e);_Tr=r(W_t,"electra"),W_t.forEach(t),uTr=r(z8e," \u2014 "),$X=n(z8e,"A",{href:!0});var Q_t=s($X);bTr=r(Q_t,"FlaxElectraForPreTraining"),Q_t.forEach(t),vTr=r(z8e," (ELECTRA model)"),z8e.forEach(t),TTr=i(me),cC=n(me,"LI",{});var W8e=s(cC);w1e=n(W8e,"STRONG",{});var H_t=s(w1e);FTr=r(H_t,"mbart"),H_t.forEach(t),CTr=r(W8e," \u2014 "),IX=n(W8e,"A",{href:!0});var U_t=s(IX);MTr=r(U_t,"FlaxMBartForConditionalGeneration"),U_t.forEach(t),ETr=r(W8e," (mBART model)"),W8e.forEach(t),yTr=i(me),fC=n(me,"LI",{});var Q8e=s(fC);A1e=n(Q8e,"STRONG",{});var J_t=s(A1e);wTr=r(J_t,"mt5"),J_t.forEach(t),ATr=r(Q8e," \u2014 "),DX=n(Q8e,"A",{href:!0});var Y_t=s(DX);LTr=r(Y_t,"FlaxMT5ForConditionalGeneration"),Y_t.forEach(t),BTr=r(Q8e," (mT5 model)"),Q8e.forEach(t),xTr=i(me),mC=n(me,"LI",{});var H8e=s(mC);L1e=n(H8e,"STRONG",{});var K_t=s(L1e);kTr=r(K_t,"roberta"),K_t.forEach(t),RTr=r(H8e," \u2014 "),jX=n(H8e,"A",{href:!0});var Z_t=s(jX);STr=r(Z_t,"FlaxRobertaForMaskedLM"),Z_t.forEach(t),PTr=r(H8e," (RoBERTa model)"),H8e.forEach(t),$Tr=i(me),gC=n(me,"LI",{});var U8e=s(gC);B1e=n(U8e,"STRONG",{});var eut=s(B1e);ITr=r(eut,"roformer"),eut.forEach(t),DTr=r(U8e," \u2014 "),NX=n(U8e,"A",{href:!0});var out=s(NX);jTr=r(out,"FlaxRoFormerForMaskedLM"),out.forEach(t),NTr=r(U8e," (RoFormer model)"),U8e.forEach(t),qTr=i(me),hC=n(me,"LI",{});var J8e=s(hC);x1e=n(J8e,"STRONG",{});var rut=s(x1e);GTr=r(rut,"t5"),rut.forEach(t),OTr=r(J8e," \u2014 "),qX=n(J8e,"A",{href:!0});var tut=s(qX);XTr=r(tut,"FlaxT5ForConditionalGeneration"),tut.forEach(t),VTr=r(J8e," (T5 model)"),J8e.forEach(t),zTr=i(me),pC=n(me,"LI",{});var Y8e=s(pC);k1e=n(Y8e,"STRONG",{});var aut=s(k1e);WTr=r(aut,"wav2vec2"),aut.forEach(t),QTr=r(Y8e," \u2014 "),GX=n(Y8e,"A",{href:!0});var nut=s(GX);HTr=r(nut,"FlaxWav2Vec2ForPreTraining"),nut.forEach(t),UTr=r(Y8e," (Wav2Vec2 model)"),Y8e.forEach(t),JTr=i(me),_C=n(me,"LI",{});var K8e=s(_C);R1e=n(K8e,"STRONG",{});var sut=s(R1e);YTr=r(sut,"xlm-roberta"),sut.forEach(t),KTr=r(K8e," \u2014 "),OX=n(K8e,"A",{href:!0});var lut=s(OX);ZTr=r(lut,"FlaxXLMRobertaForMaskedLM"),lut.forEach(t),eFr=r(K8e," (XLM-RoBERTa model)"),K8e.forEach(t),me.forEach(t),oFr=i(ya),S1e=n(ya,"P",{});var iut=s(S1e);rFr=r(iut,"Examples:"),iut.forEach(t),tFr=i(ya),m(iA.$$.fragment,ya),ya.forEach(t),ii.forEach(t),Nxe=i(c),Jc=n(c,"H2",{class:!0});var URe=s(Jc);uC=n(URe,"A",{id:!0,class:!0,href:!0});var dut=s(uC);P1e=n(dut,"SPAN",{});var cut=s(P1e);m(dA.$$.fragment,cut),cut.forEach(t),dut.forEach(t),aFr=i(URe),$1e=n(URe,"SPAN",{});var fut=s($1e);nFr=r(fut,"FlaxAutoModelForMaskedLM"),fut.forEach(t),URe.forEach(t),qxe=i(c),Sr=n(c,"DIV",{class:!0});var ci=s(Sr);m(cA.$$.fragment,ci),sFr=i(ci),Yc=n(ci,"P",{});var TW=s(Yc);lFr=r(TW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),I1e=n(TW,"CODE",{});var mut=s(I1e);iFr=r(mut,"from_pretrained()"),mut.forEach(t),dFr=r(TW,"class method or the "),D1e=n(TW,"CODE",{});var gut=s(D1e);cFr=r(gut,"from_config()"),gut.forEach(t),fFr=r(TW,`class
method.`),TW.forEach(t),mFr=i(ci),fA=n(ci,"P",{});var JRe=s(fA);gFr=r(JRe,"This class cannot be instantiated directly using "),j1e=n(JRe,"CODE",{});var hut=s(j1e);hFr=r(hut,"__init__()"),hut.forEach(t),pFr=r(JRe," (throws an error)."),JRe.forEach(t),_Fr=i(ci),wt=n(ci,"DIV",{class:!0});var fi=s(wt);m(mA.$$.fragment,fi),uFr=i(fi),N1e=n(fi,"P",{});var put=s(N1e);bFr=r(put,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),put.forEach(t),vFr=i(fi),Kc=n(fi,"P",{});var FW=s(Kc);TFr=r(FW,`Note:
Loading a model from its configuration file does `),q1e=n(FW,"STRONG",{});var _ut=s(q1e);FFr=r(_ut,"not"),_ut.forEach(t),CFr=r(FW,` load the model weights. It only affects the
model\u2019s configuration. Use `),G1e=n(FW,"CODE",{});var uut=s(G1e);MFr=r(uut,"from_pretrained()"),uut.forEach(t),EFr=r(FW,"to load the model weights."),FW.forEach(t),yFr=i(fi),O1e=n(fi,"P",{});var but=s(O1e);wFr=r(but,"Examples:"),but.forEach(t),AFr=i(fi),m(gA.$$.fragment,fi),fi.forEach(t),LFr=i(ci),Bo=n(ci,"DIV",{class:!0});var wa=s(Bo);m(hA.$$.fragment,wa),BFr=i(wa),X1e=n(wa,"P",{});var vut=s(X1e);xFr=r(vut,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),vut.forEach(t),kFr=i(wa),An=n(wa,"P",{});var F4=s(An);RFr=r(F4,"The model class to instantiate is selected based on the "),V1e=n(F4,"CODE",{});var Tut=s(V1e);SFr=r(Tut,"model_type"),Tut.forEach(t),PFr=r(F4,` property of the config object (either
passed as an argument or loaded from `),z1e=n(F4,"CODE",{});var Fut=s(z1e);$Fr=r(Fut,"pretrained_model_name_or_path"),Fut.forEach(t),IFr=r(F4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),W1e=n(F4,"CODE",{});var Cut=s(W1e);DFr=r(Cut,"pretrained_model_name_or_path"),Cut.forEach(t),jFr=r(F4,":"),F4.forEach(t),NFr=i(wa),ue=n(wa,"UL",{});var ye=s(ue);bC=n(ye,"LI",{});var Z8e=s(bC);Q1e=n(Z8e,"STRONG",{});var Mut=s(Q1e);qFr=r(Mut,"albert"),Mut.forEach(t),GFr=r(Z8e," \u2014 "),XX=n(Z8e,"A",{href:!0});var Eut=s(XX);OFr=r(Eut,"FlaxAlbertForMaskedLM"),Eut.forEach(t),XFr=r(Z8e," (ALBERT model)"),Z8e.forEach(t),VFr=i(ye),vC=n(ye,"LI",{});var e7e=s(vC);H1e=n(e7e,"STRONG",{});var yut=s(H1e);zFr=r(yut,"bart"),yut.forEach(t),WFr=r(e7e," \u2014 "),VX=n(e7e,"A",{href:!0});var wut=s(VX);QFr=r(wut,"FlaxBartForConditionalGeneration"),wut.forEach(t),HFr=r(e7e," (BART model)"),e7e.forEach(t),UFr=i(ye),TC=n(ye,"LI",{});var o7e=s(TC);U1e=n(o7e,"STRONG",{});var Aut=s(U1e);JFr=r(Aut,"bert"),Aut.forEach(t),YFr=r(o7e," \u2014 "),zX=n(o7e,"A",{href:!0});var Lut=s(zX);KFr=r(Lut,"FlaxBertForMaskedLM"),Lut.forEach(t),ZFr=r(o7e," (BERT model)"),o7e.forEach(t),e9r=i(ye),FC=n(ye,"LI",{});var r7e=s(FC);J1e=n(r7e,"STRONG",{});var But=s(J1e);o9r=r(But,"big_bird"),But.forEach(t),r9r=r(r7e," \u2014 "),WX=n(r7e,"A",{href:!0});var xut=s(WX);t9r=r(xut,"FlaxBigBirdForMaskedLM"),xut.forEach(t),a9r=r(r7e," (BigBird model)"),r7e.forEach(t),n9r=i(ye),CC=n(ye,"LI",{});var t7e=s(CC);Y1e=n(t7e,"STRONG",{});var kut=s(Y1e);s9r=r(kut,"distilbert"),kut.forEach(t),l9r=r(t7e," \u2014 "),QX=n(t7e,"A",{href:!0});var Rut=s(QX);i9r=r(Rut,"FlaxDistilBertForMaskedLM"),Rut.forEach(t),d9r=r(t7e," (DistilBERT model)"),t7e.forEach(t),c9r=i(ye),MC=n(ye,"LI",{});var a7e=s(MC);K1e=n(a7e,"STRONG",{});var Sut=s(K1e);f9r=r(Sut,"electra"),Sut.forEach(t),m9r=r(a7e," \u2014 "),HX=n(a7e,"A",{href:!0});var Put=s(HX);g9r=r(Put,"FlaxElectraForMaskedLM"),Put.forEach(t),h9r=r(a7e," (ELECTRA model)"),a7e.forEach(t),p9r=i(ye),EC=n(ye,"LI",{});var n7e=s(EC);Z1e=n(n7e,"STRONG",{});var $ut=s(Z1e);_9r=r($ut,"mbart"),$ut.forEach(t),u9r=r(n7e," \u2014 "),UX=n(n7e,"A",{href:!0});var Iut=s(UX);b9r=r(Iut,"FlaxMBartForConditionalGeneration"),Iut.forEach(t),v9r=r(n7e," (mBART model)"),n7e.forEach(t),T9r=i(ye),yC=n(ye,"LI",{});var s7e=s(yC);ebe=n(s7e,"STRONG",{});var Dut=s(ebe);F9r=r(Dut,"roberta"),Dut.forEach(t),C9r=r(s7e," \u2014 "),JX=n(s7e,"A",{href:!0});var jut=s(JX);M9r=r(jut,"FlaxRobertaForMaskedLM"),jut.forEach(t),E9r=r(s7e," (RoBERTa model)"),s7e.forEach(t),y9r=i(ye),wC=n(ye,"LI",{});var l7e=s(wC);obe=n(l7e,"STRONG",{});var Nut=s(obe);w9r=r(Nut,"roformer"),Nut.forEach(t),A9r=r(l7e," \u2014 "),YX=n(l7e,"A",{href:!0});var qut=s(YX);L9r=r(qut,"FlaxRoFormerForMaskedLM"),qut.forEach(t),B9r=r(l7e," (RoFormer model)"),l7e.forEach(t),x9r=i(ye),AC=n(ye,"LI",{});var i7e=s(AC);rbe=n(i7e,"STRONG",{});var Gut=s(rbe);k9r=r(Gut,"xlm-roberta"),Gut.forEach(t),R9r=r(i7e," \u2014 "),KX=n(i7e,"A",{href:!0});var Out=s(KX);S9r=r(Out,"FlaxXLMRobertaForMaskedLM"),Out.forEach(t),P9r=r(i7e," (XLM-RoBERTa model)"),i7e.forEach(t),ye.forEach(t),$9r=i(wa),tbe=n(wa,"P",{});var Xut=s(tbe);I9r=r(Xut,"Examples:"),Xut.forEach(t),D9r=i(wa),m(pA.$$.fragment,wa),wa.forEach(t),ci.forEach(t),Gxe=i(c),Zc=n(c,"H2",{class:!0});var YRe=s(Zc);LC=n(YRe,"A",{id:!0,class:!0,href:!0});var Vut=s(LC);abe=n(Vut,"SPAN",{});var zut=s(abe);m(_A.$$.fragment,zut),zut.forEach(t),Vut.forEach(t),j9r=i(YRe),nbe=n(YRe,"SPAN",{});var Wut=s(nbe);N9r=r(Wut,"FlaxAutoModelForSeq2SeqLM"),Wut.forEach(t),YRe.forEach(t),Oxe=i(c),Pr=n(c,"DIV",{class:!0});var mi=s(Pr);m(uA.$$.fragment,mi),q9r=i(mi),ef=n(mi,"P",{});var CW=s(ef);G9r=r(CW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),sbe=n(CW,"CODE",{});var Qut=s(sbe);O9r=r(Qut,"from_pretrained()"),Qut.forEach(t),X9r=r(CW,"class method or the "),lbe=n(CW,"CODE",{});var Hut=s(lbe);V9r=r(Hut,"from_config()"),Hut.forEach(t),z9r=r(CW,`class
method.`),CW.forEach(t),W9r=i(mi),bA=n(mi,"P",{});var KRe=s(bA);Q9r=r(KRe,"This class cannot be instantiated directly using "),ibe=n(KRe,"CODE",{});var Uut=s(ibe);H9r=r(Uut,"__init__()"),Uut.forEach(t),U9r=r(KRe," (throws an error)."),KRe.forEach(t),J9r=i(mi),At=n(mi,"DIV",{class:!0});var gi=s(At);m(vA.$$.fragment,gi),Y9r=i(gi),dbe=n(gi,"P",{});var Jut=s(dbe);K9r=r(Jut,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Jut.forEach(t),Z9r=i(gi),of=n(gi,"P",{});var MW=s(of);eCr=r(MW,`Note:
Loading a model from its configuration file does `),cbe=n(MW,"STRONG",{});var Yut=s(cbe);oCr=r(Yut,"not"),Yut.forEach(t),rCr=r(MW,` load the model weights. It only affects the
model\u2019s configuration. Use `),fbe=n(MW,"CODE",{});var Kut=s(fbe);tCr=r(Kut,"from_pretrained()"),Kut.forEach(t),aCr=r(MW,"to load the model weights."),MW.forEach(t),nCr=i(gi),mbe=n(gi,"P",{});var Zut=s(mbe);sCr=r(Zut,"Examples:"),Zut.forEach(t),lCr=i(gi),m(TA.$$.fragment,gi),gi.forEach(t),iCr=i(mi),xo=n(mi,"DIV",{class:!0});var Aa=s(xo);m(FA.$$.fragment,Aa),dCr=i(Aa),gbe=n(Aa,"P",{});var e0t=s(gbe);cCr=r(e0t,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),e0t.forEach(t),fCr=i(Aa),Ln=n(Aa,"P",{});var C4=s(Ln);mCr=r(C4,"The model class to instantiate is selected based on the "),hbe=n(C4,"CODE",{});var o0t=s(hbe);gCr=r(o0t,"model_type"),o0t.forEach(t),hCr=r(C4,` property of the config object (either
passed as an argument or loaded from `),pbe=n(C4,"CODE",{});var r0t=s(pbe);pCr=r(r0t,"pretrained_model_name_or_path"),r0t.forEach(t),_Cr=r(C4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_be=n(C4,"CODE",{});var t0t=s(_be);uCr=r(t0t,"pretrained_model_name_or_path"),t0t.forEach(t),bCr=r(C4,":"),C4.forEach(t),vCr=i(Aa),Ce=n(Aa,"UL",{});var so=s(Ce);BC=n(so,"LI",{});var d7e=s(BC);ube=n(d7e,"STRONG",{});var a0t=s(ube);TCr=r(a0t,"bart"),a0t.forEach(t),FCr=r(d7e," \u2014 "),ZX=n(d7e,"A",{href:!0});var n0t=s(ZX);CCr=r(n0t,"FlaxBartForConditionalGeneration"),n0t.forEach(t),MCr=r(d7e," (BART model)"),d7e.forEach(t),ECr=i(so),xC=n(so,"LI",{});var c7e=s(xC);bbe=n(c7e,"STRONG",{});var s0t=s(bbe);yCr=r(s0t,"blenderbot"),s0t.forEach(t),wCr=r(c7e," \u2014 "),eV=n(c7e,"A",{href:!0});var l0t=s(eV);ACr=r(l0t,"FlaxBlenderbotForConditionalGeneration"),l0t.forEach(t),LCr=r(c7e," (Blenderbot model)"),c7e.forEach(t),BCr=i(so),kC=n(so,"LI",{});var f7e=s(kC);vbe=n(f7e,"STRONG",{});var i0t=s(vbe);xCr=r(i0t,"blenderbot-small"),i0t.forEach(t),kCr=r(f7e," \u2014 "),oV=n(f7e,"A",{href:!0});var d0t=s(oV);RCr=r(d0t,"FlaxBlenderbotSmallForConditionalGeneration"),d0t.forEach(t),SCr=r(f7e," (BlenderbotSmall model)"),f7e.forEach(t),PCr=i(so),RC=n(so,"LI",{});var m7e=s(RC);Tbe=n(m7e,"STRONG",{});var c0t=s(Tbe);$Cr=r(c0t,"encoder-decoder"),c0t.forEach(t),ICr=r(m7e," \u2014 "),rV=n(m7e,"A",{href:!0});var f0t=s(rV);DCr=r(f0t,"FlaxEncoderDecoderModel"),f0t.forEach(t),jCr=r(m7e," (Encoder decoder model)"),m7e.forEach(t),NCr=i(so),SC=n(so,"LI",{});var g7e=s(SC);Fbe=n(g7e,"STRONG",{});var m0t=s(Fbe);qCr=r(m0t,"marian"),m0t.forEach(t),GCr=r(g7e," \u2014 "),tV=n(g7e,"A",{href:!0});var g0t=s(tV);OCr=r(g0t,"FlaxMarianMTModel"),g0t.forEach(t),XCr=r(g7e," (Marian model)"),g7e.forEach(t),VCr=i(so),PC=n(so,"LI",{});var h7e=s(PC);Cbe=n(h7e,"STRONG",{});var h0t=s(Cbe);zCr=r(h0t,"mbart"),h0t.forEach(t),WCr=r(h7e," \u2014 "),aV=n(h7e,"A",{href:!0});var p0t=s(aV);QCr=r(p0t,"FlaxMBartForConditionalGeneration"),p0t.forEach(t),HCr=r(h7e," (mBART model)"),h7e.forEach(t),UCr=i(so),$C=n(so,"LI",{});var p7e=s($C);Mbe=n(p7e,"STRONG",{});var _0t=s(Mbe);JCr=r(_0t,"mt5"),_0t.forEach(t),YCr=r(p7e," \u2014 "),nV=n(p7e,"A",{href:!0});var u0t=s(nV);KCr=r(u0t,"FlaxMT5ForConditionalGeneration"),u0t.forEach(t),ZCr=r(p7e," (mT5 model)"),p7e.forEach(t),eMr=i(so),IC=n(so,"LI",{});var _7e=s(IC);Ebe=n(_7e,"STRONG",{});var b0t=s(Ebe);oMr=r(b0t,"pegasus"),b0t.forEach(t),rMr=r(_7e," \u2014 "),sV=n(_7e,"A",{href:!0});var v0t=s(sV);tMr=r(v0t,"FlaxPegasusForConditionalGeneration"),v0t.forEach(t),aMr=r(_7e," (Pegasus model)"),_7e.forEach(t),nMr=i(so),DC=n(so,"LI",{});var u7e=s(DC);ybe=n(u7e,"STRONG",{});var T0t=s(ybe);sMr=r(T0t,"t5"),T0t.forEach(t),lMr=r(u7e," \u2014 "),lV=n(u7e,"A",{href:!0});var F0t=s(lV);iMr=r(F0t,"FlaxT5ForConditionalGeneration"),F0t.forEach(t),dMr=r(u7e," (T5 model)"),u7e.forEach(t),so.forEach(t),cMr=i(Aa),wbe=n(Aa,"P",{});var C0t=s(wbe);fMr=r(C0t,"Examples:"),C0t.forEach(t),mMr=i(Aa),m(CA.$$.fragment,Aa),Aa.forEach(t),mi.forEach(t),Xxe=i(c),rf=n(c,"H2",{class:!0});var ZRe=s(rf);jC=n(ZRe,"A",{id:!0,class:!0,href:!0});var M0t=s(jC);Abe=n(M0t,"SPAN",{});var E0t=s(Abe);m(MA.$$.fragment,E0t),E0t.forEach(t),M0t.forEach(t),gMr=i(ZRe),Lbe=n(ZRe,"SPAN",{});var y0t=s(Lbe);hMr=r(y0t,"FlaxAutoModelForSequenceClassification"),y0t.forEach(t),ZRe.forEach(t),Vxe=i(c),$r=n(c,"DIV",{class:!0});var hi=s($r);m(EA.$$.fragment,hi),pMr=i(hi),tf=n(hi,"P",{});var EW=s(tf);_Mr=r(EW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Bbe=n(EW,"CODE",{});var w0t=s(Bbe);uMr=r(w0t,"from_pretrained()"),w0t.forEach(t),bMr=r(EW,"class method or the "),xbe=n(EW,"CODE",{});var A0t=s(xbe);vMr=r(A0t,"from_config()"),A0t.forEach(t),TMr=r(EW,`class
method.`),EW.forEach(t),FMr=i(hi),yA=n(hi,"P",{});var eSe=s(yA);CMr=r(eSe,"This class cannot be instantiated directly using "),kbe=n(eSe,"CODE",{});var L0t=s(kbe);MMr=r(L0t,"__init__()"),L0t.forEach(t),EMr=r(eSe," (throws an error)."),eSe.forEach(t),yMr=i(hi),Lt=n(hi,"DIV",{class:!0});var pi=s(Lt);m(wA.$$.fragment,pi),wMr=i(pi),Rbe=n(pi,"P",{});var B0t=s(Rbe);AMr=r(B0t,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),B0t.forEach(t),LMr=i(pi),af=n(pi,"P",{});var yW=s(af);BMr=r(yW,`Note:
Loading a model from its configuration file does `),Sbe=n(yW,"STRONG",{});var x0t=s(Sbe);xMr=r(x0t,"not"),x0t.forEach(t),kMr=r(yW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pbe=n(yW,"CODE",{});var k0t=s(Pbe);RMr=r(k0t,"from_pretrained()"),k0t.forEach(t),SMr=r(yW,"to load the model weights."),yW.forEach(t),PMr=i(pi),$be=n(pi,"P",{});var R0t=s($be);$Mr=r(R0t,"Examples:"),R0t.forEach(t),IMr=i(pi),m(AA.$$.fragment,pi),pi.forEach(t),DMr=i(hi),ko=n(hi,"DIV",{class:!0});var La=s(ko);m(LA.$$.fragment,La),jMr=i(La),Ibe=n(La,"P",{});var S0t=s(Ibe);NMr=r(S0t,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),S0t.forEach(t),qMr=i(La),Bn=n(La,"P",{});var M4=s(Bn);GMr=r(M4,"The model class to instantiate is selected based on the "),Dbe=n(M4,"CODE",{});var P0t=s(Dbe);OMr=r(P0t,"model_type"),P0t.forEach(t),XMr=r(M4,` property of the config object (either
passed as an argument or loaded from `),jbe=n(M4,"CODE",{});var $0t=s(jbe);VMr=r($0t,"pretrained_model_name_or_path"),$0t.forEach(t),zMr=r(M4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nbe=n(M4,"CODE",{});var I0t=s(Nbe);WMr=r(I0t,"pretrained_model_name_or_path"),I0t.forEach(t),QMr=r(M4,":"),M4.forEach(t),HMr=i(La),be=n(La,"UL",{});var we=s(be);NC=n(we,"LI",{});var b7e=s(NC);qbe=n(b7e,"STRONG",{});var D0t=s(qbe);UMr=r(D0t,"albert"),D0t.forEach(t),JMr=r(b7e," \u2014 "),iV=n(b7e,"A",{href:!0});var j0t=s(iV);YMr=r(j0t,"FlaxAlbertForSequenceClassification"),j0t.forEach(t),KMr=r(b7e," (ALBERT model)"),b7e.forEach(t),ZMr=i(we),qC=n(we,"LI",{});var v7e=s(qC);Gbe=n(v7e,"STRONG",{});var N0t=s(Gbe);e4r=r(N0t,"bart"),N0t.forEach(t),o4r=r(v7e," \u2014 "),dV=n(v7e,"A",{href:!0});var q0t=s(dV);r4r=r(q0t,"FlaxBartForSequenceClassification"),q0t.forEach(t),t4r=r(v7e," (BART model)"),v7e.forEach(t),a4r=i(we),GC=n(we,"LI",{});var T7e=s(GC);Obe=n(T7e,"STRONG",{});var G0t=s(Obe);n4r=r(G0t,"bert"),G0t.forEach(t),s4r=r(T7e," \u2014 "),cV=n(T7e,"A",{href:!0});var O0t=s(cV);l4r=r(O0t,"FlaxBertForSequenceClassification"),O0t.forEach(t),i4r=r(T7e," (BERT model)"),T7e.forEach(t),d4r=i(we),OC=n(we,"LI",{});var F7e=s(OC);Xbe=n(F7e,"STRONG",{});var X0t=s(Xbe);c4r=r(X0t,"big_bird"),X0t.forEach(t),f4r=r(F7e," \u2014 "),fV=n(F7e,"A",{href:!0});var V0t=s(fV);m4r=r(V0t,"FlaxBigBirdForSequenceClassification"),V0t.forEach(t),g4r=r(F7e," (BigBird model)"),F7e.forEach(t),h4r=i(we),XC=n(we,"LI",{});var C7e=s(XC);Vbe=n(C7e,"STRONG",{});var z0t=s(Vbe);p4r=r(z0t,"distilbert"),z0t.forEach(t),_4r=r(C7e," \u2014 "),mV=n(C7e,"A",{href:!0});var W0t=s(mV);u4r=r(W0t,"FlaxDistilBertForSequenceClassification"),W0t.forEach(t),b4r=r(C7e," (DistilBERT model)"),C7e.forEach(t),v4r=i(we),VC=n(we,"LI",{});var M7e=s(VC);zbe=n(M7e,"STRONG",{});var Q0t=s(zbe);T4r=r(Q0t,"electra"),Q0t.forEach(t),F4r=r(M7e," \u2014 "),gV=n(M7e,"A",{href:!0});var H0t=s(gV);C4r=r(H0t,"FlaxElectraForSequenceClassification"),H0t.forEach(t),M4r=r(M7e," (ELECTRA model)"),M7e.forEach(t),E4r=i(we),zC=n(we,"LI",{});var E7e=s(zC);Wbe=n(E7e,"STRONG",{});var U0t=s(Wbe);y4r=r(U0t,"mbart"),U0t.forEach(t),w4r=r(E7e," \u2014 "),hV=n(E7e,"A",{href:!0});var J0t=s(hV);A4r=r(J0t,"FlaxMBartForSequenceClassification"),J0t.forEach(t),L4r=r(E7e," (mBART model)"),E7e.forEach(t),B4r=i(we),WC=n(we,"LI",{});var y7e=s(WC);Qbe=n(y7e,"STRONG",{});var Y0t=s(Qbe);x4r=r(Y0t,"roberta"),Y0t.forEach(t),k4r=r(y7e," \u2014 "),pV=n(y7e,"A",{href:!0});var K0t=s(pV);R4r=r(K0t,"FlaxRobertaForSequenceClassification"),K0t.forEach(t),S4r=r(y7e," (RoBERTa model)"),y7e.forEach(t),P4r=i(we),QC=n(we,"LI",{});var w7e=s(QC);Hbe=n(w7e,"STRONG",{});var Z0t=s(Hbe);$4r=r(Z0t,"roformer"),Z0t.forEach(t),I4r=r(w7e," \u2014 "),_V=n(w7e,"A",{href:!0});var e1t=s(_V);D4r=r(e1t,"FlaxRoFormerForSequenceClassification"),e1t.forEach(t),j4r=r(w7e," (RoFormer model)"),w7e.forEach(t),N4r=i(we),HC=n(we,"LI",{});var A7e=s(HC);Ube=n(A7e,"STRONG",{});var o1t=s(Ube);q4r=r(o1t,"xlm-roberta"),o1t.forEach(t),G4r=r(A7e," \u2014 "),uV=n(A7e,"A",{href:!0});var r1t=s(uV);O4r=r(r1t,"FlaxXLMRobertaForSequenceClassification"),r1t.forEach(t),X4r=r(A7e," (XLM-RoBERTa model)"),A7e.forEach(t),we.forEach(t),V4r=i(La),Jbe=n(La,"P",{});var t1t=s(Jbe);z4r=r(t1t,"Examples:"),t1t.forEach(t),W4r=i(La),m(BA.$$.fragment,La),La.forEach(t),hi.forEach(t),zxe=i(c),nf=n(c,"H2",{class:!0});var oSe=s(nf);UC=n(oSe,"A",{id:!0,class:!0,href:!0});var a1t=s(UC);Ybe=n(a1t,"SPAN",{});var n1t=s(Ybe);m(xA.$$.fragment,n1t),n1t.forEach(t),a1t.forEach(t),Q4r=i(oSe),Kbe=n(oSe,"SPAN",{});var s1t=s(Kbe);H4r=r(s1t,"FlaxAutoModelForQuestionAnswering"),s1t.forEach(t),oSe.forEach(t),Wxe=i(c),Ir=n(c,"DIV",{class:!0});var _i=s(Ir);m(kA.$$.fragment,_i),U4r=i(_i),sf=n(_i,"P",{});var wW=s(sf);J4r=r(wW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Zbe=n(wW,"CODE",{});var l1t=s(Zbe);Y4r=r(l1t,"from_pretrained()"),l1t.forEach(t),K4r=r(wW,"class method or the "),e5e=n(wW,"CODE",{});var i1t=s(e5e);Z4r=r(i1t,"from_config()"),i1t.forEach(t),eEr=r(wW,`class
method.`),wW.forEach(t),oEr=i(_i),RA=n(_i,"P",{});var rSe=s(RA);rEr=r(rSe,"This class cannot be instantiated directly using "),o5e=n(rSe,"CODE",{});var d1t=s(o5e);tEr=r(d1t,"__init__()"),d1t.forEach(t),aEr=r(rSe," (throws an error)."),rSe.forEach(t),nEr=i(_i),Bt=n(_i,"DIV",{class:!0});var ui=s(Bt);m(SA.$$.fragment,ui),sEr=i(ui),r5e=n(ui,"P",{});var c1t=s(r5e);lEr=r(c1t,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),c1t.forEach(t),iEr=i(ui),lf=n(ui,"P",{});var AW=s(lf);dEr=r(AW,`Note:
Loading a model from its configuration file does `),t5e=n(AW,"STRONG",{});var f1t=s(t5e);cEr=r(f1t,"not"),f1t.forEach(t),fEr=r(AW,` load the model weights. It only affects the
model\u2019s configuration. Use `),a5e=n(AW,"CODE",{});var m1t=s(a5e);mEr=r(m1t,"from_pretrained()"),m1t.forEach(t),gEr=r(AW,"to load the model weights."),AW.forEach(t),hEr=i(ui),n5e=n(ui,"P",{});var g1t=s(n5e);pEr=r(g1t,"Examples:"),g1t.forEach(t),_Er=i(ui),m(PA.$$.fragment,ui),ui.forEach(t),uEr=i(_i),Ro=n(_i,"DIV",{class:!0});var Ba=s(Ro);m($A.$$.fragment,Ba),bEr=i(Ba),s5e=n(Ba,"P",{});var h1t=s(s5e);vEr=r(h1t,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),h1t.forEach(t),TEr=i(Ba),xn=n(Ba,"P",{});var E4=s(xn);FEr=r(E4,"The model class to instantiate is selected based on the "),l5e=n(E4,"CODE",{});var p1t=s(l5e);CEr=r(p1t,"model_type"),p1t.forEach(t),MEr=r(E4,` property of the config object (either
passed as an argument or loaded from `),i5e=n(E4,"CODE",{});var _1t=s(i5e);EEr=r(_1t,"pretrained_model_name_or_path"),_1t.forEach(t),yEr=r(E4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),d5e=n(E4,"CODE",{});var u1t=s(d5e);wEr=r(u1t,"pretrained_model_name_or_path"),u1t.forEach(t),AEr=r(E4,":"),E4.forEach(t),LEr=i(Ba),ve=n(Ba,"UL",{});var Ae=s(ve);JC=n(Ae,"LI",{});var L7e=s(JC);c5e=n(L7e,"STRONG",{});var b1t=s(c5e);BEr=r(b1t,"albert"),b1t.forEach(t),xEr=r(L7e," \u2014 "),bV=n(L7e,"A",{href:!0});var v1t=s(bV);kEr=r(v1t,"FlaxAlbertForQuestionAnswering"),v1t.forEach(t),REr=r(L7e," (ALBERT model)"),L7e.forEach(t),SEr=i(Ae),YC=n(Ae,"LI",{});var B7e=s(YC);f5e=n(B7e,"STRONG",{});var T1t=s(f5e);PEr=r(T1t,"bart"),T1t.forEach(t),$Er=r(B7e," \u2014 "),vV=n(B7e,"A",{href:!0});var F1t=s(vV);IEr=r(F1t,"FlaxBartForQuestionAnswering"),F1t.forEach(t),DEr=r(B7e," (BART model)"),B7e.forEach(t),jEr=i(Ae),KC=n(Ae,"LI",{});var x7e=s(KC);m5e=n(x7e,"STRONG",{});var C1t=s(m5e);NEr=r(C1t,"bert"),C1t.forEach(t),qEr=r(x7e," \u2014 "),TV=n(x7e,"A",{href:!0});var M1t=s(TV);GEr=r(M1t,"FlaxBertForQuestionAnswering"),M1t.forEach(t),OEr=r(x7e," (BERT model)"),x7e.forEach(t),XEr=i(Ae),ZC=n(Ae,"LI",{});var k7e=s(ZC);g5e=n(k7e,"STRONG",{});var E1t=s(g5e);VEr=r(E1t,"big_bird"),E1t.forEach(t),zEr=r(k7e," \u2014 "),FV=n(k7e,"A",{href:!0});var y1t=s(FV);WEr=r(y1t,"FlaxBigBirdForQuestionAnswering"),y1t.forEach(t),QEr=r(k7e," (BigBird model)"),k7e.forEach(t),HEr=i(Ae),eM=n(Ae,"LI",{});var R7e=s(eM);h5e=n(R7e,"STRONG",{});var w1t=s(h5e);UEr=r(w1t,"distilbert"),w1t.forEach(t),JEr=r(R7e," \u2014 "),CV=n(R7e,"A",{href:!0});var A1t=s(CV);YEr=r(A1t,"FlaxDistilBertForQuestionAnswering"),A1t.forEach(t),KEr=r(R7e," (DistilBERT model)"),R7e.forEach(t),ZEr=i(Ae),oM=n(Ae,"LI",{});var S7e=s(oM);p5e=n(S7e,"STRONG",{});var L1t=s(p5e);e3r=r(L1t,"electra"),L1t.forEach(t),o3r=r(S7e," \u2014 "),MV=n(S7e,"A",{href:!0});var B1t=s(MV);r3r=r(B1t,"FlaxElectraForQuestionAnswering"),B1t.forEach(t),t3r=r(S7e," (ELECTRA model)"),S7e.forEach(t),a3r=i(Ae),rM=n(Ae,"LI",{});var P7e=s(rM);_5e=n(P7e,"STRONG",{});var x1t=s(_5e);n3r=r(x1t,"mbart"),x1t.forEach(t),s3r=r(P7e," \u2014 "),EV=n(P7e,"A",{href:!0});var k1t=s(EV);l3r=r(k1t,"FlaxMBartForQuestionAnswering"),k1t.forEach(t),i3r=r(P7e," (mBART model)"),P7e.forEach(t),d3r=i(Ae),tM=n(Ae,"LI",{});var $7e=s(tM);u5e=n($7e,"STRONG",{});var R1t=s(u5e);c3r=r(R1t,"roberta"),R1t.forEach(t),f3r=r($7e," \u2014 "),yV=n($7e,"A",{href:!0});var S1t=s(yV);m3r=r(S1t,"FlaxRobertaForQuestionAnswering"),S1t.forEach(t),g3r=r($7e," (RoBERTa model)"),$7e.forEach(t),h3r=i(Ae),aM=n(Ae,"LI",{});var I7e=s(aM);b5e=n(I7e,"STRONG",{});var P1t=s(b5e);p3r=r(P1t,"roformer"),P1t.forEach(t),_3r=r(I7e," \u2014 "),wV=n(I7e,"A",{href:!0});var $1t=s(wV);u3r=r($1t,"FlaxRoFormerForQuestionAnswering"),$1t.forEach(t),b3r=r(I7e," (RoFormer model)"),I7e.forEach(t),v3r=i(Ae),nM=n(Ae,"LI",{});var D7e=s(nM);v5e=n(D7e,"STRONG",{});var I1t=s(v5e);T3r=r(I1t,"xlm-roberta"),I1t.forEach(t),F3r=r(D7e," \u2014 "),AV=n(D7e,"A",{href:!0});var D1t=s(AV);C3r=r(D1t,"FlaxXLMRobertaForQuestionAnswering"),D1t.forEach(t),M3r=r(D7e," (XLM-RoBERTa model)"),D7e.forEach(t),Ae.forEach(t),E3r=i(Ba),T5e=n(Ba,"P",{});var j1t=s(T5e);y3r=r(j1t,"Examples:"),j1t.forEach(t),w3r=i(Ba),m(IA.$$.fragment,Ba),Ba.forEach(t),_i.forEach(t),Qxe=i(c),df=n(c,"H2",{class:!0});var tSe=s(df);sM=n(tSe,"A",{id:!0,class:!0,href:!0});var N1t=s(sM);F5e=n(N1t,"SPAN",{});var q1t=s(F5e);m(DA.$$.fragment,q1t),q1t.forEach(t),N1t.forEach(t),A3r=i(tSe),C5e=n(tSe,"SPAN",{});var G1t=s(C5e);L3r=r(G1t,"FlaxAutoModelForTokenClassification"),G1t.forEach(t),tSe.forEach(t),Hxe=i(c),Dr=n(c,"DIV",{class:!0});var bi=s(Dr);m(jA.$$.fragment,bi),B3r=i(bi),cf=n(bi,"P",{});var LW=s(cf);x3r=r(LW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),M5e=n(LW,"CODE",{});var O1t=s(M5e);k3r=r(O1t,"from_pretrained()"),O1t.forEach(t),R3r=r(LW,"class method or the "),E5e=n(LW,"CODE",{});var X1t=s(E5e);S3r=r(X1t,"from_config()"),X1t.forEach(t),P3r=r(LW,`class
method.`),LW.forEach(t),$3r=i(bi),NA=n(bi,"P",{});var aSe=s(NA);I3r=r(aSe,"This class cannot be instantiated directly using "),y5e=n(aSe,"CODE",{});var V1t=s(y5e);D3r=r(V1t,"__init__()"),V1t.forEach(t),j3r=r(aSe," (throws an error)."),aSe.forEach(t),N3r=i(bi),xt=n(bi,"DIV",{class:!0});var vi=s(xt);m(qA.$$.fragment,vi),q3r=i(vi),w5e=n(vi,"P",{});var z1t=s(w5e);G3r=r(z1t,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),z1t.forEach(t),O3r=i(vi),ff=n(vi,"P",{});var BW=s(ff);X3r=r(BW,`Note:
Loading a model from its configuration file does `),A5e=n(BW,"STRONG",{});var W1t=s(A5e);V3r=r(W1t,"not"),W1t.forEach(t),z3r=r(BW,` load the model weights. It only affects the
model\u2019s configuration. Use `),L5e=n(BW,"CODE",{});var Q1t=s(L5e);W3r=r(Q1t,"from_pretrained()"),Q1t.forEach(t),Q3r=r(BW,"to load the model weights."),BW.forEach(t),H3r=i(vi),B5e=n(vi,"P",{});var H1t=s(B5e);U3r=r(H1t,"Examples:"),H1t.forEach(t),J3r=i(vi),m(GA.$$.fragment,vi),vi.forEach(t),Y3r=i(bi),So=n(bi,"DIV",{class:!0});var xa=s(So);m(OA.$$.fragment,xa),K3r=i(xa),x5e=n(xa,"P",{});var U1t=s(x5e);Z3r=r(U1t,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),U1t.forEach(t),eyr=i(xa),kn=n(xa,"P",{});var y4=s(kn);oyr=r(y4,"The model class to instantiate is selected based on the "),k5e=n(y4,"CODE",{});var J1t=s(k5e);ryr=r(J1t,"model_type"),J1t.forEach(t),tyr=r(y4,` property of the config object (either
passed as an argument or loaded from `),R5e=n(y4,"CODE",{});var Y1t=s(R5e);ayr=r(Y1t,"pretrained_model_name_or_path"),Y1t.forEach(t),nyr=r(y4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S5e=n(y4,"CODE",{});var K1t=s(S5e);syr=r(K1t,"pretrained_model_name_or_path"),K1t.forEach(t),lyr=r(y4,":"),y4.forEach(t),iyr=i(xa),Re=n(xa,"UL",{});var Go=s(Re);lM=n(Go,"LI",{});var j7e=s(lM);P5e=n(j7e,"STRONG",{});var Z1t=s(P5e);dyr=r(Z1t,"albert"),Z1t.forEach(t),cyr=r(j7e," \u2014 "),LV=n(j7e,"A",{href:!0});var ebt=s(LV);fyr=r(ebt,"FlaxAlbertForTokenClassification"),ebt.forEach(t),myr=r(j7e," (ALBERT model)"),j7e.forEach(t),gyr=i(Go),iM=n(Go,"LI",{});var N7e=s(iM);$5e=n(N7e,"STRONG",{});var obt=s($5e);hyr=r(obt,"bert"),obt.forEach(t),pyr=r(N7e," \u2014 "),BV=n(N7e,"A",{href:!0});var rbt=s(BV);_yr=r(rbt,"FlaxBertForTokenClassification"),rbt.forEach(t),uyr=r(N7e," (BERT model)"),N7e.forEach(t),byr=i(Go),dM=n(Go,"LI",{});var q7e=s(dM);I5e=n(q7e,"STRONG",{});var tbt=s(I5e);vyr=r(tbt,"big_bird"),tbt.forEach(t),Tyr=r(q7e," \u2014 "),xV=n(q7e,"A",{href:!0});var abt=s(xV);Fyr=r(abt,"FlaxBigBirdForTokenClassification"),abt.forEach(t),Cyr=r(q7e," (BigBird model)"),q7e.forEach(t),Myr=i(Go),cM=n(Go,"LI",{});var G7e=s(cM);D5e=n(G7e,"STRONG",{});var nbt=s(D5e);Eyr=r(nbt,"distilbert"),nbt.forEach(t),yyr=r(G7e," \u2014 "),kV=n(G7e,"A",{href:!0});var sbt=s(kV);wyr=r(sbt,"FlaxDistilBertForTokenClassification"),sbt.forEach(t),Ayr=r(G7e," (DistilBERT model)"),G7e.forEach(t),Lyr=i(Go),fM=n(Go,"LI",{});var O7e=s(fM);j5e=n(O7e,"STRONG",{});var lbt=s(j5e);Byr=r(lbt,"electra"),lbt.forEach(t),xyr=r(O7e," \u2014 "),RV=n(O7e,"A",{href:!0});var ibt=s(RV);kyr=r(ibt,"FlaxElectraForTokenClassification"),ibt.forEach(t),Ryr=r(O7e," (ELECTRA model)"),O7e.forEach(t),Syr=i(Go),mM=n(Go,"LI",{});var X7e=s(mM);N5e=n(X7e,"STRONG",{});var dbt=s(N5e);Pyr=r(dbt,"roberta"),dbt.forEach(t),$yr=r(X7e," \u2014 "),SV=n(X7e,"A",{href:!0});var cbt=s(SV);Iyr=r(cbt,"FlaxRobertaForTokenClassification"),cbt.forEach(t),Dyr=r(X7e," (RoBERTa model)"),X7e.forEach(t),jyr=i(Go),gM=n(Go,"LI",{});var V7e=s(gM);q5e=n(V7e,"STRONG",{});var fbt=s(q5e);Nyr=r(fbt,"roformer"),fbt.forEach(t),qyr=r(V7e," \u2014 "),PV=n(V7e,"A",{href:!0});var mbt=s(PV);Gyr=r(mbt,"FlaxRoFormerForTokenClassification"),mbt.forEach(t),Oyr=r(V7e," (RoFormer model)"),V7e.forEach(t),Xyr=i(Go),hM=n(Go,"LI",{});var z7e=s(hM);G5e=n(z7e,"STRONG",{});var gbt=s(G5e);Vyr=r(gbt,"xlm-roberta"),gbt.forEach(t),zyr=r(z7e," \u2014 "),$V=n(z7e,"A",{href:!0});var hbt=s($V);Wyr=r(hbt,"FlaxXLMRobertaForTokenClassification"),hbt.forEach(t),Qyr=r(z7e," (XLM-RoBERTa model)"),z7e.forEach(t),Go.forEach(t),Hyr=i(xa),O5e=n(xa,"P",{});var pbt=s(O5e);Uyr=r(pbt,"Examples:"),pbt.forEach(t),Jyr=i(xa),m(XA.$$.fragment,xa),xa.forEach(t),bi.forEach(t),Uxe=i(c),mf=n(c,"H2",{class:!0});var nSe=s(mf);pM=n(nSe,"A",{id:!0,class:!0,href:!0});var _bt=s(pM);X5e=n(_bt,"SPAN",{});var ubt=s(X5e);m(VA.$$.fragment,ubt),ubt.forEach(t),_bt.forEach(t),Yyr=i(nSe),V5e=n(nSe,"SPAN",{});var bbt=s(V5e);Kyr=r(bbt,"FlaxAutoModelForMultipleChoice"),bbt.forEach(t),nSe.forEach(t),Jxe=i(c),jr=n(c,"DIV",{class:!0});var Ti=s(jr);m(zA.$$.fragment,Ti),Zyr=i(Ti),gf=n(Ti,"P",{});var xW=s(gf);ewr=r(xW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),z5e=n(xW,"CODE",{});var vbt=s(z5e);owr=r(vbt,"from_pretrained()"),vbt.forEach(t),rwr=r(xW,"class method or the "),W5e=n(xW,"CODE",{});var Tbt=s(W5e);twr=r(Tbt,"from_config()"),Tbt.forEach(t),awr=r(xW,`class
method.`),xW.forEach(t),nwr=i(Ti),WA=n(Ti,"P",{});var sSe=s(WA);swr=r(sSe,"This class cannot be instantiated directly using "),Q5e=n(sSe,"CODE",{});var Fbt=s(Q5e);lwr=r(Fbt,"__init__()"),Fbt.forEach(t),iwr=r(sSe," (throws an error)."),sSe.forEach(t),dwr=i(Ti),kt=n(Ti,"DIV",{class:!0});var Fi=s(kt);m(QA.$$.fragment,Fi),cwr=i(Fi),H5e=n(Fi,"P",{});var Cbt=s(H5e);fwr=r(Cbt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Cbt.forEach(t),mwr=i(Fi),hf=n(Fi,"P",{});var kW=s(hf);gwr=r(kW,`Note:
Loading a model from its configuration file does `),U5e=n(kW,"STRONG",{});var Mbt=s(U5e);hwr=r(Mbt,"not"),Mbt.forEach(t),pwr=r(kW,` load the model weights. It only affects the
model\u2019s configuration. Use `),J5e=n(kW,"CODE",{});var Ebt=s(J5e);_wr=r(Ebt,"from_pretrained()"),Ebt.forEach(t),uwr=r(kW,"to load the model weights."),kW.forEach(t),bwr=i(Fi),Y5e=n(Fi,"P",{});var ybt=s(Y5e);vwr=r(ybt,"Examples:"),ybt.forEach(t),Twr=i(Fi),m(HA.$$.fragment,Fi),Fi.forEach(t),Fwr=i(Ti),Po=n(Ti,"DIV",{class:!0});var ka=s(Po);m(UA.$$.fragment,ka),Cwr=i(ka),K5e=n(ka,"P",{});var wbt=s(K5e);Mwr=r(wbt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),wbt.forEach(t),Ewr=i(ka),Rn=n(ka,"P",{});var w4=s(Rn);ywr=r(w4,"The model class to instantiate is selected based on the "),Z5e=n(w4,"CODE",{});var Abt=s(Z5e);wwr=r(Abt,"model_type"),Abt.forEach(t),Awr=r(w4,` property of the config object (either
passed as an argument or loaded from `),e2e=n(w4,"CODE",{});var Lbt=s(e2e);Lwr=r(Lbt,"pretrained_model_name_or_path"),Lbt.forEach(t),Bwr=r(w4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),o2e=n(w4,"CODE",{});var Bbt=s(o2e);xwr=r(Bbt,"pretrained_model_name_or_path"),Bbt.forEach(t),kwr=r(w4,":"),w4.forEach(t),Rwr=i(ka),Se=n(ka,"UL",{});var Oo=s(Se);_M=n(Oo,"LI",{});var W7e=s(_M);r2e=n(W7e,"STRONG",{});var xbt=s(r2e);Swr=r(xbt,"albert"),xbt.forEach(t),Pwr=r(W7e," \u2014 "),IV=n(W7e,"A",{href:!0});var kbt=s(IV);$wr=r(kbt,"FlaxAlbertForMultipleChoice"),kbt.forEach(t),Iwr=r(W7e," (ALBERT model)"),W7e.forEach(t),Dwr=i(Oo),uM=n(Oo,"LI",{});var Q7e=s(uM);t2e=n(Q7e,"STRONG",{});var Rbt=s(t2e);jwr=r(Rbt,"bert"),Rbt.forEach(t),Nwr=r(Q7e," \u2014 "),DV=n(Q7e,"A",{href:!0});var Sbt=s(DV);qwr=r(Sbt,"FlaxBertForMultipleChoice"),Sbt.forEach(t),Gwr=r(Q7e," (BERT model)"),Q7e.forEach(t),Owr=i(Oo),bM=n(Oo,"LI",{});var H7e=s(bM);a2e=n(H7e,"STRONG",{});var Pbt=s(a2e);Xwr=r(Pbt,"big_bird"),Pbt.forEach(t),Vwr=r(H7e," \u2014 "),jV=n(H7e,"A",{href:!0});var $bt=s(jV);zwr=r($bt,"FlaxBigBirdForMultipleChoice"),$bt.forEach(t),Wwr=r(H7e," (BigBird model)"),H7e.forEach(t),Qwr=i(Oo),vM=n(Oo,"LI",{});var U7e=s(vM);n2e=n(U7e,"STRONG",{});var Ibt=s(n2e);Hwr=r(Ibt,"distilbert"),Ibt.forEach(t),Uwr=r(U7e," \u2014 "),NV=n(U7e,"A",{href:!0});var Dbt=s(NV);Jwr=r(Dbt,"FlaxDistilBertForMultipleChoice"),Dbt.forEach(t),Ywr=r(U7e," (DistilBERT model)"),U7e.forEach(t),Kwr=i(Oo),TM=n(Oo,"LI",{});var J7e=s(TM);s2e=n(J7e,"STRONG",{});var jbt=s(s2e);Zwr=r(jbt,"electra"),jbt.forEach(t),e6r=r(J7e," \u2014 "),qV=n(J7e,"A",{href:!0});var Nbt=s(qV);o6r=r(Nbt,"FlaxElectraForMultipleChoice"),Nbt.forEach(t),r6r=r(J7e," (ELECTRA model)"),J7e.forEach(t),t6r=i(Oo),FM=n(Oo,"LI",{});var Y7e=s(FM);l2e=n(Y7e,"STRONG",{});var qbt=s(l2e);a6r=r(qbt,"roberta"),qbt.forEach(t),n6r=r(Y7e," \u2014 "),GV=n(Y7e,"A",{href:!0});var Gbt=s(GV);s6r=r(Gbt,"FlaxRobertaForMultipleChoice"),Gbt.forEach(t),l6r=r(Y7e," (RoBERTa model)"),Y7e.forEach(t),i6r=i(Oo),CM=n(Oo,"LI",{});var K7e=s(CM);i2e=n(K7e,"STRONG",{});var Obt=s(i2e);d6r=r(Obt,"roformer"),Obt.forEach(t),c6r=r(K7e," \u2014 "),OV=n(K7e,"A",{href:!0});var Xbt=s(OV);f6r=r(Xbt,"FlaxRoFormerForMultipleChoice"),Xbt.forEach(t),m6r=r(K7e," (RoFormer model)"),K7e.forEach(t),g6r=i(Oo),MM=n(Oo,"LI",{});var Z7e=s(MM);d2e=n(Z7e,"STRONG",{});var Vbt=s(d2e);h6r=r(Vbt,"xlm-roberta"),Vbt.forEach(t),p6r=r(Z7e," \u2014 "),XV=n(Z7e,"A",{href:!0});var zbt=s(XV);_6r=r(zbt,"FlaxXLMRobertaForMultipleChoice"),zbt.forEach(t),u6r=r(Z7e," (XLM-RoBERTa model)"),Z7e.forEach(t),Oo.forEach(t),b6r=i(ka),c2e=n(ka,"P",{});var Wbt=s(c2e);v6r=r(Wbt,"Examples:"),Wbt.forEach(t),T6r=i(ka),m(JA.$$.fragment,ka),ka.forEach(t),Ti.forEach(t),Yxe=i(c),pf=n(c,"H2",{class:!0});var lSe=s(pf);EM=n(lSe,"A",{id:!0,class:!0,href:!0});var Qbt=s(EM);f2e=n(Qbt,"SPAN",{});var Hbt=s(f2e);m(YA.$$.fragment,Hbt),Hbt.forEach(t),Qbt.forEach(t),F6r=i(lSe),m2e=n(lSe,"SPAN",{});var Ubt=s(m2e);C6r=r(Ubt,"FlaxAutoModelForNextSentencePrediction"),Ubt.forEach(t),lSe.forEach(t),Kxe=i(c),Nr=n(c,"DIV",{class:!0});var Ci=s(Nr);m(KA.$$.fragment,Ci),M6r=i(Ci),_f=n(Ci,"P",{});var RW=s(_f);E6r=r(RW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),g2e=n(RW,"CODE",{});var Jbt=s(g2e);y6r=r(Jbt,"from_pretrained()"),Jbt.forEach(t),w6r=r(RW,"class method or the "),h2e=n(RW,"CODE",{});var Ybt=s(h2e);A6r=r(Ybt,"from_config()"),Ybt.forEach(t),L6r=r(RW,`class
method.`),RW.forEach(t),B6r=i(Ci),ZA=n(Ci,"P",{});var iSe=s(ZA);x6r=r(iSe,"This class cannot be instantiated directly using "),p2e=n(iSe,"CODE",{});var Kbt=s(p2e);k6r=r(Kbt,"__init__()"),Kbt.forEach(t),R6r=r(iSe," (throws an error)."),iSe.forEach(t),S6r=i(Ci),Rt=n(Ci,"DIV",{class:!0});var Mi=s(Rt);m(eL.$$.fragment,Mi),P6r=i(Mi),_2e=n(Mi,"P",{});var Zbt=s(_2e);$6r=r(Zbt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Zbt.forEach(t),I6r=i(Mi),uf=n(Mi,"P",{});var SW=s(uf);D6r=r(SW,`Note:
Loading a model from its configuration file does `),u2e=n(SW,"STRONG",{});var e5t=s(u2e);j6r=r(e5t,"not"),e5t.forEach(t),N6r=r(SW,` load the model weights. It only affects the
model\u2019s configuration. Use `),b2e=n(SW,"CODE",{});var o5t=s(b2e);q6r=r(o5t,"from_pretrained()"),o5t.forEach(t),G6r=r(SW,"to load the model weights."),SW.forEach(t),O6r=i(Mi),v2e=n(Mi,"P",{});var r5t=s(v2e);X6r=r(r5t,"Examples:"),r5t.forEach(t),V6r=i(Mi),m(oL.$$.fragment,Mi),Mi.forEach(t),z6r=i(Ci),$o=n(Ci,"DIV",{class:!0});var Ra=s($o);m(rL.$$.fragment,Ra),W6r=i(Ra),T2e=n(Ra,"P",{});var t5t=s(T2e);Q6r=r(t5t,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),t5t.forEach(t),H6r=i(Ra),Sn=n(Ra,"P",{});var A4=s(Sn);U6r=r(A4,"The model class to instantiate is selected based on the "),F2e=n(A4,"CODE",{});var a5t=s(F2e);J6r=r(a5t,"model_type"),a5t.forEach(t),Y6r=r(A4,` property of the config object (either
passed as an argument or loaded from `),C2e=n(A4,"CODE",{});var n5t=s(C2e);K6r=r(n5t,"pretrained_model_name_or_path"),n5t.forEach(t),Z6r=r(A4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),M2e=n(A4,"CODE",{});var s5t=s(M2e);eAr=r(s5t,"pretrained_model_name_or_path"),s5t.forEach(t),oAr=r(A4,":"),A4.forEach(t),rAr=i(Ra),E2e=n(Ra,"UL",{});var l5t=s(E2e);yM=n(l5t,"LI",{});var eBe=s(yM);y2e=n(eBe,"STRONG",{});var i5t=s(y2e);tAr=r(i5t,"bert"),i5t.forEach(t),aAr=r(eBe," \u2014 "),VV=n(eBe,"A",{href:!0});var d5t=s(VV);nAr=r(d5t,"FlaxBertForNextSentencePrediction"),d5t.forEach(t),sAr=r(eBe," (BERT model)"),eBe.forEach(t),l5t.forEach(t),lAr=i(Ra),w2e=n(Ra,"P",{});var c5t=s(w2e);iAr=r(c5t,"Examples:"),c5t.forEach(t),dAr=i(Ra),m(tL.$$.fragment,Ra),Ra.forEach(t),Ci.forEach(t),Zxe=i(c),bf=n(c,"H2",{class:!0});var dSe=s(bf);wM=n(dSe,"A",{id:!0,class:!0,href:!0});var f5t=s(wM);A2e=n(f5t,"SPAN",{});var m5t=s(A2e);m(aL.$$.fragment,m5t),m5t.forEach(t),f5t.forEach(t),cAr=i(dSe),L2e=n(dSe,"SPAN",{});var g5t=s(L2e);fAr=r(g5t,"FlaxAutoModelForImageClassification"),g5t.forEach(t),dSe.forEach(t),eke=i(c),qr=n(c,"DIV",{class:!0});var Ei=s(qr);m(nL.$$.fragment,Ei),mAr=i(Ei),vf=n(Ei,"P",{});var PW=s(vf);gAr=r(PW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),B2e=n(PW,"CODE",{});var h5t=s(B2e);hAr=r(h5t,"from_pretrained()"),h5t.forEach(t),pAr=r(PW,"class method or the "),x2e=n(PW,"CODE",{});var p5t=s(x2e);_Ar=r(p5t,"from_config()"),p5t.forEach(t),uAr=r(PW,`class
method.`),PW.forEach(t),bAr=i(Ei),sL=n(Ei,"P",{});var cSe=s(sL);vAr=r(cSe,"This class cannot be instantiated directly using "),k2e=n(cSe,"CODE",{});var _5t=s(k2e);TAr=r(_5t,"__init__()"),_5t.forEach(t),FAr=r(cSe," (throws an error)."),cSe.forEach(t),CAr=i(Ei),St=n(Ei,"DIV",{class:!0});var yi=s(St);m(lL.$$.fragment,yi),MAr=i(yi),R2e=n(yi,"P",{});var u5t=s(R2e);EAr=r(u5t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),u5t.forEach(t),yAr=i(yi),Tf=n(yi,"P",{});var $W=s(Tf);wAr=r($W,`Note:
Loading a model from its configuration file does `),S2e=n($W,"STRONG",{});var b5t=s(S2e);AAr=r(b5t,"not"),b5t.forEach(t),LAr=r($W,` load the model weights. It only affects the
model\u2019s configuration. Use `),P2e=n($W,"CODE",{});var v5t=s(P2e);BAr=r(v5t,"from_pretrained()"),v5t.forEach(t),xAr=r($W,"to load the model weights."),$W.forEach(t),kAr=i(yi),$2e=n(yi,"P",{});var T5t=s($2e);RAr=r(T5t,"Examples:"),T5t.forEach(t),SAr=i(yi),m(iL.$$.fragment,yi),yi.forEach(t),PAr=i(Ei),Io=n(Ei,"DIV",{class:!0});var Sa=s(Io);m(dL.$$.fragment,Sa),$Ar=i(Sa),I2e=n(Sa,"P",{});var F5t=s(I2e);IAr=r(F5t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),F5t.forEach(t),DAr=i(Sa),Pn=n(Sa,"P",{});var L4=s(Pn);jAr=r(L4,"The model class to instantiate is selected based on the "),D2e=n(L4,"CODE",{});var C5t=s(D2e);NAr=r(C5t,"model_type"),C5t.forEach(t),qAr=r(L4,` property of the config object (either
passed as an argument or loaded from `),j2e=n(L4,"CODE",{});var M5t=s(j2e);GAr=r(M5t,"pretrained_model_name_or_path"),M5t.forEach(t),OAr=r(L4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),N2e=n(L4,"CODE",{});var E5t=s(N2e);XAr=r(E5t,"pretrained_model_name_or_path"),E5t.forEach(t),VAr=r(L4,":"),L4.forEach(t),zAr=i(Sa),cL=n(Sa,"UL",{});var fSe=s(cL);AM=n(fSe,"LI",{});var oBe=s(AM);q2e=n(oBe,"STRONG",{});var y5t=s(q2e);WAr=r(y5t,"beit"),y5t.forEach(t),QAr=r(oBe," \u2014 "),zV=n(oBe,"A",{href:!0});var w5t=s(zV);HAr=r(w5t,"FlaxBeitForImageClassification"),w5t.forEach(t),UAr=r(oBe," (BEiT model)"),oBe.forEach(t),JAr=i(fSe),LM=n(fSe,"LI",{});var rBe=s(LM);G2e=n(rBe,"STRONG",{});var A5t=s(G2e);YAr=r(A5t,"vit"),A5t.forEach(t),KAr=r(rBe," \u2014 "),WV=n(rBe,"A",{href:!0});var L5t=s(WV);ZAr=r(L5t,"FlaxViTForImageClassification"),L5t.forEach(t),eLr=r(rBe," (ViT model)"),rBe.forEach(t),fSe.forEach(t),oLr=i(Sa),O2e=n(Sa,"P",{});var B5t=s(O2e);rLr=r(B5t,"Examples:"),B5t.forEach(t),tLr=i(Sa),m(fL.$$.fragment,Sa),Sa.forEach(t),Ei.forEach(t),oke=i(c),Ff=n(c,"H2",{class:!0});var mSe=s(Ff);BM=n(mSe,"A",{id:!0,class:!0,href:!0});var x5t=s(BM);X2e=n(x5t,"SPAN",{});var k5t=s(X2e);m(mL.$$.fragment,k5t),k5t.forEach(t),x5t.forEach(t),aLr=i(mSe),V2e=n(mSe,"SPAN",{});var R5t=s(V2e);nLr=r(R5t,"FlaxAutoModelForVision2Seq"),R5t.forEach(t),mSe.forEach(t),rke=i(c),Gr=n(c,"DIV",{class:!0});var wi=s(Gr);m(gL.$$.fragment,wi),sLr=i(wi),Cf=n(wi,"P",{});var IW=s(Cf);lLr=r(IW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),z2e=n(IW,"CODE",{});var S5t=s(z2e);iLr=r(S5t,"from_pretrained()"),S5t.forEach(t),dLr=r(IW,"class method or the "),W2e=n(IW,"CODE",{});var P5t=s(W2e);cLr=r(P5t,"from_config()"),P5t.forEach(t),fLr=r(IW,`class
method.`),IW.forEach(t),mLr=i(wi),hL=n(wi,"P",{});var gSe=s(hL);gLr=r(gSe,"This class cannot be instantiated directly using "),Q2e=n(gSe,"CODE",{});var $5t=s(Q2e);hLr=r($5t,"__init__()"),$5t.forEach(t),pLr=r(gSe," (throws an error)."),gSe.forEach(t),_Lr=i(wi),Pt=n(wi,"DIV",{class:!0});var Ai=s(Pt);m(pL.$$.fragment,Ai),uLr=i(Ai),H2e=n(Ai,"P",{});var I5t=s(H2e);bLr=r(I5t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),I5t.forEach(t),vLr=i(Ai),Mf=n(Ai,"P",{});var DW=s(Mf);TLr=r(DW,`Note:
Loading a model from its configuration file does `),U2e=n(DW,"STRONG",{});var D5t=s(U2e);FLr=r(D5t,"not"),D5t.forEach(t),CLr=r(DW,` load the model weights. It only affects the
model\u2019s configuration. Use `),J2e=n(DW,"CODE",{});var j5t=s(J2e);MLr=r(j5t,"from_pretrained()"),j5t.forEach(t),ELr=r(DW,"to load the model weights."),DW.forEach(t),yLr=i(Ai),Y2e=n(Ai,"P",{});var N5t=s(Y2e);wLr=r(N5t,"Examples:"),N5t.forEach(t),ALr=i(Ai),m(_L.$$.fragment,Ai),Ai.forEach(t),LLr=i(wi),Do=n(wi,"DIV",{class:!0});var Pa=s(Do);m(uL.$$.fragment,Pa),BLr=i(Pa),K2e=n(Pa,"P",{});var q5t=s(K2e);xLr=r(q5t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),q5t.forEach(t),kLr=i(Pa),$n=n(Pa,"P",{});var B4=s($n);RLr=r(B4,"The model class to instantiate is selected based on the "),Z2e=n(B4,"CODE",{});var G5t=s(Z2e);SLr=r(G5t,"model_type"),G5t.forEach(t),PLr=r(B4,` property of the config object (either
passed as an argument or loaded from `),eve=n(B4,"CODE",{});var O5t=s(eve);$Lr=r(O5t,"pretrained_model_name_or_path"),O5t.forEach(t),ILr=r(B4,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ove=n(B4,"CODE",{});var X5t=s(ove);DLr=r(X5t,"pretrained_model_name_or_path"),X5t.forEach(t),jLr=r(B4,":"),B4.forEach(t),NLr=i(Pa),rve=n(Pa,"UL",{});var V5t=s(rve);xM=n(V5t,"LI",{});var tBe=s(xM);tve=n(tBe,"STRONG",{});var z5t=s(tve);qLr=r(z5t,"vision-encoder-decoder"),z5t.forEach(t),GLr=r(tBe," \u2014 "),QV=n(tBe,"A",{href:!0});var W5t=s(QV);OLr=r(W5t,"FlaxVisionEncoderDecoderModel"),W5t.forEach(t),XLr=r(tBe," (Vision Encoder decoder model)"),tBe.forEach(t),V5t.forEach(t),VLr=i(Pa),ave=n(Pa,"P",{});var Q5t=s(ave);zLr=r(Q5t,"Examples:"),Q5t.forEach(t),WLr=i(Pa),m(bL.$$.fragment,Pa),Pa.forEach(t),wi.forEach(t),this.h()},h(){d(J,"name","hf:doc:metadata"),d(J,"content",JSON.stringify(r2t)),d(ge,"id","auto-classes"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#auto-classes"),d(ie,"class","relative group"),d(In,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoConfig"),d(jn,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoModel"),d(Nn,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoTokenizer"),d($i,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertModel"),d(Bf,"id","extending-the-auto-classes"),d(Bf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Bf,"href","#extending-the-auto-classes"),d(Ii,"class","relative group"),d(kf,"id","transformers.AutoConfig"),d(kf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(kf,"href","#transformers.AutoConfig"),d(Di,"class","relative group"),d(C8,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),d(M8,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertConfig"),d(E8,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartConfig"),d(y8,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitConfig"),d(w8,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertConfig"),d(A8,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationConfig"),d(L8,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdConfig"),d(B8,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),d(x8,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotConfig"),d(k8,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),d(R8,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertConfig"),d(S8,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineConfig"),d(P8,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPConfig"),d($8,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertConfig"),d(I8,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextConfig"),d(D8,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLConfig"),d(j8,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioConfig"),d(N8,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextConfig"),d(q8,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaConfig"),d(G8,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Config"),d(O8,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTConfig"),d(X8,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrConfig"),d(V8,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertConfig"),d(z8,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRConfig"),d(W8,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraConfig"),d(Q8,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),d(H8,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertConfig"),d(U8,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetConfig"),d(J8,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTConfig"),d(Y8,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelConfig"),d(K8,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Config"),d(Z8,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),d(e7,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJConfig"),d(o7,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertConfig"),d(r7,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertConfig"),d(t7,"href","/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTConfig"),d(a7,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMConfig"),d(n7,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),d(s7,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDConfig"),d(l7,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerConfig"),d(i7,"href","/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeConfig"),d(d7,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertConfig"),d(c7,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Config"),d(f7,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianConfig"),d(m7,"href","/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerConfig"),d(g7,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartConfig"),d(h7,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),d(p7,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertConfig"),d(_7,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetConfig"),d(u7,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Config"),d(b7,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerConfig"),d(v7,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),d(T7,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusConfig"),d(F7,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverConfig"),d(C7,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartConfig"),d(M7,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerConfig"),d(E7,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetConfig"),d(y7,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertConfig"),d(w7,"href","/docs/transformers/pr_15900/en/model_doc/rag#transformers.RagConfig"),d(A7,"href","/docs/transformers/pr_15900/en/model_doc/realm#transformers.RealmConfig"),d(L7,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerConfig"),d(B7,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertConfig"),d(x7,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertConfig"),d(k7,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaConfig"),d(R7,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerConfig"),d(S7,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerConfig"),d(P7,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWConfig"),d($7,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDConfig"),d(I7,"href","/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),d(D7,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),d(j7,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),d(N7,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterConfig"),d(q7,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),d(G7,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinConfig"),d(O7,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Config"),d(X7,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasConfig"),d(V7,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),d(z7,"href","/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRConfig"),d(W7,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechConfig"),d(Q7,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),d(H7,"href","/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltConfig"),d(U7,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),d(J7,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),d(Y7,"href","/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertConfig"),d(K7,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTConfig"),d(Z7,"href","/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEConfig"),d(eB,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),d(oB,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMConfig"),d(rB,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMConfig"),d(tB,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMConfig"),d(aB,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),d(nB,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),d(sB,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),d(lB,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetConfig"),d(iB,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoConfig"),d(fo,"class","docstring"),d(bg,"class","docstring"),d(zo,"class","docstring"),d(vg,"id","transformers.AutoTokenizer"),d(vg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(vg,"href","#transformers.AutoTokenizer"),d(Ni,"class","relative group"),d(dB,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(cB,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertTokenizer"),d(fB,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertTokenizerFast"),d(mB,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartTokenizer"),d(gB,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartTokenizerFast"),d(hB,"href","/docs/transformers/pr_15900/en/model_doc/barthez#transformers.BarthezTokenizer"),d(pB,"href","/docs/transformers/pr_15900/en/model_doc/barthez#transformers.BarthezTokenizerFast"),d(_B,"href","/docs/transformers/pr_15900/en/model_doc/bartpho#transformers.BartphoTokenizer"),d(uB,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizer"),d(bB,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizerFast"),d(vB,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),d(TB,"href","/docs/transformers/pr_15900/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),d(FB,"href","/docs/transformers/pr_15900/en/model_doc/bertweet#transformers.BertweetTokenizer"),d(CB,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdTokenizer"),d(MB,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),d(EB,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(yB,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(wB,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),d(AB,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),d(LB,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),d(BB,"href","/docs/transformers/pr_15900/en/model_doc/byt5#transformers.ByT5Tokenizer"),d(xB,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertTokenizer"),d(kB,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertTokenizerFast"),d(RB,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineTokenizer"),d(SB,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPTokenizer"),d(PB,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPTokenizerFast"),d($B,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertTokenizer"),d(IB,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),d(DB,"href","/docs/transformers/pr_15900/en/model_doc/cpm#transformers.CpmTokenizer"),d(jB,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLTokenizer"),d(NB,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaTokenizer"),d(qB,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaTokenizerFast"),d(GB,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),d(OB,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertTokenizer"),d(XB,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),d(VB,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(zB,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(WB,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraTokenizer"),d(QB,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraTokenizerFast"),d(HB,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertTokenizer"),d(UB,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetTokenizer"),d(JB,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetTokenizerFast"),d(YB,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTTokenizer"),d(KB,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelTokenizer"),d(ZB,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelTokenizerFast"),d(ex,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(ox,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(rx,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(tx,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(ax,"href","/docs/transformers/pr_15900/en/model_doc/herbert#transformers.HerbertTokenizer"),d(nx,"href","/docs/transformers/pr_15900/en/model_doc/herbert#transformers.HerbertTokenizerFast"),d(sx,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(lx,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizer"),d(ix,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(dx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),d(cx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),d(fx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),d(mx,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),d(gx,"href","/docs/transformers/pr_15900/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),d(hx,"href","/docs/transformers/pr_15900/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),d(px,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDTokenizer"),d(_x,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDTokenizerFast"),d(ux,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerTokenizer"),d(bx,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerTokenizerFast"),d(vx,"href","/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeTokenizer"),d(Tx,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertTokenizer"),d(Fx,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),d(Cx,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),d(Mx,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianTokenizer"),d(Ex,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartTokenizer"),d(yx,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartTokenizerFast"),d(wx,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBart50Tokenizer"),d(Ax,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBart50TokenizerFast"),d(Lx,"href","/docs/transformers/pr_15900/en/model_doc/mluke#transformers.MLukeTokenizer"),d(Bx,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),d(xx,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),d(kx,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetTokenizer"),d(Rx,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),d(Sx,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5Tokenizer"),d(Px,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5TokenizerFast"),d($x,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),d(Ix,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),d(Dx,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(jx,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(Nx,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverTokenizer"),d(qx,"href","/docs/transformers/pr_15900/en/model_doc/phobert#transformers.PhobertTokenizer"),d(Gx,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartTokenizer"),d(Ox,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),d(Xx,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizer"),d(Vx,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertTokenizerFast"),d(zx,"href","/docs/transformers/pr_15900/en/model_doc/rag#transformers.RagTokenizer"),d(Wx,"href","/docs/transformers/pr_15900/en/model_doc/realm#transformers.RealmTokenizer"),d(Qx,"href","/docs/transformers/pr_15900/en/model_doc/realm#transformers.RealmTokenizerFast"),d(Hx,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerTokenizer"),d(Ux,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerTokenizerFast"),d(Jx,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertTokenizer"),d(Yx,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertTokenizerFast"),d(Kx,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertTokenizer"),d(Zx,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),d(ek,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizer"),d(ok,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(rk,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerTokenizer"),d(tk,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),d(ak,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),d(nk,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),d(sk,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterTokenizer"),d(lk,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterTokenizerFast"),d(ik,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),d(dk,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),d(ck,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5Tokenizer"),d(fk,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.T5TokenizerFast"),d(mk,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasTokenizer"),d(gk,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),d(hk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(pk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),d(_k,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMTokenizer"),d(uk,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMTokenizerFast"),d(bk,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMTokenizer"),d(vk,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),d(Tk,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),d(Fk,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),d(Ck,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetTokenizer"),d(Mk,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),d(mo,"class","docstring"),d(Ug,"class","docstring"),d(Wo,"class","docstring"),d(Jg,"id","transformers.AutoFeatureExtractor"),d(Jg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Jg,"href","#transformers.AutoFeatureExtractor"),d(qi,"class","relative group"),d(Ek,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(yk,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitFeatureExtractor"),d(wk,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPFeatureExtractor"),d(Ak,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(Lk,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTFeatureExtractor"),d(Bk,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrFeatureExtractor"),d(xk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(kk,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),d(Rk,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),d(Sk,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),d(Pk,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),d($k,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),d(Ik,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Dk,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(jk,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Nk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d($e,"class","docstring"),d(gh,"class","docstring"),d(Qo,"class","docstring"),d(hh,"id","transformers.AutoProcessor"),d(hh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hh,"href","#transformers.AutoProcessor"),d(Gi,"class","relative group"),d(qk,"href","/docs/transformers/pr_15900/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(Gk,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPProcessor"),d(Ok,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),d(Xk,"href","/docs/transformers/pr_15900/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),d(Vk,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),d(zk,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),d(Wk,"href","/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRProcessor"),d(Qk,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),d(Hk,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),d(Ie,"class","docstring"),d(Eh,"class","docstring"),d(Ho,"class","docstring"),d(yh,"id","transformers.AutoModel"),d(yh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yh,"href","#transformers.AutoModel"),d(Xi,"class","relative group"),d(Or,"class","docstring"),d(Uk,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertModel"),d(Jk,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartModel"),d(Yk,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitModel"),d(Kk,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertModel"),d(Zk,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),d(eR,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdModel"),d(oR,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),d(rR,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotModel"),d(tR,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),d(aR,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertModel"),d(nR,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineModel"),d(sR,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.CLIPModel"),d(lR,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertModel"),d(iR,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextModel"),d(dR,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLModel"),d(cR,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioModel"),d(fR,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextModel"),d(mR,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaModel"),d(gR,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2Model"),d(hR,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTModel"),d(pR,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrModel"),d(_R,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertModel"),d(uR,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(bR,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraModel"),d(vR,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertModel"),d(TR,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetModel"),d(FR,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTModel"),d(CR,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelModel"),d(MR,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelBaseModel"),d(ER,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2Model"),d(yR,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoModel"),d(wR,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJModel"),d(AR,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertModel"),d(LR,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertModel"),d(BR,"href","/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTModel"),d(xR,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMModel"),d(kR,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),d(RR,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDModel"),d(SR,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerModel"),d(PR,"href","/docs/transformers/pr_15900/en/model_doc/luke#transformers.LukeModel"),d($R,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertModel"),d(IR,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100Model"),d(DR,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianModel"),d(jR,"href","/docs/transformers/pr_15900/en/model_doc/maskformer#transformers.MaskFormerModel"),d(NR,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartModel"),d(qR,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertModel"),d(GR,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertModel"),d(OR,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetModel"),d(XR,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5Model"),d(VR,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerModel"),d(zR,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),d(WR,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusModel"),d(QR,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverModel"),d(HR,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartModel"),d(UR,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerModel"),d(JR,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetModel"),d(YR,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertModel"),d(KR,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModel"),d(ZR,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertModel"),d(eS,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel"),d(oS,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaModel"),d(rS,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerModel"),d(tS,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerModel"),d(aS,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWModel"),d(nS,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDModel"),d(sS,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextModel"),d(lS,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterModel"),d(iS,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertModel"),d(dS,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinModel"),d(cS,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5Model"),d(fS,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasModel"),d(mS,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLModel"),d(gS,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechModel"),d(hS,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),d(pS,"href","/docs/transformers/pr_15900/en/model_doc/vilt#transformers.ViltModel"),d(_S,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),d(uS,"href","/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertModel"),d(bS,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTModel"),d(vS,"href","/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEModel"),d(TS,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),d(FS,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMModel"),d(CS,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMModel"),d(MS,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMModel"),d(ES,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),d(yS,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),d(wS,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),d(AS,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetModel"),d(LS,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoModel"),d(De,"class","docstring"),d(Uo,"class","docstring"),d(n_,"id","transformers.AutoModelForPreTraining"),d(n_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(n_,"href","#transformers.AutoModelForPreTraining"),d(Wi,"class","relative group"),d(Xr,"class","docstring"),d(BS,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForPreTraining"),d(xS,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(kS,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForPreTraining"),d(RS,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),d(SS,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(PS,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d($S,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(IS,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(DS,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(jS,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(NS,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForPreTraining"),d(qS,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(GS,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForPreTraining"),d(OS,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(XS,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForPreTraining"),d(VS,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(zS,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(WS,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(QS,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(HS,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForPreTraining"),d(US,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),d(JS,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),d(YS,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(KS,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(ZS,"href","/docs/transformers/pr_15900/en/model_doc/retribert#transformers.RetriBertModel"),d(eP,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(oP,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(rP,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(tP,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(aP,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(nP,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),d(sP,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),d(lP,"href","/docs/transformers/pr_15900/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),d(iP,"href","/docs/transformers/pr_15900/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),d(dP,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),d(cP,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(fP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(mP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(gP,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(je,"class","docstring"),d(Jo,"class","docstring"),d(W_,"id","transformers.AutoModelForCausalLM"),d(W_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(W_,"href","#transformers.AutoModelForCausalLM"),d(Ui,"class","relative group"),d(Vr,"class","docstring"),d(hP,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForCausalLM"),d(pP,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertLMHeadModel"),d(_P,"href","/docs/transformers/pr_15900/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),d(uP,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),d(bP,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),d(vP,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),d(TP,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),d(FP,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForCausalLM"),d(CP,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(MP,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM"),d(EP,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForCausalLM"),d(yP,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(wP,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),d(AP,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForCausalLM"),d(LP,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianForCausalLM"),d(BP,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForCausalLM"),d(xP,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),d(kP,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(RP,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForCausalLM"),d(SP,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForCausalLM"),d(PP,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),d($P,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),d(IP,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),d(DP,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForCausalLM"),d(jP,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForCausalLM"),d(NP,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForCausalLM"),d(qP,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),d(GP,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(OP,"href","/docs/transformers/pr_15900/en/model_doc/trocr#transformers.TrOCRForCausalLM"),d(XP,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.XGLMForCausalLM"),d(VP,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(zP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),d(WP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),d(QP,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),d(HP,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Ne,"class","docstring"),d(Yo,"class","docstring"),d(Bu,"id","transformers.AutoModelForMaskedLM"),d(Bu,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Bu,"href","#transformers.AutoModelForMaskedLM"),d(Ki,"class","relative group"),d(zr,"class","docstring"),d(UP,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMaskedLM"),d(JP,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(YP,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMaskedLM"),d(KP,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),d(ZP,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(e$,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),d(o$,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(r$,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(t$,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(a$,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(n$,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMaskedLM"),d(s$,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(l$,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMaskedLM"),d(i$,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMaskedLM"),d(d$,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(c$,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(f$,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(m$,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(g$,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),d(h$,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),d(p$,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(_$,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),d(u$,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(b$,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),d(v$,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForMaskedLM"),d(T$,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMaskedLM"),d(F$,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(C$,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),d(M$,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(E$,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(y$,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(w$,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(A$,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(L$,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMaskedLM"),d(qe,"class","docstring"),d(Ko,"class","docstring"),d(m0,"id","transformers.AutoModelForSeq2SeqLM"),d(m0,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(m0,"href","#transformers.AutoModelForSeq2SeqLM"),d(od,"class","relative group"),d(Wr,"class","docstring"),d(B$,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(x$,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),d(k$,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),d(R$,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),d(S$,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),d(P$,"href","/docs/transformers/pr_15900/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d($$,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForConditionalGeneration"),d(I$,"href","/docs/transformers/pr_15900/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),d(D$,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.MarianMTModel"),d(j$,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(N$,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),d(q$,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),d(G$,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),d(O$,"href","/docs/transformers/pr_15900/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),d(X$,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(V$,"href","/docs/transformers/pr_15900/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),d(Ge,"class","docstring"),d(Zo,"class","docstring"),d(x0,"id","transformers.AutoModelForSequenceClassification"),d(x0,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(x0,"href","#transformers.AutoModelForSequenceClassification"),d(ad,"class","relative group"),d(Qr,"class","docstring"),d(z$,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForSequenceClassification"),d(W$,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForSequenceClassification"),d(Q$,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForSequenceClassification"),d(H$,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),d(U$,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),d(J$,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),d(Y$,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForSequenceClassification"),d(K$,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),d(Z$,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),d(eI,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification"),d(oI,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),d(rI,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),d(tI,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),d(aI,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForSequenceClassification"),d(nI,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),d(sI,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForSequenceClassification"),d(lI,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),d(iI,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),d(dI,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),d(cI,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),d(fI,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForSequenceClassification"),d(mI,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),d(gI,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),d(hI,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForSequenceClassification"),d(pI,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),d(_I,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForSequenceClassification"),d(uI,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),d(bI,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),d(vI,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),d(TI,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),d(FI,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),d(CI,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(MI,"href","/docs/transformers/pr_15900/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),d(EI,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),d(yI,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),d(wI,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),d(AI,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),d(LI,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),d(BI,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),d(xI,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForSequenceClassification"),d(kI,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),d(RI,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForSequenceClassification"),d(SI,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),d(PI,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),d($I,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),d(II,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForSequenceClassification"),d(Oe,"class","docstring"),d(er,"class","docstring"),d(E1,"id","transformers.AutoModelForMultipleChoice"),d(E1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(E1,"href","#transformers.AutoModelForMultipleChoice"),d(ld,"class","relative group"),d(Hr,"class","docstring"),d(DI,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForMultipleChoice"),d(jI,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForMultipleChoice"),d(NI,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),d(qI,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),d(GI,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForMultipleChoice"),d(OI,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),d(XI,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice"),d(VI,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),d(zI,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForMultipleChoice"),d(WI,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),d(QI,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForMultipleChoice"),d(HI,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),d(UI,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForMultipleChoice"),d(JI,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),d(YI,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),d(KI,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),d(ZI,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),d(eD,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),d(oD,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),d(rD,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),d(tD,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),d(aD,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),d(nD,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),d(sD,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForMultipleChoice"),d(lD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),d(iD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),d(dD,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),d(cD,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForMultipleChoice"),d(Xe,"class","docstring"),d(or,"class","docstring"),d(eb,"id","transformers.AutoModelForNextSentencePrediction"),d(eb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(eb,"href","#transformers.AutoModelForNextSentencePrediction"),d(cd,"class","relative group"),d(Ur,"class","docstring"),d(fD,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForNextSentencePrediction"),d(mD,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),d(gD,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),d(hD,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),d(pD,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),d(Ve,"class","docstring"),d(rr,"class","docstring"),d(lb,"id","transformers.AutoModelForTokenClassification"),d(lb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lb,"href","#transformers.AutoModelForTokenClassification"),d(gd,"class","relative group"),d(Jr,"class","docstring"),d(_D,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForTokenClassification"),d(uD,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForTokenClassification"),d(bD,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),d(vD,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForTokenClassification"),d(TD,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForTokenClassification"),d(FD,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),d(CD,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification"),d(MD,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForTokenClassification"),d(ED,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),d(yD,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),d(wD,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForTokenClassification"),d(AD,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),d(LD,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForTokenClassification"),d(BD,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForTokenClassification"),d(xD,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),d(kD,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForTokenClassification"),d(RD,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),d(SD,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),d(PD,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForTokenClassification"),d($D,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),d(ID,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),d(DD,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),d(jD,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),d(ND,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),d(qD,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForTokenClassification"),d(GD,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForTokenClassification"),d(OD,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),d(XD,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),d(VD,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForTokenClassification"),d(zD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),d(WD,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),d(QD,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),d(HD,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForTokenClassification"),d(ze,"class","docstring"),d(tr,"class","docstring"),d(Ob,"id","transformers.AutoModelForQuestionAnswering"),d(Ob,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ob,"href","#transformers.AutoModelForQuestionAnswering"),d(_d,"class","relative group"),d(Yr,"class","docstring"),d(UD,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),d(JD,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.BartForQuestionAnswering"),d(YD,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.BertForQuestionAnswering"),d(KD,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),d(ZD,"href","/docs/transformers/pr_15900/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),d(ej,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),d(oj,"href","/docs/transformers/pr_15900/en/model_doc/canine#transformers.CanineForQuestionAnswering"),d(rj,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),d(tj,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering"),d(aj,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),d(nj,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),d(sj,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),d(lj,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),d(ij,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),d(dj,"href","/docs/transformers/pr_15900/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),d(cj,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),d(fj,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),d(mj,"href","/docs/transformers/pr_15900/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),d(gj,"href","/docs/transformers/pr_15900/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),d(hj,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.LEDForQuestionAnswering"),d(pj,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),d(_j,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),d(uj,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),d(bj,"href","/docs/transformers/pr_15900/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),d(vj,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),d(Tj,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),d(Fj,"href","/docs/transformers/pr_15900/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),d(Cj,"href","/docs/transformers/pr_15900/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),d(Mj,"href","/docs/transformers/pr_15900/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),d(Ej,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),d(yj,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),d(wj,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),d(Aj,"href","/docs/transformers/pr_15900/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),d(Lj,"href","/docs/transformers/pr_15900/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),d(Bj,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),d(xj,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),d(kj,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),d(Rj,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),d(Sj,"href","/docs/transformers/pr_15900/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),d(We,"class","docstring"),d(ar,"class","docstring"),d(B5,"id","transformers.AutoModelForTableQuestionAnswering"),d(B5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B5,"href","#transformers.AutoModelForTableQuestionAnswering"),d(vd,"class","relative group"),d(Kr,"class","docstring"),d(Pj,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),d(Qe,"class","docstring"),d(nr,"class","docstring"),d(R5,"id","transformers.AutoModelForImageClassification"),d(R5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(R5,"href","#transformers.AutoModelForImageClassification"),d(Cd,"class","relative group"),d(Zr,"class","docstring"),d($j,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForImageClassification"),d(Ij,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.ConvNextForImageClassification"),d(Dj,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassification"),d(jj,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),d(Nj,"href","/docs/transformers/pr_15900/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),d(qj,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(Gj,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d(Oj,"href","/docs/transformers/pr_15900/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(Xj,"href","/docs/transformers/pr_15900/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),d(Vj,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForImageClassification"),d(zj,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForImageClassification"),d(Wj,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForImageClassification"),d(He,"class","docstring"),d(sr,"class","docstring"),d(G5,"id","transformers.AutoModelForVision2Seq"),d(G5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G5,"href","#transformers.AutoModelForVision2Seq"),d(yd,"class","relative group"),d(et,"class","docstring"),d(Qj,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),d(Ue,"class","docstring"),d(lr,"class","docstring"),d(V5,"id","transformers.AutoModelForAudioClassification"),d(V5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(V5,"href","#transformers.AutoModelForAudioClassification"),d(Ld,"class","relative group"),d(ot,"class","docstring"),d(Hj,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification"),d(Uj,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForSequenceClassification"),d(Jj,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForSequenceClassification"),d(Yj,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),d(Kj,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),d(Zj,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),d(eN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),d(oN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),d(Je,"class","docstring"),d(ir,"class","docstring"),d(e2,"id","transformers.AutoModelForAudioFrameClassification"),d(e2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(e2,"href","#transformers.AutoModelForAudioFrameClassification"),d(kd,"class","relative group"),d(rt,"class","docstring"),d(rN,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification"),d(tN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),d(aN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),d(nN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),d(Ye,"class","docstring"),d(dr,"class","docstring"),d(s2,"id","transformers.AutoModelForCTC"),d(s2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(s2,"href","#transformers.AutoModelForCTC"),d(Pd,"class","relative group"),d(tt,"class","docstring"),d(sN,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForCTC"),d(lN,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.HubertForCTC"),d(iN,"href","/docs/transformers/pr_15900/en/model_doc/sew#transformers.SEWForCTC"),d(dN,"href","/docs/transformers/pr_15900/en/model_doc/sew-d#transformers.SEWDForCTC"),d(cN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech#transformers.UniSpeechForCTC"),d(fN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),d(mN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),d(gN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForCTC"),d(Ke,"class","docstring"),d(cr,"class","docstring"),d(_2,"id","transformers.AutoModelForSpeechSeq2Seq"),d(_2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_2,"href","#transformers.AutoModelForSpeechSeq2Seq"),d(Dd,"class","relative group"),d(at,"class","docstring"),d(hN,"href","/docs/transformers/pr_15900/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),d(pN,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),d(Ze,"class","docstring"),d(fr,"class","docstring"),d(T2,"id","transformers.AutoModelForAudioXVector"),d(T2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(T2,"href","#transformers.AutoModelForAudioXVector"),d(qd,"class","relative group"),d(nt,"class","docstring"),d(_N,"href","/docs/transformers/pr_15900/en/model_doc/data2vec#transformers.Data2VecAudioForXVector"),d(uN,"href","/docs/transformers/pr_15900/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),d(bN,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),d(vN,"href","/docs/transformers/pr_15900/en/model_doc/wavlm#transformers.WavLMForXVector"),d(eo,"class","docstring"),d(mr,"class","docstring"),d(w2,"id","transformers.AutoModelForMaskedImageModeling"),d(w2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w2,"href","#transformers.AutoModelForMaskedImageModeling"),d(Xd,"class","relative group"),d(st,"class","docstring"),d(TN,"href","/docs/transformers/pr_15900/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),d(FN,"href","/docs/transformers/pr_15900/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),d(CN,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),d(oo,"class","docstring"),d(gr,"class","docstring"),d(k2,"id","transformers.AutoModelForObjectDetection"),d(k2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(k2,"href","#transformers.AutoModelForObjectDetection"),d(Qd,"class","relative group"),d(lt,"class","docstring"),d(MN,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForObjectDetection"),d(ro,"class","docstring"),d(hr,"class","docstring"),d(P2,"id","transformers.AutoModelForImageSegmentation"),d(P2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P2,"href","#transformers.AutoModelForImageSegmentation"),d(Jd,"class","relative group"),d(it,"class","docstring"),d(EN,"href","/docs/transformers/pr_15900/en/model_doc/detr#transformers.DetrForSegmentation"),d(to,"class","docstring"),d(pr,"class","docstring"),d(D2,"id","transformers.AutoModelForSemanticSegmentation"),d(D2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(D2,"href","#transformers.AutoModelForSemanticSegmentation"),d(Zd,"class","relative group"),d(dt,"class","docstring"),d(yN,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),d(wN,"href","/docs/transformers/pr_15900/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),d(ao,"class","docstring"),d(_r,"class","docstring"),d(G2,"id","transformers.TFAutoModel"),d(G2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G2,"href","#transformers.TFAutoModel"),d(rc,"class","relative group"),d(ct,"class","docstring"),d(AN,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertModel"),d(LN,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartModel"),d(BN,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertModel"),d(xN,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),d(kN,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),d(RN,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertModel"),d(SN,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.TFCLIPModel"),d(PN,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertModel"),d($N,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextModel"),d(IN,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLModel"),d(DN,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaModel"),d(jN,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),d(NN,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertModel"),d(qN,"href","/docs/transformers/pr_15900/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(GN,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraModel"),d(ON,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertModel"),d(XN,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelModel"),d(VN,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelBaseModel"),d(zN,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2Model"),d(WN,"href","/docs/transformers/pr_15900/en/model_doc/hubert#transformers.TFHubertModel"),d(QN,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),d(HN,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDModel"),d(UN,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerModel"),d(JN,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertModel"),d(YN,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianModel"),d(KN,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartModel"),d(ZN,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertModel"),d(eq,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetModel"),d(oq,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5Model"),d(rq,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),d(tq,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusModel"),d(aq,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertModel"),d(nq,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaModel"),d(sq,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerModel"),d(lq,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),d(iq,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5Model"),d(dq,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasModel"),d(cq,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),d(fq,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTModel"),d(mq,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),d(gq,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMModel"),d(hq,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),d(pq,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetModel"),d(go,"class","docstring"),d(ur,"class","docstring"),d(Bv,"id","transformers.TFAutoModelForPreTraining"),d(Bv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Bv,"href","#transformers.TFAutoModelForPreTraining"),d(nc,"class","relative group"),d(ft,"class","docstring"),d(_q,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForPreTraining"),d(uq,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(bq,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForPreTraining"),d(vq,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(Tq,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(Fq,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(Cq,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForPreTraining"),d(Mq,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(Eq,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),d(yq,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(wq,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(Aq,"href","/docs/transformers/pr_15900/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),d(Lq,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),d(Bq,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(xq,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(kq,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(Rq,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(Sq,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(Pq,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d($q,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Iq,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(Dq,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(ho,"class","docstring"),d(br,"class","docstring"),d(Kv,"id","transformers.TFAutoModelForCausalLM"),d(Kv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Kv,"href","#transformers.TFAutoModelForCausalLM"),d(ic,"class","relative group"),d(mt,"class","docstring"),d(jq,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertLMHeadModel"),d(Nq,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(qq,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(Gq,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(Oq,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),d(Xq,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),d(Vq,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),d(zq,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(Wq,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Qq,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(po,"class","docstring"),d(vr,"class","docstring"),d(dT,"id","transformers.TFAutoModelForImageClassification"),d(dT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(dT,"href","#transformers.TFAutoModelForImageClassification"),d(fc,"class","relative group"),d(gt,"class","docstring"),d(Hq,"href","/docs/transformers/pr_15900/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),d(Uq,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.TFViTForImageClassification"),d(_o,"class","docstring"),d(Tr,"class","docstring"),d(mT,"id","transformers.TFAutoModelForMaskedLM"),d(mT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(mT,"href","#transformers.TFAutoModelForMaskedLM"),d(hc,"class","relative group"),d(ht,"class","docstring"),d(Jq,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),d(Yq,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMaskedLM"),d(Kq,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(Zq,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),d(eG,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),d(oG,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),d(rG,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(tG,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMaskedLM"),d(aG,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(nG,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),d(sG,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(lG,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),d(iG,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),d(dG,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(cG,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),d(fG,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(mG,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),d(gG,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(hG,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(pG,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(uo,"class","docstring"),d(Fr,"class","docstring"),d(ST,"id","transformers.TFAutoModelForSeq2SeqLM"),d(ST,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ST,"href","#transformers.TFAutoModelForSeq2SeqLM"),d(uc,"class","relative group"),d(pt,"class","docstring"),d(_G,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(uG,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),d(bG,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),d(vG,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),d(TG,"href","/docs/transformers/pr_15900/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),d(FG,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.TFMarianMTModel"),d(CG,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),d(MG,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),d(EG,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),d(yG,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(bo,"class","docstring"),d(Cr,"class","docstring"),d(VT,"id","transformers.TFAutoModelForSequenceClassification"),d(VT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(VT,"href","#transformers.TFAutoModelForSequenceClassification"),d(Tc,"class","relative group"),d(_t,"class","docstring"),d(wG,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),d(AG,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForSequenceClassification"),d(LG,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),d(BG,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),d(xG,"href","/docs/transformers/pr_15900/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),d(kG,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),d(RG,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),d(SG,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),d(PG,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),d($G,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),d(IG,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),d(DG,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),d(jG,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),d(NG,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),d(qG,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),d(GG,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),d(OG,"href","/docs/transformers/pr_15900/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),d(XG,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),d(VG,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),d(zG,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),d(WG,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),d(QG,"href","/docs/transformers/pr_15900/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),d(HG,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),d(UG,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),d(JG,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),d(vo,"class","docstring"),d(Mr,"class","docstring"),d(_F,"id","transformers.TFAutoModelForMultipleChoice"),d(_F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_F,"href","#transformers.TFAutoModelForMultipleChoice"),d(Mc,"class","relative group"),d(ut,"class","docstring"),d(YG,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),d(KG,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForMultipleChoice"),d(ZG,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),d(eO,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),d(oO,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),d(rO,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),d(tO,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),d(aO,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),d(nO,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),d(sO,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),d(lO,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),d(iO,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),d(dO,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),d(cO,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),d(fO,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),d(mO,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),d(gO,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),d(To,"class","docstring"),d(Er,"class","docstring"),d(PF,"id","transformers.TFAutoModelForTableQuestionAnswering"),d(PF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(PF,"href","#transformers.TFAutoModelForTableQuestionAnswering"),d(wc,"class","relative group"),d(bt,"class","docstring"),d(hO,"href","/docs/transformers/pr_15900/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),d(Fo,"class","docstring"),d(yr,"class","docstring"),d(IF,"id","transformers.TFAutoModelForTokenClassification"),d(IF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(IF,"href","#transformers.TFAutoModelForTokenClassification"),d(Bc,"class","relative group"),d(vt,"class","docstring"),d(pO,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),d(_O,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForTokenClassification"),d(uO,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),d(bO,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),d(vO,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),d(TO,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),d(FO,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),d(CO,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForTokenClassification"),d(MO,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),d(EO,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),d(yO,"href","/docs/transformers/pr_15900/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),d(wO,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),d(AO,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),d(LO,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),d(BO,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),d(xO,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),d(kO,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),d(RO,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),d(SO,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),d(PO,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),d(Co,"class","docstring"),d(wr,"class","docstring"),d(t9,"id","transformers.TFAutoModelForQuestionAnswering"),d(t9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(t9,"href","#transformers.TFAutoModelForQuestionAnswering"),d(Rc,"class","relative group"),d(Tt,"class","docstring"),d($O,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),d(IO,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),d(DO,"href","/docs/transformers/pr_15900/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),d(jO,"href","/docs/transformers/pr_15900/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),d(NO,"href","/docs/transformers/pr_15900/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),d(qO,"href","/docs/transformers/pr_15900/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),d(GO,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),d(OO,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),d(XO,"href","/docs/transformers/pr_15900/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),d(VO,"href","/docs/transformers/pr_15900/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),d(zO,"href","/docs/transformers/pr_15900/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),d(WO,"href","/docs/transformers/pr_15900/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),d(QO,"href","/docs/transformers/pr_15900/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),d(HO,"href","/docs/transformers/pr_15900/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),d(UO,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),d(JO,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),d(YO,"href","/docs/transformers/pr_15900/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),d(KO,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),d(ZO,"href","/docs/transformers/pr_15900/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),d(Mo,"class","docstring"),d(Ar,"class","docstring"),d(M9,"id","transformers.TFAutoModelForVision2Seq"),d(M9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(M9,"href","#transformers.TFAutoModelForVision2Seq"),d($c,"class","relative group"),d(Ft,"class","docstring"),d(eX,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),d(Eo,"class","docstring"),d(Lr,"class","docstring"),d(y9,"id","transformers.TFAutoModelForSpeechSeq2Seq"),d(y9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(y9,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),d(jc,"class","relative group"),d(Ct,"class","docstring"),d(oX,"href","/docs/transformers/pr_15900/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),d(yo,"class","docstring"),d(Br,"class","docstring"),d(A9,"id","transformers.FlaxAutoModel"),d(A9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(A9,"href","#transformers.FlaxAutoModel"),d(Gc,"class","relative group"),d(Mt,"class","docstring"),d(rX,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertModel"),d(tX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartModel"),d(aX,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitModel"),d(nX,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertModel"),d(sX,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),d(lX,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),d(iX,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),d(dX,"href","/docs/transformers/pr_15900/en/model_doc/clip#transformers.FlaxCLIPModel"),d(cX,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),d(fX,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraModel"),d(mX,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2Model"),d(gX,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),d(hX,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJModel"),d(pX,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianModel"),d(_X,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartModel"),d(uX,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5Model"),d(bX,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusModel"),d(vX,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaModel"),d(TX,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerModel"),d(FX,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5Model"),d(CX,"href","/docs/transformers/pr_15900/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),d(MX,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTModel"),d(EX,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),d(yX,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMModel"),d(wX,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaModel"),d(wo,"class","docstring"),d(xr,"class","docstring"),d(Z9,"id","transformers.FlaxAutoModelForCausalLM"),d(Z9,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Z9,"href","#transformers.FlaxAutoModelForCausalLM"),d(Vc,"class","relative group"),d(Et,"class","docstring"),d(AX,"href","/docs/transformers/pr_15900/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),d(LX,"href","/docs/transformers/pr_15900/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),d(BX,"href","/docs/transformers/pr_15900/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),d(xX,"href","/docs/transformers/pr_15900/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),d(Ao,"class","docstring"),d(kr,"class","docstring"),d(aC,"id","transformers.FlaxAutoModelForPreTraining"),d(aC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(aC,"href","#transformers.FlaxAutoModelForPreTraining"),d(Qc,"class","relative group"),d(yt,"class","docstring"),d(kX,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),d(RX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(SX,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForPreTraining"),d(PX,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),d($X,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),d(IX,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(DX,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(jX,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(NX,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(qX,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(GX,"href","/docs/transformers/pr_15900/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),d(OX,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM"),d(Lo,"class","docstring"),d(Rr,"class","docstring"),d(uC,"id","transformers.FlaxAutoModelForMaskedLM"),d(uC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(uC,"href","#transformers.FlaxAutoModelForMaskedLM"),d(Jc,"class","relative group"),d(wt,"class","docstring"),d(XX,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),d(VX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(zX,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),d(WX,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),d(QX,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),d(HX,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),d(UX,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(JX,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(YX,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(KX,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM"),d(Bo,"class","docstring"),d(Sr,"class","docstring"),d(LC,"id","transformers.FlaxAutoModelForSeq2SeqLM"),d(LC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(LC,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),d(Zc,"class","relative group"),d(At,"class","docstring"),d(ZX,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(eV,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),d(oV,"href","/docs/transformers/pr_15900/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),d(rV,"href","/docs/transformers/pr_15900/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),d(tV,"href","/docs/transformers/pr_15900/en/model_doc/marian#transformers.FlaxMarianMTModel"),d(aV,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(nV,"href","/docs/transformers/pr_15900/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(sV,"href","/docs/transformers/pr_15900/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),d(lV,"href","/docs/transformers/pr_15900/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(xo,"class","docstring"),d(Pr,"class","docstring"),d(jC,"id","transformers.FlaxAutoModelForSequenceClassification"),d(jC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(jC,"href","#transformers.FlaxAutoModelForSequenceClassification"),d(rf,"class","relative group"),d(Lt,"class","docstring"),d(iV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),d(dV,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),d(cV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),d(fV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),d(mV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),d(gV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),d(hV,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),d(pV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),d(_V,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),d(uV,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForSequenceClassification"),d(ko,"class","docstring"),d($r,"class","docstring"),d(UC,"id","transformers.FlaxAutoModelForQuestionAnswering"),d(UC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(UC,"href","#transformers.FlaxAutoModelForQuestionAnswering"),d(nf,"class","relative group"),d(Bt,"class","docstring"),d(bV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),d(vV,"href","/docs/transformers/pr_15900/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),d(TV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),d(FV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),d(CV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),d(MV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),d(EV,"href","/docs/transformers/pr_15900/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),d(yV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),d(wV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),d(AV,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForQuestionAnswering"),d(Ro,"class","docstring"),d(Ir,"class","docstring"),d(sM,"id","transformers.FlaxAutoModelForTokenClassification"),d(sM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(sM,"href","#transformers.FlaxAutoModelForTokenClassification"),d(df,"class","relative group"),d(xt,"class","docstring"),d(LV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),d(BV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),d(xV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),d(kV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),d(RV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),d(SV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),d(PV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),d($V,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForTokenClassification"),d(So,"class","docstring"),d(Dr,"class","docstring"),d(pM,"id","transformers.FlaxAutoModelForMultipleChoice"),d(pM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pM,"href","#transformers.FlaxAutoModelForMultipleChoice"),d(mf,"class","relative group"),d(kt,"class","docstring"),d(IV,"href","/docs/transformers/pr_15900/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),d(DV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),d(jV,"href","/docs/transformers/pr_15900/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),d(NV,"href","/docs/transformers/pr_15900/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),d(qV,"href","/docs/transformers/pr_15900/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),d(GV,"href","/docs/transformers/pr_15900/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),d(OV,"href","/docs/transformers/pr_15900/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),d(XV,"href","/docs/transformers/pr_15900/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMultipleChoice"),d(Po,"class","docstring"),d(jr,"class","docstring"),d(EM,"id","transformers.FlaxAutoModelForNextSentencePrediction"),d(EM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(EM,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),d(pf,"class","relative group"),d(Rt,"class","docstring"),d(VV,"href","/docs/transformers/pr_15900/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),d($o,"class","docstring"),d(Nr,"class","docstring"),d(wM,"id","transformers.FlaxAutoModelForImageClassification"),d(wM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wM,"href","#transformers.FlaxAutoModelForImageClassification"),d(bf,"class","relative group"),d(St,"class","docstring"),d(zV,"href","/docs/transformers/pr_15900/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),d(WV,"href","/docs/transformers/pr_15900/en/model_doc/vit#transformers.FlaxViTForImageClassification"),d(Io,"class","docstring"),d(qr,"class","docstring"),d(BM,"id","transformers.FlaxAutoModelForVision2Seq"),d(BM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(BM,"href","#transformers.FlaxAutoModelForVision2Seq"),d(Ff,"class","relative group"),d(Pt,"class","docstring"),d(QV,"href","/docs/transformers/pr_15900/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),d(Do,"class","docstring"),d(Gr,"class","docstring")},m(c,u){e(document.head,J),b(c,Pe,u),b(c,ie,u),e(ie,ge),e(ge,lo),g(fe,lo,null),e(ie,Te),e(ie,Xo),e(Xo,Bi),b(c,yf,u),b(c,sa,u),e(sa,xi),e(sa,ki),e(ki,x4),e(sa,wf),b(c,Le,u),b(c,io,u),e(io,Ri),e(io,In),e(In,k4),e(io,Dn),e(io,jn),e(jn,R4),e(io,Si),e(io,Nn),e(Nn,S4),e(io,Pi),b(c,Af,u),g($a,c,u),b(c,co,u),b(c,he,u),e(he,_8),e(he,$i),e($i,u8),e(he,b8),b(c,Vo,u),b(c,Ia,u),e(Ia,v8),e(Ia,Lf),e(Lf,T8),e(Ia,hSe),b(c,aBe,u),b(c,Ii,u),e(Ii,Bf),e(Bf,jW),g(P4,jW,null),e(Ii,pSe),e(Ii,NW),e(NW,_Se),b(c,nBe,u),b(c,qn,u),e(qn,uSe),e(qn,qW),e(qW,bSe),e(qn,vSe),e(qn,GW),e(GW,TSe),e(qn,FSe),b(c,sBe,u),g($4,c,u),b(c,lBe,u),b(c,F8,u),e(F8,CSe),b(c,iBe,u),g(xf,c,u),b(c,dBe,u),b(c,Di,u),e(Di,kf),e(kf,OW),g(I4,OW,null),e(Di,MSe),e(Di,XW),e(XW,ESe),b(c,cBe,u),b(c,zo,u),g(D4,zo,null),e(zo,ySe),e(zo,j4),e(j4,wSe),e(j4,C8),e(C8,ASe),e(j4,LSe),e(zo,BSe),e(zo,N4),e(N4,xSe),e(N4,VW),e(VW,kSe),e(N4,RSe),e(zo,SSe),e(zo,fo),g(q4,fo,null),e(fo,PSe),e(fo,zW),e(zW,$Se),e(fo,ISe),e(fo,ji),e(ji,DSe),e(ji,WW),e(WW,jSe),e(ji,NSe),e(ji,QW),e(QW,qSe),e(ji,GSe),e(fo,OSe),e(fo,v),e(v,Rf),e(Rf,HW),e(HW,XSe),e(Rf,VSe),e(Rf,M8),e(M8,zSe),e(Rf,WSe),e(v,QSe),e(v,Sf),e(Sf,UW),e(UW,HSe),e(Sf,USe),e(Sf,E8),e(E8,JSe),e(Sf,YSe),e(v,KSe),e(v,Pf),e(Pf,JW),e(JW,ZSe),e(Pf,ePe),e(Pf,y8),e(y8,oPe),e(Pf,rPe),e(v,tPe),e(v,$f),e($f,YW),e(YW,aPe),e($f,nPe),e($f,w8),e(w8,sPe),e($f,lPe),e(v,iPe),e(v,If),e(If,KW),e(KW,dPe),e(If,cPe),e(If,A8),e(A8,fPe),e(If,mPe),e(v,gPe),e(v,Df),e(Df,ZW),e(ZW,hPe),e(Df,pPe),e(Df,L8),e(L8,_Pe),e(Df,uPe),e(v,bPe),e(v,jf),e(jf,eQ),e(eQ,vPe),e(jf,TPe),e(jf,B8),e(B8,FPe),e(jf,CPe),e(v,MPe),e(v,Nf),e(Nf,oQ),e(oQ,EPe),e(Nf,yPe),e(Nf,x8),e(x8,wPe),e(Nf,APe),e(v,LPe),e(v,qf),e(qf,rQ),e(rQ,BPe),e(qf,xPe),e(qf,k8),e(k8,kPe),e(qf,RPe),e(v,SPe),e(v,Gf),e(Gf,tQ),e(tQ,PPe),e(Gf,$Pe),e(Gf,R8),e(R8,IPe),e(Gf,DPe),e(v,jPe),e(v,Of),e(Of,aQ),e(aQ,NPe),e(Of,qPe),e(Of,S8),e(S8,GPe),e(Of,OPe),e(v,XPe),e(v,Xf),e(Xf,nQ),e(nQ,VPe),e(Xf,zPe),e(Xf,P8),e(P8,WPe),e(Xf,QPe),e(v,HPe),e(v,Vf),e(Vf,sQ),e(sQ,UPe),e(Vf,JPe),e(Vf,$8),e($8,YPe),e(Vf,KPe),e(v,ZPe),e(v,zf),e(zf,lQ),e(lQ,e$e),e(zf,o$e),e(zf,I8),e(I8,r$e),e(zf,t$e),e(v,a$e),e(v,Wf),e(Wf,iQ),e(iQ,n$e),e(Wf,s$e),e(Wf,D8),e(D8,l$e),e(Wf,i$e),e(v,d$e),e(v,Qf),e(Qf,dQ),e(dQ,c$e),e(Qf,f$e),e(Qf,j8),e(j8,m$e),e(Qf,g$e),e(v,h$e),e(v,Hf),e(Hf,cQ),e(cQ,p$e),e(Hf,_$e),e(Hf,N8),e(N8,u$e),e(Hf,b$e),e(v,v$e),e(v,Uf),e(Uf,fQ),e(fQ,T$e),e(Uf,F$e),e(Uf,q8),e(q8,C$e),e(Uf,M$e),e(v,E$e),e(v,Jf),e(Jf,mQ),e(mQ,y$e),e(Jf,w$e),e(Jf,G8),e(G8,A$e),e(Jf,L$e),e(v,B$e),e(v,Yf),e(Yf,gQ),e(gQ,x$e),e(Yf,k$e),e(Yf,O8),e(O8,R$e),e(Yf,S$e),e(v,P$e),e(v,Kf),e(Kf,hQ),e(hQ,$$e),e(Kf,I$e),e(Kf,X8),e(X8,D$e),e(Kf,j$e),e(v,N$e),e(v,Zf),e(Zf,pQ),e(pQ,q$e),e(Zf,G$e),e(Zf,V8),e(V8,O$e),e(Zf,X$e),e(v,V$e),e(v,em),e(em,_Q),e(_Q,z$e),e(em,W$e),e(em,z8),e(z8,Q$e),e(em,H$e),e(v,U$e),e(v,om),e(om,uQ),e(uQ,J$e),e(om,Y$e),e(om,W8),e(W8,K$e),e(om,Z$e),e(v,eIe),e(v,rm),e(rm,bQ),e(bQ,oIe),e(rm,rIe),e(rm,Q8),e(Q8,tIe),e(rm,aIe),e(v,nIe),e(v,tm),e(tm,vQ),e(vQ,sIe),e(tm,lIe),e(tm,H8),e(H8,iIe),e(tm,dIe),e(v,cIe),e(v,am),e(am,TQ),e(TQ,fIe),e(am,mIe),e(am,U8),e(U8,gIe),e(am,hIe),e(v,pIe),e(v,nm),e(nm,FQ),e(FQ,_Ie),e(nm,uIe),e(nm,J8),e(J8,bIe),e(nm,vIe),e(v,TIe),e(v,sm),e(sm,CQ),e(CQ,FIe),e(sm,CIe),e(sm,Y8),e(Y8,MIe),e(sm,EIe),e(v,yIe),e(v,lm),e(lm,MQ),e(MQ,wIe),e(lm,AIe),e(lm,K8),e(K8,LIe),e(lm,BIe),e(v,xIe),e(v,im),e(im,EQ),e(EQ,kIe),e(im,RIe),e(im,Z8),e(Z8,SIe),e(im,PIe),e(v,$Ie),e(v,dm),e(dm,yQ),e(yQ,IIe),e(dm,DIe),e(dm,e7),e(e7,jIe),e(dm,NIe),e(v,qIe),e(v,cm),e(cm,wQ),e(wQ,GIe),e(cm,OIe),e(cm,o7),e(o7,XIe),e(cm,VIe),e(v,zIe),e(v,fm),e(fm,AQ),e(AQ,WIe),e(fm,QIe),e(fm,r7),e(r7,HIe),e(fm,UIe),e(v,JIe),e(v,mm),e(mm,LQ),e(LQ,YIe),e(mm,KIe),e(mm,t7),e(t7,ZIe),e(mm,eDe),e(v,oDe),e(v,gm),e(gm,BQ),e(BQ,rDe),e(gm,tDe),e(gm,a7),e(a7,aDe),e(gm,nDe),e(v,sDe),e(v,hm),e(hm,xQ),e(xQ,lDe),e(hm,iDe),e(hm,n7),e(n7,dDe),e(hm,cDe),e(v,fDe),e(v,pm),e(pm,kQ),e(kQ,mDe),e(pm,gDe),e(pm,s7),e(s7,hDe),e(pm,pDe),e(v,_De),e(v,_m),e(_m,RQ),e(RQ,uDe),e(_m,bDe),e(_m,l7),e(l7,vDe),e(_m,TDe),e(v,FDe),e(v,um),e(um,SQ),e(SQ,CDe),e(um,MDe),e(um,i7),e(i7,EDe),e(um,yDe),e(v,wDe),e(v,bm),e(bm,PQ),e(PQ,ADe),e(bm,LDe),e(bm,d7),e(d7,BDe),e(bm,xDe),e(v,kDe),e(v,vm),e(vm,$Q),e($Q,RDe),e(vm,SDe),e(vm,c7),e(c7,PDe),e(vm,$De),e(v,IDe),e(v,Tm),e(Tm,IQ),e(IQ,DDe),e(Tm,jDe),e(Tm,f7),e(f7,NDe),e(Tm,qDe),e(v,GDe),e(v,Fm),e(Fm,DQ),e(DQ,ODe),e(Fm,XDe),e(Fm,m7),e(m7,VDe),e(Fm,zDe),e(v,WDe),e(v,Cm),e(Cm,jQ),e(jQ,QDe),e(Cm,HDe),e(Cm,g7),e(g7,UDe),e(Cm,JDe),e(v,YDe),e(v,Mm),e(Mm,NQ),e(NQ,KDe),e(Mm,ZDe),e(Mm,h7),e(h7,eje),e(Mm,oje),e(v,rje),e(v,Em),e(Em,qQ),e(qQ,tje),e(Em,aje),e(Em,p7),e(p7,nje),e(Em,sje),e(v,lje),e(v,ym),e(ym,GQ),e(GQ,ije),e(ym,dje),e(ym,_7),e(_7,cje),e(ym,fje),e(v,mje),e(v,wm),e(wm,OQ),e(OQ,gje),e(wm,hje),e(wm,u7),e(u7,pje),e(wm,_je),e(v,uje),e(v,Am),e(Am,XQ),e(XQ,bje),e(Am,vje),e(Am,b7),e(b7,Tje),e(Am,Fje),e(v,Cje),e(v,Lm),e(Lm,VQ),e(VQ,Mje),e(Lm,Eje),e(Lm,v7),e(v7,yje),e(Lm,wje),e(v,Aje),e(v,Bm),e(Bm,zQ),e(zQ,Lje),e(Bm,Bje),e(Bm,T7),e(T7,xje),e(Bm,kje),e(v,Rje),e(v,xm),e(xm,WQ),e(WQ,Sje),e(xm,Pje),e(xm,F7),e(F7,$je),e(xm,Ije),e(v,Dje),e(v,km),e(km,QQ),e(QQ,jje),e(km,Nje),e(km,C7),e(C7,qje),e(km,Gje),e(v,Oje),e(v,Rm),e(Rm,HQ),e(HQ,Xje),e(Rm,Vje),e(Rm,M7),e(M7,zje),e(Rm,Wje),e(v,Qje),e(v,Sm),e(Sm,UQ),e(UQ,Hje),e(Sm,Uje),e(Sm,E7),e(E7,Jje),e(Sm,Yje),e(v,Kje),e(v,Pm),e(Pm,JQ),e(JQ,Zje),e(Pm,eNe),e(Pm,y7),e(y7,oNe),e(Pm,rNe),e(v,tNe),e(v,$m),e($m,YQ),e(YQ,aNe),e($m,nNe),e($m,w7),e(w7,sNe),e($m,lNe),e(v,iNe),e(v,Im),e(Im,KQ),e(KQ,dNe),e(Im,cNe),e(Im,A7),e(A7,fNe),e(Im,mNe),e(v,gNe),e(v,Dm),e(Dm,ZQ),e(ZQ,hNe),e(Dm,pNe),e(Dm,L7),e(L7,_Ne),e(Dm,uNe),e(v,bNe),e(v,jm),e(jm,eH),e(eH,vNe),e(jm,TNe),e(jm,B7),e(B7,FNe),e(jm,CNe),e(v,MNe),e(v,Nm),e(Nm,oH),e(oH,ENe),e(Nm,yNe),e(Nm,x7),e(x7,wNe),e(Nm,ANe),e(v,LNe),e(v,qm),e(qm,rH),e(rH,BNe),e(qm,xNe),e(qm,k7),e(k7,kNe),e(qm,RNe),e(v,SNe),e(v,Gm),e(Gm,tH),e(tH,PNe),e(Gm,$Ne),e(Gm,R7),e(R7,INe),e(Gm,DNe),e(v,jNe),e(v,Om),e(Om,aH),e(aH,NNe),e(Om,qNe),e(Om,S7),e(S7,GNe),e(Om,ONe),e(v,XNe),e(v,Xm),e(Xm,nH),e(nH,VNe),e(Xm,zNe),e(Xm,P7),e(P7,WNe),e(Xm,QNe),e(v,HNe),e(v,Vm),e(Vm,sH),e(sH,UNe),e(Vm,JNe),e(Vm,$7),e($7,YNe),e(Vm,KNe),e(v,ZNe),e(v,zm),e(zm,lH),e(lH,eqe),e(zm,oqe),e(zm,I7),e(I7,rqe),e(zm,tqe),e(v,aqe),e(v,Wm),e(Wm,iH),e(iH,nqe),e(Wm,sqe),e(Wm,D7),e(D7,lqe),e(Wm,iqe),e(v,dqe),e(v,Qm),e(Qm,dH),e(dH,cqe),e(Qm,fqe),e(Qm,j7),e(j7,mqe),e(Qm,gqe),e(v,hqe),e(v,Hm),e(Hm,cH),e(cH,pqe),e(Hm,_qe),e(Hm,N7),e(N7,uqe),e(Hm,bqe),e(v,vqe),e(v,Um),e(Um,fH),e(fH,Tqe),e(Um,Fqe),e(Um,q7),e(q7,Cqe),e(Um,Mqe),e(v,Eqe),e(v,Jm),e(Jm,mH),e(mH,yqe),e(Jm,wqe),e(Jm,G7),e(G7,Aqe),e(Jm,Lqe),e(v,Bqe),e(v,Ym),e(Ym,gH),e(gH,xqe),e(Ym,kqe),e(Ym,O7),e(O7,Rqe),e(Ym,Sqe),e(v,Pqe),e(v,Km),e(Km,hH),e(hH,$qe),e(Km,Iqe),e(Km,X7),e(X7,Dqe),e(Km,jqe),e(v,Nqe),e(v,Zm),e(Zm,pH),e(pH,qqe),e(Zm,Gqe),e(Zm,V7),e(V7,Oqe),e(Zm,Xqe),e(v,Vqe),e(v,eg),e(eg,_H),e(_H,zqe),e(eg,Wqe),e(eg,z7),e(z7,Qqe),e(eg,Hqe),e(v,Uqe),e(v,og),e(og,uH),e(uH,Jqe),e(og,Yqe),e(og,W7),e(W7,Kqe),e(og,Zqe),e(v,eGe),e(v,rg),e(rg,bH),e(bH,oGe),e(rg,rGe),e(rg,Q7),e(Q7,tGe),e(rg,aGe),e(v,nGe),e(v,tg),e(tg,vH),e(vH,sGe),e(tg,lGe),e(tg,H7),e(H7,iGe),e(tg,dGe),e(v,cGe),e(v,ag),e(ag,TH),e(TH,fGe),e(ag,mGe),e(ag,U7),e(U7,gGe),e(ag,hGe),e(v,pGe),e(v,ng),e(ng,FH),e(FH,_Ge),e(ng,uGe),e(ng,J7),e(J7,bGe),e(ng,vGe),e(v,TGe),e(v,sg),e(sg,CH),e(CH,FGe),e(sg,CGe),e(sg,Y7),e(Y7,MGe),e(sg,EGe),e(v,yGe),e(v,lg),e(lg,MH),e(MH,wGe),e(lg,AGe),e(lg,K7),e(K7,LGe),e(lg,BGe),e(v,xGe),e(v,ig),e(ig,EH),e(EH,kGe),e(ig,RGe),e(ig,Z7),e(Z7,SGe),e(ig,PGe),e(v,$Ge),e(v,dg),e(dg,yH),e(yH,IGe),e(dg,DGe),e(dg,eB),e(eB,jGe),e(dg,NGe),e(v,qGe),e(v,cg),e(cg,wH),e(wH,GGe),e(cg,OGe),e(cg,oB),e(oB,XGe),e(cg,VGe),e(v,zGe),e(v,fg),e(fg,AH),e(AH,WGe),e(fg,QGe),e(fg,rB),e(rB,HGe),e(fg,UGe),e(v,JGe),e(v,mg),e(mg,LH),e(LH,YGe),e(mg,KGe),e(mg,tB),e(tB,ZGe),e(mg,eOe),e(v,oOe),e(v,gg),e(gg,BH),e(BH,rOe),e(gg,tOe),e(gg,aB),e(aB,aOe),e(gg,nOe),e(v,sOe),e(v,hg),e(hg,xH),e(xH,lOe),e(hg,iOe),e(hg,nB),e(nB,dOe),e(hg,cOe),e(v,fOe),e(v,pg),e(pg,kH),e(kH,mOe),e(pg,gOe),e(pg,sB),e(sB,hOe),e(pg,pOe),e(v,_Oe),e(v,_g),e(_g,RH),e(RH,uOe),e(_g,bOe),e(_g,lB),e(lB,vOe),e(_g,TOe),e(v,FOe),e(v,ug),e(ug,SH),e(SH,COe),e(ug,MOe),e(ug,iB),e(iB,EOe),e(ug,yOe),e(fo,wOe),e(fo,PH),e(PH,AOe),e(fo,LOe),g(G4,fo,null),e(zo,BOe),e(zo,bg),g(O4,bg,null),e(bg,xOe),e(bg,$H),e($H,kOe),b(c,fBe,u),b(c,Ni,u),e(Ni,vg),e(vg,IH),g(X4,IH,null),e(Ni,ROe),e(Ni,DH),e(DH,SOe),b(c,mBe,u),b(c,Wo,u),g(V4,Wo,null),e(Wo,POe),e(Wo,z4),e(z4,$Oe),e(z4,dB),e(dB,IOe),e(z4,DOe),e(Wo,jOe),e(Wo,W4),e(W4,NOe),e(W4,jH),e(jH,qOe),e(W4,GOe),e(Wo,OOe),e(Wo,mo),g(Q4,mo,null),e(mo,XOe),e(mo,NH),e(NH,VOe),e(mo,zOe),e(mo,Da),e(Da,WOe),e(Da,qH),e(qH,QOe),e(Da,HOe),e(Da,GH),e(GH,UOe),e(Da,JOe),e(Da,OH),e(OH,YOe),e(Da,KOe),e(mo,ZOe),e(mo,M),e(M,Gn),e(Gn,XH),e(XH,eXe),e(Gn,oXe),e(Gn,cB),e(cB,rXe),e(Gn,tXe),e(Gn,fB),e(fB,aXe),e(Gn,nXe),e(M,sXe),e(M,On),e(On,VH),e(VH,lXe),e(On,iXe),e(On,mB),e(mB,dXe),e(On,cXe),e(On,gB),e(gB,fXe),e(On,mXe),e(M,gXe),e(M,Xn),e(Xn,zH),e(zH,hXe),e(Xn,pXe),e(Xn,hB),e(hB,_Xe),e(Xn,uXe),e(Xn,pB),e(pB,bXe),e(Xn,vXe),e(M,TXe),e(M,Tg),e(Tg,WH),e(WH,FXe),e(Tg,CXe),e(Tg,_B),e(_B,MXe),e(Tg,EXe),e(M,yXe),e(M,Vn),e(Vn,QH),e(QH,wXe),e(Vn,AXe),e(Vn,uB),e(uB,LXe),e(Vn,BXe),e(Vn,bB),e(bB,xXe),e(Vn,kXe),e(M,RXe),e(M,Fg),e(Fg,HH),e(HH,SXe),e(Fg,PXe),e(Fg,vB),e(vB,$Xe),e(Fg,IXe),e(M,DXe),e(M,Cg),e(Cg,UH),e(UH,jXe),e(Cg,NXe),e(Cg,TB),e(TB,qXe),e(Cg,GXe),e(M,OXe),e(M,Mg),e(Mg,JH),e(JH,XXe),e(Mg,VXe),e(Mg,FB),e(FB,zXe),e(Mg,WXe),e(M,QXe),e(M,zn),e(zn,YH),e(YH,HXe),e(zn,UXe),e(zn,CB),e(CB,JXe),e(zn,YXe),e(zn,MB),e(MB,KXe),e(zn,ZXe),e(M,eVe),e(M,Wn),e(Wn,KH),e(KH,oVe),e(Wn,rVe),e(Wn,EB),e(EB,tVe),e(Wn,aVe),e(Wn,yB),e(yB,nVe),e(Wn,sVe),e(M,lVe),e(M,Qn),e(Qn,ZH),e(ZH,iVe),e(Qn,dVe),e(Qn,wB),e(wB,cVe),e(Qn,fVe),e(Qn,AB),e(AB,mVe),e(Qn,gVe),e(M,hVe),e(M,Eg),e(Eg,eU),e(eU,pVe),e(Eg,_Ve),e(Eg,LB),e(LB,uVe),e(Eg,bVe),e(M,vVe),e(M,yg),e(yg,oU),e(oU,TVe),e(yg,FVe),e(yg,BB),e(BB,CVe),e(yg,MVe),e(M,EVe),e(M,Hn),e(Hn,rU),e(rU,yVe),e(Hn,wVe),e(Hn,xB),e(xB,AVe),e(Hn,LVe),e(Hn,kB),e(kB,BVe),e(Hn,xVe),e(M,kVe),e(M,wg),e(wg,tU),e(tU,RVe),e(wg,SVe),e(wg,RB),e(RB,PVe),e(wg,$Ve),e(M,IVe),e(M,Un),e(Un,aU),e(aU,DVe),e(Un,jVe),e(Un,SB),e(SB,NVe),e(Un,qVe),e(Un,PB),e(PB,GVe),e(Un,OVe),e(M,XVe),e(M,Jn),e(Jn,nU),e(nU,VVe),e(Jn,zVe),e(Jn,$B),e($B,WVe),e(Jn,QVe),e(Jn,IB),e(IB,HVe),e(Jn,UVe),e(M,JVe),e(M,Yn),e(Yn,sU),e(sU,YVe),e(Yn,KVe),e(Yn,DB),e(DB,ZVe),e(Yn,eze),e(Yn,lU),e(lU,oze),e(Yn,rze),e(M,tze),e(M,Ag),e(Ag,iU),e(iU,aze),e(Ag,nze),e(Ag,jB),e(jB,sze),e(Ag,lze),e(M,ize),e(M,Kn),e(Kn,dU),e(dU,dze),e(Kn,cze),e(Kn,NB),e(NB,fze),e(Kn,mze),e(Kn,qB),e(qB,gze),e(Kn,hze),e(M,pze),e(M,Lg),e(Lg,cU),e(cU,_ze),e(Lg,uze),e(Lg,GB),e(GB,bze),e(Lg,vze),e(M,Tze),e(M,Zn),e(Zn,fU),e(fU,Fze),e(Zn,Cze),e(Zn,OB),e(OB,Mze),e(Zn,Eze),e(Zn,XB),e(XB,yze),e(Zn,wze),e(M,Aze),e(M,es),e(es,mU),e(mU,Lze),e(es,Bze),e(es,VB),e(VB,xze),e(es,kze),e(es,zB),e(zB,Rze),e(es,Sze),e(M,Pze),e(M,os),e(os,gU),e(gU,$ze),e(os,Ize),e(os,WB),e(WB,Dze),e(os,jze),e(os,QB),e(QB,Nze),e(os,qze),e(M,Gze),e(M,Bg),e(Bg,hU),e(hU,Oze),e(Bg,Xze),e(Bg,HB),e(HB,Vze),e(Bg,zze),e(M,Wze),e(M,rs),e(rs,pU),e(pU,Qze),e(rs,Hze),e(rs,UB),e(UB,Uze),e(rs,Jze),e(rs,JB),e(JB,Yze),e(rs,Kze),e(M,Zze),e(M,xg),e(xg,_U),e(_U,eWe),e(xg,oWe),e(xg,YB),e(YB,rWe),e(xg,tWe),e(M,aWe),e(M,ts),e(ts,uU),e(uU,nWe),e(ts,sWe),e(ts,KB),e(KB,lWe),e(ts,iWe),e(ts,ZB),e(ZB,dWe),e(ts,cWe),e(M,fWe),e(M,as),e(as,bU),e(bU,mWe),e(as,gWe),e(as,ex),e(ex,hWe),e(as,pWe),e(as,ox),e(ox,_We),e(as,uWe),e(M,bWe),e(M,ns),e(ns,vU),e(vU,vWe),e(ns,TWe),e(ns,rx),e(rx,FWe),e(ns,CWe),e(ns,tx),e(tx,MWe),e(ns,EWe),e(M,yWe),e(M,ss),e(ss,TU),e(TU,wWe),e(ss,AWe),e(ss,ax),e(ax,LWe),e(ss,BWe),e(ss,nx),e(nx,xWe),e(ss,kWe),e(M,RWe),e(M,kg),e(kg,FU),e(FU,SWe),e(kg,PWe),e(kg,sx),e(sx,$We),e(kg,IWe),e(M,DWe),e(M,ls),e(ls,CU),e(CU,jWe),e(ls,NWe),e(ls,lx),e(lx,qWe),e(ls,GWe),e(ls,ix),e(ix,OWe),e(ls,XWe),e(M,VWe),e(M,is),e(is,MU),e(MU,zWe),e(is,WWe),e(is,dx),e(dx,QWe),e(is,HWe),e(is,cx),e(cx,UWe),e(is,JWe),e(M,YWe),e(M,ds),e(ds,EU),e(EU,KWe),e(ds,ZWe),e(ds,fx),e(fx,eQe),e(ds,oQe),e(ds,mx),e(mx,rQe),e(ds,tQe),e(M,aQe),e(M,cs),e(cs,yU),e(yU,nQe),e(cs,sQe),e(cs,gx),e(gx,lQe),e(cs,iQe),e(cs,hx),e(hx,dQe),e(cs,cQe),e(M,fQe),e(M,fs),e(fs,wU),e(wU,mQe),e(fs,gQe),e(fs,px),e(px,hQe),e(fs,pQe),e(fs,_x),e(_x,_Qe),e(fs,uQe),e(M,bQe),e(M,ms),e(ms,AU),e(AU,vQe),e(ms,TQe),e(ms,ux),e(ux,FQe),e(ms,CQe),e(ms,bx),e(bx,MQe),e(ms,EQe),e(M,yQe),e(M,Rg),e(Rg,LU),e(LU,wQe),e(Rg,AQe),e(Rg,vx),e(vx,LQe),e(Rg,BQe),e(M,xQe),e(M,gs),e(gs,BU),e(BU,kQe),e(gs,RQe),e(gs,Tx),e(Tx,SQe),e(gs,PQe),e(gs,Fx),e(Fx,$Qe),e(gs,IQe),e(M,DQe),e(M,Sg),e(Sg,xU),e(xU,jQe),e(Sg,NQe),e(Sg,Cx),e(Cx,qQe),e(Sg,GQe),e(M,OQe),e(M,Pg),e(Pg,kU),e(kU,XQe),e(Pg,VQe),e(Pg,Mx),e(Mx,zQe),e(Pg,WQe),e(M,QQe),e(M,hs),e(hs,RU),e(RU,HQe),e(hs,UQe),e(hs,Ex),e(Ex,JQe),e(hs,YQe),e(hs,yx),e(yx,KQe),e(hs,ZQe),e(M,eHe),e(M,ps),e(ps,SU),e(SU,oHe),e(ps,rHe),e(ps,wx),e(wx,tHe),e(ps,aHe),e(ps,Ax),e(Ax,nHe),e(ps,sHe),e(M,lHe),e(M,$g),e($g,PU),e(PU,iHe),e($g,dHe),e($g,Lx),e(Lx,cHe),e($g,fHe),e(M,mHe),e(M,_s),e(_s,$U),e($U,gHe),e(_s,hHe),e(_s,Bx),e(Bx,pHe),e(_s,_He),e(_s,xx),e(xx,uHe),e(_s,bHe),e(M,vHe),e(M,us),e(us,IU),e(IU,THe),e(us,FHe),e(us,kx),e(kx,CHe),e(us,MHe),e(us,Rx),e(Rx,EHe),e(us,yHe),e(M,wHe),e(M,bs),e(bs,DU),e(DU,AHe),e(bs,LHe),e(bs,Sx),e(Sx,BHe),e(bs,xHe),e(bs,Px),e(Px,kHe),e(bs,RHe),e(M,SHe),e(M,vs),e(vs,jU),e(jU,PHe),e(vs,$He),e(vs,$x),e($x,IHe),e(vs,DHe),e(vs,Ix),e(Ix,jHe),e(vs,NHe),e(M,qHe),e(M,Ts),e(Ts,NU),e(NU,GHe),e(Ts,OHe),e(Ts,Dx),e(Dx,XHe),e(Ts,VHe),e(Ts,jx),e(jx,zHe),e(Ts,WHe),e(M,QHe),e(M,Ig),e(Ig,qU),e(qU,HHe),e(Ig,UHe),e(Ig,Nx),e(Nx,JHe),e(Ig,YHe),e(M,KHe),e(M,Dg),e(Dg,GU),e(GU,ZHe),e(Dg,eUe),e(Dg,qx),e(qx,oUe),e(Dg,rUe),e(M,tUe),e(M,jg),e(jg,OU),e(OU,aUe),e(jg,nUe),e(jg,Gx),e(Gx,sUe),e(jg,lUe),e(M,iUe),e(M,Ng),e(Ng,XU),e(XU,dUe),e(Ng,cUe),e(Ng,Ox),e(Ox,fUe),e(Ng,mUe),e(M,gUe),e(M,Fs),e(Fs,VU),e(VU,hUe),e(Fs,pUe),e(Fs,Xx),e(Xx,_Ue),e(Fs,uUe),e(Fs,Vx),e(Vx,bUe),e(Fs,vUe),e(M,TUe),e(M,qg),e(qg,zU),e(zU,FUe),e(qg,CUe),e(qg,zx),e(zx,MUe),e(qg,EUe),e(M,yUe),e(M,Cs),e(Cs,WU),e(WU,wUe),e(Cs,AUe),e(Cs,Wx),e(Wx,LUe),e(Cs,BUe),e(Cs,Qx),e(Qx,xUe),e(Cs,kUe),e(M,RUe),e(M,Ms),e(Ms,QU),e(QU,SUe),e(Ms,PUe),e(Ms,Hx),e(Hx,$Ue),e(Ms,IUe),e(Ms,Ux),e(Ux,DUe),e(Ms,jUe),e(M,NUe),e(M,Es),e(Es,HU),e(HU,qUe),e(Es,GUe),e(Es,Jx),e(Jx,OUe),e(Es,XUe),e(Es,Yx),e(Yx,VUe),e(Es,zUe),e(M,WUe),e(M,ys),e(ys,UU),e(UU,QUe),e(ys,HUe),e(ys,Kx),e(Kx,UUe),e(ys,JUe),e(ys,Zx),e(Zx,YUe),e(ys,KUe),e(M,ZUe),e(M,ws),e(ws,JU),e(JU,eJe),e(ws,oJe),e(ws,ek),e(ek,rJe),e(ws,tJe),e(ws,ok),e(ok,aJe),e(ws,nJe),e(M,sJe),e(M,As),e(As,YU),e(YU,lJe),e(As,iJe),e(As,rk),e(rk,dJe),e(As,cJe),e(As,tk),e(tk,fJe),e(As,mJe),e(M,gJe),e(M,Gg),e(Gg,KU),e(KU,hJe),e(Gg,pJe),e(Gg,ak),e(ak,_Je),e(Gg,uJe),e(M,bJe),e(M,Og),e(Og,ZU),e(ZU,vJe),e(Og,TJe),e(Og,nk),e(nk,FJe),e(Og,CJe),e(M,MJe),e(M,Ls),e(Ls,eJ),e(eJ,EJe),e(Ls,yJe),e(Ls,sk),e(sk,wJe),e(Ls,AJe),e(Ls,lk),e(lk,LJe),e(Ls,BJe),e(M,xJe),e(M,Bs),e(Bs,oJ),e(oJ,kJe),e(Bs,RJe),e(Bs,ik),e(ik,SJe),e(Bs,PJe),e(Bs,dk),e(dk,$Je),e(Bs,IJe),e(M,DJe),e(M,xs),e(xs,rJ),e(rJ,jJe),e(xs,NJe),e(xs,ck),e(ck,qJe),e(xs,GJe),e(xs,fk),e(fk,OJe),e(xs,XJe),e(M,VJe),e(M,Xg),e(Xg,tJ),e(tJ,zJe),e(Xg,WJe),e(Xg,mk),e(mk,QJe),e(Xg,HJe),e(M,UJe),e(M,Vg),e(Vg,aJ),e(aJ,JJe),e(Vg,YJe),e(Vg,gk),e(gk,KJe),e(Vg,ZJe),e(M,eYe),e(M,zg),e(zg,nJ),e(nJ,oYe),e(zg,rYe),e(zg,hk),e(hk,tYe),e(zg,aYe),e(M,nYe),e(M,Wg),e(Wg,sJ),e(sJ,sYe),e(Wg,lYe),e(Wg,pk),e(pk,iYe),e(Wg,dYe),e(M,cYe),e(M,ks),e(ks,lJ),e(lJ,fYe),e(ks,mYe),e(ks,_k),e(_k,gYe),e(ks,hYe),e(ks,uk),e(uk,pYe),e(ks,_Ye),e(M,uYe),e(M,Qg),e(Qg,iJ),e(iJ,bYe),e(Qg,vYe),e(Qg,bk),e(bk,TYe),e(Qg,FYe),e(M,CYe),e(M,Hg),e(Hg,dJ),e(dJ,MYe),e(Hg,EYe),e(Hg,vk),e(vk,yYe),e(Hg,wYe),e(M,AYe),e(M,Rs),e(Rs,cJ),e(cJ,LYe),e(Rs,BYe),e(Rs,Tk),e(Tk,xYe),e(Rs,kYe),e(Rs,Fk),e(Fk,RYe),e(Rs,SYe),e(M,PYe),e(M,Ss),e(Ss,fJ),e(fJ,$Ye),e(Ss,IYe),e(Ss,Ck),e(Ck,DYe),e(Ss,jYe),e(Ss,Mk),e(Mk,NYe),e(Ss,qYe),e(mo,GYe),e(mo,mJ),e(mJ,OYe),e(mo,XYe),g(H4,mo,null),e(Wo,VYe),e(Wo,Ug),g(U4,Ug,null),e(Ug,zYe),e(Ug,gJ),e(gJ,WYe),b(c,gBe,u),b(c,qi,u),e(qi,Jg),e(Jg,hJ),g(J4,hJ,null),e(qi,QYe),e(qi,pJ),e(pJ,HYe),b(c,hBe,u),b(c,Qo,u),g(Y4,Qo,null),e(Qo,UYe),e(Qo,K4),e(K4,JYe),e(K4,Ek),e(Ek,YYe),e(K4,KYe),e(Qo,ZYe),e(Qo,Z4),e(Z4,eKe),e(Z4,_J),e(_J,oKe),e(Z4,rKe),e(Qo,tKe),e(Qo,$e),g(eE,$e,null),e($e,aKe),e($e,uJ),e(uJ,nKe),e($e,sKe),e($e,ja),e(ja,lKe),e(ja,bJ),e(bJ,iKe),e(ja,dKe),e(ja,vJ),e(vJ,cKe),e(ja,fKe),e(ja,TJ),e(TJ,mKe),e(ja,gKe),e($e,hKe),e($e,se),e(se,Yg),e(Yg,FJ),e(FJ,pKe),e(Yg,_Ke),e(Yg,yk),e(yk,uKe),e(Yg,bKe),e(se,vKe),e(se,Kg),e(Kg,CJ),e(CJ,TKe),e(Kg,FKe),e(Kg,wk),e(wk,CKe),e(Kg,MKe),e(se,EKe),e(se,Zg),e(Zg,MJ),e(MJ,yKe),e(Zg,wKe),e(Zg,Ak),e(Ak,AKe),e(Zg,LKe),e(se,BKe),e(se,eh),e(eh,EJ),e(EJ,xKe),e(eh,kKe),e(eh,Lk),e(Lk,RKe),e(eh,SKe),e(se,PKe),e(se,oh),e(oh,yJ),e(yJ,$Ke),e(oh,IKe),e(oh,Bk),e(Bk,DKe),e(oh,jKe),e(se,NKe),e(se,rh),e(rh,wJ),e(wJ,qKe),e(rh,GKe),e(rh,xk),e(xk,OKe),e(rh,XKe),e(se,VKe),e(se,th),e(th,AJ),e(AJ,zKe),e(th,WKe),e(th,kk),e(kk,QKe),e(th,HKe),e(se,UKe),e(se,ah),e(ah,LJ),e(LJ,JKe),e(ah,YKe),e(ah,Rk),e(Rk,KKe),e(ah,ZKe),e(se,eZe),e(se,nh),e(nh,BJ),e(BJ,oZe),e(nh,rZe),e(nh,Sk),e(Sk,tZe),e(nh,aZe),e(se,nZe),e(se,sh),e(sh,xJ),e(xJ,sZe),e(sh,lZe),e(sh,Pk),e(Pk,iZe),e(sh,dZe),e(se,cZe),e(se,lh),e(lh,kJ),e(kJ,fZe),e(lh,mZe),e(lh,$k),e($k,gZe),e(lh,hZe),e(se,pZe),e(se,ih),e(ih,RJ),e(RJ,_Ze),e(ih,uZe),e(ih,Ik),e(Ik,bZe),e(ih,vZe),e(se,TZe),e(se,dh),e(dh,SJ),e(SJ,FZe),e(dh,CZe),e(dh,Dk),e(Dk,MZe),e(dh,EZe),e(se,yZe),e(se,ch),e(ch,PJ),e(PJ,wZe),e(ch,AZe),e(ch,jk),e(jk,LZe),e(ch,BZe),e(se,xZe),e(se,fh),e(fh,$J),e($J,kZe),e(fh,RZe),e(fh,Nk),e(Nk,SZe),e(fh,PZe),e($e,$Ze),g(mh,$e,null),e($e,IZe),e($e,IJ),e(IJ,DZe),e($e,jZe),g(oE,$e,null),e(Qo,NZe),e(Qo,gh),g(rE,gh,null),e(gh,qZe),e(gh,DJ),e(DJ,GZe),b(c,pBe,u),b(c,Gi,u),e(Gi,hh),e(hh,jJ),g(tE,jJ,null),e(Gi,OZe),e(Gi,NJ),e(NJ,XZe),b(c,_Be,u),b(c,Ho,u),g(aE,Ho,null),e(Ho,VZe),e(Ho,nE),e(nE,zZe),e(nE,qk),e(qk,WZe),e(nE,QZe),e(Ho,HZe),e(Ho,sE),e(sE,UZe),e(sE,qJ),e(qJ,JZe),e(sE,YZe),e(Ho,KZe),e(Ho,Ie),g(lE,Ie,null),e(Ie,ZZe),e(Ie,GJ),e(GJ,eeo),e(Ie,oeo),e(Ie,Oi),e(Oi,reo),e(Oi,OJ),e(OJ,teo),e(Oi,aeo),e(Oi,XJ),e(XJ,neo),e(Oi,seo),e(Ie,leo),e(Ie,Be),e(Be,ph),e(ph,VJ),e(VJ,ieo),e(ph,deo),e(ph,Gk),e(Gk,ceo),e(ph,feo),e(Be,meo),e(Be,_h),e(_h,zJ),e(zJ,geo),e(_h,heo),e(_h,Ok),e(Ok,peo),e(_h,_eo),e(Be,ueo),e(Be,uh),e(uh,WJ),e(WJ,beo),e(uh,veo),e(uh,Xk),e(Xk,Teo),e(uh,Feo),e(Be,Ceo),e(Be,bh),e(bh,QJ),e(QJ,Meo),e(bh,Eeo),e(bh,Vk),e(Vk,yeo),e(bh,weo),e(Be,Aeo),e(Be,vh),e(vh,HJ),e(HJ,Leo),e(vh,Beo),e(vh,zk),e(zk,xeo),e(vh,keo),e(Be,Reo),e(Be,Th),e(Th,UJ),e(UJ,Seo),e(Th,Peo),e(Th,Wk),e(Wk,$eo),e(Th,Ieo),e(Be,Deo),e(Be,Fh),e(Fh,JJ),e(JJ,jeo),e(Fh,Neo),e(Fh,Qk),e(Qk,qeo),e(Fh,Geo),e(Be,Oeo),e(Be,Ch),e(Ch,YJ),e(YJ,Xeo),e(Ch,Veo),e(Ch,Hk),e(Hk,zeo),e(Ch,Weo),e(Ie,Qeo),g(Mh,Ie,null),e(Ie,Heo),e(Ie,KJ),e(KJ,Ueo),e(Ie,Jeo),g(iE,Ie,null),e(Ho,Yeo),e(Ho,Eh),g(dE,Eh,null),e(Eh,Keo),e(Eh,ZJ),e(ZJ,Zeo),b(c,uBe,u),b(c,Xi,u),e(Xi,yh),e(yh,eY),g(cE,eY,null),e(Xi,eoo),e(Xi,oY),e(oY,ooo),b(c,bBe,u),b(c,Uo,u),g(fE,Uo,null),e(Uo,roo),e(Uo,Vi),e(Vi,too),e(Vi,rY),e(rY,aoo),e(Vi,noo),e(Vi,tY),e(tY,soo),e(Vi,loo),e(Uo,ioo),e(Uo,mE),e(mE,doo),e(mE,aY),e(aY,coo),e(mE,foo),e(Uo,moo),e(Uo,Or),g(gE,Or,null),e(Or,goo),e(Or,nY),e(nY,hoo),e(Or,poo),e(Or,zi),e(zi,_oo),e(zi,sY),e(sY,uoo),e(zi,boo),e(zi,lY),e(lY,voo),e(zi,Too),e(Or,Foo),e(Or,iY),e(iY,Coo),e(Or,Moo),g(hE,Or,null),e(Uo,Eoo),e(Uo,De),g(pE,De,null),e(De,yoo),e(De,dY),e(dY,woo),e(De,Aoo),e(De,Na),e(Na,Loo),e(Na,cY),e(cY,Boo),e(Na,xoo),e(Na,fY),e(fY,koo),e(Na,Roo),e(Na,mY),e(mY,Soo),e(Na,Poo),e(De,$oo),e(De,F),e(F,wh),e(wh,gY),e(gY,Ioo),e(wh,Doo),e(wh,Uk),e(Uk,joo),e(wh,Noo),e(F,qoo),e(F,Ah),e(Ah,hY),e(hY,Goo),e(Ah,Ooo),e(Ah,Jk),e(Jk,Xoo),e(Ah,Voo),e(F,zoo),e(F,Lh),e(Lh,pY),e(pY,Woo),e(Lh,Qoo),e(Lh,Yk),e(Yk,Hoo),e(Lh,Uoo),e(F,Joo),e(F,Bh),e(Bh,_Y),e(_Y,Yoo),e(Bh,Koo),e(Bh,Kk),e(Kk,Zoo),e(Bh,ero),e(F,oro),e(F,xh),e(xh,uY),e(uY,rro),e(xh,tro),e(xh,Zk),e(Zk,aro),e(xh,nro),e(F,sro),e(F,kh),e(kh,bY),e(bY,lro),e(kh,iro),e(kh,eR),e(eR,dro),e(kh,cro),e(F,fro),e(F,Rh),e(Rh,vY),e(vY,mro),e(Rh,gro),e(Rh,oR),e(oR,hro),e(Rh,pro),e(F,_ro),e(F,Sh),e(Sh,TY),e(TY,uro),e(Sh,bro),e(Sh,rR),e(rR,vro),e(Sh,Tro),e(F,Fro),e(F,Ph),e(Ph,FY),e(FY,Cro),e(Ph,Mro),e(Ph,tR),e(tR,Ero),e(Ph,yro),e(F,wro),e(F,$h),e($h,CY),e(CY,Aro),e($h,Lro),e($h,aR),e(aR,Bro),e($h,xro),e(F,kro),e(F,Ih),e(Ih,MY),e(MY,Rro),e(Ih,Sro),e(Ih,nR),e(nR,Pro),e(Ih,$ro),e(F,Iro),e(F,Dh),e(Dh,EY),e(EY,Dro),e(Dh,jro),e(Dh,sR),e(sR,Nro),e(Dh,qro),e(F,Gro),e(F,jh),e(jh,yY),e(yY,Oro),e(jh,Xro),e(jh,lR),e(lR,Vro),e(jh,zro),e(F,Wro),e(F,Nh),e(Nh,wY),e(wY,Qro),e(Nh,Hro),e(Nh,iR),e(iR,Uro),e(Nh,Jro),e(F,Yro),e(F,qh),e(qh,AY),e(AY,Kro),e(qh,Zro),e(qh,dR),e(dR,eto),e(qh,oto),e(F,rto),e(F,Gh),e(Gh,LY),e(LY,tto),e(Gh,ato),e(Gh,cR),e(cR,nto),e(Gh,sto),e(F,lto),e(F,Oh),e(Oh,BY),e(BY,ito),e(Oh,dto),e(Oh,fR),e(fR,cto),e(Oh,fto),e(F,mto),e(F,Xh),e(Xh,xY),e(xY,gto),e(Xh,hto),e(Xh,mR),e(mR,pto),e(Xh,_to),e(F,uto),e(F,Vh),e(Vh,kY),e(kY,bto),e(Vh,vto),e(Vh,gR),e(gR,Tto),e(Vh,Fto),e(F,Cto),e(F,zh),e(zh,RY),e(RY,Mto),e(zh,Eto),e(zh,hR),e(hR,yto),e(zh,wto),e(F,Ato),e(F,Wh),e(Wh,SY),e(SY,Lto),e(Wh,Bto),e(Wh,pR),e(pR,xto),e(Wh,kto),e(F,Rto),e(F,Qh),e(Qh,PY),e(PY,Sto),e(Qh,Pto),e(Qh,_R),e(_R,$to),e(Qh,Ito),e(F,Dto),e(F,Hh),e(Hh,$Y),e($Y,jto),e(Hh,Nto),e(Hh,uR),e(uR,qto),e(Hh,Gto),e(F,Oto),e(F,Uh),e(Uh,IY),e(IY,Xto),e(Uh,Vto),e(Uh,bR),e(bR,zto),e(Uh,Wto),e(F,Qto),e(F,Jh),e(Jh,DY),e(DY,Hto),e(Jh,Uto),e(Jh,vR),e(vR,Jto),e(Jh,Yto),e(F,Kto),e(F,Yh),e(Yh,jY),e(jY,Zto),e(Yh,eao),e(Yh,TR),e(TR,oao),e(Yh,rao),e(F,tao),e(F,Kh),e(Kh,NY),e(NY,aao),e(Kh,nao),e(Kh,FR),e(FR,sao),e(Kh,lao),e(F,iao),e(F,Ps),e(Ps,qY),e(qY,dao),e(Ps,cao),e(Ps,CR),e(CR,fao),e(Ps,mao),e(Ps,MR),e(MR,gao),e(Ps,hao),e(F,pao),e(F,Zh),e(Zh,GY),e(GY,_ao),e(Zh,uao),e(Zh,ER),e(ER,bao),e(Zh,vao),e(F,Tao),e(F,ep),e(ep,OY),e(OY,Fao),e(ep,Cao),e(ep,yR),e(yR,Mao),e(ep,Eao),e(F,yao),e(F,op),e(op,XY),e(XY,wao),e(op,Aao),e(op,wR),e(wR,Lao),e(op,Bao),e(F,xao),e(F,rp),e(rp,VY),e(VY,kao),e(rp,Rao),e(rp,AR),e(AR,Sao),e(rp,Pao),e(F,$ao),e(F,tp),e(tp,zY),e(zY,Iao),e(tp,Dao),e(tp,LR),e(LR,jao),e(tp,Nao),e(F,qao),e(F,ap),e(ap,WY),e(WY,Gao),e(ap,Oao),e(ap,BR),e(BR,Xao),e(ap,Vao),e(F,zao),e(F,np),e(np,QY),e(QY,Wao),e(np,Qao),e(np,xR),e(xR,Hao),e(np,Uao),e(F,Jao),e(F,sp),e(sp,HY),e(HY,Yao),e(sp,Kao),e(sp,kR),e(kR,Zao),e(sp,eno),e(F,ono),e(F,lp),e(lp,UY),e(UY,rno),e(lp,tno),e(lp,RR),e(RR,ano),e(lp,nno),e(F,sno),e(F,ip),e(ip,JY),e(JY,lno),e(ip,ino),e(ip,SR),e(SR,dno),e(ip,cno),e(F,fno),e(F,dp),e(dp,YY),e(YY,mno),e(dp,gno),e(dp,PR),e(PR,hno),e(dp,pno),e(F,_no),e(F,cp),e(cp,KY),e(KY,uno),e(cp,bno),e(cp,$R),e($R,vno),e(cp,Tno),e(F,Fno),e(F,fp),e(fp,ZY),e(ZY,Cno),e(fp,Mno),e(fp,IR),e(IR,Eno),e(fp,yno),e(F,wno),e(F,mp),e(mp,eK),e(eK,Ano),e(mp,Lno),e(mp,DR),e(DR,Bno),e(mp,xno),e(F,kno),e(F,gp),e(gp,oK),e(oK,Rno),e(gp,Sno),e(gp,jR),e(jR,Pno),e(gp,$no),e(F,Ino),e(F,hp),e(hp,rK),e(rK,Dno),e(hp,jno),e(hp,NR),e(NR,Nno),e(hp,qno),e(F,Gno),e(F,pp),e(pp,tK),e(tK,Ono),e(pp,Xno),e(pp,qR),e(qR,Vno),e(pp,zno),e(F,Wno),e(F,_p),e(_p,aK),e(aK,Qno),e(_p,Hno),e(_p,GR),e(GR,Uno),e(_p,Jno),e(F,Yno),e(F,up),e(up,nK),e(nK,Kno),e(up,Zno),e(up,OR),e(OR,eso),e(up,oso),e(F,rso),e(F,bp),e(bp,sK),e(sK,tso),e(bp,aso),e(bp,XR),e(XR,nso),e(bp,sso),e(F,lso),e(F,vp),e(vp,lK),e(lK,iso),e(vp,dso),e(vp,VR),e(VR,cso),e(vp,fso),e(F,mso),e(F,Tp),e(Tp,iK),e(iK,gso),e(Tp,hso),e(Tp,zR),e(zR,pso),e(Tp,_so),e(F,uso),e(F,Fp),e(Fp,dK),e(dK,bso),e(Fp,vso),e(Fp,WR),e(WR,Tso),e(Fp,Fso),e(F,Cso),e(F,Cp),e(Cp,cK),e(cK,Mso),e(Cp,Eso),e(Cp,QR),e(QR,yso),e(Cp,wso),e(F,Aso),e(F,Mp),e(Mp,fK),e(fK,Lso),e(Mp,Bso),e(Mp,HR),e(HR,xso),e(Mp,kso),e(F,Rso),e(F,Ep),e(Ep,mK),e(mK,Sso),e(Ep,Pso),e(Ep,UR),e(UR,$so),e(Ep,Iso),e(F,Dso),e(F,yp),e(yp,gK),e(gK,jso),e(yp,Nso),e(yp,JR),e(JR,qso),e(yp,Gso),e(F,Oso),e(F,wp),e(wp,hK),e(hK,Xso),e(wp,Vso),e(wp,YR),e(YR,zso),e(wp,Wso),e(F,Qso),e(F,Ap),e(Ap,pK),e(pK,Hso),e(Ap,Uso),e(Ap,KR),e(KR,Jso),e(Ap,Yso),e(F,Kso),e(F,Lp),e(Lp,_K),e(_K,Zso),e(Lp,elo),e(Lp,ZR),e(ZR,olo),e(Lp,rlo),e(F,tlo),e(F,Bp),e(Bp,uK),e(uK,alo),e(Bp,nlo),e(Bp,eS),e(eS,slo),e(Bp,llo),e(F,ilo),e(F,xp),e(xp,bK),e(bK,dlo),e(xp,clo),e(xp,oS),e(oS,flo),e(xp,mlo),e(F,glo),e(F,kp),e(kp,vK),e(vK,hlo),e(kp,plo),e(kp,rS),e(rS,_lo),e(kp,ulo),e(F,blo),e(F,Rp),e(Rp,TK),e(TK,vlo),e(Rp,Tlo),e(Rp,tS),e(tS,Flo),e(Rp,Clo),e(F,Mlo),e(F,Sp),e(Sp,FK),e(FK,Elo),e(Sp,ylo),e(Sp,aS),e(aS,wlo),e(Sp,Alo),e(F,Llo),e(F,Pp),e(Pp,CK),e(CK,Blo),e(Pp,xlo),e(Pp,nS),e(nS,klo),e(Pp,Rlo),e(F,Slo),e(F,$p),e($p,MK),e(MK,Plo),e($p,$lo),e($p,sS),e(sS,Ilo),e($p,Dlo),e(F,jlo),e(F,Ip),e(Ip,EK),e(EK,Nlo),e(Ip,qlo),e(Ip,lS),e(lS,Glo),e(Ip,Olo),e(F,Xlo),e(F,Dp),e(Dp,yK),e(yK,Vlo),e(Dp,zlo),e(Dp,iS),e(iS,Wlo),e(Dp,Qlo),e(F,Hlo),e(F,jp),e(jp,wK),e(wK,Ulo),e(jp,Jlo),e(jp,dS),e(dS,Ylo),e(jp,Klo),e(F,Zlo),e(F,Np),e(Np,AK),e(AK,eio),e(Np,oio),e(Np,cS),e(cS,rio),e(Np,tio),e(F,aio),e(F,qp),e(qp,LK),e(LK,nio),e(qp,sio),e(qp,fS),e(fS,lio),e(qp,iio),e(F,dio),e(F,Gp),e(Gp,BK),e(BK,cio),e(Gp,fio),e(Gp,mS),e(mS,mio),e(Gp,gio),e(F,hio),e(F,Op),e(Op,xK),e(xK,pio),e(Op,_io),e(Op,gS),e(gS,uio),e(Op,bio),e(F,vio),e(F,Xp),e(Xp,kK),e(kK,Tio),e(Xp,Fio),e(Xp,hS),e(hS,Cio),e(Xp,Mio),e(F,Eio),e(F,Vp),e(Vp,RK),e(RK,yio),e(Vp,wio),e(Vp,pS),e(pS,Aio),e(Vp,Lio),e(F,Bio),e(F,zp),e(zp,SK),e(SK,xio),e(zp,kio),e(zp,_S),e(_S,Rio),e(zp,Sio),e(F,Pio),e(F,Wp),e(Wp,PK),e(PK,$io),e(Wp,Iio),e(Wp,uS),e(uS,Dio),e(Wp,jio),e(F,Nio),e(F,Qp),e(Qp,$K),e($K,qio),e(Qp,Gio),e(Qp,bS),e(bS,Oio),e(Qp,Xio),e(F,Vio),e(F,Hp),e(Hp,IK),e(IK,zio),e(Hp,Wio),e(Hp,vS),e(vS,Qio),e(Hp,Hio),e(F,Uio),e(F,Up),e(Up,DK),e(DK,Jio),e(Up,Yio),e(Up,TS),e(TS,Kio),e(Up,Zio),e(F,edo),e(F,Jp),e(Jp,jK),e(jK,odo),e(Jp,rdo),e(Jp,FS),e(FS,tdo),e(Jp,ado),e(F,ndo),e(F,Yp),e(Yp,NK),e(NK,sdo),e(Yp,ldo),e(Yp,CS),e(CS,ido),e(Yp,ddo),e(F,cdo),e(F,Kp),e(Kp,qK),e(qK,fdo),e(Kp,mdo),e(Kp,MS),e(MS,gdo),e(Kp,hdo),e(F,pdo),e(F,Zp),e(Zp,GK),e(GK,_do),e(Zp,udo),e(Zp,ES),e(ES,bdo),e(Zp,vdo),e(F,Tdo),e(F,e_),e(e_,OK),e(OK,Fdo),e(e_,Cdo),e(e_,yS),e(yS,Mdo),e(e_,Edo),e(F,ydo),e(F,o_),e(o_,XK),e(XK,wdo),e(o_,Ado),e(o_,wS),e(wS,Ldo),e(o_,Bdo),e(F,xdo),e(F,r_),e(r_,VK),e(VK,kdo),e(r_,Rdo),e(r_,AS),e(AS,Sdo),e(r_,Pdo),e(F,$do),e(F,t_),e(t_,zK),e(zK,Ido),e(t_,Ddo),e(t_,LS),e(LS,jdo),e(t_,Ndo),e(De,qdo),e(De,a_),e(a_,Gdo),e(a_,WK),e(WK,Odo),e(a_,Xdo),e(a_,QK),e(QK,Vdo),e(De,zdo),e(De,HK),e(HK,Wdo),e(De,Qdo),g(_E,De,null),b(c,vBe,u),b(c,Wi,u),e(Wi,n_),e(n_,UK),g(uE,UK,null),e(Wi,Hdo),e(Wi,JK),e(JK,Udo),b(c,TBe,u),b(c,Jo,u),g(bE,Jo,null),e(Jo,Jdo),e(Jo,Qi),e(Qi,Ydo),e(Qi,YK),e(YK,Kdo),e(Qi,Zdo),e(Qi,KK),e(KK,eco),e(Qi,oco),e(Jo,rco),e(Jo,vE),e(vE,tco),e(vE,ZK),e(ZK,aco),e(vE,nco),e(Jo,sco),e(Jo,Xr),g(TE,Xr,null),e(Xr,lco),e(Xr,eZ),e(eZ,ico),e(Xr,dco),e(Xr,Hi),e(Hi,cco),e(Hi,oZ),e(oZ,fco),e(Hi,mco),e(Hi,rZ),e(rZ,gco),e(Hi,hco),e(Xr,pco),e(Xr,tZ),e(tZ,_co),e(Xr,uco),g(FE,Xr,null),e(Jo,bco),e(Jo,je),g(CE,je,null),e(je,vco),e(je,aZ),e(aZ,Tco),e(je,Fco),e(je,qa),e(qa,Cco),e(qa,nZ),e(nZ,Mco),e(qa,Eco),e(qa,sZ),e(sZ,yco),e(qa,wco),e(qa,lZ),e(lZ,Aco),e(qa,Lco),e(je,Bco),e(je,k),e(k,s_),e(s_,iZ),e(iZ,xco),e(s_,kco),e(s_,BS),e(BS,Rco),e(s_,Sco),e(k,Pco),e(k,l_),e(l_,dZ),e(dZ,$co),e(l_,Ico),e(l_,xS),e(xS,Dco),e(l_,jco),e(k,Nco),e(k,i_),e(i_,cZ),e(cZ,qco),e(i_,Gco),e(i_,kS),e(kS,Oco),e(i_,Xco),e(k,Vco),e(k,d_),e(d_,fZ),e(fZ,zco),e(d_,Wco),e(d_,RS),e(RS,Qco),e(d_,Hco),e(k,Uco),e(k,c_),e(c_,mZ),e(mZ,Jco),e(c_,Yco),e(c_,SS),e(SS,Kco),e(c_,Zco),e(k,efo),e(k,f_),e(f_,gZ),e(gZ,ofo),e(f_,rfo),e(f_,PS),e(PS,tfo),e(f_,afo),e(k,nfo),e(k,m_),e(m_,hZ),e(hZ,sfo),e(m_,lfo),e(m_,$S),e($S,ifo),e(m_,dfo),e(k,cfo),e(k,g_),e(g_,pZ),e(pZ,ffo),e(g_,mfo),e(g_,IS),e(IS,gfo),e(g_,hfo),e(k,pfo),e(k,h_),e(h_,_Z),e(_Z,_fo),e(h_,ufo),e(h_,DS),e(DS,bfo),e(h_,vfo),e(k,Tfo),e(k,p_),e(p_,uZ),e(uZ,Ffo),e(p_,Cfo),e(p_,jS),e(jS,Mfo),e(p_,Efo),e(k,yfo),e(k,__),e(__,bZ),e(bZ,wfo),e(__,Afo),e(__,NS),e(NS,Lfo),e(__,Bfo),e(k,xfo),e(k,u_),e(u_,vZ),e(vZ,kfo),e(u_,Rfo),e(u_,qS),e(qS,Sfo),e(u_,Pfo),e(k,$fo),e(k,b_),e(b_,TZ),e(TZ,Ifo),e(b_,Dfo),e(b_,GS),e(GS,jfo),e(b_,Nfo),e(k,qfo),e(k,v_),e(v_,FZ),e(FZ,Gfo),e(v_,Ofo),e(v_,OS),e(OS,Xfo),e(v_,Vfo),e(k,zfo),e(k,T_),e(T_,CZ),e(CZ,Wfo),e(T_,Qfo),e(T_,XS),e(XS,Hfo),e(T_,Ufo),e(k,Jfo),e(k,F_),e(F_,MZ),e(MZ,Yfo),e(F_,Kfo),e(F_,VS),e(VS,Zfo),e(F_,emo),e(k,omo),e(k,C_),e(C_,EZ),e(EZ,rmo),e(C_,tmo),e(C_,zS),e(zS,amo),e(C_,nmo),e(k,smo),e(k,M_),e(M_,yZ),e(yZ,lmo),e(M_,imo),e(M_,WS),e(WS,dmo),e(M_,cmo),e(k,fmo),e(k,E_),e(E_,wZ),e(wZ,mmo),e(E_,gmo),e(E_,QS),e(QS,hmo),e(E_,pmo),e(k,_mo),e(k,y_),e(y_,AZ),e(AZ,umo),e(y_,bmo),e(y_,HS),e(HS,vmo),e(y_,Tmo),e(k,Fmo),e(k,w_),e(w_,LZ),e(LZ,Cmo),e(w_,Mmo),e(w_,US),e(US,Emo),e(w_,ymo),e(k,wmo),e(k,A_),e(A_,BZ),e(BZ,Amo),e(A_,Lmo),e(A_,JS),e(JS,Bmo),e(A_,xmo),e(k,kmo),e(k,L_),e(L_,xZ),e(xZ,Rmo),e(L_,Smo),e(L_,YS),e(YS,Pmo),e(L_,$mo),e(k,Imo),e(k,B_),e(B_,kZ),e(kZ,Dmo),e(B_,jmo),e(B_,KS),e(KS,Nmo),e(B_,qmo),e(k,Gmo),e(k,x_),e(x_,RZ),e(RZ,Omo),e(x_,Xmo),e(x_,ZS),e(ZS,Vmo),e(x_,zmo),e(k,Wmo),e(k,k_),e(k_,SZ),e(SZ,Qmo),e(k_,Hmo),e(k_,eP),e(eP,Umo),e(k_,Jmo),e(k,Ymo),e(k,R_),e(R_,PZ),e(PZ,Kmo),e(R_,Zmo),e(R_,oP),e(oP,ego),e(R_,ogo),e(k,rgo),e(k,S_),e(S_,$Z),e($Z,tgo),e(S_,ago),e(S_,rP),e(rP,ngo),e(S_,sgo),e(k,lgo),e(k,P_),e(P_,IZ),e(IZ,igo),e(P_,dgo),e(P_,tP),e(tP,cgo),e(P_,fgo),e(k,mgo),e(k,$_),e($_,DZ),e(DZ,ggo),e($_,hgo),e($_,aP),e(aP,pgo),e($_,_go),e(k,ugo),e(k,I_),e(I_,jZ),e(jZ,bgo),e(I_,vgo),e(I_,nP),e(nP,Tgo),e(I_,Fgo),e(k,Cgo),e(k,D_),e(D_,NZ),e(NZ,Mgo),e(D_,Ego),e(D_,sP),e(sP,ygo),e(D_,wgo),e(k,Ago),e(k,j_),e(j_,qZ),e(qZ,Lgo),e(j_,Bgo),e(j_,lP),e(lP,xgo),e(j_,kgo),e(k,Rgo),e(k,N_),e(N_,GZ),e(GZ,Sgo),e(N_,Pgo),e(N_,iP),e(iP,$go),e(N_,Igo),e(k,Dgo),e(k,q_),e(q_,OZ),e(OZ,jgo),e(q_,Ngo),e(q_,dP),e(dP,qgo),e(q_,Ggo),e(k,Ogo),e(k,G_),e(G_,XZ),e(XZ,Xgo),e(G_,Vgo),e(G_,cP),e(cP,zgo),e(G_,Wgo),e(k,Qgo),e(k,O_),e(O_,VZ),e(VZ,Hgo),e(O_,Ugo),e(O_,fP),e(fP,Jgo),e(O_,Ygo),e(k,Kgo),e(k,X_),e(X_,zZ),e(zZ,Zgo),e(X_,eho),e(X_,mP),e(mP,oho),e(X_,rho),e(k,tho),e(k,V_),e(V_,WZ),e(WZ,aho),e(V_,nho),e(V_,gP),e(gP,sho),e(V_,lho),e(je,iho),e(je,z_),e(z_,dho),e(z_,QZ),e(QZ,cho),e(z_,fho),e(z_,HZ),e(HZ,mho),e(je,gho),e(je,UZ),e(UZ,hho),e(je,pho),g(ME,je,null),b(c,FBe,u),b(c,Ui,u),e(Ui,W_),e(W_,JZ),g(EE,JZ,null),e(Ui,_ho),e(Ui,YZ),e(YZ,uho),b(c,CBe,u),b(c,Yo,u),g(yE,Yo,null),e(Yo,bho),e(Yo,Ji),e(Ji,vho),e(Ji,KZ),e(KZ,Tho),e(Ji,Fho),e(Ji,ZZ),e(ZZ,Cho),e(Ji,Mho),e(Yo,Eho),e(Yo,wE),e(wE,yho),e(wE,eee),e(eee,who),e(wE,Aho),e(Yo,Lho),e(Yo,Vr),g(AE,Vr,null),e(Vr,Bho),e(Vr,oee),e(oee,xho),e(Vr,kho),e(Vr,Yi),e(Yi,Rho),e(Yi,ree),e(ree,Sho),e(Yi,Pho),e(Yi,tee),e(tee,$ho),e(Yi,Iho),e(Vr,Dho),e(Vr,aee),e(aee,jho),e(Vr,Nho),g(LE,Vr,null),e(Yo,qho),e(Yo,Ne),g(BE,Ne,null),e(Ne,Gho),e(Ne,nee),e(nee,Oho),e(Ne,Xho),e(Ne,Ga),e(Ga,Vho),e(Ga,see),e(see,zho),e(Ga,Who),e(Ga,lee),e(lee,Qho),e(Ga,Hho),e(Ga,iee),e(iee,Uho),e(Ga,Jho),e(Ne,Yho),e(Ne,$),e($,Q_),e(Q_,dee),e(dee,Kho),e(Q_,Zho),e(Q_,hP),e(hP,epo),e(Q_,opo),e($,rpo),e($,H_),e(H_,cee),e(cee,tpo),e(H_,apo),e(H_,pP),e(pP,npo),e(H_,spo),e($,lpo),e($,U_),e(U_,fee),e(fee,ipo),e(U_,dpo),e(U_,_P),e(_P,cpo),e(U_,fpo),e($,mpo),e($,J_),e(J_,mee),e(mee,gpo),e(J_,hpo),e(J_,uP),e(uP,ppo),e(J_,_po),e($,upo),e($,Y_),e(Y_,gee),e(gee,bpo),e(Y_,vpo),e(Y_,bP),e(bP,Tpo),e(Y_,Fpo),e($,Cpo),e($,K_),e(K_,hee),e(hee,Mpo),e(K_,Epo),e(K_,vP),e(vP,ypo),e(K_,wpo),e($,Apo),e($,Z_),e(Z_,pee),e(pee,Lpo),e(Z_,Bpo),e(Z_,TP),e(TP,xpo),e(Z_,kpo),e($,Rpo),e($,eu),e(eu,_ee),e(_ee,Spo),e(eu,Ppo),e(eu,FP),e(FP,$po),e(eu,Ipo),e($,Dpo),e($,ou),e(ou,uee),e(uee,jpo),e(ou,Npo),e(ou,CP),e(CP,qpo),e(ou,Gpo),e($,Opo),e($,ru),e(ru,bee),e(bee,Xpo),e(ru,Vpo),e(ru,MP),e(MP,zpo),e(ru,Wpo),e($,Qpo),e($,tu),e(tu,vee),e(vee,Hpo),e(tu,Upo),e(tu,EP),e(EP,Jpo),e(tu,Ypo),e($,Kpo),e($,au),e(au,Tee),e(Tee,Zpo),e(au,e_o),e(au,yP),e(yP,o_o),e(au,r_o),e($,t_o),e($,nu),e(nu,Fee),e(Fee,a_o),e(nu,n_o),e(nu,wP),e(wP,s_o),e(nu,l_o),e($,i_o),e($,su),e(su,Cee),e(Cee,d_o),e(su,c_o),e(su,AP),e(AP,f_o),e(su,m_o),e($,g_o),e($,lu),e(lu,Mee),e(Mee,h_o),e(lu,p_o),e(lu,LP),e(LP,__o),e(lu,u_o),e($,b_o),e($,iu),e(iu,Eee),e(Eee,v_o),e(iu,T_o),e(iu,BP),e(BP,F_o),e(iu,C_o),e($,M_o),e($,du),e(du,yee),e(yee,E_o),e(du,y_o),e(du,xP),e(xP,w_o),e(du,A_o),e($,L_o),e($,cu),e(cu,wee),e(wee,B_o),e(cu,x_o),e(cu,kP),e(kP,k_o),e(cu,R_o),e($,S_o),e($,fu),e(fu,Aee),e(Aee,P_o),e(fu,$_o),e(fu,RP),e(RP,I_o),e(fu,D_o),e($,j_o),e($,mu),e(mu,Lee),e(Lee,N_o),e(mu,q_o),e(mu,SP),e(SP,G_o),e(mu,O_o),e($,X_o),e($,gu),e(gu,Bee),e(Bee,V_o),e(gu,z_o),e(gu,PP),e(PP,W_o),e(gu,Q_o),e($,H_o),e($,hu),e(hu,xee),e(xee,U_o),e(hu,J_o),e(hu,$P),e($P,Y_o),e(hu,K_o),e($,Z_o),e($,pu),e(pu,kee),e(kee,euo),e(pu,ouo),e(pu,IP),e(IP,ruo),e(pu,tuo),e($,auo),e($,_u),e(_u,Ree),e(Ree,nuo),e(_u,suo),e(_u,DP),e(DP,luo),e(_u,iuo),e($,duo),e($,uu),e(uu,See),e(See,cuo),e(uu,fuo),e(uu,jP),e(jP,muo),e(uu,guo),e($,huo),e($,bu),e(bu,Pee),e(Pee,puo),e(bu,_uo),e(bu,NP),e(NP,uuo),e(bu,buo),e($,vuo),e($,vu),e(vu,$ee),e($ee,Tuo),e(vu,Fuo),e(vu,qP),e(qP,Cuo),e(vu,Muo),e($,Euo),e($,Tu),e(Tu,Iee),e(Iee,yuo),e(Tu,wuo),e(Tu,GP),e(GP,Auo),e(Tu,Luo),e($,Buo),e($,Fu),e(Fu,Dee),e(Dee,xuo),e(Fu,kuo),e(Fu,OP),e(OP,Ruo),e(Fu,Suo),e($,Puo),e($,Cu),e(Cu,jee),e(jee,$uo),e(Cu,Iuo),e(Cu,XP),e(XP,Duo),e(Cu,juo),e($,Nuo),e($,Mu),e(Mu,Nee),e(Nee,quo),e(Mu,Guo),e(Mu,VP),e(VP,Ouo),e(Mu,Xuo),e($,Vuo),e($,Eu),e(Eu,qee),e(qee,zuo),e(Eu,Wuo),e(Eu,zP),e(zP,Quo),e(Eu,Huo),e($,Uuo),e($,yu),e(yu,Gee),e(Gee,Juo),e(yu,Yuo),e(yu,WP),e(WP,Kuo),e(yu,Zuo),e($,e0o),e($,wu),e(wu,Oee),e(Oee,o0o),e(wu,r0o),e(wu,QP),e(QP,t0o),e(wu,a0o),e($,n0o),e($,Au),e(Au,Xee),e(Xee,s0o),e(Au,l0o),e(Au,HP),e(HP,i0o),e(Au,d0o),e(Ne,c0o),e(Ne,Lu),e(Lu,f0o),e(Lu,Vee),e(Vee,m0o),e(Lu,g0o),e(Lu,zee),e(zee,h0o),e(Ne,p0o),e(Ne,Wee),e(Wee,_0o),e(Ne,u0o),g(xE,Ne,null),b(c,MBe,u),b(c,Ki,u),e(Ki,Bu),e(Bu,Qee),g(kE,Qee,null),e(Ki,b0o),e(Ki,Hee),e(Hee,v0o),b(c,EBe,u),b(c,Ko,u),g(RE,Ko,null),e(Ko,T0o),e(Ko,Zi),e(Zi,F0o),e(Zi,Uee),e(Uee,C0o),e(Zi,M0o),e(Zi,Jee),e(Jee,E0o),e(Zi,y0o),e(Ko,w0o),e(Ko,SE),e(SE,A0o),e(SE,Yee),e(Yee,L0o),e(SE,B0o),e(Ko,x0o),e(Ko,zr),g(PE,zr,null),e(zr,k0o),e(zr,Kee),e(Kee,R0o),e(zr,S0o),e(zr,ed),e(ed,P0o),e(ed,Zee),e(Zee,$0o),e(ed,I0o),e(ed,eoe),e(eoe,D0o),e(ed,j0o),e(zr,N0o),e(zr,ooe),e(ooe,q0o),e(zr,G0o),g($E,zr,null),e(Ko,O0o),e(Ko,qe),g(IE,qe,null),e(qe,X0o),e(qe,roe),e(roe,V0o),e(qe,z0o),e(qe,Oa),e(Oa,W0o),e(Oa,toe),e(toe,Q0o),e(Oa,H0o),e(Oa,aoe),e(aoe,U0o),e(Oa,J0o),e(Oa,noe),e(noe,Y0o),e(Oa,K0o),e(qe,Z0o),e(qe,I),e(I,xu),e(xu,soe),e(soe,e1o),e(xu,o1o),e(xu,UP),e(UP,r1o),e(xu,t1o),e(I,a1o),e(I,ku),e(ku,loe),e(loe,n1o),e(ku,s1o),e(ku,JP),e(JP,l1o),e(ku,i1o),e(I,d1o),e(I,Ru),e(Ru,ioe),e(ioe,c1o),e(Ru,f1o),e(Ru,YP),e(YP,m1o),e(Ru,g1o),e(I,h1o),e(I,Su),e(Su,doe),e(doe,p1o),e(Su,_1o),e(Su,KP),e(KP,u1o),e(Su,b1o),e(I,v1o),e(I,Pu),e(Pu,coe),e(coe,T1o),e(Pu,F1o),e(Pu,ZP),e(ZP,C1o),e(Pu,M1o),e(I,E1o),e(I,$u),e($u,foe),e(foe,y1o),e($u,w1o),e($u,e$),e(e$,A1o),e($u,L1o),e(I,B1o),e(I,Iu),e(Iu,moe),e(moe,x1o),e(Iu,k1o),e(Iu,o$),e(o$,R1o),e(Iu,S1o),e(I,P1o),e(I,Du),e(Du,goe),e(goe,$1o),e(Du,I1o),e(Du,r$),e(r$,D1o),e(Du,j1o),e(I,N1o),e(I,ju),e(ju,hoe),e(hoe,q1o),e(ju,G1o),e(ju,t$),e(t$,O1o),e(ju,X1o),e(I,V1o),e(I,Nu),e(Nu,poe),e(poe,z1o),e(Nu,W1o),e(Nu,a$),e(a$,Q1o),e(Nu,H1o),e(I,U1o),e(I,qu),e(qu,_oe),e(_oe,J1o),e(qu,Y1o),e(qu,n$),e(n$,K1o),e(qu,Z1o),e(I,ebo),e(I,Gu),e(Gu,uoe),e(uoe,obo),e(Gu,rbo),e(Gu,s$),e(s$,tbo),e(Gu,abo),e(I,nbo),e(I,Ou),e(Ou,boe),e(boe,sbo),e(Ou,lbo),e(Ou,l$),e(l$,ibo),e(Ou,dbo),e(I,cbo),e(I,Xu),e(Xu,voe),e(voe,fbo),e(Xu,mbo),e(Xu,i$),e(i$,gbo),e(Xu,hbo),e(I,pbo),e(I,Vu),e(Vu,Toe),e(Toe,_bo),e(Vu,ubo),e(Vu,d$),e(d$,bbo),e(Vu,vbo),e(I,Tbo),e(I,zu),e(zu,Foe),e(Foe,Fbo),e(zu,Cbo),e(zu,c$),e(c$,Mbo),e(zu,Ebo),e(I,ybo),e(I,Wu),e(Wu,Coe),e(Coe,wbo),e(Wu,Abo),e(Wu,f$),e(f$,Lbo),e(Wu,Bbo),e(I,xbo),e(I,Qu),e(Qu,Moe),e(Moe,kbo),e(Qu,Rbo),e(Qu,m$),e(m$,Sbo),e(Qu,Pbo),e(I,$bo),e(I,Hu),e(Hu,Eoe),e(Eoe,Ibo),e(Hu,Dbo),e(Hu,g$),e(g$,jbo),e(Hu,Nbo),e(I,qbo),e(I,Uu),e(Uu,yoe),e(yoe,Gbo),e(Uu,Obo),e(Uu,h$),e(h$,Xbo),e(Uu,Vbo),e(I,zbo),e(I,Ju),e(Ju,woe),e(woe,Wbo),e(Ju,Qbo),e(Ju,p$),e(p$,Hbo),e(Ju,Ubo),e(I,Jbo),e(I,Yu),e(Yu,Aoe),e(Aoe,Ybo),e(Yu,Kbo),e(Yu,_$),e(_$,Zbo),e(Yu,e5o),e(I,o5o),e(I,Ku),e(Ku,Loe),e(Loe,r5o),e(Ku,t5o),e(Ku,u$),e(u$,a5o),e(Ku,n5o),e(I,s5o),e(I,Zu),e(Zu,Boe),e(Boe,l5o),e(Zu,i5o),e(Zu,b$),e(b$,d5o),e(Zu,c5o),e(I,f5o),e(I,e0),e(e0,xoe),e(xoe,m5o),e(e0,g5o),e(e0,v$),e(v$,h5o),e(e0,p5o),e(I,_5o),e(I,o0),e(o0,koe),e(koe,u5o),e(o0,b5o),e(o0,T$),e(T$,v5o),e(o0,T5o),e(I,F5o),e(I,r0),e(r0,Roe),e(Roe,C5o),e(r0,M5o),e(r0,F$),e(F$,E5o),e(r0,y5o),e(I,w5o),e(I,t0),e(t0,Soe),e(Soe,A5o),e(t0,L5o),e(t0,C$),e(C$,B5o),e(t0,x5o),e(I,k5o),e(I,a0),e(a0,Poe),e(Poe,R5o),e(a0,S5o),e(a0,M$),e(M$,P5o),e(a0,$5o),e(I,I5o),e(I,n0),e(n0,$oe),e($oe,D5o),e(n0,j5o),e(n0,E$),e(E$,N5o),e(n0,q5o),e(I,G5o),e(I,s0),e(s0,Ioe),e(Ioe,O5o),e(s0,X5o),e(s0,Doe),e(Doe,V5o),e(s0,z5o),e(I,W5o),e(I,l0),e(l0,joe),e(joe,Q5o),e(l0,H5o),e(l0,y$),e(y$,U5o),e(l0,J5o),e(I,Y5o),e(I,i0),e(i0,Noe),e(Noe,K5o),e(i0,Z5o),e(i0,w$),e(w$,e2o),e(i0,o2o),e(I,r2o),e(I,d0),e(d0,qoe),e(qoe,t2o),e(d0,a2o),e(d0,A$),e(A$,n2o),e(d0,s2o),e(I,l2o),e(I,c0),e(c0,Goe),e(Goe,i2o),e(c0,d2o),e(c0,L$),e(L$,c2o),e(c0,f2o),e(qe,m2o),e(qe,f0),e(f0,g2o),e(f0,Ooe),e(Ooe,h2o),e(f0,p2o),e(f0,Xoe),e(Xoe,_2o),e(qe,u2o),e(qe,Voe),e(Voe,b2o),e(qe,v2o),g(DE,qe,null),b(c,yBe,u),b(c,od,u),e(od,m0),e(m0,zoe),g(jE,zoe,null),e(od,T2o),e(od,Woe),e(Woe,F2o),b(c,wBe,u),b(c,Zo,u),g(NE,Zo,null),e(Zo,C2o),e(Zo,rd),e(rd,M2o),e(rd,Qoe),e(Qoe,E2o),e(rd,y2o),e(rd,Hoe),e(Hoe,w2o),e(rd,A2o),e(Zo,L2o),e(Zo,qE),e(qE,B2o),e(qE,Uoe),e(Uoe,x2o),e(qE,k2o),e(Zo,R2o),e(Zo,Wr),g(GE,Wr,null),e(Wr,S2o),e(Wr,Joe),e(Joe,P2o),e(Wr,$2o),e(Wr,td),e(td,I2o),e(td,Yoe),e(Yoe,D2o),e(td,j2o),e(td,Koe),e(Koe,N2o),e(td,q2o),e(Wr,G2o),e(Wr,Zoe),e(Zoe,O2o),e(Wr,X2o),g(OE,Wr,null),e(Zo,V2o),e(Zo,Ge),g(XE,Ge,null),e(Ge,z2o),e(Ge,ere),e(ere,W2o),e(Ge,Q2o),e(Ge,Xa),e(Xa,H2o),e(Xa,ore),e(ore,U2o),e(Xa,J2o),e(Xa,rre),e(rre,Y2o),e(Xa,K2o),e(Xa,tre),e(tre,Z2o),e(Xa,evo),e(Ge,ovo),e(Ge,ae),e(ae,g0),e(g0,are),e(are,rvo),e(g0,tvo),e(g0,B$),e(B$,avo),e(g0,nvo),e(ae,svo),e(ae,h0),e(h0,nre),e(nre,lvo),e(h0,ivo),e(h0,x$),e(x$,dvo),e(h0,cvo),e(ae,fvo),e(ae,p0),e(p0,sre),e(sre,mvo),e(p0,gvo),e(p0,k$),e(k$,hvo),e(p0,pvo),e(ae,_vo),e(ae,_0),e(_0,lre),e(lre,uvo),e(_0,bvo),e(_0,R$),e(R$,vvo),e(_0,Tvo),e(ae,Fvo),e(ae,u0),e(u0,ire),e(ire,Cvo),e(u0,Mvo),e(u0,S$),e(S$,Evo),e(u0,yvo),e(ae,wvo),e(ae,b0),e(b0,dre),e(dre,Avo),e(b0,Lvo),e(b0,P$),e(P$,Bvo),e(b0,xvo),e(ae,kvo),e(ae,v0),e(v0,cre),e(cre,Rvo),e(v0,Svo),e(v0,$$),e($$,Pvo),e(v0,$vo),e(ae,Ivo),e(ae,T0),e(T0,fre),e(fre,Dvo),e(T0,jvo),e(T0,I$),e(I$,Nvo),e(T0,qvo),e(ae,Gvo),e(ae,F0),e(F0,mre),e(mre,Ovo),e(F0,Xvo),e(F0,D$),e(D$,Vvo),e(F0,zvo),e(ae,Wvo),e(ae,C0),e(C0,gre),e(gre,Qvo),e(C0,Hvo),e(C0,j$),e(j$,Uvo),e(C0,Jvo),e(ae,Yvo),e(ae,M0),e(M0,hre),e(hre,Kvo),e(M0,Zvo),e(M0,N$),e(N$,eTo),e(M0,oTo),e(ae,rTo),e(ae,E0),e(E0,pre),e(pre,tTo),e(E0,aTo),e(E0,q$),e(q$,nTo),e(E0,sTo),e(ae,lTo),e(ae,y0),e(y0,_re),e(_re,iTo),e(y0,dTo),e(y0,G$),e(G$,cTo),e(y0,fTo),e(ae,mTo),e(ae,w0),e(w0,ure),e(ure,gTo),e(w0,hTo),e(w0,O$),e(O$,pTo),e(w0,_To),e(ae,uTo),e(ae,A0),e(A0,bre),e(bre,bTo),e(A0,vTo),e(A0,X$),e(X$,TTo),e(A0,FTo),e(ae,CTo),e(ae,L0),e(L0,vre),e(vre,MTo),e(L0,ETo),e(L0,V$),e(V$,yTo),e(L0,wTo),e(Ge,ATo),e(Ge,B0),e(B0,LTo),e(B0,Tre),e(Tre,BTo),e(B0,xTo),e(B0,Fre),e(Fre,kTo),e(Ge,RTo),e(Ge,Cre),e(Cre,STo),e(Ge,PTo),g(VE,Ge,null),b(c,ABe,u),b(c,ad,u),e(ad,x0),e(x0,Mre),g(zE,Mre,null),e(ad,$To),e(ad,Ere),e(Ere,ITo),b(c,LBe,u),b(c,er,u),g(WE,er,null),e(er,DTo),e(er,nd),e(nd,jTo),e(nd,yre),e(yre,NTo),e(nd,qTo),e(nd,wre),e(wre,GTo),e(nd,OTo),e(er,XTo),e(er,QE),e(QE,VTo),e(QE,Are),e(Are,zTo),e(QE,WTo),e(er,QTo),e(er,Qr),g(HE,Qr,null),e(Qr,HTo),e(Qr,Lre),e(Lre,UTo),e(Qr,JTo),e(Qr,sd),e(sd,YTo),e(sd,Bre),e(Bre,KTo),e(sd,ZTo),e(sd,xre),e(xre,eFo),e(sd,oFo),e(Qr,rFo),e(Qr,kre),e(kre,tFo),e(Qr,aFo),g(UE,Qr,null),e(er,nFo),e(er,Oe),g(JE,Oe,null),e(Oe,sFo),e(Oe,Rre),e(Rre,lFo),e(Oe,iFo),e(Oe,Va),e(Va,dFo),e(Va,Sre),e(Sre,cFo),e(Va,fFo),e(Va,Pre),e(Pre,mFo),e(Va,gFo),e(Va,$re),e($re,hFo),e(Va,pFo),e(Oe,_Fo),e(Oe,A),e(A,k0),e(k0,Ire),e(Ire,uFo),e(k0,bFo),e(k0,z$),e(z$,vFo),e(k0,TFo),e(A,FFo),e(A,R0),e(R0,Dre),e(Dre,CFo),e(R0,MFo),e(R0,W$),e(W$,EFo),e(R0,yFo),e(A,wFo),e(A,S0),e(S0,jre),e(jre,AFo),e(S0,LFo),e(S0,Q$),e(Q$,BFo),e(S0,xFo),e(A,kFo),e(A,P0),e(P0,Nre),e(Nre,RFo),e(P0,SFo),e(P0,H$),e(H$,PFo),e(P0,$Fo),e(A,IFo),e(A,$0),e($0,qre),e(qre,DFo),e($0,jFo),e($0,U$),e(U$,NFo),e($0,qFo),e(A,GFo),e(A,I0),e(I0,Gre),e(Gre,OFo),e(I0,XFo),e(I0,J$),e(J$,VFo),e(I0,zFo),e(A,WFo),e(A,D0),e(D0,Ore),e(Ore,QFo),e(D0,HFo),e(D0,Y$),e(Y$,UFo),e(D0,JFo),e(A,YFo),e(A,j0),e(j0,Xre),e(Xre,KFo),e(j0,ZFo),e(j0,K$),e(K$,e9o),e(j0,o9o),e(A,r9o),e(A,N0),e(N0,Vre),e(Vre,t9o),e(N0,a9o),e(N0,Z$),e(Z$,n9o),e(N0,s9o),e(A,l9o),e(A,q0),e(q0,zre),e(zre,i9o),e(q0,d9o),e(q0,eI),e(eI,c9o),e(q0,f9o),e(A,m9o),e(A,G0),e(G0,Wre),e(Wre,g9o),e(G0,h9o),e(G0,oI),e(oI,p9o),e(G0,_9o),e(A,u9o),e(A,O0),e(O0,Qre),e(Qre,b9o),e(O0,v9o),e(O0,rI),e(rI,T9o),e(O0,F9o),e(A,C9o),e(A,X0),e(X0,Hre),e(Hre,M9o),e(X0,E9o),e(X0,tI),e(tI,y9o),e(X0,w9o),e(A,A9o),e(A,V0),e(V0,Ure),e(Ure,L9o),e(V0,B9o),e(V0,aI),e(aI,x9o),e(V0,k9o),e(A,R9o),e(A,z0),e(z0,Jre),e(Jre,S9o),e(z0,P9o),e(z0,nI),e(nI,$9o),e(z0,I9o),e(A,D9o),e(A,W0),e(W0,Yre),e(Yre,j9o),e(W0,N9o),e(W0,sI),e(sI,q9o),e(W0,G9o),e(A,O9o),e(A,Q0),e(Q0,Kre),e(Kre,X9o),e(Q0,V9o),e(Q0,lI),e(lI,z9o),e(Q0,W9o),e(A,Q9o),e(A,H0),e(H0,Zre),e(Zre,H9o),e(H0,U9o),e(H0,iI),e(iI,J9o),e(H0,Y9o),e(A,K9o),e(A,U0),e(U0,ete),e(ete,Z9o),e(U0,eCo),e(U0,dI),e(dI,oCo),e(U0,rCo),e(A,tCo),e(A,J0),e(J0,ote),e(ote,aCo),e(J0,nCo),e(J0,cI),e(cI,sCo),e(J0,lCo),e(A,iCo),e(A,Y0),e(Y0,rte),e(rte,dCo),e(Y0,cCo),e(Y0,fI),e(fI,fCo),e(Y0,mCo),e(A,gCo),e(A,K0),e(K0,tte),e(tte,hCo),e(K0,pCo),e(K0,mI),e(mI,_Co),e(K0,uCo),e(A,bCo),e(A,Z0),e(Z0,ate),e(ate,vCo),e(Z0,TCo),e(Z0,gI),e(gI,FCo),e(Z0,CCo),e(A,MCo),e(A,e1),e(e1,nte),e(nte,ECo),e(e1,yCo),e(e1,hI),e(hI,wCo),e(e1,ACo),e(A,LCo),e(A,o1),e(o1,ste),e(ste,BCo),e(o1,xCo),e(o1,pI),e(pI,kCo),e(o1,RCo),e(A,SCo),e(A,r1),e(r1,lte),e(lte,PCo),e(r1,$Co),e(r1,_I),e(_I,ICo),e(r1,DCo),e(A,jCo),e(A,t1),e(t1,ite),e(ite,NCo),e(t1,qCo),e(t1,uI),e(uI,GCo),e(t1,OCo),e(A,XCo),e(A,a1),e(a1,dte),e(dte,VCo),e(a1,zCo),e(a1,bI),e(bI,WCo),e(a1,QCo),e(A,HCo),e(A,n1),e(n1,cte),e(cte,UCo),e(n1,JCo),e(n1,vI),e(vI,YCo),e(n1,KCo),e(A,ZCo),e(A,s1),e(s1,fte),e(fte,eMo),e(s1,oMo),e(s1,TI),e(TI,rMo),e(s1,tMo),e(A,aMo),e(A,l1),e(l1,mte),e(mte,nMo),e(l1,sMo),e(l1,FI),e(FI,lMo),e(l1,iMo),e(A,dMo),e(A,i1),e(i1,gte),e(gte,cMo),e(i1,fMo),e(i1,CI),e(CI,mMo),e(i1,gMo),e(A,hMo),e(A,d1),e(d1,hte),e(hte,pMo),e(d1,_Mo),e(d1,MI),e(MI,uMo),e(d1,bMo),e(A,vMo),e(A,c1),e(c1,pte),e(pte,TMo),e(c1,FMo),e(c1,EI),e(EI,CMo),e(c1,MMo),e(A,EMo),e(A,f1),e(f1,_te),e(_te,yMo),e(f1,wMo),e(f1,yI),e(yI,AMo),e(f1,LMo),e(A,BMo),e(A,m1),e(m1,ute),e(ute,xMo),e(m1,kMo),e(m1,wI),e(wI,RMo),e(m1,SMo),e(A,PMo),e(A,g1),e(g1,bte),e(bte,$Mo),e(g1,IMo),e(g1,AI),e(AI,DMo),e(g1,jMo),e(A,NMo),e(A,h1),e(h1,vte),e(vte,qMo),e(h1,GMo),e(h1,LI),e(LI,OMo),e(h1,XMo),e(A,VMo),e(A,p1),e(p1,Tte),e(Tte,zMo),e(p1,WMo),e(p1,BI),e(BI,QMo),e(p1,HMo),e(A,UMo),e(A,_1),e(_1,Fte),e(Fte,JMo),e(_1,YMo),e(_1,xI),e(xI,KMo),e(_1,ZMo),e(A,e4o),e(A,u1),e(u1,Cte),e(Cte,o4o),e(u1,r4o),e(u1,kI),e(kI,t4o),e(u1,a4o),e(A,n4o),e(A,b1),e(b1,Mte),e(Mte,s4o),e(b1,l4o),e(b1,RI),e(RI,i4o),e(b1,d4o),e(A,c4o),e(A,v1),e(v1,Ete),e(Ete,f4o),e(v1,m4o),e(v1,SI),e(SI,g4o),e(v1,h4o),e(A,p4o),e(A,T1),e(T1,yte),e(yte,_4o),e(T1,u4o),e(T1,PI),e(PI,b4o),e(T1,v4o),e(A,T4o),e(A,F1),e(F1,wte),e(wte,F4o),e(F1,C4o),e(F1,$I),e($I,M4o),e(F1,E4o),e(A,y4o),e(A,C1),e(C1,Ate),e(Ate,w4o),e(C1,A4o),e(C1,II),e(II,L4o),e(C1,B4o),e(Oe,x4o),e(Oe,M1),e(M1,k4o),e(M1,Lte),e(Lte,R4o),e(M1,S4o),e(M1,Bte),e(Bte,P4o),e(Oe,$4o),e(Oe,xte),e(xte,I4o),e(Oe,D4o),g(YE,Oe,null),b(c,BBe,u),b(c,ld,u),e(ld,E1),e(E1,kte),g(KE,kte,null),e(ld,j4o),e(ld,Rte),e(Rte,N4o),b(c,xBe,u),b(c,or,u),g(ZE,or,null),e(or,q4o),e(or,id),e(id,G4o),e(id,Ste),e(Ste,O4o),e(id,X4o),e(id,Pte),e(Pte,V4o),e(id,z4o),e(or,W4o),e(or,e3),e(e3,Q4o),e(e3,$te),e($te,H4o),e(e3,U4o),e(or,J4o),e(or,Hr),g(o3,Hr,null),e(Hr,Y4o),e(Hr,Ite),e(Ite,K4o),e(Hr,Z4o),e(Hr,dd),e(dd,eEo),e(dd,Dte),e(Dte,oEo),e(dd,rEo),e(dd,jte),e(jte,tEo),e(dd,aEo),e(Hr,nEo),e(Hr,Nte),e(Nte,sEo),e(Hr,lEo),g(r3,Hr,null),e(or,iEo),e(or,Xe),g(t3,Xe,null),e(Xe,dEo),e(Xe,qte),e(qte,cEo),e(Xe,fEo),e(Xe,za),e(za,mEo),e(za,Gte),e(Gte,gEo),e(za,hEo),e(za,Ote),e(Ote,pEo),e(za,_Eo),e(za,Xte),e(Xte,uEo),e(za,bEo),e(Xe,vEo),e(Xe,G),e(G,y1),e(y1,Vte),e(Vte,TEo),e(y1,FEo),e(y1,DI),e(DI,CEo),e(y1,MEo),e(G,EEo),e(G,w1),e(w1,zte),e(zte,yEo),e(w1,wEo),e(w1,jI),e(jI,AEo),e(w1,LEo),e(G,BEo),e(G,A1),e(A1,Wte),e(Wte,xEo),e(A1,kEo),e(A1,NI),e(NI,REo),e(A1,SEo),e(G,PEo),e(G,L1),e(L1,Qte),e(Qte,$Eo),e(L1,IEo),e(L1,qI),e(qI,DEo),e(L1,jEo),e(G,NEo),e(G,B1),e(B1,Hte),e(Hte,qEo),e(B1,GEo),e(B1,GI),e(GI,OEo),e(B1,XEo),e(G,VEo),e(G,x1),e(x1,Ute),e(Ute,zEo),e(x1,WEo),e(x1,OI),e(OI,QEo),e(x1,HEo),e(G,UEo),e(G,k1),e(k1,Jte),e(Jte,JEo),e(k1,YEo),e(k1,XI),e(XI,KEo),e(k1,ZEo),e(G,e3o),e(G,R1),e(R1,Yte),e(Yte,o3o),e(R1,r3o),e(R1,VI),e(VI,t3o),e(R1,a3o),e(G,n3o),e(G,S1),e(S1,Kte),e(Kte,s3o),e(S1,l3o),e(S1,zI),e(zI,i3o),e(S1,d3o),e(G,c3o),e(G,P1),e(P1,Zte),e(Zte,f3o),e(P1,m3o),e(P1,WI),e(WI,g3o),e(P1,h3o),e(G,p3o),e(G,$1),e($1,eae),e(eae,_3o),e($1,u3o),e($1,QI),e(QI,b3o),e($1,v3o),e(G,T3o),e(G,I1),e(I1,oae),e(oae,F3o),e(I1,C3o),e(I1,HI),e(HI,M3o),e(I1,E3o),e(G,y3o),e(G,D1),e(D1,rae),e(rae,w3o),e(D1,A3o),e(D1,UI),e(UI,L3o),e(D1,B3o),e(G,x3o),e(G,j1),e(j1,tae),e(tae,k3o),e(j1,R3o),e(j1,JI),e(JI,S3o),e(j1,P3o),e(G,$3o),e(G,N1),e(N1,aae),e(aae,I3o),e(N1,D3o),e(N1,YI),e(YI,j3o),e(N1,N3o),e(G,q3o),e(G,q1),e(q1,nae),e(nae,G3o),e(q1,O3o),e(q1,KI),e(KI,X3o),e(q1,V3o),e(G,z3o),e(G,G1),e(G1,sae),e(sae,W3o),e(G1,Q3o),e(G1,ZI),e(ZI,H3o),e(G1,U3o),e(G,J3o),e(G,O1),e(O1,lae),e(lae,Y3o),e(O1,K3o),e(O1,eD),e(eD,Z3o),e(O1,eyo),e(G,oyo),e(G,X1),e(X1,iae),e(iae,ryo),e(X1,tyo),e(X1,oD),e(oD,ayo),e(X1,nyo),e(G,syo),e(G,V1),e(V1,dae),e(dae,lyo),e(V1,iyo),e(V1,rD),e(rD,dyo),e(V1,cyo),e(G,fyo),e(G,z1),e(z1,cae),e(cae,myo),e(z1,gyo),e(z1,tD),e(tD,hyo),e(z1,pyo),e(G,_yo),e(G,W1),e(W1,fae),e(fae,uyo),e(W1,byo),e(W1,aD),e(aD,vyo),e(W1,Tyo),e(G,Fyo),e(G,Q1),e(Q1,mae),e(mae,Cyo),e(Q1,Myo),e(Q1,nD),e(nD,Eyo),e(Q1,yyo),e(G,wyo),e(G,H1),e(H1,gae),e(gae,Ayo),e(H1,Lyo),e(H1,sD),e(sD,Byo),e(H1,xyo),e(G,kyo),e(G,U1),e(U1,hae),e(hae,Ryo),e(U1,Syo),e(U1,lD),e(lD,Pyo),e(U1,$yo),e(G,Iyo),e(G,J1),e(J1,pae),e(pae,Dyo),e(J1,jyo),e(J1,iD),e(iD,Nyo),e(J1,qyo),e(G,Gyo),e(G,Y1),e(Y1,_ae),e(_ae,Oyo),e(Y1,Xyo),e(Y1,dD),e(dD,Vyo),e(Y1,zyo),e(G,Wyo),e(G,K1),e(K1,uae),e(uae,Qyo),e(K1,Hyo),e(K1,cD),e(cD,Uyo),e(K1,Jyo),e(Xe,Yyo),e(Xe,Z1),e(Z1,Kyo),e(Z1,bae),e(bae,Zyo),e(Z1,ewo),e(Z1,vae),e(vae,owo),e(Xe,rwo),e(Xe,Tae),e(Tae,two),e(Xe,awo),g(a3,Xe,null),b(c,kBe,u),b(c,cd,u),e(cd,eb),e(eb,Fae),g(n3,Fae,null),e(cd,nwo),e(cd,Cae),e(Cae,swo),b(c,RBe,u),b(c,rr,u),g(s3,rr,null),e(rr,lwo),e(rr,fd),e(fd,iwo),e(fd,Mae),e(Mae,dwo),e(fd,cwo),e(fd,Eae),e(Eae,fwo),e(fd,mwo),e(rr,gwo),e(rr,l3),e(l3,hwo),e(l3,yae),e(yae,pwo),e(l3,_wo),e(rr,uwo),e(rr,Ur),g(i3,Ur,null),e(Ur,bwo),e(Ur,wae),e(wae,vwo),e(Ur,Two),e(Ur,md),e(md,Fwo),e(md,Aae),e(Aae,Cwo),e(md,Mwo),e(md,Lae),e(Lae,Ewo),e(md,ywo),e(Ur,wwo),e(Ur,Bae),e(Bae,Awo),e(Ur,Lwo),g(d3,Ur,null),e(rr,Bwo),e(rr,Ve),g(c3,Ve,null),e(Ve,xwo),e(Ve,xae),e(xae,kwo),e(Ve,Rwo),e(Ve,Wa),e(Wa,Swo),e(Wa,kae),e(kae,Pwo),e(Wa,$wo),e(Wa,Rae),e(Rae,Iwo),e(Wa,Dwo),e(Wa,Sae),e(Sae,jwo),e(Wa,Nwo),e(Ve,qwo),e(Ve,na),e(na,ob),e(ob,Pae),e(Pae,Gwo),e(ob,Owo),e(ob,fD),e(fD,Xwo),e(ob,Vwo),e(na,zwo),e(na,rb),e(rb,$ae),e($ae,Wwo),e(rb,Qwo),e(rb,mD),e(mD,Hwo),e(rb,Uwo),e(na,Jwo),e(na,tb),e(tb,Iae),e(Iae,Ywo),e(tb,Kwo),e(tb,gD),e(gD,Zwo),e(tb,e6o),e(na,o6o),e(na,ab),e(ab,Dae),e(Dae,r6o),e(ab,t6o),e(ab,hD),e(hD,a6o),e(ab,n6o),e(na,s6o),e(na,nb),e(nb,jae),e(jae,l6o),e(nb,i6o),e(nb,pD),e(pD,d6o),e(nb,c6o),e(Ve,f6o),e(Ve,sb),e(sb,m6o),e(sb,Nae),e(Nae,g6o),e(sb,h6o),e(sb,qae),e(qae,p6o),e(Ve,_6o),e(Ve,Gae),e(Gae,u6o),e(Ve,b6o),g(f3,Ve,null),b(c,SBe,u),b(c,gd,u),e(gd,lb),e(lb,Oae),g(m3,Oae,null),e(gd,v6o),e(gd,Xae),e(Xae,T6o),b(c,PBe,u),b(c,tr,u),g(g3,tr,null),e(tr,F6o),e(tr,hd),e(hd,C6o),e(hd,Vae),e(Vae,M6o),e(hd,E6o),e(hd,zae),e(zae,y6o),e(hd,w6o),e(tr,A6o),e(tr,h3),e(h3,L6o),e(h3,Wae),e(Wae,B6o),e(h3,x6o),e(tr,k6o),e(tr,Jr),g(p3,Jr,null),e(Jr,R6o),e(Jr,Qae),e(Qae,S6o),e(Jr,P6o),e(Jr,pd),e(pd,$6o),e(pd,Hae),e(Hae,I6o),e(pd,D6o),e(pd,Uae),e(Uae,j6o),e(pd,N6o),e(Jr,q6o),e(Jr,Jae),e(Jae,G6o),e(Jr,O6o),g(_3,Jr,null),e(tr,X6o),e(tr,ze),g(u3,ze,null),e(ze,V6o),e(ze,Yae),e(Yae,z6o),e(ze,W6o),e(ze,Qa),e(Qa,Q6o),e(Qa,Kae),e(Kae,H6o),e(Qa,U6o),e(Qa,Zae),e(Zae,J6o),e(Qa,Y6o),e(Qa,ene),e(ene,K6o),e(Qa,Z6o),e(ze,eAo),e(ze,N),e(N,ib),e(ib,one),e(one,oAo),e(ib,rAo),e(ib,_D),e(_D,tAo),e(ib,aAo),e(N,nAo),e(N,db),e(db,rne),e(rne,sAo),e(db,lAo),e(db,uD),e(uD,iAo),e(db,dAo),e(N,cAo),e(N,cb),e(cb,tne),e(tne,fAo),e(cb,mAo),e(cb,bD),e(bD,gAo),e(cb,hAo),e(N,pAo),e(N,fb),e(fb,ane),e(ane,_Ao),e(fb,uAo),e(fb,vD),e(vD,bAo),e(fb,vAo),e(N,TAo),e(N,mb),e(mb,nne),e(nne,FAo),e(mb,CAo),e(mb,TD),e(TD,MAo),e(mb,EAo),e(N,yAo),e(N,gb),e(gb,sne),e(sne,wAo),e(gb,AAo),e(gb,FD),e(FD,LAo),e(gb,BAo),e(N,xAo),e(N,hb),e(hb,lne),e(lne,kAo),e(hb,RAo),e(hb,CD),e(CD,SAo),e(hb,PAo),e(N,$Ao),e(N,pb),e(pb,ine),e(ine,IAo),e(pb,DAo),e(pb,MD),e(MD,jAo),e(pb,NAo),e(N,qAo),e(N,_b),e(_b,dne),e(dne,GAo),e(_b,OAo),e(_b,ED),e(ED,XAo),e(_b,VAo),e(N,zAo),e(N,ub),e(ub,cne),e(cne,WAo),e(ub,QAo),e(ub,yD),e(yD,HAo),e(ub,UAo),e(N,JAo),e(N,bb),e(bb,fne),e(fne,YAo),e(bb,KAo),e(bb,wD),e(wD,ZAo),e(bb,eLo),e(N,oLo),e(N,vb),e(vb,mne),e(mne,rLo),e(vb,tLo),e(vb,AD),e(AD,aLo),e(vb,nLo),e(N,sLo),e(N,Tb),e(Tb,gne),e(gne,lLo),e(Tb,iLo),e(Tb,LD),e(LD,dLo),e(Tb,cLo),e(N,fLo),e(N,Fb),e(Fb,hne),e(hne,mLo),e(Fb,gLo),e(Fb,BD),e(BD,hLo),e(Fb,pLo),e(N,_Lo),e(N,Cb),e(Cb,pne),e(pne,uLo),e(Cb,bLo),e(Cb,xD),e(xD,vLo),e(Cb,TLo),e(N,FLo),e(N,Mb),e(Mb,_ne),e(_ne,CLo),e(Mb,MLo),e(Mb,kD),e(kD,ELo),e(Mb,yLo),e(N,wLo),e(N,Eb),e(Eb,une),e(une,ALo),e(Eb,LLo),e(Eb,RD),e(RD,BLo),e(Eb,xLo),e(N,kLo),e(N,yb),e(yb,bne),e(bne,RLo),e(yb,SLo),e(yb,SD),e(SD,PLo),e(yb,$Lo),e(N,ILo),e(N,wb),e(wb,vne),e(vne,DLo),e(wb,jLo),e(wb,PD),e(PD,NLo),e(wb,qLo),e(N,GLo),e(N,Ab),e(Ab,Tne),e(Tne,OLo),e(Ab,XLo),e(Ab,$D),e($D,VLo),e(Ab,zLo),e(N,WLo),e(N,Lb),e(Lb,Fne),e(Fne,QLo),e(Lb,HLo),e(Lb,ID),e(ID,ULo),e(Lb,JLo),e(N,YLo),e(N,Bb),e(Bb,Cne),e(Cne,KLo),e(Bb,ZLo),e(Bb,DD),e(DD,e8o),e(Bb,o8o),e(N,r8o),e(N,xb),e(xb,Mne),e(Mne,t8o),e(xb,a8o),e(xb,jD),e(jD,n8o),e(xb,s8o),e(N,l8o),e(N,kb),e(kb,Ene),e(Ene,i8o),e(kb,d8o),e(kb,ND),e(ND,c8o),e(kb,f8o),e(N,m8o),e(N,Rb),e(Rb,yne),e(yne,g8o),e(Rb,h8o),e(Rb,qD),e(qD,p8o),e(Rb,_8o),e(N,u8o),e(N,Sb),e(Sb,wne),e(wne,b8o),e(Sb,v8o),e(Sb,GD),e(GD,T8o),e(Sb,F8o),e(N,C8o),e(N,Pb),e(Pb,Ane),e(Ane,M8o),e(Pb,E8o),e(Pb,OD),e(OD,y8o),e(Pb,w8o),e(N,A8o),e(N,$b),e($b,Lne),e(Lne,L8o),e($b,B8o),e($b,XD),e(XD,x8o),e($b,k8o),e(N,R8o),e(N,Ib),e(Ib,Bne),e(Bne,S8o),e(Ib,P8o),e(Ib,VD),e(VD,$8o),e(Ib,I8o),e(N,D8o),e(N,Db),e(Db,xne),e(xne,j8o),e(Db,N8o),e(Db,zD),e(zD,q8o),e(Db,G8o),e(N,O8o),e(N,jb),e(jb,kne),e(kne,X8o),e(jb,V8o),e(jb,WD),e(WD,z8o),e(jb,W8o),e(N,Q8o),e(N,Nb),e(Nb,Rne),e(Rne,H8o),e(Nb,U8o),e(Nb,QD),e(QD,J8o),e(Nb,Y8o),e(N,K8o),e(N,qb),e(qb,Sne),e(Sne,Z8o),e(qb,e7o),e(qb,HD),e(HD,o7o),e(qb,r7o),e(ze,t7o),e(ze,Gb),e(Gb,a7o),e(Gb,Pne),e(Pne,n7o),e(Gb,s7o),e(Gb,$ne),e($ne,l7o),e(ze,i7o),e(ze,Ine),e(Ine,d7o),e(ze,c7o),g(b3,ze,null),b(c,$Be,u),b(c,_d,u),e(_d,Ob),e(Ob,Dne),g(v3,Dne,null),e(_d,f7o),e(_d,jne),e(jne,m7o),b(c,IBe,u),b(c,ar,u),g(T3,ar,null),e(ar,g7o),e(ar,ud),e(ud,h7o),e(ud,Nne),e(Nne,p7o),e(ud,_7o),e(ud,qne),e(qne,u7o),e(ud,b7o),e(ar,v7o),e(ar,F3),e(F3,T7o),e(F3,Gne),e(Gne,F7o),e(F3,C7o),e(ar,M7o),e(ar,Yr),g(C3,Yr,null),e(Yr,E7o),e(Yr,One),e(One,y7o),e(Yr,w7o),e(Yr,bd),e(bd,A7o),e(bd,Xne),e(Xne,L7o),e(bd,B7o),e(bd,Vne),e(Vne,x7o),e(bd,k7o),e(Yr,R7o),e(Yr,zne),e(zne,S7o),e(Yr,P7o),g(M3,Yr,null),e(ar,$7o),e(ar,We),g(E3,We,null),e(We,I7o),e(We,Wne),e(Wne,D7o),e(We,j7o),e(We,Ha),e(Ha,N7o),e(Ha,Qne),e(Qne,q7o),e(Ha,G7o),e(Ha,Hne),e(Hne,O7o),e(Ha,X7o),e(Ha,Une),e(Une,V7o),e(Ha,z7o),e(We,W7o),e(We,R),e(R,Xb),e(Xb,Jne),e(Jne,Q7o),e(Xb,H7o),e(Xb,UD),e(UD,U7o),e(Xb,J7o),e(R,Y7o),e(R,Vb),e(Vb,Yne),e(Yne,K7o),e(Vb,Z7o),e(Vb,JD),e(JD,eBo),e(Vb,oBo),e(R,rBo),e(R,zb),e(zb,Kne),e(Kne,tBo),e(zb,aBo),e(zb,YD),e(YD,nBo),e(zb,sBo),e(R,lBo),e(R,Wb),e(Wb,Zne),e(Zne,iBo),e(Wb,dBo),e(Wb,KD),e(KD,cBo),e(Wb,fBo),e(R,mBo),e(R,Qb),e(Qb,ese),e(ese,gBo),e(Qb,hBo),e(Qb,ZD),e(ZD,pBo),e(Qb,_Bo),e(R,uBo),e(R,Hb),e(Hb,ose),e(ose,bBo),e(Hb,vBo),e(Hb,ej),e(ej,TBo),e(Hb,FBo),e(R,CBo),e(R,Ub),e(Ub,rse),e(rse,MBo),e(Ub,EBo),e(Ub,oj),e(oj,yBo),e(Ub,wBo),e(R,ABo),e(R,Jb),e(Jb,tse),e(tse,LBo),e(Jb,BBo),e(Jb,rj),e(rj,xBo),e(Jb,kBo),e(R,RBo),e(R,Yb),e(Yb,ase),e(ase,SBo),e(Yb,PBo),e(Yb,tj),e(tj,$Bo),e(Yb,IBo),e(R,DBo),e(R,Kb),e(Kb,nse),e(nse,jBo),e(Kb,NBo),e(Kb,aj),e(aj,qBo),e(Kb,GBo),e(R,OBo),e(R,Zb),e(Zb,sse),e(sse,XBo),e(Zb,VBo),e(Zb,nj),e(nj,zBo),e(Zb,WBo),e(R,QBo),e(R,e5),e(e5,lse),e(lse,HBo),e(e5,UBo),e(e5,sj),e(sj,JBo),e(e5,YBo),e(R,KBo),e(R,o5),e(o5,ise),e(ise,ZBo),e(o5,exo),e(o5,lj),e(lj,oxo),e(o5,rxo),e(R,txo),e(R,r5),e(r5,dse),e(dse,axo),e(r5,nxo),e(r5,ij),e(ij,sxo),e(r5,lxo),e(R,ixo),e(R,t5),e(t5,cse),e(cse,dxo),e(t5,cxo),e(t5,dj),e(dj,fxo),e(t5,mxo),e(R,gxo),e(R,a5),e(a5,fse),e(fse,hxo),e(a5,pxo),e(a5,cj),e(cj,_xo),e(a5,uxo),e(R,bxo),e(R,n5),e(n5,mse),e(mse,vxo),e(n5,Txo),e(n5,fj),e(fj,Fxo),e(n5,Cxo),e(R,Mxo),e(R,s5),e(s5,gse),e(gse,Exo),e(s5,yxo),e(s5,mj),e(mj,wxo),e(s5,Axo),e(R,Lxo),e(R,l5),e(l5,hse),e(hse,Bxo),e(l5,xxo),e(l5,gj),e(gj,kxo),e(l5,Rxo),e(R,Sxo),e(R,i5),e(i5,pse),e(pse,Pxo),e(i5,$xo),e(i5,hj),e(hj,Ixo),e(i5,Dxo),e(R,jxo),e(R,d5),e(d5,_se),e(_se,Nxo),e(d5,qxo),e(d5,pj),e(pj,Gxo),e(d5,Oxo),e(R,Xxo),e(R,c5),e(c5,use),e(use,Vxo),e(c5,zxo),e(c5,_j),e(_j,Wxo),e(c5,Qxo),e(R,Hxo),e(R,f5),e(f5,bse),e(bse,Uxo),e(f5,Jxo),e(f5,uj),e(uj,Yxo),e(f5,Kxo),e(R,Zxo),e(R,m5),e(m5,vse),e(vse,eko),e(m5,oko),e(m5,bj),e(bj,rko),e(m5,tko),e(R,ako),e(R,g5),e(g5,Tse),e(Tse,nko),e(g5,sko),e(g5,vj),e(vj,lko),e(g5,iko),e(R,dko),e(R,h5),e(h5,Fse),e(Fse,cko),e(h5,fko),e(h5,Tj),e(Tj,mko),e(h5,gko),e(R,hko),e(R,p5),e(p5,Cse),e(Cse,pko),e(p5,_ko),e(p5,Fj),e(Fj,uko),e(p5,bko),e(R,vko),e(R,_5),e(_5,Mse),e(Mse,Tko),e(_5,Fko),e(_5,Cj),e(Cj,Cko),e(_5,Mko),e(R,Eko),e(R,u5),e(u5,Ese),e(Ese,yko),e(u5,wko),e(u5,Mj),e(Mj,Ako),e(u5,Lko),e(R,Bko),e(R,b5),e(b5,yse),e(yse,xko),e(b5,kko),e(b5,Ej),e(Ej,Rko),e(b5,Sko),e(R,Pko),e(R,v5),e(v5,wse),e(wse,$ko),e(v5,Iko),e(v5,yj),e(yj,Dko),e(v5,jko),e(R,Nko),e(R,T5),e(T5,Ase),e(Ase,qko),e(T5,Gko),e(T5,wj),e(wj,Oko),e(T5,Xko),e(R,Vko),e(R,F5),e(F5,Lse),e(Lse,zko),e(F5,Wko),e(F5,Aj),e(Aj,Qko),e(F5,Hko),e(R,Uko),e(R,C5),e(C5,Bse),e(Bse,Jko),e(C5,Yko),e(C5,Lj),e(Lj,Kko),e(C5,Zko),e(R,eRo),e(R,M5),e(M5,xse),e(xse,oRo),e(M5,rRo),e(M5,Bj),e(Bj,tRo),e(M5,aRo),e(R,nRo),e(R,E5),e(E5,kse),e(kse,sRo),e(E5,lRo),e(E5,xj),e(xj,iRo),e(E5,dRo),e(R,cRo),e(R,y5),e(y5,Rse),e(Rse,fRo),e(y5,mRo),e(y5,kj),e(kj,gRo),e(y5,hRo),e(R,pRo),e(R,w5),e(w5,Sse),e(Sse,_Ro),e(w5,uRo),e(w5,Rj),e(Rj,bRo),e(w5,vRo),e(R,TRo),e(R,A5),e(A5,Pse),e(Pse,FRo),e(A5,CRo),e(A5,Sj),e(Sj,MRo),e(A5,ERo),e(We,yRo),e(We,L5),e(L5,wRo),e(L5,$se),e($se,ARo),e(L5,LRo),e(L5,Ise),e(Ise,BRo),e(We,xRo),e(We,Dse),e(Dse,kRo),e(We,RRo),g(y3,We,null),b(c,DBe,u),b(c,vd,u),e(vd,B5),e(B5,jse),g(w3,jse,null),e(vd,SRo),e(vd,Nse),e(Nse,PRo),b(c,jBe,u),b(c,nr,u),g(A3,nr,null),e(nr,$Ro),e(nr,Td),e(Td,IRo),e(Td,qse),e(qse,DRo),e(Td,jRo),e(Td,Gse),e(Gse,NRo),e(Td,qRo),e(nr,GRo),e(nr,L3),e(L3,ORo),e(L3,Ose),e(Ose,XRo),e(L3,VRo),e(nr,zRo),e(nr,Kr),g(B3,Kr,null),e(Kr,WRo),e(Kr,Xse),e(Xse,QRo),e(Kr,HRo),e(Kr,Fd),e(Fd,URo),e(Fd,Vse),e(Vse,JRo),e(Fd,YRo),e(Fd,zse),e(zse,KRo),e(Fd,ZRo),e(Kr,eSo),e(Kr,Wse),e(Wse,oSo),e(Kr,rSo),g(x3,Kr,null),e(nr,tSo),e(nr,Qe),g(k3,Qe,null),e(Qe,aSo),e(Qe,Qse),e(Qse,nSo),e(Qe,sSo),e(Qe,Ua),e(Ua,lSo),e(Ua,Hse),e(Hse,iSo),e(Ua,dSo),e(Ua,Use),e(Use,cSo),e(Ua,fSo),e(Ua,Jse),e(Jse,mSo),e(Ua,gSo),e(Qe,hSo),e(Qe,Yse),e(Yse,x5),e(x5,Kse),e(Kse,pSo),e(x5,_So),e(x5,Pj),e(Pj,uSo),e(x5,bSo),e(Qe,vSo),e(Qe,k5),e(k5,TSo),e(k5,Zse),e(Zse,FSo),e(k5,CSo),e(k5,ele),e(ele,MSo),e(Qe,ESo),e(Qe,ole),e(ole,ySo),e(Qe,wSo),g(R3,Qe,null),b(c,NBe,u),b(c,Cd,u),e(Cd,R5),e(R5,rle),g(S3,rle,null),e(Cd,ASo),e(Cd,tle),e(tle,LSo),b(c,qBe,u),b(c,sr,u),g(P3,sr,null),e(sr,BSo),e(sr,Md),e(Md,xSo),e(Md,ale),e(ale,kSo),e(Md,RSo),e(Md,nle),e(nle,SSo),e(Md,PSo),e(sr,$So),e(sr,$3),e($3,ISo),e($3,sle),e(sle,DSo),e($3,jSo),e(sr,NSo),e(sr,Zr),g(I3,Zr,null),e(Zr,qSo),e(Zr,lle),e(lle,GSo),e(Zr,OSo),e(Zr,Ed),e(Ed,XSo),e(Ed,ile),e(ile,VSo),e(Ed,zSo),e(Ed,dle),e(dle,WSo),e(Ed,QSo),e(Zr,HSo),e(Zr,cle),e(cle,USo),e(Zr,JSo),g(D3,Zr,null),e(sr,YSo),e(sr,He),g(j3,He,null),e(He,KSo),e(He,fle),e(fle,ZSo),e(He,ePo),e(He,Ja),e(Ja,oPo),e(Ja,mle),e(mle,rPo),e(Ja,tPo),e(Ja,gle),e(gle,aPo),e(Ja,nPo),e(Ja,hle),e(hle,sPo),e(Ja,lPo),e(He,iPo),e(He,Fe),e(Fe,S5),e(S5,ple),e(ple,dPo),e(S5,cPo),e(S5,$j),e($j,fPo),e(S5,mPo),e(Fe,gPo),e(Fe,P5),e(P5,_le),e(_le,hPo),e(P5,pPo),e(P5,Ij),e(Ij,_Po),e(P5,uPo),e(Fe,bPo),e(Fe,$s),e($s,ule),e(ule,vPo),e($s,TPo),e($s,Dj),e(Dj,FPo),e($s,CPo),e($s,jj),e(jj,MPo),e($s,EPo),e(Fe,yPo),e(Fe,$5),e($5,ble),e(ble,wPo),e($5,APo),e($5,Nj),e(Nj,LPo),e($5,BPo),e(Fe,xPo),e(Fe,la),e(la,vle),e(vle,kPo),e(la,RPo),e(la,qj),e(qj,SPo),e(la,PPo),e(la,Gj),e(Gj,$Po),e(la,IPo),e(la,Oj),e(Oj,DPo),e(la,jPo),e(Fe,NPo),e(Fe,I5),e(I5,Tle),e(Tle,qPo),e(I5,GPo),e(I5,Xj),e(Xj,OPo),e(I5,XPo),e(Fe,VPo),e(Fe,D5),e(D5,Fle),e(Fle,zPo),e(D5,WPo),e(D5,Vj),e(Vj,QPo),e(D5,HPo),e(Fe,UPo),e(Fe,j5),e(j5,Cle),e(Cle,JPo),e(j5,YPo),e(j5,zj),e(zj,KPo),e(j5,ZPo),e(Fe,e$o),e(Fe,N5),e(N5,Mle),e(Mle,o$o),e(N5,r$o),e(N5,Wj),e(Wj,t$o),e(N5,a$o),e(He,n$o),e(He,q5),e(q5,s$o),e(q5,Ele),e(Ele,l$o),e(q5,i$o),e(q5,yle),e(yle,d$o),e(He,c$o),e(He,wle),e(wle,f$o),e(He,m$o),g(N3,He,null),b(c,GBe,u),b(c,yd,u),e(yd,G5),e(G5,Ale),g(q3,Ale,null),e(yd,g$o),e(yd,Lle),e(Lle,h$o),b(c,OBe,u),b(c,lr,u),g(G3,lr,null),e(lr,p$o),e(lr,wd),e(wd,_$o),e(wd,Ble),e(Ble,u$o),e(wd,b$o),e(wd,xle),e(xle,v$o),e(wd,T$o),e(lr,F$o),e(lr,O3),e(O3,C$o),e(O3,kle),e(kle,M$o),e(O3,E$o),e(lr,y$o),e(lr,et),g(X3,et,null),e(et,w$o),e(et,Rle),e(Rle,A$o),e(et,L$o),e(et,Ad),e(Ad,B$o),e(Ad,Sle),e(Sle,x$o),e(Ad,k$o),e(Ad,Ple),e(Ple,R$o),e(Ad,S$o),e(et,P$o),e(et,$le),e($le,$$o),e(et,I$o),g(V3,et,null),e(lr,D$o),e(lr,Ue),g(z3,Ue,null),e(Ue,j$o),e(Ue,Ile),e(Ile,N$o),e(Ue,q$o),e(Ue,Ya),e(Ya,G$o),e(Ya,Dle),e(Dle,O$o),e(Ya,X$o),e(Ya,jle),e(jle,V$o),e(Ya,z$o),e(Ya,Nle),e(Nle,W$o),e(Ya,Q$o),e(Ue,H$o),e(Ue,qle),e(qle,O5),e(O5,Gle),e(Gle,U$o),e(O5,J$o),e(O5,Qj),e(Qj,Y$o),e(O5,K$o),e(Ue,Z$o),e(Ue,X5),e(X5,eIo),e(X5,Ole),e(Ole,oIo),e(X5,rIo),e(X5,Xle),e(Xle,tIo),e(Ue,aIo),e(Ue,Vle),e(Vle,nIo),e(Ue,sIo),g(W3,Ue,null),b(c,XBe,u),b(c,Ld,u),e(Ld,V5),e(V5,zle),g(Q3,zle,null),e(Ld,lIo),e(Ld,Wle),e(Wle,iIo),b(c,VBe,u),b(c,ir,u),g(H3,ir,null),e(ir,dIo),e(ir,Bd),e(Bd,cIo),e(Bd,Qle),e(Qle,fIo),e(Bd,mIo),e(Bd,Hle),e(Hle,gIo),e(Bd,hIo),e(ir,pIo),e(ir,U3),e(U3,_Io),e(U3,Ule),e(Ule,uIo),e(U3,bIo),e(ir,vIo),e(ir,ot),g(J3,ot,null),e(ot,TIo),e(ot,Jle),e(Jle,FIo),e(ot,CIo),e(ot,xd),e(xd,MIo),e(xd,Yle),e(Yle,EIo),e(xd,yIo),e(xd,Kle),e(Kle,wIo),e(xd,AIo),e(ot,LIo),e(ot,Zle),e(Zle,BIo),e(ot,xIo),g(Y3,ot,null),e(ir,kIo),e(ir,Je),g(K3,Je,null),e(Je,RIo),e(Je,eie),e(eie,SIo),e(Je,PIo),e(Je,Ka),e(Ka,$Io),e(Ka,oie),e(oie,IIo),e(Ka,DIo),e(Ka,rie),e(rie,jIo),e(Ka,NIo),e(Ka,tie),e(tie,qIo),e(Ka,GIo),e(Je,OIo),e(Je,xe),e(xe,z5),e(z5,aie),e(aie,XIo),e(z5,VIo),e(z5,Hj),e(Hj,zIo),e(z5,WIo),e(xe,QIo),e(xe,W5),e(W5,nie),e(nie,HIo),e(W5,UIo),e(W5,Uj),e(Uj,JIo),e(W5,YIo),e(xe,KIo),e(xe,Q5),e(Q5,sie),e(sie,ZIo),e(Q5,eDo),e(Q5,Jj),e(Jj,oDo),e(Q5,rDo),e(xe,tDo),e(xe,H5),e(H5,lie),e(lie,aDo),e(H5,nDo),e(H5,Yj),e(Yj,sDo),e(H5,lDo),e(xe,iDo),e(xe,U5),e(U5,iie),e(iie,dDo),e(U5,cDo),e(U5,Kj),e(Kj,fDo),e(U5,mDo),e(xe,gDo),e(xe,J5),e(J5,die),e(die,hDo),e(J5,pDo),e(J5,Zj),e(Zj,_Do),e(J5,uDo),e(xe,bDo),e(xe,Y5),e(Y5,cie),e(cie,vDo),e(Y5,TDo),e(Y5,eN),e(eN,FDo),e(Y5,CDo),e(xe,MDo),e(xe,K5),e(K5,fie),e(fie,EDo),e(K5,yDo),e(K5,oN),e(oN,wDo),e(K5,ADo),e(Je,LDo),e(Je,Z5),e(Z5,BDo),e(Z5,mie),e(mie,xDo),e(Z5,kDo),e(Z5,gie),e(gie,RDo),e(Je,SDo),e(Je,hie),e(hie,PDo),e(Je,$Do),g(Z3,Je,null),b(c,zBe,u),b(c,kd,u),e(kd,e2),e(e2,pie),g(ey,pie,null),e(kd,IDo),e(kd,_ie),e(_ie,DDo),b(c,WBe,u),b(c,dr,u),g(oy,dr,null),e(dr,jDo),e(dr,Rd),e(Rd,NDo),e(Rd,uie),e(uie,qDo),e(Rd,GDo),e(Rd,bie),e(bie,ODo),e(Rd,XDo),e(dr,VDo),e(dr,ry),e(ry,zDo),e(ry,vie),e(vie,WDo),e(ry,QDo),e(dr,HDo),e(dr,rt),g(ty,rt,null),e(rt,UDo),e(rt,Tie),e(Tie,JDo),e(rt,YDo),e(rt,Sd),e(Sd,KDo),e(Sd,Fie),e(Fie,ZDo),e(Sd,ejo),e(Sd,Cie),e(Cie,ojo),e(Sd,rjo),e(rt,tjo),e(rt,Mie),e(Mie,ajo),e(rt,njo),g(ay,rt,null),e(dr,sjo),e(dr,Ye),g(ny,Ye,null),e(Ye,ljo),e(Ye,Eie),e(Eie,ijo),e(Ye,djo),e(Ye,Za),e(Za,cjo),e(Za,yie),e(yie,fjo),e(Za,mjo),e(Za,wie),e(wie,gjo),e(Za,hjo),e(Za,Aie),e(Aie,pjo),e(Za,_jo),e(Ye,ujo),e(Ye,en),e(en,o2),e(o2,Lie),e(Lie,bjo),e(o2,vjo),e(o2,rN),e(rN,Tjo),e(o2,Fjo),e(en,Cjo),e(en,r2),e(r2,Bie),e(Bie,Mjo),e(r2,Ejo),e(r2,tN),e(tN,yjo),e(r2,wjo),e(en,Ajo),e(en,t2),e(t2,xie),e(xie,Ljo),e(t2,Bjo),e(t2,aN),e(aN,xjo),e(t2,kjo),e(en,Rjo),e(en,a2),e(a2,kie),e(kie,Sjo),e(a2,Pjo),e(a2,nN),e(nN,$jo),e(a2,Ijo),e(Ye,Djo),e(Ye,n2),e(n2,jjo),e(n2,Rie),e(Rie,Njo),e(n2,qjo),e(n2,Sie),e(Sie,Gjo),e(Ye,Ojo),e(Ye,Pie),e(Pie,Xjo),e(Ye,Vjo),g(sy,Ye,null),b(c,QBe,u),b(c,Pd,u),e(Pd,s2),e(s2,$ie),g(ly,$ie,null),e(Pd,zjo),e(Pd,Iie),e(Iie,Wjo),b(c,HBe,u),b(c,cr,u),g(iy,cr,null),e(cr,Qjo),e(cr,$d),e($d,Hjo),e($d,Die),e(Die,Ujo),e($d,Jjo),e($d,jie),e(jie,Yjo),e($d,Kjo),e(cr,Zjo),e(cr,dy),e(dy,eNo),e(dy,Nie),e(Nie,oNo),e(dy,rNo),e(cr,tNo),e(cr,tt),g(cy,tt,null),e(tt,aNo),e(tt,qie),e(qie,nNo),e(tt,sNo),e(tt,Id),e(Id,lNo),e(Id,Gie),e(Gie,iNo),e(Id,dNo),e(Id,Oie),e(Oie,cNo),e(Id,fNo),e(tt,mNo),e(tt,Xie),e(Xie,gNo),e(tt,hNo),g(fy,tt,null),e(cr,pNo),e(cr,Ke),g(my,Ke,null),e(Ke,_No),e(Ke,Vie),e(Vie,uNo),e(Ke,bNo),e(Ke,on),e(on,vNo),e(on,zie),e(zie,TNo),e(on,FNo),e(on,Wie),e(Wie,CNo),e(on,MNo),e(on,Qie),e(Qie,ENo),e(on,yNo),e(Ke,wNo),e(Ke,ke),e(ke,l2),e(l2,Hie),e(Hie,ANo),e(l2,LNo),e(l2,sN),e(sN,BNo),e(l2,xNo),e(ke,kNo),e(ke,i2),e(i2,Uie),e(Uie,RNo),e(i2,SNo),e(i2,lN),e(lN,PNo),e(i2,$No),e(ke,INo),e(ke,d2),e(d2,Jie),e(Jie,DNo),e(d2,jNo),e(d2,iN),e(iN,NNo),e(d2,qNo),e(ke,GNo),e(ke,c2),e(c2,Yie),e(Yie,ONo),e(c2,XNo),e(c2,dN),e(dN,VNo),e(c2,zNo),e(ke,WNo),e(ke,f2),e(f2,Kie),e(Kie,QNo),e(f2,HNo),e(f2,cN),e(cN,UNo),e(f2,JNo),e(ke,YNo),e(ke,m2),e(m2,Zie),e(Zie,KNo),e(m2,ZNo),e(m2,fN),e(fN,eqo),e(m2,oqo),e(ke,rqo),e(ke,g2),e(g2,ede),e(ede,tqo),e(g2,aqo),e(g2,mN),e(mN,nqo),e(g2,sqo),e(ke,lqo),e(ke,h2),e(h2,ode),e(ode,iqo),e(h2,dqo),e(h2,gN),e(gN,cqo),e(h2,fqo),e(Ke,mqo),e(Ke,p2),e(p2,gqo),e(p2,rde),e(rde,hqo),e(p2,pqo),e(p2,tde),e(tde,_qo),e(Ke,uqo),e(Ke,ade),e(ade,bqo),e(Ke,vqo),g(gy,Ke,null),b(c,UBe,u),b(c,Dd,u),e(Dd,_2),e(_2,nde),g(hy,nde,null),e(Dd,Tqo),e(Dd,sde),e(sde,Fqo),b(c,JBe,u),b(c,fr,u),g(py,fr,null),e(fr,Cqo),e(fr,jd),e(jd,Mqo),e(jd,lde),e(lde,Eqo),e(jd,yqo),e(jd,ide),e(ide,wqo),e(jd,Aqo),e(fr,Lqo),e(fr,_y),e(_y,Bqo),e(_y,dde),e(dde,xqo),e(_y,kqo),e(fr,Rqo),e(fr,at),g(uy,at,null),e(at,Sqo),e(at,cde),e(cde,Pqo),e(at,$qo),e(at,Nd),e(Nd,Iqo),e(Nd,fde),e(fde,Dqo),e(Nd,jqo),e(Nd,mde),e(mde,Nqo),e(Nd,qqo),e(at,Gqo),e(at,gde),e(gde,Oqo),e(at,Xqo),g(by,at,null),e(fr,Vqo),e(fr,Ze),g(vy,Ze,null),e(Ze,zqo),e(Ze,hde),e(hde,Wqo),e(Ze,Qqo),e(Ze,rn),e(rn,Hqo),e(rn,pde),e(pde,Uqo),e(rn,Jqo),e(rn,_de),e(_de,Yqo),e(rn,Kqo),e(rn,ude),e(ude,Zqo),e(rn,eGo),e(Ze,oGo),e(Ze,Ty),e(Ty,u2),e(u2,bde),e(bde,rGo),e(u2,tGo),e(u2,hN),e(hN,aGo),e(u2,nGo),e(Ty,sGo),e(Ty,b2),e(b2,vde),e(vde,lGo),e(b2,iGo),e(b2,pN),e(pN,dGo),e(b2,cGo),e(Ze,fGo),e(Ze,v2),e(v2,mGo),e(v2,Tde),e(Tde,gGo),e(v2,hGo),e(v2,Fde),e(Fde,pGo),e(Ze,_Go),e(Ze,Cde),e(Cde,uGo),e(Ze,bGo),g(Fy,Ze,null),b(c,YBe,u),b(c,qd,u),e(qd,T2),e(T2,Mde),g(Cy,Mde,null),e(qd,vGo),e(qd,Ede),e(Ede,TGo),b(c,KBe,u),b(c,mr,u),g(My,mr,null),e(mr,FGo),e(mr,Gd),e(Gd,CGo),e(Gd,yde),e(yde,MGo),e(Gd,EGo),e(Gd,wde),e(wde,yGo),e(Gd,wGo),e(mr,AGo),e(mr,Ey),e(Ey,LGo),e(Ey,Ade),e(Ade,BGo),e(Ey,xGo),e(mr,kGo),e(mr,nt),g(yy,nt,null),e(nt,RGo),e(nt,Lde),e(Lde,SGo),e(nt,PGo),e(nt,Od),e(Od,$Go),e(Od,Bde),e(Bde,IGo),e(Od,DGo),e(Od,xde),e(xde,jGo),e(Od,NGo),e(nt,qGo),e(nt,kde),e(kde,GGo),e(nt,OGo),g(wy,nt,null),e(mr,XGo),e(mr,eo),g(Ay,eo,null),e(eo,VGo),e(eo,Rde),e(Rde,zGo),e(eo,WGo),e(eo,tn),e(tn,QGo),e(tn,Sde),e(Sde,HGo),e(tn,UGo),e(tn,Pde),e(Pde,JGo),e(tn,YGo),e(tn,$de),e($de,KGo),e(tn,ZGo),e(eo,eOo),e(eo,an),e(an,F2),e(F2,Ide),e(Ide,oOo),e(F2,rOo),e(F2,_N),e(_N,tOo),e(F2,aOo),e(an,nOo),e(an,C2),e(C2,Dde),e(Dde,sOo),e(C2,lOo),e(C2,uN),e(uN,iOo),e(C2,dOo),e(an,cOo),e(an,M2),e(M2,jde),e(jde,fOo),e(M2,mOo),e(M2,bN),e(bN,gOo),e(M2,hOo),e(an,pOo),e(an,E2),e(E2,Nde),e(Nde,_Oo),e(E2,uOo),e(E2,vN),e(vN,bOo),e(E2,vOo),e(eo,TOo),e(eo,y2),e(y2,FOo),e(y2,qde),e(qde,COo),e(y2,MOo),e(y2,Gde),e(Gde,EOo),e(eo,yOo),e(eo,Ode),e(Ode,wOo),e(eo,AOo),g(Ly,eo,null),b(c,ZBe,u),b(c,Xd,u),e(Xd,w2),e(w2,Xde),g(By,Xde,null),e(Xd,LOo),e(Xd,Vde),e(Vde,BOo),b(c,exe,u),b(c,gr,u),g(xy,gr,null),e(gr,xOo),e(gr,Vd),e(Vd,kOo),e(Vd,zde),e(zde,ROo),e(Vd,SOo),e(Vd,Wde),e(Wde,POo),e(Vd,$Oo),e(gr,IOo),e(gr,ky),e(ky,DOo),e(ky,Qde),e(Qde,jOo),e(ky,NOo),e(gr,qOo),e(gr,st),g(Ry,st,null),e(st,GOo),e(st,Hde),e(Hde,OOo),e(st,XOo),e(st,zd),e(zd,VOo),e(zd,Ude),e(Ude,zOo),e(zd,WOo),e(zd,Jde),e(Jde,QOo),e(zd,HOo),e(st,UOo),e(st,Yde),e(Yde,JOo),e(st,YOo),g(Sy,st,null),e(gr,KOo),e(gr,oo),g(Py,oo,null),e(oo,ZOo),e(oo,Kde),e(Kde,eXo),e(oo,oXo),e(oo,nn),e(nn,rXo),e(nn,Zde),e(Zde,tXo),e(nn,aXo),e(nn,ece),e(ece,nXo),e(nn,sXo),e(nn,oce),e(oce,lXo),e(nn,iXo),e(oo,dXo),e(oo,Wd),e(Wd,A2),e(A2,rce),e(rce,cXo),e(A2,fXo),e(A2,TN),e(TN,mXo),e(A2,gXo),e(Wd,hXo),e(Wd,L2),e(L2,tce),e(tce,pXo),e(L2,_Xo),e(L2,FN),e(FN,uXo),e(L2,bXo),e(Wd,vXo),e(Wd,B2),e(B2,ace),e(ace,TXo),e(B2,FXo),e(B2,CN),e(CN,CXo),e(B2,MXo),e(oo,EXo),e(oo,x2),e(x2,yXo),e(x2,nce),e(nce,wXo),e(x2,AXo),e(x2,sce),e(sce,LXo),e(oo,BXo),e(oo,lce),e(lce,xXo),e(oo,kXo),g($y,oo,null),b(c,oxe,u),b(c,Qd,u),e(Qd,k2),e(k2,ice),g(Iy,ice,null),e(Qd,RXo),e(Qd,dce),e(dce,SXo),b(c,rxe,u),b(c,hr,u),g(Dy,hr,null),e(hr,PXo),e(hr,Hd),e(Hd,$Xo),e(Hd,cce),e(cce,IXo),e(Hd,DXo),e(Hd,fce),e(fce,jXo),e(Hd,NXo),e(hr,qXo),e(hr,jy),e(jy,GXo),e(jy,mce),e(mce,OXo),e(jy,XXo),e(hr,VXo),e(hr,lt),g(Ny,lt,null),e(lt,zXo),e(lt,gce),e(gce,WXo),e(lt,QXo),e(lt,Ud),e(Ud,HXo),e(Ud,hce),e(hce,UXo),e(Ud,JXo),e(Ud,pce),e(pce,YXo),e(Ud,KXo),e(lt,ZXo),e(lt,_ce),e(_ce,eVo),e(lt,oVo),g(qy,lt,null),e(hr,rVo),e(hr,ro),g(Gy,ro,null),e(ro,tVo),e(ro,uce),e(uce,aVo),e(ro,nVo),e(ro,sn),e(sn,sVo),e(sn,bce),e(bce,lVo),e(sn,iVo),e(sn,vce),e(vce,dVo),e(sn,cVo),e(sn,Tce),e(Tce,fVo),e(sn,mVo),e(ro,gVo),e(ro,Fce),e(Fce,R2),e(R2,Cce),e(Cce,hVo),e(R2,pVo),e(R2,MN),e(MN,_Vo),e(R2,uVo),e(ro,bVo),e(ro,S2),e(S2,vVo),e(S2,Mce),e(Mce,TVo),e(S2,FVo),e(S2,Ece),e(Ece,CVo),e(ro,MVo),e(ro,yce),e(yce,EVo),e(ro,yVo),g(Oy,ro,null),b(c,txe,u),b(c,Jd,u),e(Jd,P2),e(P2,wce),g(Xy,wce,null),e(Jd,wVo),e(Jd,Ace),e(Ace,AVo),b(c,axe,u),b(c,pr,u),g(Vy,pr,null),e(pr,LVo),e(pr,Yd),e(Yd,BVo),e(Yd,Lce),e(Lce,xVo),e(Yd,kVo),e(Yd,Bce),e(Bce,RVo),e(Yd,SVo),e(pr,PVo),e(pr,zy),e(zy,$Vo),e(zy,xce),e(xce,IVo),e(zy,DVo),e(pr,jVo),e(pr,it),g(Wy,it,null),e(it,NVo),e(it,kce),e(kce,qVo),e(it,GVo),e(it,Kd),e(Kd,OVo),e(Kd,Rce),e(Rce,XVo),e(Kd,VVo),e(Kd,Sce),e(Sce,zVo),e(Kd,WVo),e(it,QVo),e(it,Pce),e(Pce,HVo),e(it,UVo),g(Qy,it,null),e(pr,JVo),e(pr,to),g(Hy,to,null),e(to,YVo),e(to,$ce),e($ce,KVo),e(to,ZVo),e(to,ln),e(ln,ezo),e(ln,Ice),e(Ice,ozo),e(ln,rzo),e(ln,Dce),e(Dce,tzo),e(ln,azo),e(ln,jce),e(jce,nzo),e(ln,szo),e(to,lzo),e(to,Nce),e(Nce,$2),e($2,qce),e(qce,izo),e($2,dzo),e($2,EN),e(EN,czo),e($2,fzo),e(to,mzo),e(to,I2),e(I2,gzo),e(I2,Gce),e(Gce,hzo),e(I2,pzo),e(I2,Oce),e(Oce,_zo),e(to,uzo),e(to,Xce),e(Xce,bzo),e(to,vzo),g(Uy,to,null),b(c,nxe,u),b(c,Zd,u),e(Zd,D2),e(D2,Vce),g(Jy,Vce,null),e(Zd,Tzo),e(Zd,zce),e(zce,Fzo),b(c,sxe,u),b(c,_r,u),g(Yy,_r,null),e(_r,Czo),e(_r,ec),e(ec,Mzo),e(ec,Wce),e(Wce,Ezo),e(ec,yzo),e(ec,Qce),e(Qce,wzo),e(ec,Azo),e(_r,Lzo),e(_r,Ky),e(Ky,Bzo),e(Ky,Hce),e(Hce,xzo),e(Ky,kzo),e(_r,Rzo),e(_r,dt),g(Zy,dt,null),e(dt,Szo),e(dt,Uce),e(Uce,Pzo),e(dt,$zo),e(dt,oc),e(oc,Izo),e(oc,Jce),e(Jce,Dzo),e(oc,jzo),e(oc,Yce),e(Yce,Nzo),e(oc,qzo),e(dt,Gzo),e(dt,Kce),e(Kce,Ozo),e(dt,Xzo),g(ew,dt,null),e(_r,Vzo),e(_r,ao),g(ow,ao,null),e(ao,zzo),e(ao,Zce),e(Zce,Wzo),e(ao,Qzo),e(ao,dn),e(dn,Hzo),e(dn,efe),e(efe,Uzo),e(dn,Jzo),e(dn,ofe),e(ofe,Yzo),e(dn,Kzo),e(dn,rfe),e(rfe,Zzo),e(dn,eWo),e(ao,oWo),e(ao,rw),e(rw,j2),e(j2,tfe),e(tfe,rWo),e(j2,tWo),e(j2,yN),e(yN,aWo),e(j2,nWo),e(rw,sWo),e(rw,N2),e(N2,afe),e(afe,lWo),e(N2,iWo),e(N2,wN),e(wN,dWo),e(N2,cWo),e(ao,fWo),e(ao,q2),e(q2,mWo),e(q2,nfe),e(nfe,gWo),e(q2,hWo),e(q2,sfe),e(sfe,pWo),e(ao,_Wo),e(ao,lfe),e(lfe,uWo),e(ao,bWo),g(tw,ao,null),b(c,lxe,u),b(c,rc,u),e(rc,G2),e(G2,ife),g(aw,ife,null),e(rc,vWo),e(rc,dfe),e(dfe,TWo),b(c,ixe,u),b(c,ur,u),g(nw,ur,null),e(ur,FWo),e(ur,tc),e(tc,CWo),e(tc,cfe),e(cfe,MWo),e(tc,EWo),e(tc,ffe),e(ffe,yWo),e(tc,wWo),e(ur,AWo),e(ur,sw),e(sw,LWo),e(sw,mfe),e(mfe,BWo),e(sw,xWo),e(ur,kWo),e(ur,ct),g(lw,ct,null),e(ct,RWo),e(ct,gfe),e(gfe,SWo),e(ct,PWo),e(ct,ac),e(ac,$Wo),e(ac,hfe),e(hfe,IWo),e(ac,DWo),e(ac,pfe),e(pfe,jWo),e(ac,NWo),e(ct,qWo),e(ct,_fe),e(_fe,GWo),e(ct,OWo),g(iw,ct,null),e(ur,XWo),e(ur,go),g(dw,go,null),e(go,VWo),e(go,ufe),e(ufe,zWo),e(go,WWo),e(go,cn),e(cn,QWo),e(cn,bfe),e(bfe,HWo),e(cn,UWo),e(cn,vfe),e(vfe,JWo),e(cn,YWo),e(cn,Tfe),e(Tfe,KWo),e(cn,ZWo),e(go,eQo),e(go,B),e(B,O2),e(O2,Ffe),e(Ffe,oQo),e(O2,rQo),e(O2,AN),e(AN,tQo),e(O2,aQo),e(B,nQo),e(B,X2),e(X2,Cfe),e(Cfe,sQo),e(X2,lQo),e(X2,LN),e(LN,iQo),e(X2,dQo),e(B,cQo),e(B,V2),e(V2,Mfe),e(Mfe,fQo),e(V2,mQo),e(V2,BN),e(BN,gQo),e(V2,hQo),e(B,pQo),e(B,z2),e(z2,Efe),e(Efe,_Qo),e(z2,uQo),e(z2,xN),e(xN,bQo),e(z2,vQo),e(B,TQo),e(B,W2),e(W2,yfe),e(yfe,FQo),e(W2,CQo),e(W2,kN),e(kN,MQo),e(W2,EQo),e(B,yQo),e(B,Q2),e(Q2,wfe),e(wfe,wQo),e(Q2,AQo),e(Q2,RN),e(RN,LQo),e(Q2,BQo),e(B,xQo),e(B,H2),e(H2,Afe),e(Afe,kQo),e(H2,RQo),e(H2,SN),e(SN,SQo),e(H2,PQo),e(B,$Qo),e(B,U2),e(U2,Lfe),e(Lfe,IQo),e(U2,DQo),e(U2,PN),e(PN,jQo),e(U2,NQo),e(B,qQo),e(B,J2),e(J2,Bfe),e(Bfe,GQo),e(J2,OQo),e(J2,$N),e($N,XQo),e(J2,VQo),e(B,zQo),e(B,Y2),e(Y2,xfe),e(xfe,WQo),e(Y2,QQo),e(Y2,IN),e(IN,HQo),e(Y2,UQo),e(B,JQo),e(B,K2),e(K2,kfe),e(kfe,YQo),e(K2,KQo),e(K2,DN),e(DN,ZQo),e(K2,eHo),e(B,oHo),e(B,Z2),e(Z2,Rfe),e(Rfe,rHo),e(Z2,tHo),e(Z2,jN),e(jN,aHo),e(Z2,nHo),e(B,sHo),e(B,ev),e(ev,Sfe),e(Sfe,lHo),e(ev,iHo),e(ev,NN),e(NN,dHo),e(ev,cHo),e(B,fHo),e(B,ov),e(ov,Pfe),e(Pfe,mHo),e(ov,gHo),e(ov,qN),e(qN,hHo),e(ov,pHo),e(B,_Ho),e(B,rv),e(rv,$fe),e($fe,uHo),e(rv,bHo),e(rv,GN),e(GN,vHo),e(rv,THo),e(B,FHo),e(B,tv),e(tv,Ife),e(Ife,CHo),e(tv,MHo),e(tv,ON),e(ON,EHo),e(tv,yHo),e(B,wHo),e(B,Is),e(Is,Dfe),e(Dfe,AHo),e(Is,LHo),e(Is,XN),e(XN,BHo),e(Is,xHo),e(Is,VN),e(VN,kHo),e(Is,RHo),e(B,SHo),e(B,av),e(av,jfe),e(jfe,PHo),e(av,$Ho),e(av,zN),e(zN,IHo),e(av,DHo),e(B,jHo),e(B,nv),e(nv,Nfe),e(Nfe,NHo),e(nv,qHo),e(nv,WN),e(WN,GHo),e(nv,OHo),e(B,XHo),e(B,sv),e(sv,qfe),e(qfe,VHo),e(sv,zHo),e(sv,QN),e(QN,WHo),e(sv,QHo),e(B,HHo),e(B,lv),e(lv,Gfe),e(Gfe,UHo),e(lv,JHo),e(lv,HN),e(HN,YHo),e(lv,KHo),e(B,ZHo),e(B,iv),e(iv,Ofe),e(Ofe,eUo),e(iv,oUo),e(iv,UN),e(UN,rUo),e(iv,tUo),e(B,aUo),e(B,dv),e(dv,Xfe),e(Xfe,nUo),e(dv,sUo),e(dv,JN),e(JN,lUo),e(dv,iUo),e(B,dUo),e(B,cv),e(cv,Vfe),e(Vfe,cUo),e(cv,fUo),e(cv,YN),e(YN,mUo),e(cv,gUo),e(B,hUo),e(B,fv),e(fv,zfe),e(zfe,pUo),e(fv,_Uo),e(fv,KN),e(KN,uUo),e(fv,bUo),e(B,vUo),e(B,mv),e(mv,Wfe),e(Wfe,TUo),e(mv,FUo),e(mv,ZN),e(ZN,CUo),e(mv,MUo),e(B,EUo),e(B,gv),e(gv,Qfe),e(Qfe,yUo),e(gv,wUo),e(gv,eq),e(eq,AUo),e(gv,LUo),e(B,BUo),e(B,hv),e(hv,Hfe),e(Hfe,xUo),e(hv,kUo),e(hv,oq),e(oq,RUo),e(hv,SUo),e(B,PUo),e(B,pv),e(pv,Ufe),e(Ufe,$Uo),e(pv,IUo),e(pv,rq),e(rq,DUo),e(pv,jUo),e(B,NUo),e(B,_v),e(_v,Jfe),e(Jfe,qUo),e(_v,GUo),e(_v,tq),e(tq,OUo),e(_v,XUo),e(B,VUo),e(B,uv),e(uv,Yfe),e(Yfe,zUo),e(uv,WUo),e(uv,aq),e(aq,QUo),e(uv,HUo),e(B,UUo),e(B,bv),e(bv,Kfe),e(Kfe,JUo),e(bv,YUo),e(bv,nq),e(nq,KUo),e(bv,ZUo),e(B,eJo),e(B,vv),e(vv,Zfe),e(Zfe,oJo),e(vv,rJo),e(vv,sq),e(sq,tJo),e(vv,aJo),e(B,nJo),e(B,Tv),e(Tv,eme),e(eme,sJo),e(Tv,lJo),e(Tv,lq),e(lq,iJo),e(Tv,dJo),e(B,cJo),e(B,Fv),e(Fv,ome),e(ome,fJo),e(Fv,mJo),e(Fv,iq),e(iq,gJo),e(Fv,hJo),e(B,pJo),e(B,Cv),e(Cv,rme),e(rme,_Jo),e(Cv,uJo),e(Cv,dq),e(dq,bJo),e(Cv,vJo),e(B,TJo),e(B,Mv),e(Mv,tme),e(tme,FJo),e(Mv,CJo),e(Mv,cq),e(cq,MJo),e(Mv,EJo),e(B,yJo),e(B,Ev),e(Ev,ame),e(ame,wJo),e(Ev,AJo),e(Ev,fq),e(fq,LJo),e(Ev,BJo),e(B,xJo),e(B,yv),e(yv,nme),e(nme,kJo),e(yv,RJo),e(yv,mq),e(mq,SJo),e(yv,PJo),e(B,$Jo),e(B,wv),e(wv,sme),e(sme,IJo),e(wv,DJo),e(wv,gq),e(gq,jJo),e(wv,NJo),e(B,qJo),e(B,Av),e(Av,lme),e(lme,GJo),e(Av,OJo),e(Av,hq),e(hq,XJo),e(Av,VJo),e(B,zJo),e(B,Lv),e(Lv,ime),e(ime,WJo),e(Lv,QJo),e(Lv,pq),e(pq,HJo),e(Lv,UJo),e(go,JJo),e(go,dme),e(dme,YJo),e(go,KJo),g(cw,go,null),b(c,dxe,u),b(c,nc,u),e(nc,Bv),e(Bv,cme),g(fw,cme,null),e(nc,ZJo),e(nc,fme),e(fme,eYo),b(c,cxe,u),b(c,br,u),g(mw,br,null),e(br,oYo),e(br,sc),e(sc,rYo),e(sc,mme),e(mme,tYo),e(sc,aYo),e(sc,gme),e(gme,nYo),e(sc,sYo),e(br,lYo),e(br,gw),e(gw,iYo),e(gw,hme),e(hme,dYo),e(gw,cYo),e(br,fYo),e(br,ft),g(hw,ft,null),e(ft,mYo),e(ft,pme),e(pme,gYo),e(ft,hYo),e(ft,lc),e(lc,pYo),e(lc,_me),e(_me,_Yo),e(lc,uYo),e(lc,ume),e(ume,bYo),e(lc,vYo),e(ft,TYo),e(ft,bme),e(bme,FYo),e(ft,CYo),g(pw,ft,null),e(br,MYo),e(br,ho),g(_w,ho,null),e(ho,EYo),e(ho,vme),e(vme,yYo),e(ho,wYo),e(ho,fn),e(fn,AYo),e(fn,Tme),e(Tme,LYo),e(fn,BYo),e(fn,Fme),e(Fme,xYo),e(fn,kYo),e(fn,Cme),e(Cme,RYo),e(fn,SYo),e(ho,PYo),e(ho,H),e(H,xv),e(xv,Mme),e(Mme,$Yo),e(xv,IYo),e(xv,_q),e(_q,DYo),e(xv,jYo),e(H,NYo),e(H,kv),e(kv,Eme),e(Eme,qYo),e(kv,GYo),e(kv,uq),e(uq,OYo),e(kv,XYo),e(H,VYo),e(H,Rv),e(Rv,yme),e(yme,zYo),e(Rv,WYo),e(Rv,bq),e(bq,QYo),e(Rv,HYo),e(H,UYo),e(H,Sv),e(Sv,wme),e(wme,JYo),e(Sv,YYo),e(Sv,vq),e(vq,KYo),e(Sv,ZYo),e(H,eKo),e(H,Pv),e(Pv,Ame),e(Ame,oKo),e(Pv,rKo),e(Pv,Tq),e(Tq,tKo),e(Pv,aKo),e(H,nKo),e(H,$v),e($v,Lme),e(Lme,sKo),e($v,lKo),e($v,Fq),e(Fq,iKo),e($v,dKo),e(H,cKo),e(H,Iv),e(Iv,Bme),e(Bme,fKo),e(Iv,mKo),e(Iv,Cq),e(Cq,gKo),e(Iv,hKo),e(H,pKo),e(H,Dv),e(Dv,xme),e(xme,_Ko),e(Dv,uKo),e(Dv,Mq),e(Mq,bKo),e(Dv,vKo),e(H,TKo),e(H,jv),e(jv,kme),e(kme,FKo),e(jv,CKo),e(jv,Eq),e(Eq,MKo),e(jv,EKo),e(H,yKo),e(H,Nv),e(Nv,Rme),e(Rme,wKo),e(Nv,AKo),e(Nv,yq),e(yq,LKo),e(Nv,BKo),e(H,xKo),e(H,qv),e(qv,Sme),e(Sme,kKo),e(qv,RKo),e(qv,wq),e(wq,SKo),e(qv,PKo),e(H,$Ko),e(H,Gv),e(Gv,Pme),e(Pme,IKo),e(Gv,DKo),e(Gv,Aq),e(Aq,jKo),e(Gv,NKo),e(H,qKo),e(H,Ov),e(Ov,$me),e($me,GKo),e(Ov,OKo),e(Ov,Lq),e(Lq,XKo),e(Ov,VKo),e(H,zKo),e(H,Xv),e(Xv,Ime),e(Ime,WKo),e(Xv,QKo),e(Xv,Bq),e(Bq,HKo),e(Xv,UKo),e(H,JKo),e(H,Vv),e(Vv,Dme),e(Dme,YKo),e(Vv,KKo),e(Vv,xq),e(xq,ZKo),e(Vv,eZo),e(H,oZo),e(H,zv),e(zv,jme),e(jme,rZo),e(zv,tZo),e(zv,kq),e(kq,aZo),e(zv,nZo),e(H,sZo),e(H,Wv),e(Wv,Nme),e(Nme,lZo),e(Wv,iZo),e(Wv,Rq),e(Rq,dZo),e(Wv,cZo),e(H,fZo),e(H,Qv),e(Qv,qme),e(qme,mZo),e(Qv,gZo),e(Qv,Sq),e(Sq,hZo),e(Qv,pZo),e(H,_Zo),e(H,Hv),e(Hv,Gme),e(Gme,uZo),e(Hv,bZo),e(Hv,Pq),e(Pq,vZo),e(Hv,TZo),e(H,FZo),e(H,Uv),e(Uv,Ome),e(Ome,CZo),e(Uv,MZo),e(Uv,$q),e($q,EZo),e(Uv,yZo),e(H,wZo),e(H,Jv),e(Jv,Xme),e(Xme,AZo),e(Jv,LZo),e(Jv,Iq),e(Iq,BZo),e(Jv,xZo),e(H,kZo),e(H,Yv),e(Yv,Vme),e(Vme,RZo),e(Yv,SZo),e(Yv,Dq),e(Dq,PZo),e(Yv,$Zo),e(ho,IZo),e(ho,zme),e(zme,DZo),e(ho,jZo),g(uw,ho,null),b(c,fxe,u),b(c,ic,u),e(ic,Kv),e(Kv,Wme),g(bw,Wme,null),e(ic,NZo),e(ic,Qme),e(Qme,qZo),b(c,mxe,u),b(c,vr,u),g(vw,vr,null),e(vr,GZo),e(vr,dc),e(dc,OZo),e(dc,Hme),e(Hme,XZo),e(dc,VZo),e(dc,Ume),e(Ume,zZo),e(dc,WZo),e(vr,QZo),e(vr,Tw),e(Tw,HZo),e(Tw,Jme),e(Jme,UZo),e(Tw,JZo),e(vr,YZo),e(vr,mt),g(Fw,mt,null),e(mt,KZo),e(mt,Yme),e(Yme,ZZo),e(mt,eer),e(mt,cc),e(cc,oer),e(cc,Kme),e(Kme,rer),e(cc,ter),e(cc,Zme),e(Zme,aer),e(cc,ner),e(mt,ser),e(mt,ege),e(ege,ler),e(mt,ier),g(Cw,mt,null),e(vr,der),e(vr,po),g(Mw,po,null),e(po,cer),e(po,oge),e(oge,fer),e(po,mer),e(po,mn),e(mn,ger),e(mn,rge),e(rge,her),e(mn,per),e(mn,tge),e(tge,_er),e(mn,uer),e(mn,age),e(age,ber),e(mn,ver),e(po,Ter),e(po,pe),e(pe,Zv),e(Zv,nge),e(nge,Fer),e(Zv,Cer),e(Zv,jq),e(jq,Mer),e(Zv,Eer),e(pe,yer),e(pe,eT),e(eT,sge),e(sge,wer),e(eT,Aer),e(eT,Nq),e(Nq,Ler),e(eT,Ber),e(pe,xer),e(pe,oT),e(oT,lge),e(lge,ker),e(oT,Rer),e(oT,qq),e(qq,Ser),e(oT,Per),e(pe,$er),e(pe,rT),e(rT,ige),e(ige,Ier),e(rT,Der),e(rT,Gq),e(Gq,jer),e(rT,Ner),e(pe,qer),e(pe,tT),e(tT,dge),e(dge,Ger),e(tT,Oer),e(tT,Oq),e(Oq,Xer),e(tT,Ver),e(pe,zer),e(pe,aT),e(aT,cge),e(cge,Wer),e(aT,Qer),e(aT,Xq),e(Xq,Her),e(aT,Uer),e(pe,Jer),e(pe,nT),e(nT,fge),e(fge,Yer),e(nT,Ker),e(nT,Vq),e(Vq,Zer),e(nT,eor),e(pe,oor),e(pe,sT),e(sT,mge),e(mge,ror),e(sT,tor),e(sT,zq),e(zq,aor),e(sT,nor),e(pe,sor),e(pe,lT),e(lT,gge),e(gge,lor),e(lT,ior),e(lT,Wq),e(Wq,dor),e(lT,cor),e(pe,mor),e(pe,iT),e(iT,hge),e(hge,gor),e(iT,hor),e(iT,Qq),e(Qq,por),e(iT,_or),e(po,uor),e(po,pge),e(pge,bor),e(po,vor),g(Ew,po,null),b(c,gxe,u),b(c,fc,u),e(fc,dT),e(dT,_ge),g(yw,_ge,null),e(fc,Tor),e(fc,uge),e(uge,For),b(c,hxe,u),b(c,Tr,u),g(ww,Tr,null),e(Tr,Cor),e(Tr,mc),e(mc,Mor),e(mc,bge),e(bge,Eor),e(mc,yor),e(mc,vge),e(vge,wor),e(mc,Aor),e(Tr,Lor),e(Tr,Aw),e(Aw,Bor),e(Aw,Tge),e(Tge,xor),e(Aw,kor),e(Tr,Ror),e(Tr,gt),g(Lw,gt,null),e(gt,Sor),e(gt,Fge),e(Fge,Por),e(gt,$or),e(gt,gc),e(gc,Ior),e(gc,Cge),e(Cge,Dor),e(gc,jor),e(gc,Mge),e(Mge,Nor),e(gc,qor),e(gt,Gor),e(gt,Ege),e(Ege,Oor),e(gt,Xor),g(Bw,gt,null),e(Tr,Vor),e(Tr,_o),g(xw,_o,null),e(_o,zor),e(_o,yge),e(yge,Wor),e(_o,Qor),e(_o,gn),e(gn,Hor),e(gn,wge),e(wge,Uor),e(gn,Jor),e(gn,Age),e(Age,Yor),e(gn,Kor),e(gn,Lge),e(Lge,Zor),e(gn,err),e(_o,orr),e(_o,kw),e(kw,cT),e(cT,Bge),e(Bge,rrr),e(cT,trr),e(cT,Hq),e(Hq,arr),e(cT,nrr),e(kw,srr),e(kw,fT),e(fT,xge),e(xge,lrr),e(fT,irr),e(fT,Uq),e(Uq,drr),e(fT,crr),e(_o,frr),e(_o,kge),e(kge,mrr),e(_o,grr),g(Rw,_o,null),b(c,pxe,u),b(c,hc,u),e(hc,mT),e(mT,Rge),g(Sw,Rge,null),e(hc,hrr),e(hc,Sge),e(Sge,prr),b(c,_xe,u),b(c,Fr,u),g(Pw,Fr,null),e(Fr,_rr),e(Fr,pc),e(pc,urr),e(pc,Pge),e(Pge,brr),e(pc,vrr),e(pc,$ge),e($ge,Trr),e(pc,Frr),e(Fr,Crr),e(Fr,$w),e($w,Mrr),e($w,Ige),e(Ige,Err),e($w,yrr),e(Fr,wrr),e(Fr,ht),g(Iw,ht,null),e(ht,Arr),e(ht,Dge),e(Dge,Lrr),e(ht,Brr),e(ht,_c),e(_c,xrr),e(_c,jge),e(jge,krr),e(_c,Rrr),e(_c,Nge),e(Nge,Srr),e(_c,Prr),e(ht,$rr),e(ht,qge),e(qge,Irr),e(ht,Drr),g(Dw,ht,null),e(Fr,jrr),e(Fr,uo),g(jw,uo,null),e(uo,Nrr),e(uo,Gge),e(Gge,qrr),e(uo,Grr),e(uo,hn),e(hn,Orr),e(hn,Oge),e(Oge,Xrr),e(hn,Vrr),e(hn,Xge),e(Xge,zrr),e(hn,Wrr),e(hn,Vge),e(Vge,Qrr),e(hn,Hrr),e(uo,Urr),e(uo,Y),e(Y,gT),e(gT,zge),e(zge,Jrr),e(gT,Yrr),e(gT,Jq),e(Jq,Krr),e(gT,Zrr),e(Y,etr),e(Y,hT),e(hT,Wge),e(Wge,otr),e(hT,rtr),e(hT,Yq),e(Yq,ttr),e(hT,atr),e(Y,ntr),e(Y,pT),e(pT,Qge),e(Qge,str),e(pT,ltr),e(pT,Kq),e(Kq,itr),e(pT,dtr),e(Y,ctr),e(Y,_T),e(_T,Hge),e(Hge,ftr),e(_T,mtr),e(_T,Zq),e(Zq,gtr),e(_T,htr),e(Y,ptr),e(Y,uT),e(uT,Uge),e(Uge,_tr),e(uT,utr),e(uT,eG),e(eG,btr),e(uT,vtr),e(Y,Ttr),e(Y,bT),e(bT,Jge),e(Jge,Ftr),e(bT,Ctr),e(bT,oG),e(oG,Mtr),e(bT,Etr),e(Y,ytr),e(Y,vT),e(vT,Yge),e(Yge,wtr),e(vT,Atr),e(vT,rG),e(rG,Ltr),e(vT,Btr),e(Y,xtr),e(Y,TT),e(TT,Kge),e(Kge,ktr),e(TT,Rtr),e(TT,tG),e(tG,Str),e(TT,Ptr),e(Y,$tr),e(Y,FT),e(FT,Zge),e(Zge,Itr),e(FT,Dtr),e(FT,aG),e(aG,jtr),e(FT,Ntr),e(Y,qtr),e(Y,CT),e(CT,ehe),e(ehe,Gtr),e(CT,Otr),e(CT,nG),e(nG,Xtr),e(CT,Vtr),e(Y,ztr),e(Y,MT),e(MT,ohe),e(ohe,Wtr),e(MT,Qtr),e(MT,sG),e(sG,Htr),e(MT,Utr),e(Y,Jtr),e(Y,ET),e(ET,rhe),e(rhe,Ytr),e(ET,Ktr),e(ET,lG),e(lG,Ztr),e(ET,ear),e(Y,oar),e(Y,yT),e(yT,the),e(the,rar),e(yT,tar),e(yT,iG),e(iG,aar),e(yT,nar),e(Y,sar),e(Y,wT),e(wT,ahe),e(ahe,lar),e(wT,iar),e(wT,dG),e(dG,dar),e(wT,car),e(Y,far),e(Y,AT),e(AT,nhe),e(nhe,mar),e(AT,gar),e(AT,cG),e(cG,har),e(AT,par),e(Y,_ar),e(Y,LT),e(LT,she),e(she,uar),e(LT,bar),e(LT,fG),e(fG,Tar),e(LT,Far),e(Y,Car),e(Y,BT),e(BT,lhe),e(lhe,Mar),e(BT,Ear),e(BT,mG),e(mG,yar),e(BT,war),e(Y,Aar),e(Y,xT),e(xT,ihe),e(ihe,Lar),e(xT,Bar),e(xT,gG),e(gG,xar),e(xT,kar),e(Y,Rar),e(Y,kT),e(kT,dhe),e(dhe,Sar),e(kT,Par),e(kT,hG),e(hG,$ar),e(kT,Iar),e(Y,Dar),e(Y,RT),e(RT,che),e(che,jar),e(RT,Nar),e(RT,pG),e(pG,qar),e(RT,Gar),e(uo,Oar),e(uo,fhe),e(fhe,Xar),e(uo,Var),g(Nw,uo,null),b(c,uxe,u),b(c,uc,u),e(uc,ST),e(ST,mhe),g(qw,mhe,null),e(uc,zar),e(uc,ghe),e(ghe,War),b(c,bxe,u),b(c,Cr,u),g(Gw,Cr,null),e(Cr,Qar),e(Cr,bc),e(bc,Har),e(bc,hhe),e(hhe,Uar),e(bc,Jar),e(bc,phe),e(phe,Yar),e(bc,Kar),e(Cr,Zar),e(Cr,Ow),e(Ow,enr),e(Ow,_he),e(_he,onr),e(Ow,rnr),e(Cr,tnr),e(Cr,pt),g(Xw,pt,null),e(pt,anr),e(pt,uhe),e(uhe,nnr),e(pt,snr),e(pt,vc),e(vc,lnr),e(vc,bhe),e(bhe,inr),e(vc,dnr),e(vc,vhe),e(vhe,cnr),e(vc,fnr),e(pt,mnr),e(pt,The),e(The,gnr),e(pt,hnr),g(Vw,pt,null),e(Cr,pnr),e(Cr,bo),g(zw,bo,null),e(bo,_nr),e(bo,Fhe),e(Fhe,unr),e(bo,bnr),e(bo,pn),e(pn,vnr),e(pn,Che),e(Che,Tnr),e(pn,Fnr),e(pn,Mhe),e(Mhe,Cnr),e(pn,Mnr),e(pn,Ehe),e(Ehe,Enr),e(pn,ynr),e(bo,wnr),e(bo,_e),e(_e,PT),e(PT,yhe),e(yhe,Anr),e(PT,Lnr),e(PT,_G),e(_G,Bnr),e(PT,xnr),e(_e,knr),e(_e,$T),e($T,whe),e(whe,Rnr),e($T,Snr),e($T,uG),e(uG,Pnr),e($T,$nr),e(_e,Inr),e(_e,IT),e(IT,Ahe),e(Ahe,Dnr),e(IT,jnr),e(IT,bG),e(bG,Nnr),e(IT,qnr),e(_e,Gnr),e(_e,DT),e(DT,Lhe),e(Lhe,Onr),e(DT,Xnr),e(DT,vG),e(vG,Vnr),e(DT,znr),e(_e,Wnr),e(_e,jT),e(jT,Bhe),e(Bhe,Qnr),e(jT,Hnr),e(jT,TG),e(TG,Unr),e(jT,Jnr),e(_e,Ynr),e(_e,NT),e(NT,xhe),e(xhe,Knr),e(NT,Znr),e(NT,FG),e(FG,esr),e(NT,osr),e(_e,rsr),e(_e,qT),e(qT,khe),e(khe,tsr),e(qT,asr),e(qT,CG),e(CG,nsr),e(qT,ssr),e(_e,lsr),e(_e,GT),e(GT,Rhe),e(Rhe,isr),e(GT,dsr),e(GT,MG),e(MG,csr),e(GT,fsr),e(_e,msr),e(_e,OT),e(OT,She),e(She,gsr),e(OT,hsr),e(OT,EG),e(EG,psr),e(OT,_sr),e(_e,usr),e(_e,XT),e(XT,Phe),e(Phe,bsr),e(XT,vsr),e(XT,yG),e(yG,Tsr),e(XT,Fsr),e(bo,Csr),e(bo,$he),e($he,Msr),e(bo,Esr),g(Ww,bo,null),b(c,vxe,u),b(c,Tc,u),e(Tc,VT),e(VT,Ihe),g(Qw,Ihe,null),e(Tc,ysr),e(Tc,Dhe),e(Dhe,wsr),b(c,Txe,u),b(c,Mr,u),g(Hw,Mr,null),e(Mr,Asr),e(Mr,Fc),e(Fc,Lsr),e(Fc,jhe),e(jhe,Bsr),e(Fc,xsr),e(Fc,Nhe),e(Nhe,ksr),e(Fc,Rsr),e(Mr,Ssr),e(Mr,Uw),e(Uw,Psr),e(Uw,qhe),e(qhe,$sr),e(Uw,Isr),e(Mr,Dsr),e(Mr,_t),g(Jw,_t,null),e(_t,jsr),e(_t,Ghe),e(Ghe,Nsr),e(_t,qsr),e(_t,Cc),e(Cc,Gsr),e(Cc,Ohe),e(Ohe,Osr),e(Cc,Xsr),e(Cc,Xhe),e(Xhe,Vsr),e(Cc,zsr),e(_t,Wsr),e(_t,Vhe),e(Vhe,Qsr),e(_t,Hsr),g(Yw,_t,null),e(Mr,Usr),e(Mr,vo),g(Kw,vo,null),e(vo,Jsr),e(vo,zhe),e(zhe,Ysr),e(vo,Ksr),e(vo,_n),e(_n,Zsr),e(_n,Whe),e(Whe,elr),e(_n,olr),e(_n,Qhe),e(Qhe,rlr),e(_n,tlr),e(_n,Hhe),e(Hhe,alr),e(_n,nlr),e(vo,slr),e(vo,X),e(X,zT),e(zT,Uhe),e(Uhe,llr),e(zT,ilr),e(zT,wG),e(wG,dlr),e(zT,clr),e(X,flr),e(X,WT),e(WT,Jhe),e(Jhe,mlr),e(WT,glr),e(WT,AG),e(AG,hlr),e(WT,plr),e(X,_lr),e(X,QT),e(QT,Yhe),e(Yhe,ulr),e(QT,blr),e(QT,LG),e(LG,vlr),e(QT,Tlr),e(X,Flr),e(X,HT),e(HT,Khe),e(Khe,Clr),e(HT,Mlr),e(HT,BG),e(BG,Elr),e(HT,ylr),e(X,wlr),e(X,UT),e(UT,Zhe),e(Zhe,Alr),e(UT,Llr),e(UT,xG),e(xG,Blr),e(UT,xlr),e(X,klr),e(X,JT),e(JT,epe),e(epe,Rlr),e(JT,Slr),e(JT,kG),e(kG,Plr),e(JT,$lr),e(X,Ilr),e(X,YT),e(YT,ope),e(ope,Dlr),e(YT,jlr),e(YT,RG),e(RG,Nlr),e(YT,qlr),e(X,Glr),e(X,KT),e(KT,rpe),e(rpe,Olr),e(KT,Xlr),e(KT,SG),e(SG,Vlr),e(KT,zlr),e(X,Wlr),e(X,ZT),e(ZT,tpe),e(tpe,Qlr),e(ZT,Hlr),e(ZT,PG),e(PG,Ulr),e(ZT,Jlr),e(X,Ylr),e(X,eF),e(eF,ape),e(ape,Klr),e(eF,Zlr),e(eF,$G),e($G,eir),e(eF,oir),e(X,rir),e(X,oF),e(oF,npe),e(npe,tir),e(oF,air),e(oF,IG),e(IG,nir),e(oF,sir),e(X,lir),e(X,rF),e(rF,spe),e(spe,iir),e(rF,dir),e(rF,DG),e(DG,cir),e(rF,fir),e(X,mir),e(X,tF),e(tF,lpe),e(lpe,gir),e(tF,hir),e(tF,jG),e(jG,pir),e(tF,_ir),e(X,uir),e(X,aF),e(aF,ipe),e(ipe,bir),e(aF,vir),e(aF,NG),e(NG,Tir),e(aF,Fir),e(X,Cir),e(X,nF),e(nF,dpe),e(dpe,Mir),e(nF,Eir),e(nF,qG),e(qG,yir),e(nF,wir),e(X,Air),e(X,sF),e(sF,cpe),e(cpe,Lir),e(sF,Bir),e(sF,GG),e(GG,xir),e(sF,kir),e(X,Rir),e(X,lF),e(lF,fpe),e(fpe,Sir),e(lF,Pir),e(lF,OG),e(OG,$ir),e(lF,Iir),e(X,Dir),e(X,iF),e(iF,mpe),e(mpe,jir),e(iF,Nir),e(iF,XG),e(XG,qir),e(iF,Gir),e(X,Oir),e(X,dF),e(dF,gpe),e(gpe,Xir),e(dF,Vir),e(dF,VG),e(VG,zir),e(dF,Wir),e(X,Qir),e(X,cF),e(cF,hpe),e(hpe,Hir),e(cF,Uir),e(cF,zG),e(zG,Jir),e(cF,Yir),e(X,Kir),e(X,fF),e(fF,ppe),e(ppe,Zir),e(fF,edr),e(fF,WG),e(WG,odr),e(fF,rdr),e(X,tdr),e(X,mF),e(mF,_pe),e(_pe,adr),e(mF,ndr),e(mF,QG),e(QG,sdr),e(mF,ldr),e(X,idr),e(X,gF),e(gF,upe),e(upe,ddr),e(gF,cdr),e(gF,HG),e(HG,fdr),e(gF,mdr),e(X,gdr),e(X,hF),e(hF,bpe),e(bpe,hdr),e(hF,pdr),e(hF,UG),e(UG,_dr),e(hF,udr),e(X,bdr),e(X,pF),e(pF,vpe),e(vpe,vdr),e(pF,Tdr),e(pF,JG),e(JG,Fdr),e(pF,Cdr),e(vo,Mdr),e(vo,Tpe),e(Tpe,Edr),e(vo,ydr),g(Zw,vo,null),b(c,Fxe,u),b(c,Mc,u),e(Mc,_F),e(_F,Fpe),g(e6,Fpe,null),e(Mc,wdr),e(Mc,Cpe),e(Cpe,Adr),b(c,Cxe,u),b(c,Er,u),g(o6,Er,null),e(Er,Ldr),e(Er,Ec),e(Ec,Bdr),e(Ec,Mpe),e(Mpe,xdr),e(Ec,kdr),e(Ec,Epe),e(Epe,Rdr),e(Ec,Sdr),e(Er,Pdr),e(Er,r6),e(r6,$dr),e(r6,ype),e(ype,Idr),e(r6,Ddr),e(Er,jdr),e(Er,ut),g(t6,ut,null),e(ut,Ndr),e(ut,wpe),e(wpe,qdr),e(ut,Gdr),e(ut,yc),e(yc,Odr),e(yc,Ape),e(Ape,Xdr),e(yc,Vdr),e(yc,Lpe),e(Lpe,zdr),e(yc,Wdr),e(ut,Qdr),e(ut,Bpe),e(Bpe,Hdr),e(ut,Udr),g(a6,ut,null),e(Er,Jdr),e(Er,To),g(n6,To,null),e(To,Ydr),e(To,xpe),e(xpe,Kdr),e(To,Zdr),e(To,un),e(un,ecr),e(un,kpe),e(kpe,ocr),e(un,rcr),e(un,Rpe),e(Rpe,tcr),e(un,acr),e(un,Spe),e(Spe,ncr),e(un,scr),e(To,lcr),e(To,te),e(te,uF),e(uF,Ppe),e(Ppe,icr),e(uF,dcr),e(uF,YG),e(YG,ccr),e(uF,fcr),e(te,mcr),e(te,bF),e(bF,$pe),e($pe,gcr),e(bF,hcr),e(bF,KG),e(KG,pcr),e(bF,_cr),e(te,ucr),e(te,vF),e(vF,Ipe),e(Ipe,bcr),e(vF,vcr),e(vF,ZG),e(ZG,Tcr),e(vF,Fcr),e(te,Ccr),e(te,TF),e(TF,Dpe),e(Dpe,Mcr),e(TF,Ecr),e(TF,eO),e(eO,ycr),e(TF,wcr),e(te,Acr),e(te,FF),e(FF,jpe),e(jpe,Lcr),e(FF,Bcr),e(FF,oO),e(oO,xcr),e(FF,kcr),e(te,Rcr),e(te,CF),e(CF,Npe),e(Npe,Scr),e(CF,Pcr),e(CF,rO),e(rO,$cr),e(CF,Icr),e(te,Dcr),e(te,MF),e(MF,qpe),e(qpe,jcr),e(MF,Ncr),e(MF,tO),e(tO,qcr),e(MF,Gcr),e(te,Ocr),e(te,EF),e(EF,Gpe),e(Gpe,Xcr),e(EF,Vcr),e(EF,aO),e(aO,zcr),e(EF,Wcr),e(te,Qcr),e(te,yF),e(yF,Ope),e(Ope,Hcr),e(yF,Ucr),e(yF,nO),e(nO,Jcr),e(yF,Ycr),e(te,Kcr),e(te,wF),e(wF,Xpe),e(Xpe,Zcr),e(wF,efr),e(wF,sO),e(sO,ofr),e(wF,rfr),e(te,tfr),e(te,AF),e(AF,Vpe),e(Vpe,afr),e(AF,nfr),e(AF,lO),e(lO,sfr),e(AF,lfr),e(te,ifr),e(te,LF),e(LF,zpe),e(zpe,dfr),e(LF,cfr),e(LF,iO),e(iO,ffr),e(LF,mfr),e(te,gfr),e(te,BF),e(BF,Wpe),e(Wpe,hfr),e(BF,pfr),e(BF,dO),e(dO,_fr),e(BF,ufr),e(te,bfr),e(te,xF),e(xF,Qpe),e(Qpe,vfr),e(xF,Tfr),e(xF,cO),e(cO,Ffr),e(xF,Cfr),e(te,Mfr),e(te,kF),e(kF,Hpe),e(Hpe,Efr),e(kF,yfr),e(kF,fO),e(fO,wfr),e(kF,Afr),e(te,Lfr),e(te,RF),e(RF,Upe),e(Upe,Bfr),e(RF,xfr),e(RF,mO),e(mO,kfr),e(RF,Rfr),e(te,Sfr),e(te,SF),e(SF,Jpe),e(Jpe,Pfr),e(SF,$fr),e(SF,gO),e(gO,Ifr),e(SF,Dfr),e(To,jfr),e(To,Ype),e(Ype,Nfr),e(To,qfr),g(s6,To,null),b(c,Mxe,u),b(c,wc,u),e(wc,PF),e(PF,Kpe),g(l6,Kpe,null),e(wc,Gfr),e(wc,Zpe),e(Zpe,Ofr),b(c,Exe,u),b(c,yr,u),g(i6,yr,null),e(yr,Xfr),e(yr,Ac),e(Ac,Vfr),e(Ac,e_e),e(e_e,zfr),e(Ac,Wfr),e(Ac,o_e),e(o_e,Qfr),e(Ac,Hfr),e(yr,Ufr),e(yr,d6),e(d6,Jfr),e(d6,r_e),e(r_e,Yfr),e(d6,Kfr),e(yr,Zfr),e(yr,bt),g(c6,bt,null),e(bt,emr),e(bt,t_e),e(t_e,omr),e(bt,rmr),e(bt,Lc),e(Lc,tmr),e(Lc,a_e),e(a_e,amr),e(Lc,nmr),e(Lc,n_e),e(n_e,smr),e(Lc,lmr),e(bt,imr),e(bt,s_e),e(s_e,dmr),e(bt,cmr),g(f6,bt,null),e(yr,fmr),e(yr,Fo),g(m6,Fo,null),e(Fo,mmr),e(Fo,l_e),e(l_e,gmr),e(Fo,hmr),e(Fo,bn),e(bn,pmr),e(bn,i_e),e(i_e,_mr),e(bn,umr),e(bn,d_e),e(d_e,bmr),e(bn,vmr),e(bn,c_e),e(c_e,Tmr),e(bn,Fmr),e(Fo,Cmr),e(Fo,f_e),e(f_e,$F),e($F,m_e),e(m_e,Mmr),e($F,Emr),e($F,hO),e(hO,ymr),e($F,wmr),e(Fo,Amr),e(Fo,g_e),e(g_e,Lmr),e(Fo,Bmr),g(g6,Fo,null),b(c,yxe,u),b(c,Bc,u),e(Bc,IF),e(IF,h_e),g(h6,h_e,null),e(Bc,xmr),e(Bc,p_e),e(p_e,kmr),b(c,wxe,u),b(c,wr,u),g(p6,wr,null),e(wr,Rmr),e(wr,xc),e(xc,Smr),e(xc,__e),e(__e,Pmr),e(xc,$mr),e(xc,u_e),e(u_e,Imr),e(xc,Dmr),e(wr,jmr),e(wr,_6),e(_6,Nmr),e(_6,b_e),e(b_e,qmr),e(_6,Gmr),e(wr,Omr),e(wr,vt),g(u6,vt,null),e(vt,Xmr),e(vt,v_e),e(v_e,Vmr),e(vt,zmr),e(vt,kc),e(kc,Wmr),e(kc,T_e),e(T_e,Qmr),e(kc,Hmr),e(kc,F_e),e(F_e,Umr),e(kc,Jmr),e(vt,Ymr),e(vt,C_e),e(C_e,Kmr),e(vt,Zmr),g(b6,vt,null),e(wr,egr),e(wr,Co),g(v6,Co,null),e(Co,ogr),e(Co,M_e),e(M_e,rgr),e(Co,tgr),e(Co,vn),e(vn,agr),e(vn,E_e),e(E_e,ngr),e(vn,sgr),e(vn,y_e),e(y_e,lgr),e(vn,igr),e(vn,w_e),e(w_e,dgr),e(vn,cgr),e(Co,fgr),e(Co,K),e(K,DF),e(DF,A_e),e(A_e,mgr),e(DF,ggr),e(DF,pO),e(pO,hgr),e(DF,pgr),e(K,_gr),e(K,jF),e(jF,L_e),e(L_e,ugr),e(jF,bgr),e(jF,_O),e(_O,vgr),e(jF,Tgr),e(K,Fgr),e(K,NF),e(NF,B_e),e(B_e,Cgr),e(NF,Mgr),e(NF,uO),e(uO,Egr),e(NF,ygr),e(K,wgr),e(K,qF),e(qF,x_e),e(x_e,Agr),e(qF,Lgr),e(qF,bO),e(bO,Bgr),e(qF,xgr),e(K,kgr),e(K,GF),e(GF,k_e),e(k_e,Rgr),e(GF,Sgr),e(GF,vO),e(vO,Pgr),e(GF,$gr),e(K,Igr),e(K,OF),e(OF,R_e),e(R_e,Dgr),e(OF,jgr),e(OF,TO),e(TO,Ngr),e(OF,qgr),e(K,Ggr),e(K,XF),e(XF,S_e),e(S_e,Ogr),e(XF,Xgr),e(XF,FO),e(FO,Vgr),e(XF,zgr),e(K,Wgr),e(K,VF),e(VF,P_e),e(P_e,Qgr),e(VF,Hgr),e(VF,CO),e(CO,Ugr),e(VF,Jgr),e(K,Ygr),e(K,zF),e(zF,$_e),e($_e,Kgr),e(zF,Zgr),e(zF,MO),e(MO,ehr),e(zF,ohr),e(K,rhr),e(K,WF),e(WF,I_e),e(I_e,thr),e(WF,ahr),e(WF,EO),e(EO,nhr),e(WF,shr),e(K,lhr),e(K,QF),e(QF,D_e),e(D_e,ihr),e(QF,dhr),e(QF,yO),e(yO,chr),e(QF,fhr),e(K,mhr),e(K,HF),e(HF,j_e),e(j_e,ghr),e(HF,hhr),e(HF,wO),e(wO,phr),e(HF,_hr),e(K,uhr),e(K,UF),e(UF,N_e),e(N_e,bhr),e(UF,vhr),e(UF,AO),e(AO,Thr),e(UF,Fhr),e(K,Chr),e(K,JF),e(JF,q_e),e(q_e,Mhr),e(JF,Ehr),e(JF,LO),e(LO,yhr),e(JF,whr),e(K,Ahr),e(K,YF),e(YF,G_e),e(G_e,Lhr),e(YF,Bhr),e(YF,BO),e(BO,xhr),e(YF,khr),e(K,Rhr),e(K,KF),e(KF,O_e),e(O_e,Shr),e(KF,Phr),e(KF,xO),e(xO,$hr),e(KF,Ihr),e(K,Dhr),e(K,ZF),e(ZF,X_e),e(X_e,jhr),e(ZF,Nhr),e(ZF,kO),e(kO,qhr),e(ZF,Ghr),e(K,Ohr),e(K,e9),e(e9,V_e),e(V_e,Xhr),e(e9,Vhr),e(e9,RO),e(RO,zhr),e(e9,Whr),e(K,Qhr),e(K,o9),e(o9,z_e),e(z_e,Hhr),e(o9,Uhr),e(o9,SO),e(SO,Jhr),e(o9,Yhr),e(K,Khr),e(K,r9),e(r9,W_e),e(W_e,Zhr),e(r9,epr),e(r9,PO),e(PO,opr),e(r9,rpr),e(Co,tpr),e(Co,Q_e),e(Q_e,apr),e(Co,npr),g(T6,Co,null),b(c,Axe,u),b(c,Rc,u),e(Rc,t9),e(t9,H_e),g(F6,H_e,null),e(Rc,spr),e(Rc,U_e),e(U_e,lpr),b(c,Lxe,u),b(c,Ar,u),g(C6,Ar,null),e(Ar,ipr),e(Ar,Sc),e(Sc,dpr),e(Sc,J_e),e(J_e,cpr),e(Sc,fpr),e(Sc,Y_e),e(Y_e,mpr),e(Sc,gpr),e(Ar,hpr),e(Ar,M6),e(M6,ppr),e(M6,K_e),e(K_e,_pr),e(M6,upr),e(Ar,bpr),e(Ar,Tt),g(E6,Tt,null),e(Tt,vpr),e(Tt,Z_e),e(Z_e,Tpr),e(Tt,Fpr),e(Tt,Pc),e(Pc,Cpr),e(Pc,eue),e(eue,Mpr),e(Pc,Epr),e(Pc,oue),e(oue,ypr),e(Pc,wpr),e(Tt,Apr),e(Tt,rue),e(rue,Lpr),e(Tt,Bpr),g(y6,Tt,null),e(Ar,xpr),e(Ar,Mo),g(w6,Mo,null),e(Mo,kpr),e(Mo,tue),e(tue,Rpr),e(Mo,Spr),e(Mo,Tn),e(Tn,Ppr),e(Tn,aue),e(aue,$pr),e(Tn,Ipr),e(Tn,nue),e(nue,Dpr),e(Tn,jpr),e(Tn,sue),e(sue,Npr),e(Tn,qpr),e(Mo,Gpr),e(Mo,Z),e(Z,a9),e(a9,lue),e(lue,Opr),e(a9,Xpr),e(a9,$O),e($O,Vpr),e(a9,zpr),e(Z,Wpr),e(Z,n9),e(n9,iue),e(iue,Qpr),e(n9,Hpr),e(n9,IO),e(IO,Upr),e(n9,Jpr),e(Z,Ypr),e(Z,s9),e(s9,due),e(due,Kpr),e(s9,Zpr),e(s9,DO),e(DO,e_r),e(s9,o_r),e(Z,r_r),e(Z,l9),e(l9,cue),e(cue,t_r),e(l9,a_r),e(l9,jO),e(jO,n_r),e(l9,s_r),e(Z,l_r),e(Z,i9),e(i9,fue),e(fue,i_r),e(i9,d_r),e(i9,NO),e(NO,c_r),e(i9,f_r),e(Z,m_r),e(Z,d9),e(d9,mue),e(mue,g_r),e(d9,h_r),e(d9,qO),e(qO,p_r),e(d9,__r),e(Z,u_r),e(Z,c9),e(c9,gue),e(gue,b_r),e(c9,v_r),e(c9,GO),e(GO,T_r),e(c9,F_r),e(Z,C_r),e(Z,f9),e(f9,hue),e(hue,M_r),e(f9,E_r),e(f9,OO),e(OO,y_r),e(f9,w_r),e(Z,A_r),e(Z,m9),e(m9,pue),e(pue,L_r),e(m9,B_r),e(m9,XO),e(XO,x_r),e(m9,k_r),e(Z,R_r),e(Z,g9),e(g9,_ue),e(_ue,S_r),e(g9,P_r),e(g9,VO),e(VO,$_r),e(g9,I_r),e(Z,D_r),e(Z,h9),e(h9,uue),e(uue,j_r),e(h9,N_r),e(h9,zO),e(zO,q_r),e(h9,G_r),e(Z,O_r),e(Z,p9),e(p9,bue),e(bue,X_r),e(p9,V_r),e(p9,WO),e(WO,z_r),e(p9,W_r),e(Z,Q_r),e(Z,_9),e(_9,vue),e(vue,H_r),e(_9,U_r),e(_9,QO),e(QO,J_r),e(_9,Y_r),e(Z,K_r),e(Z,u9),e(u9,Tue),e(Tue,Z_r),e(u9,eur),e(u9,HO),e(HO,our),e(u9,rur),e(Z,tur),e(Z,b9),e(b9,Fue),e(Fue,aur),e(b9,nur),e(b9,UO),e(UO,sur),e(b9,lur),e(Z,iur),e(Z,v9),e(v9,Cue),e(Cue,dur),e(v9,cur),e(v9,JO),e(JO,fur),e(v9,mur),e(Z,gur),e(Z,T9),e(T9,Mue),e(Mue,hur),e(T9,pur),e(T9,YO),e(YO,_ur),e(T9,uur),e(Z,bur),e(Z,F9),e(F9,Eue),e(Eue,vur),e(F9,Tur),e(F9,KO),e(KO,Fur),e(F9,Cur),e(Z,Mur),e(Z,C9),e(C9,yue),e(yue,Eur),e(C9,yur),e(C9,ZO),e(ZO,wur),e(C9,Aur),e(Mo,Lur),e(Mo,wue),e(wue,Bur),e(Mo,xur),g(A6,Mo,null),b(c,Bxe,u),b(c,$c,u),e($c,M9),e(M9,Aue),g(L6,Aue,null),e($c,kur),e($c,Lue),e(Lue,Rur),b(c,xxe,u),b(c,Lr,u),g(B6,Lr,null),e(Lr,Sur),e(Lr,Ic),e(Ic,Pur),e(Ic,Bue),e(Bue,$ur),e(Ic,Iur),e(Ic,xue),e(xue,Dur),e(Ic,jur),e(Lr,Nur),e(Lr,x6),e(x6,qur),e(x6,kue),e(kue,Gur),e(x6,Our),e(Lr,Xur),e(Lr,Ft),g(k6,Ft,null),e(Ft,Vur),e(Ft,Rue),e(Rue,zur),e(Ft,Wur),e(Ft,Dc),e(Dc,Qur),e(Dc,Sue),e(Sue,Hur),e(Dc,Uur),e(Dc,Pue),e(Pue,Jur),e(Dc,Yur),e(Ft,Kur),e(Ft,$ue),e($ue,Zur),e(Ft,e0r),g(R6,Ft,null),e(Lr,o0r),e(Lr,Eo),g(S6,Eo,null),e(Eo,r0r),e(Eo,Iue),e(Iue,t0r),e(Eo,a0r),e(Eo,Fn),e(Fn,n0r),e(Fn,Due),e(Due,s0r),e(Fn,l0r),e(Fn,jue),e(jue,i0r),e(Fn,d0r),e(Fn,Nue),e(Nue,c0r),e(Fn,f0r),e(Eo,m0r),e(Eo,que),e(que,E9),e(E9,Gue),e(Gue,g0r),e(E9,h0r),e(E9,eX),e(eX,p0r),e(E9,_0r),e(Eo,u0r),e(Eo,Oue),e(Oue,b0r),e(Eo,v0r),g(P6,Eo,null),b(c,kxe,u),b(c,jc,u),e(jc,y9),e(y9,Xue),g($6,Xue,null),e(jc,T0r),e(jc,Vue),e(Vue,F0r),b(c,Rxe,u),b(c,Br,u),g(I6,Br,null),e(Br,C0r),e(Br,Nc),e(Nc,M0r),e(Nc,zue),e(zue,E0r),e(Nc,y0r),e(Nc,Wue),e(Wue,w0r),e(Nc,A0r),e(Br,L0r),e(Br,D6),e(D6,B0r),e(D6,Que),e(Que,x0r),e(D6,k0r),e(Br,R0r),e(Br,Ct),g(j6,Ct,null),e(Ct,S0r),e(Ct,Hue),e(Hue,P0r),e(Ct,$0r),e(Ct,qc),e(qc,I0r),e(qc,Uue),e(Uue,D0r),e(qc,j0r),e(qc,Jue),e(Jue,N0r),e(qc,q0r),e(Ct,G0r),e(Ct,Yue),e(Yue,O0r),e(Ct,X0r),g(N6,Ct,null),e(Br,V0r),e(Br,yo),g(q6,yo,null),e(yo,z0r),e(yo,Kue),e(Kue,W0r),e(yo,Q0r),e(yo,Cn),e(Cn,H0r),e(Cn,Zue),e(Zue,U0r),e(Cn,J0r),e(Cn,e0e),e(e0e,Y0r),e(Cn,K0r),e(Cn,o0e),e(o0e,Z0r),e(Cn,e1r),e(yo,o1r),e(yo,r0e),e(r0e,w9),e(w9,t0e),e(t0e,r1r),e(w9,t1r),e(w9,oX),e(oX,a1r),e(w9,n1r),e(yo,s1r),e(yo,a0e),e(a0e,l1r),e(yo,i1r),g(G6,yo,null),b(c,Sxe,u),b(c,Gc,u),e(Gc,A9),e(A9,n0e),g(O6,n0e,null),e(Gc,d1r),e(Gc,s0e),e(s0e,c1r),b(c,Pxe,u),b(c,xr,u),g(X6,xr,null),e(xr,f1r),e(xr,Oc),e(Oc,m1r),e(Oc,l0e),e(l0e,g1r),e(Oc,h1r),e(Oc,i0e),e(i0e,p1r),e(Oc,_1r),e(xr,u1r),e(xr,V6),e(V6,b1r),e(V6,d0e),e(d0e,v1r),e(V6,T1r),e(xr,F1r),e(xr,Mt),g(z6,Mt,null),e(Mt,C1r),e(Mt,c0e),e(c0e,M1r),e(Mt,E1r),e(Mt,Xc),e(Xc,y1r),e(Xc,f0e),e(f0e,w1r),e(Xc,A1r),e(Xc,m0e),e(m0e,L1r),e(Xc,B1r),e(Mt,x1r),e(Mt,g0e),e(g0e,k1r),e(Mt,R1r),g(W6,Mt,null),e(xr,S1r),e(xr,wo),g(Q6,wo,null),e(wo,P1r),e(wo,h0e),e(h0e,$1r),e(wo,I1r),e(wo,Mn),e(Mn,D1r),e(Mn,p0e),e(p0e,j1r),e(Mn,N1r),e(Mn,_0e),e(_0e,q1r),e(Mn,G1r),e(Mn,u0e),e(u0e,O1r),e(Mn,X1r),e(wo,V1r),e(wo,V),e(V,L9),e(L9,b0e),e(b0e,z1r),e(L9,W1r),e(L9,rX),e(rX,Q1r),e(L9,H1r),e(V,U1r),e(V,B9),e(B9,v0e),e(v0e,J1r),e(B9,Y1r),e(B9,tX),e(tX,K1r),e(B9,Z1r),e(V,ebr),e(V,x9),e(x9,T0e),e(T0e,obr),e(x9,rbr),e(x9,aX),e(aX,tbr),e(x9,abr),e(V,nbr),e(V,k9),e(k9,F0e),e(F0e,sbr),e(k9,lbr),e(k9,nX),e(nX,ibr),e(k9,dbr),e(V,cbr),e(V,R9),e(R9,C0e),e(C0e,fbr),e(R9,mbr),e(R9,sX),e(sX,gbr),e(R9,hbr),e(V,pbr),e(V,S9),e(S9,M0e),e(M0e,_br),e(S9,ubr),e(S9,lX),e(lX,bbr),e(S9,vbr),e(V,Tbr),e(V,P9),e(P9,E0e),e(E0e,Fbr),e(P9,Cbr),e(P9,iX),e(iX,Mbr),e(P9,Ebr),e(V,ybr),e(V,$9),e($9,y0e),e(y0e,wbr),e($9,Abr),e($9,dX),e(dX,Lbr),e($9,Bbr),e(V,xbr),e(V,I9),e(I9,w0e),e(w0e,kbr),e(I9,Rbr),e(I9,cX),e(cX,Sbr),e(I9,Pbr),e(V,$br),e(V,D9),e(D9,A0e),e(A0e,Ibr),e(D9,Dbr),e(D9,fX),e(fX,jbr),e(D9,Nbr),e(V,qbr),e(V,j9),e(j9,L0e),e(L0e,Gbr),e(j9,Obr),e(j9,mX),e(mX,Xbr),e(j9,Vbr),e(V,zbr),e(V,N9),e(N9,B0e),e(B0e,Wbr),e(N9,Qbr),e(N9,gX),e(gX,Hbr),e(N9,Ubr),e(V,Jbr),e(V,q9),e(q9,x0e),e(x0e,Ybr),e(q9,Kbr),e(q9,hX),e(hX,Zbr),e(q9,e5r),e(V,o5r),e(V,G9),e(G9,k0e),e(k0e,r5r),e(G9,t5r),e(G9,pX),e(pX,a5r),e(G9,n5r),e(V,s5r),e(V,O9),e(O9,R0e),e(R0e,l5r),e(O9,i5r),e(O9,_X),e(_X,d5r),e(O9,c5r),e(V,f5r),e(V,X9),e(X9,S0e),e(S0e,m5r),e(X9,g5r),e(X9,uX),e(uX,h5r),e(X9,p5r),e(V,_5r),e(V,V9),e(V9,P0e),e(P0e,u5r),e(V9,b5r),e(V9,bX),e(bX,v5r),e(V9,T5r),e(V,F5r),e(V,z9),e(z9,$0e),e($0e,C5r),e(z9,M5r),e(z9,vX),e(vX,E5r),e(z9,y5r),e(V,w5r),e(V,W9),e(W9,I0e),e(I0e,A5r),e(W9,L5r),e(W9,TX),e(TX,B5r),e(W9,x5r),e(V,k5r),e(V,Q9),e(Q9,D0e),e(D0e,R5r),e(Q9,S5r),e(Q9,FX),e(FX,P5r),e(Q9,$5r),e(V,I5r),e(V,H9),e(H9,j0e),e(j0e,D5r),e(H9,j5r),e(H9,CX),e(CX,N5r),e(H9,q5r),e(V,G5r),e(V,U9),e(U9,N0e),e(N0e,O5r),e(U9,X5r),e(U9,MX),e(MX,V5r),e(U9,z5r),e(V,W5r),e(V,J9),e(J9,q0e),e(q0e,Q5r),e(J9,H5r),e(J9,EX),e(EX,U5r),e(J9,J5r),e(V,Y5r),e(V,Y9),e(Y9,G0e),e(G0e,K5r),e(Y9,Z5r),e(Y9,yX),e(yX,e2r),e(Y9,o2r),e(V,r2r),e(V,K9),e(K9,O0e),e(O0e,t2r),e(K9,a2r),e(K9,wX),e(wX,n2r),e(K9,s2r),e(wo,l2r),e(wo,X0e),e(X0e,i2r),e(wo,d2r),g(H6,wo,null),b(c,$xe,u),b(c,Vc,u),e(Vc,Z9),e(Z9,V0e),g(U6,V0e,null),e(Vc,c2r),e(Vc,z0e),e(z0e,f2r),b(c,Ixe,u),b(c,kr,u),g(J6,kr,null),e(kr,m2r),e(kr,zc),e(zc,g2r),e(zc,W0e),e(W0e,h2r),e(zc,p2r),e(zc,Q0e),e(Q0e,_2r),e(zc,u2r),e(kr,b2r),e(kr,Y6),e(Y6,v2r),e(Y6,H0e),e(H0e,T2r),e(Y6,F2r),e(kr,C2r),e(kr,Et),g(K6,Et,null),e(Et,M2r),e(Et,U0e),e(U0e,E2r),e(Et,y2r),e(Et,Wc),e(Wc,w2r),e(Wc,J0e),e(J0e,A2r),e(Wc,L2r),e(Wc,Y0e),e(Y0e,B2r),e(Wc,x2r),e(Et,k2r),e(Et,K0e),e(K0e,R2r),e(Et,S2r),g(Z6,Et,null),e(kr,P2r),e(kr,Ao),g(eA,Ao,null),e(Ao,$2r),e(Ao,Z0e),e(Z0e,I2r),e(Ao,D2r),e(Ao,En),e(En,j2r),e(En,e1e),e(e1e,N2r),e(En,q2r),e(En,o1e),e(o1e,G2r),e(En,O2r),e(En,r1e),e(r1e,X2r),e(En,V2r),e(Ao,z2r),e(Ao,yn),e(yn,eC),e(eC,t1e),e(t1e,W2r),e(eC,Q2r),e(eC,AX),e(AX,H2r),e(eC,U2r),e(yn,J2r),e(yn,oC),e(oC,a1e),e(a1e,Y2r),e(oC,K2r),e(oC,LX),e(LX,Z2r),e(oC,evr),e(yn,ovr),e(yn,rC),e(rC,n1e),e(n1e,rvr),e(rC,tvr),e(rC,BX),e(BX,avr),e(rC,nvr),e(yn,svr),e(yn,tC),e(tC,s1e),e(s1e,lvr),e(tC,ivr),e(tC,xX),e(xX,dvr),e(tC,cvr),e(Ao,fvr),e(Ao,l1e),e(l1e,mvr),e(Ao,gvr),g(oA,Ao,null),b(c,Dxe,u),b(c,Qc,u),e(Qc,aC),e(aC,i1e),g(rA,i1e,null),e(Qc,hvr),e(Qc,d1e),e(d1e,pvr),b(c,jxe,u),b(c,Rr,u),g(tA,Rr,null),e(Rr,_vr),e(Rr,Hc),e(Hc,uvr),e(Hc,c1e),e(c1e,bvr),e(Hc,vvr),e(Hc,f1e),e(f1e,Tvr),e(Hc,Fvr),e(Rr,Cvr),e(Rr,aA),e(aA,Mvr),e(aA,m1e),e(m1e,Evr),e(aA,yvr),e(Rr,wvr),e(Rr,yt),g(nA,yt,null),e(yt,Avr),e(yt,g1e),e(g1e,Lvr),e(yt,Bvr),e(yt,Uc),e(Uc,xvr),e(Uc,h1e),e(h1e,kvr),e(Uc,Rvr),e(Uc,p1e),e(p1e,Svr),e(Uc,Pvr),e(yt,$vr),e(yt,_1e),e(_1e,Ivr),e(yt,Dvr),g(sA,yt,null),e(Rr,jvr),e(Rr,Lo),g(lA,Lo,null),e(Lo,Nvr),e(Lo,u1e),e(u1e,qvr),e(Lo,Gvr),e(Lo,wn),e(wn,Ovr),e(wn,b1e),e(b1e,Xvr),e(wn,Vvr),e(wn,v1e),e(v1e,zvr),e(wn,Wvr),e(wn,T1e),e(T1e,Qvr),e(wn,Hvr),e(Lo,Uvr),e(Lo,ce),e(ce,nC),e(nC,F1e),e(F1e,Jvr),e(nC,Yvr),e(nC,kX),e(kX,Kvr),e(nC,Zvr),e(ce,eTr),e(ce,sC),e(sC,C1e),e(C1e,oTr),e(sC,rTr),e(sC,RX),e(RX,tTr),e(sC,aTr),e(ce,nTr),e(ce,lC),e(lC,M1e),e(M1e,sTr),e(lC,lTr),e(lC,SX),e(SX,iTr),e(lC,dTr),e(ce,cTr),e(ce,iC),e(iC,E1e),e(E1e,fTr),e(iC,mTr),e(iC,PX),e(PX,gTr),e(iC,hTr),e(ce,pTr),e(ce,dC),e(dC,y1e),e(y1e,_Tr),e(dC,uTr),e(dC,$X),e($X,bTr),e(dC,vTr),e(ce,TTr),e(ce,cC),e(cC,w1e),e(w1e,FTr),e(cC,CTr),e(cC,IX),e(IX,MTr),e(cC,ETr),e(ce,yTr),e(ce,fC),e(fC,A1e),e(A1e,wTr),e(fC,ATr),e(fC,DX),e(DX,LTr),e(fC,BTr),e(ce,xTr),e(ce,mC),e(mC,L1e),e(L1e,kTr),e(mC,RTr),e(mC,jX),e(jX,STr),e(mC,PTr),e(ce,$Tr),e(ce,gC),e(gC,B1e),e(B1e,ITr),e(gC,DTr),e(gC,NX),e(NX,jTr),e(gC,NTr),e(ce,qTr),e(ce,hC),e(hC,x1e),e(x1e,GTr),e(hC,OTr),e(hC,qX),e(qX,XTr),e(hC,VTr),e(ce,zTr),e(ce,pC),e(pC,k1e),e(k1e,WTr),e(pC,QTr),e(pC,GX),e(GX,HTr),e(pC,UTr),e(ce,JTr),e(ce,_C),e(_C,R1e),e(R1e,YTr),e(_C,KTr),e(_C,OX),e(OX,ZTr),e(_C,eFr),e(Lo,oFr),e(Lo,S1e),e(S1e,rFr),e(Lo,tFr),g(iA,Lo,null),b(c,Nxe,u),b(c,Jc,u),e(Jc,uC),e(uC,P1e),g(dA,P1e,null),e(Jc,aFr),e(Jc,$1e),e($1e,nFr),b(c,qxe,u),b(c,Sr,u),g(cA,Sr,null),e(Sr,sFr),e(Sr,Yc),e(Yc,lFr),e(Yc,I1e),e(I1e,iFr),e(Yc,dFr),e(Yc,D1e),e(D1e,cFr),e(Yc,fFr),e(Sr,mFr),e(Sr,fA),e(fA,gFr),e(fA,j1e),e(j1e,hFr),e(fA,pFr),e(Sr,_Fr),e(Sr,wt),g(mA,wt,null),e(wt,uFr),e(wt,N1e),e(N1e,bFr),e(wt,vFr),e(wt,Kc),e(Kc,TFr),e(Kc,q1e),e(q1e,FFr),e(Kc,CFr),e(Kc,G1e),e(G1e,MFr),e(Kc,EFr),e(wt,yFr),e(wt,O1e),e(O1e,wFr),e(wt,AFr),g(gA,wt,null),e(Sr,LFr),e(Sr,Bo),g(hA,Bo,null),e(Bo,BFr),e(Bo,X1e),e(X1e,xFr),e(Bo,kFr),e(Bo,An),e(An,RFr),e(An,V1e),e(V1e,SFr),e(An,PFr),e(An,z1e),e(z1e,$Fr),e(An,IFr),e(An,W1e),e(W1e,DFr),e(An,jFr),e(Bo,NFr),e(Bo,ue),e(ue,bC),e(bC,Q1e),e(Q1e,qFr),e(bC,GFr),e(bC,XX),e(XX,OFr),e(bC,XFr),e(ue,VFr),e(ue,vC),e(vC,H1e),e(H1e,zFr),e(vC,WFr),e(vC,VX),e(VX,QFr),e(vC,HFr),e(ue,UFr),e(ue,TC),e(TC,U1e),e(U1e,JFr),e(TC,YFr),e(TC,zX),e(zX,KFr),e(TC,ZFr),e(ue,e9r),e(ue,FC),e(FC,J1e),e(J1e,o9r),e(FC,r9r),e(FC,WX),e(WX,t9r),e(FC,a9r),e(ue,n9r),e(ue,CC),e(CC,Y1e),e(Y1e,s9r),e(CC,l9r),e(CC,QX),e(QX,i9r),e(CC,d9r),e(ue,c9r),e(ue,MC),e(MC,K1e),e(K1e,f9r),e(MC,m9r),e(MC,HX),e(HX,g9r),e(MC,h9r),e(ue,p9r),e(ue,EC),e(EC,Z1e),e(Z1e,_9r),e(EC,u9r),e(EC,UX),e(UX,b9r),e(EC,v9r),e(ue,T9r),e(ue,yC),e(yC,ebe),e(ebe,F9r),e(yC,C9r),e(yC,JX),e(JX,M9r),e(yC,E9r),e(ue,y9r),e(ue,wC),e(wC,obe),e(obe,w9r),e(wC,A9r),e(wC,YX),e(YX,L9r),e(wC,B9r),e(ue,x9r),e(ue,AC),e(AC,rbe),e(rbe,k9r),e(AC,R9r),e(AC,KX),e(KX,S9r),e(AC,P9r),e(Bo,$9r),e(Bo,tbe),e(tbe,I9r),e(Bo,D9r),g(pA,Bo,null),b(c,Gxe,u),b(c,Zc,u),e(Zc,LC),e(LC,abe),g(_A,abe,null),e(Zc,j9r),e(Zc,nbe),e(nbe,N9r),b(c,Oxe,u),b(c,Pr,u),g(uA,Pr,null),e(Pr,q9r),e(Pr,ef),e(ef,G9r),e(ef,sbe),e(sbe,O9r),e(ef,X9r),e(ef,lbe),e(lbe,V9r),e(ef,z9r),e(Pr,W9r),e(Pr,bA),e(bA,Q9r),e(bA,ibe),e(ibe,H9r),e(bA,U9r),e(Pr,J9r),e(Pr,At),g(vA,At,null),e(At,Y9r),e(At,dbe),e(dbe,K9r),e(At,Z9r),e(At,of),e(of,eCr),e(of,cbe),e(cbe,oCr),e(of,rCr),e(of,fbe),e(fbe,tCr),e(of,aCr),e(At,nCr),e(At,mbe),e(mbe,sCr),e(At,lCr),g(TA,At,null),e(Pr,iCr),e(Pr,xo),g(FA,xo,null),e(xo,dCr),e(xo,gbe),e(gbe,cCr),e(xo,fCr),e(xo,Ln),e(Ln,mCr),e(Ln,hbe),e(hbe,gCr),e(Ln,hCr),e(Ln,pbe),e(pbe,pCr),e(Ln,_Cr),e(Ln,_be),e(_be,uCr),e(Ln,bCr),e(xo,vCr),e(xo,Ce),e(Ce,BC),e(BC,ube),e(ube,TCr),e(BC,FCr),e(BC,ZX),e(ZX,CCr),e(BC,MCr),e(Ce,ECr),e(Ce,xC),e(xC,bbe),e(bbe,yCr),e(xC,wCr),e(xC,eV),e(eV,ACr),e(xC,LCr),e(Ce,BCr),e(Ce,kC),e(kC,vbe),e(vbe,xCr),e(kC,kCr),e(kC,oV),e(oV,RCr),e(kC,SCr),e(Ce,PCr),e(Ce,RC),e(RC,Tbe),e(Tbe,$Cr),e(RC,ICr),e(RC,rV),e(rV,DCr),e(RC,jCr),e(Ce,NCr),e(Ce,SC),e(SC,Fbe),e(Fbe,qCr),e(SC,GCr),e(SC,tV),e(tV,OCr),e(SC,XCr),e(Ce,VCr),e(Ce,PC),e(PC,Cbe),e(Cbe,zCr),e(PC,WCr),e(PC,aV),e(aV,QCr),e(PC,HCr),e(Ce,UCr),e(Ce,$C),e($C,Mbe),e(Mbe,JCr),e($C,YCr),e($C,nV),e(nV,KCr),e($C,ZCr),e(Ce,eMr),e(Ce,IC),e(IC,Ebe),e(Ebe,oMr),e(IC,rMr),e(IC,sV),e(sV,tMr),e(IC,aMr),e(Ce,nMr),e(Ce,DC),e(DC,ybe),e(ybe,sMr),e(DC,lMr),e(DC,lV),e(lV,iMr),e(DC,dMr),e(xo,cMr),e(xo,wbe),e(wbe,fMr),e(xo,mMr),g(CA,xo,null),b(c,Xxe,u),b(c,rf,u),e(rf,jC),e(jC,Abe),g(MA,Abe,null),e(rf,gMr),e(rf,Lbe),e(Lbe,hMr),b(c,Vxe,u),b(c,$r,u),g(EA,$r,null),e($r,pMr),e($r,tf),e(tf,_Mr),e(tf,Bbe),e(Bbe,uMr),e(tf,bMr),e(tf,xbe),e(xbe,vMr),e(tf,TMr),e($r,FMr),e($r,yA),e(yA,CMr),e(yA,kbe),e(kbe,MMr),e(yA,EMr),e($r,yMr),e($r,Lt),g(wA,Lt,null),e(Lt,wMr),e(Lt,Rbe),e(Rbe,AMr),e(Lt,LMr),e(Lt,af),e(af,BMr),e(af,Sbe),e(Sbe,xMr),e(af,kMr),e(af,Pbe),e(Pbe,RMr),e(af,SMr),e(Lt,PMr),e(Lt,$be),e($be,$Mr),e(Lt,IMr),g(AA,Lt,null),e($r,DMr),e($r,ko),g(LA,ko,null),e(ko,jMr),e(ko,Ibe),e(Ibe,NMr),e(ko,qMr),e(ko,Bn),e(Bn,GMr),e(Bn,Dbe),e(Dbe,OMr),e(Bn,XMr),e(Bn,jbe),e(jbe,VMr),e(Bn,zMr),e(Bn,Nbe),e(Nbe,WMr),e(Bn,QMr),e(ko,HMr),e(ko,be),e(be,NC),e(NC,qbe),e(qbe,UMr),e(NC,JMr),e(NC,iV),e(iV,YMr),e(NC,KMr),e(be,ZMr),e(be,qC),e(qC,Gbe),e(Gbe,e4r),e(qC,o4r),e(qC,dV),e(dV,r4r),e(qC,t4r),e(be,a4r),e(be,GC),e(GC,Obe),e(Obe,n4r),e(GC,s4r),e(GC,cV),e(cV,l4r),e(GC,i4r),e(be,d4r),e(be,OC),e(OC,Xbe),e(Xbe,c4r),e(OC,f4r),e(OC,fV),e(fV,m4r),e(OC,g4r),e(be,h4r),e(be,XC),e(XC,Vbe),e(Vbe,p4r),e(XC,_4r),e(XC,mV),e(mV,u4r),e(XC,b4r),e(be,v4r),e(be,VC),e(VC,zbe),e(zbe,T4r),e(VC,F4r),e(VC,gV),e(gV,C4r),e(VC,M4r),e(be,E4r),e(be,zC),e(zC,Wbe),e(Wbe,y4r),e(zC,w4r),e(zC,hV),e(hV,A4r),e(zC,L4r),e(be,B4r),e(be,WC),e(WC,Qbe),e(Qbe,x4r),e(WC,k4r),e(WC,pV),e(pV,R4r),e(WC,S4r),e(be,P4r),e(be,QC),e(QC,Hbe),e(Hbe,$4r),e(QC,I4r),e(QC,_V),e(_V,D4r),e(QC,j4r),e(be,N4r),e(be,HC),e(HC,Ube),e(Ube,q4r),e(HC,G4r),e(HC,uV),e(uV,O4r),e(HC,X4r),e(ko,V4r),e(ko,Jbe),e(Jbe,z4r),e(ko,W4r),g(BA,ko,null),b(c,zxe,u),b(c,nf,u),e(nf,UC),e(UC,Ybe),g(xA,Ybe,null),e(nf,Q4r),e(nf,Kbe),e(Kbe,H4r),b(c,Wxe,u),b(c,Ir,u),g(kA,Ir,null),e(Ir,U4r),e(Ir,sf),e(sf,J4r),e(sf,Zbe),e(Zbe,Y4r),e(sf,K4r),e(sf,e5e),e(e5e,Z4r),e(sf,eEr),e(Ir,oEr),e(Ir,RA),e(RA,rEr),e(RA,o5e),e(o5e,tEr),e(RA,aEr),e(Ir,nEr),e(Ir,Bt),g(SA,Bt,null),e(Bt,sEr),e(Bt,r5e),e(r5e,lEr),e(Bt,iEr),e(Bt,lf),e(lf,dEr),e(lf,t5e),e(t5e,cEr),e(lf,fEr),e(lf,a5e),e(a5e,mEr),e(lf,gEr),e(Bt,hEr),e(Bt,n5e),e(n5e,pEr),e(Bt,_Er),g(PA,Bt,null),e(Ir,uEr),e(Ir,Ro),g($A,Ro,null),e(Ro,bEr),e(Ro,s5e),e(s5e,vEr),e(Ro,TEr),e(Ro,xn),e(xn,FEr),e(xn,l5e),e(l5e,CEr),e(xn,MEr),e(xn,i5e),e(i5e,EEr),e(xn,yEr),e(xn,d5e),e(d5e,wEr),e(xn,AEr),e(Ro,LEr),e(Ro,ve),e(ve,JC),e(JC,c5e),e(c5e,BEr),e(JC,xEr),e(JC,bV),e(bV,kEr),e(JC,REr),e(ve,SEr),e(ve,YC),e(YC,f5e),e(f5e,PEr),e(YC,$Er),e(YC,vV),e(vV,IEr),e(YC,DEr),e(ve,jEr),e(ve,KC),e(KC,m5e),e(m5e,NEr),e(KC,qEr),e(KC,TV),e(TV,GEr),e(KC,OEr),e(ve,XEr),e(ve,ZC),e(ZC,g5e),e(g5e,VEr),e(ZC,zEr),e(ZC,FV),e(FV,WEr),e(ZC,QEr),e(ve,HEr),e(ve,eM),e(eM,h5e),e(h5e,UEr),e(eM,JEr),e(eM,CV),e(CV,YEr),e(eM,KEr),e(ve,ZEr),e(ve,oM),e(oM,p5e),e(p5e,e3r),e(oM,o3r),e(oM,MV),e(MV,r3r),e(oM,t3r),e(ve,a3r),e(ve,rM),e(rM,_5e),e(_5e,n3r),e(rM,s3r),e(rM,EV),e(EV,l3r),e(rM,i3r),e(ve,d3r),e(ve,tM),e(tM,u5e),e(u5e,c3r),e(tM,f3r),e(tM,yV),e(yV,m3r),e(tM,g3r),e(ve,h3r),e(ve,aM),e(aM,b5e),e(b5e,p3r),e(aM,_3r),e(aM,wV),e(wV,u3r),e(aM,b3r),e(ve,v3r),e(ve,nM),e(nM,v5e),e(v5e,T3r),e(nM,F3r),e(nM,AV),e(AV,C3r),e(nM,M3r),e(Ro,E3r),e(Ro,T5e),e(T5e,y3r),e(Ro,w3r),g(IA,Ro,null),b(c,Qxe,u),b(c,df,u),e(df,sM),e(sM,F5e),g(DA,F5e,null),e(df,A3r),e(df,C5e),e(C5e,L3r),b(c,Hxe,u),b(c,Dr,u),g(jA,Dr,null),e(Dr,B3r),e(Dr,cf),e(cf,x3r),e(cf,M5e),e(M5e,k3r),e(cf,R3r),e(cf,E5e),e(E5e,S3r),e(cf,P3r),e(Dr,$3r),e(Dr,NA),e(NA,I3r),e(NA,y5e),e(y5e,D3r),e(NA,j3r),e(Dr,N3r),e(Dr,xt),g(qA,xt,null),e(xt,q3r),e(xt,w5e),e(w5e,G3r),e(xt,O3r),e(xt,ff),e(ff,X3r),e(ff,A5e),e(A5e,V3r),e(ff,z3r),e(ff,L5e),e(L5e,W3r),e(ff,Q3r),e(xt,H3r),e(xt,B5e),e(B5e,U3r),e(xt,J3r),g(GA,xt,null),e(Dr,Y3r),e(Dr,So),g(OA,So,null),e(So,K3r),e(So,x5e),e(x5e,Z3r),e(So,eyr),e(So,kn),e(kn,oyr),e(kn,k5e),e(k5e,ryr),e(kn,tyr),e(kn,R5e),e(R5e,ayr),e(kn,nyr),e(kn,S5e),e(S5e,syr),e(kn,lyr),e(So,iyr),e(So,Re),e(Re,lM),e(lM,P5e),e(P5e,dyr),e(lM,cyr),e(lM,LV),e(LV,fyr),e(lM,myr),e(Re,gyr),e(Re,iM),e(iM,$5e),e($5e,hyr),e(iM,pyr),e(iM,BV),e(BV,_yr),e(iM,uyr),e(Re,byr),e(Re,dM),e(dM,I5e),e(I5e,vyr),e(dM,Tyr),e(dM,xV),e(xV,Fyr),e(dM,Cyr),e(Re,Myr),e(Re,cM),e(cM,D5e),e(D5e,Eyr),e(cM,yyr),e(cM,kV),e(kV,wyr),e(cM,Ayr),e(Re,Lyr),e(Re,fM),e(fM,j5e),e(j5e,Byr),e(fM,xyr),e(fM,RV),e(RV,kyr),e(fM,Ryr),e(Re,Syr),e(Re,mM),e(mM,N5e),e(N5e,Pyr),e(mM,$yr),e(mM,SV),e(SV,Iyr),e(mM,Dyr),e(Re,jyr),e(Re,gM),e(gM,q5e),e(q5e,Nyr),e(gM,qyr),e(gM,PV),e(PV,Gyr),e(gM,Oyr),e(Re,Xyr),e(Re,hM),e(hM,G5e),e(G5e,Vyr),e(hM,zyr),e(hM,$V),e($V,Wyr),e(hM,Qyr),e(So,Hyr),e(So,O5e),e(O5e,Uyr),e(So,Jyr),g(XA,So,null),b(c,Uxe,u),b(c,mf,u),e(mf,pM),e(pM,X5e),g(VA,X5e,null),e(mf,Yyr),e(mf,V5e),e(V5e,Kyr),b(c,Jxe,u),b(c,jr,u),g(zA,jr,null),e(jr,Zyr),e(jr,gf),e(gf,ewr),e(gf,z5e),e(z5e,owr),e(gf,rwr),e(gf,W5e),e(W5e,twr),e(gf,awr),e(jr,nwr),e(jr,WA),e(WA,swr),e(WA,Q5e),e(Q5e,lwr),e(WA,iwr),e(jr,dwr),e(jr,kt),g(QA,kt,null),e(kt,cwr),e(kt,H5e),e(H5e,fwr),e(kt,mwr),e(kt,hf),e(hf,gwr),e(hf,U5e),e(U5e,hwr),e(hf,pwr),e(hf,J5e),e(J5e,_wr),e(hf,uwr),e(kt,bwr),e(kt,Y5e),e(Y5e,vwr),e(kt,Twr),g(HA,kt,null),e(jr,Fwr),e(jr,Po),g(UA,Po,null),e(Po,Cwr),e(Po,K5e),e(K5e,Mwr),e(Po,Ewr),e(Po,Rn),e(Rn,ywr),e(Rn,Z5e),e(Z5e,wwr),e(Rn,Awr),e(Rn,e2e),e(e2e,Lwr),e(Rn,Bwr),e(Rn,o2e),e(o2e,xwr),e(Rn,kwr),e(Po,Rwr),e(Po,Se),e(Se,_M),e(_M,r2e),e(r2e,Swr),e(_M,Pwr),e(_M,IV),e(IV,$wr),e(_M,Iwr),e(Se,Dwr),e(Se,uM),e(uM,t2e),e(t2e,jwr),e(uM,Nwr),e(uM,DV),e(DV,qwr),e(uM,Gwr),e(Se,Owr),e(Se,bM),e(bM,a2e),e(a2e,Xwr),e(bM,Vwr),e(bM,jV),e(jV,zwr),e(bM,Wwr),e(Se,Qwr),e(Se,vM),e(vM,n2e),e(n2e,Hwr),e(vM,Uwr),e(vM,NV),e(NV,Jwr),e(vM,Ywr),e(Se,Kwr),e(Se,TM),e(TM,s2e),e(s2e,Zwr),e(TM,e6r),e(TM,qV),e(qV,o6r),e(TM,r6r),e(Se,t6r),e(Se,FM),e(FM,l2e),e(l2e,a6r),e(FM,n6r),e(FM,GV),e(GV,s6r),e(FM,l6r),e(Se,i6r),e(Se,CM),e(CM,i2e),e(i2e,d6r),e(CM,c6r),e(CM,OV),e(OV,f6r),e(CM,m6r),e(Se,g6r),e(Se,MM),e(MM,d2e),e(d2e,h6r),e(MM,p6r),e(MM,XV),e(XV,_6r),e(MM,u6r),e(Po,b6r),e(Po,c2e),e(c2e,v6r),e(Po,T6r),g(JA,Po,null),b(c,Yxe,u),b(c,pf,u),e(pf,EM),e(EM,f2e),g(YA,f2e,null),e(pf,F6r),e(pf,m2e),e(m2e,C6r),b(c,Kxe,u),b(c,Nr,u),g(KA,Nr,null),e(Nr,M6r),e(Nr,_f),e(_f,E6r),e(_f,g2e),e(g2e,y6r),e(_f,w6r),e(_f,h2e),e(h2e,A6r),e(_f,L6r),e(Nr,B6r),e(Nr,ZA),e(ZA,x6r),e(ZA,p2e),e(p2e,k6r),e(ZA,R6r),e(Nr,S6r),e(Nr,Rt),g(eL,Rt,null),e(Rt,P6r),e(Rt,_2e),e(_2e,$6r),e(Rt,I6r),e(Rt,uf),e(uf,D6r),e(uf,u2e),e(u2e,j6r),e(uf,N6r),e(uf,b2e),e(b2e,q6r),e(uf,G6r),e(Rt,O6r),e(Rt,v2e),e(v2e,X6r),e(Rt,V6r),g(oL,Rt,null),e(Nr,z6r),e(Nr,$o),g(rL,$o,null),e($o,W6r),e($o,T2e),e(T2e,Q6r),e($o,H6r),e($o,Sn),e(Sn,U6r),e(Sn,F2e),e(F2e,J6r),e(Sn,Y6r),e(Sn,C2e),e(C2e,K6r),e(Sn,Z6r),e(Sn,M2e),e(M2e,eAr),e(Sn,oAr),e($o,rAr),e($o,E2e),e(E2e,yM),e(yM,y2e),e(y2e,tAr),e(yM,aAr),e(yM,VV),e(VV,nAr),e(yM,sAr),e($o,lAr),e($o,w2e),e(w2e,iAr),e($o,dAr),g(tL,$o,null),b(c,Zxe,u),b(c,bf,u),e(bf,wM),e(wM,A2e),g(aL,A2e,null),e(bf,cAr),e(bf,L2e),e(L2e,fAr),b(c,eke,u),b(c,qr,u),g(nL,qr,null),e(qr,mAr),e(qr,vf),e(vf,gAr),e(vf,B2e),e(B2e,hAr),e(vf,pAr),e(vf,x2e),e(x2e,_Ar),e(vf,uAr),e(qr,bAr),e(qr,sL),e(sL,vAr),e(sL,k2e),e(k2e,TAr),e(sL,FAr),e(qr,CAr),e(qr,St),g(lL,St,null),e(St,MAr),e(St,R2e),e(R2e,EAr),e(St,yAr),e(St,Tf),e(Tf,wAr),e(Tf,S2e),e(S2e,AAr),e(Tf,LAr),e(Tf,P2e),e(P2e,BAr),e(Tf,xAr),e(St,kAr),e(St,$2e),e($2e,RAr),e(St,SAr),g(iL,St,null),e(qr,PAr),e(qr,Io),g(dL,Io,null),e(Io,$Ar),e(Io,I2e),e(I2e,IAr),e(Io,DAr),e(Io,Pn),e(Pn,jAr),e(Pn,D2e),e(D2e,NAr),e(Pn,qAr),e(Pn,j2e),e(j2e,GAr),e(Pn,OAr),e(Pn,N2e),e(N2e,XAr),e(Pn,VAr),e(Io,zAr),e(Io,cL),e(cL,AM),e(AM,q2e),e(q2e,WAr),e(AM,QAr),e(AM,zV),e(zV,HAr),e(AM,UAr),e(cL,JAr),e(cL,LM),e(LM,G2e),e(G2e,YAr),e(LM,KAr),e(LM,WV),e(WV,ZAr),e(LM,eLr),e(Io,oLr),e(Io,O2e),e(O2e,rLr),e(Io,tLr),g(fL,Io,null),b(c,oke,u),b(c,Ff,u),e(Ff,BM),e(BM,X2e),g(mL,X2e,null),e(Ff,aLr),e(Ff,V2e),e(V2e,nLr),b(c,rke,u),b(c,Gr,u),g(gL,Gr,null),e(Gr,sLr),e(Gr,Cf),e(Cf,lLr),e(Cf,z2e),e(z2e,iLr),e(Cf,dLr),e(Cf,W2e),e(W2e,cLr),e(Cf,fLr),e(Gr,mLr),e(Gr,hL),e(hL,gLr),e(hL,Q2e),e(Q2e,hLr),e(hL,pLr),e(Gr,_Lr),e(Gr,Pt),g(pL,Pt,null),e(Pt,uLr),e(Pt,H2e),e(H2e,bLr),e(Pt,vLr),e(Pt,Mf),e(Mf,TLr),e(Mf,U2e),e(U2e,FLr),e(Mf,CLr),e(Mf,J2e),e(J2e,MLr),e(Mf,ELr),e(Pt,yLr),e(Pt,Y2e),e(Y2e,wLr),e(Pt,ALr),g(_L,Pt,null),e(Gr,LLr),e(Gr,Do),g(uL,Do,null),e(Do,BLr),e(Do,K2e),e(K2e,xLr),e(Do,kLr),e(Do,$n),e($n,RLr),e($n,Z2e),e(Z2e,SLr),e($n,PLr),e($n,eve),e(eve,$Lr),e($n,ILr),e($n,ove),e(ove,DLr),e($n,jLr),e(Do,NLr),e(Do,rve),e(rve,xM),e(xM,tve),e(tve,qLr),e(xM,GLr),e(xM,QV),e(QV,OLr),e(xM,XLr),e(Do,VLr),e(Do,ave),e(ave,zLr),e(Do,WLr),g(bL,Do,null),tke=!0},p(c,[u]){const vL={};u&2&&(vL.$$scope={dirty:u,ctx:c}),xf.$set(vL);const nve={};u&2&&(nve.$$scope={dirty:u,ctx:c}),mh.$set(nve);const sve={};u&2&&(sve.$$scope={dirty:u,ctx:c}),Mh.$set(sve)},i(c){tke||(h(fe.$$.fragment,c),h($a.$$.fragment,c),h(P4.$$.fragment,c),h($4.$$.fragment,c),h(xf.$$.fragment,c),h(I4.$$.fragment,c),h(D4.$$.fragment,c),h(q4.$$.fragment,c),h(G4.$$.fragment,c),h(O4.$$.fragment,c),h(X4.$$.fragment,c),h(V4.$$.fragment,c),h(Q4.$$.fragment,c),h(H4.$$.fragment,c),h(U4.$$.fragment,c),h(J4.$$.fragment,c),h(Y4.$$.fragment,c),h(eE.$$.fragment,c),h(mh.$$.fragment,c),h(oE.$$.fragment,c),h(rE.$$.fragment,c),h(tE.$$.fragment,c),h(aE.$$.fragment,c),h(lE.$$.fragment,c),h(Mh.$$.fragment,c),h(iE.$$.fragment,c),h(dE.$$.fragment,c),h(cE.$$.fragment,c),h(fE.$$.fragment,c),h(gE.$$.fragment,c),h(hE.$$.fragment,c),h(pE.$$.fragment,c),h(_E.$$.fragment,c),h(uE.$$.fragment,c),h(bE.$$.fragment,c),h(TE.$$.fragment,c),h(FE.$$.fragment,c),h(CE.$$.fragment,c),h(ME.$$.fragment,c),h(EE.$$.fragment,c),h(yE.$$.fragment,c),h(AE.$$.fragment,c),h(LE.$$.fragment,c),h(BE.$$.fragment,c),h(xE.$$.fragment,c),h(kE.$$.fragment,c),h(RE.$$.fragment,c),h(PE.$$.fragment,c),h($E.$$.fragment,c),h(IE.$$.fragment,c),h(DE.$$.fragment,c),h(jE.$$.fragment,c),h(NE.$$.fragment,c),h(GE.$$.fragment,c),h(OE.$$.fragment,c),h(XE.$$.fragment,c),h(VE.$$.fragment,c),h(zE.$$.fragment,c),h(WE.$$.fragment,c),h(HE.$$.fragment,c),h(UE.$$.fragment,c),h(JE.$$.fragment,c),h(YE.$$.fragment,c),h(KE.$$.fragment,c),h(ZE.$$.fragment,c),h(o3.$$.fragment,c),h(r3.$$.fragment,c),h(t3.$$.fragment,c),h(a3.$$.fragment,c),h(n3.$$.fragment,c),h(s3.$$.fragment,c),h(i3.$$.fragment,c),h(d3.$$.fragment,c),h(c3.$$.fragment,c),h(f3.$$.fragment,c),h(m3.$$.fragment,c),h(g3.$$.fragment,c),h(p3.$$.fragment,c),h(_3.$$.fragment,c),h(u3.$$.fragment,c),h(b3.$$.fragment,c),h(v3.$$.fragment,c),h(T3.$$.fragment,c),h(C3.$$.fragment,c),h(M3.$$.fragment,c),h(E3.$$.fragment,c),h(y3.$$.fragment,c),h(w3.$$.fragment,c),h(A3.$$.fragment,c),h(B3.$$.fragment,c),h(x3.$$.fragment,c),h(k3.$$.fragment,c),h(R3.$$.fragment,c),h(S3.$$.fragment,c),h(P3.$$.fragment,c),h(I3.$$.fragment,c),h(D3.$$.fragment,c),h(j3.$$.fragment,c),h(N3.$$.fragment,c),h(q3.$$.fragment,c),h(G3.$$.fragment,c),h(X3.$$.fragment,c),h(V3.$$.fragment,c),h(z3.$$.fragment,c),h(W3.$$.fragment,c),h(Q3.$$.fragment,c),h(H3.$$.fragment,c),h(J3.$$.fragment,c),h(Y3.$$.fragment,c),h(K3.$$.fragment,c),h(Z3.$$.fragment,c),h(ey.$$.fragment,c),h(oy.$$.fragment,c),h(ty.$$.fragment,c),h(ay.$$.fragment,c),h(ny.$$.fragment,c),h(sy.$$.fragment,c),h(ly.$$.fragment,c),h(iy.$$.fragment,c),h(cy.$$.fragment,c),h(fy.$$.fragment,c),h(my.$$.fragment,c),h(gy.$$.fragment,c),h(hy.$$.fragment,c),h(py.$$.fragment,c),h(uy.$$.fragment,c),h(by.$$.fragment,c),h(vy.$$.fragment,c),h(Fy.$$.fragment,c),h(Cy.$$.fragment,c),h(My.$$.fragment,c),h(yy.$$.fragment,c),h(wy.$$.fragment,c),h(Ay.$$.fragment,c),h(Ly.$$.fragment,c),h(By.$$.fragment,c),h(xy.$$.fragment,c),h(Ry.$$.fragment,c),h(Sy.$$.fragment,c),h(Py.$$.fragment,c),h($y.$$.fragment,c),h(Iy.$$.fragment,c),h(Dy.$$.fragment,c),h(Ny.$$.fragment,c),h(qy.$$.fragment,c),h(Gy.$$.fragment,c),h(Oy.$$.fragment,c),h(Xy.$$.fragment,c),h(Vy.$$.fragment,c),h(Wy.$$.fragment,c),h(Qy.$$.fragment,c),h(Hy.$$.fragment,c),h(Uy.$$.fragment,c),h(Jy.$$.fragment,c),h(Yy.$$.fragment,c),h(Zy.$$.fragment,c),h(ew.$$.fragment,c),h(ow.$$.fragment,c),h(tw.$$.fragment,c),h(aw.$$.fragment,c),h(nw.$$.fragment,c),h(lw.$$.fragment,c),h(iw.$$.fragment,c),h(dw.$$.fragment,c),h(cw.$$.fragment,c),h(fw.$$.fragment,c),h(mw.$$.fragment,c),h(hw.$$.fragment,c),h(pw.$$.fragment,c),h(_w.$$.fragment,c),h(uw.$$.fragment,c),h(bw.$$.fragment,c),h(vw.$$.fragment,c),h(Fw.$$.fragment,c),h(Cw.$$.fragment,c),h(Mw.$$.fragment,c),h(Ew.$$.fragment,c),h(yw.$$.fragment,c),h(ww.$$.fragment,c),h(Lw.$$.fragment,c),h(Bw.$$.fragment,c),h(xw.$$.fragment,c),h(Rw.$$.fragment,c),h(Sw.$$.fragment,c),h(Pw.$$.fragment,c),h(Iw.$$.fragment,c),h(Dw.$$.fragment,c),h(jw.$$.fragment,c),h(Nw.$$.fragment,c),h(qw.$$.fragment,c),h(Gw.$$.fragment,c),h(Xw.$$.fragment,c),h(Vw.$$.fragment,c),h(zw.$$.fragment,c),h(Ww.$$.fragment,c),h(Qw.$$.fragment,c),h(Hw.$$.fragment,c),h(Jw.$$.fragment,c),h(Yw.$$.fragment,c),h(Kw.$$.fragment,c),h(Zw.$$.fragment,c),h(e6.$$.fragment,c),h(o6.$$.fragment,c),h(t6.$$.fragment,c),h(a6.$$.fragment,c),h(n6.$$.fragment,c),h(s6.$$.fragment,c),h(l6.$$.fragment,c),h(i6.$$.fragment,c),h(c6.$$.fragment,c),h(f6.$$.fragment,c),h(m6.$$.fragment,c),h(g6.$$.fragment,c),h(h6.$$.fragment,c),h(p6.$$.fragment,c),h(u6.$$.fragment,c),h(b6.$$.fragment,c),h(v6.$$.fragment,c),h(T6.$$.fragment,c),h(F6.$$.fragment,c),h(C6.$$.fragment,c),h(E6.$$.fragment,c),h(y6.$$.fragment,c),h(w6.$$.fragment,c),h(A6.$$.fragment,c),h(L6.$$.fragment,c),h(B6.$$.fragment,c),h(k6.$$.fragment,c),h(R6.$$.fragment,c),h(S6.$$.fragment,c),h(P6.$$.fragment,c),h($6.$$.fragment,c),h(I6.$$.fragment,c),h(j6.$$.fragment,c),h(N6.$$.fragment,c),h(q6.$$.fragment,c),h(G6.$$.fragment,c),h(O6.$$.fragment,c),h(X6.$$.fragment,c),h(z6.$$.fragment,c),h(W6.$$.fragment,c),h(Q6.$$.fragment,c),h(H6.$$.fragment,c),h(U6.$$.fragment,c),h(J6.$$.fragment,c),h(K6.$$.fragment,c),h(Z6.$$.fragment,c),h(eA.$$.fragment,c),h(oA.$$.fragment,c),h(rA.$$.fragment,c),h(tA.$$.fragment,c),h(nA.$$.fragment,c),h(sA.$$.fragment,c),h(lA.$$.fragment,c),h(iA.$$.fragment,c),h(dA.$$.fragment,c),h(cA.$$.fragment,c),h(mA.$$.fragment,c),h(gA.$$.fragment,c),h(hA.$$.fragment,c),h(pA.$$.fragment,c),h(_A.$$.fragment,c),h(uA.$$.fragment,c),h(vA.$$.fragment,c),h(TA.$$.fragment,c),h(FA.$$.fragment,c),h(CA.$$.fragment,c),h(MA.$$.fragment,c),h(EA.$$.fragment,c),h(wA.$$.fragment,c),h(AA.$$.fragment,c),h(LA.$$.fragment,c),h(BA.$$.fragment,c),h(xA.$$.fragment,c),h(kA.$$.fragment,c),h(SA.$$.fragment,c),h(PA.$$.fragment,c),h($A.$$.fragment,c),h(IA.$$.fragment,c),h(DA.$$.fragment,c),h(jA.$$.fragment,c),h(qA.$$.fragment,c),h(GA.$$.fragment,c),h(OA.$$.fragment,c),h(XA.$$.fragment,c),h(VA.$$.fragment,c),h(zA.$$.fragment,c),h(QA.$$.fragment,c),h(HA.$$.fragment,c),h(UA.$$.fragment,c),h(JA.$$.fragment,c),h(YA.$$.fragment,c),h(KA.$$.fragment,c),h(eL.$$.fragment,c),h(oL.$$.fragment,c),h(rL.$$.fragment,c),h(tL.$$.fragment,c),h(aL.$$.fragment,c),h(nL.$$.fragment,c),h(lL.$$.fragment,c),h(iL.$$.fragment,c),h(dL.$$.fragment,c),h(fL.$$.fragment,c),h(mL.$$.fragment,c),h(gL.$$.fragment,c),h(pL.$$.fragment,c),h(_L.$$.fragment,c),h(uL.$$.fragment,c),h(bL.$$.fragment,c),tke=!0)},o(c){p(fe.$$.fragment,c),p($a.$$.fragment,c),p(P4.$$.fragment,c),p($4.$$.fragment,c),p(xf.$$.fragment,c),p(I4.$$.fragment,c),p(D4.$$.fragment,c),p(q4.$$.fragment,c),p(G4.$$.fragment,c),p(O4.$$.fragment,c),p(X4.$$.fragment,c),p(V4.$$.fragment,c),p(Q4.$$.fragment,c),p(H4.$$.fragment,c),p(U4.$$.fragment,c),p(J4.$$.fragment,c),p(Y4.$$.fragment,c),p(eE.$$.fragment,c),p(mh.$$.fragment,c),p(oE.$$.fragment,c),p(rE.$$.fragment,c),p(tE.$$.fragment,c),p(aE.$$.fragment,c),p(lE.$$.fragment,c),p(Mh.$$.fragment,c),p(iE.$$.fragment,c),p(dE.$$.fragment,c),p(cE.$$.fragment,c),p(fE.$$.fragment,c),p(gE.$$.fragment,c),p(hE.$$.fragment,c),p(pE.$$.fragment,c),p(_E.$$.fragment,c),p(uE.$$.fragment,c),p(bE.$$.fragment,c),p(TE.$$.fragment,c),p(FE.$$.fragment,c),p(CE.$$.fragment,c),p(ME.$$.fragment,c),p(EE.$$.fragment,c),p(yE.$$.fragment,c),p(AE.$$.fragment,c),p(LE.$$.fragment,c),p(BE.$$.fragment,c),p(xE.$$.fragment,c),p(kE.$$.fragment,c),p(RE.$$.fragment,c),p(PE.$$.fragment,c),p($E.$$.fragment,c),p(IE.$$.fragment,c),p(DE.$$.fragment,c),p(jE.$$.fragment,c),p(NE.$$.fragment,c),p(GE.$$.fragment,c),p(OE.$$.fragment,c),p(XE.$$.fragment,c),p(VE.$$.fragment,c),p(zE.$$.fragment,c),p(WE.$$.fragment,c),p(HE.$$.fragment,c),p(UE.$$.fragment,c),p(JE.$$.fragment,c),p(YE.$$.fragment,c),p(KE.$$.fragment,c),p(ZE.$$.fragment,c),p(o3.$$.fragment,c),p(r3.$$.fragment,c),p(t3.$$.fragment,c),p(a3.$$.fragment,c),p(n3.$$.fragment,c),p(s3.$$.fragment,c),p(i3.$$.fragment,c),p(d3.$$.fragment,c),p(c3.$$.fragment,c),p(f3.$$.fragment,c),p(m3.$$.fragment,c),p(g3.$$.fragment,c),p(p3.$$.fragment,c),p(_3.$$.fragment,c),p(u3.$$.fragment,c),p(b3.$$.fragment,c),p(v3.$$.fragment,c),p(T3.$$.fragment,c),p(C3.$$.fragment,c),p(M3.$$.fragment,c),p(E3.$$.fragment,c),p(y3.$$.fragment,c),p(w3.$$.fragment,c),p(A3.$$.fragment,c),p(B3.$$.fragment,c),p(x3.$$.fragment,c),p(k3.$$.fragment,c),p(R3.$$.fragment,c),p(S3.$$.fragment,c),p(P3.$$.fragment,c),p(I3.$$.fragment,c),p(D3.$$.fragment,c),p(j3.$$.fragment,c),p(N3.$$.fragment,c),p(q3.$$.fragment,c),p(G3.$$.fragment,c),p(X3.$$.fragment,c),p(V3.$$.fragment,c),p(z3.$$.fragment,c),p(W3.$$.fragment,c),p(Q3.$$.fragment,c),p(H3.$$.fragment,c),p(J3.$$.fragment,c),p(Y3.$$.fragment,c),p(K3.$$.fragment,c),p(Z3.$$.fragment,c),p(ey.$$.fragment,c),p(oy.$$.fragment,c),p(ty.$$.fragment,c),p(ay.$$.fragment,c),p(ny.$$.fragment,c),p(sy.$$.fragment,c),p(ly.$$.fragment,c),p(iy.$$.fragment,c),p(cy.$$.fragment,c),p(fy.$$.fragment,c),p(my.$$.fragment,c),p(gy.$$.fragment,c),p(hy.$$.fragment,c),p(py.$$.fragment,c),p(uy.$$.fragment,c),p(by.$$.fragment,c),p(vy.$$.fragment,c),p(Fy.$$.fragment,c),p(Cy.$$.fragment,c),p(My.$$.fragment,c),p(yy.$$.fragment,c),p(wy.$$.fragment,c),p(Ay.$$.fragment,c),p(Ly.$$.fragment,c),p(By.$$.fragment,c),p(xy.$$.fragment,c),p(Ry.$$.fragment,c),p(Sy.$$.fragment,c),p(Py.$$.fragment,c),p($y.$$.fragment,c),p(Iy.$$.fragment,c),p(Dy.$$.fragment,c),p(Ny.$$.fragment,c),p(qy.$$.fragment,c),p(Gy.$$.fragment,c),p(Oy.$$.fragment,c),p(Xy.$$.fragment,c),p(Vy.$$.fragment,c),p(Wy.$$.fragment,c),p(Qy.$$.fragment,c),p(Hy.$$.fragment,c),p(Uy.$$.fragment,c),p(Jy.$$.fragment,c),p(Yy.$$.fragment,c),p(Zy.$$.fragment,c),p(ew.$$.fragment,c),p(ow.$$.fragment,c),p(tw.$$.fragment,c),p(aw.$$.fragment,c),p(nw.$$.fragment,c),p(lw.$$.fragment,c),p(iw.$$.fragment,c),p(dw.$$.fragment,c),p(cw.$$.fragment,c),p(fw.$$.fragment,c),p(mw.$$.fragment,c),p(hw.$$.fragment,c),p(pw.$$.fragment,c),p(_w.$$.fragment,c),p(uw.$$.fragment,c),p(bw.$$.fragment,c),p(vw.$$.fragment,c),p(Fw.$$.fragment,c),p(Cw.$$.fragment,c),p(Mw.$$.fragment,c),p(Ew.$$.fragment,c),p(yw.$$.fragment,c),p(ww.$$.fragment,c),p(Lw.$$.fragment,c),p(Bw.$$.fragment,c),p(xw.$$.fragment,c),p(Rw.$$.fragment,c),p(Sw.$$.fragment,c),p(Pw.$$.fragment,c),p(Iw.$$.fragment,c),p(Dw.$$.fragment,c),p(jw.$$.fragment,c),p(Nw.$$.fragment,c),p(qw.$$.fragment,c),p(Gw.$$.fragment,c),p(Xw.$$.fragment,c),p(Vw.$$.fragment,c),p(zw.$$.fragment,c),p(Ww.$$.fragment,c),p(Qw.$$.fragment,c),p(Hw.$$.fragment,c),p(Jw.$$.fragment,c),p(Yw.$$.fragment,c),p(Kw.$$.fragment,c),p(Zw.$$.fragment,c),p(e6.$$.fragment,c),p(o6.$$.fragment,c),p(t6.$$.fragment,c),p(a6.$$.fragment,c),p(n6.$$.fragment,c),p(s6.$$.fragment,c),p(l6.$$.fragment,c),p(i6.$$.fragment,c),p(c6.$$.fragment,c),p(f6.$$.fragment,c),p(m6.$$.fragment,c),p(g6.$$.fragment,c),p(h6.$$.fragment,c),p(p6.$$.fragment,c),p(u6.$$.fragment,c),p(b6.$$.fragment,c),p(v6.$$.fragment,c),p(T6.$$.fragment,c),p(F6.$$.fragment,c),p(C6.$$.fragment,c),p(E6.$$.fragment,c),p(y6.$$.fragment,c),p(w6.$$.fragment,c),p(A6.$$.fragment,c),p(L6.$$.fragment,c),p(B6.$$.fragment,c),p(k6.$$.fragment,c),p(R6.$$.fragment,c),p(S6.$$.fragment,c),p(P6.$$.fragment,c),p($6.$$.fragment,c),p(I6.$$.fragment,c),p(j6.$$.fragment,c),p(N6.$$.fragment,c),p(q6.$$.fragment,c),p(G6.$$.fragment,c),p(O6.$$.fragment,c),p(X6.$$.fragment,c),p(z6.$$.fragment,c),p(W6.$$.fragment,c),p(Q6.$$.fragment,c),p(H6.$$.fragment,c),p(U6.$$.fragment,c),p(J6.$$.fragment,c),p(K6.$$.fragment,c),p(Z6.$$.fragment,c),p(eA.$$.fragment,c),p(oA.$$.fragment,c),p(rA.$$.fragment,c),p(tA.$$.fragment,c),p(nA.$$.fragment,c),p(sA.$$.fragment,c),p(lA.$$.fragment,c),p(iA.$$.fragment,c),p(dA.$$.fragment,c),p(cA.$$.fragment,c),p(mA.$$.fragment,c),p(gA.$$.fragment,c),p(hA.$$.fragment,c),p(pA.$$.fragment,c),p(_A.$$.fragment,c),p(uA.$$.fragment,c),p(vA.$$.fragment,c),p(TA.$$.fragment,c),p(FA.$$.fragment,c),p(CA.$$.fragment,c),p(MA.$$.fragment,c),p(EA.$$.fragment,c),p(wA.$$.fragment,c),p(AA.$$.fragment,c),p(LA.$$.fragment,c),p(BA.$$.fragment,c),p(xA.$$.fragment,c),p(kA.$$.fragment,c),p(SA.$$.fragment,c),p(PA.$$.fragment,c),p($A.$$.fragment,c),p(IA.$$.fragment,c),p(DA.$$.fragment,c),p(jA.$$.fragment,c),p(qA.$$.fragment,c),p(GA.$$.fragment,c),p(OA.$$.fragment,c),p(XA.$$.fragment,c),p(VA.$$.fragment,c),p(zA.$$.fragment,c),p(QA.$$.fragment,c),p(HA.$$.fragment,c),p(UA.$$.fragment,c),p(JA.$$.fragment,c),p(YA.$$.fragment,c),p(KA.$$.fragment,c),p(eL.$$.fragment,c),p(oL.$$.fragment,c),p(rL.$$.fragment,c),p(tL.$$.fragment,c),p(aL.$$.fragment,c),p(nL.$$.fragment,c),p(lL.$$.fragment,c),p(iL.$$.fragment,c),p(dL.$$.fragment,c),p(fL.$$.fragment,c),p(mL.$$.fragment,c),p(gL.$$.fragment,c),p(pL.$$.fragment,c),p(_L.$$.fragment,c),p(uL.$$.fragment,c),p(bL.$$.fragment,c),tke=!1},d(c){t(J),c&&t(Pe),c&&t(ie),_(fe),c&&t(yf),c&&t(sa),c&&t(Le),c&&t(io),c&&t(Af),_($a,c),c&&t(co),c&&t(he),c&&t(Vo),c&&t(Ia),c&&t(aBe),c&&t(Ii),_(P4),c&&t(nBe),c&&t(qn),c&&t(sBe),_($4,c),c&&t(lBe),c&&t(F8),c&&t(iBe),_(xf,c),c&&t(dBe),c&&t(Di),_(I4),c&&t(cBe),c&&t(zo),_(D4),_(q4),_(G4),_(O4),c&&t(fBe),c&&t(Ni),_(X4),c&&t(mBe),c&&t(Wo),_(V4),_(Q4),_(H4),_(U4),c&&t(gBe),c&&t(qi),_(J4),c&&t(hBe),c&&t(Qo),_(Y4),_(eE),_(mh),_(oE),_(rE),c&&t(pBe),c&&t(Gi),_(tE),c&&t(_Be),c&&t(Ho),_(aE),_(lE),_(Mh),_(iE),_(dE),c&&t(uBe),c&&t(Xi),_(cE),c&&t(bBe),c&&t(Uo),_(fE),_(gE),_(hE),_(pE),_(_E),c&&t(vBe),c&&t(Wi),_(uE),c&&t(TBe),c&&t(Jo),_(bE),_(TE),_(FE),_(CE),_(ME),c&&t(FBe),c&&t(Ui),_(EE),c&&t(CBe),c&&t(Yo),_(yE),_(AE),_(LE),_(BE),_(xE),c&&t(MBe),c&&t(Ki),_(kE),c&&t(EBe),c&&t(Ko),_(RE),_(PE),_($E),_(IE),_(DE),c&&t(yBe),c&&t(od),_(jE),c&&t(wBe),c&&t(Zo),_(NE),_(GE),_(OE),_(XE),_(VE),c&&t(ABe),c&&t(ad),_(zE),c&&t(LBe),c&&t(er),_(WE),_(HE),_(UE),_(JE),_(YE),c&&t(BBe),c&&t(ld),_(KE),c&&t(xBe),c&&t(or),_(ZE),_(o3),_(r3),_(t3),_(a3),c&&t(kBe),c&&t(cd),_(n3),c&&t(RBe),c&&t(rr),_(s3),_(i3),_(d3),_(c3),_(f3),c&&t(SBe),c&&t(gd),_(m3),c&&t(PBe),c&&t(tr),_(g3),_(p3),_(_3),_(u3),_(b3),c&&t($Be),c&&t(_d),_(v3),c&&t(IBe),c&&t(ar),_(T3),_(C3),_(M3),_(E3),_(y3),c&&t(DBe),c&&t(vd),_(w3),c&&t(jBe),c&&t(nr),_(A3),_(B3),_(x3),_(k3),_(R3),c&&t(NBe),c&&t(Cd),_(S3),c&&t(qBe),c&&t(sr),_(P3),_(I3),_(D3),_(j3),_(N3),c&&t(GBe),c&&t(yd),_(q3),c&&t(OBe),c&&t(lr),_(G3),_(X3),_(V3),_(z3),_(W3),c&&t(XBe),c&&t(Ld),_(Q3),c&&t(VBe),c&&t(ir),_(H3),_(J3),_(Y3),_(K3),_(Z3),c&&t(zBe),c&&t(kd),_(ey),c&&t(WBe),c&&t(dr),_(oy),_(ty),_(ay),_(ny),_(sy),c&&t(QBe),c&&t(Pd),_(ly),c&&t(HBe),c&&t(cr),_(iy),_(cy),_(fy),_(my),_(gy),c&&t(UBe),c&&t(Dd),_(hy),c&&t(JBe),c&&t(fr),_(py),_(uy),_(by),_(vy),_(Fy),c&&t(YBe),c&&t(qd),_(Cy),c&&t(KBe),c&&t(mr),_(My),_(yy),_(wy),_(Ay),_(Ly),c&&t(ZBe),c&&t(Xd),_(By),c&&t(exe),c&&t(gr),_(xy),_(Ry),_(Sy),_(Py),_($y),c&&t(oxe),c&&t(Qd),_(Iy),c&&t(rxe),c&&t(hr),_(Dy),_(Ny),_(qy),_(Gy),_(Oy),c&&t(txe),c&&t(Jd),_(Xy),c&&t(axe),c&&t(pr),_(Vy),_(Wy),_(Qy),_(Hy),_(Uy),c&&t(nxe),c&&t(Zd),_(Jy),c&&t(sxe),c&&t(_r),_(Yy),_(Zy),_(ew),_(ow),_(tw),c&&t(lxe),c&&t(rc),_(aw),c&&t(ixe),c&&t(ur),_(nw),_(lw),_(iw),_(dw),_(cw),c&&t(dxe),c&&t(nc),_(fw),c&&t(cxe),c&&t(br),_(mw),_(hw),_(pw),_(_w),_(uw),c&&t(fxe),c&&t(ic),_(bw),c&&t(mxe),c&&t(vr),_(vw),_(Fw),_(Cw),_(Mw),_(Ew),c&&t(gxe),c&&t(fc),_(yw),c&&t(hxe),c&&t(Tr),_(ww),_(Lw),_(Bw),_(xw),_(Rw),c&&t(pxe),c&&t(hc),_(Sw),c&&t(_xe),c&&t(Fr),_(Pw),_(Iw),_(Dw),_(jw),_(Nw),c&&t(uxe),c&&t(uc),_(qw),c&&t(bxe),c&&t(Cr),_(Gw),_(Xw),_(Vw),_(zw),_(Ww),c&&t(vxe),c&&t(Tc),_(Qw),c&&t(Txe),c&&t(Mr),_(Hw),_(Jw),_(Yw),_(Kw),_(Zw),c&&t(Fxe),c&&t(Mc),_(e6),c&&t(Cxe),c&&t(Er),_(o6),_(t6),_(a6),_(n6),_(s6),c&&t(Mxe),c&&t(wc),_(l6),c&&t(Exe),c&&t(yr),_(i6),_(c6),_(f6),_(m6),_(g6),c&&t(yxe),c&&t(Bc),_(h6),c&&t(wxe),c&&t(wr),_(p6),_(u6),_(b6),_(v6),_(T6),c&&t(Axe),c&&t(Rc),_(F6),c&&t(Lxe),c&&t(Ar),_(C6),_(E6),_(y6),_(w6),_(A6),c&&t(Bxe),c&&t($c),_(L6),c&&t(xxe),c&&t(Lr),_(B6),_(k6),_(R6),_(S6),_(P6),c&&t(kxe),c&&t(jc),_($6),c&&t(Rxe),c&&t(Br),_(I6),_(j6),_(N6),_(q6),_(G6),c&&t(Sxe),c&&t(Gc),_(O6),c&&t(Pxe),c&&t(xr),_(X6),_(z6),_(W6),_(Q6),_(H6),c&&t($xe),c&&t(Vc),_(U6),c&&t(Ixe),c&&t(kr),_(J6),_(K6),_(Z6),_(eA),_(oA),c&&t(Dxe),c&&t(Qc),_(rA),c&&t(jxe),c&&t(Rr),_(tA),_(nA),_(sA),_(lA),_(iA),c&&t(Nxe),c&&t(Jc),_(dA),c&&t(qxe),c&&t(Sr),_(cA),_(mA),_(gA),_(hA),_(pA),c&&t(Gxe),c&&t(Zc),_(_A),c&&t(Oxe),c&&t(Pr),_(uA),_(vA),_(TA),_(FA),_(CA),c&&t(Xxe),c&&t(rf),_(MA),c&&t(Vxe),c&&t($r),_(EA),_(wA),_(AA),_(LA),_(BA),c&&t(zxe),c&&t(nf),_(xA),c&&t(Wxe),c&&t(Ir),_(kA),_(SA),_(PA),_($A),_(IA),c&&t(Qxe),c&&t(df),_(DA),c&&t(Hxe),c&&t(Dr),_(jA),_(qA),_(GA),_(OA),_(XA),c&&t(Uxe),c&&t(mf),_(VA),c&&t(Jxe),c&&t(jr),_(zA),_(QA),_(HA),_(UA),_(JA),c&&t(Yxe),c&&t(pf),_(YA),c&&t(Kxe),c&&t(Nr),_(KA),_(eL),_(oL),_(rL),_(tL),c&&t(Zxe),c&&t(bf),_(aL),c&&t(eke),c&&t(qr),_(nL),_(lL),_(iL),_(dL),_(fL),c&&t(oke),c&&t(Ff),_(mL),c&&t(rke),c&&t(Gr),_(gL),_(pL),_(_L),_(uL),_(bL)}}}const r2t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function t2t(Li,J,Pe){let{fw:ie}=J;return Li.$$set=ge=>{"fw"in ge&&Pe(0,ie=ge.fw)},[ie]}class c2t extends H5t{constructor(J){super();U5t(this,J,t2t,o2t,J5t,{fw:0})}}export{c2t as default,r2t as metadata};
