import{S as gf,i as _f,s as $f,e as t,k as u,w as f,t as p,M as vf,c as l,d as a,m as c,a as r,x as d,h as o,b as i,N as df,F as e,g as h,y as b,q as j,o as g,B as _,v as wf,L as bf}from"../chunks/vendor-6b77c823.js";import{T as kf}from"../chunks/Tip-39098574.js";import{Y as yf}from"../chunks/Youtube-5c6e11e6.js";import{I as x}from"../chunks/IconCopyLink-7a11ce68.js";import{C as v}from"../chunks/CodeBlock-3a8b25a8.js";import{D as xf}from"../chunks/DocNotebookDropdown-f2b55cd8.js";import{F as Ef,M as jf}from"../chunks/Markdown-9acf6d91.js";function qf(P){let $,w,m,k,y;return{c(){$=t("p"),w=p("If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the "),m=t("em"),k=p("vocab"),y=p(") during pretraining.")},l(E){$=l(E,"P",{});var T=r($);w=o(T,"If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the "),m=l(T,"EM",{});var hs=r(m);k=o(hs,"vocab"),hs.forEach(a),y=o(T,") during pretraining."),T.forEach(a)},m(E,T){h(E,$,T),e($,w),e($,m),e(m,k),e($,y)},d(E){E&&a($)}}}function Af(P){let $,w;return $=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,   <span class="hljs-number">153</span>,  <span class="hljs-number">7719</span>, <span class="hljs-number">21490</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">1114</span>,  <span class="hljs-number">9582</span>,  <span class="hljs-number">1623</span>,   <span class="hljs-number">102</span>],
                      [  <span class="hljs-number">101</span>,  <span class="hljs-number">5226</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">9649</span>,  <span class="hljs-number">1199</span>,  <span class="hljs-number">2610</span>,  <span class="hljs-number">1236</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])}`}}),{c(){f($.$$.fragment)},l(m){d($.$$.fragment,m)},m(m,k){b($,m,k),w=!0},p:bf,i(m){w||(j($.$$.fragment,m),w=!0)},o(m){g($.$$.fragment,m),w=!1},d(m){_($,m)}}}function Pf(P){let $,w;return $=new jf({props:{$$slots:{default:[Af]},$$scope:{ctx:P}}}),{c(){f($.$$.fragment)},l(m){d($.$$.fragment,m)},m(m,k){b($,m,k),w=!0},p(m,k){const y={};k&2&&(y.$$scope={dirty:k,ctx:m}),$.$set(y)},i(m){w||(j($.$$.fragment,m),w=!0)},o(m){g($.$$.fragment,m),w=!1},d(m){_($,m)}}}function zf(P){let $,w;return $=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,   <span class="hljs-number">153</span>,  <span class="hljs-number">7719</span>, <span class="hljs-number">21490</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">1114</span>,  <span class="hljs-number">9582</span>,  <span class="hljs-number">1623</span>,   <span class="hljs-number">102</span>],
       [  <span class="hljs-number">101</span>,  <span class="hljs-number">5226</span>,  <span class="hljs-number">1122</span>,  <span class="hljs-number">9649</span>,  <span class="hljs-number">1199</span>,  <span class="hljs-number">2610</span>,  <span class="hljs-number">1236</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>]],
      dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
       [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}`}}),{c(){f($.$$.fragment)},l(m){d($.$$.fragment,m)},m(m,k){b($,m,k),w=!0},p:bf,i(m){w||(j($.$$.fragment,m),w=!0)},o(m){g($.$$.fragment,m),w=!1},d(m){_($,m)}}}function Tf(P){let $,w;return $=new jf({props:{$$slots:{default:[zf]},$$scope:{ctx:P}}}),{c(){f($.$$.fragment)},l(m){d($.$$.fragment,m)},m(m,k){b($,m,k),w=!0},p(m,k){const y={};k&2&&(y.$$scope={dirty:k,ctx:m}),$.$set(y)},i(m){w||(j($.$$.fragment,m),w=!0)},o(m){g($.$$.fragment,m),w=!1},d(m){_($,m)}}}function Cf(P){let $,w,m,k,y,E,T,hs,vp,Ft,Js,Ht,oe,wp,Rt,C,mn,kp,yp,fn,xp,Ep,dn,qp,Bt,V,us,bn,Ms,Ap,jn,Pp,Wt,Us,Jt,D,zp,he,Tp,Cp,gn,Dp,Sp,Mt,cs,Ut,S,Lp,ue,Np,Op,_n,Ip,Fp,Vt,G,is,$n,Vs,Hp,vn,Rp,Gt,ms,Bp,ce,Wp,Jp,Yt,Gs,Kt,ie,Mp,Qt,Ys,Xt,me,Up,Zt,L,fe,de,Vp,Gp,Yp,be,je,Kp,Qp,Xp,ge,_e,Zp,so,sl,fs,ao,wn,eo,no,al,Ks,el,N,to,kn,lo,ro,yn,po,oo,nl,$e,ho,tl,Qs,ll,Y,ds,xn,Xs,uo,En,co,rl,bs,io,qn,mo,fo,pl,O,bo,An,jo,go,Pn,_o,$o,ol,Zs,hl,js,vo,zn,wo,ko,ul,K,gs,Tn,sa,yo,Cn,xo,cl,ve,Eo,il,I,qo,Dn,Ao,Po,Sn,zo,To,ml,aa,fl,Q,_s,Ln,ea,Co,Nn,Do,dl,we,So,bl,q,Lo,On,No,Oo,In,Io,Fo,Fn,Ho,Ro,jl,$s,gl,X,vs,Hn,na,Bo,Rn,Wo,_l,ws,Jo,ke,Mo,Uo,$l,ta,vl,F,Vo,la,Go,Yo,ra,Ko,Qo,wl,pa,kl,H,Xo,Bn,Zo,sh,Wn,ah,eh,yl,oa,xl,ye,nh,El,R,xe,Jn,th,lh,rh,Ee,Mn,ph,oh,hh,qe,Un,uh,ch,ql,Z,ks,Vn,ha,ih,Gn,mh,Al,ys,fh,ua,dh,bh,Pl,xs,jh,ca,gh,_h,zl,ia,Tl,Ae,ma,$h,fa,Yn,vh,wh,Cl,da,Dl,ba,Kn,kh,Sl,ja,Ll,Es,yh,Qn,xh,Eh,Nl,ss,qs,Xn,ga,qh,Zn,Ah,Ol,A,Ph,st,zh,Th,at,Ch,Dh,et,Sh,Lh,Il,As,Nh,Pe,Oh,Ih,Fl,_a,Hl,B,Fh,nt,Hh,Rh,tt,Bh,Wh,Rl,$a,Bl,as,Ps,lt,va,Jh,rt,Mh,Wl,ze,Uh,Jl,wa,Ml,Te,Vh,Ul,ka,Vl,Ce,Gh,Gl,ya,Yl,De,Yh,Kl,xa,Ql,Se,Kh,Xl,es,zs,pt,Ea,Qh,ot,Xh,Zl,Le,Zh,sr,W,su,qa,au,eu,ht,nu,tu,ar,Aa,er,Ts,lu,Pa,ut,ru,pu,nr,za,tr,Ne,Oe,Ac,lr,ns,Cs,ct,Ta,ou,it,hu,rr,Ds,uu,Ie,cu,iu,pr,Ca,or,ts,Ss,mt,Da,mu,ft,fu,hr,Ls,du,Sa,dt,bu,ju,ur,Fe,z,gu,La,bt,_u,$u,Na,jt,vu,wu,Oa,gt,ku,yu,cr,Ia,ir,Fa,ls,xu,He,_t,Eu,qu,$t,Au,Pu,mr,Ha,fr,Ra,Ba,zu,Wa,vt,Tu,Cu,dr,Ja,br,Ma,Ua,Du,wt,Su,Lu,jr,Va,gr,Re,Nu,_r,Ga,$r,Be,We,Pc,vr,rs,Ns,kt,Ya,Ou,yt,Iu,wr,Je,Fu,kr,Os,xt,Hu,Ru,Et,Bu,yr,Is,Wu,Ka,Ju,Mu,xr,Qa,Er,J,Uu,qt,Vu,Gu,At,Yu,Ku,qr,Xa,Ar,M,Qu,Pt,Xu,Zu,zt,sc,ac,Pr,Za,zr,Fs,ec,Me,nc,tc,Tr,se,Cr,ps,Hs,Tt,ae,lc,Ct,rc,Dr,Ue,pc,Sr,ee,Lr,Ve,os,oc,Dt,hc,uc,St,cc,ic,Nr,ne,Or,te,le,mc,Lt,fc,dc,Ir,re,Fr,U,bc,Nt,jc,gc,Ot,_c,$c,Hr,Ge,vc,Rr;return E=new x({}),Js=new xf({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/preprocessing.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/preprocessing.ipynb"}]}}),Ms=new x({}),Us=new yf({props:{id:"Yffk5aydLzg"}}),cs=new kf({props:{$$slots:{default:[qf]},$$scope:{ctx:P}}}),Vs=new x({}),Gs=new v({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),Ys=new v({props:{code:`encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2079</span>, <span class="hljs-number">2025</span>, <span class="hljs-number">19960</span>, <span class="hljs-number">10362</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">3821</span>, <span class="hljs-number">1997</span>, <span class="hljs-number">16657</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">2027</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">11259</span>, <span class="hljs-number">1998</span>, <span class="hljs-number">4248</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">4963</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ks=new v({props:{code:'tokenizer.decode(encoded_input["input_ids"])',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]&#x27;</span>`}}),Qs=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),Xs=new x({}),Zs=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),sa=new x({}),aa=new v({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),ea=new x({}),$s=new Ef({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Tf],pytorch:[Pf]},$$scope:{ctx:P}}}),na=new x({}),ta=new v({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),pa=new v({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),oa=new v({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),ha=new x({}),ia=new v({props:{code:`dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")
dataset[0]["audio"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),da=new v({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),ja=new v({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.3443763e-05</span>,  <span class="hljs-number">2.1729663e-04</span>,  <span class="hljs-number">2.2145823e-04</span>, ...,
         <span class="hljs-number">3.8356509e-05</span>, -<span class="hljs-number">7.3497440e-06</span>, -<span class="hljs-number">2.1754686e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),ga=new x({}),_a=new v({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),$a=new v({props:{code:`audio_input = [dataset[0]["audio"]["array"]]
feature_extractor(audio_input, sampling_rate=16000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>audio_input = [dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor(audio_input, sampling_rate=<span class="hljs-number">16000</span>)
{<span class="hljs-string">&#x27;input_values&#x27;</span>: [array([ <span class="hljs-number">3.8106556e-04</span>,  <span class="hljs-number">2.7506407e-03</span>,  <span class="hljs-number">2.8015103e-03</span>, ...,
        <span class="hljs-number">5.6335266e-04</span>,  <span class="hljs-number">4.6588284e-06</span>, -<span class="hljs-number">1.7142107e-04</span>], dtype=float32)]}`}}),va=new x({}),wa=new v({props:{code:`dataset[0]["audio"]["array"].shape

dataset[1]["audio"]["array"].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">173398</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">106496</span>,)`}}),ka=new v({props:{code:`def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays,
<span class="hljs-meta">... </span>        sampling_rate=<span class="hljs-number">16000</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">100000</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`}}),ya=new v({props:{code:"processed_dataset = preprocess_function(dataset[:5])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset = preprocess_function(dataset[:<span class="hljs-number">5</span>])'}}),xa=new v({props:{code:`processed_dataset["input_values"][0].shape

processed_dataset["input_values"][1].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>].shape
(<span class="hljs-number">100000</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">1</span>].shape
(<span class="hljs-number">100000</span>,)`}}),Ea=new x({}),Aa=new v({props:{code:`from datasets import load_dataset

dataset = load_dataset("food101", split="train[:100]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:100]&quot;</span>)`}}),za=new v({props:{code:'dataset[0]["image"]',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]'}}),Ta=new x({}),Ca=new v({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)`}}),Da=new x({}),Ia=new v({props:{code:`from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose(
    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose(
<span class="hljs-meta">... </span>    [RandomResizedCrop(feature_extractor.size), ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor(), normalize]
<span class="hljs-meta">... </span>)`}}),Ha=new v({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Ja=new v({props:{code:"dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(transforms)'}}),Va=new v({props:{code:'dataset[0]["image"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at <span class="hljs-number">0x7F1A7B0630D0</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">6</span>,
 <span class="hljs-string">&#x27;pixel_values&#x27;</span>: tensor([[[ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.1216</span>,  ..., -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>],
          [-<span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.1294</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9843</span>, -<span class="hljs-number">0.9922</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.1137</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9686</span>, -<span class="hljs-number">0.8667</span>],
          ...,
          [ <span class="hljs-number">0.0275</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.0510</span>,  ..., -<span class="hljs-number">0.1137</span>, -<span class="hljs-number">0.1216</span>, -<span class="hljs-number">0.0824</span>],
          [ <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.0667</span>,  ..., -<span class="hljs-number">0.0588</span>, -<span class="hljs-number">0.0745</span>, -<span class="hljs-number">0.0980</span>],
          [ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0431</span>,  ..., -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0588</span>]],
 
         [[ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.2863</span>,  ..., -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>],
          [ <span class="hljs-number">0.1608</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.3098</span>,  ..., -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>],
          [ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2706</span>,  <span class="hljs-number">0.3020</span>,  ..., -<span class="hljs-number">0.9608</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.8275</span>],
          ...,
          [-<span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.2392</span>, -<span class="hljs-number">0.2471</span>, -<span class="hljs-number">0.2078</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0196</span>,  ..., -<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.2000</span>, -<span class="hljs-number">0.2235</span>],
          [-<span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.1529</span>]],
 
         [[ <span class="hljs-number">0.3961</span>,  <span class="hljs-number">0.4431</span>,  <span class="hljs-number">0.4980</span>,  ..., -<span class="hljs-number">0.9216</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9216</span>],
          [ <span class="hljs-number">0.3569</span>,  <span class="hljs-number">0.4510</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9059</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9137</span>],
          [ <span class="hljs-number">0.4118</span>,  <span class="hljs-number">0.4745</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.8902</span>, -<span class="hljs-number">0.7804</span>],
          ...,
          [-<span class="hljs-number">0.2314</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.2078</span>,  ..., -<span class="hljs-number">0.4196</span>, -<span class="hljs-number">0.4275</span>, -<span class="hljs-number">0.3882</span>],
          [-<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.1686</span>, -<span class="hljs-number">0.2000</span>,  ..., -<span class="hljs-number">0.3647</span>, -<span class="hljs-number">0.3804</span>, -<span class="hljs-number">0.4039</span>],
          [-<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>,  ..., -<span class="hljs-number">0.2941</span>, -<span class="hljs-number">0.2863</span>, -<span class="hljs-number">0.3412</span>]]])}`}}),Ga=new v({props:{code:`import numpy as np
import matplotlib.pyplot as plt

img = dataset[0]["pixel_values"]
plt.imshow(img.permute(1, 2, 0))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>img = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;pixel_values&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))`}}),Ya=new x({}),Qa=new v({props:{code:`from datasets import load_dataset

lj_speech = load_dataset("lj_speech", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = load_dataset(<span class="hljs-string">&quot;lj_speech&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Xa=new v({props:{code:'lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.<span class="hljs-built_in">map</span>(remove_columns=[<span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;normalized_text&quot;</span>])'}}),Za=new v({props:{code:`lj_speech[0]["audio"]

lj_speech[0]["text"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">7.3242188e-04</span>, -<span class="hljs-number">7.6293945e-04</span>, -<span class="hljs-number">6.4086914e-04</span>, ...,
         <span class="hljs-number">7.3242188e-04</span>,  <span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">22050</span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>]
<span class="hljs-string">&#x27;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition&#x27;</span>`}}),se=new v({props:{code:'lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),ae=new x({}),ee=new v({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),ne=new v({props:{code:`def prepare_dataset(example):
    audio = example["audio"]

    example["input_values"] = processor(audio["array"], sampling_rate=16000)

    with processor.as_target_processor():
        example["labels"] = processor(example["text"]).input_ids
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    audio = example[<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-meta">... </span>    example[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        example[<span class="hljs-string">&quot;labels&quot;</span>] = processor(example[<span class="hljs-string">&quot;text&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),re=new v({props:{code:"prepare_dataset(lj_speech[0])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>prepare_dataset(lj_speech[<span class="hljs-number">0</span>])'}}),{c(){$=t("meta"),w=u(),m=t("h1"),k=t("a"),y=t("span"),f(E.$$.fragment),T=u(),hs=t("span"),vp=p("Preprocess"),Ft=u(),f(Js.$$.fragment),Ht=u(),oe=t("p"),wp=p("Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:"),Rt=u(),C=t("ul"),mn=t("li"),kp=p("Preprocess textual data with a tokenizer."),yp=u(),fn=t("li"),xp=p("Preprocess image or audio data with a feature extractor."),Ep=u(),dn=t("li"),qp=p("Preprocess data for a multimodal task with a processor."),Bt=u(),V=t("h2"),us=t("a"),bn=t("span"),f(Ms.$$.fragment),Ap=u(),jn=t("span"),Pp=p("NLP"),Wt=u(),f(Us.$$.fragment),Jt=u(),D=t("p"),zp=p("The main tool for processing textual data is a "),he=t("a"),Tp=p("tokenizer"),Cp=p(". A tokenizer starts by splitting text into "),gn=t("em"),Dp=p("tokens"),Sp=p(" according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer."),Mt=u(),f(cs.$$.fragment),Ut=u(),S=t("p"),Lp=p("Get started quickly by loading a pretrained tokenizer with the "),ue=t("a"),Np=p("AutoTokenizer"),Op=p(" class. This downloads the "),_n=t("em"),Ip=p("vocab"),Fp=p(" used when a model is pretrained."),Vt=u(),G=t("h3"),is=t("a"),$n=t("span"),f(Vs.$$.fragment),Hp=u(),vn=t("span"),Rp=p("Tokenize"),Gt=u(),ms=t("p"),Bp=p("Load a pretrained tokenizer with "),ce=t("a"),Wp=p("AutoTokenizer.from_pretrained()"),Jp=p(":"),Yt=u(),f(Gs.$$.fragment),Kt=u(),ie=t("p"),Mp=p("Then pass your sentence to the tokenizer:"),Qt=u(),f(Ys.$$.fragment),Xt=u(),me=t("p"),Up=p("The tokenizer returns a dictionary with three important itmes:"),Zt=u(),L=t("ul"),fe=t("li"),de=t("a"),Vp=p("input_ids"),Gp=p(" are the indices corresponding to each token in the sentence."),Yp=u(),be=t("li"),je=t("a"),Kp=p("attention_mask"),Qp=p(" indicates whether a token should be attended to or not."),Xp=u(),ge=t("li"),_e=t("a"),Zp=p("token_type_ids"),so=p(" identifies which sequence a token belongs to when there is more than one sequence."),sl=u(),fs=t("p"),ao=p("You can decode the "),wn=t("code"),eo=p("input_ids"),no=p(" to return the original input:"),al=u(),f(Ks.$$.fragment),el=u(),N=t("p"),to=p("As you can see, the tokenizer added two special tokens - "),kn=t("code"),lo=p("CLS"),ro=p(" and "),yn=t("code"),po=p("SEP"),oo=p(` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer will automatically add them for you.`),nl=u(),$e=t("p"),ho=p("If there are several sentences you want to process, pass the sentences as a list to the tokenizer:"),tl=u(),f(Qs.$$.fragment),ll=u(),Y=t("h3"),ds=t("a"),xn=t("span"),f(Xs.$$.fragment),uo=u(),En=t("span"),co=p("Pad"),rl=u(),bs=t("p"),io=p("This brings us to an important topic. When you process a batch of sentences, they aren\u2019t always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special "),qn=t("em"),mo=p("padding token"),fo=p(" to sentences with fewer tokens."),pl=u(),O=t("p"),bo=p("Set the "),An=t("code"),jo=p("padding"),go=p(" parameter to "),Pn=t("code"),_o=p("True"),$o=p(" to pad the shorter sequences in the batch to match the longest sequence:"),ol=u(),f(Zs.$$.fragment),hl=u(),js=t("p"),vo=p("Notice the tokenizer padded the first and third sentences with a "),zn=t("code"),wo=p("0"),ko=p(" because they are shorter!"),ul=u(),K=t("h3"),gs=t("a"),Tn=t("span"),f(sa.$$.fragment),yo=u(),Cn=t("span"),xo=p("Truncation"),cl=u(),ve=t("p"),Eo=p("On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length."),il=u(),I=t("p"),qo=p("Set the "),Dn=t("code"),Ao=p("truncation"),Po=p(" parameter to "),Sn=t("code"),zo=p("True"),To=p(" to truncate a sequence to the maximum length accepted by the model:"),ml=u(),f(aa.$$.fragment),fl=u(),Q=t("h3"),_s=t("a"),Ln=t("span"),f(ea.$$.fragment),Co=u(),Nn=t("span"),Do=p("Build tensors"),dl=u(),we=t("p"),So=p("Finally, you want the tokenizer to return the actual tensors that are fed to the model."),bl=u(),q=t("p"),Lo=p("Set the "),On=t("code"),No=p("return_tensors"),Oo=p(" parameter to either "),In=t("code"),Io=p("pt"),Fo=p(" for PyTorch, or "),Fn=t("code"),Ho=p("tf"),Ro=p(" for TensorFlow:"),jl=u(),f($s.$$.fragment),gl=u(),X=t("h2"),vs=t("a"),Hn=t("span"),f(na.$$.fragment),Bo=u(),Rn=t("span"),Wo=p("Audio"),_l=u(),ws=t("p"),Jo=p("Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A "),ke=t("a"),Mo=p("feature extractor"),Uo=p(" is designed for the express purpose of extracting features from raw image or audio data and converting them into tensors. Before you begin, install \u{1F917} Datasets to load an audio dataset to experiment with:"),$l=u(),f(ta.$$.fragment),vl=u(),F=t("p"),Vo=p("Load the "),la=t("a"),Go=p("MInDS-14"),Yo=p(" dataset (see the \u{1F917} "),ra=t("a"),Ko=p("Datasets tutorial"),Qo=p(" for more details on how to load a dataset):"),wl=u(),f(pa.$$.fragment),kl=u(),H=t("p"),Xo=p("Access the first element of the "),Bn=t("code"),Zo=p("audio"),sh=p(" column to take a look at the input. Calling the "),Wn=t("code"),ah=p("audio"),eh=p(" column will automatically load and resample the audio file:"),yl=u(),f(oa.$$.fragment),xl=u(),ye=t("p"),nh=p("This returns three items:"),El=u(),R=t("ul"),xe=t("li"),Jn=t("code"),th=p("array"),lh=p(" is the speech signal loaded - and potentially resampled - as a 1D array."),rh=u(),Ee=t("li"),Mn=t("code"),ph=p("path"),oh=p(" points to the location of the audio file."),hh=u(),qe=t("li"),Un=t("code"),uh=p("sampling_rate"),ch=p(" refers to how many data points in the speech signal are measured per second."),ql=u(),Z=t("h3"),ks=t("a"),Vn=t("span"),f(ha.$$.fragment),ih=u(),Gn=t("span"),mh=p("Resample"),Al=u(),ys=t("p"),fh=p("For this tutorial, you will use the "),ua=t("a"),dh=p("Wav2Vec2"),bh=p(" model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your audio data."),Pl=u(),xs=t("p"),jh=p("For example, the "),ca=t("a"),gh=p("MInDS-14"),_h=p(" dataset has a sampling rate of 8000kHz. In order to use the Wav2Vec2 model with this dataset, upsample the sampling rate to 16kHz:"),zl=u(),f(ia.$$.fragment),Tl=u(),Ae=t("ol"),ma=t("li"),$h=p("Use \u{1F917} Datasets\u2019 "),fa=t("a"),Yn=t("code"),vh=p("cast_column"),wh=p(" method to upsample the sampling rate to 16kHz:"),Cl=u(),f(da.$$.fragment),Dl=u(),ba=t("ol"),Kn=t("li"),kh=p("Load the audio file:"),Sl=u(),f(ja.$$.fragment),Ll=u(),Es=t("p"),yh=p("As you can see, the "),Qn=t("code"),xh=p("sampling_rate"),Eh=p(" is now 16kHz!"),Nl=u(),ss=t("h3"),qs=t("a"),Xn=t("span"),f(ga.$$.fragment),qh=u(),Zn=t("span"),Ah=p("Feature extractor"),Ol=u(),A=t("p"),Ph=p("The next step is to load a feature extractor to normalize and pad the input. When padding textual data, a "),st=t("code"),zh=p("0"),Th=p(" is added for shorter sequences. The same idea applies to audio data, and the audio feature extractor will add a "),at=t("code"),Ch=p("0"),Dh=p(" - interpreted as silence - to "),et=t("code"),Sh=p("array"),Lh=p("."),Il=u(),As=t("p"),Nh=p("Load the feature extractor with "),Pe=t("a"),Oh=p("AutoFeatureExtractor.from_pretrained()"),Ih=p(":"),Fl=u(),f(_a.$$.fragment),Hl=u(),B=t("p"),Fh=p("Pass the audio "),nt=t("code"),Hh=p("array"),Rh=p(" to the feature extractor. We also recommend adding the "),tt=t("code"),Bh=p("sampling_rate"),Wh=p(" argument in the feature extractor in order to better debug any silent errors that may occur."),Rl=u(),f($a.$$.fragment),Bl=u(),as=t("h3"),Ps=t("a"),lt=t("span"),f(va.$$.fragment),Jh=u(),rt=t("span"),Mh=p("Pad and truncate"),Wl=u(),ze=t("p"),Uh=p("Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),Jl=u(),f(wa.$$.fragment),Ml=u(),Te=t("p"),Vh=p("As you can see, the first sample has a longer sequence than the second sample. Let\u2019s create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),Ul=u(),f(ka.$$.fragment),Vl=u(),Ce=t("p"),Gh=p("Apply the function to the the first few examples in the dataset:"),Gl=u(),f(ya.$$.fragment),Yl=u(),De=t("p"),Yh=p("Now take another look at the processed sample lengths:"),Kl=u(),f(xa.$$.fragment),Ql=u(),Se=t("p"),Kh=p("The lengths of the first two samples now match the maximum length you specified."),Xl=u(),es=t("h2"),zs=t("a"),pt=t("span"),f(Ea.$$.fragment),Qh=u(),ot=t("span"),Xh=p("Vision"),Zl=u(),Le=t("p"),Zh=p("A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input."),sr=u(),W=t("p"),su=p("Let\u2019s load the "),qa=t("a"),au=p("food101"),eu=p(" dataset for this tutorial. Use \u{1F917} Datasets "),ht=t("code"),nu=p("split"),tu=p(" parameter to only load a small sample from the training split since the dataset is quite large:"),ar=u(),f(Aa.$$.fragment),er=u(),Ts=t("p"),lu=p("Next, take a look at the image with \u{1F917} Datasets "),Pa=t("a"),ut=t("code"),ru=p("Image"),pu=p(" feature:"),nr=u(),f(za.$$.fragment),tr=u(),Ne=t("p"),Oe=t("img"),lr=u(),ns=t("h3"),Cs=t("a"),ct=t("span"),f(Ta.$$.fragment),ou=u(),it=t("span"),hu=p("Feature extractor"),rr=u(),Ds=t("p"),uu=p("Load the feature extractor with "),Ie=t("a"),cu=p("AutoFeatureExtractor.from_pretrained()"),iu=p(":"),pr=u(),f(Ca.$$.fragment),or=u(),ts=t("h3"),Ss=t("a"),mt=t("span"),f(Da.$$.fragment),mu=u(),ft=t("span"),fu=p("Data augmentation"),hr=u(),Ls=t("p"),du=p("For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you will use torchvision\u2019s "),Sa=t("a"),dt=t("code"),bu=p("transforms"),ju=p(" module."),ur=u(),Fe=t("ol"),z=t("li"),gu=p("Normalize the image and use "),La=t("a"),bt=t("code"),_u=p("Compose"),$u=p(" to chain some transforms - "),Na=t("a"),jt=t("code"),vu=p("RandomResizedCrop"),wu=p(" and "),Oa=t("a"),gt=t("code"),ku=p("ColorJitter"),yu=p(" - together:"),cr=u(),f(Ia.$$.fragment),ir=u(),Fa=t("ol"),ls=t("li"),xu=p("The model accepts "),He=t("a"),_t=t("code"),Eu=p("pixel_values"),qu=p(" as it\u2019s input. This value is generated by the feature extractor. Create a function that generates "),$t=t("code"),Au=p("pixel_values"),Pu=p(" from the transforms:"),mr=u(),f(Ha.$$.fragment),fr=u(),Ra=t("ol"),Ba=t("li"),zu=p("Then use \u{1F917} Datasets "),Wa=t("a"),vt=t("code"),Tu=p("set_transform"),Cu=p(" to apply the transforms on-the-fly:"),dr=u(),f(Ja.$$.fragment),br=u(),Ma=t("ol"),Ua=t("li"),Du=p("Now when you access the image, you will notice the feature extractor has added the model input "),wt=t("code"),Su=p("pixel_values"),Lu=p(":"),jr=u(),f(Va.$$.fragment),gr=u(),Re=t("p"),Nu=p("Here is what the image looks like after you preprocess it. Just as you\u2019d expect from the applied transforms, the image has been randomly cropped and it\u2019s color properties are different."),_r=u(),f(Ga.$$.fragment),$r=u(),Be=t("p"),We=t("img"),vr=u(),rs=t("h2"),Ns=t("a"),kt=t("span"),f(Ya.$$.fragment),Ou=u(),yt=t("span"),Iu=p("Multimodal"),wr=u(),Je=t("p"),Fu=p("For multimodal tasks. you will use a combination of everything you\u2019ve learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:"),kr=u(),Os=t("ul"),xt=t("li"),Hu=p("Feature extractor to preprocess the audio data."),Ru=u(),Et=t("li"),Bu=p("Tokenizer to process the text."),yr=u(),Is=t("p"),Wu=p("Let\u2019s return to the "),Ka=t("a"),Ju=p("LJ Speech"),Mu=p(" dataset:"),xr=u(),f(Qa.$$.fragment),Er=u(),J=t("p"),Uu=p("Since you are mainly interested in the "),qt=t("code"),Vu=p("audio"),Gu=p(" and "),At=t("code"),Yu=p("text"),Ku=p(" column, remove the other columns:"),qr=u(),f(Xa.$$.fragment),Ar=u(),M=t("p"),Qu=p("Now take a look at the "),Pt=t("code"),Xu=p("audio"),Zu=p(" and "),zt=t("code"),sc=p("text"),ac=p(" columns:"),Pr=u(),f(Za.$$.fragment),zr=u(),Fs=t("p"),ec=p("Remember from the earlier section on processing audio data, you should always "),Me=t("a"),nc=p("resample"),tc=p(" your audio data\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model:"),Tr=u(),f(se.$$.fragment),Cr=u(),ps=t("h3"),Hs=t("a"),Tt=t("span"),f(ae.$$.fragment),lc=u(),Ct=t("span"),rc=p("Processor"),Dr=u(),Ue=t("p"),pc=p("A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"),Sr=u(),f(ee.$$.fragment),Lr=u(),Ve=t("ol"),os=t("li"),oc=p("Create a function to process the audio data to "),Dt=t("code"),hc=p("input_values"),uc=p(", and tokenizes the text to "),St=t("code"),cc=p("labels"),ic=p(". These are your inputs to the model:"),Nr=u(),f(ne.$$.fragment),Or=u(),te=t("ol"),le=t("li"),mc=p("Apply the "),Lt=t("code"),fc=p("prepare_dataset"),dc=p(" function to a sample:"),Ir=u(),f(re.$$.fragment),Fr=u(),U=t("p"),bc=p("Notice the processor has added "),Nt=t("code"),jc=p("input_values"),gc=p(" and "),Ot=t("code"),_c=p("labels"),$c=p(". The sampling rate has also been correctly downsampled to 16kHz."),Hr=u(),Ge=t("p"),vc=p("Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."),this.h()},l(s){const n=vf('[data-svelte="svelte-1phssyn"]',document.head);$=l(n,"META",{name:!0,content:!0}),n.forEach(a),w=c(s),m=l(s,"H1",{class:!0});var pe=r(m);k=l(pe,"A",{id:!0,class:!0,href:!0});var It=r(k);y=l(It,"SPAN",{});var zc=r(y);d(E.$$.fragment,zc),zc.forEach(a),It.forEach(a),T=c(pe),hs=l(pe,"SPAN",{});var Tc=r(hs);vp=o(Tc,"Preprocess"),Tc.forEach(a),pe.forEach(a),Ft=c(s),d(Js.$$.fragment,s),Ht=c(s),oe=l(s,"P",{});var Cc=r(oe);wp=o(Cc,"Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors. In this tutorial, you will:"),Cc.forEach(a),Rt=c(s),C=l(s,"UL",{});var Ye=r(C);mn=l(Ye,"LI",{});var Dc=r(mn);kp=o(Dc,"Preprocess textual data with a tokenizer."),Dc.forEach(a),yp=c(Ye),fn=l(Ye,"LI",{});var Sc=r(fn);xp=o(Sc,"Preprocess image or audio data with a feature extractor."),Sc.forEach(a),Ep=c(Ye),dn=l(Ye,"LI",{});var Lc=r(dn);qp=o(Lc,"Preprocess data for a multimodal task with a processor."),Lc.forEach(a),Ye.forEach(a),Bt=c(s),V=l(s,"H2",{class:!0});var Br=r(V);us=l(Br,"A",{id:!0,class:!0,href:!0});var Nc=r(us);bn=l(Nc,"SPAN",{});var Oc=r(bn);d(Ms.$$.fragment,Oc),Oc.forEach(a),Nc.forEach(a),Ap=c(Br),jn=l(Br,"SPAN",{});var Ic=r(jn);Pp=o(Ic,"NLP"),Ic.forEach(a),Br.forEach(a),Wt=c(s),d(Us.$$.fragment,s),Jt=c(s),D=l(s,"P",{});var Ke=r(D);zp=o(Ke,"The main tool for processing textual data is a "),he=l(Ke,"A",{href:!0});var Fc=r(he);Tp=o(Fc,"tokenizer"),Fc.forEach(a),Cp=o(Ke,". A tokenizer starts by splitting text into "),gn=l(Ke,"EM",{});var Hc=r(gn);Dp=o(Hc,"tokens"),Hc.forEach(a),Sp=o(Ke," according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer."),Ke.forEach(a),Mt=c(s),d(cs.$$.fragment,s),Ut=c(s),S=l(s,"P",{});var Qe=r(S);Lp=o(Qe,"Get started quickly by loading a pretrained tokenizer with the "),ue=l(Qe,"A",{href:!0});var Rc=r(ue);Np=o(Rc,"AutoTokenizer"),Rc.forEach(a),Op=o(Qe," class. This downloads the "),_n=l(Qe,"EM",{});var Bc=r(_n);Ip=o(Bc,"vocab"),Bc.forEach(a),Fp=o(Qe," used when a model is pretrained."),Qe.forEach(a),Vt=c(s),G=l(s,"H3",{class:!0});var Wr=r(G);is=l(Wr,"A",{id:!0,class:!0,href:!0});var Wc=r(is);$n=l(Wc,"SPAN",{});var Jc=r($n);d(Vs.$$.fragment,Jc),Jc.forEach(a),Wc.forEach(a),Hp=c(Wr),vn=l(Wr,"SPAN",{});var Mc=r(vn);Rp=o(Mc,"Tokenize"),Mc.forEach(a),Wr.forEach(a),Gt=c(s),ms=l(s,"P",{});var Jr=r(ms);Bp=o(Jr,"Load a pretrained tokenizer with "),ce=l(Jr,"A",{href:!0});var Uc=r(ce);Wp=o(Uc,"AutoTokenizer.from_pretrained()"),Uc.forEach(a),Jp=o(Jr,":"),Jr.forEach(a),Yt=c(s),d(Gs.$$.fragment,s),Kt=c(s),ie=l(s,"P",{});var Vc=r(ie);Mp=o(Vc,"Then pass your sentence to the tokenizer:"),Vc.forEach(a),Qt=c(s),d(Ys.$$.fragment,s),Xt=c(s),me=l(s,"P",{});var Gc=r(me);Up=o(Gc,"The tokenizer returns a dictionary with three important itmes:"),Gc.forEach(a),Zt=c(s),L=l(s,"UL",{});var Xe=r(L);fe=l(Xe,"LI",{});var wc=r(fe);de=l(wc,"A",{href:!0});var Yc=r(de);Vp=o(Yc,"input_ids"),Yc.forEach(a),Gp=o(wc," are the indices corresponding to each token in the sentence."),wc.forEach(a),Yp=c(Xe),be=l(Xe,"LI",{});var kc=r(be);je=l(kc,"A",{href:!0});var Kc=r(je);Kp=o(Kc,"attention_mask"),Kc.forEach(a),Qp=o(kc," indicates whether a token should be attended to or not."),kc.forEach(a),Xp=c(Xe),ge=l(Xe,"LI",{});var yc=r(ge);_e=l(yc,"A",{href:!0});var Qc=r(_e);Zp=o(Qc,"token_type_ids"),Qc.forEach(a),so=o(yc," identifies which sequence a token belongs to when there is more than one sequence."),yc.forEach(a),Xe.forEach(a),sl=c(s),fs=l(s,"P",{});var Mr=r(fs);ao=o(Mr,"You can decode the "),wn=l(Mr,"CODE",{});var Xc=r(wn);eo=o(Xc,"input_ids"),Xc.forEach(a),no=o(Mr," to return the original input:"),Mr.forEach(a),al=c(s),d(Ks.$$.fragment,s),el=c(s),N=l(s,"P",{});var Ze=r(N);to=o(Ze,"As you can see, the tokenizer added two special tokens - "),kn=l(Ze,"CODE",{});var Zc=r(kn);lo=o(Zc,"CLS"),Zc.forEach(a),ro=o(Ze," and "),yn=l(Ze,"CODE",{});var si=r(yn);po=o(si,"SEP"),si.forEach(a),oo=o(Ze,` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer will automatically add them for you.`),Ze.forEach(a),nl=c(s),$e=l(s,"P",{});var ai=r($e);ho=o(ai,"If there are several sentences you want to process, pass the sentences as a list to the tokenizer:"),ai.forEach(a),tl=c(s),d(Qs.$$.fragment,s),ll=c(s),Y=l(s,"H3",{class:!0});var Ur=r(Y);ds=l(Ur,"A",{id:!0,class:!0,href:!0});var ei=r(ds);xn=l(ei,"SPAN",{});var ni=r(xn);d(Xs.$$.fragment,ni),ni.forEach(a),ei.forEach(a),uo=c(Ur),En=l(Ur,"SPAN",{});var ti=r(En);co=o(ti,"Pad"),ti.forEach(a),Ur.forEach(a),rl=c(s),bs=l(s,"P",{});var Vr=r(bs);io=o(Vr,"This brings us to an important topic. When you process a batch of sentences, they aren\u2019t always the same length. This is a problem because tensors, the input to the model, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special "),qn=l(Vr,"EM",{});var li=r(qn);mo=o(li,"padding token"),li.forEach(a),fo=o(Vr," to sentences with fewer tokens."),Vr.forEach(a),pl=c(s),O=l(s,"P",{});var sn=r(O);bo=o(sn,"Set the "),An=l(sn,"CODE",{});var ri=r(An);jo=o(ri,"padding"),ri.forEach(a),go=o(sn," parameter to "),Pn=l(sn,"CODE",{});var pi=r(Pn);_o=o(pi,"True"),pi.forEach(a),$o=o(sn," to pad the shorter sequences in the batch to match the longest sequence:"),sn.forEach(a),ol=c(s),d(Zs.$$.fragment,s),hl=c(s),js=l(s,"P",{});var Gr=r(js);vo=o(Gr,"Notice the tokenizer padded the first and third sentences with a "),zn=l(Gr,"CODE",{});var oi=r(zn);wo=o(oi,"0"),oi.forEach(a),ko=o(Gr," because they are shorter!"),Gr.forEach(a),ul=c(s),K=l(s,"H3",{class:!0});var Yr=r(K);gs=l(Yr,"A",{id:!0,class:!0,href:!0});var hi=r(gs);Tn=l(hi,"SPAN",{});var ui=r(Tn);d(sa.$$.fragment,ui),ui.forEach(a),hi.forEach(a),yo=c(Yr),Cn=l(Yr,"SPAN",{});var ci=r(Cn);xo=o(ci,"Truncation"),ci.forEach(a),Yr.forEach(a),cl=c(s),ve=l(s,"P",{});var ii=r(ve);Eo=o(ii,"On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you will need to truncate the sequence to a shorter length."),ii.forEach(a),il=c(s),I=l(s,"P",{});var an=r(I);qo=o(an,"Set the "),Dn=l(an,"CODE",{});var mi=r(Dn);Ao=o(mi,"truncation"),mi.forEach(a),Po=o(an," parameter to "),Sn=l(an,"CODE",{});var fi=r(Sn);zo=o(fi,"True"),fi.forEach(a),To=o(an," to truncate a sequence to the maximum length accepted by the model:"),an.forEach(a),ml=c(s),d(aa.$$.fragment,s),fl=c(s),Q=l(s,"H3",{class:!0});var Kr=r(Q);_s=l(Kr,"A",{id:!0,class:!0,href:!0});var di=r(_s);Ln=l(di,"SPAN",{});var bi=r(Ln);d(ea.$$.fragment,bi),bi.forEach(a),di.forEach(a),Co=c(Kr),Nn=l(Kr,"SPAN",{});var ji=r(Nn);Do=o(ji,"Build tensors"),ji.forEach(a),Kr.forEach(a),dl=c(s),we=l(s,"P",{});var gi=r(we);So=o(gi,"Finally, you want the tokenizer to return the actual tensors that are fed to the model."),gi.forEach(a),bl=c(s),q=l(s,"P",{});var Rs=r(q);Lo=o(Rs,"Set the "),On=l(Rs,"CODE",{});var _i=r(On);No=o(_i,"return_tensors"),_i.forEach(a),Oo=o(Rs," parameter to either "),In=l(Rs,"CODE",{});var $i=r(In);Io=o($i,"pt"),$i.forEach(a),Fo=o(Rs," for PyTorch, or "),Fn=l(Rs,"CODE",{});var vi=r(Fn);Ho=o(vi,"tf"),vi.forEach(a),Ro=o(Rs," for TensorFlow:"),Rs.forEach(a),jl=c(s),d($s.$$.fragment,s),gl=c(s),X=l(s,"H2",{class:!0});var Qr=r(X);vs=l(Qr,"A",{id:!0,class:!0,href:!0});var wi=r(vs);Hn=l(wi,"SPAN",{});var ki=r(Hn);d(na.$$.fragment,ki),ki.forEach(a),wi.forEach(a),Bo=c(Qr),Rn=l(Qr,"SPAN",{});var yi=r(Rn);Wo=o(yi,"Audio"),yi.forEach(a),Qr.forEach(a),_l=c(s),ws=l(s,"P",{});var Xr=r(ws);Jo=o(Xr,"Audio inputs are preprocessed differently than textual inputs, but the end goal remains the same: create numerical sequences the model can understand. A "),ke=l(Xr,"A",{href:!0});var xi=r(ke);Mo=o(xi,"feature extractor"),xi.forEach(a),Uo=o(Xr," is designed for the express purpose of extracting features from raw image or audio data and converting them into tensors. Before you begin, install \u{1F917} Datasets to load an audio dataset to experiment with:"),Xr.forEach(a),$l=c(s),d(ta.$$.fragment,s),vl=c(s),F=l(s,"P",{});var en=r(F);Vo=o(en,"Load the "),la=l(en,"A",{href:!0,rel:!0});var Ei=r(la);Go=o(Ei,"MInDS-14"),Ei.forEach(a),Yo=o(en," dataset (see the \u{1F917} "),ra=l(en,"A",{href:!0,rel:!0});var qi=r(ra);Ko=o(qi,"Datasets tutorial"),qi.forEach(a),Qo=o(en," for more details on how to load a dataset):"),en.forEach(a),wl=c(s),d(pa.$$.fragment,s),kl=c(s),H=l(s,"P",{});var nn=r(H);Xo=o(nn,"Access the first element of the "),Bn=l(nn,"CODE",{});var Ai=r(Bn);Zo=o(Ai,"audio"),Ai.forEach(a),sh=o(nn," column to take a look at the input. Calling the "),Wn=l(nn,"CODE",{});var Pi=r(Wn);ah=o(Pi,"audio"),Pi.forEach(a),eh=o(nn," column will automatically load and resample the audio file:"),nn.forEach(a),yl=c(s),d(oa.$$.fragment,s),xl=c(s),ye=l(s,"P",{});var zi=r(ye);nh=o(zi,"This returns three items:"),zi.forEach(a),El=c(s),R=l(s,"UL",{});var tn=r(R);xe=l(tn,"LI",{});var xc=r(xe);Jn=l(xc,"CODE",{});var Ti=r(Jn);th=o(Ti,"array"),Ti.forEach(a),lh=o(xc," is the speech signal loaded - and potentially resampled - as a 1D array."),xc.forEach(a),rh=c(tn),Ee=l(tn,"LI",{});var Ec=r(Ee);Mn=l(Ec,"CODE",{});var Ci=r(Mn);ph=o(Ci,"path"),Ci.forEach(a),oh=o(Ec," points to the location of the audio file."),Ec.forEach(a),hh=c(tn),qe=l(tn,"LI",{});var qc=r(qe);Un=l(qc,"CODE",{});var Di=r(Un);uh=o(Di,"sampling_rate"),Di.forEach(a),ch=o(qc," refers to how many data points in the speech signal are measured per second."),qc.forEach(a),tn.forEach(a),ql=c(s),Z=l(s,"H3",{class:!0});var Zr=r(Z);ks=l(Zr,"A",{id:!0,class:!0,href:!0});var Si=r(ks);Vn=l(Si,"SPAN",{});var Li=r(Vn);d(ha.$$.fragment,Li),Li.forEach(a),Si.forEach(a),ih=c(Zr),Gn=l(Zr,"SPAN",{});var Ni=r(Gn);mh=o(Ni,"Resample"),Ni.forEach(a),Zr.forEach(a),Al=c(s),ys=l(s,"P",{});var sp=r(ys);fh=o(sp,"For this tutorial, you will use the "),ua=l(sp,"A",{href:!0,rel:!0});var Oi=r(ua);dh=o(Oi,"Wav2Vec2"),Oi.forEach(a),bh=o(sp," model. As you can see from the model card, the Wav2Vec2 model is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your audio data."),sp.forEach(a),Pl=c(s),xs=l(s,"P",{});var ap=r(xs);jh=o(ap,"For example, the "),ca=l(ap,"A",{href:!0,rel:!0});var Ii=r(ca);gh=o(Ii,"MInDS-14"),Ii.forEach(a),_h=o(ap," dataset has a sampling rate of 8000kHz. In order to use the Wav2Vec2 model with this dataset, upsample the sampling rate to 16kHz:"),ap.forEach(a),zl=c(s),d(ia.$$.fragment,s),Tl=c(s),Ae=l(s,"OL",{});var Fi=r(Ae);ma=l(Fi,"LI",{});var ep=r(ma);$h=o(ep,"Use \u{1F917} Datasets\u2019 "),fa=l(ep,"A",{href:!0,rel:!0});var Hi=r(fa);Yn=l(Hi,"CODE",{});var Ri=r(Yn);vh=o(Ri,"cast_column"),Ri.forEach(a),Hi.forEach(a),wh=o(ep," method to upsample the sampling rate to 16kHz:"),ep.forEach(a),Fi.forEach(a),Cl=c(s),d(da.$$.fragment,s),Dl=c(s),ba=l(s,"OL",{start:!0});var Bi=r(ba);Kn=l(Bi,"LI",{});var Wi=r(Kn);kh=o(Wi,"Load the audio file:"),Wi.forEach(a),Bi.forEach(a),Sl=c(s),d(ja.$$.fragment,s),Ll=c(s),Es=l(s,"P",{});var np=r(Es);yh=o(np,"As you can see, the "),Qn=l(np,"CODE",{});var Ji=r(Qn);xh=o(Ji,"sampling_rate"),Ji.forEach(a),Eh=o(np," is now 16kHz!"),np.forEach(a),Nl=c(s),ss=l(s,"H3",{class:!0});var tp=r(ss);qs=l(tp,"A",{id:!0,class:!0,href:!0});var Mi=r(qs);Xn=l(Mi,"SPAN",{});var Ui=r(Xn);d(ga.$$.fragment,Ui),Ui.forEach(a),Mi.forEach(a),qh=c(tp),Zn=l(tp,"SPAN",{});var Vi=r(Zn);Ah=o(Vi,"Feature extractor"),Vi.forEach(a),tp.forEach(a),Ol=c(s),A=l(s,"P",{});var Bs=r(A);Ph=o(Bs,"The next step is to load a feature extractor to normalize and pad the input. When padding textual data, a "),st=l(Bs,"CODE",{});var Gi=r(st);zh=o(Gi,"0"),Gi.forEach(a),Th=o(Bs," is added for shorter sequences. The same idea applies to audio data, and the audio feature extractor will add a "),at=l(Bs,"CODE",{});var Yi=r(at);Ch=o(Yi,"0"),Yi.forEach(a),Dh=o(Bs," - interpreted as silence - to "),et=l(Bs,"CODE",{});var Ki=r(et);Sh=o(Ki,"array"),Ki.forEach(a),Lh=o(Bs,"."),Bs.forEach(a),Il=c(s),As=l(s,"P",{});var lp=r(As);Nh=o(lp,"Load the feature extractor with "),Pe=l(lp,"A",{href:!0});var Qi=r(Pe);Oh=o(Qi,"AutoFeatureExtractor.from_pretrained()"),Qi.forEach(a),Ih=o(lp,":"),lp.forEach(a),Fl=c(s),d(_a.$$.fragment,s),Hl=c(s),B=l(s,"P",{});var ln=r(B);Fh=o(ln,"Pass the audio "),nt=l(ln,"CODE",{});var Xi=r(nt);Hh=o(Xi,"array"),Xi.forEach(a),Rh=o(ln," to the feature extractor. We also recommend adding the "),tt=l(ln,"CODE",{});var Zi=r(tt);Bh=o(Zi,"sampling_rate"),Zi.forEach(a),Wh=o(ln," argument in the feature extractor in order to better debug any silent errors that may occur."),ln.forEach(a),Rl=c(s),d($a.$$.fragment,s),Bl=c(s),as=l(s,"H3",{class:!0});var rp=r(as);Ps=l(rp,"A",{id:!0,class:!0,href:!0});var sm=r(Ps);lt=l(sm,"SPAN",{});var am=r(lt);d(va.$$.fragment,am),am.forEach(a),sm.forEach(a),Jh=c(rp),rt=l(rp,"SPAN",{});var em=r(rt);Mh=o(em,"Pad and truncate"),em.forEach(a),rp.forEach(a),Wl=c(s),ze=l(s,"P",{});var nm=r(ze);Uh=o(nm,"Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),nm.forEach(a),Jl=c(s),d(wa.$$.fragment,s),Ml=c(s),Te=l(s,"P",{});var tm=r(Te);Vh=o(tm,"As you can see, the first sample has a longer sequence than the second sample. Let\u2019s create a function that will preprocess the dataset. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),tm.forEach(a),Ul=c(s),d(ka.$$.fragment,s),Vl=c(s),Ce=l(s,"P",{});var lm=r(Ce);Gh=o(lm,"Apply the function to the the first few examples in the dataset:"),lm.forEach(a),Gl=c(s),d(ya.$$.fragment,s),Yl=c(s),De=l(s,"P",{});var rm=r(De);Yh=o(rm,"Now take another look at the processed sample lengths:"),rm.forEach(a),Kl=c(s),d(xa.$$.fragment,s),Ql=c(s),Se=l(s,"P",{});var pm=r(Se);Kh=o(pm,"The lengths of the first two samples now match the maximum length you specified."),pm.forEach(a),Xl=c(s),es=l(s,"H2",{class:!0});var pp=r(es);zs=l(pp,"A",{id:!0,class:!0,href:!0});var om=r(zs);pt=l(om,"SPAN",{});var hm=r(pt);d(Ea.$$.fragment,hm),hm.forEach(a),om.forEach(a),Qh=c(pp),ot=l(pp,"SPAN",{});var um=r(ot);Xh=o(um,"Vision"),um.forEach(a),pp.forEach(a),Zl=c(s),Le=l(s,"P",{});var cm=r(Le);Zh=o(cm,"A feature extractor is also used to process images for vision tasks. Once again, the goal is to convert the raw image into a batch of tensors as input."),cm.forEach(a),sr=c(s),W=l(s,"P",{});var rn=r(W);su=o(rn,"Let\u2019s load the "),qa=l(rn,"A",{href:!0,rel:!0});var im=r(qa);au=o(im,"food101"),im.forEach(a),eu=o(rn," dataset for this tutorial. Use \u{1F917} Datasets "),ht=l(rn,"CODE",{});var mm=r(ht);nu=o(mm,"split"),mm.forEach(a),tu=o(rn," parameter to only load a small sample from the training split since the dataset is quite large:"),rn.forEach(a),ar=c(s),d(Aa.$$.fragment,s),er=c(s),Ts=l(s,"P",{});var op=r(Ts);lu=o(op,"Next, take a look at the image with \u{1F917} Datasets "),Pa=l(op,"A",{href:!0,rel:!0});var fm=r(Pa);ut=l(fm,"CODE",{});var dm=r(ut);ru=o(dm,"Image"),dm.forEach(a),fm.forEach(a),pu=o(op," feature:"),op.forEach(a),nr=c(s),d(za.$$.fragment,s),tr=c(s),Ne=l(s,"P",{});var bm=r(Ne);Oe=l(bm,"IMG",{src:!0,alt:!0}),bm.forEach(a),lr=c(s),ns=l(s,"H3",{class:!0});var hp=r(ns);Cs=l(hp,"A",{id:!0,class:!0,href:!0});var jm=r(Cs);ct=l(jm,"SPAN",{});var gm=r(ct);d(Ta.$$.fragment,gm),gm.forEach(a),jm.forEach(a),ou=c(hp),it=l(hp,"SPAN",{});var _m=r(it);hu=o(_m,"Feature extractor"),_m.forEach(a),hp.forEach(a),rr=c(s),Ds=l(s,"P",{});var up=r(Ds);uu=o(up,"Load the feature extractor with "),Ie=l(up,"A",{href:!0});var $m=r(Ie);cu=o($m,"AutoFeatureExtractor.from_pretrained()"),$m.forEach(a),iu=o(up,":"),up.forEach(a),pr=c(s),d(Ca.$$.fragment,s),or=c(s),ts=l(s,"H3",{class:!0});var cp=r(ts);Ss=l(cp,"A",{id:!0,class:!0,href:!0});var vm=r(Ss);mt=l(vm,"SPAN",{});var wm=r(mt);d(Da.$$.fragment,wm),wm.forEach(a),vm.forEach(a),mu=c(cp),ft=l(cp,"SPAN",{});var km=r(ft);fu=o(km,"Data augmentation"),km.forEach(a),cp.forEach(a),hr=c(s),Ls=l(s,"P",{});var ip=r(Ls);du=o(ip,"For vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you will use torchvision\u2019s "),Sa=l(ip,"A",{href:!0,rel:!0});var ym=r(Sa);dt=l(ym,"CODE",{});var xm=r(dt);bu=o(xm,"transforms"),xm.forEach(a),ym.forEach(a),ju=o(ip," module."),ip.forEach(a),ur=c(s),Fe=l(s,"OL",{});var Em=r(Fe);z=l(Em,"LI",{});var Ws=r(z);gu=o(Ws,"Normalize the image and use "),La=l(Ws,"A",{href:!0,rel:!0});var qm=r(La);bt=l(qm,"CODE",{});var Am=r(bt);_u=o(Am,"Compose"),Am.forEach(a),qm.forEach(a),$u=o(Ws," to chain some transforms - "),Na=l(Ws,"A",{href:!0,rel:!0});var Pm=r(Na);jt=l(Pm,"CODE",{});var zm=r(jt);vu=o(zm,"RandomResizedCrop"),zm.forEach(a),Pm.forEach(a),wu=o(Ws," and "),Oa=l(Ws,"A",{href:!0,rel:!0});var Tm=r(Oa);gt=l(Tm,"CODE",{});var Cm=r(gt);ku=o(Cm,"ColorJitter"),Cm.forEach(a),Tm.forEach(a),yu=o(Ws," - together:"),Ws.forEach(a),Em.forEach(a),cr=c(s),d(Ia.$$.fragment,s),ir=c(s),Fa=l(s,"OL",{start:!0});var Dm=r(Fa);ls=l(Dm,"LI",{});var pn=r(ls);xu=o(pn,"The model accepts "),He=l(pn,"A",{href:!0});var Sm=r(He);_t=l(Sm,"CODE",{});var Lm=r(_t);Eu=o(Lm,"pixel_values"),Lm.forEach(a),Sm.forEach(a),qu=o(pn," as it\u2019s input. This value is generated by the feature extractor. Create a function that generates "),$t=l(pn,"CODE",{});var Nm=r($t);Au=o(Nm,"pixel_values"),Nm.forEach(a),Pu=o(pn," from the transforms:"),pn.forEach(a),Dm.forEach(a),mr=c(s),d(Ha.$$.fragment,s),fr=c(s),Ra=l(s,"OL",{start:!0});var Om=r(Ra);Ba=l(Om,"LI",{});var mp=r(Ba);zu=o(mp,"Then use \u{1F917} Datasets "),Wa=l(mp,"A",{href:!0,rel:!0});var Im=r(Wa);vt=l(Im,"CODE",{});var Fm=r(vt);Tu=o(Fm,"set_transform"),Fm.forEach(a),Im.forEach(a),Cu=o(mp," to apply the transforms on-the-fly:"),mp.forEach(a),Om.forEach(a),dr=c(s),d(Ja.$$.fragment,s),br=c(s),Ma=l(s,"OL",{start:!0});var Hm=r(Ma);Ua=l(Hm,"LI",{});var fp=r(Ua);Du=o(fp,"Now when you access the image, you will notice the feature extractor has added the model input "),wt=l(fp,"CODE",{});var Rm=r(wt);Su=o(Rm,"pixel_values"),Rm.forEach(a),Lu=o(fp,":"),fp.forEach(a),Hm.forEach(a),jr=c(s),d(Va.$$.fragment,s),gr=c(s),Re=l(s,"P",{});var Bm=r(Re);Nu=o(Bm,"Here is what the image looks like after you preprocess it. Just as you\u2019d expect from the applied transforms, the image has been randomly cropped and it\u2019s color properties are different."),Bm.forEach(a),_r=c(s),d(Ga.$$.fragment,s),$r=c(s),Be=l(s,"P",{});var Wm=r(Be);We=l(Wm,"IMG",{src:!0,alt:!0}),Wm.forEach(a),vr=c(s),rs=l(s,"H2",{class:!0});var dp=r(rs);Ns=l(dp,"A",{id:!0,class:!0,href:!0});var Jm=r(Ns);kt=l(Jm,"SPAN",{});var Mm=r(kt);d(Ya.$$.fragment,Mm),Mm.forEach(a),Jm.forEach(a),Ou=c(dp),yt=l(dp,"SPAN",{});var Um=r(yt);Iu=o(Um,"Multimodal"),Um.forEach(a),dp.forEach(a),wr=c(s),Je=l(s,"P",{});var Vm=r(Je);Fu=o(Vm,"For multimodal tasks. you will use a combination of everything you\u2019ve learned so far and apply your skills to a automatic speech recognition (ASR) task. This means you will need a:"),Vm.forEach(a),kr=c(s),Os=l(s,"UL",{});var bp=r(Os);xt=l(bp,"LI",{});var Gm=r(xt);Hu=o(Gm,"Feature extractor to preprocess the audio data."),Gm.forEach(a),Ru=c(bp),Et=l(bp,"LI",{});var Ym=r(Et);Bu=o(Ym,"Tokenizer to process the text."),Ym.forEach(a),bp.forEach(a),yr=c(s),Is=l(s,"P",{});var jp=r(Is);Wu=o(jp,"Let\u2019s return to the "),Ka=l(jp,"A",{href:!0,rel:!0});var Km=r(Ka);Ju=o(Km,"LJ Speech"),Km.forEach(a),Mu=o(jp," dataset:"),jp.forEach(a),xr=c(s),d(Qa.$$.fragment,s),Er=c(s),J=l(s,"P",{});var on=r(J);Uu=o(on,"Since you are mainly interested in the "),qt=l(on,"CODE",{});var Qm=r(qt);Vu=o(Qm,"audio"),Qm.forEach(a),Gu=o(on," and "),At=l(on,"CODE",{});var Xm=r(At);Yu=o(Xm,"text"),Xm.forEach(a),Ku=o(on," column, remove the other columns:"),on.forEach(a),qr=c(s),d(Xa.$$.fragment,s),Ar=c(s),M=l(s,"P",{});var hn=r(M);Qu=o(hn,"Now take a look at the "),Pt=l(hn,"CODE",{});var Zm=r(Pt);Xu=o(Zm,"audio"),Zm.forEach(a),Zu=o(hn," and "),zt=l(hn,"CODE",{});var sf=r(zt);sc=o(sf,"text"),sf.forEach(a),ac=o(hn," columns:"),hn.forEach(a),Pr=c(s),d(Za.$$.fragment,s),zr=c(s),Fs=l(s,"P",{});var gp=r(Fs);ec=o(gp,"Remember from the earlier section on processing audio data, you should always "),Me=l(gp,"A",{href:!0});var af=r(Me);nc=o(af,"resample"),af.forEach(a),tc=o(gp," your audio data\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model:"),gp.forEach(a),Tr=c(s),d(se.$$.fragment,s),Cr=c(s),ps=l(s,"H3",{class:!0});var _p=r(ps);Hs=l(_p,"A",{id:!0,class:!0,href:!0});var ef=r(Hs);Tt=l(ef,"SPAN",{});var nf=r(Tt);d(ae.$$.fragment,nf),nf.forEach(a),ef.forEach(a),lc=c(_p),Ct=l(_p,"SPAN",{});var tf=r(Ct);rc=o(tf,"Processor"),tf.forEach(a),_p.forEach(a),Dr=c(s),Ue=l(s,"P",{});var lf=r(Ue);pc=o(lf,"A processor combines a feature extractor and tokenizer. Load a processor with [`AutoProcessor.from_pretrained]:"),lf.forEach(a),Sr=c(s),d(ee.$$.fragment,s),Lr=c(s),Ve=l(s,"OL",{});var rf=r(Ve);os=l(rf,"LI",{});var un=r(os);oc=o(un,"Create a function to process the audio data to "),Dt=l(un,"CODE",{});var pf=r(Dt);hc=o(pf,"input_values"),pf.forEach(a),uc=o(un,", and tokenizes the text to "),St=l(un,"CODE",{});var of=r(St);cc=o(of,"labels"),of.forEach(a),ic=o(un,". These are your inputs to the model:"),un.forEach(a),rf.forEach(a),Nr=c(s),d(ne.$$.fragment,s),Or=c(s),te=l(s,"OL",{start:!0});var hf=r(te);le=l(hf,"LI",{});var $p=r(le);mc=o($p,"Apply the "),Lt=l($p,"CODE",{});var uf=r(Lt);fc=o(uf,"prepare_dataset"),uf.forEach(a),dc=o($p," function to a sample:"),$p.forEach(a),hf.forEach(a),Ir=c(s),d(re.$$.fragment,s),Fr=c(s),U=l(s,"P",{});var cn=r(U);bc=o(cn,"Notice the processor has added "),Nt=l(cn,"CODE",{});var cf=r(Nt);jc=o(cf,"input_values"),cf.forEach(a),gc=o(cn," and "),Ot=l(cn,"CODE",{});var mf=r(Ot);_c=o(mf,"labels"),mf.forEach(a),$c=o(cn,". The sampling rate has also been correctly downsampled to 16kHz."),cn.forEach(a),Hr=c(s),Ge=l(s,"P",{});var ff=r(Ge);vc=o(ff,"Awesome, you should now be able to preprocess data for any modality and even combine different modalities! In the next tutorial, learn how to fine-tune a model on your newly preprocessed data."),ff.forEach(a),this.h()},h(){i($,"name","hf:doc:metadata"),i($,"content",JSON.stringify(Df)),i(k,"id","preprocess"),i(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(k,"href","#preprocess"),i(m,"class","relative group"),i(us,"id","nlp"),i(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(us,"href","#nlp"),i(V,"class","relative group"),i(he,"href","main_classes/tokenizer"),i(ue,"href","/docs/transformers/pr_16901/en/model_doc/auto#transformers.AutoTokenizer"),i(is,"id","tokenize"),i(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(is,"href","#tokenize"),i(G,"class","relative group"),i(ce,"href","/docs/transformers/pr_16901/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),i(de,"href","glossary#input-ids"),i(je,"href","glossary#attention-mask"),i(_e,"href","glossary#token-type-ids"),i(ds,"id","pad"),i(ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ds,"href","#pad"),i(Y,"class","relative group"),i(gs,"id","truncation"),i(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(gs,"href","#truncation"),i(K,"class","relative group"),i(_s,"id","build-tensors"),i(_s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(_s,"href","#build-tensors"),i(Q,"class","relative group"),i(vs,"id","audio"),i(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(vs,"href","#audio"),i(X,"class","relative group"),i(ke,"href","main_classes/feature_extractor"),i(la,"href","https://huggingface.co/datasets/PolyAI/minds14"),i(la,"rel","nofollow"),i(ra,"href","https://huggingface.co/docs/datasets/load_hub.html"),i(ra,"rel","nofollow"),i(ks,"id","resample"),i(ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ks,"href","#resample"),i(Z,"class","relative group"),i(ua,"href","https://huggingface.co/facebook/wav2vec2-base"),i(ua,"rel","nofollow"),i(ca,"href","https://huggingface.co/datasets/PolyAI/minds14"),i(ca,"rel","nofollow"),i(fa,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.cast_column"),i(fa,"rel","nofollow"),i(ba,"start","2"),i(qs,"id","feature-extractor"),i(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(qs,"href","#feature-extractor"),i(ss,"class","relative group"),i(Pe,"href","/docs/transformers/pr_16901/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),i(Ps,"id","pad-and-truncate"),i(Ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ps,"href","#pad-and-truncate"),i(as,"class","relative group"),i(zs,"id","vision"),i(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(zs,"href","#vision"),i(es,"class","relative group"),i(qa,"href","https://huggingface.co/datasets/food101"),i(qa,"rel","nofollow"),i(Pa,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image"),i(Pa,"rel","nofollow"),df(Oe.src,Ac="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png")||i(Oe,"src",Ac),i(Oe,"alt","vision-preprocess-tutorial.png"),i(Cs,"id","feature-extractor"),i(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Cs,"href","#feature-extractor"),i(ns,"class","relative group"),i(Ie,"href","/docs/transformers/pr_16901/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),i(Ss,"id","data-augmentation"),i(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ss,"href","#data-augmentation"),i(ts,"class","relative group"),i(Sa,"href","https://pytorch.org/vision/stable/transforms.html"),i(Sa,"rel","nofollow"),i(La,"href","https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html"),i(La,"rel","nofollow"),i(Na,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html"),i(Na,"rel","nofollow"),i(Oa,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html"),i(Oa,"rel","nofollow"),i(He,"href","model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values"),i(Fa,"start","2"),i(Wa,"href","https://huggingface.co/docs/datasets/process.html#format-transform"),i(Wa,"rel","nofollow"),i(Ra,"start","3"),i(Ma,"start","4"),df(We.src,Pc="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png")||i(We,"src",Pc),i(We,"alt","preprocessed_image"),i(Ns,"id","multimodal"),i(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ns,"href","#multimodal"),i(rs,"class","relative group"),i(Ka,"href","https://huggingface.co/datasets/lj_speech"),i(Ka,"rel","nofollow"),i(Me,"href","preprocessing#audio"),i(Hs,"id","processor"),i(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Hs,"href","#processor"),i(ps,"class","relative group"),i(te,"start","2")},m(s,n){e(document.head,$),h(s,w,n),h(s,m,n),e(m,k),e(k,y),b(E,y,null),e(m,T),e(m,hs),e(hs,vp),h(s,Ft,n),b(Js,s,n),h(s,Ht,n),h(s,oe,n),e(oe,wp),h(s,Rt,n),h(s,C,n),e(C,mn),e(mn,kp),e(C,yp),e(C,fn),e(fn,xp),e(C,Ep),e(C,dn),e(dn,qp),h(s,Bt,n),h(s,V,n),e(V,us),e(us,bn),b(Ms,bn,null),e(V,Ap),e(V,jn),e(jn,Pp),h(s,Wt,n),b(Us,s,n),h(s,Jt,n),h(s,D,n),e(D,zp),e(D,he),e(he,Tp),e(D,Cp),e(D,gn),e(gn,Dp),e(D,Sp),h(s,Mt,n),b(cs,s,n),h(s,Ut,n),h(s,S,n),e(S,Lp),e(S,ue),e(ue,Np),e(S,Op),e(S,_n),e(_n,Ip),e(S,Fp),h(s,Vt,n),h(s,G,n),e(G,is),e(is,$n),b(Vs,$n,null),e(G,Hp),e(G,vn),e(vn,Rp),h(s,Gt,n),h(s,ms,n),e(ms,Bp),e(ms,ce),e(ce,Wp),e(ms,Jp),h(s,Yt,n),b(Gs,s,n),h(s,Kt,n),h(s,ie,n),e(ie,Mp),h(s,Qt,n),b(Ys,s,n),h(s,Xt,n),h(s,me,n),e(me,Up),h(s,Zt,n),h(s,L,n),e(L,fe),e(fe,de),e(de,Vp),e(fe,Gp),e(L,Yp),e(L,be),e(be,je),e(je,Kp),e(be,Qp),e(L,Xp),e(L,ge),e(ge,_e),e(_e,Zp),e(ge,so),h(s,sl,n),h(s,fs,n),e(fs,ao),e(fs,wn),e(wn,eo),e(fs,no),h(s,al,n),b(Ks,s,n),h(s,el,n),h(s,N,n),e(N,to),e(N,kn),e(kn,lo),e(N,ro),e(N,yn),e(yn,po),e(N,oo),h(s,nl,n),h(s,$e,n),e($e,ho),h(s,tl,n),b(Qs,s,n),h(s,ll,n),h(s,Y,n),e(Y,ds),e(ds,xn),b(Xs,xn,null),e(Y,uo),e(Y,En),e(En,co),h(s,rl,n),h(s,bs,n),e(bs,io),e(bs,qn),e(qn,mo),e(bs,fo),h(s,pl,n),h(s,O,n),e(O,bo),e(O,An),e(An,jo),e(O,go),e(O,Pn),e(Pn,_o),e(O,$o),h(s,ol,n),b(Zs,s,n),h(s,hl,n),h(s,js,n),e(js,vo),e(js,zn),e(zn,wo),e(js,ko),h(s,ul,n),h(s,K,n),e(K,gs),e(gs,Tn),b(sa,Tn,null),e(K,yo),e(K,Cn),e(Cn,xo),h(s,cl,n),h(s,ve,n),e(ve,Eo),h(s,il,n),h(s,I,n),e(I,qo),e(I,Dn),e(Dn,Ao),e(I,Po),e(I,Sn),e(Sn,zo),e(I,To),h(s,ml,n),b(aa,s,n),h(s,fl,n),h(s,Q,n),e(Q,_s),e(_s,Ln),b(ea,Ln,null),e(Q,Co),e(Q,Nn),e(Nn,Do),h(s,dl,n),h(s,we,n),e(we,So),h(s,bl,n),h(s,q,n),e(q,Lo),e(q,On),e(On,No),e(q,Oo),e(q,In),e(In,Io),e(q,Fo),e(q,Fn),e(Fn,Ho),e(q,Ro),h(s,jl,n),b($s,s,n),h(s,gl,n),h(s,X,n),e(X,vs),e(vs,Hn),b(na,Hn,null),e(X,Bo),e(X,Rn),e(Rn,Wo),h(s,_l,n),h(s,ws,n),e(ws,Jo),e(ws,ke),e(ke,Mo),e(ws,Uo),h(s,$l,n),b(ta,s,n),h(s,vl,n),h(s,F,n),e(F,Vo),e(F,la),e(la,Go),e(F,Yo),e(F,ra),e(ra,Ko),e(F,Qo),h(s,wl,n),b(pa,s,n),h(s,kl,n),h(s,H,n),e(H,Xo),e(H,Bn),e(Bn,Zo),e(H,sh),e(H,Wn),e(Wn,ah),e(H,eh),h(s,yl,n),b(oa,s,n),h(s,xl,n),h(s,ye,n),e(ye,nh),h(s,El,n),h(s,R,n),e(R,xe),e(xe,Jn),e(Jn,th),e(xe,lh),e(R,rh),e(R,Ee),e(Ee,Mn),e(Mn,ph),e(Ee,oh),e(R,hh),e(R,qe),e(qe,Un),e(Un,uh),e(qe,ch),h(s,ql,n),h(s,Z,n),e(Z,ks),e(ks,Vn),b(ha,Vn,null),e(Z,ih),e(Z,Gn),e(Gn,mh),h(s,Al,n),h(s,ys,n),e(ys,fh),e(ys,ua),e(ua,dh),e(ys,bh),h(s,Pl,n),h(s,xs,n),e(xs,jh),e(xs,ca),e(ca,gh),e(xs,_h),h(s,zl,n),b(ia,s,n),h(s,Tl,n),h(s,Ae,n),e(Ae,ma),e(ma,$h),e(ma,fa),e(fa,Yn),e(Yn,vh),e(ma,wh),h(s,Cl,n),b(da,s,n),h(s,Dl,n),h(s,ba,n),e(ba,Kn),e(Kn,kh),h(s,Sl,n),b(ja,s,n),h(s,Ll,n),h(s,Es,n),e(Es,yh),e(Es,Qn),e(Qn,xh),e(Es,Eh),h(s,Nl,n),h(s,ss,n),e(ss,qs),e(qs,Xn),b(ga,Xn,null),e(ss,qh),e(ss,Zn),e(Zn,Ah),h(s,Ol,n),h(s,A,n),e(A,Ph),e(A,st),e(st,zh),e(A,Th),e(A,at),e(at,Ch),e(A,Dh),e(A,et),e(et,Sh),e(A,Lh),h(s,Il,n),h(s,As,n),e(As,Nh),e(As,Pe),e(Pe,Oh),e(As,Ih),h(s,Fl,n),b(_a,s,n),h(s,Hl,n),h(s,B,n),e(B,Fh),e(B,nt),e(nt,Hh),e(B,Rh),e(B,tt),e(tt,Bh),e(B,Wh),h(s,Rl,n),b($a,s,n),h(s,Bl,n),h(s,as,n),e(as,Ps),e(Ps,lt),b(va,lt,null),e(as,Jh),e(as,rt),e(rt,Mh),h(s,Wl,n),h(s,ze,n),e(ze,Uh),h(s,Jl,n),b(wa,s,n),h(s,Ml,n),h(s,Te,n),e(Te,Vh),h(s,Ul,n),b(ka,s,n),h(s,Vl,n),h(s,Ce,n),e(Ce,Gh),h(s,Gl,n),b(ya,s,n),h(s,Yl,n),h(s,De,n),e(De,Yh),h(s,Kl,n),b(xa,s,n),h(s,Ql,n),h(s,Se,n),e(Se,Kh),h(s,Xl,n),h(s,es,n),e(es,zs),e(zs,pt),b(Ea,pt,null),e(es,Qh),e(es,ot),e(ot,Xh),h(s,Zl,n),h(s,Le,n),e(Le,Zh),h(s,sr,n),h(s,W,n),e(W,su),e(W,qa),e(qa,au),e(W,eu),e(W,ht),e(ht,nu),e(W,tu),h(s,ar,n),b(Aa,s,n),h(s,er,n),h(s,Ts,n),e(Ts,lu),e(Ts,Pa),e(Pa,ut),e(ut,ru),e(Ts,pu),h(s,nr,n),b(za,s,n),h(s,tr,n),h(s,Ne,n),e(Ne,Oe),h(s,lr,n),h(s,ns,n),e(ns,Cs),e(Cs,ct),b(Ta,ct,null),e(ns,ou),e(ns,it),e(it,hu),h(s,rr,n),h(s,Ds,n),e(Ds,uu),e(Ds,Ie),e(Ie,cu),e(Ds,iu),h(s,pr,n),b(Ca,s,n),h(s,or,n),h(s,ts,n),e(ts,Ss),e(Ss,mt),b(Da,mt,null),e(ts,mu),e(ts,ft),e(ft,fu),h(s,hr,n),h(s,Ls,n),e(Ls,du),e(Ls,Sa),e(Sa,dt),e(dt,bu),e(Ls,ju),h(s,ur,n),h(s,Fe,n),e(Fe,z),e(z,gu),e(z,La),e(La,bt),e(bt,_u),e(z,$u),e(z,Na),e(Na,jt),e(jt,vu),e(z,wu),e(z,Oa),e(Oa,gt),e(gt,ku),e(z,yu),h(s,cr,n),b(Ia,s,n),h(s,ir,n),h(s,Fa,n),e(Fa,ls),e(ls,xu),e(ls,He),e(He,_t),e(_t,Eu),e(ls,qu),e(ls,$t),e($t,Au),e(ls,Pu),h(s,mr,n),b(Ha,s,n),h(s,fr,n),h(s,Ra,n),e(Ra,Ba),e(Ba,zu),e(Ba,Wa),e(Wa,vt),e(vt,Tu),e(Ba,Cu),h(s,dr,n),b(Ja,s,n),h(s,br,n),h(s,Ma,n),e(Ma,Ua),e(Ua,Du),e(Ua,wt),e(wt,Su),e(Ua,Lu),h(s,jr,n),b(Va,s,n),h(s,gr,n),h(s,Re,n),e(Re,Nu),h(s,_r,n),b(Ga,s,n),h(s,$r,n),h(s,Be,n),e(Be,We),h(s,vr,n),h(s,rs,n),e(rs,Ns),e(Ns,kt),b(Ya,kt,null),e(rs,Ou),e(rs,yt),e(yt,Iu),h(s,wr,n),h(s,Je,n),e(Je,Fu),h(s,kr,n),h(s,Os,n),e(Os,xt),e(xt,Hu),e(Os,Ru),e(Os,Et),e(Et,Bu),h(s,yr,n),h(s,Is,n),e(Is,Wu),e(Is,Ka),e(Ka,Ju),e(Is,Mu),h(s,xr,n),b(Qa,s,n),h(s,Er,n),h(s,J,n),e(J,Uu),e(J,qt),e(qt,Vu),e(J,Gu),e(J,At),e(At,Yu),e(J,Ku),h(s,qr,n),b(Xa,s,n),h(s,Ar,n),h(s,M,n),e(M,Qu),e(M,Pt),e(Pt,Xu),e(M,Zu),e(M,zt),e(zt,sc),e(M,ac),h(s,Pr,n),b(Za,s,n),h(s,zr,n),h(s,Fs,n),e(Fs,ec),e(Fs,Me),e(Me,nc),e(Fs,tc),h(s,Tr,n),b(se,s,n),h(s,Cr,n),h(s,ps,n),e(ps,Hs),e(Hs,Tt),b(ae,Tt,null),e(ps,lc),e(ps,Ct),e(Ct,rc),h(s,Dr,n),h(s,Ue,n),e(Ue,pc),h(s,Sr,n),b(ee,s,n),h(s,Lr,n),h(s,Ve,n),e(Ve,os),e(os,oc),e(os,Dt),e(Dt,hc),e(os,uc),e(os,St),e(St,cc),e(os,ic),h(s,Nr,n),b(ne,s,n),h(s,Or,n),h(s,te,n),e(te,le),e(le,mc),e(le,Lt),e(Lt,fc),e(le,dc),h(s,Ir,n),b(re,s,n),h(s,Fr,n),h(s,U,n),e(U,bc),e(U,Nt),e(Nt,jc),e(U,gc),e(U,Ot),e(Ot,_c),e(U,$c),h(s,Hr,n),h(s,Ge,n),e(Ge,vc),Rr=!0},p(s,[n]){const pe={};n&2&&(pe.$$scope={dirty:n,ctx:s}),cs.$set(pe);const It={};n&2&&(It.$$scope={dirty:n,ctx:s}),$s.$set(It)},i(s){Rr||(j(E.$$.fragment,s),j(Js.$$.fragment,s),j(Ms.$$.fragment,s),j(Us.$$.fragment,s),j(cs.$$.fragment,s),j(Vs.$$.fragment,s),j(Gs.$$.fragment,s),j(Ys.$$.fragment,s),j(Ks.$$.fragment,s),j(Qs.$$.fragment,s),j(Xs.$$.fragment,s),j(Zs.$$.fragment,s),j(sa.$$.fragment,s),j(aa.$$.fragment,s),j(ea.$$.fragment,s),j($s.$$.fragment,s),j(na.$$.fragment,s),j(ta.$$.fragment,s),j(pa.$$.fragment,s),j(oa.$$.fragment,s),j(ha.$$.fragment,s),j(ia.$$.fragment,s),j(da.$$.fragment,s),j(ja.$$.fragment,s),j(ga.$$.fragment,s),j(_a.$$.fragment,s),j($a.$$.fragment,s),j(va.$$.fragment,s),j(wa.$$.fragment,s),j(ka.$$.fragment,s),j(ya.$$.fragment,s),j(xa.$$.fragment,s),j(Ea.$$.fragment,s),j(Aa.$$.fragment,s),j(za.$$.fragment,s),j(Ta.$$.fragment,s),j(Ca.$$.fragment,s),j(Da.$$.fragment,s),j(Ia.$$.fragment,s),j(Ha.$$.fragment,s),j(Ja.$$.fragment,s),j(Va.$$.fragment,s),j(Ga.$$.fragment,s),j(Ya.$$.fragment,s),j(Qa.$$.fragment,s),j(Xa.$$.fragment,s),j(Za.$$.fragment,s),j(se.$$.fragment,s),j(ae.$$.fragment,s),j(ee.$$.fragment,s),j(ne.$$.fragment,s),j(re.$$.fragment,s),Rr=!0)},o(s){g(E.$$.fragment,s),g(Js.$$.fragment,s),g(Ms.$$.fragment,s),g(Us.$$.fragment,s),g(cs.$$.fragment,s),g(Vs.$$.fragment,s),g(Gs.$$.fragment,s),g(Ys.$$.fragment,s),g(Ks.$$.fragment,s),g(Qs.$$.fragment,s),g(Xs.$$.fragment,s),g(Zs.$$.fragment,s),g(sa.$$.fragment,s),g(aa.$$.fragment,s),g(ea.$$.fragment,s),g($s.$$.fragment,s),g(na.$$.fragment,s),g(ta.$$.fragment,s),g(pa.$$.fragment,s),g(oa.$$.fragment,s),g(ha.$$.fragment,s),g(ia.$$.fragment,s),g(da.$$.fragment,s),g(ja.$$.fragment,s),g(ga.$$.fragment,s),g(_a.$$.fragment,s),g($a.$$.fragment,s),g(va.$$.fragment,s),g(wa.$$.fragment,s),g(ka.$$.fragment,s),g(ya.$$.fragment,s),g(xa.$$.fragment,s),g(Ea.$$.fragment,s),g(Aa.$$.fragment,s),g(za.$$.fragment,s),g(Ta.$$.fragment,s),g(Ca.$$.fragment,s),g(Da.$$.fragment,s),g(Ia.$$.fragment,s),g(Ha.$$.fragment,s),g(Ja.$$.fragment,s),g(Va.$$.fragment,s),g(Ga.$$.fragment,s),g(Ya.$$.fragment,s),g(Qa.$$.fragment,s),g(Xa.$$.fragment,s),g(Za.$$.fragment,s),g(se.$$.fragment,s),g(ae.$$.fragment,s),g(ee.$$.fragment,s),g(ne.$$.fragment,s),g(re.$$.fragment,s),Rr=!1},d(s){a($),s&&a(w),s&&a(m),_(E),s&&a(Ft),_(Js,s),s&&a(Ht),s&&a(oe),s&&a(Rt),s&&a(C),s&&a(Bt),s&&a(V),_(Ms),s&&a(Wt),_(Us,s),s&&a(Jt),s&&a(D),s&&a(Mt),_(cs,s),s&&a(Ut),s&&a(S),s&&a(Vt),s&&a(G),_(Vs),s&&a(Gt),s&&a(ms),s&&a(Yt),_(Gs,s),s&&a(Kt),s&&a(ie),s&&a(Qt),_(Ys,s),s&&a(Xt),s&&a(me),s&&a(Zt),s&&a(L),s&&a(sl),s&&a(fs),s&&a(al),_(Ks,s),s&&a(el),s&&a(N),s&&a(nl),s&&a($e),s&&a(tl),_(Qs,s),s&&a(ll),s&&a(Y),_(Xs),s&&a(rl),s&&a(bs),s&&a(pl),s&&a(O),s&&a(ol),_(Zs,s),s&&a(hl),s&&a(js),s&&a(ul),s&&a(K),_(sa),s&&a(cl),s&&a(ve),s&&a(il),s&&a(I),s&&a(ml),_(aa,s),s&&a(fl),s&&a(Q),_(ea),s&&a(dl),s&&a(we),s&&a(bl),s&&a(q),s&&a(jl),_($s,s),s&&a(gl),s&&a(X),_(na),s&&a(_l),s&&a(ws),s&&a($l),_(ta,s),s&&a(vl),s&&a(F),s&&a(wl),_(pa,s),s&&a(kl),s&&a(H),s&&a(yl),_(oa,s),s&&a(xl),s&&a(ye),s&&a(El),s&&a(R),s&&a(ql),s&&a(Z),_(ha),s&&a(Al),s&&a(ys),s&&a(Pl),s&&a(xs),s&&a(zl),_(ia,s),s&&a(Tl),s&&a(Ae),s&&a(Cl),_(da,s),s&&a(Dl),s&&a(ba),s&&a(Sl),_(ja,s),s&&a(Ll),s&&a(Es),s&&a(Nl),s&&a(ss),_(ga),s&&a(Ol),s&&a(A),s&&a(Il),s&&a(As),s&&a(Fl),_(_a,s),s&&a(Hl),s&&a(B),s&&a(Rl),_($a,s),s&&a(Bl),s&&a(as),_(va),s&&a(Wl),s&&a(ze),s&&a(Jl),_(wa,s),s&&a(Ml),s&&a(Te),s&&a(Ul),_(ka,s),s&&a(Vl),s&&a(Ce),s&&a(Gl),_(ya,s),s&&a(Yl),s&&a(De),s&&a(Kl),_(xa,s),s&&a(Ql),s&&a(Se),s&&a(Xl),s&&a(es),_(Ea),s&&a(Zl),s&&a(Le),s&&a(sr),s&&a(W),s&&a(ar),_(Aa,s),s&&a(er),s&&a(Ts),s&&a(nr),_(za,s),s&&a(tr),s&&a(Ne),s&&a(lr),s&&a(ns),_(Ta),s&&a(rr),s&&a(Ds),s&&a(pr),_(Ca,s),s&&a(or),s&&a(ts),_(Da),s&&a(hr),s&&a(Ls),s&&a(ur),s&&a(Fe),s&&a(cr),_(Ia,s),s&&a(ir),s&&a(Fa),s&&a(mr),_(Ha,s),s&&a(fr),s&&a(Ra),s&&a(dr),_(Ja,s),s&&a(br),s&&a(Ma),s&&a(jr),_(Va,s),s&&a(gr),s&&a(Re),s&&a(_r),_(Ga,s),s&&a($r),s&&a(Be),s&&a(vr),s&&a(rs),_(Ya),s&&a(wr),s&&a(Je),s&&a(kr),s&&a(Os),s&&a(yr),s&&a(Is),s&&a(xr),_(Qa,s),s&&a(Er),s&&a(J),s&&a(qr),_(Xa,s),s&&a(Ar),s&&a(M),s&&a(Pr),_(Za,s),s&&a(zr),s&&a(Fs),s&&a(Tr),_(se,s),s&&a(Cr),s&&a(ps),_(ae),s&&a(Dr),s&&a(Ue),s&&a(Sr),_(ee,s),s&&a(Lr),s&&a(Ve),s&&a(Nr),_(ne,s),s&&a(Or),s&&a(te),s&&a(Ir),_(re,s),s&&a(Fr),s&&a(U),s&&a(Hr),s&&a(Ge)}}}const Df={local:"preprocess",sections:[{local:"nlp",sections:[{local:"tokenize",title:"Tokenize"},{local:"pad",title:"Pad"},{local:"truncation",title:"Truncation"},{local:"build-tensors",title:"Build tensors"}],title:"NLP"},{local:"audio",sections:[{local:"resample",title:"Resample"},{local:"feature-extractor",title:"Feature extractor"},{local:"pad-and-truncate",title:"Pad and truncate"}],title:"Audio"},{local:"vision",sections:[{local:"feature-extractor",title:"Feature extractor"},{local:"data-augmentation",title:"Data augmentation"}],title:"Vision"},{local:"multimodal",sections:[{local:"processor",title:"Processor"}],title:"Multimodal"}],title:"Preprocess"};function Sf(P){return wf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bf extends gf{constructor($){super();_f(this,$,Sf,Cf,$f,{})}}export{Bf as default,Df as metadata};
