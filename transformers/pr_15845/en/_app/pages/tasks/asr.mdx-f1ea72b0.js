import{S as ye,i as ke,s as xe,e as o,k as u,w as y,t as e,M as Ee,c as r,d as t,m,a as i,x as k,h as n,b as d,F as a,g as c,y as x,q as E,o as T,B as q}from"../../chunks/vendor-6b77c823.js";import{T as zt}from"../../chunks/Tip-39098574.js";import{Y as Te}from"../../chunks/Youtube-5c6e11e6.js";import{I as Ha}from"../../chunks/IconCopyLink-7a11ce68.js";import{C as X}from"../../chunks/CodeBlock-3a8b25a8.js";import{F as qe,M as Ae}from"../../chunks/Markdown-4489c441.js";function Ce(F){let p,$,h,g,b;return{c(){p=o("p"),$=e("See the automatic speech recognition "),h=o("a"),g=e("task page"),b=e(" for more information about its associated models, datasets, and metrics."),this.h()},l(_){p=r(_,"P",{});var w=i(p);$=n(w,"See the automatic speech recognition "),h=r(w,"A",{href:!0,rel:!0});var A=i(h);g=n(A,"task page"),A.forEach(t),b=n(w," for more information about its associated models, datasets, and metrics."),w.forEach(t),this.h()},h(){d(h,"href","https://huggingface.co/tasks/automatic-speech-recognition"),d(h,"rel","nofollow")},m(_,w){c(_,p,w),a(p,$),a(p,h),a(h,g),a(p,b)},d(_){_&&t(p)}}}function De(F){let p,$,h,g,b,_,w,A;return{c(){p=o("p"),$=e("If you aren\u2019t familiar with fine-tuning a model with the "),h=o("a"),g=e("Trainer"),b=e(", take a look at the basic tutorial "),_=o("a"),w=e("here"),A=e("!"),this.h()},l(C){p=r(C,"P",{});var j=i(p);$=n(j,"If you aren\u2019t familiar with fine-tuning a model with the "),h=r(j,"A",{href:!0});var P=i(h);g=n(P,"Trainer"),P.forEach(t),b=n(j,", take a look at the basic tutorial "),_=r(j,"A",{href:!0});var D=i(_);w=n(D,"here"),D.forEach(t),A=n(j,"!"),j.forEach(t),this.h()},h(){d(h,"href","/docs/transformers/pr_15845/en/main_classes/trainer#transformers.Trainer"),d(_,"href","../training#finetune-with-trainer")},m(C,j){c(C,p,j),a(p,$),a(p,h),a(h,g),a(p,b),a(p,_),a(_,w),a(p,A)},d(C){C&&t(p)}}}function Pe(F){let p,$,h,g,b,_,w,A,C,j,P,D,B,Z,hs,S,W,L,ws,ss,J,ys,ks,R,N,G,z,U,os,M,xs,V,Es,fs,K,Q,rs;return j=new X({props:{code:`from transformers import AutoModelForCTC, TrainingArguments, Trainer

model = AutoModelForCTC.from_pretrained(
    "facebook/wav2vec-base",
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec-base&quot;</span>,
<span class="hljs-meta">... </span>    ctc_loss_reduction=<span class="hljs-string">&quot;mean&quot;</span>,
<span class="hljs-meta">... </span>    pad_token_id=processor.tokenizer.pad_token_id,
<span class="hljs-meta">... </span>)`}}),D=new zt({props:{$$slots:{default:[De]},$$scope:{ctx:F}}}),Q=new X({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    group_by_length=True,
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=3,
    fp16=True,
    gradient_checkpointing=True,
    learning_rate=1e-4,
    weight_decay=0.005,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=timit["train"],
    eval_dataset=timit["test"],
    tokenizer=processor.feature_extractor,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    group_by_length=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    gradient_checkpointing=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">1e-4</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.005</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=timit[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=timit[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=processor.feature_extractor,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){p=o("p"),$=e("Load Wav2Vec2 with "),h=o("a"),g=e("AutoModelForCTC"),b=e(". For "),_=o("code"),w=e("ctc_loss_reduction"),A=e(", it is often better to use the average instead of the default summation:"),C=u(),y(j.$$.fragment),P=u(),y(D.$$.fragment),B=u(),Z=o("p"),hs=e("At this point, only three steps remain:"),S=u(),W=o("ol"),L=o("li"),ws=e("Define your training hyperparameters in "),ss=o("a"),J=e("TrainingArguments"),ys=e("."),ks=u(),R=o("li"),N=e("Pass the training arguments to "),G=o("a"),z=e("Trainer"),U=e(" along with the model, datasets, tokenizer, and data collator."),os=u(),M=o("li"),xs=e("Call "),V=o("a"),Es=e("train()"),fs=e(" to fine-tune your model."),K=u(),y(Q.$$.fragment),this.h()},l(f){p=r(f,"P",{});var v=i(p);$=n(v,"Load Wav2Vec2 with "),h=r(v,"A",{href:!0});var Y=i(h);g=n(Y,"AutoModelForCTC"),Y.forEach(t),b=n(v,". For "),_=r(v,"CODE",{});var Rs=i(_);w=n(Rs,"ctc_loss_reduction"),Rs.forEach(t),A=n(v,", it is often better to use the average instead of the default summation:"),v.forEach(t),C=m(f),k(j.$$.fragment,f),P=m(f),k(D.$$.fragment,f),B=m(f),Z=r(f,"P",{});var Ts=i(Z);hs=n(Ts,"At this point, only three steps remain:"),Ts.forEach(t),S=m(f),W=r(f,"OL",{});var O=i(W);L=r(O,"LI",{});var is=i(L);ws=n(is,"Define your training hyperparameters in "),ss=r(is,"A",{href:!0});var I=i(ss);J=n(I,"TrainingArguments"),I.forEach(t),ys=n(is,"."),is.forEach(t),ks=m(O),R=r(O,"LI",{});var us=i(R);N=n(us,"Pass the training arguments to "),G=r(us,"A",{href:!0});var ms=i(G);z=n(ms,"Trainer"),ms.forEach(t),U=n(us," along with the model, datasets, tokenizer, and data collator."),us.forEach(t),os=m(O),M=r(O,"LI",{});var ds=i(M);xs=n(ds,"Call "),V=r(ds,"A",{href:!0});var Ns=i(V);Es=n(Ns,"train()"),Ns.forEach(t),fs=n(ds," to fine-tune your model."),ds.forEach(t),O.forEach(t),K=m(f),k(Q.$$.fragment,f),this.h()},h(){d(h,"href","/docs/transformers/pr_15845/en/model_doc/auto#transformers.AutoModelForCTC"),d(ss,"href","/docs/transformers/pr_15845/en/main_classes/trainer#transformers.TrainingArguments"),d(G,"href","/docs/transformers/pr_15845/en/main_classes/trainer#transformers.Trainer"),d(V,"href","/docs/transformers/pr_15845/en/main_classes/trainer#transformers.Trainer.train")},m(f,v){c(f,p,v),a(p,$),a(p,h),a(h,g),a(p,b),a(p,_),a(_,w),a(p,A),c(f,C,v),x(j,f,v),c(f,P,v),x(D,f,v),c(f,B,v),c(f,Z,v),a(Z,hs),c(f,S,v),c(f,W,v),a(W,L),a(L,ws),a(L,ss),a(ss,J),a(L,ys),a(W,ks),a(W,R),a(R,N),a(R,G),a(G,z),a(R,U),a(W,os),a(W,M),a(M,xs),a(M,V),a(V,Es),a(M,fs),c(f,K,v),x(Q,f,v),rs=!0},p(f,v){const Y={};v&2&&(Y.$$scope={dirty:v,ctx:f}),D.$set(Y)},i(f){rs||(E(j.$$.fragment,f),E(D.$$.fragment,f),E(Q.$$.fragment,f),rs=!0)},o(f){T(j.$$.fragment,f),T(D.$$.fragment,f),T(Q.$$.fragment,f),rs=!1},d(f){f&&t(p),f&&t(C),q(j,f),f&&t(P),q(D,f),f&&t(B),f&&t(Z),f&&t(S),f&&t(W),f&&t(K),q(Q,f)}}}function Ie(F){let p,$;return p=new Ae({props:{$$slots:{default:[Pe]},$$scope:{ctx:F}}}),{c(){y(p.$$.fragment)},l(h){k(p.$$.fragment,h)},m(h,g){x(p,h,g),$=!0},p(h,g){const b={};g&2&&(b.$$scope={dirty:g,ctx:h}),p.$set(b)},i(h){$||(E(p.$$.fragment,h),$=!0)},o(h){T(p.$$.fragment,h),$=!1},d(h){q(p,h)}}}function Se(F){let p,$,h,g,b,_,w,A;return{c(){p=o("p"),$=e("For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog "),h=o("a"),g=e("post"),b=e(" for English ASR and this "),_=o("a"),w=e("post"),A=e(" for multilingual ASR."),this.h()},l(C){p=r(C,"P",{});var j=i(p);$=n(j,"For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog "),h=r(j,"A",{href:!0,rel:!0});var P=i(h);g=n(P,"post"),P.forEach(t),b=n(j," for English ASR and this "),_=r(j,"A",{href:!0,rel:!0});var D=i(_);w=n(D,"post"),D.forEach(t),A=n(j," for multilingual ASR."),j.forEach(t),this.h()},h(){d(h,"href","https://huggingface.co/blog/fine-tune-wav2vec2-english"),d(h,"rel","nofollow"),d(_,"href","https://huggingface.co/blog/fine-tune-xlsr-wav2vec2"),d(_,"rel","nofollow")},m(C,j){c(C,p,j),a(p,$),a(p,h),a(h,g),a(p,b),a(p,_),a(_,w),a(p,A)},d(C){C&&t(p)}}}function Le(F){let p,$,h,g,b,_,w,A,C,j,P,D,B,Z,hs,S,W,L,ws,ss,J,ys,ks,R,N,G,z,U,os,M,xs,V,Es,fs,K,Q,rs,f,v,Y,Rs,Ts,O,is,I,us,ms,ds,Ns,Xs,Ba,Ja,Zs,Ga,Ka,sa,Qa,Xa,$a,qs,va,Us,Za,ba,As,wa,as,st,aa,at,tt,ta,et,nt,ya,ps,_s,ea,Cs,lt,na,ot,ka,Vs,rt,xa,Ds,Ea,Ys,it,Ta,ts,Ps,pt,la,ct,ht,ft,Is,ut,oa,mt,dt,_t,ra,gt,qa,Ss,Aa,es,jt,Ls,ia,$t,vt,pa,bt,wt,Ca,Ms,Da,H,yt,Hs,kt,xt,ca,Et,Tt,ha,qt,At,Pa,ns,Ct,fa,Dt,Pt,ua,It,St,Ia,Ws,Sa,gs,Lt,ma,Mt,Wt,La,Os,Ma,cs,js,da,Fs,Ot,_a,Ft,Wa,$s,Oa,vs,Fa;return _=new Ha({}),P=new Te({props:{id:"TksaY_FDgnk"}}),N=new zt({props:{$$slots:{default:[Ce]},$$scope:{ctx:F}}}),M=new Ha({}),f=new X({props:{code:`from datasets import load_dataset

timit = load_dataset("timit_asr")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>timit = load_dataset(<span class="hljs-string">&quot;timit_asr&quot;</span>)`}}),O=new X({props:{code:"timit",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>timit
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;phonetic_detail&#x27;</span>, <span class="hljs-string">&#x27;word_detail&#x27;</span>, <span class="hljs-string">&#x27;dialect_region&#x27;</span>, <span class="hljs-string">&#x27;sentence_type&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">4620</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;file&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;phonetic_detail&#x27;</span>, <span class="hljs-string">&#x27;word_detail&#x27;</span>, <span class="hljs-string">&#x27;dialect_region&#x27;</span>, <span class="hljs-string">&#x27;sentence_type&#x27;</span>, <span class="hljs-string">&#x27;speaker_id&#x27;</span>, <span class="hljs-string">&#x27;id&#x27;</span>],
        num_rows: <span class="hljs-number">1680</span>
    })
})`}}),qs=new X({props:{code:`timit = timit.remove_columns(
    ["phonetic_detail", "word_detail", "dialect_region", "id", "sentence_type", "speaker_id"]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>timit = timit.remove_columns(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;phonetic_detail&quot;</span>, <span class="hljs-string">&quot;word_detail&quot;</span>, <span class="hljs-string">&quot;dialect_region&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;sentence_type&quot;</span>, <span class="hljs-string">&quot;speaker_id&quot;</span>]
<span class="hljs-meta">... </span>)`}}),As=new X({props:{code:'timit["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>timit[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>,  <span class="hljs-number">3.0517578e-05</span>, ...,
         -<span class="hljs-number">3.0517578e-05</span>, -<span class="hljs-number">9.1552734e-05</span>, -<span class="hljs-number">6.1035156e-05</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TRAIN/DR4/MMDM0/SI681.WAV&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;file&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/404950a46da14eac65eb4e2a8317b1372fb3971d980d91d5d5b221275b1fd7e0/data/TRAIN/DR4/MMDM0/SI681.WAV&#x27;</span>,
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Would such an act of refusal be useful?&#x27;</span>}`}}),Cs=new Ha({}),Ds=new X({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),Ss=new X({props:{code:`def prepare_dataset(batch):
    audio = batch["audio"]

    batch["input_values"] = processor(audio["array"], sampling_rate=audio["sampling_rate"]).input_values[0]
    batch["input_length"] = len(batch["input_values"])

    with processor.as_target_processor():
        batch["labels"] = processor(batch["text"]).input_ids
    return batch`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_values&quot;</span>] = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>]).input_values[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = processor(batch[<span class="hljs-string">&quot;text&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch`}}),Ms=new X({props:{code:'timit = timit.map(prepare_dataset, remove_columns=timit.column_names["train"], num_proc=4)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>timit = timit.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=timit.column_names[<span class="hljs-string">&quot;train&quot;</span>], num_proc=<span class="hljs-number">4</span>)'}}),Ws=new X({props:{code:`import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union


@dataclass
class DataCollatorCTCWithPadding:

    processor: AutoProcessor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            return_tensors="pt",
        )
        with self.processor.as_target_processor():
            labels_batch = self.processor.pad(
                label_features,
                padding=self.padding,
                return_tensors="pt",
            )

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Union</span>


<span class="hljs-meta">&gt;&gt;&gt; </span>@dataclass
<span class="hljs-meta">... </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorCTCWithPadding</span>:

<span class="hljs-meta">... </span>    processor: AutoProcessor
<span class="hljs-meta">... </span>    padding: <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>] = <span class="hljs-literal">True</span>

<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], torch.Tensor]]]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:
<span class="hljs-meta">... </span>        <span class="hljs-comment"># split inputs and labels since they have to be of different lengths and need</span>
<span class="hljs-meta">... </span>        <span class="hljs-comment"># different padding methods</span>
<span class="hljs-meta">... </span>        input_features = [{<span class="hljs-string">&quot;input_values&quot;</span>: feature[<span class="hljs-string">&quot;input_values&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
<span class="hljs-meta">... </span>        label_features = [{<span class="hljs-string">&quot;input_ids&quot;</span>: feature[<span class="hljs-string">&quot;labels&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]

<span class="hljs-meta">... </span>        batch = self.processor.pad(
<span class="hljs-meta">... </span>            input_features,
<span class="hljs-meta">... </span>            padding=self.padding,
<span class="hljs-meta">... </span>            return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>        <span class="hljs-keyword">with</span> self.processor.as_target_processor():
<span class="hljs-meta">... </span>            labels_batch = self.processor.pad(
<span class="hljs-meta">... </span>                label_features,
<span class="hljs-meta">... </span>                padding=self.padding,
<span class="hljs-meta">... </span>                return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>            )

<span class="hljs-meta">... </span>        <span class="hljs-comment"># replace padding with -100 to ignore loss correctly</span>
<span class="hljs-meta">... </span>        labels = labels_batch[<span class="hljs-string">&quot;input_ids&quot;</span>].masked_fill(labels_batch.attention_mask.ne(<span class="hljs-number">1</span>), -<span class="hljs-number">100</span>)

<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = labels

<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> batch`}}),Os=new X({props:{code:"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorCTCWithPadding(processor=processor, padding=<span class="hljs-literal">True</span>)'}}),Fs=new Ha({}),$s=new qe({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Ie]},$$scope:{ctx:F}}}),vs=new zt({props:{$$slots:{default:[Se]},$$scope:{ctx:F}}}),{c(){p=o("meta"),$=u(),h=o("h1"),g=o("a"),b=o("span"),y(_.$$.fragment),w=u(),A=o("span"),C=e("Automatic speech recognition"),j=u(),y(P.$$.fragment),D=u(),B=o("p"),Z=e("Automatic speech recognition (ASR) converts a speech signal to text. It is an example of a sequence-to-sequence task, going from a sequence of audio inputs to textual outputs. Voice assistants like Siri and Alexa utilize ASR models to assist users."),hs=u(),S=o("p"),W=e("This guide will show you how to fine-tune "),L=o("a"),ws=e("Wav2Vec2"),ss=e(" on the "),J=o("a"),ys=e("TIMIT"),ks=e(" dataset to transcribe audio to text."),R=u(),y(N.$$.fragment),G=u(),z=o("h2"),U=o("a"),os=o("span"),y(M.$$.fragment),xs=u(),V=o("span"),Es=e("Load TIMIT dataset"),fs=u(),K=o("p"),Q=e("Load the TIMIT dataset from the \u{1F917} Datasets library:"),rs=u(),y(f.$$.fragment),v=u(),Y=o("p"),Rs=e("Then take a look at an example:"),Ts=u(),y(O.$$.fragment),is=u(),I=o("p"),us=e("While the dataset contains a lot of helpful information, like "),ms=o("code"),ds=e("dialect_region"),Ns=e(" and "),Xs=o("code"),Ba=e("sentence_type"),Ja=e(", you will focus on the "),Zs=o("code"),Ga=e("audio"),Ka=e(" and "),sa=o("code"),Qa=e("text"),Xa=e(" fields in this guide. Remove the other columns:"),$a=u(),y(qs.$$.fragment),va=u(),Us=o("p"),Za=e("Take a look at the example again:"),ba=u(),y(As.$$.fragment),wa=u(),as=o("p"),st=e("The "),aa=o("code"),at=e("audio"),tt=e(" column contains a 1-dimensional "),ta=o("code"),et=e("array"),nt=e(" of the speech signal that must be called to load and resample the audio file."),ya=u(),ps=o("h2"),_s=o("a"),ea=o("span"),y(Cs.$$.fragment),lt=u(),na=o("span"),ot=e("Preprocess"),ka=u(),Vs=o("p"),rt=e("Load the Wav2Vec2 processor to process the audio signal and transcribed text:"),xa=u(),y(Ds.$$.fragment),Ea=u(),Ys=o("p"),it=e("The preprocessing function needs to:"),Ta=u(),ts=o("ol"),Ps=o("li"),pt=e("Call the "),la=o("code"),ct=e("audio"),ht=e(" column to load and resample the audio file."),ft=u(),Is=o("li"),ut=e("Extract the "),oa=o("code"),mt=e("input_values"),dt=e(" from the audio file."),_t=u(),ra=o("li"),gt=e("Typically, when you call the processor, you call the feature extractor. Since you also want to tokenize text, instruct the processor to call the tokenizer instead with a context manager."),qa=u(),y(Ss.$$.fragment),Aa=u(),es=o("p"),jt=e("Use \u{1F917} Datasets "),Ls=o("a"),ia=o("code"),$t=e("map"),vt=e(" function to apply the preprocessing function over the entire dataset. You can speed up the map function by increasing the number of processes with "),pa=o("code"),bt=e("num_proc"),wt=e(". Remove the columns you don\u2019t need:"),Ca=u(),y(Ms.$$.fragment),Da=u(),H=o("p"),yt=e("\u{1F917} Transformers doesn\u2019t have a data collator for automatic speech recognition, so you will need to create one. You can adapt the "),Hs=o("a"),kt=e("DataCollatorWithPadding"),xt=e(" to create a batch of examples for automatic speech recognition. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ca=o("code"),Et=e("tokenizer"),Tt=e(" function by setting "),ha=o("code"),qt=e("padding=True"),At=e(", dynamic padding is more efficient."),Pa=u(),ns=o("p"),Ct=e("Unlike other data collators, this specific data collator needs to apply a different padding method to "),fa=o("code"),Dt=e("input_values"),Pt=e(" and "),ua=o("code"),It=e("labels"),St=e(". You can apply a different padding method with a context manager:"),Ia=u(),y(Ws.$$.fragment),Sa=u(),gs=o("p"),Lt=e("Create a batch of examples and dynamically pad them with "),ma=o("code"),Mt=e("DataCollatorForCTCWithPadding"),Wt=e(":"),La=u(),y(Os.$$.fragment),Ma=u(),cs=o("h2"),js=o("a"),da=o("span"),y(Fs.$$.fragment),Ot=u(),_a=o("span"),Ft=e("Train"),Wa=u(),y($s.$$.fragment),Oa=u(),y(vs.$$.fragment),this.h()},l(s){const l=Ee('[data-svelte="svelte-1phssyn"]',document.head);p=r(l,"META",{name:!0,content:!0}),l.forEach(t),$=m(s),h=r(s,"H1",{class:!0});var zs=i(h);g=r(zs,"A",{id:!0,class:!0,href:!0});var ga=i(g);b=r(ga,"SPAN",{});var ja=i(b);k(_.$$.fragment,ja),ja.forEach(t),ga.forEach(t),w=m(zs),A=r(zs,"SPAN",{});var Rt=i(A);C=n(Rt,"Automatic speech recognition"),Rt.forEach(t),zs.forEach(t),j=m(s),k(P.$$.fragment,s),D=m(s),B=r(s,"P",{});var Nt=i(B);Z=n(Nt,"Automatic speech recognition (ASR) converts a speech signal to text. It is an example of a sequence-to-sequence task, going from a sequence of audio inputs to textual outputs. Voice assistants like Siri and Alexa utilize ASR models to assist users."),Nt.forEach(t),hs=m(s),S=r(s,"P",{});var Bs=i(S);W=n(Bs,"This guide will show you how to fine-tune "),L=r(Bs,"A",{href:!0,rel:!0});var Ut=i(L);ws=n(Ut,"Wav2Vec2"),Ut.forEach(t),ss=n(Bs," on the "),J=r(Bs,"A",{href:!0,rel:!0});var Vt=i(J);ys=n(Vt,"TIMIT"),Vt.forEach(t),ks=n(Bs," dataset to transcribe audio to text."),Bs.forEach(t),R=m(s),k(N.$$.fragment,s),G=m(s),z=r(s,"H2",{class:!0});var za=i(z);U=r(za,"A",{id:!0,class:!0,href:!0});var Yt=i(U);os=r(Yt,"SPAN",{});var Ht=i(os);k(M.$$.fragment,Ht),Ht.forEach(t),Yt.forEach(t),xs=m(za),V=r(za,"SPAN",{});var Bt=i(V);Es=n(Bt,"Load TIMIT dataset"),Bt.forEach(t),za.forEach(t),fs=m(s),K=r(s,"P",{});var Jt=i(K);Q=n(Jt,"Load the TIMIT dataset from the \u{1F917} Datasets library:"),Jt.forEach(t),rs=m(s),k(f.$$.fragment,s),v=m(s),Y=r(s,"P",{});var Gt=i(Y);Rs=n(Gt,"Then take a look at an example:"),Gt.forEach(t),Ts=m(s),k(O.$$.fragment,s),is=m(s),I=r(s,"P",{});var ls=i(I);us=n(ls,"While the dataset contains a lot of helpful information, like "),ms=r(ls,"CODE",{});var Kt=i(ms);ds=n(Kt,"dialect_region"),Kt.forEach(t),Ns=n(ls," and "),Xs=r(ls,"CODE",{});var Qt=i(Xs);Ba=n(Qt,"sentence_type"),Qt.forEach(t),Ja=n(ls,", you will focus on the "),Zs=r(ls,"CODE",{});var Xt=i(Zs);Ga=n(Xt,"audio"),Xt.forEach(t),Ka=n(ls," and "),sa=r(ls,"CODE",{});var Zt=i(sa);Qa=n(Zt,"text"),Zt.forEach(t),Xa=n(ls," fields in this guide. Remove the other columns:"),ls.forEach(t),$a=m(s),k(qs.$$.fragment,s),va=m(s),Us=r(s,"P",{});var se=i(Us);Za=n(se,"Take a look at the example again:"),se.forEach(t),ba=m(s),k(As.$$.fragment,s),wa=m(s),as=r(s,"P",{});var Js=i(as);st=n(Js,"The "),aa=r(Js,"CODE",{});var ae=i(aa);at=n(ae,"audio"),ae.forEach(t),tt=n(Js," column contains a 1-dimensional "),ta=r(Js,"CODE",{});var te=i(ta);et=n(te,"array"),te.forEach(t),nt=n(Js," of the speech signal that must be called to load and resample the audio file."),Js.forEach(t),ya=m(s),ps=r(s,"H2",{class:!0});var Ra=i(ps);_s=r(Ra,"A",{id:!0,class:!0,href:!0});var ee=i(_s);ea=r(ee,"SPAN",{});var ne=i(ea);k(Cs.$$.fragment,ne),ne.forEach(t),ee.forEach(t),lt=m(Ra),na=r(Ra,"SPAN",{});var le=i(na);ot=n(le,"Preprocess"),le.forEach(t),Ra.forEach(t),ka=m(s),Vs=r(s,"P",{});var oe=i(Vs);rt=n(oe,"Load the Wav2Vec2 processor to process the audio signal and transcribed text:"),oe.forEach(t),xa=m(s),k(Ds.$$.fragment,s),Ea=m(s),Ys=r(s,"P",{});var re=i(Ys);it=n(re,"The preprocessing function needs to:"),re.forEach(t),Ta=m(s),ts=r(s,"OL",{});var Gs=i(ts);Ps=r(Gs,"LI",{});var Na=i(Ps);pt=n(Na,"Call the "),la=r(Na,"CODE",{});var ie=i(la);ct=n(ie,"audio"),ie.forEach(t),ht=n(Na," column to load and resample the audio file."),Na.forEach(t),ft=m(Gs),Is=r(Gs,"LI",{});var Ua=i(Is);ut=n(Ua,"Extract the "),oa=r(Ua,"CODE",{});var pe=i(oa);mt=n(pe,"input_values"),pe.forEach(t),dt=n(Ua," from the audio file."),Ua.forEach(t),_t=m(Gs),ra=r(Gs,"LI",{});var ce=i(ra);gt=n(ce,"Typically, when you call the processor, you call the feature extractor. Since you also want to tokenize text, instruct the processor to call the tokenizer instead with a context manager."),ce.forEach(t),Gs.forEach(t),qa=m(s),k(Ss.$$.fragment,s),Aa=m(s),es=r(s,"P",{});var Ks=i(es);jt=n(Ks,"Use \u{1F917} Datasets "),Ls=r(Ks,"A",{href:!0,rel:!0});var he=i(Ls);ia=r(he,"CODE",{});var fe=i(ia);$t=n(fe,"map"),fe.forEach(t),he.forEach(t),vt=n(Ks," function to apply the preprocessing function over the entire dataset. You can speed up the map function by increasing the number of processes with "),pa=r(Ks,"CODE",{});var ue=i(pa);bt=n(ue,"num_proc"),ue.forEach(t),wt=n(Ks,". Remove the columns you don\u2019t need:"),Ks.forEach(t),Ca=m(s),k(Ms.$$.fragment,s),Da=m(s),H=r(s,"P",{});var bs=i(H);yt=n(bs,"\u{1F917} Transformers doesn\u2019t have a data collator for automatic speech recognition, so you will need to create one. You can adapt the "),Hs=r(bs,"A",{href:!0});var me=i(Hs);kt=n(me,"DataCollatorWithPadding"),me.forEach(t),xt=n(bs," to create a batch of examples for automatic speech recognition. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ca=r(bs,"CODE",{});var de=i(ca);Et=n(de,"tokenizer"),de.forEach(t),Tt=n(bs," function by setting "),ha=r(bs,"CODE",{});var _e=i(ha);qt=n(_e,"padding=True"),_e.forEach(t),At=n(bs,", dynamic padding is more efficient."),bs.forEach(t),Pa=m(s),ns=r(s,"P",{});var Qs=i(ns);Ct=n(Qs,"Unlike other data collators, this specific data collator needs to apply a different padding method to "),fa=r(Qs,"CODE",{});var ge=i(fa);Dt=n(ge,"input_values"),ge.forEach(t),Pt=n(Qs," and "),ua=r(Qs,"CODE",{});var je=i(ua);It=n(je,"labels"),je.forEach(t),St=n(Qs,". You can apply a different padding method with a context manager:"),Qs.forEach(t),Ia=m(s),k(Ws.$$.fragment,s),Sa=m(s),gs=r(s,"P",{});var Va=i(gs);Lt=n(Va,"Create a batch of examples and dynamically pad them with "),ma=r(Va,"CODE",{});var $e=i(ma);Mt=n($e,"DataCollatorForCTCWithPadding"),$e.forEach(t),Wt=n(Va,":"),Va.forEach(t),La=m(s),k(Os.$$.fragment,s),Ma=m(s),cs=r(s,"H2",{class:!0});var Ya=i(cs);js=r(Ya,"A",{id:!0,class:!0,href:!0});var ve=i(js);da=r(ve,"SPAN",{});var be=i(da);k(Fs.$$.fragment,be),be.forEach(t),ve.forEach(t),Ot=m(Ya),_a=r(Ya,"SPAN",{});var we=i(_a);Ft=n(we,"Train"),we.forEach(t),Ya.forEach(t),Wa=m(s),k($s.$$.fragment,s),Oa=m(s),k(vs.$$.fragment,s),this.h()},h(){d(p,"name","hf:doc:metadata"),d(p,"content",JSON.stringify(Me)),d(g,"id","automatic-speech-recognition"),d(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g,"href","#automatic-speech-recognition"),d(h,"class","relative group"),d(L,"href","https://huggingface.co/facebook/wav2vec2-base"),d(L,"rel","nofollow"),d(J,"href","https://huggingface.co/datasets/timit_asr"),d(J,"rel","nofollow"),d(U,"id","load-timit-dataset"),d(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(U,"href","#load-timit-dataset"),d(z,"class","relative group"),d(_s,"id","preprocess"),d(_s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_s,"href","#preprocess"),d(ps,"class","relative group"),d(Ls,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),d(Ls,"rel","nofollow"),d(Hs,"href","/docs/transformers/pr_15845/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),d(js,"id","train"),d(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(js,"href","#train"),d(cs,"class","relative group")},m(s,l){a(document.head,p),c(s,$,l),c(s,h,l),a(h,g),a(g,b),x(_,b,null),a(h,w),a(h,A),a(A,C),c(s,j,l),x(P,s,l),c(s,D,l),c(s,B,l),a(B,Z),c(s,hs,l),c(s,S,l),a(S,W),a(S,L),a(L,ws),a(S,ss),a(S,J),a(J,ys),a(S,ks),c(s,R,l),x(N,s,l),c(s,G,l),c(s,z,l),a(z,U),a(U,os),x(M,os,null),a(z,xs),a(z,V),a(V,Es),c(s,fs,l),c(s,K,l),a(K,Q),c(s,rs,l),x(f,s,l),c(s,v,l),c(s,Y,l),a(Y,Rs),c(s,Ts,l),x(O,s,l),c(s,is,l),c(s,I,l),a(I,us),a(I,ms),a(ms,ds),a(I,Ns),a(I,Xs),a(Xs,Ba),a(I,Ja),a(I,Zs),a(Zs,Ga),a(I,Ka),a(I,sa),a(sa,Qa),a(I,Xa),c(s,$a,l),x(qs,s,l),c(s,va,l),c(s,Us,l),a(Us,Za),c(s,ba,l),x(As,s,l),c(s,wa,l),c(s,as,l),a(as,st),a(as,aa),a(aa,at),a(as,tt),a(as,ta),a(ta,et),a(as,nt),c(s,ya,l),c(s,ps,l),a(ps,_s),a(_s,ea),x(Cs,ea,null),a(ps,lt),a(ps,na),a(na,ot),c(s,ka,l),c(s,Vs,l),a(Vs,rt),c(s,xa,l),x(Ds,s,l),c(s,Ea,l),c(s,Ys,l),a(Ys,it),c(s,Ta,l),c(s,ts,l),a(ts,Ps),a(Ps,pt),a(Ps,la),a(la,ct),a(Ps,ht),a(ts,ft),a(ts,Is),a(Is,ut),a(Is,oa),a(oa,mt),a(Is,dt),a(ts,_t),a(ts,ra),a(ra,gt),c(s,qa,l),x(Ss,s,l),c(s,Aa,l),c(s,es,l),a(es,jt),a(es,Ls),a(Ls,ia),a(ia,$t),a(es,vt),a(es,pa),a(pa,bt),a(es,wt),c(s,Ca,l),x(Ms,s,l),c(s,Da,l),c(s,H,l),a(H,yt),a(H,Hs),a(Hs,kt),a(H,xt),a(H,ca),a(ca,Et),a(H,Tt),a(H,ha),a(ha,qt),a(H,At),c(s,Pa,l),c(s,ns,l),a(ns,Ct),a(ns,fa),a(fa,Dt),a(ns,Pt),a(ns,ua),a(ua,It),a(ns,St),c(s,Ia,l),x(Ws,s,l),c(s,Sa,l),c(s,gs,l),a(gs,Lt),a(gs,ma),a(ma,Mt),a(gs,Wt),c(s,La,l),x(Os,s,l),c(s,Ma,l),c(s,cs,l),a(cs,js),a(js,da),x(Fs,da,null),a(cs,Ot),a(cs,_a),a(_a,Ft),c(s,Wa,l),x($s,s,l),c(s,Oa,l),x(vs,s,l),Fa=!0},p(s,[l]){const zs={};l&2&&(zs.$$scope={dirty:l,ctx:s}),N.$set(zs);const ga={};l&2&&(ga.$$scope={dirty:l,ctx:s}),$s.$set(ga);const ja={};l&2&&(ja.$$scope={dirty:l,ctx:s}),vs.$set(ja)},i(s){Fa||(E(_.$$.fragment,s),E(P.$$.fragment,s),E(N.$$.fragment,s),E(M.$$.fragment,s),E(f.$$.fragment,s),E(O.$$.fragment,s),E(qs.$$.fragment,s),E(As.$$.fragment,s),E(Cs.$$.fragment,s),E(Ds.$$.fragment,s),E(Ss.$$.fragment,s),E(Ms.$$.fragment,s),E(Ws.$$.fragment,s),E(Os.$$.fragment,s),E(Fs.$$.fragment,s),E($s.$$.fragment,s),E(vs.$$.fragment,s),Fa=!0)},o(s){T(_.$$.fragment,s),T(P.$$.fragment,s),T(N.$$.fragment,s),T(M.$$.fragment,s),T(f.$$.fragment,s),T(O.$$.fragment,s),T(qs.$$.fragment,s),T(As.$$.fragment,s),T(Cs.$$.fragment,s),T(Ds.$$.fragment,s),T(Ss.$$.fragment,s),T(Ms.$$.fragment,s),T(Ws.$$.fragment,s),T(Os.$$.fragment,s),T(Fs.$$.fragment,s),T($s.$$.fragment,s),T(vs.$$.fragment,s),Fa=!1},d(s){t(p),s&&t($),s&&t(h),q(_),s&&t(j),q(P,s),s&&t(D),s&&t(B),s&&t(hs),s&&t(S),s&&t(R),q(N,s),s&&t(G),s&&t(z),q(M),s&&t(fs),s&&t(K),s&&t(rs),q(f,s),s&&t(v),s&&t(Y),s&&t(Ts),q(O,s),s&&t(is),s&&t(I),s&&t($a),q(qs,s),s&&t(va),s&&t(Us),s&&t(ba),q(As,s),s&&t(wa),s&&t(as),s&&t(ya),s&&t(ps),q(Cs),s&&t(ka),s&&t(Vs),s&&t(xa),q(Ds,s),s&&t(Ea),s&&t(Ys),s&&t(Ta),s&&t(ts),s&&t(qa),q(Ss,s),s&&t(Aa),s&&t(es),s&&t(Ca),q(Ms,s),s&&t(Da),s&&t(H),s&&t(Pa),s&&t(ns),s&&t(Ia),q(Ws,s),s&&t(Sa),s&&t(gs),s&&t(La),q(Os,s),s&&t(Ma),s&&t(cs),q(Fs),s&&t(Wa),q($s,s),s&&t(Oa),q(vs,s)}}}const Me={local:"automatic-speech-recognition",sections:[{local:"load-timit-dataset",title:"Load TIMIT dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Automatic speech recognition"};function We(F,p,$){let{fw:h}=p;return F.$$set=g=>{"fw"in g&&$(0,h=g.fw)},[h]}class Ve extends ye{constructor(p){super();ke(this,p,We,Le,xe,{fw:0})}}export{Ve as default,Me as metadata};
