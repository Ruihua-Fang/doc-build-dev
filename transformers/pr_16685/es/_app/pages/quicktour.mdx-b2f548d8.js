import{D as Yi,S as Ue,i as Ve,s as Ke,O,P as L,a as n,d as o,b as u,g as h,F as s,L as ne,t as i,h as c,e as f,w as P,k as w,c as d,x as D,m as E,y as S,Q as dn,q as A,o as C,B as F,n as ca,p as pa,Y as Zi,v as Gi,Z as Mo,X as Wi,V as Qi,H as hs,I as $s,J as _s,K as gs,M as Xi}from"../chunks/vendor-c570b7f7.js";import{T as ia}from"../chunks/Tip-4965f0b6.js";import{Y as Ii}from"../chunks/Youtube-ea859fc9.js";import{I as gt}from"../chunks/IconCopyLink-8ab47bfe.js";import{C as oe}from"../chunks/CodeBlock-5b876388.js";import{D as ec}from"../chunks/DocNotebookDropdown-e51be72e.js";var Fe=(m=>(m.OPEN="OPEN",m.CLOSED="CLOSED",m.HASHASHLINK="HASHASHLINK",m))(Fe||{});const fn={};function tc(m){return fn[m]||(fn[m]=Yi("OPEN")),fn[m]}function ac(m){let e,l,t,r,p,_;return{c(){e=O("svg"),l=O("defs"),t=O("clipPath"),r=O("rect"),p=O("g"),_=O("path"),this.h()},l(g){e=L(g,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var v=n(e);l=L(v,"defs",{});var k=n(l);t=L(k,"clipPath",{id:!0});var z=n(t);r=L(z,"rect",{x:!0,y:!0,width:!0,height:!0,fill:!0}),n(r).forEach(o),z.forEach(o),k.forEach(o),p=L(v,"g",{"clip-path":!0});var y=n(p);_=L(y,"path",{d:!0,fill:!0}),n(_).forEach(o),y.forEach(o),v.forEach(o),this.h()},h(){u(r,"x","3.05"),u(r,"y","0.5"),u(r,"width","25.73"),u(r,"height","31"),u(r,"fill","none"),u(t,"id","a"),u(_,"d","M24.94,9.51a12.81,12.81,0,0,1,0,18.16,12.68,12.68,0,0,1-18,0,12.81,12.81,0,0,1,0-18.16l9-9V5l-.84.83-6,6a9.58,9.58,0,1,0,13.55,0ZM20.44,9a1.68,1.68,0,1,1,1.67-1.67A1.68,1.68,0,0,1,20.44,9Z"),u(_,"fill","#ee4c2c"),u(p,"clip-path","url(#a)"),u(e,"class",m[0]),u(e,"xmlns","http://www.w3.org/2000/svg"),u(e,"xmlns:xlink","http://www.w3.org/1999/xlink"),u(e,"aria-hidden","true"),u(e,"focusable","false"),u(e,"role","img"),u(e,"width","1em"),u(e,"height","1em"),u(e,"preserveAspectRatio","xMidYMid meet"),u(e,"viewBox","0 0 32 32")},m(g,v){h(g,e,v),s(e,l),s(l,t),s(t,r),s(e,p),s(p,_)},p(g,[v]){v&1&&u(e,"class",g[0])},i:ne,o:ne,d(g){g&&o(e)}}}function sc(m,e,l){let{classNames:t=""}=e;return m.$$set=r=>{"classNames"in r&&l(0,t=r.classNames)},[t]}class oc extends Ue{constructor(e){super();Ve(this,e,sc,ac,Ke,{classNames:0})}}function rc(m){let e,l,t,r;return{c(){e=O("svg"),l=O("path"),t=O("path"),r=O("path"),this.h()},l(p){e=L(p,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var _=n(e);l=L(_,"path",{d:!0,fill:!0}),n(l).forEach(o),t=L(_,"path",{d:!0,fill:!0}),n(t).forEach(o),r=L(_,"path",{d:!0,fill:!0}),n(r).forEach(o),_.forEach(o),this.h()},h(){u(l,"d","M145.726 42.065v42.07l72.861 42.07v-42.07l-72.86-42.07zM0 84.135v42.07l36.43 21.03V105.17L0 84.135zm109.291 21.035l-36.43 21.034v126.2l36.43 21.035v-84.135l36.435 21.035v-42.07l-36.435-21.034V105.17z"),u(l,"fill","#E55B2D"),u(t,"d","M145.726 42.065L36.43 105.17v42.065l72.861-42.065v42.065l36.435-21.03v-84.14zM255.022 63.1l-36.435 21.035v42.07l36.435-21.035V63.1zm-72.865 84.135l-36.43 21.035v42.07l36.43-21.036v-42.07zm-36.43 63.104l-36.436-21.035v84.135l36.435-21.035V210.34z"),u(t,"fill","#ED8E24"),u(r,"d","M145.726 0L0 84.135l36.43 21.035l109.296-63.105l72.861 42.07L255.022 63.1L145.726 0zm0 126.204l-36.435 21.03l36.435 21.036l36.43-21.035l-36.43-21.03z"),u(r,"fill","#F8BF3C"),u(e,"class",m[0]),u(e,"xmlns","http://www.w3.org/2000/svg"),u(e,"xmlns:xlink","http://www.w3.org/1999/xlink"),u(e,"aria-hidden","true"),u(e,"focusable","false"),u(e,"role","img"),u(e,"width","0.94em"),u(e,"height","1em"),u(e,"preserveAspectRatio","xMidYMid meet"),u(e,"viewBox","0 0 256 274")},m(p,_){h(p,e,_),s(e,l),s(e,t),s(e,r)},p(p,[_]){_&1&&u(e,"class",p[0])},i:ne,o:ne,d(p){p&&o(e)}}}function lc(m,e,l){let{classNames:t=""}=e;return m.$$set=r=>{"classNames"in r&&l(0,t=r.classNames)},[t]}class nc extends Ue{constructor(e){super();Ve(this,e,lc,rc,Ke,{classNames:0})}}function ic(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R,N,V,b,T,G,U,I,K,Z,B,Y,re,ae,se,ue,ie,fe,le,Q,ce,de,M,H,ee,x,J,pe;return{c(){e=O("svg"),l=O("style"),t=i(`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),r=O("g"),p=O("path"),_=O("path"),g=O("path"),v=O("path"),k=O("path"),z=O("path"),y=O("path"),j=O("path"),q=O("g"),R=O("path"),N=O("path"),V=O("path"),b=O("g"),T=O("path"),G=O("path"),U=O("path"),I=O("g"),K=O("path"),Z=O("path"),B=O("g"),Y=O("path"),re=O("path"),ae=O("path"),se=O("path"),ue=O("path"),ie=O("path"),fe=O("path"),le=O("path"),Q=O("g"),ce=O("path"),de=O("path"),M=O("path"),H=O("path"),ee=O("g"),x=O("path"),J=O("path"),pe=O("path"),this.h()},l($e){e=L($e,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var W=n(e);l=L(W,"style",{});var ke=n(l);t=c(ke,`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),ke.forEach(o),r=L(W,"g",{fill:!0,class:!0});var te=n(r);p=L(te,"path",{d:!0}),n(p).forEach(o),_=L(te,"path",{d:!0}),n(_).forEach(o),g=L(te,"path",{d:!0}),n(g).forEach(o),v=L(te,"path",{d:!0}),n(v).forEach(o),k=L(te,"path",{d:!0}),n(k).forEach(o),z=L(te,"path",{d:!0}),n(z).forEach(o),y=L(te,"path",{d:!0}),n(y).forEach(o),j=L(te,"path",{d:!0}),n(j).forEach(o),te.forEach(o),q=L(W,"g",{fill:!0,class:!0});var je=n(q);R=L(je,"path",{d:!0}),n(R).forEach(o),N=L(je,"path",{d:!0}),n(N).forEach(o),V=L(je,"path",{d:!0}),n(V).forEach(o),je.forEach(o),b=L(W,"g",{fill:!0,class:!0});var ve=n(b);T=L(ve,"path",{d:!0}),n(T).forEach(o),G=L(ve,"path",{d:!0}),n(G).forEach(o),ve.forEach(o),U=L(W,"path",{d:!0,fill:!0,class:!0}),n(U).forEach(o),I=L(W,"g",{fill:!0,class:!0});var Ae=n(I);K=L(Ae,"path",{d:!0}),n(K).forEach(o),Z=L(Ae,"path",{d:!0}),n(Z).forEach(o),Ae.forEach(o),B=L(W,"g",{fill:!0,class:!0});var _e=n(B);Y=L(_e,"path",{d:!0}),n(Y).forEach(o),re=L(_e,"path",{d:!0}),n(re).forEach(o),ae=L(_e,"path",{d:!0}),n(ae).forEach(o),se=L(_e,"path",{d:!0}),n(se).forEach(o),ue=L(_e,"path",{d:!0}),n(ue).forEach(o),ie=L(_e,"path",{d:!0}),n(ie).forEach(o),fe=L(_e,"path",{d:!0}),n(fe).forEach(o),_e.forEach(o),le=L(W,"path",{d:!0,fill:!0,class:!0}),n(le).forEach(o),Q=L(W,"g",{fill:!0,class:!0});var X=n(Q);ce=L(X,"path",{d:!0}),n(ce).forEach(o),de=L(X,"path",{d:!0}),n(de).forEach(o),M=L(X,"path",{d:!0}),n(M).forEach(o),H=L(X,"path",{d:!0}),n(H).forEach(o),X.forEach(o),ee=L(W,"g",{fill:!0,class:!0});var ze=n(ee);x=L(ze,"path",{d:!0}),n(x).forEach(o),J=L(ze,"path",{d:!0}),n(J).forEach(o),pe=L(ze,"path",{d:!0}),n(pe).forEach(o),ze.forEach(o),W.forEach(o),this.h()},h(){u(p,"d","M50.5 130.4l-25 43.31h50l25-43.31h-50z"),u(_,"d","M.5 217.01l25-43.3h50l-25 43.3H.5z"),u(g,"d","M125.5 173.71h-50l-25 43.3h50l25-43.3z"),u(v,"d","M175.5 173.71h-50l-25 43.3h50l25-43.3z"),u(k,"d","M150.5 130.4l-25 43.31h50l25-43.31h-50z"),u(z,"d","M175.5 87.1l-25 43.3h50l25-43.3h-50z"),u(y,"d","M200.5 43.8l-25 43.3h50l25-43.3h-50z"),u(j,"d","M225.5.5l-25 43.3h50l25-43.3h-50z"),u(r,"fill","#5e97f6"),u(r,"class","J K"),u(R,"d","M.5 217.01l25 43.3h50l-25-43.3H.5z"),u(N,"d","M125.5 260.31h-50l-25-43.3h50l25 43.3z"),u(V,"d","M175.5 260.31h-50l-25-43.3h50l25 43.3z"),u(q,"fill","#2a56c6"),u(q,"class","J K"),u(T,"d","M200.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zm50-86.61l-25-43.3-25 43.3h50z"),u(G,"d","M250.5 43.8l-25 43.3 25 43.3 25-43.3-25-43.3z"),u(b,"fill","#00796b"),u(b,"class","J K"),u(U,"d","M125.5 173.71l-25-43.31-25 43.31h50z"),u(U,"fill","#3367d6"),u(U,"class","J K"),u(K,"d","M250.5 130.4h-50l-25 43.31h50l25-43.31z"),u(Z,"d","M300.5 130.4h-50l-25 43.31h50l25-43.31z"),u(I,"fill","#26a69a"),u(I,"class","J K"),u(Y,"d","M350.5 43.8L325.5.5l-25 43.3 25 43.3 25-43.3z"),u(re,"d","M375.5 87.1l-25-43.3-25 43.3 25 43.3 25-43.3z"),u(ae,"d","M400.5 130.4l-25-43.3-25 43.3 25 43.31 25-43.31z"),u(se,"d","M425.5 173.71l-25-43.31-25 43.31 25 43.3 25-43.3z"),u(ue,"d","M450.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zM425.5.5l-25 43.3 25 43.3 25-43.3-25-43.3z"),u(ie,"d","M375.5 87.1l25-43.3 25 43.3-25 43.3-25-43.3zm-25 43.3l-25 43.31 25 43.3 25-43.3-25-43.31z"),u(fe,"d","M325.5 260.31l-25-43.3 25-43.3 25 43.3-25 43.3z"),u(B,"fill","#9c27b0"),u(B,"class","J K"),u(le,"d","M275.5 260.31l-25-43.3h50l25 43.3h-50z"),u(le,"fill","#6a1b9a"),u(le,"class","J K"),u(ce,"d","M225.5 173.71h-50l25 43.3h50l-25-43.3z"),u(de,"d","M275.5 173.71h-50l25 43.3 25-43.3zm0-86.61l25 43.3h50l-25-43.3h-50z"),u(M,"d","M300.5 43.8h-50l25 43.3h50l-25-43.3zm125 216.51l-25-43.3h-50l25 43.3h50z"),u(H,"d","M375.5 173.71l-25 43.3h50l-25-43.3z"),u(Q,"fill","#00695c"),u(Q,"class","J K"),u(x,"d","M325.5.5h-50l-25 43.3h50l25-43.3zm0 173.21h-50l-25 43.3h50l25-43.3z"),u(J,"d","M350.5 130.4h-50l-25 43.31h50l25-43.31zM425.5.5h-50l-25 43.3h50l25-43.3z"),u(pe,"d","M375.5 87.1l-25-43.3h50l-25 43.3z"),u(ee,"fill","#ea80fc"),u(ee,"class","J K"),u(e,"class",m[0]),u(e,"xmlns","http://www.w3.org/2000/svg"),u(e,"xmlns:xlink","http://www.w3.org/1999/xlink"),u(e,"aria-hidden","true"),u(e,"focusable","false"),u(e,"role","img"),u(e,"width","1.73em"),u(e,"height","1em"),u(e,"preserveAspectRatio","xMidYMid meet"),u(e,"viewBox","0 0 451 260.81")},m($e,W){h($e,e,W),s(e,l),s(l,t),s(e,r),s(r,p),s(r,_),s(r,g),s(r,v),s(r,k),s(r,z),s(r,y),s(r,j),s(e,q),s(q,R),s(q,N),s(q,V),s(e,b),s(b,T),s(b,G),s(e,U),s(e,I),s(I,K),s(I,Z),s(e,B),s(B,Y),s(B,re),s(B,ae),s(B,se),s(B,ue),s(B,ie),s(B,fe),s(e,le),s(e,Q),s(Q,ce),s(Q,de),s(Q,M),s(Q,H),s(e,ee),s(ee,x),s(ee,J),s(ee,pe)},p($e,[W]){W&1&&u(e,"class",$e[0])},i:ne,o:ne,d($e){$e&&o(e)}}}function cc(m,e,l){let{classNames:t=""}=e;return m.$$set=r=>{"classNames"in r&&l(0,t=r.classNames)},[t]}class pc extends Ue{constructor(e){super();Ve(this,e,cc,ic,Ke,{classNames:0})}}function uc(m){let e,l;return{c(){e=O("svg"),l=O("path"),this.h()},l(t){e=L(t,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var r=n(e);l=L(r,"path",{d:!0,fill:!0}),n(l).forEach(o),r.forEach(o),this.h()},h(){u(l,"d","M0 4.50001C0.390979 2.37042 2.25728 0.756592 4.5 0.756592C6.74272 0.756592 8.60861 2.37042 9 4.50001C8.60902 6.62959 6.74272 8.24342 4.5 8.24342C2.25728 8.24342 0.391395 6.62959 0 4.50001ZM4.5 6.57968C5.05156 6.57968 5.58054 6.36057 5.97055 5.97056C6.36057 5.58054 6.57967 5.05157 6.57967 4.50001C6.57967 3.94844 6.36057 3.41947 5.97055 3.02945C5.58054 2.63944 5.05156 2.42033 4.5 2.42033C3.94844 2.42033 3.41946 2.63944 3.02945 3.02945C2.63943 3.41947 2.42033 3.94844 2.42033 4.50001C2.42033 5.05157 2.63943 5.58054 3.02945 5.97056C3.41946 6.36057 3.94844 6.57968 4.5 6.57968ZM4.5 5.74781C4.16906 5.74781 3.85168 5.61635 3.61767 5.38234C3.38366 5.14833 3.2522 4.83094 3.2522 4.50001C3.2522 4.16907 3.38366 3.85168 3.61767 3.61767C3.85168 3.38367 4.16906 3.2522 4.5 3.2522C4.83094 3.2522 5.14832 3.38367 5.38233 3.61767C5.61634 3.85168 5.7478 4.16907 5.7478 4.50001C5.7478 4.83094 5.61634 5.14833 5.38233 5.38234C5.14832 5.61635 4.83094 5.74781 4.5 5.74781Z"),u(l,"fill","currentColor"),u(e,"class",m[0]),u(e,"width",m[1]),u(e,"height",m[1]),u(e,"viewBox","0 0 9 9"),u(e,"fill","currentColor"),u(e,"xmlns","http://www.w3.org/2000/svg")},m(t,r){h(t,e,r),s(e,l)},p(t,[r]){r&1&&u(e,"class",t[0]),r&2&&u(e,"width",t[1]),r&2&&u(e,"height",t[1])},i:ne,o:ne,d(t){t&&o(e)}}}function fc(m,e,l){let{classNames:t=""}=e,{size:r="1em"}=e;return m.$$set=p=>{"classNames"in p&&l(0,t=p.classNames),"size"in p&&l(1,r=p.size)},[t,r]}class dc extends Ue{constructor(e){super();Ve(this,e,fc,uc,Ke,{classNames:0,size:1})}}function mc(m){let e,l;return{c(){e=O("svg"),l=O("path"),this.h()},l(t){e=L(t,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var r=n(e);l=L(r,"path",{d:!0,fill:!0}),n(l).forEach(o),r.forEach(o),this.h()},h(){u(l,"d","M1.39125 1.9725L0.0883333 0.669997L0.677917 0.0804138L8.9275 8.33041L8.33792 8.91958L6.95875 7.54041C6.22592 8.00523 5.37572 8.25138 4.50792 8.25C2.26125 8.25 0.392083 6.63333 0 4.5C0.179179 3.52946 0.667345 2.64287 1.39167 1.9725H1.39125ZM5.65667 6.23833L5.04667 5.62833C4.81335 5.73996 4.55116 5.77647 4.29622 5.73282C4.04129 5.68918 3.80617 5.56752 3.62328 5.38463C3.44039 5.20175 3.31874 4.96663 3.27509 4.71169C3.23144 4.45676 3.26795 4.19456 3.37958 3.96125L2.76958 3.35125C2.50447 3.75187 2.38595 4.2318 2.4341 4.70978C2.48225 5.18777 2.6941 5.63442 3.0338 5.97411C3.37349 6.31381 3.82015 6.52567 4.29813 6.57382C4.77611 6.62197 5.25605 6.50345 5.65667 6.23833ZM2.83042 1.06666C3.35 0.862497 3.91625 0.749997 4.50792 0.749997C6.75458 0.749997 8.62375 2.36666 9.01583 4.5C8.88816 5.19404 8.60119 5.84899 8.1775 6.41333L6.56917 4.805C6.61694 4.48317 6.58868 4.15463 6.48664 3.84569C6.3846 3.53675 6.21162 3.256 5.98156 3.02594C5.7515 2.79588 5.47075 2.6229 5.16181 2.52086C4.85287 2.41882 4.52433 2.39056 4.2025 2.43833L2.83042 1.06708V1.06666Z"),u(l,"fill","currentColor"),u(e,"class",m[0]),u(e,"width",m[1]),u(e,"height",m[1]),u(e,"viewBox","0 0 10 9"),u(e,"fill","currentColor"),u(e,"xmlns","http://www.w3.org/2000/svg")},m(t,r){h(t,e,r),s(e,l)},p(t,[r]){r&1&&u(e,"class",t[0]),r&2&&u(e,"width",t[1]),r&2&&u(e,"height",t[1])},i:ne,o:ne,d(t){t&&o(e)}}}function hc(m,e,l){let{classNames:t=""}=e,{size:r="1em"}=e;return m.$$set=p=>{"classNames"in p&&l(0,t=p.classNames),"size"in p&&l(1,r=p.size)},[t,r]}class $c extends Ue{constructor(e){super();Ve(this,e,hc,mc,Ke,{classNames:0,size:1})}}const{window:_c}=Wi;function Hi(m){let e,l,t,r,p,_,g,v,k,z;return l=new $c({props:{size:"0.9em"}}),{c(){e=f("div"),P(l.$$.fragment),t=w(),r=f("span"),p=i("Hide "),_=i(m[3]),g=i(" content"),this.h()},l(y){e=d(y,"DIV",{class:!0});var j=n(e);D(l.$$.fragment,j),t=E(j),r=d(j,"SPAN",{});var q=n(r);p=c(q,"Hide "),_=c(q,m[3]),g=c(q," content"),q.forEach(o),j.forEach(o),this.h()},h(){u(e,"class","cursor-pointer flex items-center justify-center space-x-1 text-sm px-2 bg-white dark:bg-gray-950 hover:underline leading-none")},m(y,j){h(y,e,j),S(l,e,null),s(e,t),s(e,r),s(r,p),s(r,_),s(r,g),v=!0,k||(z=dn(e,"click",m[5]),k=!0)},p:ne,i(y){v||(A(l.$$.fragment,y),v=!0)},o(y){C(l.$$.fragment,y),v=!1},d(y){y&&o(e),F(l),k=!1,z()}}}function gc(m){let e,l;const t=m[10].default,r=hs(t,m,m[9],null);return{c(){e=f("div"),r&&r.c(),this.h()},l(p){e=d(p,"DIV",{class:!0});var _=n(e);r&&r.l(_),_.forEach(o),this.h()},h(){u(e,"class","framework-content")},m(p,_){h(p,e,_),r&&r.m(e,null),l=!0},p(p,_){r&&r.p&&(!l||_&512)&&$s(r,t,p,p[9],l?gs(t,p[9],_,null):_s(p[9]),null)},i(p){l||(A(r,p),l=!0)},o(p){C(r,p),l=!1},d(p){p&&o(e),r&&r.d(p)}}}function vc(m){let e,l,t,r,p,_,g,v,k,z;return l=new dc({props:{size:"0.9em"}}),{c(){e=f("div"),P(l.$$.fragment),t=w(),r=f("span"),p=i("Show "),_=i(m[3]),g=i(" content"),this.h()},l(y){e=d(y,"DIV",{class:!0});var j=n(e);D(l.$$.fragment,j),t=E(j),r=d(j,"SPAN",{});var q=n(r);p=c(q,"Show "),_=c(q,m[3]),g=c(q," content"),q.forEach(o),j.forEach(o),this.h()},h(){u(e,"class","cursor-pointer mt-[-12.5px] flex items-center justify-center space-x-1 py-4 text-sm hover:underline leading-none")},m(y,j){h(y,e,j),S(l,e,null),s(e,t),s(e,r),s(r,p),s(r,_),s(r,g),v=!0,k||(z=dn(e,"click",m[5]),k=!0)},p:ne,i(y){v||(A(l.$$.fragment,y),v=!0)},o(y){C(l.$$.fragment,y),v=!1},d(y){y&&o(e),F(l),k=!1,z()}}}function bc(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R;var N=m[2];function V(I){return{}}N&&(r=new N(V()));let b=!m[1]&&Hi(m);const T=[vc,gc],G=[];function U(I,K){return I[1]?0:1}return z=U(m),y=G[z]=T[z](m),{c(){e=f("div"),l=f("div"),t=f("div"),r&&P(r.$$.fragment),p=w(),_=f("span"),g=i(m[3]),v=w(),b&&b.c(),k=w(),y.c(),this.h()},l(I){e=d(I,"DIV",{class:!0});var K=n(e);l=d(K,"DIV",{class:!0});var Z=n(l);t=d(Z,"DIV",{class:!0});var B=n(t);r&&D(r.$$.fragment,B),p=E(B),_=d(B,"SPAN",{});var Y=n(_);g=c(Y,m[3]),Y.forEach(o),B.forEach(o),v=E(Z),b&&b.l(Z),Z.forEach(o),k=E(K),y.l(K),K.forEach(o),this.h()},h(){u(t,"class","flex px-1 items-center space-x-1 bg-white dark:bg-gray-950"),u(l,"class","flex h-[22px] mt-[-12.5px] justify-between leading-none"),u(e,"class","border border-gray-200 rounded-xl px-4 relative")},m(I,K){h(I,e,K),s(e,l),s(l,t),r&&S(r,t,null),s(t,p),s(t,_),s(_,g),s(l,v),b&&b.m(l,null),s(e,k),G[z].m(e,null),m[11](e),j=!0,q||(R=dn(_c,"hashchange",m[6]),q=!0)},p(I,[K]){if(N!==(N=I[2])){if(r){ca();const B=r;C(B.$$.fragment,1,0,()=>{F(B,1)}),pa()}N?(r=new N(V()),P(r.$$.fragment),A(r.$$.fragment,1),S(r,t,p)):r=null}I[1]?b&&(ca(),C(b,1,1,()=>{b=null}),pa()):b?(b.p(I,K),K&2&&A(b,1)):(b=Hi(I),b.c(),A(b,1),b.m(l,null));let Z=z;z=U(I),z===Z?G[z].p(I,K):(ca(),C(G[Z],1,1,()=>{G[Z]=null}),pa(),y=G[z],y?y.p(I,K):(y=G[z]=T[z](I),y.c()),A(y,1),y.m(e,null))},i(I){j||(r&&A(r.$$.fragment,I),A(b),A(y),j=!0)},o(I){r&&C(r.$$.fragment,I),C(b),C(y),j=!1},d(I){I&&o(e),r&&F(r),b&&b.d(),G[z].d(),m[11](null),q=!1,R()}}}function wc(m,e,l){let t,r,{$$slots:p={},$$scope:_}=e,{framework:g}=e,v,k=new Set;const z={pytorch:{Icon:oc,label:"Pytorch"},tensorflow:{Icon:nc,label:"TensorFlow"},jax:{Icon:pc,label:"JAX"}},{Icon:y,label:j}=z[g],q=`hf_doc_framework_${g}_is_hidden`,R=tc(g);Zi(m,R,T=>l(8,r=T));function N(){Mo(R,r=r!==Fe.CLOSED?Fe.CLOSED:Fe.OPEN,r),localStorage.setItem(q,r)}function V(){const T=window.location.hash.slice(1);k.has(T)&&(Mo(R,r=Fe.HASHASHLINK,r),localStorage.setItem(q,r))}Gi(()=>{const T=window.location.hash.slice(1),G="header-link",U=v.querySelectorAll(`.${G}`);k=new Set([...U].map(K=>K.id));const I=localStorage.getItem(q);k.has(T)?Mo(R,r=Fe.HASHASHLINK,r):I===Fe.CLOSED&&r!==Fe.HASHASHLINK&&Mo(R,r=Fe.CLOSED,r)});function b(T){Qi[T?"unshift":"push"](()=>{v=T,l(0,v)})}return m.$$set=T=>{"framework"in T&&l(7,g=T.framework),"$$scope"in T&&l(9,_=T.$$scope)},m.$$.update=()=>{m.$$.dirty&256&&l(1,t=r===Fe.CLOSED)},[v,t,y,j,R,N,V,g,r,_,p,b]}class mn extends Ue{constructor(e){super();Ve(this,e,wc,bc,Ke,{framework:7})}}const Ec=m=>({}),Ri=m=>({}),kc=m=>({}),Ui=m=>({}),jc=m=>({}),Vi=m=>({});function Ki(m){let e,l;return e=new mn({props:{framework:"pytorch",$$slots:{default:[yc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&16&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function yc(m){let e;const l=m[3].pytorch,t=hs(l,m,m[4],Vi);return{c(){t&&t.c()},l(r){t&&t.l(r)},m(r,p){t&&t.m(r,p),e=!0},p(r,p){t&&t.p&&(!e||p&16)&&$s(t,l,r,r[4],e?gs(l,r[4],p,jc):_s(r[4]),Vi)},i(r){e||(A(t,r),e=!0)},o(r){C(t,r),e=!1},d(r){t&&t.d(r)}}}function Bi(m){let e,l;return e=new mn({props:{framework:"tensorflow",$$slots:{default:[Ac]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&16&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Ac(m){let e;const l=m[3].tensorflow,t=hs(l,m,m[4],Ui);return{c(){t&&t.c()},l(r){t&&t.l(r)},m(r,p){t&&t.m(r,p),e=!0},p(r,p){t&&t.p&&(!e||p&16)&&$s(t,l,r,r[4],e?gs(l,r[4],p,kc):_s(r[4]),Ui)},i(r){e||(A(t,r),e=!0)},o(r){C(t,r),e=!1},d(r){t&&t.d(r)}}}function Ji(m){let e,l;return e=new mn({props:{framework:"jax",$$slots:{default:[zc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&16&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function zc(m){let e;const l=m[3].jax,t=hs(l,m,m[4],Ri);return{c(){t&&t.c()},l(r){t&&t.l(r)},m(r,p){t&&t.m(r,p),e=!0},p(r,p){t&&t.p&&(!e||p&16)&&$s(t,l,r,r[4],e?gs(l,r[4],p,Ec):_s(r[4]),Ri)},i(r){e||(A(t,r),e=!0)},o(r){C(t,r),e=!1},d(r){t&&t.d(r)}}}function Cc(m){let e,l,t,r,p=m[0]&&Ki(m),_=m[1]&&Bi(m),g=m[2]&&Ji(m);return{c(){e=f("div"),p&&p.c(),l=w(),_&&_.c(),t=w(),g&&g.c(),this.h()},l(v){e=d(v,"DIV",{class:!0});var k=n(e);p&&p.l(k),l=E(k),_&&_.l(k),t=E(k),g&&g.l(k),k.forEach(o),this.h()},h(){u(e,"class","space-y-10 py-6 2xl:py-8 2xl:-mx-4")},m(v,k){h(v,e,k),p&&p.m(e,null),s(e,l),_&&_.m(e,null),s(e,t),g&&g.m(e,null),r=!0},p(v,[k]){v[0]?p?(p.p(v,k),k&1&&A(p,1)):(p=Ki(v),p.c(),A(p,1),p.m(e,l)):p&&(ca(),C(p,1,1,()=>{p=null}),pa()),v[1]?_?(_.p(v,k),k&2&&A(_,1)):(_=Bi(v),_.c(),A(_,1),_.m(e,t)):_&&(ca(),C(_,1,1,()=>{_=null}),pa()),v[2]?g?(g.p(v,k),k&4&&A(g,1)):(g=Ji(v),g.c(),A(g,1),g.m(e,null)):g&&(ca(),C(g,1,1,()=>{g=null}),pa())},i(v){r||(A(p),A(_),A(g),r=!0)},o(v){C(p),C(_),C(g),r=!1},d(v){v&&o(e),p&&p.d(),_&&_.d(),g&&g.d()}}}function qc(m,e,l){let{$$slots:t={},$$scope:r}=e,{pytorch:p=!1}=e,{tensorflow:_=!1}=e,{jax:g=!1}=e;return m.$$set=v=>{"pytorch"in v&&l(0,p=v.pytorch),"tensorflow"in v&&l(1,_=v.tensorflow),"jax"in v&&l(2,g=v.jax),"$$scope"in v&&l(4,r=v.$$scope)},[p,_,g,t,r]}class na extends Ue{constructor(e){super();Ve(this,e,qc,Cc,Ke,{pytorch:0,tensorflow:1,jax:2})}}function Tc(m){let e;const l=m[1].default,t=hs(l,m,m[0],null);return{c(){t&&t.c()},l(r){t&&t.l(r)},m(r,p){t&&t.m(r,p),e=!0},p(r,[p]){t&&t.p&&(!e||p&1)&&$s(t,l,r,r[0],e?gs(l,r[0],p,null):_s(r[0]),null)},i(r){e||(A(t,r),e=!0)},o(r){C(t,r),e=!1},d(r){t&&t.d(r)}}}function xc(m,e,l){let{$$slots:t={},$$scope:r}=e;return m.$$set=p=>{"$$scope"in p&&l(0,r=p.$$scope)},[r,t]}class ye extends Ue{constructor(e){super();Ve(this,e,xc,Tc,Ke,{})}}function Mc(m){let e,l;return{c(){e=f("p"),l=i(`Todos los ejemplos de c\xF3digo presentados en la documentaci\xF3n tienen un bot\xF3n arriba a la izquierda para elegir entre Pytorch y TensorFlow.
Si no fuese as\xED, se espera que el c\xF3digo funcione para ambos backends sin ning\xFAn cambio.`)},l(t){e=d(t,"P",{});var r=n(e);l=c(r,`Todos los ejemplos de c\xF3digo presentados en la documentaci\xF3n tienen un bot\xF3n arriba a la izquierda para elegir entre Pytorch y TensorFlow.
Si no fuese as\xED, se espera que el c\xF3digo funcione para ambos backends sin ning\xFAn cambio.`),r.forEach(o)},m(t,r){h(t,e,r),s(e,l)},d(t){t&&o(e)}}}function Pc(m){let e,l,t,r,p,_,g,v;return{c(){e=f("p"),l=i("Para m\xE1s detalles acerca del "),t=f("code"),r=i("pipeline()"),p=i(" y tareas asociadas, consulta la documentaci\xF3n "),_=f("a"),g=i("aqu\xED"),v=i("."),this.h()},l(k){e=d(k,"P",{});var z=n(e);l=c(z,"Para m\xE1s detalles acerca del "),t=d(z,"CODE",{});var y=n(t);r=c(y,"pipeline()"),y.forEach(o),p=c(z," y tareas asociadas, consulta la documentaci\xF3n "),_=d(z,"A",{href:!0});var j=n(_);g=c(j,"aqu\xED"),j.forEach(o),v=c(z,"."),z.forEach(o),this.h()},h(){u(_,"href","./main_classes/pipelines")},m(k,z){h(k,e,z),s(e,l),s(e,t),s(t,r),s(e,p),s(e,_),s(_,g),s(e,v)},d(k){k&&o(e)}}}function Sc(m){let e,l;return e=new oe({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p:ne,i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Fc(m){let e,l;return e=new ye({props:{$$slots:{default:[Sc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Dc(m){let e,l;return e=new oe({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p:ne,i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Oc(m){let e,l;return e=new ye({props:{$$slots:{default:[Dc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Lc(m){let e,l,t,r,p,_,g,v,k,z,y;return z=new oe({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){e=f("p"),l=i("Usa "),t=f("code"),r=i("AutoModelForSequenceClassification"),p=i(" y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=f("code"),g=i("AutoClass"),v=i(" debajo):"),k=w(),P(z.$$.fragment)},l(j){e=d(j,"P",{});var q=n(e);l=c(q,"Usa "),t=d(q,"CODE",{});var R=n(t);r=c(R,"AutoModelForSequenceClassification"),R.forEach(o),p=c(q," y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=d(q,"CODE",{});var N=n(_);g=c(N,"AutoClass"),N.forEach(o),v=c(q," debajo):"),q.forEach(o),k=E(j),D(z.$$.fragment,j)},m(j,q){h(j,e,q),s(e,l),s(e,t),s(t,r),s(e,p),s(e,_),s(_,g),s(e,v),h(j,k,q),S(z,j,q),y=!0},p:ne,i(j){y||(A(z.$$.fragment,j),y=!0)},o(j){C(z.$$.fragment,j),y=!1},d(j){j&&o(e),j&&o(k),F(z,j)}}}function Nc(m){let e,l;return e=new ye({props:{$$slots:{default:[Lc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Ic(m){let e,l,t,r,p,_,g,v,k,z,y;return z=new oe({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){e=f("p"),l=i("Usa "),t=f("code"),r=i("TFAutoModelForSequenceClassification"),p=i(" y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=f("code"),g=i("TFAutoClass"),v=i(" debajo):"),k=w(),P(z.$$.fragment)},l(j){e=d(j,"P",{});var q=n(e);l=c(q,"Usa "),t=d(q,"CODE",{});var R=n(t);r=c(R,"TFAutoModelForSequenceClassification"),R.forEach(o),p=c(q," y [\u2018AutoTokenizer\u2019] para cargar un modelo preentrenado y un tokenizador asociado (m\xE1s en un "),_=d(q,"CODE",{});var N=n(_);g=c(N,"TFAutoClass"),N.forEach(o),v=c(q," debajo):"),q.forEach(o),k=E(j),D(z.$$.fragment,j)},m(j,q){h(j,e,q),s(e,l),s(e,t),s(t,r),s(e,p),s(e,_),s(_,g),s(e,v),h(j,k,q),S(z,j,q),y=!0},p:ne,i(j){y||(A(z.$$.fragment,j),y=!0)},o(j){C(z.$$.fragment,j),y=!1},d(j){j&&o(e),j&&o(k),F(z,j)}}}function Hc(m){let e,l;return e=new ye({props:{$$slots:{default:[Ic]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Rc(m){let e,l;return e=new oe({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p:ne,i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Uc(m){let e,l;return e=new ye({props:{$$slots:{default:[Rc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Vc(m){let e,l;return e=new oe({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p:ne,i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Kc(m){let e,l;return e=new ye({props:{$$slots:{default:[Vc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Bc(m){let e,l,t,r,p,_,g,v;return{c(){e=f("p"),l=i("Ve el "),t=f("a"),r=i("task summary"),p=i(" para revisar qu\xE9 clase del "),_=f("code"),g=i("AutoModel"),v=i(" deber\xEDas usar para cada tarea."),this.h()},l(k){e=d(k,"P",{});var z=n(e);l=c(z,"Ve el "),t=d(z,"A",{href:!0});var y=n(t);r=c(y,"task summary"),y.forEach(o),p=c(z," para revisar qu\xE9 clase del "),_=d(z,"CODE",{});var j=n(_);g=c(j,"AutoModel"),j.forEach(o),v=c(z," deber\xEDas usar para cada tarea."),z.forEach(o),this.h()},h(){u(t,"href","./task_summary")},m(k,z){h(k,e,z),s(e,l),s(e,t),s(t,r),s(e,p),s(e,_),s(_,g),s(e,v)},d(k){k&&o(e)}}}function Jc(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R,N,V,b,T,G,U,I,K,Z,B,Y,re,ae,se,ue,ie,fe,le,Q,ce,de,M,H,ee;return V=new oe({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),T=new ia({props:{$$slots:{default:[Bc]},$$scope:{ctx:m}}}),re=new oe({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),H=new oe({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){e=f("p"),l=i("\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),t=f("code"),r=i("AutoModel"),p=i(" como cargar\xEDas un "),_=f("code"),g=i("AutoTokenizer"),v=i(". La \xFAnica diferencia es seleccionar el "),k=f("code"),z=i("AutoModel"),y=i(" correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),j=f("code"),q=i("AutoModelForSequenceClassification"),R=i(":"),N=w(),P(V.$$.fragment),b=w(),P(T.$$.fragment),G=w(),U=f("p"),I=i("Ahora puedes pasar tu lote (batch) preprocesado de inputs directamente al modelo. Solo tienes que desempacar el diccionario a\xF1adiendo "),K=f("code"),Z=i("**"),B=i(":"),Y=w(),P(re.$$.fragment),ae=w(),se=f("p"),ue=i("El modelo producir\xE1 las activaciones finales en el atributo "),ie=f("code"),fe=i("logits"),le=i(". Aplica la funci\xF3n softmax a "),Q=f("code"),ce=i("logits"),de=i(" para obtener las probabilidades:"),M=w(),P(H.$$.fragment)},l(x){e=d(x,"P",{});var J=n(e);l=c(J,"\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),t=d(J,"CODE",{});var pe=n(t);r=c(pe,"AutoModel"),pe.forEach(o),p=c(J," como cargar\xEDas un "),_=d(J,"CODE",{});var $e=n(_);g=c($e,"AutoTokenizer"),$e.forEach(o),v=c(J,". La \xFAnica diferencia es seleccionar el "),k=d(J,"CODE",{});var W=n(k);z=c(W,"AutoModel"),W.forEach(o),y=c(J," correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),j=d(J,"CODE",{});var ke=n(j);q=c(ke,"AutoModelForSequenceClassification"),ke.forEach(o),R=c(J,":"),J.forEach(o),N=E(x),D(V.$$.fragment,x),b=E(x),D(T.$$.fragment,x),G=E(x),U=d(x,"P",{});var te=n(U);I=c(te,"Ahora puedes pasar tu lote (batch) preprocesado de inputs directamente al modelo. Solo tienes que desempacar el diccionario a\xF1adiendo "),K=d(te,"CODE",{});var je=n(K);Z=c(je,"**"),je.forEach(o),B=c(te,":"),te.forEach(o),Y=E(x),D(re.$$.fragment,x),ae=E(x),se=d(x,"P",{});var ve=n(se);ue=c(ve,"El modelo producir\xE1 las activaciones finales en el atributo "),ie=d(ve,"CODE",{});var Ae=n(ie);fe=c(Ae,"logits"),Ae.forEach(o),le=c(ve,". Aplica la funci\xF3n softmax a "),Q=d(ve,"CODE",{});var _e=n(Q);ce=c(_e,"logits"),_e.forEach(o),de=c(ve," para obtener las probabilidades:"),ve.forEach(o),M=E(x),D(H.$$.fragment,x)},m(x,J){h(x,e,J),s(e,l),s(e,t),s(t,r),s(e,p),s(e,_),s(_,g),s(e,v),s(e,k),s(k,z),s(e,y),s(e,j),s(j,q),s(e,R),h(x,N,J),S(V,x,J),h(x,b,J),S(T,x,J),h(x,G,J),h(x,U,J),s(U,I),s(U,K),s(K,Z),s(U,B),h(x,Y,J),S(re,x,J),h(x,ae,J),h(x,se,J),s(se,ue),s(se,ie),s(ie,fe),s(se,le),s(se,Q),s(Q,ce),s(se,de),h(x,M,J),S(H,x,J),ee=!0},p(x,J){const pe={};J&2&&(pe.$$scope={dirty:J,ctx:x}),T.$set(pe)},i(x){ee||(A(V.$$.fragment,x),A(T.$$.fragment,x),A(re.$$.fragment,x),A(H.$$.fragment,x),ee=!0)},o(x){C(V.$$.fragment,x),C(T.$$.fragment,x),C(re.$$.fragment,x),C(H.$$.fragment,x),ee=!1},d(x){x&&o(e),x&&o(N),F(V,x),x&&o(b),F(T,x),x&&o(G),x&&o(U),x&&o(Y),F(re,x),x&&o(ae),x&&o(se),x&&o(M),F(H,x)}}}function Gc(m){let e,l;return e=new ye({props:{$$slots:{default:[Jc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Yc(m){let e;return{c(){e=i("Ve el [task summary](./task_summary) para revisar qu\xE9 clase del `AutoModel`\n  deber\xEDas usar para cada tarea.")},l(l){e=c(l,"Ve el [task summary](./task_summary) para revisar qu\xE9 clase del `AutoModel`\n  deber\xEDas usar para cada tarea.")},m(l,t){h(l,e,t)},d(l){l&&o(e)}}}function Zc(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R,N,V,b,T,G,U,I,K,Z,B,Y,re,ae,se,ue,ie,fe,le,Q,ce,de;return V=new oe({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),T=new ia({props:{$$slots:{default:[Yc]},$$scope:{ctx:m}}}),Z=new oe({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ce=new oe({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf.math.round(tf_predictions * 10**4) / 10**4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf.math.<span class="hljs-built_in">round</span>(tf_predictions * <span class="hljs-number">10</span>**<span class="hljs-number">4</span>) / <span class="hljs-number">10</span>**<span class="hljs-number">4</span>)
tf.Tensor(
[[<span class="hljs-number">0.0021</span> <span class="hljs-number">0.0018</span> <span class="hljs-number">0.0116</span> <span class="hljs-number">0.2121</span> <span class="hljs-number">0.7725</span>]
 [<span class="hljs-number">0.2084</span> <span class="hljs-number">0.1826</span> <span class="hljs-number">0.1969</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.2365</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}),{c(){e=f("p"),l=i("\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),t=f("code"),r=i("TFAutoModel"),p=i(" como cargar\xEDas un "),_=f("code"),g=i("AutoTokenizer"),v=i(". La \xFAnica diferencia es seleccionar el "),k=f("code"),z=i("TFAutoModel"),y=i(" correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),j=f("code"),q=i("TFAutoModelForSequenceClassification"),R=i(":"),N=w(),P(V.$$.fragment),b=w(),P(T.$$.fragment),G=w(),U=f("p"),I=i("Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:"),K=w(),P(Z.$$.fragment),B=w(),Y=f("p"),re=i("El modelo producir\xE1 las activaciones finales en el atributo "),ae=f("code"),se=i("logits"),ue=i(". Aplica la funci\xF3n softmax a "),ie=f("code"),fe=i("logits"),le=i(" para obtener las probabilidades:"),Q=w(),P(ce.$$.fragment)},l(M){e=d(M,"P",{});var H=n(e);l=c(H,"\u{1F917} Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un "),t=d(H,"CODE",{});var ee=n(t);r=c(ee,"TFAutoModel"),ee.forEach(o),p=c(H," como cargar\xEDas un "),_=d(H,"CODE",{});var x=n(_);g=c(x,"AutoTokenizer"),x.forEach(o),v=c(H,". La \xFAnica diferencia es seleccionar el "),k=d(H,"CODE",{});var J=n(k);z=c(J,"TFAutoModel"),J.forEach(o),y=c(H," correcto para la tarea. Ya que est\xE1s clasificando texto, o secuencias, carga "),j=d(H,"CODE",{});var pe=n(j);q=c(pe,"TFAutoModelForSequenceClassification"),pe.forEach(o),R=c(H,":"),H.forEach(o),N=E(M),D(V.$$.fragment,M),b=E(M),D(T.$$.fragment,M),G=E(M),U=d(M,"P",{});var $e=n(U);I=c($e,"Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:"),$e.forEach(o),K=E(M),D(Z.$$.fragment,M),B=E(M),Y=d(M,"P",{});var W=n(Y);re=c(W,"El modelo producir\xE1 las activaciones finales en el atributo "),ae=d(W,"CODE",{});var ke=n(ae);se=c(ke,"logits"),ke.forEach(o),ue=c(W,". Aplica la funci\xF3n softmax a "),ie=d(W,"CODE",{});var te=n(ie);fe=c(te,"logits"),te.forEach(o),le=c(W," para obtener las probabilidades:"),W.forEach(o),Q=E(M),D(ce.$$.fragment,M)},m(M,H){h(M,e,H),s(e,l),s(e,t),s(t,r),s(e,p),s(e,_),s(_,g),s(e,v),s(e,k),s(k,z),s(e,y),s(e,j),s(j,q),s(e,R),h(M,N,H),S(V,M,H),h(M,b,H),S(T,M,H),h(M,G,H),h(M,U,H),s(U,I),h(M,K,H),S(Z,M,H),h(M,B,H),h(M,Y,H),s(Y,re),s(Y,ae),s(ae,se),s(Y,ue),s(Y,ie),s(ie,fe),s(Y,le),h(M,Q,H),S(ce,M,H),de=!0},p(M,H){const ee={};H&2&&(ee.$$scope={dirty:H,ctx:M}),T.$set(ee)},i(M){de||(A(V.$$.fragment,M),A(T.$$.fragment,M),A(Z.$$.fragment,M),A(ce.$$.fragment,M),de=!0)},o(M){C(V.$$.fragment,M),C(T.$$.fragment,M),C(Z.$$.fragment,M),C(ce.$$.fragment,M),de=!1},d(M){M&&o(e),M&&o(N),F(V,M),M&&o(b),F(T,M),M&&o(G),M&&o(U),M&&o(K),F(Z,M),M&&o(B),M&&o(Y),M&&o(Q),F(ce,M)}}}function Wc(m){let e,l;return e=new ye({props:{$$slots:{default:[Zc]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function Qc(m){let e,l,t,r,p;return{c(){e=f("p"),l=i("Todos los modelos de \u{1F917} Transformers (PyTorch o TensorFlow) producir\xE1n los tensores "),t=f("em"),r=i("antes"),p=i(` de la funci\xF3n de activaci\xF3n
final (como softmax) porque la funci\xF3n de activaci\xF3n final es com\xFAnmente fusionada con la p\xE9rdida.`)},l(_){e=d(_,"P",{});var g=n(e);l=c(g,"Todos los modelos de \u{1F917} Transformers (PyTorch o TensorFlow) producir\xE1n los tensores "),t=d(g,"EM",{});var v=n(t);r=c(v,"antes"),v.forEach(o),p=c(g,` de la funci\xF3n de activaci\xF3n
final (como softmax) porque la funci\xF3n de activaci\xF3n final es com\xFAnmente fusionada con la p\xE9rdida.`),g.forEach(o)},m(_,g){h(_,e,g),s(e,l),s(e,t),s(t,r),s(e,p)},d(_){_&&o(e)}}}function Xc(m){let e,l,t,r,p;return{c(){e=f("p"),l=i(`Los outputs del modelo de \u{1F917} Transformers son dataclasses especiales por lo que sus atributos pueden ser completados en un IDE.
Los outputs del modelo tambi\xE9n se comportan como tuplas o diccionarios (e.g., puedes indexar con un entero, un slice o una cadena) en cuyo caso los atributos que son `),t=f("code"),r=i("None"),p=i(" son ignorados.")},l(_){e=d(_,"P",{});var g=n(e);l=c(g,`Los outputs del modelo de \u{1F917} Transformers son dataclasses especiales por lo que sus atributos pueden ser completados en un IDE.
Los outputs del modelo tambi\xE9n se comportan como tuplas o diccionarios (e.g., puedes indexar con un entero, un slice o una cadena) en cuyo caso los atributos que son `),t=d(g,"CODE",{});var v=n(t);r=c(v,"None"),v.forEach(o),p=c(g," son ignorados."),g.forEach(o)},m(_,g){h(_,e,g),s(e,l),s(e,t),s(t,r),s(e,p)},d(_){_&&o(e)}}}function ep(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R,N,V;return g=new oe({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),N=new oe({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){e=f("p"),l=i("Una vez que se haya hecho fine-tuning a tu modelo puedes guardarlo con tu tokenizador usando "),t=f("code"),r=i("PreTrainedModel.save_pretrained()"),p=i(":"),_=w(),P(g.$$.fragment),v=w(),k=f("p"),z=i("Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=f("code"),j=i("PreTrainedModel.from_pretrained()"),q=i(":"),R=w(),P(N.$$.fragment)},l(b){e=d(b,"P",{});var T=n(e);l=c(T,"Una vez que se haya hecho fine-tuning a tu modelo puedes guardarlo con tu tokenizador usando "),t=d(T,"CODE",{});var G=n(t);r=c(G,"PreTrainedModel.save_pretrained()"),G.forEach(o),p=c(T,":"),T.forEach(o),_=E(b),D(g.$$.fragment,b),v=E(b),k=d(b,"P",{});var U=n(k);z=c(U,"Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=d(U,"CODE",{});var I=n(y);j=c(I,"PreTrainedModel.from_pretrained()"),I.forEach(o),q=c(U,":"),U.forEach(o),R=E(b),D(N.$$.fragment,b)},m(b,T){h(b,e,T),s(e,l),s(e,t),s(t,r),s(e,p),h(b,_,T),S(g,b,T),h(b,v,T),h(b,k,T),s(k,z),s(k,y),s(y,j),s(k,q),h(b,R,T),S(N,b,T),V=!0},p:ne,i(b){V||(A(g.$$.fragment,b),A(N.$$.fragment,b),V=!0)},o(b){C(g.$$.fragment,b),C(N.$$.fragment,b),V=!1},d(b){b&&o(e),b&&o(_),F(g,b),b&&o(v),b&&o(k),b&&o(R),F(N,b)}}}function tp(m){let e,l;return e=new ye({props:{$$slots:{default:[ep]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function ap(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R,N,V;return g=new oe({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),N=new oe({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){e=f("p"),l=i("Una vez que se haya hecho fine-tuning a tu modelo puedes guardarlo con tu tokenizador usando "),t=f("code"),r=i("TFPreTrainedModel.save_pretrained()"),p=i(":"),_=w(),P(g.$$.fragment),v=w(),k=f("p"),z=i("Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=f("code"),j=i("TFPreTrainedModel.from_pretrained()"),q=i(":"),R=w(),P(N.$$.fragment)},l(b){e=d(b,"P",{});var T=n(e);l=c(T,"Una vez que se haya hecho fine-tuning a tu modelo puedes guardarlo con tu tokenizador usando "),t=d(T,"CODE",{});var G=n(t);r=c(G,"TFPreTrainedModel.save_pretrained()"),G.forEach(o),p=c(T,":"),T.forEach(o),_=E(b),D(g.$$.fragment,b),v=E(b),k=d(b,"P",{});var U=n(k);z=c(U,"Cuando quieras usar el modelo otra vez c\xE1rgalo con "),y=d(U,"CODE",{});var I=n(y);j=c(I,"TFPreTrainedModel.from_pretrained()"),I.forEach(o),q=c(U,":"),U.forEach(o),R=E(b),D(N.$$.fragment,b)},m(b,T){h(b,e,T),s(e,l),s(e,t),s(t,r),s(e,p),h(b,_,T),S(g,b,T),h(b,v,T),h(b,k,T),s(k,z),s(k,y),s(y,j),s(k,q),h(b,R,T),S(N,b,T),V=!0},p:ne,i(b){V||(A(g.$$.fragment,b),A(N.$$.fragment,b),V=!0)},o(b){C(g.$$.fragment,b),C(N.$$.fragment,b),V=!1},d(b){b&&o(e),b&&o(_),F(g,b),b&&o(v),b&&o(k),b&&o(R),F(N,b)}}}function sp(m){let e,l;return e=new ye({props:{$$slots:{default:[ap]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function op(m){let e,l;return e=new oe({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p:ne,i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function rp(m){let e,l;return e=new ye({props:{$$slots:{default:[op]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function lp(m){let e,l;return e=new oe({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p:ne,i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function np(m){let e,l;return e=new ye({props:{$$slots:{default:[lp]},$$scope:{ctx:m}}}),{c(){P(e.$$.fragment)},l(t){D(e.$$.fragment,t)},m(t,r){S(e,t,r),l=!0},p(t,r){const p={};r&2&&(p.$$scope={dirty:r,ctx:t}),e.$set(p)},i(t){l||(A(e.$$.fragment,t),l=!0)},o(t){C(e.$$.fragment,t),l=!1},d(t){F(e,t)}}}function ip(m){let e,l,t,r,p,_,g,v,k,z,y,j,q,R,N,V,b,T,G,U,I,K,Z,B,Y,re,ae,se,ue,ie,fe,le,Q,ce,de,M,H,ee,x,J,pe,$e,W,ke,te,je,ve,Ae,_e,X,ze,Po,So,ua,Fo,Do,fa,Oo,Lo,da,No,Io,ma,Ho,Ro,ha,Uo,Vo,$a,Ko,Bo,_a,Jo,vs,vt,ga,Go,Yo,bs,Ce,va,Zo,Wo,ba,Qo,Xo,wa,er,ws,bt,Ea,tr,ar,Es,Be,ka,sr,or,ja,rr,ks,Je,js,De,Ge,ya,wt,lr,Aa,nr,ys,Ye,ir,za,cr,pr,As,Ht,ur,zs,Ze,Cs,We,fr,Ca,dr,mr,qs,Qe,hr,qa,$r,_r,Ts,Et,xs,Xe,gr,Ta,vr,br,Ms,kt,Ps,et,wr,Rt,Er,kr,Ss,Oe,tt,xa,jt,jr,Ma,yr,Fs,be,Ar,Pa,zr,Cr,yt,qr,Tr,Sa,xr,Mr,At,Pr,Sr,Ds,zt,Os,at,Ls,qe,Fr,Fa,Dr,Or,Da,Lr,Nr,Ns,Ct,Is,Te,Ir,Ut,Hr,Rr,Vt,Ur,Vr,Hs,Le,st,Oa,qt,Kr,La,Br,Rs,Tt,Us,me,Jr,Na,Gr,Yr,Ia,Zr,Wr,Ha,Qr,Xr,Kt,el,tl,Ra,al,sl,Ua,ol,rl,Vs,xe,ll,Va,nl,il,Ka,cl,pl,Ks,Ne,ot,Ba,xt,ul,Ja,fl,Bs,Me,dl,Ga,ml,hl,Bt,$l,_l,Js,rt,gl,Ya,vl,bl,Gs,Mt,Ys,lt,wl,Za,El,kl,Zs,Jt,jl,Ws,Pt,Qs,Gt,yl,Xs,nt,Yt,Zt,Al,zl,Cl,Wt,Qt,ql,Tl,eo,it,xl,Wa,Ml,Pl,to,ct,ao,pt,Sl,Xt,Fl,Dl,so,Ie,ut,Qa,St,Ol,Xa,Ll,oo,ft,ro,dt,lo,he,Nl,Ft,es,Il,Hl,Dt,ts,Rl,Ul,as,Vl,Kl,ss,Bl,Jl,Ot,Gl,Yl,ea,Zl,Wl,no,mt,io,He,ht,os,Lt,Ql,rs,Xl,co,$t,po,Pe,en,ls,tn,an,ns,sn,on,uo,_t,fo;return _=new gt({}),y=new ec({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/tensorflow/quicktour.ipynb"}]}}),K=new ia({props:{$$slots:{default:[Mc]},$$scope:{ctx:m}}}),ae=new gt({}),H=new Ii({props:{id:"tiZFewofSLM"}}),Je=new ia({props:{$$slots:{default:[Pc]},$$scope:{ctx:m}}}),wt=new gt({}),Ze=new na({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Oc],pytorch:[Fc]},$$scope:{ctx:m}}}),Et=new oe({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=reconocedor_de_voz.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=reconocedor_de_voz.feature_extractor.sampling_rate))'}}),kt=new oe({props:{code:`resultado = reconocedor_de_voz(dataset[:4]["audio"])
print([d["text"] for d in resultado])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>resultado = reconocedor_de_voz(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> resultado])
[<span class="hljs-string">&#x27;ahora buenas e a ver  tengo un problema como vuestra aplicaci\xF3n resulta que que quiero hacer una transferencia bancaria a una cuenta conocida pero me da error la aplicaci\xF3n a ver que a ver que puede ser&#x27;</span>, <span class="hljs-string">&#x27;la aplicaci\xF3n no cargue salda de mi nueva cuenta&#x27;</span>, <span class="hljs-string">&#x27;hola tengo un problema con la aplicaci\xF3n no carga y y tampoco veo que carga el saldo de mi cuenta nueva dice que la aplicaci\xF3n est\xE1 siendo reparada y ahora no puedo aceder a mi cuenta no necesito inmediatamente&#x27;</span>, <span class="hljs-string">&#x27;ora buena la aplicaci\xF3n no se carga la viladad no carga el saldo de mi cuenta nueva dice que la villadenta siendo reparada y oro no puede hacer a mi cuenta&#x27;</span>]`}}),jt=new gt({}),zt=new oe({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),at=new na({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Hc],pytorch:[Nc]},$$scope:{ctx:m}}}),Ct=new oe({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),qt=new gt({}),Tt=new Ii({props:{id:"AhChOFRegn4"}}),xt=new gt({}),Mt=new oe({props:{code:`from transformers import AutoTokenizer

nombre_del_modelo = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(nombre_del_modelo)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>nombre_del_modelo = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(nombre_del_modelo)`}}),Pt=new oe({props:{code:`encoding = tokenizer("Estamos muy felices de mostrarte la biblioteca de \u{1F917} Transformers.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;Estamos muy felices de mostrarte la biblioteca de \u{1F917} Transformers.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">10602</span>, <span class="hljs-number">14000</span>, <span class="hljs-number">13653</span>, <span class="hljs-number">43353</span>, <span class="hljs-number">10107</span>, <span class="hljs-number">10102</span>, <span class="hljs-number">47201</span>, <span class="hljs-number">10218</span>, <span class="hljs-number">10106</span>, <span class="hljs-number">18283</span>, <span class="hljs-number">10102</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ct=new na({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Kc],pytorch:[Uc]},$$scope:{ctx:m}}}),St=new gt({}),ft=new na({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Wc],pytorch:[Gc]},$$scope:{ctx:m}}}),dt=new ia({props:{$$slots:{default:[Qc]},$$scope:{ctx:m}}}),mt=new ia({props:{$$slots:{default:[Xc]},$$scope:{ctx:m}}}),Lt=new gt({}),$t=new na({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[sp],pytorch:[tp]},$$scope:{ctx:m}}}),_t=new na({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[np],pytorch:[rp]},$$scope:{ctx:m}}}),{c(){e=f("meta"),l=w(),t=f("h1"),r=f("a"),p=f("span"),P(_.$$.fragment),g=w(),v=f("span"),k=i("Quick tour"),z=w(),P(y.$$.fragment),j=w(),q=f("p"),R=i("\xA1Entra en marcha con los \u{1F917} Transformers! Comienza usando "),N=f("code"),V=i("pipeline()"),b=i(" para una inferencia veloz, carga un modelo preentrenado y un tokenizador con una "),T=f("a"),G=i("AutoClass"),U=i(" para resolver tu tarea de texto, visi\xF3n o audio."),I=w(),P(K.$$.fragment),Z=w(),B=f("h2"),Y=f("a"),re=f("span"),P(ae.$$.fragment),se=w(),ue=f("span"),ie=i("Pipeline"),fe=w(),le=f("p"),Q=f("code"),ce=i("pipeline()"),de=i(" es la forma m\xE1s f\xE1cil de usar un modelo preentrenado para una tarea dada."),M=w(),P(H.$$.fragment),ee=w(),x=f("p"),J=i("El "),pe=f("code"),$e=i("pipeline()"),W=i(" soporta muchas tareas comunes listas para usar:"),ke=w(),te=f("p"),je=f("strong"),ve=i("Texto"),Ae=i(":"),_e=w(),X=f("ul"),ze=f("li"),Po=i("An\xE1lisis de Sentimiento (Sentiment Analysis, en ingl\xE9s): clasifica la polaridad de un texto dado."),So=w(),ua=f("li"),Fo=i("Generaci\xF3n de Texto (Text Generation, en ingl\xE9s): genera texto a partir de un input dado."),Do=w(),fa=f("li"),Oo=i("Reconocimiento de Entidades (Name Entity Recognition o NER, en ingl\xE9s): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicaci\xF3n, etc.)."),Lo=w(),da=f("li"),No=i("Responder Preguntas (Question answering, en ingl\xE9s): extrae la respuesta del contexto dado un contexto y una pregunta."),Io=w(),ma=f("li"),Ho=i("Rellenar M\xE1scara (Fill-mask, en ingl\xE9s): rellena el espacio faltante dado un texto con palabras enmascaradas."),Ro=w(),ha=f("li"),Uo=i("Resumir (Summarization, en ingl\xE9s): genera un resumen de una secuencia larga de texto o un documento."),Vo=w(),$a=f("li"),Ko=i("Traducci\xF3n (Translation, en ingl\xE9s): traduce un texto a otro idioma."),Bo=w(),_a=f("li"),Jo=i("Extracci\xF3n de Caracter\xEDsticas (Feature Extraction, en ingl\xE9s): crea una representaci\xF3n tensorial del texto."),vs=w(),vt=f("p"),ga=f("strong"),Go=i("Imagen"),Yo=i(":"),bs=w(),Ce=f("ul"),va=f("li"),Zo=i("Clasificaci\xF3n de Im\xE1genes (Image Classification, en ingl\xE9s): clasifica una imagen."),Wo=w(),ba=f("li"),Qo=i("Segmentaci\xF3n de Im\xE1genes (Image Segmentation, en ingl\xE9s): clasifica cada pixel de una imagen."),Xo=w(),wa=f("li"),er=i("Detecci\xF3n de Objetos (Object Detection, en ingl\xE9s): detecta objetos dentro de una imagen."),ws=w(),bt=f("p"),Ea=f("strong"),tr=i("Audio"),ar=i(":"),Es=w(),Be=f("ul"),ka=f("li"),sr=i("Clasificaci\xF3n de Audios (Audio Classification, en ingl\xE9s): asigna una etiqueta a un segmento de audio."),or=w(),ja=f("li"),rr=i("Reconocimiento de Voz Autom\xE1tico (Automatic Speech Recognition o ASR, en ingl\xE9s): transcribe datos de audio a un texto."),ks=w(),P(Je.$$.fragment),js=w(),De=f("h3"),Ge=f("a"),ya=f("span"),P(wt.$$.fragment),lr=w(),Aa=f("span"),nr=i("Uso del Pipeline"),ys=w(),Ye=f("p"),ir=i("En el siguiente ejemplo, usar\xE1s el "),za=f("code"),cr=i("pipeline()"),pr=i(" para an\xE1lisis de sentimiento."),As=w(),Ht=f("p"),ur=i("Instala las siguientes dependencias si a\xFAn no lo has hecho:"),zs=w(),P(Ze.$$.fragment),Cs=w(),We=f("p"),fr=i("Importa "),Ca=f("code"),dr=i("pipeline()"),mr=i(" y especifica la tarea que deseas completar:"),qs=w(),Qe=f("p"),hr=i("Debemos asegurarnos de que la frecuencia de muestreo del conjunto de datos coincide con la frecuencia de muestreo con la que se entren\xF3 "),qa=f("code"),$r=i("jonatasgrosman/wav2vec2-large-xlsr-53-spanish"),_r=i("."),Ts=w(),P(Et.$$.fragment),xs=w(),Xe=f("p"),gr=i("Los archivos de audio se cargan y remuestrean autom\xE1ticamente cuando llamamos a la columna "),Ta=f("code"),vr=i('"audio"'),br=i(`.
Extraigamos las matrices de onda cruda (raw waveform, en ingl\xE9s) de las primeras 4 muestras y pas\xE9mosla como una lista al pipeline:`),Ms=w(),P(kt.$$.fragment),Ps=w(),et=f("p"),wr=i("Para un dataset m\xE1s grande, donde los inputs son de mayor tama\xF1o (como en habla/audio o visi\xF3n), querr\xE1s pasar un generador en lugar de una lista que carga todos los inputs en memoria. Ve la "),Rt=f("a"),Er=i("documentaci\xF3n del pipeline"),kr=i(" para m\xE1s informaci\xF3n."),Ss=w(),Oe=f("h3"),tt=f("a"),xa=f("span"),P(jt.$$.fragment),jr=w(),Ma=f("span"),yr=i("Usa otro modelo y otro tokenizador en el pipeline"),Fs=w(),be=f("p"),Ar=i("El "),Pa=f("code"),zr=i("pipeline()"),Cr=i(" puede acomodarse a cualquier modelo del "),yt=f("a"),qr=i("Model Hub"),Tr=i(" haciendo m\xE1s f\xE1cil adaptar el "),Sa=f("code"),xr=i("pipeline()"),Mr=i(" para otros casos de uso. Por ejemplo, si quisieras un modelo capaz de manejar texto en franc\xE9s, usa los tags en el Model Hub para filtrar entre los modelos apropiados. El resultado mejor filtrado devuelve un "),At=f("a"),Pr=i("modelo BERT"),Sr=i(" multilingual fine-tuned para el an\xE1lisis de sentimiento. Genial, \xA1vamos a usar este modelo!"),Ds=w(),P(zt.$$.fragment),Os=w(),P(at.$$.fragment),Ls=w(),qe=f("p"),Fr=i("Despu\xE9s puedes especificar el modelo y el tokenizador en el "),Fa=f("code"),Dr=i("pipeline()"),Or=i(", y aplicar el "),Da=f("code"),Lr=i("classifier"),Nr=i(" en tu texto objetivo:"),Ns=w(),P(Ct.$$.fragment),Is=w(),Te=f("p"),Ir=i("Si no pudieras encontrar el modelo para tu caso respectivo de uso necesitar\xE1s ajustar un modelo preentrenado a tus datos. Mira nuestro "),Ut=f("a"),Hr=i("tutorial de fine-tuning"),Rr=i(" para aprender c\xF3mo. Finalmente, despu\xE9s de que has ajustado tu modelo preentrenado, \xA1por favor considera compartirlo (ve el tutorial "),Vt=f("a"),Ur=i("aqu\xED"),Vr=i(") con la comunidad en el Model Hub para democratizar el NLP! \u{1F917}"),Hs=w(),Le=f("h2"),st=f("a"),Oa=f("span"),P(qt.$$.fragment),Kr=w(),La=f("span"),Br=i("AutoClass"),Rs=w(),P(Tt.$$.fragment),Us=w(),me=f("p"),Jr=i("Debajo del cap\xF3, las clases "),Na=f("code"),Gr=i("AutoModelForSequenceClassification"),Yr=i(" y "),Ia=f("code"),Zr=i("AutoTokenizer"),Wr=i(" trabajan juntas para dar poder al "),Ha=f("code"),Qr=i("pipeline()"),Xr=i(". Una "),Kt=f("a"),el=i("AutoClass"),tl=i(" es un atajo que autom\xE1ticamente recupera la arquitectura de un modelo preentrenado con su nombre o el path. S\xF3lo necesitar\xE1s seleccionar el "),Ra=f("code"),al=i("AutoClass"),sl=i(" apropiado para tu tarea y tu tokenizador asociado con "),Ua=f("code"),ol=i("AutoTokenizer"),rl=i("."),Vs=w(),xe=f("p"),ll=i("Regresemos a nuestro ejemplo y veamos c\xF3mo puedes usar el "),Va=f("code"),nl=i("AutoClass"),il=i(" para reproducir los resultados del "),Ka=f("code"),cl=i("pipeline()"),pl=i("."),Ks=w(),Ne=f("h3"),ot=f("a"),Ba=f("span"),P(xt.$$.fragment),ul=w(),Ja=f("span"),fl=i("AutoTokenizer"),Bs=w(),Me=f("p"),dl=i("Un tokenizador es responsable de procesar el texto a un formato que sea entendible para el modelo. Primero, el tokenizador separar\xE1 el texto en palabras llamadas "),Ga=f("em"),ml=i("tokens"),hl=i(". Hay m\xFAltiples reglas que gobiernan el proceso de tokenizaci\xF3n incluyendo el c\xF3mo separar una palabra y en qu\xE9 nivel (aprende m\xE1s sobre tokenizaci\xF3n "),Bt=f("a"),$l=i("aqu\xED"),_l=i("). Lo m\xE1s importante es recordar que necesitar\xE1s instanciar el tokenizador con el mismo nombre del modelo para asegurar que est\xE1s usando las mismas reglas de tokenizaci\xF3n con las que el modelo fue preentrenado."),Js=w(),rt=f("p"),gl=i("Carga un tokenizador con "),Ya=f("code"),vl=i("AutoTokenizer"),bl=i(":"),Gs=w(),P(Mt.$$.fragment),Ys=w(),lt=f("p"),wl=i("Despu\xE9s, el tokenizador convierte los tokens a n\xFAmeros para construir un tensor que servir\xE1 como input para el modelo. Esto es conocido como el "),Za=f("em"),El=i("vocabulario"),kl=i(" del modelo."),Zs=w(),Jt=f("p"),jl=i("Pasa tu texto al tokenizador:"),Ws=w(),P(Pt.$$.fragment),Qs=w(),Gt=f("p"),yl=i("El tokenizador devolver\xE1 un diccionario conteniendo:"),Xs=w(),nt=f("ul"),Yt=f("li"),Zt=f("a"),Al=i("input_ids"),zl=i(": representaciones num\xE9ricas de los tokens."),Cl=w(),Wt=f("li"),Qt=f("a"),ql=i("atttention_mask"),Tl=i(": indica cu\xE1les tokens deben ser atendidos."),eo=w(),it=f("p"),xl=i("Como con el "),Wa=f("code"),Ml=i("pipeline()"),Pl=i(", el tokenizador aceptar\xE1 una lista de inputs. Adem\xE1s, el tokenizador tambi\xE9n puede rellenar (pad, en ingl\xE9s) y truncar el texto para devolver un lote (batch, en ingl\xE9s) de longitud uniforme:"),to=w(),P(ct.$$.fragment),ao=w(),pt=f("p"),Sl=i("Lee el tutorial de "),Xt=f("a"),Fl=i("preprocessing"),Dl=i(" para m\xE1s detalles acerca de la tokenizaci\xF3n."),so=w(),Ie=f("h3"),ut=f("a"),Qa=f("span"),P(St.$$.fragment),Ol=w(),Xa=f("span"),Ll=i("AutoModel"),oo=w(),P(ft.$$.fragment),ro=w(),P(dt.$$.fragment),lo=w(),he=f("p"),Nl=i("Los modelos son "),Ft=f("a"),es=f("code"),Il=i("torch.nn.Module"),Hl=i(" o "),Dt=f("a"),ts=f("code"),Rl=i("tf.keras.Model"),Ul=i(" est\xE1ndares as\xED que podr\xE1s usarlos en tu training loop usual. Sin embargo, para facilitar las cosas, \u{1F917} Transformers provee una clase "),as=f("code"),Vl=i("Trainer"),Kl=i(" para PyTorch que a\xF1ade funcionalidades para entrenamiento distribuido, precici\xF3n mixta, y m\xE1s. Para TensorFlow, puedes usar el m\xE9todo "),ss=f("code"),Bl=i("fit"),Jl=i(" desde "),Ot=f("a"),Gl=i("Keras"),Yl=i(". Consulta el "),ea=f("a"),Zl=i("tutorial de entrenamiento"),Wl=i(" para m\xE1s detalles."),no=w(),P(mt.$$.fragment),io=w(),He=f("h3"),ht=f("a"),os=f("span"),P(Lt.$$.fragment),Ql=w(),rs=f("span"),Xl=i("Guarda un modelo"),co=w(),P($t.$$.fragment),po=w(),Pe=f("p"),en=i("Una caracter\xEDstica particularmente interesante de \u{1F917} Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El par\xE1metro "),ls=f("code"),tn=i("from_pt"),an=i(" o "),ns=f("code"),sn=i("from_tf"),on=i(" puede convertir el modelo de un framework al otro:"),uo=w(),P(_t.$$.fragment),this.h()},l(a){const $=Xi('[data-svelte="svelte-1phssyn"]',document.head);e=d($,"META",{name:!0,content:!0}),$.forEach(o),l=E(a),t=d(a,"H1",{class:!0});var Nt=n(t);r=d(Nt,"A",{id:!0,class:!0,href:!0});var is=n(r);p=d(is,"SPAN",{});var cs=n(p);D(_.$$.fragment,cs),cs.forEach(o),is.forEach(o),g=E(Nt),v=d(Nt,"SPAN",{});var ps=n(v);k=c(ps,"Quick tour"),ps.forEach(o),Nt.forEach(o),z=E(a),D(y.$$.fragment,a),j=E(a),q=d(a,"P",{});var Re=n(q);R=c(Re,"\xA1Entra en marcha con los \u{1F917} Transformers! Comienza usando "),N=d(Re,"CODE",{});var us=n(N);V=c(us,"pipeline()"),us.forEach(o),b=c(Re," para una inferencia veloz, carga un modelo preentrenado y un tokenizador con una "),T=d(Re,"A",{href:!0});var fs=n(T);G=c(fs,"AutoClass"),fs.forEach(o),U=c(Re," para resolver tu tarea de texto, visi\xF3n o audio."),Re.forEach(o),I=E(a),D(K.$$.fragment,a),Z=E(a),B=d(a,"H2",{class:!0});var It=n(B);Y=d(It,"A",{id:!0,class:!0,href:!0});var ds=n(Y);re=d(ds,"SPAN",{});var ms=n(re);D(ae.$$.fragment,ms),ms.forEach(o),ds.forEach(o),se=E(It),ue=d(It,"SPAN",{});var hn=n(ue);ie=c(hn,"Pipeline"),hn.forEach(o),It.forEach(o),fe=E(a),le=d(a,"P",{});var rn=n(le);Q=d(rn,"CODE",{});var $n=n(Q);ce=c($n,"pipeline()"),$n.forEach(o),de=c(rn," es la forma m\xE1s f\xE1cil de usar un modelo preentrenado para una tarea dada."),rn.forEach(o),M=E(a),D(H.$$.fragment,a),ee=E(a),x=d(a,"P",{});var mo=n(x);J=c(mo,"El "),pe=d(mo,"CODE",{});var _n=n(pe);$e=c(_n,"pipeline()"),_n.forEach(o),W=c(mo," soporta muchas tareas comunes listas para usar:"),mo.forEach(o),ke=E(a),te=d(a,"P",{});var ln=n(te);je=d(ln,"STRONG",{});var gn=n(je);ve=c(gn,"Texto"),gn.forEach(o),Ae=c(ln,":"),ln.forEach(o),_e=E(a),X=d(a,"UL",{});var ge=n(X);ze=d(ge,"LI",{});var vn=n(ze);Po=c(vn,"An\xE1lisis de Sentimiento (Sentiment Analysis, en ingl\xE9s): clasifica la polaridad de un texto dado."),vn.forEach(o),So=E(ge),ua=d(ge,"LI",{});var bn=n(ua);Fo=c(bn,"Generaci\xF3n de Texto (Text Generation, en ingl\xE9s): genera texto a partir de un input dado."),bn.forEach(o),Do=E(ge),fa=d(ge,"LI",{});var wn=n(fa);Oo=c(wn,"Reconocimiento de Entidades (Name Entity Recognition o NER, en ingl\xE9s): etiqueta cada palabra con la entidad que representa (persona, fecha, ubicaci\xF3n, etc.)."),wn.forEach(o),Lo=E(ge),da=d(ge,"LI",{});var En=n(da);No=c(En,"Responder Preguntas (Question answering, en ingl\xE9s): extrae la respuesta del contexto dado un contexto y una pregunta."),En.forEach(o),Io=E(ge),ma=d(ge,"LI",{});var kn=n(ma);Ho=c(kn,"Rellenar M\xE1scara (Fill-mask, en ingl\xE9s): rellena el espacio faltante dado un texto con palabras enmascaradas."),kn.forEach(o),Ro=E(ge),ha=d(ge,"LI",{});var jn=n(ha);Uo=c(jn,"Resumir (Summarization, en ingl\xE9s): genera un resumen de una secuencia larga de texto o un documento."),jn.forEach(o),Vo=E(ge),$a=d(ge,"LI",{});var yn=n($a);Ko=c(yn,"Traducci\xF3n (Translation, en ingl\xE9s): traduce un texto a otro idioma."),yn.forEach(o),Bo=E(ge),_a=d(ge,"LI",{});var An=n(_a);Jo=c(An,"Extracci\xF3n de Caracter\xEDsticas (Feature Extraction, en ingl\xE9s): crea una representaci\xF3n tensorial del texto."),An.forEach(o),ge.forEach(o),vs=E(a),vt=d(a,"P",{});var nn=n(vt);ga=d(nn,"STRONG",{});var zn=n(ga);Go=c(zn,"Imagen"),zn.forEach(o),Yo=c(nn,":"),nn.forEach(o),bs=E(a),Ce=d(a,"UL",{});var ta=n(Ce);va=d(ta,"LI",{});var Cn=n(va);Zo=c(Cn,"Clasificaci\xF3n de Im\xE1genes (Image Classification, en ingl\xE9s): clasifica una imagen."),Cn.forEach(o),Wo=E(ta),ba=d(ta,"LI",{});var qn=n(ba);Qo=c(qn,"Segmentaci\xF3n de Im\xE1genes (Image Segmentation, en ingl\xE9s): clasifica cada pixel de una imagen."),qn.forEach(o),Xo=E(ta),wa=d(ta,"LI",{});var Tn=n(wa);er=c(Tn,"Detecci\xF3n de Objetos (Object Detection, en ingl\xE9s): detecta objetos dentro de una imagen."),Tn.forEach(o),ta.forEach(o),ws=E(a),bt=d(a,"P",{});var cn=n(bt);Ea=d(cn,"STRONG",{});var xn=n(Ea);tr=c(xn,"Audio"),xn.forEach(o),ar=c(cn,":"),cn.forEach(o),Es=E(a),Be=d(a,"UL",{});var ho=n(Be);ka=d(ho,"LI",{});var Mn=n(ka);sr=c(Mn,"Clasificaci\xF3n de Audios (Audio Classification, en ingl\xE9s): asigna una etiqueta a un segmento de audio."),Mn.forEach(o),or=E(ho),ja=d(ho,"LI",{});var Pn=n(ja);rr=c(Pn,"Reconocimiento de Voz Autom\xE1tico (Automatic Speech Recognition o ASR, en ingl\xE9s): transcribe datos de audio a un texto."),Pn.forEach(o),ho.forEach(o),ks=E(a),D(Je.$$.fragment,a),js=E(a),De=d(a,"H3",{class:!0});var $o=n(De);Ge=d($o,"A",{id:!0,class:!0,href:!0});var Sn=n(Ge);ya=d(Sn,"SPAN",{});var Fn=n(ya);D(wt.$$.fragment,Fn),Fn.forEach(o),Sn.forEach(o),lr=E($o),Aa=d($o,"SPAN",{});var Dn=n(Aa);nr=c(Dn,"Uso del Pipeline"),Dn.forEach(o),$o.forEach(o),ys=E(a),Ye=d(a,"P",{});var _o=n(Ye);ir=c(_o,"En el siguiente ejemplo, usar\xE1s el "),za=d(_o,"CODE",{});var On=n(za);cr=c(On,"pipeline()"),On.forEach(o),pr=c(_o," para an\xE1lisis de sentimiento."),_o.forEach(o),As=E(a),Ht=d(a,"P",{});var Ln=n(Ht);ur=c(Ln,"Instala las siguientes dependencias si a\xFAn no lo has hecho:"),Ln.forEach(o),zs=E(a),D(Ze.$$.fragment,a),Cs=E(a),We=d(a,"P",{});var go=n(We);fr=c(go,"Importa "),Ca=d(go,"CODE",{});var Nn=n(Ca);dr=c(Nn,"pipeline()"),Nn.forEach(o),mr=c(go," y especifica la tarea que deseas completar:"),go.forEach(o),qs=E(a),Qe=d(a,"P",{});var vo=n(Qe);hr=c(vo,"Debemos asegurarnos de que la frecuencia de muestreo del conjunto de datos coincide con la frecuencia de muestreo con la que se entren\xF3 "),qa=d(vo,"CODE",{});var In=n(qa);$r=c(In,"jonatasgrosman/wav2vec2-large-xlsr-53-spanish"),In.forEach(o),_r=c(vo,"."),vo.forEach(o),Ts=E(a),D(Et.$$.fragment,a),xs=E(a),Xe=d(a,"P",{});var bo=n(Xe);gr=c(bo,"Los archivos de audio se cargan y remuestrean autom\xE1ticamente cuando llamamos a la columna "),Ta=d(bo,"CODE",{});var Hn=n(Ta);vr=c(Hn,'"audio"'),Hn.forEach(o),br=c(bo,`.
Extraigamos las matrices de onda cruda (raw waveform, en ingl\xE9s) de las primeras 4 muestras y pas\xE9mosla como una lista al pipeline:`),bo.forEach(o),Ms=E(a),D(kt.$$.fragment,a),Ps=E(a),et=d(a,"P",{});var wo=n(et);wr=c(wo,"Para un dataset m\xE1s grande, donde los inputs son de mayor tama\xF1o (como en habla/audio o visi\xF3n), querr\xE1s pasar un generador en lugar de una lista que carga todos los inputs en memoria. Ve la "),Rt=d(wo,"A",{href:!0});var Rn=n(Rt);Er=c(Rn,"documentaci\xF3n del pipeline"),Rn.forEach(o),kr=c(wo," para m\xE1s informaci\xF3n."),wo.forEach(o),Ss=E(a),Oe=d(a,"H3",{class:!0});var Eo=n(Oe);tt=d(Eo,"A",{id:!0,class:!0,href:!0});var Un=n(tt);xa=d(Un,"SPAN",{});var Vn=n(xa);D(jt.$$.fragment,Vn),Vn.forEach(o),Un.forEach(o),jr=E(Eo),Ma=d(Eo,"SPAN",{});var Kn=n(Ma);yr=c(Kn,"Usa otro modelo y otro tokenizador en el pipeline"),Kn.forEach(o),Eo.forEach(o),Fs=E(a),be=d(a,"P",{});var Se=n(be);Ar=c(Se,"El "),Pa=d(Se,"CODE",{});var Bn=n(Pa);zr=c(Bn,"pipeline()"),Bn.forEach(o),Cr=c(Se," puede acomodarse a cualquier modelo del "),yt=d(Se,"A",{href:!0,rel:!0});var Jn=n(yt);qr=c(Jn,"Model Hub"),Jn.forEach(o),Tr=c(Se," haciendo m\xE1s f\xE1cil adaptar el "),Sa=d(Se,"CODE",{});var Gn=n(Sa);xr=c(Gn,"pipeline()"),Gn.forEach(o),Mr=c(Se," para otros casos de uso. Por ejemplo, si quisieras un modelo capaz de manejar texto en franc\xE9s, usa los tags en el Model Hub para filtrar entre los modelos apropiados. El resultado mejor filtrado devuelve un "),At=d(Se,"A",{href:!0,rel:!0});var Yn=n(At);Pr=c(Yn,"modelo BERT"),Yn.forEach(o),Sr=c(Se," multilingual fine-tuned para el an\xE1lisis de sentimiento. Genial, \xA1vamos a usar este modelo!"),Se.forEach(o),Ds=E(a),D(zt.$$.fragment,a),Os=E(a),D(at.$$.fragment,a),Ls=E(a),qe=d(a,"P",{});var aa=n(qe);Fr=c(aa,"Despu\xE9s puedes especificar el modelo y el tokenizador en el "),Fa=d(aa,"CODE",{});var Zn=n(Fa);Dr=c(Zn,"pipeline()"),Zn.forEach(o),Or=c(aa,", y aplicar el "),Da=d(aa,"CODE",{});var Wn=n(Da);Lr=c(Wn,"classifier"),Wn.forEach(o),Nr=c(aa," en tu texto objetivo:"),aa.forEach(o),Ns=E(a),D(Ct.$$.fragment,a),Is=E(a),Te=d(a,"P",{});var sa=n(Te);Ir=c(sa,"Si no pudieras encontrar el modelo para tu caso respectivo de uso necesitar\xE1s ajustar un modelo preentrenado a tus datos. Mira nuestro "),Ut=d(sa,"A",{href:!0});var Qn=n(Ut);Hr=c(Qn,"tutorial de fine-tuning"),Qn.forEach(o),Rr=c(sa," para aprender c\xF3mo. Finalmente, despu\xE9s de que has ajustado tu modelo preentrenado, \xA1por favor considera compartirlo (ve el tutorial "),Vt=d(sa,"A",{href:!0});var Xn=n(Vt);Ur=c(Xn,"aqu\xED"),Xn.forEach(o),Vr=c(sa,") con la comunidad en el Model Hub para democratizar el NLP! \u{1F917}"),sa.forEach(o),Hs=E(a),Le=d(a,"H2",{class:!0});var ko=n(Le);st=d(ko,"A",{id:!0,class:!0,href:!0});var ei=n(st);Oa=d(ei,"SPAN",{});var ti=n(Oa);D(qt.$$.fragment,ti),ti.forEach(o),ei.forEach(o),Kr=E(ko),La=d(ko,"SPAN",{});var ai=n(La);Br=c(ai,"AutoClass"),ai.forEach(o),ko.forEach(o),Rs=E(a),D(Tt.$$.fragment,a),Us=E(a),me=d(a,"P",{});var we=n(me);Jr=c(we,"Debajo del cap\xF3, las clases "),Na=d(we,"CODE",{});var si=n(Na);Gr=c(si,"AutoModelForSequenceClassification"),si.forEach(o),Yr=c(we," y "),Ia=d(we,"CODE",{});var oi=n(Ia);Zr=c(oi,"AutoTokenizer"),oi.forEach(o),Wr=c(we," trabajan juntas para dar poder al "),Ha=d(we,"CODE",{});var ri=n(Ha);Qr=c(ri,"pipeline()"),ri.forEach(o),Xr=c(we,". Una "),Kt=d(we,"A",{href:!0});var li=n(Kt);el=c(li,"AutoClass"),li.forEach(o),tl=c(we," es un atajo que autom\xE1ticamente recupera la arquitectura de un modelo preentrenado con su nombre o el path. S\xF3lo necesitar\xE1s seleccionar el "),Ra=d(we,"CODE",{});var ni=n(Ra);al=c(ni,"AutoClass"),ni.forEach(o),sl=c(we," apropiado para tu tarea y tu tokenizador asociado con "),Ua=d(we,"CODE",{});var ii=n(Ua);ol=c(ii,"AutoTokenizer"),ii.forEach(o),rl=c(we,"."),we.forEach(o),Vs=E(a),xe=d(a,"P",{});var oa=n(xe);ll=c(oa,"Regresemos a nuestro ejemplo y veamos c\xF3mo puedes usar el "),Va=d(oa,"CODE",{});var ci=n(Va);nl=c(ci,"AutoClass"),ci.forEach(o),il=c(oa," para reproducir los resultados del "),Ka=d(oa,"CODE",{});var pi=n(Ka);cl=c(pi,"pipeline()"),pi.forEach(o),pl=c(oa,"."),oa.forEach(o),Ks=E(a),Ne=d(a,"H3",{class:!0});var jo=n(Ne);ot=d(jo,"A",{id:!0,class:!0,href:!0});var ui=n(ot);Ba=d(ui,"SPAN",{});var fi=n(Ba);D(xt.$$.fragment,fi),fi.forEach(o),ui.forEach(o),ul=E(jo),Ja=d(jo,"SPAN",{});var di=n(Ja);fl=c(di,"AutoTokenizer"),di.forEach(o),jo.forEach(o),Bs=E(a),Me=d(a,"P",{});var ra=n(Me);dl=c(ra,"Un tokenizador es responsable de procesar el texto a un formato que sea entendible para el modelo. Primero, el tokenizador separar\xE1 el texto en palabras llamadas "),Ga=d(ra,"EM",{});var mi=n(Ga);ml=c(mi,"tokens"),mi.forEach(o),hl=c(ra,". Hay m\xFAltiples reglas que gobiernan el proceso de tokenizaci\xF3n incluyendo el c\xF3mo separar una palabra y en qu\xE9 nivel (aprende m\xE1s sobre tokenizaci\xF3n "),Bt=d(ra,"A",{href:!0});var hi=n(Bt);$l=c(hi,"aqu\xED"),hi.forEach(o),_l=c(ra,"). Lo m\xE1s importante es recordar que necesitar\xE1s instanciar el tokenizador con el mismo nombre del modelo para asegurar que est\xE1s usando las mismas reglas de tokenizaci\xF3n con las que el modelo fue preentrenado."),ra.forEach(o),Js=E(a),rt=d(a,"P",{});var yo=n(rt);gl=c(yo,"Carga un tokenizador con "),Ya=d(yo,"CODE",{});var $i=n(Ya);vl=c($i,"AutoTokenizer"),$i.forEach(o),bl=c(yo,":"),yo.forEach(o),Gs=E(a),D(Mt.$$.fragment,a),Ys=E(a),lt=d(a,"P",{});var Ao=n(lt);wl=c(Ao,"Despu\xE9s, el tokenizador convierte los tokens a n\xFAmeros para construir un tensor que servir\xE1 como input para el modelo. Esto es conocido como el "),Za=d(Ao,"EM",{});var _i=n(Za);El=c(_i,"vocabulario"),_i.forEach(o),kl=c(Ao," del modelo."),Ao.forEach(o),Zs=E(a),Jt=d(a,"P",{});var gi=n(Jt);jl=c(gi,"Pasa tu texto al tokenizador:"),gi.forEach(o),Ws=E(a),D(Pt.$$.fragment,a),Qs=E(a),Gt=d(a,"P",{});var vi=n(Gt);yl=c(vi,"El tokenizador devolver\xE1 un diccionario conteniendo:"),vi.forEach(o),Xs=E(a),nt=d(a,"UL",{});var zo=n(nt);Yt=d(zo,"LI",{});var pn=n(Yt);Zt=d(pn,"A",{href:!0});var bi=n(Zt);Al=c(bi,"input_ids"),bi.forEach(o),zl=c(pn,": representaciones num\xE9ricas de los tokens."),pn.forEach(o),Cl=E(zo),Wt=d(zo,"LI",{});var un=n(Wt);Qt=d(un,"A",{href:!0});var wi=n(Qt);ql=c(wi,"atttention_mask"),wi.forEach(o),Tl=c(un,": indica cu\xE1les tokens deben ser atendidos."),un.forEach(o),zo.forEach(o),eo=E(a),it=d(a,"P",{});var Co=n(it);xl=c(Co,"Como con el "),Wa=d(Co,"CODE",{});var Ei=n(Wa);Ml=c(Ei,"pipeline()"),Ei.forEach(o),Pl=c(Co,", el tokenizador aceptar\xE1 una lista de inputs. Adem\xE1s, el tokenizador tambi\xE9n puede rellenar (pad, en ingl\xE9s) y truncar el texto para devolver un lote (batch, en ingl\xE9s) de longitud uniforme:"),Co.forEach(o),to=E(a),D(ct.$$.fragment,a),ao=E(a),pt=d(a,"P",{});var qo=n(pt);Sl=c(qo,"Lee el tutorial de "),Xt=d(qo,"A",{href:!0});var ki=n(Xt);Fl=c(ki,"preprocessing"),ki.forEach(o),Dl=c(qo," para m\xE1s detalles acerca de la tokenizaci\xF3n."),qo.forEach(o),so=E(a),Ie=d(a,"H3",{class:!0});var To=n(Ie);ut=d(To,"A",{id:!0,class:!0,href:!0});var ji=n(ut);Qa=d(ji,"SPAN",{});var yi=n(Qa);D(St.$$.fragment,yi),yi.forEach(o),ji.forEach(o),Ol=E(To),Xa=d(To,"SPAN",{});var Ai=n(Xa);Ll=c(Ai,"AutoModel"),Ai.forEach(o),To.forEach(o),oo=E(a),D(ft.$$.fragment,a),ro=E(a),D(dt.$$.fragment,a),lo=E(a),he=d(a,"P",{});var Ee=n(he);Nl=c(Ee,"Los modelos son "),Ft=d(Ee,"A",{href:!0,rel:!0});var zi=n(Ft);es=d(zi,"CODE",{});var Ci=n(es);Il=c(Ci,"torch.nn.Module"),Ci.forEach(o),zi.forEach(o),Hl=c(Ee," o "),Dt=d(Ee,"A",{href:!0,rel:!0});var qi=n(Dt);ts=d(qi,"CODE",{});var Ti=n(ts);Rl=c(Ti,"tf.keras.Model"),Ti.forEach(o),qi.forEach(o),Ul=c(Ee," est\xE1ndares as\xED que podr\xE1s usarlos en tu training loop usual. Sin embargo, para facilitar las cosas, \u{1F917} Transformers provee una clase "),as=d(Ee,"CODE",{});var xi=n(as);Vl=c(xi,"Trainer"),xi.forEach(o),Kl=c(Ee," para PyTorch que a\xF1ade funcionalidades para entrenamiento distribuido, precici\xF3n mixta, y m\xE1s. Para TensorFlow, puedes usar el m\xE9todo "),ss=d(Ee,"CODE",{});var Mi=n(ss);Bl=c(Mi,"fit"),Mi.forEach(o),Jl=c(Ee," desde "),Ot=d(Ee,"A",{href:!0,rel:!0});var Pi=n(Ot);Gl=c(Pi,"Keras"),Pi.forEach(o),Yl=c(Ee,". Consulta el "),ea=d(Ee,"A",{href:!0});var Si=n(ea);Zl=c(Si,"tutorial de entrenamiento"),Si.forEach(o),Wl=c(Ee," para m\xE1s detalles."),Ee.forEach(o),no=E(a),D(mt.$$.fragment,a),io=E(a),He=d(a,"H3",{class:!0});var xo=n(He);ht=d(xo,"A",{id:!0,class:!0,href:!0});var Fi=n(ht);os=d(Fi,"SPAN",{});var Di=n(os);D(Lt.$$.fragment,Di),Di.forEach(o),Fi.forEach(o),Ql=E(xo),rs=d(xo,"SPAN",{});var Oi=n(rs);Xl=c(Oi,"Guarda un modelo"),Oi.forEach(o),xo.forEach(o),co=E(a),D($t.$$.fragment,a),po=E(a),Pe=d(a,"P",{});var la=n(Pe);en=c(la,"Una caracter\xEDstica particularmente interesante de \u{1F917} Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El par\xE1metro "),ls=d(la,"CODE",{});var Li=n(ls);tn=c(Li,"from_pt"),Li.forEach(o),an=c(la," o "),ns=d(la,"CODE",{});var Ni=n(ns);sn=c(Ni,"from_tf"),Ni.forEach(o),on=c(la," puede convertir el modelo de un framework al otro:"),la.forEach(o),uo=E(a),D(_t.$$.fragment,a),this.h()},h(){u(e,"name","hf:doc:metadata"),u(e,"content",JSON.stringify(cp)),u(r,"id","quick-tour"),u(r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(r,"href","#quick-tour"),u(t,"class","relative group"),u(T,"href","./model_doc/auto"),u(Y,"id","pipeline"),u(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Y,"href","#pipeline"),u(B,"class","relative group"),u(Ge,"id","uso-del-pipeline"),u(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ge,"href","#uso-del-pipeline"),u(De,"class","relative group"),u(Rt,"href","./main_classes/pipelines"),u(tt,"id","usa-otro-modelo-y-otro-tokenizador-en-el-pipeline"),u(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(tt,"href","#usa-otro-modelo-y-otro-tokenizador-en-el-pipeline"),u(Oe,"class","relative group"),u(yt,"href","https://huggingface.co/models"),u(yt,"rel","nofollow"),u(At,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),u(At,"rel","nofollow"),u(Ut,"href","./training"),u(Vt,"href","./model_sharing"),u(st,"id","autoclass"),u(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(st,"href","#autoclass"),u(Le,"class","relative group"),u(Kt,"href","./model_doc/auto"),u(ot,"id","autotokenizer"),u(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ot,"href","#autotokenizer"),u(Ne,"class","relative group"),u(Bt,"href","./tokenizer_summary"),u(Zt,"href","./glossary#input-ids"),u(Qt,"href",".glossary#attention-mask"),u(Xt,"href","./preprocessing"),u(ut,"id","automodel"),u(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ut,"href","#automodel"),u(Ie,"class","relative group"),u(Ft,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),u(Ft,"rel","nofollow"),u(Dt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),u(Dt,"rel","nofollow"),u(Ot,"href","https://keras.io/"),u(Ot,"rel","nofollow"),u(ea,"href","./training"),u(ht,"id","guarda-un-modelo"),u(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ht,"href","#guarda-un-modelo"),u(He,"class","relative group")},m(a,$){s(document.head,e),h(a,l,$),h(a,t,$),s(t,r),s(r,p),S(_,p,null),s(t,g),s(t,v),s(v,k),h(a,z,$),S(y,a,$),h(a,j,$),h(a,q,$),s(q,R),s(q,N),s(N,V),s(q,b),s(q,T),s(T,G),s(q,U),h(a,I,$),S(K,a,$),h(a,Z,$),h(a,B,$),s(B,Y),s(Y,re),S(ae,re,null),s(B,se),s(B,ue),s(ue,ie),h(a,fe,$),h(a,le,$),s(le,Q),s(Q,ce),s(le,de),h(a,M,$),S(H,a,$),h(a,ee,$),h(a,x,$),s(x,J),s(x,pe),s(pe,$e),s(x,W),h(a,ke,$),h(a,te,$),s(te,je),s(je,ve),s(te,Ae),h(a,_e,$),h(a,X,$),s(X,ze),s(ze,Po),s(X,So),s(X,ua),s(ua,Fo),s(X,Do),s(X,fa),s(fa,Oo),s(X,Lo),s(X,da),s(da,No),s(X,Io),s(X,ma),s(ma,Ho),s(X,Ro),s(X,ha),s(ha,Uo),s(X,Vo),s(X,$a),s($a,Ko),s(X,Bo),s(X,_a),s(_a,Jo),h(a,vs,$),h(a,vt,$),s(vt,ga),s(ga,Go),s(vt,Yo),h(a,bs,$),h(a,Ce,$),s(Ce,va),s(va,Zo),s(Ce,Wo),s(Ce,ba),s(ba,Qo),s(Ce,Xo),s(Ce,wa),s(wa,er),h(a,ws,$),h(a,bt,$),s(bt,Ea),s(Ea,tr),s(bt,ar),h(a,Es,$),h(a,Be,$),s(Be,ka),s(ka,sr),s(Be,or),s(Be,ja),s(ja,rr),h(a,ks,$),S(Je,a,$),h(a,js,$),h(a,De,$),s(De,Ge),s(Ge,ya),S(wt,ya,null),s(De,lr),s(De,Aa),s(Aa,nr),h(a,ys,$),h(a,Ye,$),s(Ye,ir),s(Ye,za),s(za,cr),s(Ye,pr),h(a,As,$),h(a,Ht,$),s(Ht,ur),h(a,zs,$),S(Ze,a,$),h(a,Cs,$),h(a,We,$),s(We,fr),s(We,Ca),s(Ca,dr),s(We,mr),h(a,qs,$),h(a,Qe,$),s(Qe,hr),s(Qe,qa),s(qa,$r),s(Qe,_r),h(a,Ts,$),S(Et,a,$),h(a,xs,$),h(a,Xe,$),s(Xe,gr),s(Xe,Ta),s(Ta,vr),s(Xe,br),h(a,Ms,$),S(kt,a,$),h(a,Ps,$),h(a,et,$),s(et,wr),s(et,Rt),s(Rt,Er),s(et,kr),h(a,Ss,$),h(a,Oe,$),s(Oe,tt),s(tt,xa),S(jt,xa,null),s(Oe,jr),s(Oe,Ma),s(Ma,yr),h(a,Fs,$),h(a,be,$),s(be,Ar),s(be,Pa),s(Pa,zr),s(be,Cr),s(be,yt),s(yt,qr),s(be,Tr),s(be,Sa),s(Sa,xr),s(be,Mr),s(be,At),s(At,Pr),s(be,Sr),h(a,Ds,$),S(zt,a,$),h(a,Os,$),S(at,a,$),h(a,Ls,$),h(a,qe,$),s(qe,Fr),s(qe,Fa),s(Fa,Dr),s(qe,Or),s(qe,Da),s(Da,Lr),s(qe,Nr),h(a,Ns,$),S(Ct,a,$),h(a,Is,$),h(a,Te,$),s(Te,Ir),s(Te,Ut),s(Ut,Hr),s(Te,Rr),s(Te,Vt),s(Vt,Ur),s(Te,Vr),h(a,Hs,$),h(a,Le,$),s(Le,st),s(st,Oa),S(qt,Oa,null),s(Le,Kr),s(Le,La),s(La,Br),h(a,Rs,$),S(Tt,a,$),h(a,Us,$),h(a,me,$),s(me,Jr),s(me,Na),s(Na,Gr),s(me,Yr),s(me,Ia),s(Ia,Zr),s(me,Wr),s(me,Ha),s(Ha,Qr),s(me,Xr),s(me,Kt),s(Kt,el),s(me,tl),s(me,Ra),s(Ra,al),s(me,sl),s(me,Ua),s(Ua,ol),s(me,rl),h(a,Vs,$),h(a,xe,$),s(xe,ll),s(xe,Va),s(Va,nl),s(xe,il),s(xe,Ka),s(Ka,cl),s(xe,pl),h(a,Ks,$),h(a,Ne,$),s(Ne,ot),s(ot,Ba),S(xt,Ba,null),s(Ne,ul),s(Ne,Ja),s(Ja,fl),h(a,Bs,$),h(a,Me,$),s(Me,dl),s(Me,Ga),s(Ga,ml),s(Me,hl),s(Me,Bt),s(Bt,$l),s(Me,_l),h(a,Js,$),h(a,rt,$),s(rt,gl),s(rt,Ya),s(Ya,vl),s(rt,bl),h(a,Gs,$),S(Mt,a,$),h(a,Ys,$),h(a,lt,$),s(lt,wl),s(lt,Za),s(Za,El),s(lt,kl),h(a,Zs,$),h(a,Jt,$),s(Jt,jl),h(a,Ws,$),S(Pt,a,$),h(a,Qs,$),h(a,Gt,$),s(Gt,yl),h(a,Xs,$),h(a,nt,$),s(nt,Yt),s(Yt,Zt),s(Zt,Al),s(Yt,zl),s(nt,Cl),s(nt,Wt),s(Wt,Qt),s(Qt,ql),s(Wt,Tl),h(a,eo,$),h(a,it,$),s(it,xl),s(it,Wa),s(Wa,Ml),s(it,Pl),h(a,to,$),S(ct,a,$),h(a,ao,$),h(a,pt,$),s(pt,Sl),s(pt,Xt),s(Xt,Fl),s(pt,Dl),h(a,so,$),h(a,Ie,$),s(Ie,ut),s(ut,Qa),S(St,Qa,null),s(Ie,Ol),s(Ie,Xa),s(Xa,Ll),h(a,oo,$),S(ft,a,$),h(a,ro,$),S(dt,a,$),h(a,lo,$),h(a,he,$),s(he,Nl),s(he,Ft),s(Ft,es),s(es,Il),s(he,Hl),s(he,Dt),s(Dt,ts),s(ts,Rl),s(he,Ul),s(he,as),s(as,Vl),s(he,Kl),s(he,ss),s(ss,Bl),s(he,Jl),s(he,Ot),s(Ot,Gl),s(he,Yl),s(he,ea),s(ea,Zl),s(he,Wl),h(a,no,$),S(mt,a,$),h(a,io,$),h(a,He,$),s(He,ht),s(ht,os),S(Lt,os,null),s(He,Ql),s(He,rs),s(rs,Xl),h(a,co,$),S($t,a,$),h(a,po,$),h(a,Pe,$),s(Pe,en),s(Pe,ls),s(ls,tn),s(Pe,an),s(Pe,ns),s(ns,sn),s(Pe,on),h(a,uo,$),S(_t,a,$),fo=!0},p(a,[$]){const Nt={};$&2&&(Nt.$$scope={dirty:$,ctx:a}),K.$set(Nt);const is={};$&2&&(is.$$scope={dirty:$,ctx:a}),Je.$set(is);const cs={};$&2&&(cs.$$scope={dirty:$,ctx:a}),Ze.$set(cs);const ps={};$&2&&(ps.$$scope={dirty:$,ctx:a}),at.$set(ps);const Re={};$&2&&(Re.$$scope={dirty:$,ctx:a}),ct.$set(Re);const us={};$&2&&(us.$$scope={dirty:$,ctx:a}),ft.$set(us);const fs={};$&2&&(fs.$$scope={dirty:$,ctx:a}),dt.$set(fs);const It={};$&2&&(It.$$scope={dirty:$,ctx:a}),mt.$set(It);const ds={};$&2&&(ds.$$scope={dirty:$,ctx:a}),$t.$set(ds);const ms={};$&2&&(ms.$$scope={dirty:$,ctx:a}),_t.$set(ms)},i(a){fo||(A(_.$$.fragment,a),A(y.$$.fragment,a),A(K.$$.fragment,a),A(ae.$$.fragment,a),A(H.$$.fragment,a),A(Je.$$.fragment,a),A(wt.$$.fragment,a),A(Ze.$$.fragment,a),A(Et.$$.fragment,a),A(kt.$$.fragment,a),A(jt.$$.fragment,a),A(zt.$$.fragment,a),A(at.$$.fragment,a),A(Ct.$$.fragment,a),A(qt.$$.fragment,a),A(Tt.$$.fragment,a),A(xt.$$.fragment,a),A(Mt.$$.fragment,a),A(Pt.$$.fragment,a),A(ct.$$.fragment,a),A(St.$$.fragment,a),A(ft.$$.fragment,a),A(dt.$$.fragment,a),A(mt.$$.fragment,a),A(Lt.$$.fragment,a),A($t.$$.fragment,a),A(_t.$$.fragment,a),fo=!0)},o(a){C(_.$$.fragment,a),C(y.$$.fragment,a),C(K.$$.fragment,a),C(ae.$$.fragment,a),C(H.$$.fragment,a),C(Je.$$.fragment,a),C(wt.$$.fragment,a),C(Ze.$$.fragment,a),C(Et.$$.fragment,a),C(kt.$$.fragment,a),C(jt.$$.fragment,a),C(zt.$$.fragment,a),C(at.$$.fragment,a),C(Ct.$$.fragment,a),C(qt.$$.fragment,a),C(Tt.$$.fragment,a),C(xt.$$.fragment,a),C(Mt.$$.fragment,a),C(Pt.$$.fragment,a),C(ct.$$.fragment,a),C(St.$$.fragment,a),C(ft.$$.fragment,a),C(dt.$$.fragment,a),C(mt.$$.fragment,a),C(Lt.$$.fragment,a),C($t.$$.fragment,a),C(_t.$$.fragment,a),fo=!1},d(a){o(e),a&&o(l),a&&o(t),F(_),a&&o(z),F(y,a),a&&o(j),a&&o(q),a&&o(I),F(K,a),a&&o(Z),a&&o(B),F(ae),a&&o(fe),a&&o(le),a&&o(M),F(H,a),a&&o(ee),a&&o(x),a&&o(ke),a&&o(te),a&&o(_e),a&&o(X),a&&o(vs),a&&o(vt),a&&o(bs),a&&o(Ce),a&&o(ws),a&&o(bt),a&&o(Es),a&&o(Be),a&&o(ks),F(Je,a),a&&o(js),a&&o(De),F(wt),a&&o(ys),a&&o(Ye),a&&o(As),a&&o(Ht),a&&o(zs),F(Ze,a),a&&o(Cs),a&&o(We),a&&o(qs),a&&o(Qe),a&&o(Ts),F(Et,a),a&&o(xs),a&&o(Xe),a&&o(Ms),F(kt,a),a&&o(Ps),a&&o(et),a&&o(Ss),a&&o(Oe),F(jt),a&&o(Fs),a&&o(be),a&&o(Ds),F(zt,a),a&&o(Os),F(at,a),a&&o(Ls),a&&o(qe),a&&o(Ns),F(Ct,a),a&&o(Is),a&&o(Te),a&&o(Hs),a&&o(Le),F(qt),a&&o(Rs),F(Tt,a),a&&o(Us),a&&o(me),a&&o(Vs),a&&o(xe),a&&o(Ks),a&&o(Ne),F(xt),a&&o(Bs),a&&o(Me),a&&o(Js),a&&o(rt),a&&o(Gs),F(Mt,a),a&&o(Ys),a&&o(lt),a&&o(Zs),a&&o(Jt),a&&o(Ws),F(Pt,a),a&&o(Qs),a&&o(Gt),a&&o(Xs),a&&o(nt),a&&o(eo),a&&o(it),a&&o(to),F(ct,a),a&&o(ao),a&&o(pt),a&&o(so),a&&o(Ie),F(St),a&&o(oo),F(ft,a),a&&o(ro),F(dt,a),a&&o(lo),a&&o(he),a&&o(no),F(mt,a),a&&o(io),a&&o(He),F(Lt),a&&o(co),F($t,a),a&&o(po),a&&o(Pe),a&&o(uo),F(_t,a)}}}const cp={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"uso-del-pipeline",title:"Uso del Pipeline"},{local:"usa-otro-modelo-y-otro-tokenizador-en-el-pipeline",title:"Usa otro modelo y otro tokenizador en el pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"guarda-un-modelo",title:"Guarda un modelo"}],title:"AutoClass"}],title:"Quick tour"};function pp(m){return Gi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _p extends Ue{constructor(e){super();Ve(this,e,pp,ip,Ke,{})}}export{_p as default,cp as metadata};
