import{S as Pa,i as Ca,s as Ia,e as o,k as m,w as u,t as l,M as Fa,c as a,d as t,m as h,a as n,x as d,h as i,b as f,F as s,g as p,y as c,q as v,o as g,B as w}from"../chunks/vendor-6b77c823.js";import{T as Ua}from"../chunks/Tip-39098574.js";import{Y as Aa}from"../chunks/Youtube-5c6e11e6.js";import{I as xe}from"../chunks/IconCopyLink-7a11ce68.js";import{C as T}from"../chunks/CodeBlock-3a8b25a8.js";function Sa(ot){let _,A,$,y,C;return{c(){_=o("p"),A=l("Refer to the Performance "),$=o("a"),y=l("guide"),C=l(" for more details about memory-saving techniques."),this.h()},l(b){_=a(b,"P",{});var P=n(_);A=i(P,"Refer to the Performance "),$=a(P,"A",{href:!0});var N=n($);y=i(N,"guide"),N.forEach(t),C=i(P," for more details about memory-saving techniques."),P.forEach(t),this.h()},h(){f($,"href","performance")},m(b,P){p(b,_,P),s(_,A),s(_,$),s($,y),s(_,C)},d(b){b&&t(_)}}}function xa(ot){let _,A,$,y,C,b,P,N,Ms,Ut,De,Gs,St,ae,xt,qe,E,Ns,ne,Bs,Hs,le,Os,zs,ie,Vs,Rs,Dt,pe,qt,I,at,fe,Ys,me,Js,Ks,Ws,nt,he,Qs,Le,Xs,Zs,Lt,B,er,ue,tr,sr,Mt,F,H,lt,de,rr,it,or,Gt,Me,ar,Nt,ce,Bt,O,nr,Ge,lr,ir,Ht,U,z,pt,ve,pr,ft,fr,Ot,Ne,mr,zt,ge,Vt,Be,hr,Rt,V,S,ur,He,mt,dr,cr,Oe,vr,gr,wr,x,_r,ze,ht,$r,yr,Ve,br,kr,Yt,R,Jt,D,Y,ut,we,Er,dt,jr,Kt,J,Tr,_e,Ar,Pr,Wt,Re,j,Cr,ct,Ir,Fr,$e,vt,Ur,Sr,Ye,xr,Dr,Qt,ye,Xt,Je,q,qr,gt,Lr,Mr,Ke,Gr,Nr,Zt,be,es,L,K,wt,ke,Br,_t,Hr,ts,W,Or,$t,zr,Vr,ss,Ee,rs,We,Rr,os,je,as,M,Q,yt,Te,Yr,bt,Jr,ns,Qe,Kr,ls,Ae,is,Xe,Wr,ps,Pe,fs,Ze,Qr,ms,Ce,hs,G,X,kt,Ie,Xr,Et,Zr,us,k,eo,jt,to,so,Tt,ro,oo,At,ao,no,ds,Fe,cs,et,lo,vs,Z,io,Pt,po,fo,gs,Ue,ws,ee,mo,Ct,ho,uo,_s,te,It,co,vo,Ft,go,$s;return b=new xe({}),ae=new Aa({props:{id:"S2EEG3JIt2A"}}),pe=new Aa({props:{id:"_PAli-V4wj0"}}),de=new xe({}),ce=new T({props:{code:`ValueError: Connection error, and we cannot find the requested files in the cached path.
Please try again or make sure your Internet connection is on.`,highlighted:`ValueError: Connection error, <span class="hljs-built_in">and</span> we cannot <span class="hljs-keyword">find</span> the requested <span class="hljs-keyword">files</span> in the cached path.
Please <span class="hljs-keyword">try</span> again <span class="hljs-built_in">or</span> <span class="hljs-keyword">make</span> sure your Internet connection <span class="hljs-keyword">is</span> <span class="hljs-keyword">on</span>.`}}),ve=new xe({}),ge=new T({props:{code:"CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 9.70 GiB already allocated; 179.81 MiB free; 9.85 GiB reserved in total by PyTorch)",highlighted:'<span class="hljs-attribute">CUDA</span> out of memory. Tried to allocate <span class="hljs-number">256</span>.<span class="hljs-number">00</span> MiB (GPU <span class="hljs-number">0</span>; <span class="hljs-number">11</span>.<span class="hljs-number">17</span> GiB total capacity; <span class="hljs-number">9</span>.<span class="hljs-number">70</span> GiB already allocated; <span class="hljs-number">179</span>.<span class="hljs-number">81</span> MiB free; <span class="hljs-number">9</span>.<span class="hljs-number">85</span> GiB reserved in total by PyTorch)'}}),R=new Ua({props:{$$slots:{default:[Sa]},$$scope:{ctx:ot}}}),we=new xe({}),ye=new T({props:{code:`from transformers import TFPreTrainedModel
from tensorflow import keras

model.save_weights("some_folder/tf_model.h5")
model = TFPreTrainedModel.from_pretrained("some_folder")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFPreTrainedModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras

<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_weights(<span class="hljs-string">&quot;some_folder/tf_model.h5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFPreTrainedModel.from_pretrained(<span class="hljs-string">&quot;some_folder&quot;</span>)`}}),be=new T({props:{code:`from transformers import TFPreTrainedModel

model.save_pretrained("path_to/model")
model = TFPreTrainedModel.from_pretrained("path_to/model")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFPreTrainedModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;path_to/model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFPreTrainedModel.from_pretrained(<span class="hljs-string">&quot;path_to/model&quot;</span>)`}}),ke=new xe({}),Ee=new T({props:{code:"ImportError: cannot import name 'ImageGPTFeatureExtractor' from 'transformers' (unknown location)",highlighted:'ImportError: cannot <span class="hljs-keyword">import</span> <span class="hljs-type">name</span> <span class="hljs-string">&#x27;ImageGPTFeatureExtractor&#x27;</span> <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;transformers&#x27;</span> (<span class="hljs-type">unknown</span> <span class="hljs-keyword">location</span>)'}}),je=new T({props:{code:"pip install transformers --upgrade",highlighted:"pip install transformers --upgrade"}}),Te=new xe({}),Ae=new T({props:{code:"RuntimeError: CUDA error: device-side assert triggered",highlighted:'RuntimeError: CUDA <span class="hljs-literal">error</span>: device-<span class="hljs-literal">side</span> <span class="hljs-keyword">assert</span> triggered'}}),Pe=new T({props:{code:`import os

os.environ["CUDA_VISIBLE_DEVICES"] = ""`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;&quot;</span>`}}),Ce=new T({props:{code:`import os

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_LAUNCH_BLOCKING&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span>`}}),Ie=new xe({}),Fe=new T({props:{code:`padding_id = 100
input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 100, 100, 100, 100, 100]])
outputs = model(input_ids)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>padding_id = <span class="hljs-number">100</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([[<span class="hljs-number">7592</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2097</span>, <span class="hljs-number">2393</span>, <span class="hljs-number">9611</span>, <span class="hljs-number">2115</span>], [<span class="hljs-number">7592</span>, <span class="hljs-number">100</span>, <span class="hljs-number">100</span>, <span class="hljs-number">100</span>, <span class="hljs-number">100</span>, <span class="hljs-number">100</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)`}}),Ue=new T({props:{code:`attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])
outputs = model(input_ids, attention_mask=attention_mask)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>attention_mask = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids, attention_mask=attention_mask)`}}),{c(){_=o("meta"),A=m(),$=o("h1"),y=o("a"),C=o("span"),u(b.$$.fragment),P=m(),N=o("span"),Ms=l("Troubleshoot"),Ut=m(),De=o("p"),Gs=l("Sometimes errors occur, but we are here to help! This guide covers some of the most common issues we\u2019ve seen and how you can resolve them. However, this guide isn\u2019t meant to be a comprehensive collection of every \u{1F917} Transformers issue. For more help with troubleshooting your issue, try:"),St=m(),u(ae.$$.fragment),xt=m(),qe=o("ol"),E=o("li"),Ns=l("Asking for help on the "),ne=o("a"),Bs=l("forums"),Hs=l(". There are specific categories you can post your question to, like "),le=o("a"),Os=l("Beginners"),zs=l(" or "),ie=o("a"),Vs=l("\u{1F917} Transformers"),Rs=l(". Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!"),Dt=m(),u(pe.$$.fragment),qt=m(),I=o("ol"),at=o("li"),fe=o("p"),Ys=l("Create an "),me=o("a"),Js=l("Issue"),Ks=l(" on the \u{1F917} Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out what\u2019s wrong and how we can fix it."),Ws=m(),nt=o("li"),he=o("p"),Qs=l("Check the "),Le=o("a"),Xs=l("Migration"),Zs=l(" guide if you use an older version of \u{1F917} Transformers since some important changes have been introduced between versions."),Lt=m(),B=o("p"),er=l("For more details about troubleshooting and getting help, take a look at "),ue=o("a"),tr=l("Chapter 8"),sr=l(" of the Hugging Face course."),Mt=m(),F=o("h2"),H=o("a"),lt=o("span"),u(de.$$.fragment),rr=m(),it=o("span"),or=l("Firewalled environments"),Gt=m(),Me=o("p"),ar=l("Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:"),Nt=m(),u(ce.$$.fragment),Bt=m(),O=o("p"),nr=l("In this case, you should try to run \u{1F917} Transformers on "),Ge=o("a"),lr=l("offline mode"),ir=l(" to avoid the connection error."),Ht=m(),U=o("h2"),z=o("a"),pt=o("span"),u(ve.$$.fragment),pr=m(),ft=o("span"),fr=l("CUDA out of memory"),Ot=m(),Ne=o("p"),mr=l("Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:"),zt=m(),u(ge.$$.fragment),Vt=m(),Be=o("p"),hr=l("Here are some potential solutions you can try to lessen memory use:"),Rt=m(),V=o("ul"),S=o("li"),ur=l("Reduce the "),He=o("a"),mt=o("code"),dr=l("per_device_train_batch_size"),cr=l(" value in "),Oe=o("a"),vr=l("TrainingArguments"),gr=l("."),wr=m(),x=o("li"),_r=l("Try using "),ze=o("a"),ht=o("code"),$r=l("gradient_accumulation_steps"),yr=l(" in "),Ve=o("a"),br=l("TrainingArguments"),kr=l(" to effectively increase overall batch size."),Yt=m(),u(R.$$.fragment),Jt=m(),D=o("h2"),Y=o("a"),ut=o("span"),u(we.$$.fragment),Er=m(),dt=o("span"),jr=l("Unable to load a saved TensorFlow model"),Kt=m(),J=o("p"),Tr=l("TensorFlow\u2019s "),_e=o("a"),Ar=l("model.save"),Pr=l(" method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \u{1F917} Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:"),Wt=m(),Re=o("ul"),j=o("li"),Cr=l("Save the model weights as a "),ct=o("code"),Ir=l("h5"),Fr=l(" file extension with "),$e=o("a"),vt=o("code"),Ur=l("model.save_weights"),Sr=l(" and then reload the model with "),Ye=o("a"),xr=l("from_pretrained()"),Dr=l(":"),Qt=m(),u(ye.$$.fragment),Xt=m(),Je=o("ul"),q=o("li"),qr=l("Save the model with "),gt=o("code"),Lr=l("save_pretrained"),Mr=l(" and load it again with "),Ke=o("a"),Gr=l("from_pretrained()"),Nr=l(":"),Zt=m(),u(be.$$.fragment),es=m(),L=o("h2"),K=o("a"),wt=o("span"),u(ke.$$.fragment),Br=m(),_t=o("span"),Hr=l("ImportError"),ts=m(),W=o("p"),Or=l("Another common error you may encounter, especially if it is a newly released model, is "),$t=o("code"),zr=l("ImportError"),Vr=l(":"),ss=m(),u(Ee.$$.fragment),rs=m(),We=o("p"),Rr=l("For these error types, check to make sure you have the latest version of \u{1F917} Transformers installed to access the most recent models:"),os=m(),u(je.$$.fragment),as=m(),M=o("h2"),Q=o("a"),yt=o("span"),u(Te.$$.fragment),Yr=m(),bt=o("span"),Jr=l("CUDA error: device-side assert triggered"),ns=m(),Qe=o("p"),Kr=l("Sometimes you may run into a generic CUDA error about an error in the device code."),ls=m(),u(Ae.$$.fragment),is=m(),Xe=o("p"),Wr=l("You should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to switch to a CPU:"),ps=m(),u(Pe.$$.fragment),fs=m(),Ze=o("p"),Qr=l("Another option is to get a better traceback from the GPU. Add the following environment variable to the beginning of your code to get the traceback to point to the source of the error:"),ms=m(),u(Ce.$$.fragment),hs=m(),G=o("h2"),X=o("a"),kt=o("span"),u(Ie.$$.fragment),Xr=m(),Et=o("span"),Zr=l("Incorrect output when padding tokens aren't masked"),us=m(),k=o("p"),eo=l("In some cases, you may notice the output "),jt=o("code"),to=l("hidden_state"),so=l(" may be incorrect if the "),Tt=o("code"),ro=l("input_ids"),oo=l(" include padding tokens. For example, a batch of tensors may look like this before you pass it to "),At=o("code"),ao=l("forward()"),no=l(":"),ds=m(),u(Fe.$$.fragment),cs=m(),et=o("p"),lo=l("The output value here will be different than if you included an attention mask because the model also attends to the padding tokens."),vs=m(),Z=o("p"),io=l("Most of the time, you should provide an "),Pt=o("code"),po=l("attention_mask"),fo=l(" to your model to ignore the padding tokens:"),gs=m(),u(Ue.$$.fragment),ws=m(),ee=o("p"),mo=l("\u{1F917} Transformers doesn\u2019t automatically create an "),Ct=o("code"),ho=l("attention_mask"),uo=l(" if a padding token is present because:"),_s=m(),te=o("ul"),It=o("li"),co=l("Some models don\u2019t have a padding token."),vo=m(),Ft=o("li"),go=l("For some use-cases, users want a model to attend to a padding token."),this.h()},l(e){const r=Fa('[data-svelte="svelte-1phssyn"]',document.head);_=a(r,"META",{name:!0,content:!0}),r.forEach(t),A=h(e),$=a(e,"H1",{class:!0});var Se=n($);y=a(Se,"A",{id:!0,class:!0,href:!0});var wo=n(y);C=a(wo,"SPAN",{});var _o=n(C);d(b.$$.fragment,_o),_o.forEach(t),wo.forEach(t),P=h(Se),N=a(Se,"SPAN",{});var $o=n(N);Ms=i($o,"Troubleshoot"),$o.forEach(t),Se.forEach(t),Ut=h(e),De=a(e,"P",{});var yo=n(De);Gs=i(yo,"Sometimes errors occur, but we are here to help! This guide covers some of the most common issues we\u2019ve seen and how you can resolve them. However, this guide isn\u2019t meant to be a comprehensive collection of every \u{1F917} Transformers issue. For more help with troubleshooting your issue, try:"),yo.forEach(t),St=h(e),d(ae.$$.fragment,e),xt=h(e),qe=a(e,"OL",{});var bo=n(qe);E=a(bo,"LI",{});var se=n(E);Ns=i(se,"Asking for help on the "),ne=a(se,"A",{href:!0,rel:!0});var ko=n(ne);Bs=i(ko,"forums"),ko.forEach(t),Hs=i(se,". There are specific categories you can post your question to, like "),le=a(se,"A",{href:!0,rel:!0});var Eo=n(le);Os=i(Eo,"Beginners"),Eo.forEach(t),zs=i(se," or "),ie=a(se,"A",{href:!0,rel:!0});var jo=n(ie);Vs=i(jo,"\u{1F917} Transformers"),jo.forEach(t),Rs=i(se,". Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!"),se.forEach(t),bo.forEach(t),Dt=h(e),d(pe.$$.fragment,e),qt=h(e),I=a(e,"OL",{start:!0});var ys=n(I);at=a(ys,"LI",{});var To=n(at);fe=a(To,"P",{});var bs=n(fe);Ys=i(bs,"Create an "),me=a(bs,"A",{href:!0,rel:!0});var Ao=n(me);Js=i(Ao,"Issue"),Ao.forEach(t),Ks=i(bs," on the \u{1F917} Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out what\u2019s wrong and how we can fix it."),bs.forEach(t),To.forEach(t),Ws=h(ys),nt=a(ys,"LI",{});var Po=n(nt);he=a(Po,"P",{});var ks=n(he);Qs=i(ks,"Check the "),Le=a(ks,"A",{href:!0});var Co=n(Le);Xs=i(Co,"Migration"),Co.forEach(t),Zs=i(ks," guide if you use an older version of \u{1F917} Transformers since some important changes have been introduced between versions."),ks.forEach(t),Po.forEach(t),ys.forEach(t),Lt=h(e),B=a(e,"P",{});var Es=n(B);er=i(Es,"For more details about troubleshooting and getting help, take a look at "),ue=a(Es,"A",{href:!0,rel:!0});var Io=n(ue);tr=i(Io,"Chapter 8"),Io.forEach(t),sr=i(Es," of the Hugging Face course."),Es.forEach(t),Mt=h(e),F=a(e,"H2",{class:!0});var js=n(F);H=a(js,"A",{id:!0,class:!0,href:!0});var Fo=n(H);lt=a(Fo,"SPAN",{});var Uo=n(lt);d(de.$$.fragment,Uo),Uo.forEach(t),Fo.forEach(t),rr=h(js),it=a(js,"SPAN",{});var So=n(it);or=i(So,"Firewalled environments"),So.forEach(t),js.forEach(t),Gt=h(e),Me=a(e,"P",{});var xo=n(Me);ar=i(xo,"Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:"),xo.forEach(t),Nt=h(e),d(ce.$$.fragment,e),Bt=h(e),O=a(e,"P",{});var Ts=n(O);nr=i(Ts,"In this case, you should try to run \u{1F917} Transformers on "),Ge=a(Ts,"A",{href:!0});var Do=n(Ge);lr=i(Do,"offline mode"),Do.forEach(t),ir=i(Ts," to avoid the connection error."),Ts.forEach(t),Ht=h(e),U=a(e,"H2",{class:!0});var As=n(U);z=a(As,"A",{id:!0,class:!0,href:!0});var qo=n(z);pt=a(qo,"SPAN",{});var Lo=n(pt);d(ve.$$.fragment,Lo),Lo.forEach(t),qo.forEach(t),pr=h(As),ft=a(As,"SPAN",{});var Mo=n(ft);fr=i(Mo,"CUDA out of memory"),Mo.forEach(t),As.forEach(t),Ot=h(e),Ne=a(e,"P",{});var Go=n(Ne);mr=i(Go,"Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:"),Go.forEach(t),zt=h(e),d(ge.$$.fragment,e),Vt=h(e),Be=a(e,"P",{});var No=n(Be);hr=i(No,"Here are some potential solutions you can try to lessen memory use:"),No.forEach(t),Rt=h(e),V=a(e,"UL",{});var Ps=n(V);S=a(Ps,"LI",{});var tt=n(S);ur=i(tt,"Reduce the "),He=a(tt,"A",{href:!0});var Bo=n(He);mt=a(Bo,"CODE",{});var Ho=n(mt);dr=i(Ho,"per_device_train_batch_size"),Ho.forEach(t),Bo.forEach(t),cr=i(tt," value in "),Oe=a(tt,"A",{href:!0});var Oo=n(Oe);vr=i(Oo,"TrainingArguments"),Oo.forEach(t),gr=i(tt,"."),tt.forEach(t),wr=h(Ps),x=a(Ps,"LI",{});var st=n(x);_r=i(st,"Try using "),ze=a(st,"A",{href:!0});var zo=n(ze);ht=a(zo,"CODE",{});var Vo=n(ht);$r=i(Vo,"gradient_accumulation_steps"),Vo.forEach(t),zo.forEach(t),yr=i(st," in "),Ve=a(st,"A",{href:!0});var Ro=n(Ve);br=i(Ro,"TrainingArguments"),Ro.forEach(t),kr=i(st," to effectively increase overall batch size."),st.forEach(t),Ps.forEach(t),Yt=h(e),d(R.$$.fragment,e),Jt=h(e),D=a(e,"H2",{class:!0});var Cs=n(D);Y=a(Cs,"A",{id:!0,class:!0,href:!0});var Yo=n(Y);ut=a(Yo,"SPAN",{});var Jo=n(ut);d(we.$$.fragment,Jo),Jo.forEach(t),Yo.forEach(t),Er=h(Cs),dt=a(Cs,"SPAN",{});var Ko=n(dt);jr=i(Ko,"Unable to load a saved TensorFlow model"),Ko.forEach(t),Cs.forEach(t),Kt=h(e),J=a(e,"P",{});var Is=n(J);Tr=i(Is,"TensorFlow\u2019s "),_e=a(Is,"A",{href:!0,rel:!0});var Wo=n(_e);Ar=i(Wo,"model.save"),Wo.forEach(t),Pr=i(Is," method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \u{1F917} Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:"),Is.forEach(t),Wt=h(e),Re=a(e,"UL",{});var Qo=n(Re);j=a(Qo,"LI",{});var re=n(j);Cr=i(re,"Save the model weights as a "),ct=a(re,"CODE",{});var Xo=n(ct);Ir=i(Xo,"h5"),Xo.forEach(t),Fr=i(re," file extension with "),$e=a(re,"A",{href:!0,rel:!0});var Zo=n($e);vt=a(Zo,"CODE",{});var ea=n(vt);Ur=i(ea,"model.save_weights"),ea.forEach(t),Zo.forEach(t),Sr=i(re," and then reload the model with "),Ye=a(re,"A",{href:!0});var ta=n(Ye);xr=i(ta,"from_pretrained()"),ta.forEach(t),Dr=i(re,":"),re.forEach(t),Qo.forEach(t),Qt=h(e),d(ye.$$.fragment,e),Xt=h(e),Je=a(e,"UL",{});var sa=n(Je);q=a(sa,"LI",{});var rt=n(q);qr=i(rt,"Save the model with "),gt=a(rt,"CODE",{});var ra=n(gt);Lr=i(ra,"save_pretrained"),ra.forEach(t),Mr=i(rt," and load it again with "),Ke=a(rt,"A",{href:!0});var oa=n(Ke);Gr=i(oa,"from_pretrained()"),oa.forEach(t),Nr=i(rt,":"),rt.forEach(t),sa.forEach(t),Zt=h(e),d(be.$$.fragment,e),es=h(e),L=a(e,"H2",{class:!0});var Fs=n(L);K=a(Fs,"A",{id:!0,class:!0,href:!0});var aa=n(K);wt=a(aa,"SPAN",{});var na=n(wt);d(ke.$$.fragment,na),na.forEach(t),aa.forEach(t),Br=h(Fs),_t=a(Fs,"SPAN",{});var la=n(_t);Hr=i(la,"ImportError"),la.forEach(t),Fs.forEach(t),ts=h(e),W=a(e,"P",{});var Us=n(W);Or=i(Us,"Another common error you may encounter, especially if it is a newly released model, is "),$t=a(Us,"CODE",{});var ia=n($t);zr=i(ia,"ImportError"),ia.forEach(t),Vr=i(Us,":"),Us.forEach(t),ss=h(e),d(Ee.$$.fragment,e),rs=h(e),We=a(e,"P",{});var pa=n(We);Rr=i(pa,"For these error types, check to make sure you have the latest version of \u{1F917} Transformers installed to access the most recent models:"),pa.forEach(t),os=h(e),d(je.$$.fragment,e),as=h(e),M=a(e,"H2",{class:!0});var Ss=n(M);Q=a(Ss,"A",{id:!0,class:!0,href:!0});var fa=n(Q);yt=a(fa,"SPAN",{});var ma=n(yt);d(Te.$$.fragment,ma),ma.forEach(t),fa.forEach(t),Yr=h(Ss),bt=a(Ss,"SPAN",{});var ha=n(bt);Jr=i(ha,"CUDA error: device-side assert triggered"),ha.forEach(t),Ss.forEach(t),ns=h(e),Qe=a(e,"P",{});var ua=n(Qe);Kr=i(ua,"Sometimes you may run into a generic CUDA error about an error in the device code."),ua.forEach(t),ls=h(e),d(Ae.$$.fragment,e),is=h(e),Xe=a(e,"P",{});var da=n(Xe);Wr=i(da,"You should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to switch to a CPU:"),da.forEach(t),ps=h(e),d(Pe.$$.fragment,e),fs=h(e),Ze=a(e,"P",{});var ca=n(Ze);Qr=i(ca,"Another option is to get a better traceback from the GPU. Add the following environment variable to the beginning of your code to get the traceback to point to the source of the error:"),ca.forEach(t),ms=h(e),d(Ce.$$.fragment,e),hs=h(e),G=a(e,"H2",{class:!0});var xs=n(G);X=a(xs,"A",{id:!0,class:!0,href:!0});var va=n(X);kt=a(va,"SPAN",{});var ga=n(kt);d(Ie.$$.fragment,ga),ga.forEach(t),va.forEach(t),Xr=h(xs),Et=a(xs,"SPAN",{});var wa=n(Et);Zr=i(wa,"Incorrect output when padding tokens aren't masked"),wa.forEach(t),xs.forEach(t),us=h(e),k=a(e,"P",{});var oe=n(k);eo=i(oe,"In some cases, you may notice the output "),jt=a(oe,"CODE",{});var _a=n(jt);to=i(_a,"hidden_state"),_a.forEach(t),so=i(oe," may be incorrect if the "),Tt=a(oe,"CODE",{});var $a=n(Tt);ro=i($a,"input_ids"),$a.forEach(t),oo=i(oe," include padding tokens. For example, a batch of tensors may look like this before you pass it to "),At=a(oe,"CODE",{});var ya=n(At);ao=i(ya,"forward()"),ya.forEach(t),no=i(oe,":"),oe.forEach(t),ds=h(e),d(Fe.$$.fragment,e),cs=h(e),et=a(e,"P",{});var ba=n(et);lo=i(ba,"The output value here will be different than if you included an attention mask because the model also attends to the padding tokens."),ba.forEach(t),vs=h(e),Z=a(e,"P",{});var Ds=n(Z);io=i(Ds,"Most of the time, you should provide an "),Pt=a(Ds,"CODE",{});var ka=n(Pt);po=i(ka,"attention_mask"),ka.forEach(t),fo=i(Ds," to your model to ignore the padding tokens:"),Ds.forEach(t),gs=h(e),d(Ue.$$.fragment,e),ws=h(e),ee=a(e,"P",{});var qs=n(ee);mo=i(qs,"\u{1F917} Transformers doesn\u2019t automatically create an "),Ct=a(qs,"CODE",{});var Ea=n(Ct);ho=i(Ea,"attention_mask"),Ea.forEach(t),uo=i(qs," if a padding token is present because:"),qs.forEach(t),_s=h(e),te=a(e,"UL",{});var Ls=n(te);It=a(Ls,"LI",{});var ja=n(It);co=i(ja,"Some models don\u2019t have a padding token."),ja.forEach(t),vo=h(Ls),Ft=a(Ls,"LI",{});var Ta=n(Ft);go=i(Ta,"For some use-cases, users want a model to attend to a padding token."),Ta.forEach(t),Ls.forEach(t),this.h()},h(){f(_,"name","hf:doc:metadata"),f(_,"content",JSON.stringify(Da)),f(y,"id","troubleshoot"),f(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(y,"href","#troubleshoot"),f($,"class","relative group"),f(ne,"href","https://discuss.huggingface.co/"),f(ne,"rel","nofollow"),f(le,"href","https://discuss.huggingface.co/c/beginners/5"),f(le,"rel","nofollow"),f(ie,"href","https://discuss.huggingface.co/c/transformers/9"),f(ie,"rel","nofollow"),f(me,"href","https://github.com/huggingface/transformers/issues/new/choose"),f(me,"rel","nofollow"),f(Le,"href","migration"),f(I,"start","2"),f(ue,"href","https://huggingface.co/course/chapter8/1?fw=pt"),f(ue,"rel","nofollow"),f(H,"id","firewalled-environments"),f(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(H,"href","#firewalled-environments"),f(F,"class","relative group"),f(Ge,"href","installation#offline-mode"),f(z,"id","cuda-out-of-memory"),f(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(z,"href","#cuda-out-of-memory"),f(U,"class","relative group"),f(He,"href","main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size"),f(Oe,"href","/docs/transformers/pr_16243/en/main_classes/trainer#transformers.TrainingArguments"),f(ze,"href","main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps"),f(Ve,"href","/docs/transformers/pr_16243/en/main_classes/trainer#transformers.TrainingArguments"),f(Y,"id","unable-to-load-a-saved-tensorflow-model"),f(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Y,"href","#unable-to-load-a-saved-tensorflow-model"),f(D,"class","relative group"),f(_e,"href","https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"),f(_e,"rel","nofollow"),f($e,"href","https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model"),f($e,"rel","nofollow"),f(Ye,"href","/docs/transformers/pr_16243/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained"),f(Ke,"href","/docs/transformers/pr_16243/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained"),f(K,"id","importerror"),f(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(K,"href","#importerror"),f(L,"class","relative group"),f(Q,"id","cuda-error-deviceside-assert-triggered"),f(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Q,"href","#cuda-error-deviceside-assert-triggered"),f(M,"class","relative group"),f(X,"id","incorrect-output-when-padding-tokens-arent-masked"),f(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(X,"href","#incorrect-output-when-padding-tokens-arent-masked"),f(G,"class","relative group")},m(e,r){s(document.head,_),p(e,A,r),p(e,$,r),s($,y),s(y,C),c(b,C,null),s($,P),s($,N),s(N,Ms),p(e,Ut,r),p(e,De,r),s(De,Gs),p(e,St,r),c(ae,e,r),p(e,xt,r),p(e,qe,r),s(qe,E),s(E,Ns),s(E,ne),s(ne,Bs),s(E,Hs),s(E,le),s(le,Os),s(E,zs),s(E,ie),s(ie,Vs),s(E,Rs),p(e,Dt,r),c(pe,e,r),p(e,qt,r),p(e,I,r),s(I,at),s(at,fe),s(fe,Ys),s(fe,me),s(me,Js),s(fe,Ks),s(I,Ws),s(I,nt),s(nt,he),s(he,Qs),s(he,Le),s(Le,Xs),s(he,Zs),p(e,Lt,r),p(e,B,r),s(B,er),s(B,ue),s(ue,tr),s(B,sr),p(e,Mt,r),p(e,F,r),s(F,H),s(H,lt),c(de,lt,null),s(F,rr),s(F,it),s(it,or),p(e,Gt,r),p(e,Me,r),s(Me,ar),p(e,Nt,r),c(ce,e,r),p(e,Bt,r),p(e,O,r),s(O,nr),s(O,Ge),s(Ge,lr),s(O,ir),p(e,Ht,r),p(e,U,r),s(U,z),s(z,pt),c(ve,pt,null),s(U,pr),s(U,ft),s(ft,fr),p(e,Ot,r),p(e,Ne,r),s(Ne,mr),p(e,zt,r),c(ge,e,r),p(e,Vt,r),p(e,Be,r),s(Be,hr),p(e,Rt,r),p(e,V,r),s(V,S),s(S,ur),s(S,He),s(He,mt),s(mt,dr),s(S,cr),s(S,Oe),s(Oe,vr),s(S,gr),s(V,wr),s(V,x),s(x,_r),s(x,ze),s(ze,ht),s(ht,$r),s(x,yr),s(x,Ve),s(Ve,br),s(x,kr),p(e,Yt,r),c(R,e,r),p(e,Jt,r),p(e,D,r),s(D,Y),s(Y,ut),c(we,ut,null),s(D,Er),s(D,dt),s(dt,jr),p(e,Kt,r),p(e,J,r),s(J,Tr),s(J,_e),s(_e,Ar),s(J,Pr),p(e,Wt,r),p(e,Re,r),s(Re,j),s(j,Cr),s(j,ct),s(ct,Ir),s(j,Fr),s(j,$e),s($e,vt),s(vt,Ur),s(j,Sr),s(j,Ye),s(Ye,xr),s(j,Dr),p(e,Qt,r),c(ye,e,r),p(e,Xt,r),p(e,Je,r),s(Je,q),s(q,qr),s(q,gt),s(gt,Lr),s(q,Mr),s(q,Ke),s(Ke,Gr),s(q,Nr),p(e,Zt,r),c(be,e,r),p(e,es,r),p(e,L,r),s(L,K),s(K,wt),c(ke,wt,null),s(L,Br),s(L,_t),s(_t,Hr),p(e,ts,r),p(e,W,r),s(W,Or),s(W,$t),s($t,zr),s(W,Vr),p(e,ss,r),c(Ee,e,r),p(e,rs,r),p(e,We,r),s(We,Rr),p(e,os,r),c(je,e,r),p(e,as,r),p(e,M,r),s(M,Q),s(Q,yt),c(Te,yt,null),s(M,Yr),s(M,bt),s(bt,Jr),p(e,ns,r),p(e,Qe,r),s(Qe,Kr),p(e,ls,r),c(Ae,e,r),p(e,is,r),p(e,Xe,r),s(Xe,Wr),p(e,ps,r),c(Pe,e,r),p(e,fs,r),p(e,Ze,r),s(Ze,Qr),p(e,ms,r),c(Ce,e,r),p(e,hs,r),p(e,G,r),s(G,X),s(X,kt),c(Ie,kt,null),s(G,Xr),s(G,Et),s(Et,Zr),p(e,us,r),p(e,k,r),s(k,eo),s(k,jt),s(jt,to),s(k,so),s(k,Tt),s(Tt,ro),s(k,oo),s(k,At),s(At,ao),s(k,no),p(e,ds,r),c(Fe,e,r),p(e,cs,r),p(e,et,r),s(et,lo),p(e,vs,r),p(e,Z,r),s(Z,io),s(Z,Pt),s(Pt,po),s(Z,fo),p(e,gs,r),c(Ue,e,r),p(e,ws,r),p(e,ee,r),s(ee,mo),s(ee,Ct),s(Ct,ho),s(ee,uo),p(e,_s,r),p(e,te,r),s(te,It),s(It,co),s(te,vo),s(te,Ft),s(Ft,go),$s=!0},p(e,[r]){const Se={};r&2&&(Se.$$scope={dirty:r,ctx:e}),R.$set(Se)},i(e){$s||(v(b.$$.fragment,e),v(ae.$$.fragment,e),v(pe.$$.fragment,e),v(de.$$.fragment,e),v(ce.$$.fragment,e),v(ve.$$.fragment,e),v(ge.$$.fragment,e),v(R.$$.fragment,e),v(we.$$.fragment,e),v(ye.$$.fragment,e),v(be.$$.fragment,e),v(ke.$$.fragment,e),v(Ee.$$.fragment,e),v(je.$$.fragment,e),v(Te.$$.fragment,e),v(Ae.$$.fragment,e),v(Pe.$$.fragment,e),v(Ce.$$.fragment,e),v(Ie.$$.fragment,e),v(Fe.$$.fragment,e),v(Ue.$$.fragment,e),$s=!0)},o(e){g(b.$$.fragment,e),g(ae.$$.fragment,e),g(pe.$$.fragment,e),g(de.$$.fragment,e),g(ce.$$.fragment,e),g(ve.$$.fragment,e),g(ge.$$.fragment,e),g(R.$$.fragment,e),g(we.$$.fragment,e),g(ye.$$.fragment,e),g(be.$$.fragment,e),g(ke.$$.fragment,e),g(Ee.$$.fragment,e),g(je.$$.fragment,e),g(Te.$$.fragment,e),g(Ae.$$.fragment,e),g(Pe.$$.fragment,e),g(Ce.$$.fragment,e),g(Ie.$$.fragment,e),g(Fe.$$.fragment,e),g(Ue.$$.fragment,e),$s=!1},d(e){t(_),e&&t(A),e&&t($),w(b),e&&t(Ut),e&&t(De),e&&t(St),w(ae,e),e&&t(xt),e&&t(qe),e&&t(Dt),w(pe,e),e&&t(qt),e&&t(I),e&&t(Lt),e&&t(B),e&&t(Mt),e&&t(F),w(de),e&&t(Gt),e&&t(Me),e&&t(Nt),w(ce,e),e&&t(Bt),e&&t(O),e&&t(Ht),e&&t(U),w(ve),e&&t(Ot),e&&t(Ne),e&&t(zt),w(ge,e),e&&t(Vt),e&&t(Be),e&&t(Rt),e&&t(V),e&&t(Yt),w(R,e),e&&t(Jt),e&&t(D),w(we),e&&t(Kt),e&&t(J),e&&t(Wt),e&&t(Re),e&&t(Qt),w(ye,e),e&&t(Xt),e&&t(Je),e&&t(Zt),w(be,e),e&&t(es),e&&t(L),w(ke),e&&t(ts),e&&t(W),e&&t(ss),w(Ee,e),e&&t(rs),e&&t(We),e&&t(os),w(je,e),e&&t(as),e&&t(M),w(Te),e&&t(ns),e&&t(Qe),e&&t(ls),w(Ae,e),e&&t(is),e&&t(Xe),e&&t(ps),w(Pe,e),e&&t(fs),e&&t(Ze),e&&t(ms),w(Ce,e),e&&t(hs),e&&t(G),w(Ie),e&&t(us),e&&t(k),e&&t(ds),w(Fe,e),e&&t(cs),e&&t(et),e&&t(vs),e&&t(Z),e&&t(gs),w(Ue,e),e&&t(ws),e&&t(ee),e&&t(_s),e&&t(te)}}}const Da={local:"troubleshoot",sections:[{local:"firewalled-environments",title:"Firewalled environments"},{local:"cuda-out-of-memory",title:"CUDA out of memory"},{local:"unable-to-load-a-saved-tensorflow-model",title:"Unable to load a saved TensorFlow model"},{local:"importerror",title:"ImportError"},{local:"cuda-error-deviceside-assert-triggered",title:"CUDA error: device-side assert triggered"},{local:"incorrect-output-when-padding-tokens-arent-masked",title:"Incorrect output when padding tokens aren't masked"}],title:"Troubleshoot"};function qa(ot,_,A){let{fw:$}=_;return ot.$$set=y=>{"fw"in y&&A(0,$=y.fw)},[$]}class Ha extends Pa{constructor(_){super();Ca(this,_,qa,xa,Ia,{fw:0})}}export{Ha as default,Da as metadata};
