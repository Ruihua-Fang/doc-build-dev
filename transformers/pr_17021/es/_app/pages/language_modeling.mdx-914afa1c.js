import{S as fo,i as ho,s as go,e as i,k as f,w as D,t as l,M as _o,c as p,d as t,m as h,a as c,x as C,h as o,b as E,F as a,g as m,y as T,q as P,o as A,B as M,v as $o,L as uo}from"../chunks/vendor-5fc3b424.js";import{T as Ca}from"../chunks/Tip-12425c03.js";import{Y as es}from"../chunks/Youtube-22419068.js";import{I as wa,C as se}from"../chunks/CodeBlock-f0535003.js";import{F as ul,M as Ta}from"../chunks/Markdown-269919ba.js";function jo(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U,_;return{c(){s=i("p"),j=l("Puedes realizar fine-tune de otras arquitecturas para modelos de lenguaje como "),r=i("a"),g=l("GPT-Neo"),w=l(", "),$=i("a"),b=l("GPT-J"),q=l(", y "),v=i("a"),x=l("BERT"),F=l(", siguiendo los mismos pasos presentados en esta gu\xEDa!"),N=f(),G=i("p"),z=l("Mira la "),R=i("a"),S=l("p\xE1gina de tarea"),B=l(" para generaci\xF3n de texto y la "),I=i("a"),U=l("p\xE1gina de tarea"),_=l(" para modelos de lenguajes por enmascaramiento para obtener m\xE1s informaci\xF3n sobre los modelos, conjuntos de datos, y m\xE9tricas asociadas."),this.h()},l(k){s=p(k,"P",{});var H=c(s);j=o(H,"Puedes realizar fine-tune de otras arquitecturas para modelos de lenguaje como "),r=p(H,"A",{href:!0,rel:!0});var L=c(r);g=o(L,"GPT-Neo"),L.forEach(t),w=o(H,", "),$=p(H,"A",{href:!0,rel:!0});var W=c($);b=o(W,"GPT-J"),W.forEach(t),q=o(H,", y "),v=p(H,"A",{href:!0,rel:!0});var V=c(v);x=o(V,"BERT"),V.forEach(t),F=o(H,", siguiendo los mismos pasos presentados en esta gu\xEDa!"),H.forEach(t),N=h(k),G=p(k,"P",{});var ee=c(G);z=o(ee,"Mira la "),R=p(ee,"A",{href:!0,rel:!0});var ae=c(R);S=o(ae,"p\xE1gina de tarea"),ae.forEach(t),B=o(ee," para generaci\xF3n de texto y la "),I=p(ee,"A",{href:!0,rel:!0});var pe=c(I);U=o(pe,"p\xE1gina de tarea"),pe.forEach(t),_=o(ee," para modelos de lenguajes por enmascaramiento para obtener m\xE1s informaci\xF3n sobre los modelos, conjuntos de datos, y m\xE9tricas asociadas."),ee.forEach(t),this.h()},h(){E(r,"href","https://huggingface.co/EleutherAI/gpt-neo-125M"),E(r,"rel","nofollow"),E($,"href","https://huggingface.co/EleutherAI/gpt-j-6B"),E($,"rel","nofollow"),E(v,"href","https://huggingface.co/bert-base-uncased"),E(v,"rel","nofollow"),E(R,"href","https://huggingface.co/tasks/text-generation"),E(R,"rel","nofollow"),E(I,"href","https://huggingface.co/tasks/fill-mask"),E(I,"rel","nofollow")},m(k,H){m(k,s,H),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q),a(s,v),a(v,x),a(s,F),m(k,N,H),m(k,G,H),a(G,z),a(G,R),a(R,S),a(G,B),a(G,I),a(I,U),a(G,_)},d(k){k&&t(s),k&&t(N),k&&t(G)}}}function bo(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U;return b=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>)`}}),I=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`}}),{c(){s=i("p"),j=l("Puedes usar el token de final de secuencia como el token de relleno, y asignar "),r=i("code"),g=l("mlm=False"),w=l(". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),$=f(),D(b.$$.fragment),q=f(),v=i("p"),x=l("Para modelados de lenguajes por enmascaramiento usa el mismo "),F=i("code"),N=l("DataCollatorForLanguageModeling"),G=l(" excepto que deber\xEDas especificar "),z=i("code"),R=l("mlm_probability"),S=l(" para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),B=f(),D(I.$$.fragment)},l(_){s=p(_,"P",{});var k=c(s);j=o(k,"Puedes usar el token de final de secuencia como el token de relleno, y asignar "),r=p(k,"CODE",{});var H=c(r);g=o(H,"mlm=False"),H.forEach(t),w=o(k,". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),k.forEach(t),$=h(_),C(b.$$.fragment,_),q=h(_),v=p(_,"P",{});var L=c(v);x=o(L,"Para modelados de lenguajes por enmascaramiento usa el mismo "),F=p(L,"CODE",{});var W=c(F);N=o(W,"DataCollatorForLanguageModeling"),W.forEach(t),G=o(L," excepto que deber\xEDas especificar "),z=p(L,"CODE",{});var V=c(z);R=o(V,"mlm_probability"),V.forEach(t),S=o(L," para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),L.forEach(t),B=h(_),C(I.$$.fragment,_)},m(_,k){m(_,s,k),a(s,j),a(s,r),a(r,g),a(s,w),m(_,$,k),T(b,_,k),m(_,q,k),m(_,v,k),a(v,x),a(v,F),a(F,N),a(v,G),a(v,z),a(z,R),a(v,S),m(_,B,k),T(I,_,k),U=!0},p:uo,i(_){U||(P(b.$$.fragment,_),P(I.$$.fragment,_),U=!0)},o(_){A(b.$$.fragment,_),A(I.$$.fragment,_),U=!1},d(_){_&&t(s),_&&t($),M(b,_),_&&t(q),_&&t(v),_&&t(B),M(I,_)}}}function vo(J){let s,j;return s=new Ta({props:{$$slots:{default:[bo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){C(s.$$.fragment,r)},m(r,g){T(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(P(s.$$.fragment,r),j=!0)},o(r){A(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function ko(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U;return b=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),I=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){s=i("p"),j=l("Puedes usar el token de final de secuencia como el token de relleno, y asignar "),r=i("code"),g=l("mlm=False"),w=l(". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),$=f(),D(b.$$.fragment),q=f(),v=i("p"),x=l("Para modelados de lenguajes por enmascaramiento usa el mismo "),F=i("code"),N=l("DataCollatorForLanguageModeling"),G=l(" excepto que deber\xEDas especificar "),z=i("code"),R=l("mlm_probability"),S=l(" para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),B=f(),D(I.$$.fragment)},l(_){s=p(_,"P",{});var k=c(s);j=o(k,"Puedes usar el token de final de secuencia como el token de relleno, y asignar "),r=p(k,"CODE",{});var H=c(r);g=o(H,"mlm=False"),H.forEach(t),w=o(k,". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),k.forEach(t),$=h(_),C(b.$$.fragment,_),q=h(_),v=p(_,"P",{});var L=c(v);x=o(L,"Para modelados de lenguajes por enmascaramiento usa el mismo "),F=p(L,"CODE",{});var W=c(F);N=o(W,"DataCollatorForLanguageModeling"),W.forEach(t),G=o(L," excepto que deber\xEDas especificar "),z=p(L,"CODE",{});var V=c(z);R=o(V,"mlm_probability"),V.forEach(t),S=o(L," para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),L.forEach(t),B=h(_),C(I.$$.fragment,_)},m(_,k){m(_,s,k),a(s,j),a(s,r),a(r,g),a(s,w),m(_,$,k),T(b,_,k),m(_,q,k),m(_,v,k),a(v,x),a(v,F),a(F,N),a(v,G),a(v,z),a(z,R),a(v,S),m(_,B,k),T(I,_,k),U=!0},p:uo,i(_){U||(P(b.$$.fragment,_),P(I.$$.fragment,_),U=!0)},o(_){A(b.$$.fragment,_),A(I.$$.fragment,_),U=!1},d(_){_&&t(s),_&&t($),M(b,_),_&&t(q),_&&t(v),_&&t(B),M(I,_)}}}function Eo(J){let s,j;return s=new Ta({props:{$$slots:{default:[ko]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){C(s.$$.fragment,r)},m(r,g){T(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(P(s.$$.fragment,r),j=!0)},o(r){A(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function wo(J){let s,j,r,g,w,$,b,q;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizado con el proceso de realizar fine-tune sobre un modelo con "),r=i("code"),g=l("Trainer"),w=l(", considera el tutorial b\xE1sico "),$=i("a"),b=l("aqu\xED"),q=l("!"),this.h()},l(v){s=p(v,"P",{});var x=c(s);j=o(x,"Si no est\xE1s familiarizado con el proceso de realizar fine-tune sobre un modelo con "),r=p(x,"CODE",{});var F=c(r);g=o(F,"Trainer"),F.forEach(t),w=o(x,", considera el tutorial b\xE1sico "),$=p(x,"A",{href:!0});var N=c($);b=o(N,"aqu\xED"),N.forEach(t),q=o(x,"!"),x.forEach(t),this.h()},h(){E($,"href","../training#finetune-with-trainer")},m(v,x){m(v,s,x),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q)},d(v){v&&t(s)}}}function yo(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U,_,k,H,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne;return b=new se({props:{code:`from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

model = AutoModelForCausalLM.from_pretrained("distilgpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)`}}),v=new Ca({props:{$$slots:{default:[wo]},$$scope:{ctx:J}}}),Y=new se({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){s=i("p"),j=l("Carga DistilGPT2 con "),r=i("code"),g=l("AutoModelForCausalLM"),w=l(":"),$=f(),D(b.$$.fragment),q=f(),D(v.$$.fragment),x=f(),F=i("p"),N=l("A este punto, solo faltan tres pasos:"),G=f(),z=i("ol"),R=i("li"),S=l("Definir tus hiperpar\xE1metros de entrenamiento en "),B=i("code"),I=l("TrainingArguments"),U=l("."),_=f(),k=i("li"),H=l("Pasarle los argumentos de entrenamiento a "),L=i("code"),W=l("Trainer"),V=l(" junto con el modelo, conjunto de datos, y el data collator."),ee=f(),ae=i("li"),pe=l("Realiza la llamada "),K=i("code"),ue=l("train()"),te=l(" para realizar el fine-tune sobre tu modelo."),oe=f(),D(Y.$$.fragment)},l(u){s=p(u,"P",{});var O=c(s);j=o(O,"Carga DistilGPT2 con "),r=p(O,"CODE",{});var re=c(r);g=o(re,"AutoModelForCausalLM"),re.forEach(t),w=o(O,":"),O.forEach(t),$=h(u),C(b.$$.fragment,u),q=h(u),C(v.$$.fragment,u),x=h(u),F=p(u,"P",{});var le=c(F);N=o(le,"A este punto, solo faltan tres pasos:"),le.forEach(t),G=h(u),z=p(u,"OL",{});var X=c(z);R=p(X,"LI",{});var Z=c(R);S=o(Z,"Definir tus hiperpar\xE1metros de entrenamiento en "),B=p(Z,"CODE",{});var ce=c(B);I=o(ce,"TrainingArguments"),ce.forEach(t),U=o(Z,"."),Z.forEach(t),_=h(X),k=p(X,"LI",{});var Q=c(k);H=o(Q,"Pasarle los argumentos de entrenamiento a "),L=p(Q,"CODE",{});var me=c(L);W=o(me,"Trainer"),me.forEach(t),V=o(Q," junto con el modelo, conjunto de datos, y el data collator."),Q.forEach(t),ee=h(X),ae=p(X,"LI",{});var ie=c(ae);pe=o(ie,"Realiza la llamada "),K=p(ie,"CODE",{});var fe=c(K);ue=o(fe,"train()"),fe.forEach(t),te=o(ie," para realizar el fine-tune sobre tu modelo."),ie.forEach(t),X.forEach(t),oe=h(u),C(Y.$$.fragment,u)},m(u,O){m(u,s,O),a(s,j),a(s,r),a(r,g),a(s,w),m(u,$,O),T(b,u,O),m(u,q,O),T(v,u,O),m(u,x,O),m(u,F,O),a(F,N),m(u,G,O),m(u,z,O),a(z,R),a(R,S),a(R,B),a(B,I),a(R,U),a(z,_),a(z,k),a(k,H),a(k,L),a(L,W),a(k,V),a(z,ee),a(z,ae),a(ae,pe),a(ae,K),a(K,ue),a(ae,te),m(u,oe,O),T(Y,u,O),ne=!0},p(u,O){const re={};O&2&&(re.$$scope={dirty:O,ctx:u}),v.$set(re)},i(u){ne||(P(b.$$.fragment,u),P(v.$$.fragment,u),P(Y.$$.fragment,u),ne=!0)},o(u){A(b.$$.fragment,u),A(v.$$.fragment,u),A(Y.$$.fragment,u),ne=!1},d(u){u&&t(s),u&&t($),M(b,u),u&&t(q),M(v,u),u&&t(x),u&&t(F),u&&t(G),u&&t(z),u&&t(oe),M(Y,u)}}}function xo(J){let s,j;return s=new Ta({props:{$$slots:{default:[yo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){C(s.$$.fragment,r)},m(r,g){T(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(P(s.$$.fragment,r),j=!0)},o(r){A(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function zo(J){let s,j,r,g,w;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizados con realizar fine-tune de tus modelos con Keras, considera el tutorial b\xE1sico "),r=i("a"),g=l("aqu\xED"),w=l("!"),this.h()},l($){s=p($,"P",{});var b=c(s);j=o(b,"Si no est\xE1s familiarizados con realizar fine-tune de tus modelos con Keras, considera el tutorial b\xE1sico "),r=p(b,"A",{href:!0});var q=c(r);g=o(q,"aqu\xED"),q.forEach(t),w=o(b,"!"),b.forEach(t),this.h()},h(){E(r,"href","training#finetune-with-keras")},m($,b){m($,s,b),a(s,j),a(s,r),a(r,g),a(s,w)},d($){$&&t(s)}}}function qo(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U,_,k,H,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne,u,O,re,le,X,Z,ce,Q,me,ie,fe,ge,de,_e;return z=new se({props:{code:`tf_train_set = lm_dataset["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = lm_dataset["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),S=new Ca({props:{$$slots:{default:[zo]},$$scope:{ctx:J}}}),k=new se({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),K=new se({props:{code:`from transformers import TFAutoModelForCausalLM

model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)`}}),le=new se({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),de=new se({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),j=l("Para realizar el fine-tune de un modelo en TensorFlow, comienza por convertir tus conjuntos de datos al formato "),r=i("code"),g=l("tf.data.Dataset"),w=l(" con "),$=i("a"),b=i("code"),q=l("to_tf_dataset"),v=l(". Especifica los inputs y etiquetas en "),x=i("code"),F=l("columns"),N=l(", ya sea para mezclar el conjunto de datos, tama\xF1o de lote, y el data collator:"),G=f(),D(z.$$.fragment),R=f(),D(S.$$.fragment),B=f(),I=i("p"),U=l("Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),_=f(),D(k.$$.fragment),H=f(),L=i("p"),W=l("Carga DistilGPT2 con "),V=i("code"),ee=l("TFAutoModelForCausalLM"),ae=l(":"),pe=f(),D(K.$$.fragment),ue=f(),te=i("p"),oe=l("Configura el modelo para entrenamiento con "),Y=i("a"),ne=i("code"),u=l("compile"),O=l(":"),re=f(),D(le.$$.fragment),X=f(),Z=i("p"),ce=l("Haz el llamado a "),Q=i("a"),me=i("code"),ie=l("fit"),fe=l(" para realizar el fine-tune del modelo:"),ge=f(),D(de.$$.fragment),this.h()},l(n){s=p(n,"P",{});var y=c(s);j=o(y,"Para realizar el fine-tune de un modelo en TensorFlow, comienza por convertir tus conjuntos de datos al formato "),r=p(y,"CODE",{});var he=c(r);g=o(he,"tf.data.Dataset"),he.forEach(t),w=o(y," con "),$=p(y,"A",{href:!0,rel:!0});var Ee=c($);b=p(Ee,"CODE",{});var De=c(b);q=o(De,"to_tf_dataset"),De.forEach(t),Ee.forEach(t),v=o(y,". Especifica los inputs y etiquetas en "),x=p(y,"CODE",{});var xe=c(x);F=o(xe,"columns"),xe.forEach(t),N=o(y,", ya sea para mezclar el conjunto de datos, tama\xF1o de lote, y el data collator:"),y.forEach(t),G=h(n),C(z.$$.fragment,n),R=h(n),C(S.$$.fragment,n),B=h(n),I=p(n,"P",{});var Le=c(I);U=o(Le,"Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),Le.forEach(t),_=h(n),C(k.$$.fragment,n),H=h(n),L=p(n,"P",{});var $e=c(L);W=o($e,"Carga DistilGPT2 con "),V=p($e,"CODE",{});var we=c(V);ee=o(we,"TFAutoModelForCausalLM"),we.forEach(t),ae=o($e,":"),$e.forEach(t),pe=h(n),C(K.$$.fragment,n),ue=h(n),te=p(n,"P",{});var je=c(te);oe=o(je,"Configura el modelo para entrenamiento con "),Y=p(je,"A",{href:!0,rel:!0});var ze=c(Y);ne=p(ze,"CODE",{});var Oe=c(ne);u=o(Oe,"compile"),Oe.forEach(t),ze.forEach(t),O=o(je,":"),je.forEach(t),re=h(n),C(le.$$.fragment,n),X=h(n),Z=p(n,"P",{});var be=c(Z);ce=o(be,"Haz el llamado a "),Q=p(be,"A",{href:!0,rel:!0});var ye=c(Q);me=p(ye,"CODE",{});var Ce=c(me);ie=o(Ce,"fit"),Ce.forEach(t),ye.forEach(t),fe=o(be," para realizar el fine-tune del modelo:"),be.forEach(t),ge=h(n),C(de.$$.fragment,n),this.h()},h(){E($,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),E($,"rel","nofollow"),E(Y,"href","https://keras.io/api/models/model_training_apis/#compile-method"),E(Y,"rel","nofollow"),E(Q,"href","https://keras.io/api/models/model_training_apis/#fit-method"),E(Q,"rel","nofollow")},m(n,y){m(n,s,y),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(b,q),a(s,v),a(s,x),a(x,F),a(s,N),m(n,G,y),T(z,n,y),m(n,R,y),T(S,n,y),m(n,B,y),m(n,I,y),a(I,U),m(n,_,y),T(k,n,y),m(n,H,y),m(n,L,y),a(L,W),a(L,V),a(V,ee),a(L,ae),m(n,pe,y),T(K,n,y),m(n,ue,y),m(n,te,y),a(te,oe),a(te,Y),a(Y,ne),a(ne,u),a(te,O),m(n,re,y),T(le,n,y),m(n,X,y),m(n,Z,y),a(Z,ce),a(Z,Q),a(Q,me),a(me,ie),a(Z,fe),m(n,ge,y),T(de,n,y),_e=!0},p(n,y){const he={};y&2&&(he.$$scope={dirty:y,ctx:n}),S.$set(he)},i(n){_e||(P(z.$$.fragment,n),P(S.$$.fragment,n),P(k.$$.fragment,n),P(K.$$.fragment,n),P(le.$$.fragment,n),P(de.$$.fragment,n),_e=!0)},o(n){A(z.$$.fragment,n),A(S.$$.fragment,n),A(k.$$.fragment,n),A(K.$$.fragment,n),A(le.$$.fragment,n),A(de.$$.fragment,n),_e=!1},d(n){n&&t(s),n&&t(G),M(z,n),n&&t(R),M(S,n),n&&t(B),n&&t(I),n&&t(_),M(k,n),n&&t(H),n&&t(L),n&&t(pe),M(K,n),n&&t(ue),n&&t(te),n&&t(re),M(le,n),n&&t(X),n&&t(Z),n&&t(ge),M(de,n)}}}function Do(J){let s,j;return s=new Ta({props:{$$slots:{default:[qo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){C(s.$$.fragment,r)},m(r,g){T(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(P(s.$$.fragment,r),j=!0)},o(r){A(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function Co(J){let s,j,r,g,w,$,b,q;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizado con el proceso de realizar fine-tune sobre un modelo con "),r=i("code"),g=l("Trainer"),w=l(", considera el tutorial b\xE1sico "),$=i("a"),b=l("aqu\xED"),q=l("!"),this.h()},l(v){s=p(v,"P",{});var x=c(s);j=o(x,"Si no est\xE1s familiarizado con el proceso de realizar fine-tune sobre un modelo con "),r=p(x,"CODE",{});var F=c(r);g=o(F,"Trainer"),F.forEach(t),w=o(x,", considera el tutorial b\xE1sico "),$=p(x,"A",{href:!0});var N=c($);b=o(N,"aqu\xED"),N.forEach(t),q=o(x,"!"),x.forEach(t),this.h()},h(){E($,"href","../training#finetune-with-trainer")},m(v,x){m(v,s,x),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q)},d(v){v&&t(s)}}}function To(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U,_,k,H,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne;return b=new se({props:{code:`from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained("distilroberta-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilroberta-base&quot;</span>)`}}),v=new Ca({props:{$$slots:{default:[Co]},$$scope:{ctx:J}}}),Y=new se({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){s=i("p"),j=l("Carga DistilRoBERTa con "),r=i("code"),g=l("AutoModelForMaskedlM"),w=l(":"),$=f(),D(b.$$.fragment),q=f(),D(v.$$.fragment),x=f(),F=i("p"),N=l("A este punto, solo faltan tres pasos:"),G=f(),z=i("ol"),R=i("li"),S=l("Definir tus hiperpar\xE1metros de entrenamiento en "),B=i("code"),I=l("TrainingArguments"),U=l("."),_=f(),k=i("li"),H=l("Pasarle los argumentos de entrenamiento a "),L=i("code"),W=l("Trainer"),V=l(" junto con el modelo, conjunto de datos, y el data collator."),ee=f(),ae=i("li"),pe=l("Realiza la llamada "),K=i("code"),ue=l("train()"),te=l(" para realizar el fine-tune de tu modelo."),oe=f(),D(Y.$$.fragment)},l(u){s=p(u,"P",{});var O=c(s);j=o(O,"Carga DistilRoBERTa con "),r=p(O,"CODE",{});var re=c(r);g=o(re,"AutoModelForMaskedlM"),re.forEach(t),w=o(O,":"),O.forEach(t),$=h(u),C(b.$$.fragment,u),q=h(u),C(v.$$.fragment,u),x=h(u),F=p(u,"P",{});var le=c(F);N=o(le,"A este punto, solo faltan tres pasos:"),le.forEach(t),G=h(u),z=p(u,"OL",{});var X=c(z);R=p(X,"LI",{});var Z=c(R);S=o(Z,"Definir tus hiperpar\xE1metros de entrenamiento en "),B=p(Z,"CODE",{});var ce=c(B);I=o(ce,"TrainingArguments"),ce.forEach(t),U=o(Z,"."),Z.forEach(t),_=h(X),k=p(X,"LI",{});var Q=c(k);H=o(Q,"Pasarle los argumentos de entrenamiento a "),L=p(Q,"CODE",{});var me=c(L);W=o(me,"Trainer"),me.forEach(t),V=o(Q," junto con el modelo, conjunto de datos, y el data collator."),Q.forEach(t),ee=h(X),ae=p(X,"LI",{});var ie=c(ae);pe=o(ie,"Realiza la llamada "),K=p(ie,"CODE",{});var fe=c(K);ue=o(fe,"train()"),fe.forEach(t),te=o(ie," para realizar el fine-tune de tu modelo."),ie.forEach(t),X.forEach(t),oe=h(u),C(Y.$$.fragment,u)},m(u,O){m(u,s,O),a(s,j),a(s,r),a(r,g),a(s,w),m(u,$,O),T(b,u,O),m(u,q,O),T(v,u,O),m(u,x,O),m(u,F,O),a(F,N),m(u,G,O),m(u,z,O),a(z,R),a(R,S),a(R,B),a(B,I),a(R,U),a(z,_),a(z,k),a(k,H),a(k,L),a(L,W),a(k,V),a(z,ee),a(z,ae),a(ae,pe),a(ae,K),a(K,ue),a(ae,te),m(u,oe,O),T(Y,u,O),ne=!0},p(u,O){const re={};O&2&&(re.$$scope={dirty:O,ctx:u}),v.$set(re)},i(u){ne||(P(b.$$.fragment,u),P(v.$$.fragment,u),P(Y.$$.fragment,u),ne=!0)},o(u){A(b.$$.fragment,u),A(v.$$.fragment,u),A(Y.$$.fragment,u),ne=!1},d(u){u&&t(s),u&&t($),M(b,u),u&&t(q),M(v,u),u&&t(x),u&&t(F),u&&t(G),u&&t(z),u&&t(oe),M(Y,u)}}}function Po(J){let s,j;return s=new Ta({props:{$$slots:{default:[To]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){C(s.$$.fragment,r)},m(r,g){T(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(P(s.$$.fragment,r),j=!0)},o(r){A(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function Ao(J){let s,j,r,g,w;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizados con realizar fine-tune de tus modelos con Keras, considera el tutorial b\xE1sico "),r=i("a"),g=l("aqu\xED"),w=l("!"),this.h()},l($){s=p($,"P",{});var b=c(s);j=o(b,"Si no est\xE1s familiarizados con realizar fine-tune de tus modelos con Keras, considera el tutorial b\xE1sico "),r=p(b,"A",{href:!0});var q=c(r);g=o(q,"aqu\xED"),q.forEach(t),w=o(b,"!"),b.forEach(t),this.h()},h(){E(r,"href","training#finetune-with-keras")},m($,b){m($,s,b),a(s,j),a(s,r),a(r,g),a(s,w)},d($){$&&t(s)}}}function Mo(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U,_,k,H,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne,u,O,re,le,X,Z,ce,Q,me,ie,fe,ge,de,_e;return z=new se({props:{code:`tf_train_set = lm_dataset["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = lm_dataset["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),S=new Ca({props:{$$slots:{default:[Ao]},$$scope:{ctx:J}}}),k=new se({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),K=new se({props:{code:`from transformers import TFAutoModelForMaskedLM

model = TFAutoModelForCausalLM.from_pretrained("distilroberta-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilroberta-base&quot;</span>)`}}),le=new se({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),de=new se({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),j=l("Para realizar el fine-tune de un modelo en TensorFlow, comienza por convertir tus conjuntos de datos al formato "),r=i("code"),g=l("tf.data.Dataset"),w=l(" con "),$=i("a"),b=i("code"),q=l("to_tf_dataset"),v=l(". Especifica los inputs y etiquetas en "),x=i("code"),F=l("columns"),N=l(", ya sea para mezclar el conjunto de datos, tama\xF1o de lote, y el data collator:"),G=f(),D(z.$$.fragment),R=f(),D(S.$$.fragment),B=f(),I=i("p"),U=l("Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),_=f(),D(k.$$.fragment),H=f(),L=i("p"),W=l("Carga DistilRoBERTa con "),V=i("code"),ee=l("TFAutoModelForMaskedLM"),ae=l(":"),pe=f(),D(K.$$.fragment),ue=f(),te=i("p"),oe=l("Configura el modelo para entrenamiento con "),Y=i("a"),ne=i("code"),u=l("compile"),O=l(":"),re=f(),D(le.$$.fragment),X=f(),Z=i("p"),ce=l("Haz el llamado a "),Q=i("a"),me=i("code"),ie=l("fit"),fe=l(" para realizar el fine-tune del modelo:"),ge=f(),D(de.$$.fragment),this.h()},l(n){s=p(n,"P",{});var y=c(s);j=o(y,"Para realizar el fine-tune de un modelo en TensorFlow, comienza por convertir tus conjuntos de datos al formato "),r=p(y,"CODE",{});var he=c(r);g=o(he,"tf.data.Dataset"),he.forEach(t),w=o(y," con "),$=p(y,"A",{href:!0,rel:!0});var Ee=c($);b=p(Ee,"CODE",{});var De=c(b);q=o(De,"to_tf_dataset"),De.forEach(t),Ee.forEach(t),v=o(y,". Especifica los inputs y etiquetas en "),x=p(y,"CODE",{});var xe=c(x);F=o(xe,"columns"),xe.forEach(t),N=o(y,", ya sea para mezclar el conjunto de datos, tama\xF1o de lote, y el data collator:"),y.forEach(t),G=h(n),C(z.$$.fragment,n),R=h(n),C(S.$$.fragment,n),B=h(n),I=p(n,"P",{});var Le=c(I);U=o(Le,"Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),Le.forEach(t),_=h(n),C(k.$$.fragment,n),H=h(n),L=p(n,"P",{});var $e=c(L);W=o($e,"Carga DistilRoBERTa con "),V=p($e,"CODE",{});var we=c(V);ee=o(we,"TFAutoModelForMaskedLM"),we.forEach(t),ae=o($e,":"),$e.forEach(t),pe=h(n),C(K.$$.fragment,n),ue=h(n),te=p(n,"P",{});var je=c(te);oe=o(je,"Configura el modelo para entrenamiento con "),Y=p(je,"A",{href:!0,rel:!0});var ze=c(Y);ne=p(ze,"CODE",{});var Oe=c(ne);u=o(Oe,"compile"),Oe.forEach(t),ze.forEach(t),O=o(je,":"),je.forEach(t),re=h(n),C(le.$$.fragment,n),X=h(n),Z=p(n,"P",{});var be=c(Z);ce=o(be,"Haz el llamado a "),Q=p(be,"A",{href:!0,rel:!0});var ye=c(Q);me=p(ye,"CODE",{});var Ce=c(me);ie=o(Ce,"fit"),Ce.forEach(t),ye.forEach(t),fe=o(be," para realizar el fine-tune del modelo:"),be.forEach(t),ge=h(n),C(de.$$.fragment,n),this.h()},h(){E($,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),E($,"rel","nofollow"),E(Y,"href","https://keras.io/api/models/model_training_apis/#compile-method"),E(Y,"rel","nofollow"),E(Q,"href","https://keras.io/api/models/model_training_apis/#fit-method"),E(Q,"rel","nofollow")},m(n,y){m(n,s,y),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(b,q),a(s,v),a(s,x),a(x,F),a(s,N),m(n,G,y),T(z,n,y),m(n,R,y),T(S,n,y),m(n,B,y),m(n,I,y),a(I,U),m(n,_,y),T(k,n,y),m(n,H,y),m(n,L,y),a(L,W),a(L,V),a(V,ee),a(L,ae),m(n,pe,y),T(K,n,y),m(n,ue,y),m(n,te,y),a(te,oe),a(te,Y),a(Y,ne),a(ne,u),a(te,O),m(n,re,y),T(le,n,y),m(n,X,y),m(n,Z,y),a(Z,ce),a(Z,Q),a(Q,me),a(me,ie),a(Z,fe),m(n,ge,y),T(de,n,y),_e=!0},p(n,y){const he={};y&2&&(he.$$scope={dirty:y,ctx:n}),S.$set(he)},i(n){_e||(P(z.$$.fragment,n),P(S.$$.fragment,n),P(k.$$.fragment,n),P(K.$$.fragment,n),P(le.$$.fragment,n),P(de.$$.fragment,n),_e=!0)},o(n){A(z.$$.fragment,n),A(S.$$.fragment,n),A(k.$$.fragment,n),A(K.$$.fragment,n),A(le.$$.fragment,n),A(de.$$.fragment,n),_e=!1},d(n){n&&t(s),n&&t(G),M(z,n),n&&t(R),M(S,n),n&&t(B),n&&t(I),n&&t(_),M(k,n),n&&t(H),n&&t(L),n&&t(pe),M(K,n),n&&t(ue),n&&t(te),n&&t(re),M(le,n),n&&t(X),n&&t(Z),n&&t(ge),M(de,n)}}}function Fo(J){let s,j;return s=new Ta({props:{$$slots:{default:[Mo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){C(s.$$.fragment,r)},m(r,g){T(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(P(s.$$.fragment,r),j=!0)},o(r){A(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function Lo(J){let s,j,r,g,w,$,b,q;return{c(){s=i("p"),j=l(`Para un ejemplo en mayor profundidad sobre c\xF3mo realizar el fine-tune sobre un modelo de lenguaje causal, considera considera
`),r=i("a"),g=l("PyTorch notebook"),w=l(`
o `),$=i("a"),b=l("TensorFlow notebook"),q=l("."),this.h()},l(v){s=p(v,"P",{});var x=c(s);j=o(x,`Para un ejemplo en mayor profundidad sobre c\xF3mo realizar el fine-tune sobre un modelo de lenguaje causal, considera considera
`),r=p(x,"A",{href:!0,rel:!0});var F=c(r);g=o(F,"PyTorch notebook"),F.forEach(t),w=o(x,`
o `),$=p(x,"A",{href:!0,rel:!0});var N=c($);b=o(N,"TensorFlow notebook"),N.forEach(t),q=o(x,"."),x.forEach(t),this.h()},h(){E(r,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb"),E(r,"rel","nofollow"),E($,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb"),E($,"rel","nofollow")},m(v,x){m(v,s,x),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q)},d(v){v&&t(s)}}}function Oo(J){let s,j,r,g,w,$,b,q,v,x,F,N,G,z,R,S,B,I,U,_,k,H,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne,u,O,re,le,X,Z,ce,Q,me,ie,fe,ge,de,_e,n,y,he,Ee,De,xe,Le,$e,we,je,ze,Oe,be,ye,Ce,qe,as,Pa,ts,ss,Aa,ls,os,Ma,ns,rs,mt,Ie,He,Fa,la,is,La,ps,ut,oa,dt,Be,cs,Oa,ms,us,ft,na,ht,ra,gt,ya,ds,_t,ia,$t,Te,fs,Ia,hs,gs,pa,Ra,_s,$s,jt,ca,bt,Pe,js,Sa,bs,vs,Ga,ks,Es,vt,xa,ws,kt,ma,Et,ve,ys,ua,Na,xs,zs,Ha,qs,Ds,Ba,Cs,Ts,Wa,Ps,As,wt,da,yt,za,Ms,xt,We,Ua,Fs,Ls,fa,Os,Ja,Is,Rs,zt,ha,qt,Ue,Ss,Ka,Gs,Ns,Dt,ga,Ct,ke,Hs,Ya,Bs,Ws,Qa,Us,Js,Va,Ks,Ys,Xa,Qs,Vs,Tt,Je,Pt,Re,Ke,Za,_a,Xs,et,Zs,At,Ye,el,$a,al,tl,Mt,Se,Qe,at,ja,sl,tt,ll,Ft,Ve,Lt,Ge,Xe,st,ba,ol,lt,nl,Ot,Ze,rl,va,il,pl,It,Ne,ea,ot,ka,cl,nt,ml,Rt,aa,St,ta,Gt;return $=new wa({}),z=new es({props:{id:"Vpjb1lu0MDk"}}),U=new es({props:{id:"mqElG5QJWUg"}}),X=new Ca({props:{$$slots:{default:[jo]},$$scope:{ctx:J}}}),ie=new wa({}),Ee=new se({props:{code:`from datasets import load_dataset

eli5 = load_dataset("eli5", split="train_asks[:5000]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = load_dataset(<span class="hljs-string">&quot;eli5&quot;</span>, split=<span class="hljs-string">&quot;train_asks[:5000]&quot;</span>)`}}),we=new se({props:{code:"eli5 = eli5.train_test_split(test_size=0.2)",highlighted:'eli5 = eli5.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),ye=new se({props:{code:'eli5["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
  <span class="hljs-string">&#x27;score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
  <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
   <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>]},
 <span class="hljs-string">&#x27;answers_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []},
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>]},
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []}}`}}),la=new wa({}),oa=new es({props:{id:"ma1TrR7gE7I"}}),na=new se({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)`}}),ra=new es({props:{id:"8PmhEIXhBvI"}}),ia=new se({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilroberta-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilroberta-base&quot;</span>)`}}),ca=new se({props:{code:`eli5 = eli5.flatten()
eli5["train"][0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = eli5.flatten()
<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers.a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
 <span class="hljs-string">&#x27;answers.score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
 <span class="hljs-string">&#x27;answers.text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
  <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>],
 <span class="hljs-string">&#x27;answers_urls.url&#x27;</span>: [],
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls.url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>],
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls.url&#x27;</span>: []}`}}),ma=new se({props:{code:`def preprocess_function(examples):
    return tokenizer([" ".join(x) for x in examples["answers.text"]], truncation=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer([<span class="hljs-string">&quot; &quot;</span>.join(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;answers.text&quot;</span>]], truncation=<span class="hljs-literal">True</span>)`}}),da=new se({props:{code:`tokenized_eli5 = eli5.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=eli5["train"].column_names,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_eli5 = eli5.<span class="hljs-built_in">map</span>(
<span class="hljs-meta">... </span>    preprocess_function,
<span class="hljs-meta">... </span>    batched=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    num_proc=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    remove_columns=eli5[<span class="hljs-string">&quot;train&quot;</span>].column_names,
<span class="hljs-meta">... </span>)`}}),ha=new se({props:{code:`block_size = 128


def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>block_size = <span class="hljs-number">128</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
<span class="hljs-meta">... </span>    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
<span class="hljs-meta">... </span>    result = {
<span class="hljs-meta">... </span>        k: [t[i : i + block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, block_size)]
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
<span class="hljs-meta">... </span>    }
<span class="hljs-meta">... </span>    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> result`}}),ga=new se({props:{code:"lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lm_dataset = tokenized_eli5.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>, num_proc=<span class="hljs-number">4</span>)'}}),Je=new ul({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Eo],pytorch:[vo]},$$scope:{ctx:J}}}),_a=new wa({}),ja=new wa({}),Ve=new ul({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Do],pytorch:[xo]},$$scope:{ctx:J}}}),ba=new wa({}),ka=new wa({}),aa=new ul({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Fo],pytorch:[Po]},$$scope:{ctx:J}}}),ta=new Ca({props:{$$slots:{default:[Lo]},$$scope:{ctx:J}}}),{c(){s=i("meta"),j=f(),r=i("h1"),g=i("a"),w=i("span"),D($.$$.fragment),b=f(),q=i("span"),v=l("Modelado de lenguaje"),x=f(),F=i("p"),N=l("El modelado de lenguaje predice palabras en un enunciado. Hay dos formas de modelado de lenguaje."),G=f(),D(z.$$.fragment),R=f(),S=i("p"),B=l("El modelado de lenguaje causal predice el siguiente token en una secuencia de tokens, y el modelo solo puede considerar a los tokens a la izquierda."),I=f(),D(U.$$.fragment),_=f(),k=i("p"),H=l("El modelado de lenguaje por enmascaramiento predice un token enmascarado en una secuencia, y el modelo puede considerar los tokens bidireccionalmente."),L=f(),W=i("p"),V=l("Esta gu\xEDa te mostrar\xE1 c\xF3mo realizar fine-tune "),ee=i("a"),ae=l("DistilGPT2"),pe=l(" para modelos de lenguaje causales y "),K=i("a"),ue=l("DistilRoBERTa"),te=l(" para modelos de lenguajes por enmascaramiento en el "),oe=i("a"),Y=l("r/askscience"),ne=l(" subconjunto del "),u=i("a"),O=l("ELI5"),re=l(" conjunto de datos."),le=f(),D(X.$$.fragment),Z=f(),ce=i("h2"),Q=i("a"),me=i("span"),D(ie.$$.fragment),fe=f(),ge=i("span"),de=l("Cargar el conjunto de datos ELI5"),_e=f(),n=i("p"),y=l("Carga solo los primeros 5000 registros desde los la librer\xEDa \u{1F917} Datasets, dado que es bastante grande:"),he=f(),D(Ee.$$.fragment),De=f(),xe=i("p"),Le=l("Divide este conjunto de datos en subconjuntos para el entrenamiento y el test:"),$e=f(),D(we.$$.fragment),je=f(),ze=i("p"),Oe=l("Luego observa un ejemplo:"),be=f(),D(ye.$$.fragment),Ce=f(),qe=i("p"),as=l("Observa que "),Pa=i("code"),ts=l("text"),ss=l(" es un subcampo anidado dentro del diccionario "),Aa=i("code"),ls=l("answers"),os=l(". Cuando preproceses el conjunto de datos, deber\xE1s extraer el subcampo "),Ma=i("code"),ns=l("text"),rs=l(" en una columna aparte."),mt=f(),Ie=i("h2"),He=i("a"),Fa=i("span"),D(la.$$.fragment),is=f(),La=i("span"),ps=l("Preprocesamiento"),ut=f(),D(oa.$$.fragment),dt=f(),Be=i("p"),cs=l("Para modelados de lenguajes causales, carga el tokenizador DistilGPT2 para procesar el subcampo "),Oa=i("code"),ms=l("text"),us=l(":"),ft=f(),D(na.$$.fragment),ht=f(),D(ra.$$.fragment),gt=f(),ya=i("p"),ds=l("Para modelados de lenguajes por enmascaramiento, carga el tokenizador DistilRoBERTa, en lugar de DistilGPT2:"),_t=f(),D(ia.$$.fragment),$t=f(),Te=i("p"),fs=l("Extrae el subcampo "),Ia=i("code"),hs=l("text"),gs=l(" desde su estructura anidad con el m\xE9todo "),pa=i("a"),Ra=i("code"),_s=l("flatten"),$s=l(":"),jt=f(),D(ca.$$.fragment),bt=f(),Pe=i("p"),js=l("Cada subcampo es ahora una columna separado, como lo indica el prefijo "),Sa=i("code"),bs=l("answers"),vs=l(". Observa que "),Ga=i("code"),ks=l("answers.text"),Es=l(" es una lista. En lugar de tokenizar cada enunciado por separado, convierte la lista en un string para tokenizarlos conjuntamente."),vt=f(),xa=i("p"),ws=l("Here is how you can create a preprocessing function to convert the list to a string and truncate sequences to be no longer than DistilGPT2\u2019s maximum input length:"),kt=f(),D(ma.$$.fragment),Et=f(),ve=i("p"),ys=l("Usa de \u{1F917} Datasets  la funci\xF3n "),ua=i("a"),Na=i("code"),xs=l("map"),zs=l(" para aplicar la funci\xF3n de preprocesamiento sobre el conjunto de datos en su totalidad. Puedes acelerar la funci\xF3n "),Ha=i("code"),qs=l("map"),Ds=l(" configurando el argumento "),Ba=i("code"),Cs=l("batched=True"),Ts=l(" para procesar m\xFAltiples elementos del conjunto de datos a la vez y aumentando la cantidad de procesos con "),Wa=i("code"),Ps=l("num_proc"),As=l(". Elimina las columnas que no necesitas:"),wt=f(),D(da.$$.fragment),yt=f(),za=i("p"),Ms=l("Ahora necesitas una segunda funci\xF3n de preprocesamiento para capturar el texto truncado de cualquier ejemplo demasiado largo para evitar cualquier p\xE9rdida de informaci\xF3n. Esta funci\xF3n de preprocesamiento deber\xEDa:"),xt=f(),We=i("ul"),Ua=i("li"),Fs=l("Concatenar todo el texto."),Ls=f(),fa=i("li"),Os=l("Dividir el texto concatenado en trozos m\xE1s peque\xF1os definidos por un "),Ja=i("code"),Is=l("block_size"),Rs=l("."),zt=f(),D(ha.$$.fragment),qt=f(),Ue=i("p"),Ss=l("Aplica la funci\xF3n "),Ka=i("code"),Gs=l("group_texts"),Ns=l(" sobre todo el conjunto de datos:"),Dt=f(),D(ga.$$.fragment),Ct=f(),ke=i("p"),Hs=l("Para modelados de lenguajes causales, usa "),Ya=i("code"),Bs=l("DataCollatorForLanguageModeling"),Ws=l(" para crear un lote de ejemplos. Esto tambi\xE9n "),Qa=i("em"),Us=l("rellenar\xE1 din\xE1micamente"),Js=l(" tu texto a la dimensi\xF3n del elemento m\xE1s largo del lote, para que de esta manera tengan largo uniforme. Si bien es posible rellenar tu texto en la funci\xF3n "),Va=i("code"),Ks=l("tokenizer"),Ys=l(" mediante el argumento "),Xa=i("code"),Qs=l("padding=True"),Vs=l(", el rellenado din\xE1mico es m\xE1s eficiente."),Tt=f(),D(Je.$$.fragment),Pt=f(),Re=i("h2"),Ke=i("a"),Za=i("span"),D(_a.$$.fragment),Xs=f(),et=i("span"),Zs=l("Causal language modeling"),At=f(),Ye=i("p"),el=l("El modelado de lenguaje causal es frecuentemente utilizado para generaci\xF3n de texto. Esta secci\xF3n te muestra como realizar fine-tune de "),$a=i("a"),al=l("DistilGPT2"),tl=l(" para generar nuevo texto."),Mt=f(),Se=i("h3"),Qe=i("a"),at=i("span"),D(ja.$$.fragment),sl=f(),tt=i("span"),ll=l("Entrenamiento"),Ft=f(),D(Ve.$$.fragment),Lt=f(),Ge=i("h2"),Xe=i("a"),st=i("span"),D(ba.$$.fragment),ol=f(),lt=i("span"),nl=l("Modelado de lenguaje por enmascaramiento"),Ot=f(),Ze=i("p"),rl=l("El modelado de lenguaje por enmascaramiento es tambi\xE9n conocido como una tarea de rellenar la m\xE1scara, pues predice un token enmascarado dada una secuencia. Los modelos de lenguaje por enmascaramiento requieren una buena comprensi\xF3n del contexto de una secuencia entera, en lugar de solo el contexto a la izquierda. Esta secci\xF3n te ense\xF1a como realizar el fine-tune de "),va=i("a"),il=l("DistilRoBERTa"),pl=l(" para predecir una palabra enmascarada."),It=f(),Ne=i("h3"),ea=i("a"),ot=i("span"),D(ka.$$.fragment),cl=f(),nt=i("span"),ml=l("Entrenamiento"),Rt=f(),D(aa.$$.fragment),St=f(),D(ta.$$.fragment),this.h()},l(e){const d=_o('[data-svelte="svelte-1phssyn"]',document.head);s=p(d,"META",{name:!0,content:!0}),d.forEach(t),j=h(e),r=p(e,"H1",{class:!0});var Ea=c(r);g=p(Ea,"A",{id:!0,class:!0,href:!0});var rt=c(g);w=p(rt,"SPAN",{});var it=c(w);C($.$$.fragment,it),it.forEach(t),rt.forEach(t),b=h(Ea),q=p(Ea,"SPAN",{});var pt=c(q);v=o(pt,"Modelado de lenguaje"),pt.forEach(t),Ea.forEach(t),x=h(e),F=p(e,"P",{});var ct=c(F);N=o(ct,"El modelado de lenguaje predice palabras en un enunciado. Hay dos formas de modelado de lenguaje."),ct.forEach(t),G=h(e),C(z.$$.fragment,e),R=h(e),S=p(e,"P",{});var dl=c(S);B=o(dl,"El modelado de lenguaje causal predice el siguiente token en una secuencia de tokens, y el modelo solo puede considerar a los tokens a la izquierda."),dl.forEach(t),I=h(e),C(U.$$.fragment,e),_=h(e),k=p(e,"P",{});var fl=c(k);H=o(fl,"El modelado de lenguaje por enmascaramiento predice un token enmascarado en una secuencia, y el modelo puede considerar los tokens bidireccionalmente."),fl.forEach(t),L=h(e),W=p(e,"P",{});var Ae=c(W);V=o(Ae,"Esta gu\xEDa te mostrar\xE1 c\xF3mo realizar fine-tune "),ee=p(Ae,"A",{href:!0,rel:!0});var hl=c(ee);ae=o(hl,"DistilGPT2"),hl.forEach(t),pe=o(Ae," para modelos de lenguaje causales y "),K=p(Ae,"A",{href:!0,rel:!0});var gl=c(K);ue=o(gl,"DistilRoBERTa"),gl.forEach(t),te=o(Ae," para modelos de lenguajes por enmascaramiento en el "),oe=p(Ae,"A",{href:!0,rel:!0});var _l=c(oe);Y=o(_l,"r/askscience"),_l.forEach(t),ne=o(Ae," subconjunto del "),u=p(Ae,"A",{href:!0,rel:!0});var $l=c(u);O=o($l,"ELI5"),$l.forEach(t),re=o(Ae," conjunto de datos."),Ae.forEach(t),le=h(e),C(X.$$.fragment,e),Z=h(e),ce=p(e,"H2",{class:!0});var Nt=c(ce);Q=p(Nt,"A",{id:!0,class:!0,href:!0});var jl=c(Q);me=p(jl,"SPAN",{});var bl=c(me);C(ie.$$.fragment,bl),bl.forEach(t),jl.forEach(t),fe=h(Nt),ge=p(Nt,"SPAN",{});var vl=c(ge);de=o(vl,"Cargar el conjunto de datos ELI5"),vl.forEach(t),Nt.forEach(t),_e=h(e),n=p(e,"P",{});var kl=c(n);y=o(kl,"Carga solo los primeros 5000 registros desde los la librer\xEDa \u{1F917} Datasets, dado que es bastante grande:"),kl.forEach(t),he=h(e),C(Ee.$$.fragment,e),De=h(e),xe=p(e,"P",{});var El=c(xe);Le=o(El,"Divide este conjunto de datos en subconjuntos para el entrenamiento y el test:"),El.forEach(t),$e=h(e),C(we.$$.fragment,e),je=h(e),ze=p(e,"P",{});var wl=c(ze);Oe=o(wl,"Luego observa un ejemplo:"),wl.forEach(t),be=h(e),C(ye.$$.fragment,e),Ce=h(e),qe=p(e,"P",{});var sa=c(qe);as=o(sa,"Observa que "),Pa=p(sa,"CODE",{});var yl=c(Pa);ts=o(yl,"text"),yl.forEach(t),ss=o(sa," es un subcampo anidado dentro del diccionario "),Aa=p(sa,"CODE",{});var xl=c(Aa);ls=o(xl,"answers"),xl.forEach(t),os=o(sa,". Cuando preproceses el conjunto de datos, deber\xE1s extraer el subcampo "),Ma=p(sa,"CODE",{});var zl=c(Ma);ns=o(zl,"text"),zl.forEach(t),rs=o(sa," en una columna aparte."),sa.forEach(t),mt=h(e),Ie=p(e,"H2",{class:!0});var Ht=c(Ie);He=p(Ht,"A",{id:!0,class:!0,href:!0});var ql=c(He);Fa=p(ql,"SPAN",{});var Dl=c(Fa);C(la.$$.fragment,Dl),Dl.forEach(t),ql.forEach(t),is=h(Ht),La=p(Ht,"SPAN",{});var Cl=c(La);ps=o(Cl,"Preprocesamiento"),Cl.forEach(t),Ht.forEach(t),ut=h(e),C(oa.$$.fragment,e),dt=h(e),Be=p(e,"P",{});var Bt=c(Be);cs=o(Bt,"Para modelados de lenguajes causales, carga el tokenizador DistilGPT2 para procesar el subcampo "),Oa=p(Bt,"CODE",{});var Tl=c(Oa);ms=o(Tl,"text"),Tl.forEach(t),us=o(Bt,":"),Bt.forEach(t),ft=h(e),C(na.$$.fragment,e),ht=h(e),C(ra.$$.fragment,e),gt=h(e),ya=p(e,"P",{});var Pl=c(ya);ds=o(Pl,"Para modelados de lenguajes por enmascaramiento, carga el tokenizador DistilRoBERTa, en lugar de DistilGPT2:"),Pl.forEach(t),_t=h(e),C(ia.$$.fragment,e),$t=h(e),Te=p(e,"P",{});var qa=c(Te);fs=o(qa,"Extrae el subcampo "),Ia=p(qa,"CODE",{});var Al=c(Ia);hs=o(Al,"text"),Al.forEach(t),gs=o(qa," desde su estructura anidad con el m\xE9todo "),pa=p(qa,"A",{href:!0,rel:!0});var Ml=c(pa);Ra=p(Ml,"CODE",{});var Fl=c(Ra);_s=o(Fl,"flatten"),Fl.forEach(t),Ml.forEach(t),$s=o(qa,":"),qa.forEach(t),jt=h(e),C(ca.$$.fragment,e),bt=h(e),Pe=p(e,"P",{});var Da=c(Pe);js=o(Da,"Cada subcampo es ahora una columna separado, como lo indica el prefijo "),Sa=p(Da,"CODE",{});var Ll=c(Sa);bs=o(Ll,"answers"),Ll.forEach(t),vs=o(Da,". Observa que "),Ga=p(Da,"CODE",{});var Ol=c(Ga);ks=o(Ol,"answers.text"),Ol.forEach(t),Es=o(Da," es una lista. En lugar de tokenizar cada enunciado por separado, convierte la lista en un string para tokenizarlos conjuntamente."),Da.forEach(t),vt=h(e),xa=p(e,"P",{});var Il=c(xa);ws=o(Il,"Here is how you can create a preprocessing function to convert the list to a string and truncate sequences to be no longer than DistilGPT2\u2019s maximum input length:"),Il.forEach(t),kt=h(e),C(ma.$$.fragment,e),Et=h(e),ve=p(e,"P",{});var Me=c(ve);ys=o(Me,"Usa de \u{1F917} Datasets  la funci\xF3n "),ua=p(Me,"A",{href:!0,rel:!0});var Rl=c(ua);Na=p(Rl,"CODE",{});var Sl=c(Na);xs=o(Sl,"map"),Sl.forEach(t),Rl.forEach(t),zs=o(Me," para aplicar la funci\xF3n de preprocesamiento sobre el conjunto de datos en su totalidad. Puedes acelerar la funci\xF3n "),Ha=p(Me,"CODE",{});var Gl=c(Ha);qs=o(Gl,"map"),Gl.forEach(t),Ds=o(Me," configurando el argumento "),Ba=p(Me,"CODE",{});var Nl=c(Ba);Cs=o(Nl,"batched=True"),Nl.forEach(t),Ts=o(Me," para procesar m\xFAltiples elementos del conjunto de datos a la vez y aumentando la cantidad de procesos con "),Wa=p(Me,"CODE",{});var Hl=c(Wa);Ps=o(Hl,"num_proc"),Hl.forEach(t),As=o(Me,". Elimina las columnas que no necesitas:"),Me.forEach(t),wt=h(e),C(da.$$.fragment,e),yt=h(e),za=p(e,"P",{});var Bl=c(za);Ms=o(Bl,"Ahora necesitas una segunda funci\xF3n de preprocesamiento para capturar el texto truncado de cualquier ejemplo demasiado largo para evitar cualquier p\xE9rdida de informaci\xF3n. Esta funci\xF3n de preprocesamiento deber\xEDa:"),Bl.forEach(t),xt=h(e),We=p(e,"UL",{});var Wt=c(We);Ua=p(Wt,"LI",{});var Wl=c(Ua);Fs=o(Wl,"Concatenar todo el texto."),Wl.forEach(t),Ls=h(Wt),fa=p(Wt,"LI",{});var Ut=c(fa);Os=o(Ut,"Dividir el texto concatenado en trozos m\xE1s peque\xF1os definidos por un "),Ja=p(Ut,"CODE",{});var Ul=c(Ja);Is=o(Ul,"block_size"),Ul.forEach(t),Rs=o(Ut,"."),Ut.forEach(t),Wt.forEach(t),zt=h(e),C(ha.$$.fragment,e),qt=h(e),Ue=p(e,"P",{});var Jt=c(Ue);Ss=o(Jt,"Aplica la funci\xF3n "),Ka=p(Jt,"CODE",{});var Jl=c(Ka);Gs=o(Jl,"group_texts"),Jl.forEach(t),Ns=o(Jt," sobre todo el conjunto de datos:"),Jt.forEach(t),Dt=h(e),C(ga.$$.fragment,e),Ct=h(e),ke=p(e,"P",{});var Fe=c(ke);Hs=o(Fe,"Para modelados de lenguajes causales, usa "),Ya=p(Fe,"CODE",{});var Kl=c(Ya);Bs=o(Kl,"DataCollatorForLanguageModeling"),Kl.forEach(t),Ws=o(Fe," para crear un lote de ejemplos. Esto tambi\xE9n "),Qa=p(Fe,"EM",{});var Yl=c(Qa);Us=o(Yl,"rellenar\xE1 din\xE1micamente"),Yl.forEach(t),Js=o(Fe," tu texto a la dimensi\xF3n del elemento m\xE1s largo del lote, para que de esta manera tengan largo uniforme. Si bien es posible rellenar tu texto en la funci\xF3n "),Va=p(Fe,"CODE",{});var Ql=c(Va);Ks=o(Ql,"tokenizer"),Ql.forEach(t),Ys=o(Fe," mediante el argumento "),Xa=p(Fe,"CODE",{});var Vl=c(Xa);Qs=o(Vl,"padding=True"),Vl.forEach(t),Vs=o(Fe,", el rellenado din\xE1mico es m\xE1s eficiente."),Fe.forEach(t),Tt=h(e),C(Je.$$.fragment,e),Pt=h(e),Re=p(e,"H2",{class:!0});var Kt=c(Re);Ke=p(Kt,"A",{id:!0,class:!0,href:!0});var Xl=c(Ke);Za=p(Xl,"SPAN",{});var Zl=c(Za);C(_a.$$.fragment,Zl),Zl.forEach(t),Xl.forEach(t),Xs=h(Kt),et=p(Kt,"SPAN",{});var eo=c(et);Zs=o(eo,"Causal language modeling"),eo.forEach(t),Kt.forEach(t),At=h(e),Ye=p(e,"P",{});var Yt=c(Ye);el=o(Yt,"El modelado de lenguaje causal es frecuentemente utilizado para generaci\xF3n de texto. Esta secci\xF3n te muestra como realizar fine-tune de "),$a=p(Yt,"A",{href:!0,rel:!0});var ao=c($a);al=o(ao,"DistilGPT2"),ao.forEach(t),tl=o(Yt," para generar nuevo texto."),Yt.forEach(t),Mt=h(e),Se=p(e,"H3",{class:!0});var Qt=c(Se);Qe=p(Qt,"A",{id:!0,class:!0,href:!0});var to=c(Qe);at=p(to,"SPAN",{});var so=c(at);C(ja.$$.fragment,so),so.forEach(t),to.forEach(t),sl=h(Qt),tt=p(Qt,"SPAN",{});var lo=c(tt);ll=o(lo,"Entrenamiento"),lo.forEach(t),Qt.forEach(t),Ft=h(e),C(Ve.$$.fragment,e),Lt=h(e),Ge=p(e,"H2",{class:!0});var Vt=c(Ge);Xe=p(Vt,"A",{id:!0,class:!0,href:!0});var oo=c(Xe);st=p(oo,"SPAN",{});var no=c(st);C(ba.$$.fragment,no),no.forEach(t),oo.forEach(t),ol=h(Vt),lt=p(Vt,"SPAN",{});var ro=c(lt);nl=o(ro,"Modelado de lenguaje por enmascaramiento"),ro.forEach(t),Vt.forEach(t),Ot=h(e),Ze=p(e,"P",{});var Xt=c(Ze);rl=o(Xt,"El modelado de lenguaje por enmascaramiento es tambi\xE9n conocido como una tarea de rellenar la m\xE1scara, pues predice un token enmascarado dada una secuencia. Los modelos de lenguaje por enmascaramiento requieren una buena comprensi\xF3n del contexto de una secuencia entera, en lugar de solo el contexto a la izquierda. Esta secci\xF3n te ense\xF1a como realizar el fine-tune de "),va=p(Xt,"A",{href:!0,rel:!0});var io=c(va);il=o(io,"DistilRoBERTa"),io.forEach(t),pl=o(Xt," para predecir una palabra enmascarada."),Xt.forEach(t),It=h(e),Ne=p(e,"H3",{class:!0});var Zt=c(Ne);ea=p(Zt,"A",{id:!0,class:!0,href:!0});var po=c(ea);ot=p(po,"SPAN",{});var co=c(ot);C(ka.$$.fragment,co),co.forEach(t),po.forEach(t),cl=h(Zt),nt=p(Zt,"SPAN",{});var mo=c(nt);ml=o(mo,"Entrenamiento"),mo.forEach(t),Zt.forEach(t),Rt=h(e),C(aa.$$.fragment,e),St=h(e),C(ta.$$.fragment,e),this.h()},h(){E(s,"name","hf:doc:metadata"),E(s,"content",JSON.stringify(Io)),E(g,"id","modelado-de-lenguaje"),E(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(g,"href","#modelado-de-lenguaje"),E(r,"class","relative group"),E(ee,"href","https://huggingface.co/distilgpt2"),E(ee,"rel","nofollow"),E(K,"href","https://huggingface.co/distilroberta-base"),E(K,"rel","nofollow"),E(oe,"href","https://www.reddit.com/r/askscience/"),E(oe,"rel","nofollow"),E(u,"href","https://huggingface.co/datasets/eli5"),E(u,"rel","nofollow"),E(Q,"id","cargar-el-conjunto-de-datos-eli5"),E(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Q,"href","#cargar-el-conjunto-de-datos-eli5"),E(ce,"class","relative group"),E(He,"id","preprocesamiento"),E(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(He,"href","#preprocesamiento"),E(Ie,"class","relative group"),E(pa,"href","https://huggingface.co/docs/datasets/process.html#flatten"),E(pa,"rel","nofollow"),E(ua,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),E(ua,"rel","nofollow"),E(Ke,"id","causal-language-modeling"),E(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ke,"href","#causal-language-modeling"),E(Re,"class","relative group"),E($a,"href","https://huggingface.co/distilgpt2"),E($a,"rel","nofollow"),E(Qe,"id","entrenamiento"),E(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Qe,"href","#entrenamiento"),E(Se,"class","relative group"),E(Xe,"id","modelado-de-lenguaje-por-enmascaramiento"),E(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Xe,"href","#modelado-de-lenguaje-por-enmascaramiento"),E(Ge,"class","relative group"),E(va,"href","https://huggingface.co/distilroberta-base"),E(va,"rel","nofollow"),E(ea,"id","entrenamiento"),E(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(ea,"href","#entrenamiento"),E(Ne,"class","relative group")},m(e,d){a(document.head,s),m(e,j,d),m(e,r,d),a(r,g),a(g,w),T($,w,null),a(r,b),a(r,q),a(q,v),m(e,x,d),m(e,F,d),a(F,N),m(e,G,d),T(z,e,d),m(e,R,d),m(e,S,d),a(S,B),m(e,I,d),T(U,e,d),m(e,_,d),m(e,k,d),a(k,H),m(e,L,d),m(e,W,d),a(W,V),a(W,ee),a(ee,ae),a(W,pe),a(W,K),a(K,ue),a(W,te),a(W,oe),a(oe,Y),a(W,ne),a(W,u),a(u,O),a(W,re),m(e,le,d),T(X,e,d),m(e,Z,d),m(e,ce,d),a(ce,Q),a(Q,me),T(ie,me,null),a(ce,fe),a(ce,ge),a(ge,de),m(e,_e,d),m(e,n,d),a(n,y),m(e,he,d),T(Ee,e,d),m(e,De,d),m(e,xe,d),a(xe,Le),m(e,$e,d),T(we,e,d),m(e,je,d),m(e,ze,d),a(ze,Oe),m(e,be,d),T(ye,e,d),m(e,Ce,d),m(e,qe,d),a(qe,as),a(qe,Pa),a(Pa,ts),a(qe,ss),a(qe,Aa),a(Aa,ls),a(qe,os),a(qe,Ma),a(Ma,ns),a(qe,rs),m(e,mt,d),m(e,Ie,d),a(Ie,He),a(He,Fa),T(la,Fa,null),a(Ie,is),a(Ie,La),a(La,ps),m(e,ut,d),T(oa,e,d),m(e,dt,d),m(e,Be,d),a(Be,cs),a(Be,Oa),a(Oa,ms),a(Be,us),m(e,ft,d),T(na,e,d),m(e,ht,d),T(ra,e,d),m(e,gt,d),m(e,ya,d),a(ya,ds),m(e,_t,d),T(ia,e,d),m(e,$t,d),m(e,Te,d),a(Te,fs),a(Te,Ia),a(Ia,hs),a(Te,gs),a(Te,pa),a(pa,Ra),a(Ra,_s),a(Te,$s),m(e,jt,d),T(ca,e,d),m(e,bt,d),m(e,Pe,d),a(Pe,js),a(Pe,Sa),a(Sa,bs),a(Pe,vs),a(Pe,Ga),a(Ga,ks),a(Pe,Es),m(e,vt,d),m(e,xa,d),a(xa,ws),m(e,kt,d),T(ma,e,d),m(e,Et,d),m(e,ve,d),a(ve,ys),a(ve,ua),a(ua,Na),a(Na,xs),a(ve,zs),a(ve,Ha),a(Ha,qs),a(ve,Ds),a(ve,Ba),a(Ba,Cs),a(ve,Ts),a(ve,Wa),a(Wa,Ps),a(ve,As),m(e,wt,d),T(da,e,d),m(e,yt,d),m(e,za,d),a(za,Ms),m(e,xt,d),m(e,We,d),a(We,Ua),a(Ua,Fs),a(We,Ls),a(We,fa),a(fa,Os),a(fa,Ja),a(Ja,Is),a(fa,Rs),m(e,zt,d),T(ha,e,d),m(e,qt,d),m(e,Ue,d),a(Ue,Ss),a(Ue,Ka),a(Ka,Gs),a(Ue,Ns),m(e,Dt,d),T(ga,e,d),m(e,Ct,d),m(e,ke,d),a(ke,Hs),a(ke,Ya),a(Ya,Bs),a(ke,Ws),a(ke,Qa),a(Qa,Us),a(ke,Js),a(ke,Va),a(Va,Ks),a(ke,Ys),a(ke,Xa),a(Xa,Qs),a(ke,Vs),m(e,Tt,d),T(Je,e,d),m(e,Pt,d),m(e,Re,d),a(Re,Ke),a(Ke,Za),T(_a,Za,null),a(Re,Xs),a(Re,et),a(et,Zs),m(e,At,d),m(e,Ye,d),a(Ye,el),a(Ye,$a),a($a,al),a(Ye,tl),m(e,Mt,d),m(e,Se,d),a(Se,Qe),a(Qe,at),T(ja,at,null),a(Se,sl),a(Se,tt),a(tt,ll),m(e,Ft,d),T(Ve,e,d),m(e,Lt,d),m(e,Ge,d),a(Ge,Xe),a(Xe,st),T(ba,st,null),a(Ge,ol),a(Ge,lt),a(lt,nl),m(e,Ot,d),m(e,Ze,d),a(Ze,rl),a(Ze,va),a(va,il),a(Ze,pl),m(e,It,d),m(e,Ne,d),a(Ne,ea),a(ea,ot),T(ka,ot,null),a(Ne,cl),a(Ne,nt),a(nt,ml),m(e,Rt,d),T(aa,e,d),m(e,St,d),T(ta,e,d),Gt=!0},p(e,[d]){const Ea={};d&2&&(Ea.$$scope={dirty:d,ctx:e}),X.$set(Ea);const rt={};d&2&&(rt.$$scope={dirty:d,ctx:e}),Je.$set(rt);const it={};d&2&&(it.$$scope={dirty:d,ctx:e}),Ve.$set(it);const pt={};d&2&&(pt.$$scope={dirty:d,ctx:e}),aa.$set(pt);const ct={};d&2&&(ct.$$scope={dirty:d,ctx:e}),ta.$set(ct)},i(e){Gt||(P($.$$.fragment,e),P(z.$$.fragment,e),P(U.$$.fragment,e),P(X.$$.fragment,e),P(ie.$$.fragment,e),P(Ee.$$.fragment,e),P(we.$$.fragment,e),P(ye.$$.fragment,e),P(la.$$.fragment,e),P(oa.$$.fragment,e),P(na.$$.fragment,e),P(ra.$$.fragment,e),P(ia.$$.fragment,e),P(ca.$$.fragment,e),P(ma.$$.fragment,e),P(da.$$.fragment,e),P(ha.$$.fragment,e),P(ga.$$.fragment,e),P(Je.$$.fragment,e),P(_a.$$.fragment,e),P(ja.$$.fragment,e),P(Ve.$$.fragment,e),P(ba.$$.fragment,e),P(ka.$$.fragment,e),P(aa.$$.fragment,e),P(ta.$$.fragment,e),Gt=!0)},o(e){A($.$$.fragment,e),A(z.$$.fragment,e),A(U.$$.fragment,e),A(X.$$.fragment,e),A(ie.$$.fragment,e),A(Ee.$$.fragment,e),A(we.$$.fragment,e),A(ye.$$.fragment,e),A(la.$$.fragment,e),A(oa.$$.fragment,e),A(na.$$.fragment,e),A(ra.$$.fragment,e),A(ia.$$.fragment,e),A(ca.$$.fragment,e),A(ma.$$.fragment,e),A(da.$$.fragment,e),A(ha.$$.fragment,e),A(ga.$$.fragment,e),A(Je.$$.fragment,e),A(_a.$$.fragment,e),A(ja.$$.fragment,e),A(Ve.$$.fragment,e),A(ba.$$.fragment,e),A(ka.$$.fragment,e),A(aa.$$.fragment,e),A(ta.$$.fragment,e),Gt=!1},d(e){t(s),e&&t(j),e&&t(r),M($),e&&t(x),e&&t(F),e&&t(G),M(z,e),e&&t(R),e&&t(S),e&&t(I),M(U,e),e&&t(_),e&&t(k),e&&t(L),e&&t(W),e&&t(le),M(X,e),e&&t(Z),e&&t(ce),M(ie),e&&t(_e),e&&t(n),e&&t(he),M(Ee,e),e&&t(De),e&&t(xe),e&&t($e),M(we,e),e&&t(je),e&&t(ze),e&&t(be),M(ye,e),e&&t(Ce),e&&t(qe),e&&t(mt),e&&t(Ie),M(la),e&&t(ut),M(oa,e),e&&t(dt),e&&t(Be),e&&t(ft),M(na,e),e&&t(ht),M(ra,e),e&&t(gt),e&&t(ya),e&&t(_t),M(ia,e),e&&t($t),e&&t(Te),e&&t(jt),M(ca,e),e&&t(bt),e&&t(Pe),e&&t(vt),e&&t(xa),e&&t(kt),M(ma,e),e&&t(Et),e&&t(ve),e&&t(wt),M(da,e),e&&t(yt),e&&t(za),e&&t(xt),e&&t(We),e&&t(zt),M(ha,e),e&&t(qt),e&&t(Ue),e&&t(Dt),M(ga,e),e&&t(Ct),e&&t(ke),e&&t(Tt),M(Je,e),e&&t(Pt),e&&t(Re),M(_a),e&&t(At),e&&t(Ye),e&&t(Mt),e&&t(Se),M(ja),e&&t(Ft),M(Ve,e),e&&t(Lt),e&&t(Ge),M(ba),e&&t(Ot),e&&t(Ze),e&&t(It),e&&t(Ne),M(ka),e&&t(Rt),M(aa,e),e&&t(St),M(ta,e)}}}const Io={local:"modelado-de-lenguaje",sections:[{local:"cargar-el-conjunto-de-datos-eli5",title:"Cargar el conjunto de datos ELI5"},{local:"preprocesamiento",title:"Preprocesamiento"},{local:"causal-language-modeling",sections:[{local:"entrenamiento",title:"Entrenamiento"}],title:"Causal language modeling"},{local:"modelado-de-lenguaje-por-enmascaramiento",sections:[{local:"entrenamiento",title:"Entrenamiento"}],title:"Modelado de lenguaje por enmascaramiento"}],title:"Modelado de lenguaje"};function Ro(J){return $o(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Wo extends fo{constructor(s){super();ho(this,s,Ro,Oo,go,{})}}export{Wo as default,Io as metadata};
