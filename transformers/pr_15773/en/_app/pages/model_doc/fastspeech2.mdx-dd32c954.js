import{S as Mn,i as Pn,s as qn,e as a,k as d,w as _,t as s,M as Dn,c as n,d as o,m as l,a as r,x as v,h as i,b as c,F as e,g as h,y,q as b,o as k,B as S,v as jn}from"../../chunks/vendor-6b77c823.js";import{T as An}from"../../chunks/Tip-39098574.js";import{D as ae}from"../../chunks/Docstring-17b815d9.js";import{C as Cn}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as yt}from"../../chunks/IconCopyLink-7a11ce68.js";function Ln(bt){let m,B,F,x,L;return{c(){m=a("p"),B=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),F=a("code"),x=s("Module"),L=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l($){m=n($,"P",{});var P=r(m);B=i(P,"Although the recipe for forward pass needs to be defined within this function, one should call the "),F=n(P,"CODE",{});var Y=r(F);x=i(Y,"Module"),Y.forEach(o),L=i(P,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),P.forEach(o)},m($,P){h($,m,P),e(m,B),e(m,F),e(F,x),e(m,L)},d($){$&&o(m)}}}function On(bt){let m,B,F,x,L,$,P,Y,Vt,kt,O,K,Qe,ne,Wt,Ze,Qt,St,U,Zt,re,Bt,Yt,Ft,Ee,Kt,wt,ze,se,Ut,ie,Xt,Gt,Tt,q,Jt,ce,eo,to,de,oo,ao,$t,I,X,Be,le,no,Ye,ro,xt,w,pe,so,N,io,Ce,co,lo,he,po,ho,mo,H,fo,Me,uo,go,Pe,_o,vo,yo,Ke,bo,ko,me,Et,R,G,Ue,fe,So,Xe,Fo,zt,f,ue,wo,Ge,To,$o,D,ge,xo,Je,Eo,zo,et,Co,Mo,J,_e,Po,V,qo,tt,Do,jo,ot,Ao,Lo,Oo,j,ve,Io,qe,No,De,Ho,Ro,at,Vo,Wo,ee,ye,Qo,nt,Zo,Ct,W,te,rt,be,Bo,st,Yo,Mt,M,ke,Ko,Se,Uo,Fe,Xo,Go,Jo,g,we,ea,Q,ta,je,oa,aa,it,na,ra,sa,oe,ia,ct,Te,E,dt,ca,da,Ae,la,pa,lt,ha,ma,pt,fa,ua,ga,u,_a,ht,va,ya,mt,ba,ka,ft,Sa,Fa,ut,wa,Ta,gt,$a,xa,_t,Ea,za,Ca,vt,Ma,Pa,$e,Pt;return $=new yt({}),ne=new yt({}),le=new yt({}),pe=new ae({props:{name:"class transformers.FastSpeech2Config",anchor:"transformers.FastSpeech2Config",parameters:[{name:"vocab_size",val:" = 75"},{name:"encoder_embed_dim",val:" = 256"},{name:"encoder_attention_heads",val:" = 2"},{name:"encoder_layers",val:" = 4"},{name:"decoder_embed_dim",val:" = 256"},{name:"decoder_attention_heads",val:" = 2"},{name:"decoder_layers",val:" = 4"},{name:"attention_dropout",val:" = 0"},{name:"fft_hidden_dim",val:" = 1024"},{name:"fft_kernel_size",val:" = 9"},{name:"fft_dropout",val:" = 0.2"},{name:"var_pred_hidden_dim",val:" = 256"},{name:"var_pred_kernel_size",val:" = 3"},{name:"var_pred_dropout",val:" = 0.5"},{name:"add_postnet",val:" = False"},{name:"postnet_conv_dim",val:" = 512"},{name:"postnet_conv_kernel_size",val:" = 5"},{name:"postnet_layers",val:" = 5"},{name:"postnet_dropout",val:" = 0.5"},{name:"pitch_min",val:" = -4.660287183665281"},{name:"pitch_max",val:" = 5.733940816898645"},{name:"energy_min",val:" = -4.9544901847839355"},{name:"energy_max",val:" = 3.2244551181793213"},{name:"speaker_embed_dim",val:" = 64"},{name:"num_speakers",val:" = 1"},{name:"max_source_positions",val:" = 1024"},{name:"initializer_range",val:" = 0.0625"},{name:"use_mean",val:" = True"},{name:"use_standard_deviation",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Vocabulary size of the FastSpeech2 model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model">~FastSpeech2Model</a>.`,name:"vocab_size"},{anchor:"transformers.FastSpeech2Config.encoder_embed_dim",description:`<strong>encoder_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the encoder layers.`,name:"encoder_embed_dim"},{anchor:"transformers.FastSpeech2Config.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the encoder.`,name:"encoder_layers"},{anchor:"transformers.FastSpeech2Config.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.decoder_embed_dim",description:`<strong>decoder_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the decoder layers.`,name:"decoder_embed_dim"},{anchor:"transformers.FastSpeech2Config.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the decoder.`,name:"decoder_layers"},{anchor:"transformers.FastSpeech2Config.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FastSpeech2Config.fft_hidden_dim",description:`<strong>fft_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the feed forward layers.`,name:"fft_hidden_dim"},{anchor:"transformers.FastSpeech2Config.fft_kernel_size",description:`<strong>fft_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 9) &#x2014;
Kernel size of the feed forward layers.`,name:"fft_kernel_size"},{anchor:"transformers.FastSpeech2Config.fft_dropout",description:`<strong>fft_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout ratio for the feedforward layers.`,name:"fft_dropout"},{anchor:"transformers.FastSpeech2Config.var_pred_hidden_dim",description:`<strong>var_pred_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden size of the variance predictor.`,name:"var_pred_hidden_dim"},{anchor:"transformers.FastSpeech2Config.var_pred_kernel_size",description:`<strong>var_pred_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Kernel size of the variance predictor layer.`,name:"var_pred_kernel_size"},{anchor:"transformers.FastSpeech2Config.var_pred_dropout",description:`<strong>var_pred_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout ratio for the variance predictor.`,name:"var_pred_dropout"},{anchor:"transformers.FastSpeech2Config.add_postnet",description:`<strong>add_postnet</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Flag that specifies whether or not to add postnet.`,name:"add_postnet"},{anchor:"transformers.FastSpeech2Config.postnet_conv_dim",description:`<strong>postnet_conv_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the postnet convolution layers.`,name:"postnet_conv_dim"},{anchor:"transformers.FastSpeech2Config.postnet_conv_kernel_size",description:`<strong>postnet_conv_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Kernel size of the convolution layers in the postnet.`,name:"postnet_conv_kernel_size"},{anchor:"transformers.FastSpeech2Config.postnet_layers",description:`<strong>postnet_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Number of hidden layers in the postnet.`,name:"postnet_layers"},{anchor:"transformers.FastSpeech2Config.postnet_dropout",description:`<strong>postnet_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout ratio for the postnet.`,name:"postnet_dropout"},{anchor:"transformers.FastSpeech2Config.pitch_min",description:`<strong>pitch_min</strong> (<code>float</code>, <em>optional</em>, defaults to -4.660287183665281) &#x2014;
The minimum pitch value of the pitch bucket in the variance predictor.`,name:"pitch_min"},{anchor:"transformers.FastSpeech2Config.pitch_max",description:`<strong>pitch_max</strong> (<code>float</code>, <em>optional</em>, defaults to 5.733940816898645) &#x2014;
The maximum pitch value of the pitch bucket in the variance predictor.`,name:"pitch_max"},{anchor:"transformers.FastSpeech2Config.energy_min",description:`<strong>energy_min</strong> (<code>float</code>, <em>optional</em>, defaults to -4.9544901847839355) &#x2014;
The minimum energy value of the pitch bucket in the variance predictor.`,name:"energy_min"},{anchor:"transformers.FastSpeech2Config.energy_max",description:`<strong>energy_max</strong> (<code>float</code>, <em>optional</em>, defaults to 3.2244551181793213) &#x2014;
The maximum energy value of the pitch bucket in the variance predictor.`,name:"energy_max"},{anchor:"transformers.FastSpeech2Config.speaker_embed_dim",description:`<strong>speaker_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the speaker identity embedding.`,name:"speaker_embed_dim"},{anchor:"transformers.FastSpeech2Config.num_speakers",description:`<strong>num_speakers</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of speakers. Set to 1 if the dataset is a single-speaker dataset. Otherwise, set to the number of
speakers in the multi-speaker training dataset.`,name:"num_speakers"},{anchor:"transformers.FastSpeech2Config.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_source_positions"},{anchor:"transformers.FastSpeech2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0625) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all embedding weight matrices.`,name:"initializer_range"},{anchor:"transformers.FastSpeech2Config.use_mean",description:`<strong>use_mean</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Flag that specifies whether or not to denormalize predicted output using cepstral mean and variance
normalization. For more information, please refer to <a href="https://en.wikipedia.org/wiki/Cepstral_mean_and_variance_normalization" rel="nofollow">cepstral mean and variance
normalization</a>.`,name:"use_mean"},{anchor:"transformers.FastSpeech2Config.use_standard_deviation",description:`<strong>use_standard_deviation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Flag that specifies whether or not to scale predicted output using cepstral mean and variance
normalization. For more information, please refer to <a href="https://en.wikipedia.org/wiki/Cepstral_mean_and_variance_normalization" rel="nofollow">cepstral mean and variance
normalization</a>.`,name:"use_standard_deviation"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/configuration_fastspeech2.py#L30"}}),me=new Cn({props:{code:`from transformers import FastSpeech2Model, FastSpeech2Config

# Initializing a FastSpeech2 fastspeech2 style configuration
configuration = FastSpeech2Config()

# Initializing a model from the fastspeech2 style configuration
model = FastSpeech2Model(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FastSpeech2Model, FastSpeech2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FastSpeech2 fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FastSpeech2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FastSpeech2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),fe=new yt({}),ue=new ae({props:{name:"class transformers.FastSpeech2Tokenizer",anchor:"transformers.FastSpeech2Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"do_phonemize",val:" = True"},{name:"preserve_punctuation",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L42"}}),ge=new ae({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2876",returnDescription:`
<p>The model input with special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),_e=new ae({props:{name:"get_special_tokens_mask",anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List"},{name:"token_ids_1",val:": typing.Optional[typing.List] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils.py#L842",returnDescription:`
<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p>A list of integers in the range [0, 1]</p>
`}}),ve=new ae({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2856",returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ye=new ae({props:{name:"save_vocabulary",anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
The directory in which to save the vocabulary.`,name:"save_directory"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L126",returnDescription:`
<p>Paths to the files saved.</p>
`,returnType:`
<p><code>Tuple(str)</code></p>
`}}),be=new yt({}),ke=new ae({props:{name:"class transformers.FastSpeech2Model",anchor:"transformers.FastSpeech2Model",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Model.This",description:'<strong>This</strong> model inherits from <a href="/docs/transformers/pr_15773/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the &#x2014;',name:"This"},{anchor:"transformers.FastSpeech2Model.library",description:`<strong>library</strong> implements for all its model (such as downloading or saving, etc.) This model is also a PyTorch &#x2014;
[torch.nn.Module](https &#x2014;//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it as a regular PyTorch`,name:"library"},{anchor:"transformers.FastSpeech2Model.Module",description:`<strong>Module</strong> and refer to the PyTorch documentation for all matter related to general usage and behavior. &#x2014;
config (<a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Config">FastSpeech2Config</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15773/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"Module"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L683"}}),we=new ae({props:{name:"forward",anchor:"transformers.FastSpeech2Model.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"speaker_id",val:": typing.Optional[torch.Tensor] = None"},{name:"durations",val:": typing.Optional[torch.Tensor] = None"},{name:"pitches",val:": typing.Optional[torch.Tensor] = None"},{name:"energies",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Tokenizer">FastSpeech2Tokenizer</a>.
See <a href="/docs/transformers/pr_15773/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/pr_15773/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FastSpeech2Model.forward.speaker_id",description:`<strong>speaker_id</strong> (<code>torch.LongTensor</code> of shape <code>(1)</code>, <em>optional</em>) &#x2014;
Index of a single speaker id. The index must be between 0 and <code>FastSpeech2Config.num_speakers - 1</code>.`,name:"speaker_id"},{anchor:"transformers.FastSpeech2Model.forward.durations",description:`<strong>durations</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Duration information for each token. The value of <code>durations[batch_idx][i]</code> represents the number of
mel-spectrogram frames attributed to the <code>i</code>-th token in the <code>batch_idx</code> batch.`,name:"durations"},{anchor:"transformers.FastSpeech2Model.forward.pitches",description:`<strong>pitches</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Pitch level information for each token.`,name:"pitches"},{anchor:"transformers.FastSpeech2Model.forward.energies",description:`<strong>energies</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Amount of energy attributed to each token.`,name:"energies"},{anchor:"transformers.FastSpeech2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15773/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L732",returnDescription:`
<p>A <code>transformers.models.fastspeech2.modeling_fastspeech2.FastSpeech2ModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Config"
>FastSpeech2Config</a>) and inputs.</p>
`,returnType:`
<p><code>transformers.models.fastspeech2.modeling_fastspeech2.FastSpeech2ModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new An({props:{$$slots:{default:[Ln]},$$scope:{ctx:bt}}}),$e=new Cn({props:{code:`from transformers import FastSpeech2Tokenizer, FastSpeech2Model
import torch

tokenizer = FastSpeech2Tokenizer.from_pretrained("fastspeech2")
model = FastSpeech2Model.from_pretrained("fastspeech2")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FastSpeech2Tokenizer, FastSpeech2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FastSpeech2Tokenizer.from_pretrained(<span class="hljs-string">&quot;fastspeech2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FastSpeech2Model.from_pretrained(<span class="hljs-string">&quot;fastspeech2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){m=a("meta"),B=d(),F=a("h1"),x=a("a"),L=a("span"),_($.$$.fragment),P=d(),Y=a("span"),Vt=s("FastSpeech2"),kt=d(),O=a("h2"),K=a("a"),Qe=a("span"),_(ne.$$.fragment),Wt=d(),Ze=a("span"),Qt=s("Overview"),St=d(),U=a("p"),Zt=s("The FastSpeech2 model was proposed in "),re=a("a"),Bt=s("FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),Yt=s("  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),Ft=d(),Ee=a("p"),Kt=s("The abstract from the paper is the following:"),wt=d(),ze=a("p"),se=a("em"),Ut=s("Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),ie=a("a"),Xt=s("https://speechresearch.github.io/fastspeech2/"),Gt=s("."),Tt=d(),q=a("p"),Jt=s("This model was contributed by "),ce=a("a"),eo=s("jaketae"),to=s(". The original code can be found "),de=a("a"),oo=s("here"),ao=s("."),$t=d(),I=a("h2"),X=a("a"),Be=a("span"),_(le.$$.fragment),no=d(),Ye=a("span"),ro=s("FastSpeech2Config"),xt=d(),w=a("div"),_(pe.$$.fragment),so=d(),N=a("p"),io=s("This is the configuration class to store the configuration of a "),Ce=a("a"),co=s("FastSpeech2Model"),lo=s(`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),he=a("a"),po=s("fastspeech2"),ho=s(" architecture."),mo=d(),H=a("p"),fo=s("Configuration objects inherit from "),Me=a("a"),uo=s("PretrainedConfig"),go=s(` and can be used to control the model outputs. Read the
documentation from `),Pe=a("a"),_o=s("PretrainedConfig"),vo=s(" for more information."),yo=d(),Ke=a("p"),bo=s("Example:"),ko=d(),_(me.$$.fragment),Et=d(),R=a("h2"),G=a("a"),Ue=a("span"),_(fe.$$.fragment),So=d(),Xe=a("span"),Fo=s("FastSpeech2Tokenizer"),zt=d(),f=a("div"),_(ue.$$.fragment),wo=d(),Ge=a("p"),To=s("Construct a FastSpeech2 tokenizer."),$o=d(),D=a("div"),_(ge.$$.fragment),xo=d(),Je=a("p"),Eo=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),zo=d(),et=a("p"),Co=s("This implementation does not add special tokens and this method should be overridden in a subclass."),Mo=d(),J=a("div"),_(_e.$$.fragment),Po=d(),V=a("p"),qo=s(`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),tt=a("code"),Do=s("prepare_for_model"),jo=s(" or "),ot=a("code"),Ao=s("encode_plus"),Lo=s(" methods."),Oo=d(),j=a("div"),_(ve.$$.fragment),Io=d(),qe=a("p"),No=s("Create the token type IDs corresponding to the sequences passed. "),De=a("a"),Ho=s(`What are token type
IDs?`),Ro=d(),at=a("p"),Vo=s("Should be overridden in a subclass if the model has a special way of building those."),Wo=d(),ee=a("div"),_(ye.$$.fragment),Qo=d(),nt=a("p"),Zo=s("Save the vocabulary and special tokens file to a directory."),Ct=d(),W=a("h2"),te=a("a"),rt=a("span"),_(be.$$.fragment),Bo=d(),st=a("span"),Yo=s("FastSpeech2Model"),Mt=d(),M=a("div"),_(ke.$$.fragment),Ko=d(),Se=a("p"),Uo=s(`The FastSpeech2 Model that outputs predicted mel-spectrograms.
FastSpeech2 was proposed in `),Fe=a("a"),Xo=s(`FastSpeech 2: Fast and High-Quality End-to-End Text to
Speech`),Go=s(` by Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan
Liu.`),Jo=d(),g=a("div"),_(we.$$.fragment),ea=d(),Q=a("p"),ta=s("The "),je=a("a"),oa=s("FastSpeech2Model"),aa=s(" forward method, overrides the "),it=a("code"),na=s("__call__"),ra=s(" special method."),sa=d(),_(oe.$$.fragment),ia=d(),ct=a("ul"),Te=a("li"),E=a("p"),dt=a("strong"),ca=s("Output"),da=s(" type of "),Ae=a("a"),la=s("FastSpeech2Model"),pa=s(`.
mel_spectrogram (`),lt=a("code"),ha=s("torch.FloatTensor"),ma=s(" of shape "),pt=a("code"),fa=s("(batch_size, sequence_length, mel_s)"),ua=s(`) \u2014 Total loss as the sum of the masked language modeling loss and the next sequence prediction
(classification) loss.`),ga=d(),u=a("p"),_a=s("hidden_states ("),ht=a("code"),va=s("tuple(torch.FloatTensor)"),ya=s(", "),mt=a("em"),ba=s("optional"),ka=s(", returned when "),ft=a("code"),Sa=s("output_hidden_states=True"),Fa=s(` is passed or
when `),ut=a("code"),wa=s("config.output_hidden_states=True"),Ta=s(") \u2014 Tuple of "),gt=a("code"),$a=s("torch.FloatTensor"),xa=s(` (one for the output of the embeddings + one for the output of each layer) of
shape `),_t=a("code"),Ea=s("(batch_size, sequence_length, hidden_size)"),za=s(`. Hidden-states of the model at the output of each layer
plus the initial embedding outputs.`),Ca=d(),vt=a("p"),Ma=s("Example:"),Pa=d(),_($e.$$.fragment),this.h()},l(t){const p=Dn('[data-svelte="svelte-1phssyn"]',document.head);m=n(p,"META",{name:!0,content:!0}),p.forEach(o),B=l(t),F=n(t,"H1",{class:!0});var xe=r(F);x=n(xe,"A",{id:!0,class:!0,href:!0});var Da=r(x);L=n(Da,"SPAN",{});var ja=r(L);v($.$$.fragment,ja),ja.forEach(o),Da.forEach(o),P=l(xe),Y=n(xe,"SPAN",{});var Aa=r(Y);Vt=i(Aa,"FastSpeech2"),Aa.forEach(o),xe.forEach(o),kt=l(t),O=n(t,"H2",{class:!0});var qt=r(O);K=n(qt,"A",{id:!0,class:!0,href:!0});var La=r(K);Qe=n(La,"SPAN",{});var Oa=r(Qe);v(ne.$$.fragment,Oa),Oa.forEach(o),La.forEach(o),Wt=l(qt),Ze=n(qt,"SPAN",{});var Ia=r(Ze);Qt=i(Ia,"Overview"),Ia.forEach(o),qt.forEach(o),St=l(t),U=n(t,"P",{});var Dt=r(U);Zt=i(Dt,"The FastSpeech2 model was proposed in "),re=n(Dt,"A",{href:!0,rel:!0});var Na=r(re);Bt=i(Na,"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),Na.forEach(o),Yt=i(Dt,"  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),Dt.forEach(o),Ft=l(t),Ee=n(t,"P",{});var Ha=r(Ee);Kt=i(Ha,"The abstract from the paper is the following:"),Ha.forEach(o),wt=l(t),ze=n(t,"P",{});var Ra=r(ze);se=n(Ra,"EM",{});var jt=r(se);Ut=i(jt,"Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),ie=n(jt,"A",{href:!0,rel:!0});var Va=r(ie);Xt=i(Va,"https://speechresearch.github.io/fastspeech2/"),Va.forEach(o),Gt=i(jt,"."),jt.forEach(o),Ra.forEach(o),Tt=l(t),q=n(t,"P",{});var Le=r(q);Jt=i(Le,"This model was contributed by "),ce=n(Le,"A",{href:!0,rel:!0});var Wa=r(ce);eo=i(Wa,"jaketae"),Wa.forEach(o),to=i(Le,". The original code can be found "),de=n(Le,"A",{href:!0,rel:!0});var Qa=r(de);oo=i(Qa,"here"),Qa.forEach(o),ao=i(Le,"."),Le.forEach(o),$t=l(t),I=n(t,"H2",{class:!0});var At=r(I);X=n(At,"A",{id:!0,class:!0,href:!0});var Za=r(X);Be=n(Za,"SPAN",{});var Ba=r(Be);v(le.$$.fragment,Ba),Ba.forEach(o),Za.forEach(o),no=l(At),Ye=n(At,"SPAN",{});var Ya=r(Ye);ro=i(Ya,"FastSpeech2Config"),Ya.forEach(o),At.forEach(o),xt=l(t),w=n(t,"DIV",{class:!0});var A=r(w);v(pe.$$.fragment,A),so=l(A),N=n(A,"P",{});var Oe=r(N);io=i(Oe,"This is the configuration class to store the configuration of a "),Ce=n(Oe,"A",{href:!0});var Ka=r(Ce);co=i(Ka,"FastSpeech2Model"),Ka.forEach(o),lo=i(Oe,`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),he=n(Oe,"A",{href:!0,rel:!0});var Ua=r(he);po=i(Ua,"fastspeech2"),Ua.forEach(o),ho=i(Oe," architecture."),Oe.forEach(o),mo=l(A),H=n(A,"P",{});var Ie=r(H);fo=i(Ie,"Configuration objects inherit from "),Me=n(Ie,"A",{href:!0});var Xa=r(Me);uo=i(Xa,"PretrainedConfig"),Xa.forEach(o),go=i(Ie,` and can be used to control the model outputs. Read the
documentation from `),Pe=n(Ie,"A",{href:!0});var Ga=r(Pe);_o=i(Ga,"PretrainedConfig"),Ga.forEach(o),vo=i(Ie," for more information."),Ie.forEach(o),yo=l(A),Ke=n(A,"P",{});var Ja=r(Ke);bo=i(Ja,"Example:"),Ja.forEach(o),ko=l(A),v(me.$$.fragment,A),A.forEach(o),Et=l(t),R=n(t,"H2",{class:!0});var Lt=r(R);G=n(Lt,"A",{id:!0,class:!0,href:!0});var en=r(G);Ue=n(en,"SPAN",{});var tn=r(Ue);v(fe.$$.fragment,tn),tn.forEach(o),en.forEach(o),So=l(Lt),Xe=n(Lt,"SPAN",{});var on=r(Xe);Fo=i(on,"FastSpeech2Tokenizer"),on.forEach(o),Lt.forEach(o),zt=l(t),f=n(t,"DIV",{class:!0});var z=r(f);v(ue.$$.fragment,z),wo=l(z),Ge=n(z,"P",{});var an=r(Ge);To=i(an,"Construct a FastSpeech2 tokenizer."),an.forEach(o),$o=l(z),D=n(z,"DIV",{class:!0});var Ne=r(D);v(ge.$$.fragment,Ne),xo=l(Ne),Je=n(Ne,"P",{});var nn=r(Je);Eo=i(nn,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),nn.forEach(o),zo=l(Ne),et=n(Ne,"P",{});var rn=r(et);Co=i(rn,"This implementation does not add special tokens and this method should be overridden in a subclass."),rn.forEach(o),Ne.forEach(o),Mo=l(z),J=n(z,"DIV",{class:!0});var Ot=r(J);v(_e.$$.fragment,Ot),Po=l(Ot),V=n(Ot,"P",{});var He=r(V);qo=i(He,`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),tt=n(He,"CODE",{});var sn=r(tt);Do=i(sn,"prepare_for_model"),sn.forEach(o),jo=i(He," or "),ot=n(He,"CODE",{});var cn=r(ot);Ao=i(cn,"encode_plus"),cn.forEach(o),Lo=i(He," methods."),He.forEach(o),Ot.forEach(o),Oo=l(z),j=n(z,"DIV",{class:!0});var Re=r(j);v(ve.$$.fragment,Re),Io=l(Re),qe=n(Re,"P",{});var qa=r(qe);No=i(qa,"Create the token type IDs corresponding to the sequences passed. "),De=n(qa,"A",{href:!0});var dn=r(De);Ho=i(dn,`What are token type
IDs?`),dn.forEach(o),qa.forEach(o),Ro=l(Re),at=n(Re,"P",{});var ln=r(at);Vo=i(ln,"Should be overridden in a subclass if the model has a special way of building those."),ln.forEach(o),Re.forEach(o),Wo=l(z),ee=n(z,"DIV",{class:!0});var It=r(ee);v(ye.$$.fragment,It),Qo=l(It),nt=n(It,"P",{});var pn=r(nt);Zo=i(pn,"Save the vocabulary and special tokens file to a directory."),pn.forEach(o),It.forEach(o),z.forEach(o),Ct=l(t),W=n(t,"H2",{class:!0});var Nt=r(W);te=n(Nt,"A",{id:!0,class:!0,href:!0});var hn=r(te);rt=n(hn,"SPAN",{});var mn=r(rt);v(be.$$.fragment,mn),mn.forEach(o),hn.forEach(o),Bo=l(Nt),st=n(Nt,"SPAN",{});var fn=r(st);Yo=i(fn,"FastSpeech2Model"),fn.forEach(o),Nt.forEach(o),Mt=l(t),M=n(t,"DIV",{class:!0});var Ve=r(M);v(ke.$$.fragment,Ve),Ko=l(Ve),Se=n(Ve,"P",{});var Ht=r(Se);Uo=i(Ht,`The FastSpeech2 Model that outputs predicted mel-spectrograms.
FastSpeech2 was proposed in `),Fe=n(Ht,"A",{href:!0,rel:!0});var un=r(Fe);Xo=i(un,`FastSpeech 2: Fast and High-Quality End-to-End Text to
Speech`),un.forEach(o),Go=i(Ht,` by Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan
Liu.`),Ht.forEach(o),Jo=l(Ve),g=n(Ve,"DIV",{class:!0});var C=r(g);v(we.$$.fragment,C),ea=l(C),Q=n(C,"P",{});var We=r(Q);ta=i(We,"The "),je=n(We,"A",{href:!0});var gn=r(je);oa=i(gn,"FastSpeech2Model"),gn.forEach(o),aa=i(We," forward method, overrides the "),it=n(We,"CODE",{});var _n=r(it);na=i(_n,"__call__"),_n.forEach(o),ra=i(We," special method."),We.forEach(o),sa=l(C),v(oe.$$.fragment,C),ia=l(C),ct=n(C,"UL",{});var vn=r(ct);Te=n(vn,"LI",{});var Rt=r(Te);E=n(Rt,"P",{});var Z=r(E);dt=n(Z,"STRONG",{});var yn=r(dt);ca=i(yn,"Output"),yn.forEach(o),da=i(Z," type of "),Ae=n(Z,"A",{href:!0});var bn=r(Ae);la=i(bn,"FastSpeech2Model"),bn.forEach(o),pa=i(Z,`.
mel_spectrogram (`),lt=n(Z,"CODE",{});var kn=r(lt);ha=i(kn,"torch.FloatTensor"),kn.forEach(o),ma=i(Z," of shape "),pt=n(Z,"CODE",{});var Sn=r(pt);fa=i(Sn,"(batch_size, sequence_length, mel_s)"),Sn.forEach(o),ua=i(Z,`) \u2014 Total loss as the sum of the masked language modeling loss and the next sequence prediction
(classification) loss.`),Z.forEach(o),ga=l(Rt),u=n(Rt,"P",{});var T=r(u);_a=i(T,"hidden_states ("),ht=n(T,"CODE",{});var Fn=r(ht);va=i(Fn,"tuple(torch.FloatTensor)"),Fn.forEach(o),ya=i(T,", "),mt=n(T,"EM",{});var wn=r(mt);ba=i(wn,"optional"),wn.forEach(o),ka=i(T,", returned when "),ft=n(T,"CODE",{});var Tn=r(ft);Sa=i(Tn,"output_hidden_states=True"),Tn.forEach(o),Fa=i(T,` is passed or
when `),ut=n(T,"CODE",{});var $n=r(ut);wa=i($n,"config.output_hidden_states=True"),$n.forEach(o),Ta=i(T,") \u2014 Tuple of "),gt=n(T,"CODE",{});var xn=r(gt);$a=i(xn,"torch.FloatTensor"),xn.forEach(o),xa=i(T,` (one for the output of the embeddings + one for the output of each layer) of
shape `),_t=n(T,"CODE",{});var En=r(_t);Ea=i(En,"(batch_size, sequence_length, hidden_size)"),En.forEach(o),za=i(T,`. Hidden-states of the model at the output of each layer
plus the initial embedding outputs.`),T.forEach(o),Rt.forEach(o),vn.forEach(o),Ca=l(C),vt=n(C,"P",{});var zn=r(vt);Ma=i(zn,"Example:"),zn.forEach(o),Pa=l(C),v($e.$$.fragment,C),C.forEach(o),Ve.forEach(o),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(In)),c(x,"id","fastspeech2"),c(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x,"href","#fastspeech2"),c(F,"class","relative group"),c(K,"id","overview"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#overview"),c(O,"class","relative group"),c(re,"href","https://arxiv.org/abs/2006.04558"),c(re,"rel","nofollow"),c(ie,"href","https://speechresearch.github.io/fastspeech2/"),c(ie,"rel","nofollow"),c(ce,"href","https://huggingface.co/jaketae"),c(ce,"rel","nofollow"),c(de,"href","https://github.com/pytorch/fairseq"),c(de,"rel","nofollow"),c(X,"id","transformers.FastSpeech2Config"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#transformers.FastSpeech2Config"),c(I,"class","relative group"),c(Ce,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),c(he,"href","https://huggingface.co/jaketae/fastspeech2"),c(he,"rel","nofollow"),c(Me,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),c(Pe,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),c(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(G,"id","transformers.FastSpeech2Tokenizer"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#transformers.FastSpeech2Tokenizer"),c(R,"class","relative group"),c(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(De,"href","../glossary#token-type-ids"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(te,"id","transformers.FastSpeech2Model"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#transformers.FastSpeech2Model"),c(W,"class","relative group"),c(Fe,"href","https://arxiv.org/abs/2006.04558"),c(Fe,"rel","nofollow"),c(je,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),c(Ae,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),c(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,m),h(t,B,p),h(t,F,p),e(F,x),e(x,L),y($,L,null),e(F,P),e(F,Y),e(Y,Vt),h(t,kt,p),h(t,O,p),e(O,K),e(K,Qe),y(ne,Qe,null),e(O,Wt),e(O,Ze),e(Ze,Qt),h(t,St,p),h(t,U,p),e(U,Zt),e(U,re),e(re,Bt),e(U,Yt),h(t,Ft,p),h(t,Ee,p),e(Ee,Kt),h(t,wt,p),h(t,ze,p),e(ze,se),e(se,Ut),e(se,ie),e(ie,Xt),e(se,Gt),h(t,Tt,p),h(t,q,p),e(q,Jt),e(q,ce),e(ce,eo),e(q,to),e(q,de),e(de,oo),e(q,ao),h(t,$t,p),h(t,I,p),e(I,X),e(X,Be),y(le,Be,null),e(I,no),e(I,Ye),e(Ye,ro),h(t,xt,p),h(t,w,p),y(pe,w,null),e(w,so),e(w,N),e(N,io),e(N,Ce),e(Ce,co),e(N,lo),e(N,he),e(he,po),e(N,ho),e(w,mo),e(w,H),e(H,fo),e(H,Me),e(Me,uo),e(H,go),e(H,Pe),e(Pe,_o),e(H,vo),e(w,yo),e(w,Ke),e(Ke,bo),e(w,ko),y(me,w,null),h(t,Et,p),h(t,R,p),e(R,G),e(G,Ue),y(fe,Ue,null),e(R,So),e(R,Xe),e(Xe,Fo),h(t,zt,p),h(t,f,p),y(ue,f,null),e(f,wo),e(f,Ge),e(Ge,To),e(f,$o),e(f,D),y(ge,D,null),e(D,xo),e(D,Je),e(Je,Eo),e(D,zo),e(D,et),e(et,Co),e(f,Mo),e(f,J),y(_e,J,null),e(J,Po),e(J,V),e(V,qo),e(V,tt),e(tt,Do),e(V,jo),e(V,ot),e(ot,Ao),e(V,Lo),e(f,Oo),e(f,j),y(ve,j,null),e(j,Io),e(j,qe),e(qe,No),e(qe,De),e(De,Ho),e(j,Ro),e(j,at),e(at,Vo),e(f,Wo),e(f,ee),y(ye,ee,null),e(ee,Qo),e(ee,nt),e(nt,Zo),h(t,Ct,p),h(t,W,p),e(W,te),e(te,rt),y(be,rt,null),e(W,Bo),e(W,st),e(st,Yo),h(t,Mt,p),h(t,M,p),y(ke,M,null),e(M,Ko),e(M,Se),e(Se,Uo),e(Se,Fe),e(Fe,Xo),e(Se,Go),e(M,Jo),e(M,g),y(we,g,null),e(g,ea),e(g,Q),e(Q,ta),e(Q,je),e(je,oa),e(Q,aa),e(Q,it),e(it,na),e(Q,ra),e(g,sa),y(oe,g,null),e(g,ia),e(g,ct),e(ct,Te),e(Te,E),e(E,dt),e(dt,ca),e(E,da),e(E,Ae),e(Ae,la),e(E,pa),e(E,lt),e(lt,ha),e(E,ma),e(E,pt),e(pt,fa),e(E,ua),e(Te,ga),e(Te,u),e(u,_a),e(u,ht),e(ht,va),e(u,ya),e(u,mt),e(mt,ba),e(u,ka),e(u,ft),e(ft,Sa),e(u,Fa),e(u,ut),e(ut,wa),e(u,Ta),e(u,gt),e(gt,$a),e(u,xa),e(u,_t),e(_t,Ea),e(u,za),e(g,Ca),e(g,vt),e(vt,Ma),e(g,Pa),y($e,g,null),Pt=!0},p(t,[p]){const xe={};p&2&&(xe.$$scope={dirty:p,ctx:t}),oe.$set(xe)},i(t){Pt||(b($.$$.fragment,t),b(ne.$$.fragment,t),b(le.$$.fragment,t),b(pe.$$.fragment,t),b(me.$$.fragment,t),b(fe.$$.fragment,t),b(ue.$$.fragment,t),b(ge.$$.fragment,t),b(_e.$$.fragment,t),b(ve.$$.fragment,t),b(ye.$$.fragment,t),b(be.$$.fragment,t),b(ke.$$.fragment,t),b(we.$$.fragment,t),b(oe.$$.fragment,t),b($e.$$.fragment,t),Pt=!0)},o(t){k($.$$.fragment,t),k(ne.$$.fragment,t),k(le.$$.fragment,t),k(pe.$$.fragment,t),k(me.$$.fragment,t),k(fe.$$.fragment,t),k(ue.$$.fragment,t),k(ge.$$.fragment,t),k(_e.$$.fragment,t),k(ve.$$.fragment,t),k(ye.$$.fragment,t),k(be.$$.fragment,t),k(ke.$$.fragment,t),k(we.$$.fragment,t),k(oe.$$.fragment,t),k($e.$$.fragment,t),Pt=!1},d(t){o(m),t&&o(B),t&&o(F),S($),t&&o(kt),t&&o(O),S(ne),t&&o(St),t&&o(U),t&&o(Ft),t&&o(Ee),t&&o(wt),t&&o(ze),t&&o(Tt),t&&o(q),t&&o($t),t&&o(I),S(le),t&&o(xt),t&&o(w),S(pe),S(me),t&&o(Et),t&&o(R),S(fe),t&&o(zt),t&&o(f),S(ue),S(ge),S(_e),S(ve),S(ye),t&&o(Ct),t&&o(W),S(be),t&&o(Mt),t&&o(M),S(ke),S(we),S(oe),S($e)}}}const In={local:"fastspeech2",sections:[{local:"overview",title:"Overview"},{local:"transformers.FastSpeech2Config",title:"FastSpeech2Config"},{local:"transformers.FastSpeech2Tokenizer",title:"FastSpeech2Tokenizer"},{local:"transformers.FastSpeech2Model",title:"FastSpeech2Model"}],title:"FastSpeech2"};function Nn(bt){return jn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zn extends Mn{constructor(m){super();Pn(this,m,Nn,On,qn,{})}}export{Zn as default,In as metadata};
