import{S as Za,i as en,s as tn,e as n,k as l,w as f,t as i,M as an,c as s,d as a,m as p,a as o,x as u,h as c,b as r,F as t,g as h,y as g,L as nn,q as _,o as v,B as y,v as sn}from"../../chunks/vendor-6b77c823.js";import{D as B}from"../../chunks/Docstring-af1d0ae0.js";import{C as on}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Re}from"../../chunks/IconCopyLink-7a11ce68.js";function rn(va){let w,Ve,T,L,we,W,ht,Te,mt,Be,E,N,Ee,Q,ft,xe,ut,We,I,gt,J,_t,vt,Qe,de,yt,Je,he,U,kt,G,bt,St,Ue,S,Ft,K,$t,wt,X,Tt,Et,Ge,x,M,ze,Y,xt,Ce,zt,Ke,k,Z,Ct,z,Pt,me,qt,At,ee,Dt,Lt,Nt,C,It,fe,Mt,jt,ue,Ot,Ht,Rt,te,Xe,P,j,Pe,ae,Vt,qe,Bt,Ye,m,ne,Wt,Ae,Qt,Jt,F,se,Ut,De,Gt,Kt,Le,Xt,Yt,O,oe,Zt,q,ea,Ne,ta,aa,Ie,na,sa,oa,$,re,ra,ge,ia,_e,ca,la,Me,pa,da,H,ie,ha,je,ma,Ze,A,R,Oe,ce,fa,He,ua,et,D,le,ga,ve,pe,tt;return W=new Re({}),Q=new Re({}),Y=new Re({}),Z=new B({props:{name:"class transformers.FastSpeech2Config",anchor:"transformers.FastSpeech2Config",parameters:[{name:"n_frames_per_step",val:" = 1"},{name:"output_frame_dim",val:" = 80"},{name:"encoder_embed_dim",val:" = 256"},{name:"speaker_embed_dim",val:" = 64"},{name:"dropout",val:" = 0.2"},{name:"max_source_positions",val:" = 1024"},{name:"encoder_attention_heads",val:" = 2"},{name:"fft_hidden_dim",val:" = 1024"},{name:"fft_kernel_size",val:" = 9"},{name:"attention_dropout",val:" = 0"},{name:"encoder_layers",val:" = 4"},{name:"decoder_embed_dim",val:" = 256"},{name:"decoder_attention_heads",val:" = 2"},{name:"decoder_layers",val:" = 4"},{name:"add_postnet",val:" = False"},{name:"postnet_conv_dim",val:" = 512"},{name:"postnet_conv_kernel_size",val:" = 5"},{name:"postnet_layers",val:" = 5"},{name:"postnet_dropout",val:" = 0.5"},{name:"vocab_size",val:" = 75"},{name:"num_speakers",val:" = 1"},{name:"var_pred_n_bins",val:" = 256"},{name:"var_pred_hidden_dim",val:" = 256"},{name:"var_pred_kernel_size",val:" = 3"},{name:"var_pred_dropout",val:" = 0.5"},{name:"pitch_max",val:" = 5.733940816898645"},{name:"pitch_min",val:" = -4.660287183665281"},{name:"energy_max",val:" = 3.2244551181793213"},{name:"energy_min",val:" = -4.9544901847839355"},{name:"initializer_range",val:" = 0.0625"},{name:"mean",val:" = True"},{name:"std",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/configuration_fastspeech2.py#L29",parametersDescription:[{anchor:"transformers.FastSpeech2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Vocabulary size of the FastSpeech2 model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model">~FastSpeech2Model</a> or <code>~TFFastSpeech2Model</code>.`,name:"vocab_size"},{anchor:"transformers.FastSpeech2Config.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the FastSpeech2 encoder.`,name:"encoder_layers"},{anchor:"transformers.FastSpeech2Config.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the FastSpeech2 decoder.`,name:"decoder_layers"},{anchor:"transformers.FastSpeech2Config.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the FastSpeech2 encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the FastSpeech2 decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FastSpeech2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FastSpeech2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FastSpeech2Config.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_source_positions"},{anchor:"transformers.FastSpeech2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
Example &#x2014;`,name:"initializer_range"}]}}),te=new on({props:{code:`from transformers import FastSpeech2Model, FastSpeech2Config

# Initializing a FastSpeech2 fastspeech2 style configuration
configuration = FastSpeech2Config()

# Initializing a model from the fastspeech2 style configuration
model = FastSpeech2Model(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FastSpeech2Model, FastSpeech2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FastSpeech2 fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FastSpeech2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FastSpeech2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),ae=new Re({}),ne=new B({props:{name:"class transformers.FastSpeech2Tokenizer",anchor:"transformers.FastSpeech2Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"do_phonemize",val:" = True"},{name:"preserve_punctuation",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L40",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"}]}}),se=new B({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2876",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The model input with special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),oe=new B({props:{name:"get_special_tokens_mask",anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List"},{name:"token_ids_1",val:": typing.Optional[typing.List] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils.py#L842",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p>A list of integers in the range [0, 1]</p>
`}}),re=new B({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2856",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ie=new B({props:{name:"save_vocabulary",anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L124",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
The directory in which to save the vocabulary.`,name:"save_directory"}],returnDescription:`
<p>Paths to the files saved.</p>
`,returnType:`
<p><code>Tuple(str)</code></p>
`}}),ce=new Re({}),le=new B({props:{name:"class transformers.FastSpeech2Model",anchor:"transformers.FastSpeech2Model",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L638"}}),pe=new B({props:{name:"forward",anchor:"transformers.FastSpeech2Model.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"speaker",val:": typing.Optional[torch.Tensor] = None"},{name:"durations",val:": typing.Optional[torch.Tensor] = None"},{name:"pitches",val:": typing.Optional[torch.Tensor] = None"},{name:"energies",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L679"}}),{c(){w=n("meta"),Ve=l(),T=n("h1"),L=n("a"),we=n("span"),f(W.$$.fragment),ht=l(),Te=n("span"),mt=i("FastSpeech2"),Be=l(),E=n("h2"),N=n("a"),Ee=n("span"),f(Q.$$.fragment),ft=l(),xe=n("span"),ut=i("Overview"),We=l(),I=n("p"),gt=i("The FastSpeech2 model was proposed in "),J=n("a"),_t=i("FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),vt=i("  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),Qe=l(),de=n("p"),yt=i("The abstract from the paper is the following:"),Je=l(),he=n("p"),U=n("em"),kt=i("Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),G=n("a"),bt=i("https://speechresearch.github.io/fastspeech2/"),St=i("."),Ue=l(),S=n("p"),Ft=i("This model was contributed by "),K=n("a"),$t=i("jaketae"),wt=i(". The original code can be found "),X=n("a"),Tt=i("here"),Et=i("."),Ge=l(),x=n("h2"),M=n("a"),ze=n("span"),f(Y.$$.fragment),xt=l(),Ce=n("span"),zt=i("FastSpeech2Config"),Ke=l(),k=n("div"),f(Z.$$.fragment),Ct=l(),z=n("p"),Pt=i("This is the configuration class to store the configuration of a "),me=n("a"),qt=i("FastSpeech2Model"),At=i(`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),ee=n("a"),Dt=i("fastspeech2"),Lt=i(" architecture."),Nt=l(),C=n("p"),It=i("Configuration objects inherit from "),fe=n("a"),Mt=i("PretrainedConfig"),jt=i(` and can be used to control the model outputs. Read the
documentation from `),ue=n("a"),Ot=i("PretrainedConfig"),Ht=i(" for more information."),Rt=l(),f(te.$$.fragment),Xe=l(),P=n("h2"),j=n("a"),Pe=n("span"),f(ae.$$.fragment),Vt=l(),qe=n("span"),Bt=i("FastSpeech2Tokenizer"),Ye=l(),m=n("div"),f(ne.$$.fragment),Wt=l(),Ae=n("p"),Qt=i("Construct a FastSpeech2 tokenizer. Based on byte-level Byte-Pair-Encoding."),Jt=l(),F=n("div"),f(se.$$.fragment),Ut=l(),De=n("p"),Gt=i(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),Kt=l(),Le=n("p"),Xt=i("This implementation does not add special tokens and this method should be overridden in a subclass."),Yt=l(),O=n("div"),f(oe.$$.fragment),Zt=l(),q=n("p"),ea=i(`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ne=n("code"),ta=i("prepare_for_model"),aa=i(" or "),Ie=n("code"),na=i("encode_plus"),sa=i(" methods."),oa=l(),$=n("div"),f(re.$$.fragment),ra=l(),ge=n("p"),ia=i("Create the token type IDs corresponding to the sequences passed. "),_e=n("a"),ca=i(`What are token type
IDs?`),la=l(),Me=n("p"),pa=i("Should be overridden in a subclass if the model has a special way of building those."),da=l(),H=n("div"),f(ie.$$.fragment),ha=l(),je=n("p"),ma=i("Save the vocabulary and special tokens file to a directory."),Ze=l(),A=n("h2"),R=n("a"),Oe=n("span"),f(ce.$$.fragment),fa=l(),He=n("span"),ua=i("FastSpeech2Model"),et=l(),D=n("div"),f(le.$$.fragment),ga=l(),ve=n("div"),f(pe.$$.fragment),this.h()},l(e){const d=an('[data-svelte="svelte-1phssyn"]',document.head);w=s(d,"META",{name:!0,content:!0}),d.forEach(a),Ve=p(e),T=s(e,"H1",{class:!0});var at=o(T);L=s(at,"A",{id:!0,class:!0,href:!0});var ya=o(L);we=s(ya,"SPAN",{});var ka=o(we);u(W.$$.fragment,ka),ka.forEach(a),ya.forEach(a),ht=p(at),Te=s(at,"SPAN",{});var ba=o(Te);mt=c(ba,"FastSpeech2"),ba.forEach(a),at.forEach(a),Be=p(e),E=s(e,"H2",{class:!0});var nt=o(E);N=s(nt,"A",{id:!0,class:!0,href:!0});var Sa=o(N);Ee=s(Sa,"SPAN",{});var Fa=o(Ee);u(Q.$$.fragment,Fa),Fa.forEach(a),Sa.forEach(a),ft=p(nt),xe=s(nt,"SPAN",{});var $a=o(xe);ut=c($a,"Overview"),$a.forEach(a),nt.forEach(a),We=p(e),I=s(e,"P",{});var st=o(I);gt=c(st,"The FastSpeech2 model was proposed in "),J=s(st,"A",{href:!0,rel:!0});var wa=o(J);_t=c(wa,"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),wa.forEach(a),vt=c(st,"  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),st.forEach(a),Qe=p(e),de=s(e,"P",{});var Ta=o(de);yt=c(Ta,"The abstract from the paper is the following:"),Ta.forEach(a),Je=p(e),he=s(e,"P",{});var Ea=o(he);U=s(Ea,"EM",{});var ot=o(U);kt=c(ot,"Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),G=s(ot,"A",{href:!0,rel:!0});var xa=o(G);bt=c(xa,"https://speechresearch.github.io/fastspeech2/"),xa.forEach(a),St=c(ot,"."),ot.forEach(a),Ea.forEach(a),Ue=p(e),S=s(e,"P",{});var ye=o(S);Ft=c(ye,"This model was contributed by "),K=s(ye,"A",{href:!0,rel:!0});var za=o(K);$t=c(za,"jaketae"),za.forEach(a),wt=c(ye,". The original code can be found "),X=s(ye,"A",{href:!0,rel:!0});var Ca=o(X);Tt=c(Ca,"here"),Ca.forEach(a),Et=c(ye,"."),ye.forEach(a),Ge=p(e),x=s(e,"H2",{class:!0});var rt=o(x);M=s(rt,"A",{id:!0,class:!0,href:!0});var Pa=o(M);ze=s(Pa,"SPAN",{});var qa=o(ze);u(Y.$$.fragment,qa),qa.forEach(a),Pa.forEach(a),xt=p(rt),Ce=s(rt,"SPAN",{});var Aa=o(Ce);zt=c(Aa,"FastSpeech2Config"),Aa.forEach(a),rt.forEach(a),Ke=p(e),k=s(e,"DIV",{class:!0});var V=o(k);u(Z.$$.fragment,V),Ct=p(V),z=s(V,"P",{});var ke=o(z);Pt=c(ke,"This is the configuration class to store the configuration of a "),me=s(ke,"A",{href:!0});var Da=o(me);qt=c(Da,"FastSpeech2Model"),Da.forEach(a),At=c(ke,`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),ee=s(ke,"A",{href:!0,rel:!0});var La=o(ee);Dt=c(La,"fastspeech2"),La.forEach(a),Lt=c(ke," architecture."),ke.forEach(a),Nt=p(V),C=s(V,"P",{});var be=o(C);It=c(be,"Configuration objects inherit from "),fe=s(be,"A",{href:!0});var Na=o(fe);Mt=c(Na,"PretrainedConfig"),Na.forEach(a),jt=c(be,` and can be used to control the model outputs. Read the
documentation from `),ue=s(be,"A",{href:!0});var Ia=o(ue);Ot=c(Ia,"PretrainedConfig"),Ia.forEach(a),Ht=c(be," for more information."),be.forEach(a),Rt=p(V),u(te.$$.fragment,V),V.forEach(a),Xe=p(e),P=s(e,"H2",{class:!0});var it=o(P);j=s(it,"A",{id:!0,class:!0,href:!0});var Ma=o(j);Pe=s(Ma,"SPAN",{});var ja=o(Pe);u(ae.$$.fragment,ja),ja.forEach(a),Ma.forEach(a),Vt=p(it),qe=s(it,"SPAN",{});var Oa=o(qe);Bt=c(Oa,"FastSpeech2Tokenizer"),Oa.forEach(a),it.forEach(a),Ye=p(e),m=s(e,"DIV",{class:!0});var b=o(m);u(ne.$$.fragment,b),Wt=p(b),Ae=s(b,"P",{});var Ha=o(Ae);Qt=c(Ha,"Construct a FastSpeech2 tokenizer. Based on byte-level Byte-Pair-Encoding."),Ha.forEach(a),Jt=p(b),F=s(b,"DIV",{class:!0});var Se=o(F);u(se.$$.fragment,Se),Ut=p(Se),De=s(Se,"P",{});var Ra=o(De);Gt=c(Ra,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),Ra.forEach(a),Kt=p(Se),Le=s(Se,"P",{});var Va=o(Le);Xt=c(Va,"This implementation does not add special tokens and this method should be overridden in a subclass."),Va.forEach(a),Se.forEach(a),Yt=p(b),O=s(b,"DIV",{class:!0});var ct=o(O);u(oe.$$.fragment,ct),Zt=p(ct),q=s(ct,"P",{});var Fe=o(q);ea=c(Fe,`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ne=s(Fe,"CODE",{});var Ba=o(Ne);ta=c(Ba,"prepare_for_model"),Ba.forEach(a),aa=c(Fe," or "),Ie=s(Fe,"CODE",{});var Wa=o(Ie);na=c(Wa,"encode_plus"),Wa.forEach(a),sa=c(Fe," methods."),Fe.forEach(a),ct.forEach(a),oa=p(b),$=s(b,"DIV",{class:!0});var $e=o($);u(re.$$.fragment,$e),ra=p($e),ge=s($e,"P",{});var _a=o(ge);ia=c(_a,"Create the token type IDs corresponding to the sequences passed. "),_e=s(_a,"A",{href:!0});var Qa=o(_e);ca=c(Qa,`What are token type
IDs?`),Qa.forEach(a),_a.forEach(a),la=p($e),Me=s($e,"P",{});var Ja=o(Me);pa=c(Ja,"Should be overridden in a subclass if the model has a special way of building those."),Ja.forEach(a),$e.forEach(a),da=p(b),H=s(b,"DIV",{class:!0});var lt=o(H);u(ie.$$.fragment,lt),ha=p(lt),je=s(lt,"P",{});var Ua=o(je);ma=c(Ua,"Save the vocabulary and special tokens file to a directory."),Ua.forEach(a),lt.forEach(a),b.forEach(a),Ze=p(e),A=s(e,"H2",{class:!0});var pt=o(A);R=s(pt,"A",{id:!0,class:!0,href:!0});var Ga=o(R);Oe=s(Ga,"SPAN",{});var Ka=o(Oe);u(ce.$$.fragment,Ka),Ka.forEach(a),Ga.forEach(a),fa=p(pt),He=s(pt,"SPAN",{});var Xa=o(He);ua=c(Xa,"FastSpeech2Model"),Xa.forEach(a),pt.forEach(a),et=p(e),D=s(e,"DIV",{class:!0});var dt=o(D);u(le.$$.fragment,dt),ga=p(dt),ve=s(dt,"DIV",{class:!0});var Ya=o(ve);u(pe.$$.fragment,Ya),Ya.forEach(a),dt.forEach(a),this.h()},h(){r(w,"name","hf:doc:metadata"),r(w,"content",JSON.stringify(cn)),r(L,"id","fastspeech2"),r(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(L,"href","#fastspeech2"),r(T,"class","relative group"),r(N,"id","overview"),r(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(N,"href","#overview"),r(E,"class","relative group"),r(J,"href","https://arxiv.org/abs/2006.04558"),r(J,"rel","nofollow"),r(G,"href","https://speechresearch.github.io/fastspeech2/"),r(G,"rel","nofollow"),r(K,"href","https://huggingface.co/jaketae"),r(K,"rel","nofollow"),r(X,"href","https://github.com/pytorch/fairseq"),r(X,"rel","nofollow"),r(M,"id","transformers.FastSpeech2Config"),r(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(M,"href","#transformers.FastSpeech2Config"),r(x,"class","relative group"),r(me,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),r(ee,"href","https://huggingface.co/fastspeech2"),r(ee,"rel","nofollow"),r(fe,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),r(ue,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),r(k,"class","docstring"),r(j,"id","transformers.FastSpeech2Tokenizer"),r(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(j,"href","#transformers.FastSpeech2Tokenizer"),r(P,"class","relative group"),r(F,"class","docstring"),r(O,"class","docstring"),r(_e,"href","../glossary#token-type-ids"),r($,"class","docstring"),r(H,"class","docstring"),r(m,"class","docstring"),r(R,"id","transformers.FastSpeech2Model"),r(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(R,"href","#transformers.FastSpeech2Model"),r(A,"class","relative group"),r(ve,"class","docstring"),r(D,"class","docstring")},m(e,d){t(document.head,w),h(e,Ve,d),h(e,T,d),t(T,L),t(L,we),g(W,we,null),t(T,ht),t(T,Te),t(Te,mt),h(e,Be,d),h(e,E,d),t(E,N),t(N,Ee),g(Q,Ee,null),t(E,ft),t(E,xe),t(xe,ut),h(e,We,d),h(e,I,d),t(I,gt),t(I,J),t(J,_t),t(I,vt),h(e,Qe,d),h(e,de,d),t(de,yt),h(e,Je,d),h(e,he,d),t(he,U),t(U,kt),t(U,G),t(G,bt),t(U,St),h(e,Ue,d),h(e,S,d),t(S,Ft),t(S,K),t(K,$t),t(S,wt),t(S,X),t(X,Tt),t(S,Et),h(e,Ge,d),h(e,x,d),t(x,M),t(M,ze),g(Y,ze,null),t(x,xt),t(x,Ce),t(Ce,zt),h(e,Ke,d),h(e,k,d),g(Z,k,null),t(k,Ct),t(k,z),t(z,Pt),t(z,me),t(me,qt),t(z,At),t(z,ee),t(ee,Dt),t(z,Lt),t(k,Nt),t(k,C),t(C,It),t(C,fe),t(fe,Mt),t(C,jt),t(C,ue),t(ue,Ot),t(C,Ht),t(k,Rt),g(te,k,null),h(e,Xe,d),h(e,P,d),t(P,j),t(j,Pe),g(ae,Pe,null),t(P,Vt),t(P,qe),t(qe,Bt),h(e,Ye,d),h(e,m,d),g(ne,m,null),t(m,Wt),t(m,Ae),t(Ae,Qt),t(m,Jt),t(m,F),g(se,F,null),t(F,Ut),t(F,De),t(De,Gt),t(F,Kt),t(F,Le),t(Le,Xt),t(m,Yt),t(m,O),g(oe,O,null),t(O,Zt),t(O,q),t(q,ea),t(q,Ne),t(Ne,ta),t(q,aa),t(q,Ie),t(Ie,na),t(q,sa),t(m,oa),t(m,$),g(re,$,null),t($,ra),t($,ge),t(ge,ia),t(ge,_e),t(_e,ca),t($,la),t($,Me),t(Me,pa),t(m,da),t(m,H),g(ie,H,null),t(H,ha),t(H,je),t(je,ma),h(e,Ze,d),h(e,A,d),t(A,R),t(R,Oe),g(ce,Oe,null),t(A,fa),t(A,He),t(He,ua),h(e,et,d),h(e,D,d),g(le,D,null),t(D,ga),t(D,ve),g(pe,ve,null),tt=!0},p:nn,i(e){tt||(_(W.$$.fragment,e),_(Q.$$.fragment,e),_(Y.$$.fragment,e),_(Z.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(le.$$.fragment,e),_(pe.$$.fragment,e),tt=!0)},o(e){v(W.$$.fragment,e),v(Q.$$.fragment,e),v(Y.$$.fragment,e),v(Z.$$.fragment,e),v(te.$$.fragment,e),v(ae.$$.fragment,e),v(ne.$$.fragment,e),v(se.$$.fragment,e),v(oe.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(ce.$$.fragment,e),v(le.$$.fragment,e),v(pe.$$.fragment,e),tt=!1},d(e){a(w),e&&a(Ve),e&&a(T),y(W),e&&a(Be),e&&a(E),y(Q),e&&a(We),e&&a(I),e&&a(Qe),e&&a(de),e&&a(Je),e&&a(he),e&&a(Ue),e&&a(S),e&&a(Ge),e&&a(x),y(Y),e&&a(Ke),e&&a(k),y(Z),y(te),e&&a(Xe),e&&a(P),y(ae),e&&a(Ye),e&&a(m),y(ne),y(se),y(oe),y(re),y(ie),e&&a(Ze),e&&a(A),y(ce),e&&a(et),e&&a(D),y(le),y(pe)}}}const cn={local:"fastspeech2",sections:[{local:"overview",title:"Overview"},{local:"transformers.FastSpeech2Config",title:"FastSpeech2Config"},{local:"transformers.FastSpeech2Tokenizer",title:"FastSpeech2Tokenizer"},{local:"transformers.FastSpeech2Model",title:"FastSpeech2Model"}],title:"FastSpeech2"};function ln(va){return sn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fn extends Za{constructor(w){super();en(this,w,ln,rn,tn,{})}}export{fn as default,cn as metadata};
