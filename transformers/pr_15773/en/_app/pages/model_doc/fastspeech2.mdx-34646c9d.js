import{S as Qa,i as Za,s as Ba,e as a,k as d,w as u,t as r,M as Ya,c as n,d as o,m as l,a as s,x as g,h as i,b as c,F as e,g as h,y as _,q as v,o as k,B as y,v as Ka}from"../../chunks/vendor-6b77c823.js";import{T as Ua}from"../../chunks/Tip-39098574.js";import{D as te}from"../../chunks/Docstring-90e3aa51.js";import{C as Wa}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as it}from"../../chunks/IconCopyLink-7a11ce68.js";function Xa(ct){let m,W,S,$,D;return{c(){m=a("p"),W=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),S=a("code"),$=r("Module"),D=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){m=n(w,"P",{});var z=s(m);W=i(z,"Although the recipe for forward pass needs to be defined within this function, one should call the "),S=n(z,"CODE",{});var Q=s(S);$=i(Q,"Module"),Q.forEach(o),D=i(z,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),z.forEach(o)},m(w,z){h(w,m,z),e(m,W),e(m,S),e(S,$),e(m,D)},d(w){w&&o(m)}}}function Ja(ct){let m,W,S,$,D,w,z,Q,Ct,dt,j,Z,Ve,oe,Mt,We,Pt,lt,B,qt,ae,Dt,jt,pt,$e,At,ht,xe,ne,Lt,se,It,Nt,mt,E,Ot,re,Ht,Rt,ie,Vt,Wt,ft,A,Y,Qe,ce,Qt,Ze,Zt,ut,F,de,Bt,L,Yt,ze,Kt,Ut,le,Xt,Jt,Gt,I,eo,Ee,to,oo,Ce,ao,no,so,Be,ro,io,pe,gt,N,K,Ye,he,co,Ke,lo,_t,f,me,po,Ue,ho,mo,C,fe,fo,Xe,uo,go,Je,_o,vo,U,ue,ko,O,yo,Ge,So,Fo,et,bo,wo,To,M,ge,$o,Me,xo,Pe,zo,Eo,tt,Co,Mo,X,_e,Po,ot,qo,vt,H,J,at,ve,Do,nt,jo,kt,T,ke,Ao,ye,Lo,Se,Io,No,Oo,R,Ho,qe,Ro,Vo,Fe,Wo,Qo,Zo,b,be,Bo,V,Yo,De,Ko,Uo,st,Xo,Jo,Go,G,ea,rt,ta,oa,we,yt;return w=new it({}),oe=new it({}),ce=new it({}),de=new te({props:{name:"class transformers.FastSpeech2Config",anchor:"transformers.FastSpeech2Config",parameters:[{name:"vocab_size",val:" = 75"},{name:"encoder_embed_dim",val:" = 256"},{name:"encoder_attention_heads",val:" = 2"},{name:"encoder_layers",val:" = 4"},{name:"decoder_embed_dim",val:" = 256"},{name:"decoder_attention_heads",val:" = 2"},{name:"decoder_layers",val:" = 4"},{name:"attention_dropout",val:" = 0"},{name:"fft_hidden_dim",val:" = 1024"},{name:"fft_kernel_size",val:" = 9"},{name:"fft_dropout",val:" = 0.2"},{name:"var_pred_n_bins",val:" = 256"},{name:"var_pred_hidden_dim",val:" = 256"},{name:"var_pred_kernel_size",val:" = 3"},{name:"var_pred_dropout",val:" = 0.5"},{name:"add_postnet",val:" = False"},{name:"postnet_conv_dim",val:" = 512"},{name:"postnet_conv_kernel_size",val:" = 5"},{name:"postnet_layers",val:" = 5"},{name:"postnet_dropout",val:" = 0.5"},{name:"pitch_min",val:" = -4.660287183665281"},{name:"pitch_max",val:" = 5.733940816898645"},{name:"energy_min",val:" = -4.9544901847839355"},{name:"energy_max",val:" = 3.2244551181793213"},{name:"speaker_embed_dim",val:" = 64"},{name:"num_speakers",val:" = 1"},{name:"max_source_positions",val:" = 1024"},{name:"initializer_range",val:" = 0.0625"},{name:"use_mean",val:" = True"},{name:"use_standard_deviation",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Vocabulary size of the FastSpeech2 model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model">~FastSpeech2Model</a>.`,name:"vocab_size"},{anchor:"transformers.FastSpeech2Config.encoder_embed_dim",description:`<strong>encoder_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the encoder layers.`,name:"encoder_embed_dim"},{anchor:"transformers.FastSpeech2Config.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the encoder.`,name:"encoder_layers"},{anchor:"transformers.FastSpeech2Config.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.decoder_embed_dim",description:`<strong>decoder_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the decoder layers.`,name:"decoder_embed_dim"},{anchor:"transformers.FastSpeech2Config.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the decoder.`,name:"decoder_layers"},{anchor:"transformers.FastSpeech2Config.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FastSpeech2Config.fft_hidden_dim",description:`<strong>fft_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the feed forward layers.`,name:"fft_hidden_dim"},{anchor:"transformers.FastSpeech2Config.fft_kernel_size",description:`<strong>fft_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 9) &#x2014;
Kernel size of the feed forward layers.`,name:"fft_kernel_size"},{anchor:"transformers.FastSpeech2Config.fft_dropout",description:`<strong>fft_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout ratio for the feedforward layers.`,name:"fft_dropout"},{anchor:"transformers.FastSpeech2Config.var_pred_n_bins",description:`<strong>var_pred_n_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Number of bins in the variance predictors.`,name:"var_pred_n_bins"},{anchor:"transformers.FastSpeech2Config.var_pred_hidden_dim",description:`<strong>var_pred_hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the hidden size of the variance predictor.`,name:"var_pred_hidden_dim"},{anchor:"transformers.FastSpeech2Config.var_pred_kernel_size",description:`<strong>var_pred_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Kernel size of the variance predictor layer.`,name:"var_pred_kernel_size"},{anchor:"transformers.FastSpeech2Config.var_pred_dropout",description:`<strong>var_pred_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout ratio for the variance predictor.`,name:"var_pred_dropout"},{anchor:"transformers.FastSpeech2Config.add_postnet",description:`<strong>add_postnet</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Flag that specifies whether or not to add postnet.`,name:"add_postnet"},{anchor:"transformers.FastSpeech2Config.postnet_conv_dim",description:`<strong>postnet_conv_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the postnet convolution layers.`,name:"postnet_conv_dim"},{anchor:"transformers.FastSpeech2Config.postnet_conv_kernel_size",description:`<strong>postnet_conv_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Kernel size of the convolution layers in the postnet.`,name:"postnet_conv_kernel_size"},{anchor:"transformers.FastSpeech2Config.postnet_layers",description:`<strong>postnet_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Number of hidden layers in the postnet.`,name:"postnet_layers"},{anchor:"transformers.FastSpeech2Config.postnet_dropout",description:`<strong>postnet_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The dropout ratio for the postnet.`,name:"postnet_dropout"},{anchor:"transformers.FastSpeech2Config.pitch_min",description:`<strong>pitch_min</strong> (<code>float</code>, <em>optional</em>, defaults to -4.660287183665281) &#x2014;
The minimum pitch value of the pitch bucket in the variance predictor.`,name:"pitch_min"},{anchor:"transformers.FastSpeech2Config.pitch_max",description:`<strong>pitch_max</strong> (<code>float</code>, <em>optional</em>, defaults to 5.733940816898645) &#x2014;
The maximum pitch value of the pitch bucket in the variance predictor.`,name:"pitch_max"},{anchor:"transformers.FastSpeech2Config.energy_min",description:`<strong>energy_min</strong> (<code>float</code>, <em>optional</em>, defaults to -4.9544901847839355) &#x2014;
The minimum energy value of the pitch bucket in the variance predictor.`,name:"energy_min"},{anchor:"transformers.FastSpeech2Config.energy_max",description:`<strong>energy_max</strong> (<code>float</code>, <em>optional</em>, defaults to 3.2244551181793213) &#x2014;
The maximum energy value of the pitch bucket in the variance predictor.`,name:"energy_max"},{anchor:"transformers.FastSpeech2Config.speaker_embed_dim",description:`<strong>speaker_embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the speaker identity embedding.`,name:"speaker_embed_dim"},{anchor:"transformers.FastSpeech2Config.num_speakers",description:`<strong>num_speakers</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of speakers. Set to 1 if the dataset is a single-speaker dataset. Otherwise, set to the number of speakers in the multi-speaker training dataset.`,name:"num_speakers"},{anchor:"transformers.FastSpeech2Config.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).`,name:"max_source_positions"},{anchor:"transformers.FastSpeech2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0625) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all embedding weight matrices.`,name:"initializer_range"},{anchor:"transformers.FastSpeech2Config.use_mean",description:`<strong>use_mean</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Flag that specifies whether or not to denormalize predicted output using cepstral mean and variance normalization. For more information, please refer to <a href="https://en.wikipedia.org/wiki/Cepstral_mean_and_variance_normalization" rel="nofollow">cepstral mean and variance normalization</a>.`,name:"use_mean"},{anchor:"transformers.FastSpeech2Config.use_standard_deviation",description:`<strong>use_standard_deviation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Flag that specifies whether or not to scale predicted output using cepstral mean and variance normalization. For more information, please refer to <a href="https://en.wikipedia.org/wiki/Cepstral_mean_and_variance_normalization" rel="nofollow">cepstral mean and variance normalization</a>.`,name:"use_standard_deviation"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/configuration_fastspeech2.py#L30"}}),pe=new Wa({props:{code:`from transformers import FastSpeech2Model, FastSpeech2Config

# Initializing a FastSpeech2 fastspeech2 style configuration
configuration = FastSpeech2Config()

# Initializing a model from the fastspeech2 style configuration
model = FastSpeech2Model(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FastSpeech2Model, FastSpeech2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FastSpeech2 fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FastSpeech2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FastSpeech2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),he=new it({}),me=new te({props:{name:"class transformers.FastSpeech2Tokenizer",anchor:"transformers.FastSpeech2Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"do_phonemize",val:" = True"},{name:"preserve_punctuation",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L42"}}),fe=new te({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2876",returnDescription:`
<p>The model input with special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ue=new te({props:{name:"get_special_tokens_mask",anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List"},{name:"token_ids_1",val:": typing.Optional[typing.List] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils.py#L842",returnDescription:`
<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p>A list of integers in the range [0, 1]</p>
`}}),ge=new te({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2856",returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),_e=new te({props:{name:"save_vocabulary",anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
The directory in which to save the vocabulary.`,name:"save_directory"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L126",returnDescription:`
<p>Paths to the files saved.</p>
`,returnType:`
<p><code>Tuple(str)</code></p>
`}}),ve=new it({}),ke=new te({props:{name:"class transformers.FastSpeech2Model",anchor:"transformers.FastSpeech2Model",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Config">FastSpeech2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/pr_15773/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L685"}}),be=new te({props:{name:"forward",anchor:"transformers.FastSpeech2Model.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"speaker_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"durations",val:": typing.Optional[torch.Tensor] = None"},{name:"pitches",val:": typing.Optional[torch.Tensor] = None"},{name:"energies",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FastSpeech2Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.
Indices can be obtained using <a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Tokenizer">FastSpeech2Tokenizer</a>. See <a href="/docs/transformers/pr_15773/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/pr_15773/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.
<a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FastSpeech2Model.forward.speaker_ids",description:`<strong>speaker_ids</strong> (<code>torch.LongTensor</code>, <em>optional</em>) &#x2014;
Indices of speaker ids.`,name:"speaker_ids"},{anchor:"transformers.FastSpeech2Model.forward.durations",description:`<strong>durations</strong> (<code>torch.LongTensor</code>, <em>optional</em>) &#x2014;
pass`,name:"durations"},{anchor:"transformers.FastSpeech2Model.forward.pitches",description:`<strong>pitches</strong> (<code>torch.FloatTensor</code> of shape <code>()</code>, <em>optional</em>) &#x2014;
pass`,name:"pitches"},{anchor:"transformers.FastSpeech2Model.forward.energies",description:`<strong>energies</strong> (<code>torch.FloatTensor</code> of shape <code>()</code>, <em>optional</em>) &#x2014;
pass`,name:"energies"},{anchor:"transformers.FastSpeech2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/pr_15773/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L734",returnDescription:`
<p>A <code>transformers.models.fastspeech2.modeling_fastspeech2.FastSpeech2ModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Config"
>FastSpeech2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>mel_spectrogram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, mel_s)</code>) \u2014 Total loss as the sum of the masked language modeling loss and the next sequence prediction
(classification) loss.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.
Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.fastspeech2.modeling_fastspeech2.FastSpeech2ModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new Ua({props:{$$slots:{default:[Xa]},$$scope:{ctx:ct}}}),we=new Wa({props:{code:`from transformers import FastSpeech2Tokenizer, FastSpeech2Model
import torch

tokenizer = FastSpeech2Tokenizer.from_pretrained("fastspeech2")
model = FastSpeech2Model.from_pretrained("fastspeech2")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FastSpeech2Tokenizer, FastSpeech2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FastSpeech2Tokenizer.from_pretrained(<span class="hljs-string">&quot;fastspeech2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FastSpeech2Model.from_pretrained(<span class="hljs-string">&quot;fastspeech2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){m=a("meta"),W=d(),S=a("h1"),$=a("a"),D=a("span"),u(w.$$.fragment),z=d(),Q=a("span"),Ct=r("FastSpeech2"),dt=d(),j=a("h2"),Z=a("a"),Ve=a("span"),u(oe.$$.fragment),Mt=d(),We=a("span"),Pt=r("Overview"),lt=d(),B=a("p"),qt=r("The FastSpeech2 model was proposed in "),ae=a("a"),Dt=r("FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),jt=r("  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),pt=d(),$e=a("p"),At=r("The abstract from the paper is the following:"),ht=d(),xe=a("p"),ne=a("em"),Lt=r("Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),se=a("a"),It=r("https://speechresearch.github.io/fastspeech2/"),Nt=r("."),mt=d(),E=a("p"),Ot=r("This model was contributed by "),re=a("a"),Ht=r("jaketae"),Rt=r(". The original code can be found "),ie=a("a"),Vt=r("here"),Wt=r("."),ft=d(),A=a("h2"),Y=a("a"),Qe=a("span"),u(ce.$$.fragment),Qt=d(),Ze=a("span"),Zt=r("FastSpeech2Config"),ut=d(),F=a("div"),u(de.$$.fragment),Bt=d(),L=a("p"),Yt=r("This is the configuration class to store the configuration of a "),ze=a("a"),Kt=r("FastSpeech2Model"),Ut=r(`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),le=a("a"),Xt=r("fastspeech2"),Jt=r(" architecture."),Gt=d(),I=a("p"),eo=r("Configuration objects inherit from "),Ee=a("a"),to=r("PretrainedConfig"),oo=r(` and can be used to control the model outputs. Read the
documentation from `),Ce=a("a"),ao=r("PretrainedConfig"),no=r(" for more information."),so=d(),Be=a("p"),ro=r("Example:"),io=d(),u(pe.$$.fragment),gt=d(),N=a("h2"),K=a("a"),Ye=a("span"),u(he.$$.fragment),co=d(),Ke=a("span"),lo=r("FastSpeech2Tokenizer"),_t=d(),f=a("div"),u(me.$$.fragment),po=d(),Ue=a("p"),ho=r("Construct a FastSpeech2 tokenizer."),mo=d(),C=a("div"),u(fe.$$.fragment),fo=d(),Xe=a("p"),uo=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),go=d(),Je=a("p"),_o=r("This implementation does not add special tokens and this method should be overridden in a subclass."),vo=d(),U=a("div"),u(ue.$$.fragment),ko=d(),O=a("p"),yo=r(`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ge=a("code"),So=r("prepare_for_model"),Fo=r(" or "),et=a("code"),bo=r("encode_plus"),wo=r(" methods."),To=d(),M=a("div"),u(ge.$$.fragment),$o=d(),Me=a("p"),xo=r("Create the token type IDs corresponding to the sequences passed. "),Pe=a("a"),zo=r(`What are token type
IDs?`),Eo=d(),tt=a("p"),Co=r("Should be overridden in a subclass if the model has a special way of building those."),Mo=d(),X=a("div"),u(_e.$$.fragment),Po=d(),ot=a("p"),qo=r("Save the vocabulary and special tokens file to a directory."),vt=d(),H=a("h2"),J=a("a"),at=a("span"),u(ve.$$.fragment),Do=d(),nt=a("span"),jo=r("FastSpeech2Model"),kt=d(),T=a("div"),u(ke.$$.fragment),Ao=d(),ye=a("p"),Lo=r(`The FastSpeech2 Model that outputs predicted mel-spectrograms.
FastSpeech2 was proposed in `),Se=a("a"),Io=r("FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),No=r(" by Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu."),Oo=d(),R=a("p"),Ho=r("This model inherits from "),qe=a("a"),Ro=r("PreTrainedModel"),Vo=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, etc.)
This model is also a PyTorch `),Fe=a("a"),Wo=r("torch.nn.Module"),Qo=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zo=d(),b=a("div"),u(be.$$.fragment),Bo=d(),V=a("p"),Yo=r("The "),De=a("a"),Ko=r("FastSpeech2Model"),Uo=r(" forward method, overrides the "),st=a("code"),Xo=r("__call__"),Jo=r(" special method."),Go=d(),u(G.$$.fragment),ea=d(),rt=a("p"),ta=r("Example:"),oa=d(),u(we.$$.fragment),this.h()},l(t){const p=Ya('[data-svelte="svelte-1phssyn"]',document.head);m=n(p,"META",{name:!0,content:!0}),p.forEach(o),W=l(t),S=n(t,"H1",{class:!0});var Te=s(S);$=n(Te,"A",{id:!0,class:!0,href:!0});var na=s($);D=n(na,"SPAN",{});var sa=s(D);g(w.$$.fragment,sa),sa.forEach(o),na.forEach(o),z=l(Te),Q=n(Te,"SPAN",{});var ra=s(Q);Ct=i(ra,"FastSpeech2"),ra.forEach(o),Te.forEach(o),dt=l(t),j=n(t,"H2",{class:!0});var St=s(j);Z=n(St,"A",{id:!0,class:!0,href:!0});var ia=s(Z);Ve=n(ia,"SPAN",{});var ca=s(Ve);g(oe.$$.fragment,ca),ca.forEach(o),ia.forEach(o),Mt=l(St),We=n(St,"SPAN",{});var da=s(We);Pt=i(da,"Overview"),da.forEach(o),St.forEach(o),lt=l(t),B=n(t,"P",{});var Ft=s(B);qt=i(Ft,"The FastSpeech2 model was proposed in "),ae=n(Ft,"A",{href:!0,rel:!0});var la=s(ae);Dt=i(la,"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),la.forEach(o),jt=i(Ft,"  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),Ft.forEach(o),pt=l(t),$e=n(t,"P",{});var pa=s($e);At=i(pa,"The abstract from the paper is the following:"),pa.forEach(o),ht=l(t),xe=n(t,"P",{});var ha=s(xe);ne=n(ha,"EM",{});var bt=s(ne);Lt=i(bt,"Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),se=n(bt,"A",{href:!0,rel:!0});var ma=s(se);It=i(ma,"https://speechresearch.github.io/fastspeech2/"),ma.forEach(o),Nt=i(bt,"."),bt.forEach(o),ha.forEach(o),mt=l(t),E=n(t,"P",{});var je=s(E);Ot=i(je,"This model was contributed by "),re=n(je,"A",{href:!0,rel:!0});var fa=s(re);Ht=i(fa,"jaketae"),fa.forEach(o),Rt=i(je,". The original code can be found "),ie=n(je,"A",{href:!0,rel:!0});var ua=s(ie);Vt=i(ua,"here"),ua.forEach(o),Wt=i(je,"."),je.forEach(o),ft=l(t),A=n(t,"H2",{class:!0});var wt=s(A);Y=n(wt,"A",{id:!0,class:!0,href:!0});var ga=s(Y);Qe=n(ga,"SPAN",{});var _a=s(Qe);g(ce.$$.fragment,_a),_a.forEach(o),ga.forEach(o),Qt=l(wt),Ze=n(wt,"SPAN",{});var va=s(Ze);Zt=i(va,"FastSpeech2Config"),va.forEach(o),wt.forEach(o),ut=l(t),F=n(t,"DIV",{class:!0});var P=s(F);g(de.$$.fragment,P),Bt=l(P),L=n(P,"P",{});var Ae=s(L);Yt=i(Ae,"This is the configuration class to store the configuration of a "),ze=n(Ae,"A",{href:!0});var ka=s(ze);Kt=i(ka,"FastSpeech2Model"),ka.forEach(o),Ut=i(Ae,`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),le=n(Ae,"A",{href:!0,rel:!0});var ya=s(le);Xt=i(ya,"fastspeech2"),ya.forEach(o),Jt=i(Ae," architecture."),Ae.forEach(o),Gt=l(P),I=n(P,"P",{});var Le=s(I);eo=i(Le,"Configuration objects inherit from "),Ee=n(Le,"A",{href:!0});var Sa=s(Ee);to=i(Sa,"PretrainedConfig"),Sa.forEach(o),oo=i(Le,` and can be used to control the model outputs. Read the
documentation from `),Ce=n(Le,"A",{href:!0});var Fa=s(Ce);ao=i(Fa,"PretrainedConfig"),Fa.forEach(o),no=i(Le," for more information."),Le.forEach(o),so=l(P),Be=n(P,"P",{});var ba=s(Be);ro=i(ba,"Example:"),ba.forEach(o),io=l(P),g(pe.$$.fragment,P),P.forEach(o),gt=l(t),N=n(t,"H2",{class:!0});var Tt=s(N);K=n(Tt,"A",{id:!0,class:!0,href:!0});var wa=s(K);Ye=n(wa,"SPAN",{});var Ta=s(Ye);g(he.$$.fragment,Ta),Ta.forEach(o),wa.forEach(o),co=l(Tt),Ke=n(Tt,"SPAN",{});var $a=s(Ke);lo=i($a,"FastSpeech2Tokenizer"),$a.forEach(o),Tt.forEach(o),_t=l(t),f=n(t,"DIV",{class:!0});var x=s(f);g(me.$$.fragment,x),po=l(x),Ue=n(x,"P",{});var xa=s(Ue);ho=i(xa,"Construct a FastSpeech2 tokenizer."),xa.forEach(o),mo=l(x),C=n(x,"DIV",{class:!0});var Ie=s(C);g(fe.$$.fragment,Ie),fo=l(Ie),Xe=n(Ie,"P",{});var za=s(Xe);uo=i(za,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),za.forEach(o),go=l(Ie),Je=n(Ie,"P",{});var Ea=s(Je);_o=i(Ea,"This implementation does not add special tokens and this method should be overridden in a subclass."),Ea.forEach(o),Ie.forEach(o),vo=l(x),U=n(x,"DIV",{class:!0});var $t=s(U);g(ue.$$.fragment,$t),ko=l($t),O=n($t,"P",{});var Ne=s(O);yo=i(Ne,`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Ge=n(Ne,"CODE",{});var Ca=s(Ge);So=i(Ca,"prepare_for_model"),Ca.forEach(o),Fo=i(Ne," or "),et=n(Ne,"CODE",{});var Ma=s(et);bo=i(Ma,"encode_plus"),Ma.forEach(o),wo=i(Ne," methods."),Ne.forEach(o),$t.forEach(o),To=l(x),M=n(x,"DIV",{class:!0});var Oe=s(M);g(ge.$$.fragment,Oe),$o=l(Oe),Me=n(Oe,"P",{});var aa=s(Me);xo=i(aa,"Create the token type IDs corresponding to the sequences passed. "),Pe=n(aa,"A",{href:!0});var Pa=s(Pe);zo=i(Pa,`What are token type
IDs?`),Pa.forEach(o),aa.forEach(o),Eo=l(Oe),tt=n(Oe,"P",{});var qa=s(tt);Co=i(qa,"Should be overridden in a subclass if the model has a special way of building those."),qa.forEach(o),Oe.forEach(o),Mo=l(x),X=n(x,"DIV",{class:!0});var xt=s(X);g(_e.$$.fragment,xt),Po=l(xt),ot=n(xt,"P",{});var Da=s(ot);qo=i(Da,"Save the vocabulary and special tokens file to a directory."),Da.forEach(o),xt.forEach(o),x.forEach(o),vt=l(t),H=n(t,"H2",{class:!0});var zt=s(H);J=n(zt,"A",{id:!0,class:!0,href:!0});var ja=s(J);at=n(ja,"SPAN",{});var Aa=s(at);g(ve.$$.fragment,Aa),Aa.forEach(o),ja.forEach(o),Do=l(zt),nt=n(zt,"SPAN",{});var La=s(nt);jo=i(La,"FastSpeech2Model"),La.forEach(o),zt.forEach(o),kt=l(t),T=n(t,"DIV",{class:!0});var ee=s(T);g(ke.$$.fragment,ee),Ao=l(ee),ye=n(ee,"P",{});var Et=s(ye);Lo=i(Et,`The FastSpeech2 Model that outputs predicted mel-spectrograms.
FastSpeech2 was proposed in `),Se=n(Et,"A",{href:!0,rel:!0});var Ia=s(Se);Io=i(Ia,"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),Ia.forEach(o),No=i(Et," by Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, Tie-Yan Liu."),Et.forEach(o),Oo=l(ee),R=n(ee,"P",{});var He=s(R);Ho=i(He,"This model inherits from "),qe=n(He,"A",{href:!0});var Na=s(qe);Ro=i(Na,"PreTrainedModel"),Na.forEach(o),Vo=i(He,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, etc.)
This model is also a PyTorch `),Fe=n(He,"A",{href:!0,rel:!0});var Oa=s(Fe);Wo=i(Oa,"torch.nn.Module"),Oa.forEach(o),Qo=i(He,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),He.forEach(o),Zo=l(ee),b=n(ee,"DIV",{class:!0});var q=s(b);g(be.$$.fragment,q),Bo=l(q),V=n(q,"P",{});var Re=s(V);Yo=i(Re,"The "),De=n(Re,"A",{href:!0});var Ha=s(De);Ko=i(Ha,"FastSpeech2Model"),Ha.forEach(o),Uo=i(Re," forward method, overrides the "),st=n(Re,"CODE",{});var Ra=s(st);Xo=i(Ra,"__call__"),Ra.forEach(o),Jo=i(Re," special method."),Re.forEach(o),Go=l(q),g(G.$$.fragment,q),ea=l(q),rt=n(q,"P",{});var Va=s(rt);ta=i(Va,"Example:"),Va.forEach(o),oa=l(q),g(we.$$.fragment,q),q.forEach(o),ee.forEach(o),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(Ga)),c($,"id","fastspeech2"),c($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($,"href","#fastspeech2"),c(S,"class","relative group"),c(Z,"id","overview"),c(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z,"href","#overview"),c(j,"class","relative group"),c(ae,"href","https://arxiv.org/abs/2006.04558"),c(ae,"rel","nofollow"),c(se,"href","https://speechresearch.github.io/fastspeech2/"),c(se,"rel","nofollow"),c(re,"href","https://huggingface.co/jaketae"),c(re,"rel","nofollow"),c(ie,"href","https://github.com/pytorch/fairseq"),c(ie,"rel","nofollow"),c(Y,"id","transformers.FastSpeech2Config"),c(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y,"href","#transformers.FastSpeech2Config"),c(A,"class","relative group"),c(ze,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),c(le,"href","https://huggingface.co/jaketae/fastspeech2"),c(le,"rel","nofollow"),c(Ee,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ce,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),c(F,"class","docstring"),c(K,"id","transformers.FastSpeech2Tokenizer"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#transformers.FastSpeech2Tokenizer"),c(N,"class","relative group"),c(C,"class","docstring"),c(U,"class","docstring"),c(Pe,"href","../glossary#token-type-ids"),c(M,"class","docstring"),c(X,"class","docstring"),c(f,"class","docstring"),c(J,"id","transformers.FastSpeech2Model"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#transformers.FastSpeech2Model"),c(H,"class","relative group"),c(Se,"href","https://arxiv.org/abs/2006.04558"),c(Se,"rel","nofollow"),c(qe,"href","/docs/transformers/pr_15773/en/main_classes/model#transformers.PreTrainedModel"),c(Fe,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Fe,"rel","nofollow"),c(De,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),c(b,"class","docstring"),c(T,"class","docstring")},m(t,p){e(document.head,m),h(t,W,p),h(t,S,p),e(S,$),e($,D),_(w,D,null),e(S,z),e(S,Q),e(Q,Ct),h(t,dt,p),h(t,j,p),e(j,Z),e(Z,Ve),_(oe,Ve,null),e(j,Mt),e(j,We),e(We,Pt),h(t,lt,p),h(t,B,p),e(B,qt),e(B,ae),e(ae,Dt),e(B,jt),h(t,pt,p),h(t,$e,p),e($e,At),h(t,ht,p),h(t,xe,p),e(xe,ne),e(ne,Lt),e(ne,se),e(se,It),e(ne,Nt),h(t,mt,p),h(t,E,p),e(E,Ot),e(E,re),e(re,Ht),e(E,Rt),e(E,ie),e(ie,Vt),e(E,Wt),h(t,ft,p),h(t,A,p),e(A,Y),e(Y,Qe),_(ce,Qe,null),e(A,Qt),e(A,Ze),e(Ze,Zt),h(t,ut,p),h(t,F,p),_(de,F,null),e(F,Bt),e(F,L),e(L,Yt),e(L,ze),e(ze,Kt),e(L,Ut),e(L,le),e(le,Xt),e(L,Jt),e(F,Gt),e(F,I),e(I,eo),e(I,Ee),e(Ee,to),e(I,oo),e(I,Ce),e(Ce,ao),e(I,no),e(F,so),e(F,Be),e(Be,ro),e(F,io),_(pe,F,null),h(t,gt,p),h(t,N,p),e(N,K),e(K,Ye),_(he,Ye,null),e(N,co),e(N,Ke),e(Ke,lo),h(t,_t,p),h(t,f,p),_(me,f,null),e(f,po),e(f,Ue),e(Ue,ho),e(f,mo),e(f,C),_(fe,C,null),e(C,fo),e(C,Xe),e(Xe,uo),e(C,go),e(C,Je),e(Je,_o),e(f,vo),e(f,U),_(ue,U,null),e(U,ko),e(U,O),e(O,yo),e(O,Ge),e(Ge,So),e(O,Fo),e(O,et),e(et,bo),e(O,wo),e(f,To),e(f,M),_(ge,M,null),e(M,$o),e(M,Me),e(Me,xo),e(Me,Pe),e(Pe,zo),e(M,Eo),e(M,tt),e(tt,Co),e(f,Mo),e(f,X),_(_e,X,null),e(X,Po),e(X,ot),e(ot,qo),h(t,vt,p),h(t,H,p),e(H,J),e(J,at),_(ve,at,null),e(H,Do),e(H,nt),e(nt,jo),h(t,kt,p),h(t,T,p),_(ke,T,null),e(T,Ao),e(T,ye),e(ye,Lo),e(ye,Se),e(Se,Io),e(ye,No),e(T,Oo),e(T,R),e(R,Ho),e(R,qe),e(qe,Ro),e(R,Vo),e(R,Fe),e(Fe,Wo),e(R,Qo),e(T,Zo),e(T,b),_(be,b,null),e(b,Bo),e(b,V),e(V,Yo),e(V,De),e(De,Ko),e(V,Uo),e(V,st),e(st,Xo),e(V,Jo),e(b,Go),_(G,b,null),e(b,ea),e(b,rt),e(rt,ta),e(b,oa),_(we,b,null),yt=!0},p(t,[p]){const Te={};p&2&&(Te.$$scope={dirty:p,ctx:t}),G.$set(Te)},i(t){yt||(v(w.$$.fragment,t),v(oe.$$.fragment,t),v(ce.$$.fragment,t),v(de.$$.fragment,t),v(pe.$$.fragment,t),v(he.$$.fragment,t),v(me.$$.fragment,t),v(fe.$$.fragment,t),v(ue.$$.fragment,t),v(ge.$$.fragment,t),v(_e.$$.fragment,t),v(ve.$$.fragment,t),v(ke.$$.fragment,t),v(be.$$.fragment,t),v(G.$$.fragment,t),v(we.$$.fragment,t),yt=!0)},o(t){k(w.$$.fragment,t),k(oe.$$.fragment,t),k(ce.$$.fragment,t),k(de.$$.fragment,t),k(pe.$$.fragment,t),k(he.$$.fragment,t),k(me.$$.fragment,t),k(fe.$$.fragment,t),k(ue.$$.fragment,t),k(ge.$$.fragment,t),k(_e.$$.fragment,t),k(ve.$$.fragment,t),k(ke.$$.fragment,t),k(be.$$.fragment,t),k(G.$$.fragment,t),k(we.$$.fragment,t),yt=!1},d(t){o(m),t&&o(W),t&&o(S),y(w),t&&o(dt),t&&o(j),y(oe),t&&o(lt),t&&o(B),t&&o(pt),t&&o($e),t&&o(ht),t&&o(xe),t&&o(mt),t&&o(E),t&&o(ft),t&&o(A),y(ce),t&&o(ut),t&&o(F),y(de),y(pe),t&&o(gt),t&&o(N),y(he),t&&o(_t),t&&o(f),y(me),y(fe),y(ue),y(ge),y(_e),t&&o(vt),t&&o(H),y(ve),t&&o(kt),t&&o(T),y(ke),y(be),y(G),y(we)}}}const Ga={local:"fastspeech2",sections:[{local:"overview",title:"Overview"},{local:"transformers.FastSpeech2Config",title:"FastSpeech2Config"},{local:"transformers.FastSpeech2Tokenizer",title:"FastSpeech2Tokenizer"},{local:"transformers.FastSpeech2Model",title:"FastSpeech2Model"}],title:"FastSpeech2"};function en(ct){return Ka(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class rn extends Qa{constructor(m){super();Za(this,m,en,Ja,Ba,{})}}export{rn as default,Ga as metadata};
