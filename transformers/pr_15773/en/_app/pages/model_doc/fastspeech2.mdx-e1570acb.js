import{S as Xa,i as Ya,s as Za,e as n,k as l,w as f,t as i,M as en,c as s,d as a,m as p,a as o,x as u,h as c,b as r,F as t,g as h,y as g,L as tn,q as _,o as v,B as y,v as an}from"../../chunks/vendor-6b77c823.js";import{D as W}from"../../chunks/Docstring-abef54e3.js";import{C as nn}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as He}from"../../chunks/IconCopyLink-7a11ce68.js";function sn(_a){let w,Re,T,L,$e,Q,dt,we,ht,Ve,E,N,Te,J,mt,Ee,ft,Be,I,ut,U,gt,_t,We,de,vt,Qe,j,yt,G,kt,bt,Je,S,St,K,Ft,$t,X,wt,Tt,Ue,x,M,xe,Y,Et,ze,xt,Ge,k,Z,zt,z,Ct,he,Pt,qt,ee,At,Dt,Lt,C,Nt,me,It,jt,fe,Mt,Ot,Ht,te,Ke,P,O,Ce,ae,Rt,Pe,Vt,Xe,m,ne,Bt,qe,Wt,Qt,F,se,Jt,Ae,Ut,Gt,De,Kt,Xt,H,oe,Yt,q,Zt,Le,ea,ta,Ne,aa,na,sa,$,re,oa,ue,ra,ge,ia,ca,Ie,la,pa,R,ie,da,je,ha,Ye,A,V,Me,ce,ma,Oe,fa,Ze,D,le,ua,_e,pe,et;return Q=new He({}),J=new He({}),Y=new He({}),Z=new W({props:{name:"class transformers.FastSpeech2Config",anchor:"transformers.FastSpeech2Config",parameters:[{name:"n_frames_per_step",val:" = 1"},{name:"output_frame_dim",val:" = 80"},{name:"encoder_embed_dim",val:" = 256"},{name:"speaker_embed_dim",val:" = 64"},{name:"dropout",val:" = 0.2"},{name:"max_source_positions",val:" = 1024"},{name:"encoder_attention_heads",val:" = 2"},{name:"fft_hidden_dim",val:" = 1024"},{name:"fft_kernel_size",val:" = 9"},{name:"attention_dropout",val:" = 0"},{name:"encoder_layers",val:" = 4"},{name:"decoder_embed_dim",val:" = 256"},{name:"decoder_attention_heads",val:" = 2"},{name:"decoder_layers",val:" = 4"},{name:"add_postnet",val:" = False"},{name:"postnet_conv_dim",val:" = 512"},{name:"postnet_conv_kernel_size",val:" = 5"},{name:"postnet_layers",val:" = 5"},{name:"postnet_dropout",val:" = 0.5"},{name:"vocab_size",val:" = 75"},{name:"num_speakers",val:" = 1"},{name:"var_pred_n_bins",val:" = 256"},{name:"var_pred_hidden_dim",val:" = 256"},{name:"var_pred_kernel_size",val:" = 3"},{name:"var_pred_dropout",val:" = 0.5"},{name:"pitch_max",val:" = 5.733940816898645"},{name:"pitch_min",val:" = -4.660287183665281"},{name:"energy_max",val:" = 3.2244551181793213"},{name:"energy_min",val:" = -4.9544901847839355"},{name:"initializer_range",val:" = 0.0625"},{name:"mean",val:" = True"},{name:"std",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/configuration_fastspeech2.py#L29",parametersDescription:[{anchor:"transformers.FastSpeech2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Vocabulary size of the FastSpeech2 model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model">~FastSpeech2Model</a> or <code>~TFFastSpeech2Model</code>.`,name:"vocab_size"},{anchor:"transformers.FastSpeech2Config.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the FastSpeech2 encoder.`,name:"encoder_layers"},{anchor:"transformers.FastSpeech2Config.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of hidden layers in the FastSpeech2 decoder.`,name:"decoder_layers"},{anchor:"transformers.FastSpeech2Config.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the FastSpeech2 encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads for each attention layer in the FastSpeech2 decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FastSpeech2Config.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FastSpeech2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FastSpeech2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FastSpeech2Config.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_source_positions"},{anchor:"transformers.FastSpeech2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
Example &#x2014;`,name:"initializer_range"}]}}),te=new nn({props:{code:`from transformers import FastSpeech2Model, FastSpeech2Config

# Initializing a FastSpeech2 fastspeech2 style configuration
configuration = FastSpeech2Config()

# Initializing a model from the fastspeech2 style configuration
model = FastSpeech2Model(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FastSpeech2Model, FastSpeech2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FastSpeech2 fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FastSpeech2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the fastspeech2 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FastSpeech2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),ae=new He({}),ne=new W({props:{name:"class transformers.FastSpeech2Tokenizer",anchor:"transformers.FastSpeech2Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"do_phonemize",val:" = True"},{name:"preserve_punctuation",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L40",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"}]}}),se=new W({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2876",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The model input with special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),oe=new W({props:{name:"get_special_tokens_mask",anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List"},{name:"token_ids_1",val:": typing.Optional[typing.List] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils.py#L842",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.FastSpeech2Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p>A list of integers in the range [0, 1]</p>
`}}),re=new W({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/tokenization_utils_base.py#L2856",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.FastSpeech2Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ie=new W({props:{name:"save_vocabulary",anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/tokenization_fastspeech2.py#L124",parametersDescription:[{anchor:"transformers.FastSpeech2Tokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
The directory in which to save the vocabulary.`,name:"save_directory"}],returnDescription:`
<p>Paths to the files saved.</p>
`,returnType:`
<p><code>Tuple(str)</code></p>
`}}),ce=new He({}),le=new W({props:{name:"class transformers.FastSpeech2Model",anchor:"transformers.FastSpeech2Model",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L639"}}),pe=new W({props:{name:"forward",anchor:"transformers.FastSpeech2Model.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"speaker",val:": typing.Optional[torch.Tensor] = None"},{name:"durations",val:": typing.Optional[torch.Tensor] = None"},{name:"pitches",val:": typing.Optional[torch.Tensor] = None"},{name:"energies",val:": typing.Optional[torch.Tensor] = None"},{name:"return_dict",val:": typing.Optional[bool] = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/pr_15773/src/transformers/models/fastspeech2/modeling_fastspeech2.py#L680"}}),{c(){w=n("meta"),Re=l(),T=n("h1"),L=n("a"),$e=n("span"),f(Q.$$.fragment),dt=l(),we=n("span"),ht=i("FastSpeech2"),Ve=l(),E=n("h2"),N=n("a"),Te=n("span"),f(J.$$.fragment),mt=l(),Ee=n("span"),ft=i("Overview"),Be=l(),I=n("p"),ut=i("The FastSpeech2 model was proposed in "),U=n("a"),gt=i("FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),_t=i("  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),We=l(),de=n("p"),vt=i("The abstract from the paper is the following:"),Qe=l(),j=n("p"),yt=i("Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),G=n("a"),kt=i("https://speechresearch.github.io/fastspeech2/"),bt=i("."),Je=l(),S=n("p"),St=i("This model was contributed by "),K=n("a"),Ft=i("jaketae"),$t=i(". The original code can be found "),X=n("a"),wt=i("here"),Tt=i("."),Ue=l(),x=n("h2"),M=n("a"),xe=n("span"),f(Y.$$.fragment),Et=l(),ze=n("span"),xt=i("FastSpeech2Config"),Ge=l(),k=n("div"),f(Z.$$.fragment),zt=l(),z=n("p"),Ct=i("This is the configuration class to store the configuration of a "),he=n("a"),Pt=i("~FastSpeech2Model"),qt=i(`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),ee=n("a"),At=i("fastspeech2"),Dt=i(" architecture."),Lt=l(),C=n("p"),Nt=i("Configuration objects inherit from "),me=n("a"),It=i("PretrainedConfig"),jt=i(` and can be used to control the model outputs. Read the
documentation from `),fe=n("a"),Mt=i("PretrainedConfig"),Ot=i(" for more information."),Ht=l(),f(te.$$.fragment),Ke=l(),P=n("h2"),O=n("a"),Ce=n("span"),f(ae.$$.fragment),Rt=l(),Pe=n("span"),Vt=i("FastSpeech2Tokenizer"),Xe=l(),m=n("div"),f(ne.$$.fragment),Bt=l(),qe=n("p"),Wt=i("Construct a FastSpeech2 tokenizer. Based on byte-level Byte-Pair-Encoding."),Qt=l(),F=n("div"),f(se.$$.fragment),Jt=l(),Ae=n("p"),Ut=i(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),Gt=l(),De=n("p"),Kt=i("This implementation does not add special tokens and this method should be overridden in a subclass."),Xt=l(),H=n("div"),f(oe.$$.fragment),Yt=l(),q=n("p"),Zt=i(`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Le=n("code"),ea=i("prepare_for_model"),ta=i(" or "),Ne=n("code"),aa=i("encode_plus"),na=i(" methods."),sa=l(),$=n("div"),f(re.$$.fragment),oa=l(),ue=n("p"),ra=i("Create the token type IDs corresponding to the sequences passed. "),ge=n("a"),ia=i(`What are token type
IDs?`),ca=l(),Ie=n("p"),la=i("Should be overridden in a subclass if the model has a special way of building those."),pa=l(),R=n("div"),f(ie.$$.fragment),da=l(),je=n("p"),ha=i("Save the vocabulary and special tokens file to a directory."),Ye=l(),A=n("h2"),V=n("a"),Me=n("span"),f(ce.$$.fragment),ma=l(),Oe=n("span"),fa=i("FastSpeech2Model"),Ze=l(),D=n("div"),f(le.$$.fragment),ua=l(),_e=n("div"),f(pe.$$.fragment),this.h()},l(e){const d=en('[data-svelte="svelte-1phssyn"]',document.head);w=s(d,"META",{name:!0,content:!0}),d.forEach(a),Re=p(e),T=s(e,"H1",{class:!0});var tt=o(T);L=s(tt,"A",{id:!0,class:!0,href:!0});var va=o(L);$e=s(va,"SPAN",{});var ya=o($e);u(Q.$$.fragment,ya),ya.forEach(a),va.forEach(a),dt=p(tt),we=s(tt,"SPAN",{});var ka=o(we);ht=c(ka,"FastSpeech2"),ka.forEach(a),tt.forEach(a),Ve=p(e),E=s(e,"H2",{class:!0});var at=o(E);N=s(at,"A",{id:!0,class:!0,href:!0});var ba=o(N);Te=s(ba,"SPAN",{});var Sa=o(Te);u(J.$$.fragment,Sa),Sa.forEach(a),ba.forEach(a),mt=p(at),Ee=s(at,"SPAN",{});var Fa=o(Ee);ft=c(Fa,"Overview"),Fa.forEach(a),at.forEach(a),Be=p(e),I=s(e,"P",{});var nt=o(I);ut=c(nt,"The FastSpeech2 model was proposed in "),U=s(nt,"A",{href:!0,rel:!0});var $a=o(U);gt=c($a,"FastSpeech 2: Fast and High-Quality End-to-End Text to Speech"),$a.forEach(a),_t=c(nt,"  by Ren et. al. FastSpeech2 is an all-encoder transformer text-to-speech (TTS) model that outputs mel-spectrograms given user input text."),nt.forEach(a),We=p(e),de=s(e,"P",{});var wa=o(de);vt=c(wa,"The abstract from the paper is the following:"),wa.forEach(a),Qe=p(e),j=s(e,"P",{});var st=o(j);yt=c(st,"Non-autoregressive text to speech (TTS) models such as FastSpeech (Ren et al., 2019) can synthesize speech significantly faster than previous autoregressive mod- els with comparable quality. The training of FastSpeech model relies on an au- toregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in out- put), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has sev- eral disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accu- rate enough, and the target mel-spectrograms distilled from teacher model suf- fer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the is- sues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simpli- fied output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifi- cally, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x train- ing speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and Fast- Speech 2 can even surpass autoregressive models. Audio samples are available at "),G=s(st,"A",{href:!0,rel:!0});var Ta=o(G);kt=c(Ta,"https://speechresearch.github.io/fastspeech2/"),Ta.forEach(a),bt=c(st,"."),st.forEach(a),Je=p(e),S=s(e,"P",{});var ve=o(S);St=c(ve,"This model was contributed by "),K=s(ve,"A",{href:!0,rel:!0});var Ea=o(K);Ft=c(Ea,"jaketae"),Ea.forEach(a),$t=c(ve,". The original code can be found "),X=s(ve,"A",{href:!0,rel:!0});var xa=o(X);wt=c(xa,"here"),xa.forEach(a),Tt=c(ve,"."),ve.forEach(a),Ue=p(e),x=s(e,"H2",{class:!0});var ot=o(x);M=s(ot,"A",{id:!0,class:!0,href:!0});var za=o(M);xe=s(za,"SPAN",{});var Ca=o(xe);u(Y.$$.fragment,Ca),Ca.forEach(a),za.forEach(a),Et=p(ot),ze=s(ot,"SPAN",{});var Pa=o(ze);xt=c(Pa,"FastSpeech2Config"),Pa.forEach(a),ot.forEach(a),Ge=p(e),k=s(e,"DIV",{class:!0});var B=o(k);u(Z.$$.fragment,B),zt=p(B),z=s(B,"P",{});var ye=o(z);Ct=c(ye,"This is the configuration class to store the configuration of a "),he=s(ye,"A",{href:!0});var qa=o(he);Pt=c(qa,"~FastSpeech2Model"),qa.forEach(a),qt=c(ye,`. It is used to instantiate an
FastSpeech2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the FastSpeech2
`),ee=s(ye,"A",{href:!0,rel:!0});var Aa=o(ee);At=c(Aa,"fastspeech2"),Aa.forEach(a),Dt=c(ye," architecture."),ye.forEach(a),Lt=p(B),C=s(B,"P",{});var ke=o(C);Nt=c(ke,"Configuration objects inherit from "),me=s(ke,"A",{href:!0});var Da=o(me);It=c(Da,"PretrainedConfig"),Da.forEach(a),jt=c(ke,` and can be used to control the model outputs. Read the
documentation from `),fe=s(ke,"A",{href:!0});var La=o(fe);Mt=c(La,"PretrainedConfig"),La.forEach(a),Ot=c(ke," for more information."),ke.forEach(a),Ht=p(B),u(te.$$.fragment,B),B.forEach(a),Ke=p(e),P=s(e,"H2",{class:!0});var rt=o(P);O=s(rt,"A",{id:!0,class:!0,href:!0});var Na=o(O);Ce=s(Na,"SPAN",{});var Ia=o(Ce);u(ae.$$.fragment,Ia),Ia.forEach(a),Na.forEach(a),Rt=p(rt),Pe=s(rt,"SPAN",{});var ja=o(Pe);Vt=c(ja,"FastSpeech2Tokenizer"),ja.forEach(a),rt.forEach(a),Xe=p(e),m=s(e,"DIV",{class:!0});var b=o(m);u(ne.$$.fragment,b),Bt=p(b),qe=s(b,"P",{});var Ma=o(qe);Wt=c(Ma,"Construct a FastSpeech2 tokenizer. Based on byte-level Byte-Pair-Encoding."),Ma.forEach(a),Qt=p(b),F=s(b,"DIV",{class:!0});var be=o(F);u(se.$$.fragment,be),Jt=p(be),Ae=s(be,"P",{});var Oa=o(Ae);Ut=c(Oa,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens.`),Oa.forEach(a),Gt=p(be),De=s(be,"P",{});var Ha=o(De);Kt=c(Ha,"This implementation does not add special tokens and this method should be overridden in a subclass."),Ha.forEach(a),be.forEach(a),Xt=p(b),H=s(b,"DIV",{class:!0});var it=o(H);u(oe.$$.fragment,it),Yt=p(it),q=s(it,"P",{});var Se=o(q);Zt=c(Se,`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Le=s(Se,"CODE",{});var Ra=o(Le);ea=c(Ra,"prepare_for_model"),Ra.forEach(a),ta=c(Se," or "),Ne=s(Se,"CODE",{});var Va=o(Ne);aa=c(Va,"encode_plus"),Va.forEach(a),na=c(Se," methods."),Se.forEach(a),it.forEach(a),sa=p(b),$=s(b,"DIV",{class:!0});var Fe=o($);u(re.$$.fragment,Fe),oa=p(Fe),ue=s(Fe,"P",{});var ga=o(ue);ra=c(ga,"Create the token type IDs corresponding to the sequences passed. "),ge=s(ga,"A",{href:!0});var Ba=o(ge);ia=c(Ba,`What are token type
IDs?`),Ba.forEach(a),ga.forEach(a),ca=p(Fe),Ie=s(Fe,"P",{});var Wa=o(Ie);la=c(Wa,"Should be overridden in a subclass if the model has a special way of building those."),Wa.forEach(a),Fe.forEach(a),pa=p(b),R=s(b,"DIV",{class:!0});var ct=o(R);u(ie.$$.fragment,ct),da=p(ct),je=s(ct,"P",{});var Qa=o(je);ha=c(Qa,"Save the vocabulary and special tokens file to a directory."),Qa.forEach(a),ct.forEach(a),b.forEach(a),Ye=p(e),A=s(e,"H2",{class:!0});var lt=o(A);V=s(lt,"A",{id:!0,class:!0,href:!0});var Ja=o(V);Me=s(Ja,"SPAN",{});var Ua=o(Me);u(ce.$$.fragment,Ua),Ua.forEach(a),Ja.forEach(a),ma=p(lt),Oe=s(lt,"SPAN",{});var Ga=o(Oe);fa=c(Ga,"FastSpeech2Model"),Ga.forEach(a),lt.forEach(a),Ze=p(e),D=s(e,"DIV",{class:!0});var pt=o(D);u(le.$$.fragment,pt),ua=p(pt),_e=s(pt,"DIV",{class:!0});var Ka=o(_e);u(pe.$$.fragment,Ka),Ka.forEach(a),pt.forEach(a),this.h()},h(){r(w,"name","hf:doc:metadata"),r(w,"content",JSON.stringify(on)),r(L,"id","fastspeech2"),r(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(L,"href","#fastspeech2"),r(T,"class","relative group"),r(N,"id","overview"),r(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(N,"href","#overview"),r(E,"class","relative group"),r(U,"href","https://arxiv.org/abs/2006.04558"),r(U,"rel","nofollow"),r(G,"href","https://speechresearch.github.io/fastspeech2/"),r(G,"rel","nofollow"),r(K,"href","https://huggingface.co/jaketae"),r(K,"rel","nofollow"),r(X,"href","https://github.com/pytorch/fairseq"),r(X,"rel","nofollow"),r(M,"id","transformers.FastSpeech2Config"),r(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(M,"href","#transformers.FastSpeech2Config"),r(x,"class","relative group"),r(he,"href","/docs/transformers/pr_15773/en/model_doc/fastspeech2#transformers.FastSpeech2Model"),r(ee,"href","https://huggingface.co/fastspeech2"),r(ee,"rel","nofollow"),r(me,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),r(fe,"href","/docs/transformers/pr_15773/en/main_classes/configuration#transformers.PretrainedConfig"),r(k,"class","docstring"),r(O,"id","transformers.FastSpeech2Tokenizer"),r(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(O,"href","#transformers.FastSpeech2Tokenizer"),r(P,"class","relative group"),r(F,"class","docstring"),r(H,"class","docstring"),r(ge,"href","../glossary#token-type-ids"),r($,"class","docstring"),r(R,"class","docstring"),r(m,"class","docstring"),r(V,"id","transformers.FastSpeech2Model"),r(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(V,"href","#transformers.FastSpeech2Model"),r(A,"class","relative group"),r(_e,"class","docstring"),r(D,"class","docstring")},m(e,d){t(document.head,w),h(e,Re,d),h(e,T,d),t(T,L),t(L,$e),g(Q,$e,null),t(T,dt),t(T,we),t(we,ht),h(e,Ve,d),h(e,E,d),t(E,N),t(N,Te),g(J,Te,null),t(E,mt),t(E,Ee),t(Ee,ft),h(e,Be,d),h(e,I,d),t(I,ut),t(I,U),t(U,gt),t(I,_t),h(e,We,d),h(e,de,d),t(de,vt),h(e,Qe,d),h(e,j,d),t(j,yt),t(j,G),t(G,kt),t(j,bt),h(e,Je,d),h(e,S,d),t(S,St),t(S,K),t(K,Ft),t(S,$t),t(S,X),t(X,wt),t(S,Tt),h(e,Ue,d),h(e,x,d),t(x,M),t(M,xe),g(Y,xe,null),t(x,Et),t(x,ze),t(ze,xt),h(e,Ge,d),h(e,k,d),g(Z,k,null),t(k,zt),t(k,z),t(z,Ct),t(z,he),t(he,Pt),t(z,qt),t(z,ee),t(ee,At),t(z,Dt),t(k,Lt),t(k,C),t(C,Nt),t(C,me),t(me,It),t(C,jt),t(C,fe),t(fe,Mt),t(C,Ot),t(k,Ht),g(te,k,null),h(e,Ke,d),h(e,P,d),t(P,O),t(O,Ce),g(ae,Ce,null),t(P,Rt),t(P,Pe),t(Pe,Vt),h(e,Xe,d),h(e,m,d),g(ne,m,null),t(m,Bt),t(m,qe),t(qe,Wt),t(m,Qt),t(m,F),g(se,F,null),t(F,Jt),t(F,Ae),t(Ae,Ut),t(F,Gt),t(F,De),t(De,Kt),t(m,Xt),t(m,H),g(oe,H,null),t(H,Yt),t(H,q),t(q,Zt),t(q,Le),t(Le,ea),t(q,ta),t(q,Ne),t(Ne,aa),t(q,na),t(m,sa),t(m,$),g(re,$,null),t($,oa),t($,ue),t(ue,ra),t(ue,ge),t(ge,ia),t($,ca),t($,Ie),t(Ie,la),t(m,pa),t(m,R),g(ie,R,null),t(R,da),t(R,je),t(je,ha),h(e,Ye,d),h(e,A,d),t(A,V),t(V,Me),g(ce,Me,null),t(A,ma),t(A,Oe),t(Oe,fa),h(e,Ze,d),h(e,D,d),g(le,D,null),t(D,ua),t(D,_e),g(pe,_e,null),et=!0},p:tn,i(e){et||(_(Q.$$.fragment,e),_(J.$$.fragment,e),_(Y.$$.fragment,e),_(Z.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(ce.$$.fragment,e),_(le.$$.fragment,e),_(pe.$$.fragment,e),et=!0)},o(e){v(Q.$$.fragment,e),v(J.$$.fragment,e),v(Y.$$.fragment,e),v(Z.$$.fragment,e),v(te.$$.fragment,e),v(ae.$$.fragment,e),v(ne.$$.fragment,e),v(se.$$.fragment,e),v(oe.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(ce.$$.fragment,e),v(le.$$.fragment,e),v(pe.$$.fragment,e),et=!1},d(e){a(w),e&&a(Re),e&&a(T),y(Q),e&&a(Ve),e&&a(E),y(J),e&&a(Be),e&&a(I),e&&a(We),e&&a(de),e&&a(Qe),e&&a(j),e&&a(Je),e&&a(S),e&&a(Ue),e&&a(x),y(Y),e&&a(Ge),e&&a(k),y(Z),y(te),e&&a(Ke),e&&a(P),y(ae),e&&a(Xe),e&&a(m),y(ne),y(se),y(oe),y(re),y(ie),e&&a(Ye),e&&a(A),y(ce),e&&a(Ze),e&&a(D),y(le),y(pe)}}}const on={local:"fastspeech2",sections:[{local:"overview",title:"Overview"},{local:"transformers.FastSpeech2Config",title:"FastSpeech2Config"},{local:"transformers.FastSpeech2Tokenizer",title:"FastSpeech2Tokenizer"},{local:"transformers.FastSpeech2Model",title:"FastSpeech2Model"}],title:"FastSpeech2"};function rn(_a){return an(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hn extends Xa{constructor(w){super();Ya(this,w,rn,sn,Za,{})}}export{hn as default,on as metadata};
